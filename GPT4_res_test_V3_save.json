{"621454435aee126c0f1d7065": ["Based on the provided text and the titles of the referenced papers, the source papers that most directly inspired the given paper seem to be:\n\n{\"b39\": 0.9, \"b11\": 0.8, \"b42\": 0.8, \"b36\": 0.8, \"b44\": 0.7, \"b33\": 0.7, \"b21\": 0.7, \"b46\": 0.9, \"b5\": 0.8}\n\nExplanation:\n- b39, b11, b42, and b36: These references are explicitly mentioned in the context of the core methodology (automated fact-checking and evidence retrieval) employed by the paper.\n- b33 and b44: These references are mentioned in the context of successful applications of the fact-checking framework, implying that the author(s) may have been inspired by these works.\n- b21: This reference is cited in relation to the challenge of misinformation infiltration in fact-checking repositories, which is a key problem addressed by the paper.\n- b46 and b5: These references are cited in the context of the potential for NLP tools to generate convincing disinformation, a central concern of the paper.\n\nThe confidence scores are estimated based on how directly each reference is associated with the methodology, challenges, or novel contributions of the paper.", "Based on the text and the titles of the referenced papers, the following papers appear to be most influential to the author's work:\n\n1. \"Automated fact checking task formulations methods and future directions\" [b39]: This paper is referenced multiple times throughout the text, indicating that it is a significant source of inspiration for the author's work on automated fact-checking.\n\n2. \"Fact or fiction verifying scientific claims\" [b42]: This paper is also referenced multiple times, particularly in the context of the author's work on claim verification against a large repository of evidence documents.\n\n3. \"Defending against neural fake news\" [b46]: This paper is relevant to the author's discussion on the potential misuse of NLP tools for generating coherent disinformation, which could pose significant challenges for automated fact-checking systems.\n\n4. \"Truth lies and automation how language models could change disinformation\" [b5]: This paper is referenced in the same context as [b46], further highlighting its relevance to the author's work.\n\n5. \"The malicious use of artificial intelligence forecasting prevention and mitigation\" [b4]: This paper is referenced in the section discussing the potential dangers of disinformation campaigns and the role of NLP tools in such scenarios.\n\n6. \"Language models as fact checkers\" [b23]: This paper's title suggests a direct alignment with the author's work, making it a likely source of inspiration.\n\nBased on these observations, the confidence scores (from 0 to 1) for these papers as sources of inspiration could be as follows:\n\n{\n\"b39\": 0.9,\n\"b42\": 0.8,\n\"b46\": 0.7,\n\"b5\": 0.7,\n\"b4\": 0.6,\n\"b23\": 0.6\n}", "Based on the text and titles of the papers, the following references seem to be the most relevant source papers that inspire the author:\n\n{ \n\"b39\": 0.9,\n\"b46\": 0.8,\n\"b42\": 0.8,\n\"b36\": 0.7,\n\"b33\": 0.7,\n\"b13\": 0.6,\n\"b3\": 0.6,\n\"b21\": 0.7\n} \n\nThe above references have been chosen based on their relevance to the research paper's novel methods, challenges, and contributions. The confidence score is assigned based on the frequency and context of their mentions in the paper. Note that the confidence score is subject to further verification and may change based on a more detailed analysis of the paper.", "Based on the text and the titles of the references, the source papers that most directly inspired the given paper could be:\n\n1. \"Automated fact checking: task formulations methods and future directions\" [b39] (confidence score: 0.9)\n2. \"FEVER a large-scale dataset for fact extraction and verification\" [b40] (confidence score: 0.8)\n3. \"Scientific claim verification with vert erini\" [b33] (confidence score: 0.8)\n4. \"The spread of true and false news online\" [b41] (confidence score: 0.7)\n5. \"Truth lies and automation how language models could change disinformation\" [b5] (confidence score: 0.7)\n6. \"Defending against neural fake news\" [b46] (confidence score: 0.7)\n\nNote: The confidence score is given on a scale of 0 to 1, where 1 indicates high confidence in the relevance and influence of the source paper on the given paper. The scores are assigned based on the frequency of citation, the context of citation, and the relevance of the source paper's title to the given paper's content.", "Based on the text provided, the following references seem to be the most pertinent source papers:\n\n1. 'b39': 'automated fact checking: task formulations methods and future directions' - The paper heavily relies on the concepts and methods outlined in this work. Confidence Score: 0.9\n2. 'b46': 'defending against neural fake news' - This paper is directly related to the topic of disinformation and its impacts on fact-checking, which is a central theme in the given paper. Confidence Score: 0.85\n3. 'b42': 'fact or fiction: verifying scientific claims' - This reference is mentioned multiple times, indicating its importance in the development of the paper's core contributions. Confidence Score: 0.8\n4. 'b36': 'COVIDFACT: Fact extraction and verification of real-world claims on COVID pandemic' - The paper discusses the application of fact-checking in the context of the COVID-19 pandemic, making this source directly relevant. Confidence Score: 0.75\n5. 'b21': 'disinformation on the web: impact, characteristics and detection of Wikipedia hoaxes' - The given paper discusses the presence of misinformation in repositories like Wikipedia, which is a topic addressed in this reference. Confidence Score: 0.7\n6. 'b5': 'truth, lies and automation: how language models could change disinformation' - This paper addresses the impact of language models on disinformation, a topic that is central to the text provided. Confidence Score: 0.65\n7. 'b33': 'scientific claim verification with vert erini' - This reference is related to the scientific fact verification systems which are discussed in the paper. Confidence Score: 0.6\n\nThe JSON format of this result is: \n{\n\"b39\": 0.9,\n\"b46\": 0.85,\n\"b42\": 0.8,\n\"b36\": 0.75,\n\"b21\": 0.7,\n\"b5\": 0.65,\n\"b33\": 0.6\n}"], "6221834e5aee126c0f23c25c": ["Based on the text and the titles of the references, the most pertinent source papers are:\n\n- \"dataset condensation with gradient matching\" [b53]: The paper directly refers to this work and makes multiple comparisons, indicating strong reliance on it for developing its core contributions.\n- \"dataset condensation with distribution matching\" [b52]: This source is also directly mentioned and compared with the paper's proposed method, indicating that it has had a substantial influence on the author's approach.\n- \"dataset condensation with differentiable siamese augmentation\" [b51]: This paper is mentioned as part of the development in the field of dataset condensation, indicating that it likely influenced the author's approach.\n- \"generative teaching networks accelerating neural architecture search by learning to generate synthetic training data\" [b31]: This paper is mentioned in context of synthetic set generation, which is a key contribution of the current paper.\n- \"approximating extent measures of points\" [b0]: This paper is mentioned in the context of coreset selection, suggesting that it influenced the author's approach.\n\nBased on the text, these papers have directly contributed to the development of the author's algorithm or methodology. \n\nHere is the output in JSON format:\n\n{\n  \"b53\": 0.9,\n  \"b52\": 0.8,\n  \"b51\": 0.7,\n  \"b31\": 0.7,\n  \"b0\": 0.6\n}\n\nThe confidence scores reflect the degree of influence each paper appears to have had on the author's work, with 1 being the highest possible score.", "Based on the text and the titles of the references, the most pertinent source papers are:\n\n1. [b53]: 'Dataset condensation with gradient matching' - The method proposed in the paper is explicitly compared with this method and it's mentioned multiple times in the text. Confidence score: 0.9\n2. [b37]: Title not given - The paper refers to it as 'the pioneering approach' and is mentioned multiple times in the text. Confidence score: 0.8\n3. [b52]: 'Dataset condensation with distribution matching' - The paper compares its method with this one and lists the differences. Confidence score: 0.8\n4. [b51]: 'Dataset condensation with differentiable siamese augmentation' - The paper compares its method with this one. Confidence score: 0.7\n5. [b0]: 'Approximating extent measures of points' - The paper refers to it when discussing coreset or subset selection. Confidence score: 0.6\n6. [b28]: 'Active learning for convolutional neural networks a core set approach' - The paper refers to it when discussing coreset or subset selection. Confidence score: 0.6\n\nAnswer: \n{\n\"b53\": 0.9,\n\"b37\": 0.8,\n\"b52\": 0.8,\n\"b51\": 0.7,\n\"b0\": 0.6,\n\"b28\": 0.6\n}", "Based on the text and the title of the references, the following references appear to be the most relevant as source papers:\n\n1. 'b53': 'dataset condensation with gradient matching'\n2. 'b37': Unknown Title\n3. 'b52': 'dataset condensation with distribution matching'\n4. 'b51': 'dataset condensation with differentiable siamese augmentation'\n5. 'b31': 'generative teaching networks accelerating neural architecture search by learning to generate synthetic training data'\n6. 'b5': 'super samples from kernel herding'\n7. 'b0': 'approximating extent measures of points'\n8. 'b28': 'active learning for convolutional neural networks a core set approach'\n\nThe text frequently mentions the work of [b37] and [b53], indicating direct inspiration from these papers. The other papers are mentioned in relation to the approaches and methods used in the paper. For example, [b52] and [b51] are mentioned in relation to dataset condensation, while [b31] is mentioned for its approach to generating synthetic training data. [b5], [b0], and [b28] are mentioned as methodologies to condense the training set.\n\nHere is the confidence score for each source paper:\n\n{\"b53\": 0.9, \"b37\": 0.9, \"b52\": 0.8, \"b51\": 0.8, \"b31\": 0.7, \"b5\": 0.7, \"b0\": 0.7, \"b28\": 0.7}", "Based on the text of the academic paper, the source papers that have most inspired the author are:\n\n1. \"Dataset condensation with gradient matching\" [b53]\n2. \"Dataset condensation with distribution matching\" [b52]\n3. \"Dataset condensation with differentiable siamese augmentation\" [b51]\n4. \"Dataset meta learning from kernel ridge regression\" [b23]\n5. \"Generative teaching networks accelerating neural architecture search by learning to generate synthetic training data\" [b31]\n6. \"Online coreset selection for rehearsal based continual learning\" [b46]\n7. \"A ptas for k means clustering based on weak coresets\" [b12]\n\nThe confidence scores given to each paper are inferred based on the number of mentions and the context in which they were mentioned throughout the paper text. The paper [b53] is mentioned multiple times and in critical parts of the paper, particularly in relation to the challenges and solutions proposed by the author. The same applies to [b52] and [b51], which are also cited as important references in the development of the author's work. The other papers were chosen due to their relevance to the methods proposed by the author.\n\n{\n  \"b53\": 1,\n  \"b52\": 0.9,\n  \"b51\": 0.9,\n  \"b23\": 0.8,\n  \"b31\": 0.8,\n  \"b46\": 0.7,\n  \"b12\": 0.7\n}", "Based on the provided text and the title of the references, the following references appear to be source papers that inspired the author's work:\n\n1. \"b53\": \"dataset condensation with gradient matching\" - This paper is mentioned multiple times, and the authors contrast their approach to it.\n2. \"b37\": (no title provided) - The authors describe this paper as a pioneering approach but also mention its limitations.\n3. \"b52\": \"dataset condensation with distribution matching\" - The authors compare their approach to this paper, highlighting differences.\n4. \"b51\": \"dataset condensation with differentiable siamese augmentation\" - This paper is discussed in relation to the author's own method.\n5. \"b5\": \"super samples from kernel herding\" - Mentioned in the context of existing approaches to the problem tackled by the authors.\n6. \"b28\": \"active learning for convolutional neural networks a core set approach\" - Mentioned as an example of coreset or subset selection methods.\n\nThe confidence scores, which are subjective and based on the frequency and context of these references in the text, could be:\n\n{ \"b53\": 0.9, \"b37\": 0.8, \"b52\": 0.75, \"b51\": 0.7, \"b5\": 0.65, \"b28\": 0.65 }"], "621ee1835aee126c0f26a902": ["Based on the provided text, the primary challenges outlined in the paper are inadequate generalization, incremental overfitting, and un-scalable development of Graph Neural Networks (GNNs). The proposed algorithm is called GraphWorld, a tunable, scalable, and reproducible method for analyzing the performance of GNN models on synthetic benchmark data.\n\nThe key phrases that indicate source papers inspiring this work are found in the following sentences:\n\n1. \u201cGraph Neural Networks (GNNs) have extended the benefits of deep learning to the non-Euclidean domain, allowing for standardized and re-usable machine learning approaches to problems that involve relational (graph-structured) data [b47]\u201d\n2. \u201cGNNs now admit an extremely wide range of architectures and possible tasks, including node classification, whole-graph classification, and link prediction [b7]\u201d\n3. \u201cDespite the proliferation of new GNN models, only a few handpicked benchmarked datasets are currently used to evaluate them [b17]\u201d\n4. \u201cGraphWorld is directly motivated by challenges observed in developing and applying GNNs to a wide variety of datasets at Google.\u201d\n5. \u201cWe currently use GraphWorld for model understanding, and will soon be incorporating it into the newly-released TF-GNN package [b0]\u201d\n6. \u201c(3) Insights. We use GraphWorld to conduct large-scale experimental study on over 1 million graph datasets for each of three GNN tasks -node classification, link prediction, and graph property prediction.\u201d\n\nBased on these phrases, the source papers that inspired their work are [b47], [b7], [b17], and [b0].\n\nHere is the result in JSON format:\n{\n   \"b47\": 0.9,\n   \"b7\": 0.8,\n   \"b17\": 0.7,\n   \"b0\": 0.6\n} \n\nThe confidence score is assigned based on the occurrence and relevance of the paper in the text, with [b47] being the most relevant and [b0] being the least among the four.", "Based on the given text and the titles of the referenced papers, the most relevant source papers seem to be:\n\n{\"b1\": 0.8, \"b17\": 0.9, \"b3\": 0.7, \"b22\": 0.7, \"b48\": 0.7, \"b47\": 0.8, \"b14\": 0.7, \"b33\": 0.6}\n\n- \"b1\": 'community detection and stochastic block models recent developments' - This paper is likely to be a source as it talks about Stochastic Block Models which are used in the text for synthetic dataset generation.\n- \"b17\": 'open graph benchmark datasets for machine learning on graphs' - This paper is a source as it is mentioned in the context of benchmark datasets which is a major theme in the given text.\n- \"b3\": 'graph neural networks with convolutional filters' - This paper is likely to be a source as it discusses graph neural networks with convolutional filters, an architecture mentioned in the text.\n- \"b22\": 'semi-supervised classification with graph convolutional networks' - This paper is likely to be a source as it discusses a seminal model that averages neighbor state at each iteration.\n- \"b48\": 'how powerful are graph neural networks arxiv preprint' - This paper is likely to be a source as it discusses the power of GNNs, a topic that is discussed in the text.\n- \"b47\": 'a comprehensive survey on graph neural networks' - This paper seems like a source as it is a comprehensive survey on GNNs, and the text heavily discusses GNNs.\n- \"b14\": 'fast graph representation learning with pytorch geometric' - This paper is likely to be a source as it discusses the PyTorch-Geometric library which is used in the experiments of the paper.\n- \"b33\": 'do cifar classifiers generalize to cifar' - This paper is likely to be a source as it discusses the problem of overfitting, a theme that is also discussed in the text.", "Based on the text of the paper and the given references, the source papers which inspired the author the most can be identified as:\n\n- \"Machine learning on graphs a model and comprehensive taxonomy [b7]\": The author mentions the wide range of tasks that can be accomplished with GNNs, including node classification, whole-graph classification, and link prediction, which is the main focus of this paper.\n\n- \"A fair comparison of graph neural networks for graph classification [b13]\": The author emphasizes the need for fair comparisons of GNN models in reproducible settings, which indicates that this paper had significant influence on the methodology.\n\n- \"Open graph benchmark datasets for machine learning on graphs [b17]\": The author criticizes the limited scope of benchmark datasets and their potential for architectural overfitting, which is the main point of this paper.\n\n- \"Community detection and stochastic block models recent developments [b1]\": The author uses Degree-Corrected Stochastic Block Model (DC-SBM) for the synthetic dataset generation, indicating its influence on the paper.\n\n- \"Graph neural networks with feature wise linear modulation [b5]\": As one of the GNN models tested in the paper, it shows its influence on the author's method.\n\n- \"Fast graph representation learning with pytorch geometric [b14]\": The author uses the reference implementation from the PyTorch-Geometric library for GNN models, indicating its influence on the implementation.\n\nTherefore, the most pertinent source papers are: b7, b13, b17, b1, b5, b14.\n\nThe json format result is:\n{\n  \"b7\": 0.8,\n  \"b13\": 0.8,\n  \"b17\": 0.8,\n  \"b1\": 0.8,\n  \"b5\": 0.8,\n  \"b14\": 0.8\n}", "Based on the text and the titles of the references, the source papers that inspire the given paper the most are:\n\n{\n\"b1\": 0.8,\n\"b12\": 0.9,\n\"b17\": 1.0,\n\"b22\": 0.7,\n\"b30\": 0.8,\n\"b33\": 0.7,\n\"b47\": 0.9,\n\"b48\": 0.8\n}\n\nExplanation:\nReference b1 (Community detection and stochastic block models recent developments) is mentioned in the context of generating synthetic datasets, suggesting it inspired the GraphWorld method.\nb12 (Benchmarking graph neural networks) is referred in the context of comparative benchmarking studies, which are crucial for the proposed work.\nb17 (Open graph benchmark datasets for machine learning on graphs) is mentioned multiple times, indicating its significant influence on the paper's work.\nb22 (Semi-supervised classification with graph convolutional networks) and b30 (TUDataset a collection of benchmark datasets for learning with graphs) are listed in the context of graph neural network models and benchmark datasets, suggesting their methods may have been adapted in this paper.\nb33 (Do CIFAR classifiers generalize to CIFAR?) is mentioned in the context of overfitting, a challenge the paper addresses.\nb47 (A comprehensive survey on graph neural networks) and b48 (How powerful are graph neural networks) are mentioned in the context of explaining the development and application of GNNs, suggesting they shape the paper's understanding of the field. \n\nThe confidence scores are assigned based on the frequency and context in which the references are mentioned in the text, with a higher score indicating a higher level of influence.", "Based on the provided paper text and the titles of the references, the source papers that most directly influenced the main contributions of this paper are likely to be:\n\n1. \"A comprehensive survey on graph neural networks\" [b47]. This is a survey paper on Graph Neural Networks, which is the main topic of the provided paper.\n2. \"Design space for graph neural networks\" [b49]. The paper mentioned the need for proper GNN experimental design which indicates a possible influence from this paper.\n3. \"Benchmarking graph neural networks\" [b12]. This paper might be a significant reference as the provided paper talks about benchmark datasets multiple times.\n4. \"Open graph benchmark datasets for machine learning on graphs\" [b17]. The paper mentioned benchmark datasets and their limitations, which suggests that it might have been influenced by this reference.\n5. \"Pitfalls of graph neural network evaluation\" [b36]. This paper might have influenced the provided paper on its discussion about the challenges in evaluating GNNs.\n6. \"A pipeline for fair comparison of graph neural networks in node classification tasks\" [b50]. The paper's focus on the performance of GNN models and comparisons between them indicates a possible influence from this source.\n7. \"How powerful are graph neural networks\" [b48]. This paper is likely to be a key source as it discusses the capabilities of GNNs, which is a central theme of the provided paper.\n\nThe JSON format of the result with confidence scores is:\n{\"b47\": 0.9, \"b49\": 0.85, \"b12\": 0.8, \"b17\": 0.7, \"b36\": 0.7, \"b50\": 0.8, \"b48\": 0.9}"], "621635aa91e011b46d7ce15d": ["Based on the text and the titles of the references, the source papers that inspired this paper are:\n\n1. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\" (b2): This paper is directly mentioned as the inspiration for the Ada-GNN framework in the paper and the use of MAML for training a base model that can be adapted quickly for a new task.\n   \n2. \"A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs\" (b7): This paper is cited as the method used for partitioning the graph into multiple non-overlapped subgroups in Ada-GNN.\n\n3. \"Scalable Inception Graph Neural Networks\" (b14): This paper is referred multiple times in the context of large-scale graphs, and it seems to be the base model used in the Ada-GNN framework.\n\n4. \"Graph Convolutional Neural Networks for Web Scale Recommender Systems\" (b24): This paper is mentioned in the context of recommendation systems which is one of the application areas of GNNs discussed in the paper.\n\n5. \"Graph Sampling Based Inductive Learning Method\" (b25): This paper is referred in the context of subgraph generation, which is part of the Ada-GNN framework.\n\n6. \"Scalable and Adaptive Graph Neural Networks with Self Label Enhanced Training\" (b16): This paper is used as an example of decoupling based GNN models and is referred to in the context of the SAGN model used in the Ada-GNN framework.\n\n7. \"Inductive Representation Learning on Large Graphs\" (b3): This paper is mentioned in the context of neighborhood sampling to control the number of neighbors to be aggregated, an approach used in the Ada-GNN framework.\n\nThe confidence score for each reference is:\n\n{\n\"b2\" : 0.9,\n\"b7\" : 0.8,\n\"b14\" : 0.7,\n\"b24\" : 0.6,\n\"b25\" : 0.6,\n\"b16\" : 0.5,\n\"b3\" : 0.5\n}", "Based on the text of the paper and the titles of the references, the source papers that inspire the author most are likely to be:\n\n1. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\" [b2]: The paper frequently mentions being inspired by this work and its framework, which is central to the proposed approach. Confidence score: 0.9\n\n2. \"Scalable and Adaptive Graph Neural Networks with Self Label Enhanced Training\" [b16]: The paper mentions this work in the context of the methodological approach. Confidence score: 0.8\n\n3. \"Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks\" [b1]: This paper is referenced as a strategy for large scale graphs, and its approach is contrasted with the proposed method. Confidence score: 0.75\n\n4. \"SIGN: Scalable Inception Graph Neural Networks\" [b14]: This work is frequently mentioned, indicating a strong reliance on its research. Confidence score: 0.85\n\n5. \"GraphSAGE: Inductive Representation Learning on Large Graphs\" [b3]: The paper refers to this work as an example of a technique for large-scale graphs, indicating influence. Confidence score: 0.7\n\n6. \"A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs\" [b7]: This work is mentioned multiple times in the context of graph partitioning, which is a key part of the proposed method. Confidence score: 0.8\n\n7. \"Policy-GNN: Aggregation Optimization for Graph Neural Networks\" [b9]: This paper is mentioned in the context of personalized models for each node, indicating direct relevance to the paper's topic. Confidence score: 0.7\n\nJSON format:\n\n{\n\"b2\": 0.9,\n\"b16\": 0.8,\n\"b1\": 0.75,\n\"b14\": 0.85,\n\"b3\": 0.7,\n\"b7\": 0.8,\n\"b9\": 0.7\n}", "Based on the paper text and reference titles, the most pertinent source papers that influenced the given paper are:\n\n- 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks' [b2]: The paper's core contribution, Ada-GNN, is directly inspired by the Model-Agnostic Meta-Learning (MAML) framework proposed in this source. Ada-GNN adapts MAML's idea of learning a base model which can rapidly adapt to serve for new tasks, applying it to generate personalized models for different subgroups in a graph. \n\n- 'Cluster GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks' [b1]: The paper cites this source in relation to the use of graph partition methods to divide the original graph into multiple non-overlapped subgraphs. \n\n- 'SIGN: Scalable Inception Graph Neural Networks' [b14]: The paper cites that the results of performing separate performance tests on each subgroup using SIGN trained on the whole graph in advance. In addition, the authors take inspiration from this to design a new method for model personalization at the subgroup-level.\n\n- 'Policy-GNN: Aggregation Optimization for Graph Neural Networks' [b9]: The authors cite this source paper as prior work that also aimed to reflect the diversity among nodes, which is a similar problem they are addressing in their paper. \n\n- 'A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs' [b7]: The authors mention using the METIS algorithm proposed in this paper for graph partitioning. \n\nBased on these references, the source papers in JSON format with confidence scores would be:\n\n{\n  \"b2\": 1,\n  \"b1\": 0.7,\n  \"b14\": 0.8,\n  \"b9\": 0.6,\n  \"b7\": 0.7\n}", "Based on the text, the papers that most directly inspired the work are:\n\n1.  \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\" [b2] - The authors directly mention that their proposed framework, Ada-GNN, is inspired by the Model-Agnostic Meta-Learning (MAML) framework proposed in this paper. Confidence score: 1\n\n2. \"SIGN: Scalable Inception Graph Neural Networks\" [b14] - The authors use the SIGN method as a reference point, make comparisons, and show how their work improves upon it. Confidence score: 0.9\n\n3.  \"A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs\" [b7] - The authors make multiple references to using the METIS algorithm for graph partitioning, which is introduced in this paper. Confidence score: 0.9\n\n4.  \"GraphSAGE: Inductive Representation Learning on Large Graphs\" [b3] - The authors reference GraphSAGE as an example of a method for training GNNs efficiently on large-scale graphs. Confidence score: 0.8\n\n5.  \"Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks\" [b1] - The authors reference ClusterGCN as an example of a method for training GNNs efficiently on large-scale graphs. Confidence score: 0.8\n\n6.  \"Policy-GNN: Aggregation Optimization for Graph Neural Networks\" [b9] - The authors reference Policy-GNN as a method which has made progress on the personalized model for each node. Confidence score: 0.8\n\nThe JSON format of the result would be:\n{\n\"b2\": 1,\n\"b14\": 0.9,\n\"b7\": 0.9,\n\"b3\": 0.8,\n\"b1\": 0.8,\n\"b9\": 0.8\n}", "Based on the text and the titles of the cited papers, the source papers that likely most inspired the given paper are:\n\n{ \"b2\": 0.9,\n  \"b14\": 0.8,\n  \"b3\": 0.7,\n  \"b1\": 0.7,\n  \"b25\": 0.6,\n  \"b7\": 0.6,\n  \"b9\": 0.6,\n  \"b16\": 0.5 }\n\nThese papers are referenced at key points in the text and are linked to the methods and challenges that the authors address. For example, \"b2\" (Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks) is directly mentioned as the inspiration for the Ada-GNN framework. \"b14\" (SIGN: Scalable Inception Graph Neural Networks) and \"b3\" (Inductive Representation Learning on Large Graphs) are used in the comparison and performance tests. \"b1\" (Cluster GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks) and \"b7\" (A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs) are referenced related to the process of dividing the graph into non-overlapped subgraphs. \"b25\" (GraphSaint: Graph Sampling Based Inductive Learning Method) is referred to as a previous effort to generate subgraphs. \"b9\" (Policy-GNN: Aggregation Optimization for Graph Neural Networks) is mentioned as a previous approach to personalized models. And \"b16\" (Scalable and Adaptive Graph Neural Networks with Self Label Enhanced Training) is used as a base model in their methodology."], "620f0e735aee126c0fec4734": ["Based on the analysis of the text and the reference titles, the most pertinent source papers for this academic paper are:\n\n{\n\"b9\": 0.95,\n\"b10\": 0.9,\n\"b14\": 0.85,\n\"b33\": 0.8,\n\"b8\": 0.8,\n\"b11\": 0.75,\n\"b35\": 0.7,\n\"b18\": 0.65\n}\n\nThe paper heavily relies on the methods and insights from these references. It is \"inspired by\" the success in the vision and language domains, which have been widely adopted by recent advances of graph neural networks (GNNs) including contrastive learning (\"b9\"). The work also adopts methods and approaches from \"b10\", which provides datasets for machine learning on graphs. The paper introduces a new GCL framework that learns expressive node representations using a much simpler and parameter-less learning backbone, a concept that is linked to \"b14\". The paper also adopts structural contrastive learning module and semantic contrastive learning module, which are directly relevant to \"b33\", \"b8\", and \"b11\". The paper uses graph diffusion (\"b35\") for structure augmentation. Finally, the paper adopts a Bayesian non-parametric prototype inference algorithm for semantic contrastive learning (\"b18\").", "Based on the provided text and the given criteria, the following papers are potential source papers that inspired the given paper:\n\n1. \"Infograph: unsupervised and semi-supervised graph-level representation learning via mutual information maximization\" [b30]: It was mentioned in the introduction that the graph domain is labor-intensive and resource-expensive, and the authors were inspired by the success in the vision and language domains. This indicates that [b30] might have been a source of inspiration, given that it focuses on graph-level representation learning.\n\n2. \"A simple framework for contrastive learning of visual representations\" [b4]: This paper is referred to when discussing the application of an instance-level contrastive learning algorithm, suggesting it influenced the methodology used in the given paper.\n\n3. \"Graph neural networks with heterophily\" [b41]: This paper is referred to when discussing the limitations of the shallow encoding architecture of existing GCL frameworks, indicating it might have inspired the research direction of the paper.\n\n4. \"Deep graph Infomax (DGI)\" [b33]: This paper is mentioned in the related works section and seems to have inspired the contrastive learning approach applied to graph-structured data.\n\n5. \"Contrastive multi-view representation learning on graphs\" [b8]: This paper is referenced multiple times, suggesting that its methodologies have been adopted or have directly influenced the paper's core contributions.\n\n6. \"Multi-scale contrastive siamese networks for self-supervised graph representation learning\" [b11]: This paper is being compared with in the experiments section, indicating it might have inspired some aspects of the novel contributions of the paper.\n\n7. \"Graph contrastive learning with augmentations\" [b37]: This paper is mentioned when discussing the prevailing paradigm in self-supervised node representation learning, indicating it might be a source of inspiration.\n\nHere is the confidence score for each reference:\n{\n\"b30\": 0.8,\n\"b4\": 0.7,\n\"b41\": 0.7,\n\"b33\": 0.8,\n\"b8\": 0.8,\n\"b11\": 0.7,\n\"b37\": 0.7\n}", "Based on the given text and the titles of the references, the source papers that inspired the author's work are:\n\n1. \"Graph Neural Diffusion\" (b0): The paper mentions the use of graph diffusion for augmentation, which is likely inspired by this paper.\n\n2. \"A Simple Framework for Contrastive Learning of Visual Representations\" (b4): The concept of contrastive learning, which is a key part of the proposed algorithm, is likely inspired by this paper.\n\n3. \"Contrastive Multi-View Representation Learning on Graphs\" (b8): This paper is likely the source of the idea of contrastive learning on graphs.\n\n4. \"Revisiting K-Means: New Algorithms via Bayesian Nonparametrics\" (b17): This paper likely inspired the Bayesian non-parametric prototype inference algorithm used in the author's framework.\n\n5. \"Prototypical Contrastive Learning of Unsupervised Representations\" (b18): The author's use of prototypes and the concept of intra-cluster compactness and inter-cluster separability is likely inspired by this paper.\n\n6. \"Infograph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization\" (b30): This paper likely inspired the author's approach to graph-level representation learning.\n\n7. \"Graph Contrastive Learning with Augmentations\" (b37): This paper is likely a source of inspiration for the author's approach to graph contrastive learning.\n\n8. \"Deep Graph Contrastive Representation Learning\" (b43): This paper is likely another source of inspiration for the author's approach to graph contrastive learning.\n\nReturning the result in JSON format:\n\n{\n  \"b0\": 0.9,\n  \"b4\": 0.8,\n  \"b8\": 0.7,\n  \"b17\": 0.7,\n  \"b18\": 0.8,\n  \"b30\": 0.8,\n  \"b37\": 0.7,\n  \"b43\": 0.9\n}", "Based on the given information and criteria, the most pertinent source papers for the provided academic paper are:\n\n1. \"Graph neural diffusion\" [b0]\n2. \"A simple framework for contrastive learning of visual representations\" [b4]\n3. \"Contrastive multi-view representation learning on graphs\" [b8]\n4. \"Multi-scale contrastive siamese networks for self-supervised graph representation learning\" [b11]\n5. \"Revisiting k-means: New algorithms via Bayesian nonparametrics\" [b17]\n6. \"Prototypical contrastive learning of unsupervised representations\" [b18]\n7. \"Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization\" [b30]\n8. \"Deep graph contrastive representation learning\" [b43]\n\nThe JSON representation of the result with the confidence score is:\n\n{ \n   \"b0\":0.8,\n   \"b4\":0.9,\n   \"b8\":0.85,\n   \"b11\":0.7,\n   \"b17\":0.6,\n   \"b18\":0.75,\n   \"b30\":0.85,\n   \"b43\":0.9\n}", "Based on the provided text and the titles of the references, the most pertinent source papers that inspired the given paper are:\n\n1. \"Graph neural diffusion\" (b0): The paper discussed the idea of graph diffusion and its importance in their structural contrastive learning module.\n\n2. \"A simple framework for contrastive learning of visual representations\" (b4): The paper is mentioned in the context of applying contrastive learning methods in their structural contrastive learning module.\n\n3. \"Momentum contrast for unsupervised visual representation learning\" (b9): The paper described the usage of a momentum encoder, which is applied in their learning process.\n\n4. \"Contrastive multi-view representation learning on graphs\" (b8): The paper is referred to when discussing the success of contrastive learning in graph-structured data.\n\n5. \"Multi-scale contrastive siamese networks for self-supervised graph representation learning\" (b11): The paper is mentioned when discussing graph contrastive learning approaches.\n\n6. \"Revisiting k means new algorithms via bayesian nonparametrics\" (b17): The paper introduced a Bayesian non-parametric prototype inference algorithm, which is adopted in their semantic contrastive learning module.\n\n7. \"Prototypical contrastive learning of unsupervised representations\" (b18): The paper discussed the importance of intra-cluster compactness and inter-cluster separability which is applied in the semantic contrastive learning module.\n\nThe confidence scores for each of these papers are difficult to determine without a more detailed analysis of the text, but based on the above, I would estimate that each paper has a high level of relevance, perhaps a confidence score of around 0.8 - 1.\n\nHere is the result in JSON format:\n\n{\n    \"b0\": 0.9,\n    \"b4\": 0.9,\n    \"b9\": 0.8,\n    \"b8\": 0.8,\n    \"b11\": 0.9,\n    \"b17\": 1.0,\n    \"b18\": 1.0\n}"], "622183525aee126c0f23c7c2": ["Based on the provided text, the references that likely inspired the core contributions of the paper are:\n\n1. \"b1\": 'The BQ corpus: a large scale domain specific Chinese corpus for sentence semantic equivalence identification' - The paper uses the BQ corpus for its experiments, indicating that the work done in this reference is crucial to the presented study. Confidence score: 0.9.\n\n2. \"b9\": 'LCQMC: a large scale Chinese question matching corpus' - This paper also uses the LCQMC dataset in its experiments, implying that it has been heavily influenced by the work done in this reference. Confidence score: 0.9.\n\n3. \"b10\": 'LET: Linguistic knowledge Enhanced graph Transformer for Chinese short text matching' - This method is compared to the proposed approach, indicating that it's a source of inspiration or a benchmark for the study. Confidence score: 0.8.\n\n4. \"b3\": 'Neural graph matching networks for Chinese short text matching' - The paper mentions a neural graph matching method for Chinese short text matching which indicates that this could have influenced their proposed model. Confidence score: 0.7.\n\n5. \"b5\": 'Pre-training with whole word masking for Chinese BERT' - The paper uses a pretrained BERT model for its experiments, which is likely to have been influenced by this reference. Confidence score: 0.7.\n\n6. \"b6\": 'BERT: Pre-training of deep bidirectional transformers for language understanding' - The paper uses BERT model in its methodology, indicating that this reference is likely to have inspired their proposed model. Confidence score: 0.7.\n\nIt's important to note that while the paper mentions other references, these seem to be the most significant ones in relation to the paper's novel contributions. Other references might have played a role in shaping the broader context and background of the study, but they are not directly linked to the paper's core method and findings. \n\nThe JSON format of the result is:\n\n{\n  \"b1\": 0.9,\n  \"b9\": 0.9,\n  \"b10\": 0.8,\n  \"b3\": 0.7,\n  \"b5\": 0.7,\n  \"b6\": 0.7\n}", "Based on the content of the paper and the titles of the references, the source papers that most inspired this work are:\n\n- 'b1': 'the bq corpus a large scale domain specific chinese corpus for sentence semantic equivalence identification' (Confidence Score: 0.9)\n- 'b9': 'lcqmc a large scale chinese question matching corpus' (Confidence Score: 0.9)\n- 'b10': 'let linguistic knowledge enhanced graph transformer for chinese short text matching' (Confidence Score: 0.9)\n- 'b5': 'pre training with whole word masking for chinese bert' (Confidence Score: 0.8)\n- 'b15': 'ernie large scale knowledge enhanced pre training for language understanding and generation' (Confidence Score: 0.8)\n- 'b3': 'neural graph matching networks for chinese short text matching' (Confidence Score: 0.7)\n\nThe paper introduces a context-aware BERT matching model (CBM) for short text matching, which is influenced heavily by previous works on short text matching and pre-training models for Chinese Bert ('b5', 'b10', 'b3'). The corpus used ('b1', 'b9') also directly influence the methodology and results of the paper. 'b15' is included in the list as the paper appears to use a similar pre-training framework to the one proposed in 'b15'. The confidence scores are assigned based on the critical role these papers play in the development and evaluation of the CBM model.", "Based on the text and the titles of the references, the source papers that most likely inspired the author of the paper are:\n\n1. \"Neural graph matching networks for Chinese short text matching\" [b3]\n2. \"LET linguistic knowledge enhanced graph transformer for Chinese short text matching\" [b10]\n3. \"Pre-training with whole word masking for Chinese BERT\" [b5]\n4. \"The BQ corpus: a large-scale domain-specific Chinese corpus for sentence semantic equivalence identification\" [b1]\n5. \"LCQMC: a large-scale Chinese question matching corpus\" [b9]\n6. \"BERT: Pre-training of deep bidirectional transformers for language understanding\" [b6]\n7. \"ERNIE: large-scale knowledge enhanced pre-training for language understanding and generation\" [b15]\n\nThese papers are selected based on the mentions in the text, the methods, and the challenges outlined in the paper. The confidence scores are difficult to determine without knowing the full context and content of the source papers, but the list of papers should provide a good starting point.\n\n{\n\"b3\": 0.9,\n\"b10\": 0.85,\n\"b5\": 0.8,\n\"b1\": 0.75,\n\"b9\": 0.7,\n\"b6\": 0.65,\n\"b15\": 0.6\n}", "Based on the text and titles of the references, the most pertinent source papers are:\n\n1. \"BERT pre-training of deep bidirectional transformers for language understanding\" [b6]: The paper mentions the usage of BERT models multiple times and they also mention a BERT-based baseline, which indicates a heavy reliance on BERT models. Therefore, this paper should be considered a significant source paper.\n\n2. \"Pre-training with whole word masking for Chinese BERT\" [b5]: The authors mention the usage of a Chinese pretrained BERT model in their experiments.\n\n3. \"Neural graph matching networks for Chinese short text matching\" [b3]: The authors mention the \"interactive-based\" framework, which is a key part of the paper, and refer to a paper that seems to discuss this topic.\n\n4. \"Let linguistic knowledge enhanced graph transformer for Chinese short text matching\" [b10]: The paper discusses the usage of word sense information and refers to this paper.\n\n5. \"ERNIE: Large-scale knowledge-enhanced pre-training for language understanding and generation\" [b15]: The authors mention ERNIE 2.0, a pre-training framework, in their experiments.\n\n6. \"The BQ corpus: A large-scale domain-specific Chinese corpus for sentence semantic equivalence identification\" [b1]: The BQ dataset is used in the experiments, indicating this paper's relevance.\n\n7. \"LCQMC: A large-scale Chinese question matching corpus\" [b9]: The LCQMC dataset is used in the experiments, indicating this paper's relevance.\n\nHere is the JSON output:\n{\n\"b6\": 0.9,\n\"b5\": 0.8,\n\"b3\": 0.9,\n\"b10\": 0.8,\n\"b15\": 0.8,\n\"b1\": 0.8,\n\"b9\": 0.8\n}\n\nThe confidence score of 0.9 is assigned to papers [b6] and [b3] as they seem to heavily influence the methodologies used in the paper. The rest of the papers receive a score of 0.8 as they also contribute to the foundation of the study, but their influence seems to be slightly less central compared to [b6] and [b3]. The confidence scores are subjective and can be adjusted based on additional context or expert judgement.", "{\n\"b1\": 0.9,\n\"b10\": 0.7,\n\"b9\": 0.7,\n\"b5\": 0.6\n}"], "62451c2b5aee126c0f47ac08": ["Based on the given text, the main contributions and methods of the paper are the development of a new pretraining strategy for Graph Neural Networks (GNNs) that makes use of supervised contrastive learning to tackle the few-shot node classification problem. The paper appears to be primarily based on, or inspired by, the following references:\n\n- [b1] (\"A baseline for few-shot image classification\"): This paper offered a perspective on the importance of pretraining and feature reuse in few-shot learning that inspired the authors to explore this idea in the graph domain.\n- [b2] (\"Scaling personalized web search\"): This paper introduced a measure of node importance (Personalized PageRank) that the authors adopted for their subgraph sampling mechanism.\n- [b7] (\"Rethinking few-shot image classification: a good embedding is all you need\"): This paper inspired the authors to investigate the transductive fine-tuning paradigm for graph few-shot learning problems.\n- [b12] (\"On few-shot node classification in graph metalearning\"): This paper outlines the problem of few-shot node classification, which is the primary challenge addressed by the authors.\n\nThe confidence scores for these sources are subjective and could be evaluated in further analysis. However, based on their direct relevance to the paper's novel contributions, a possible confidence score allocation could be as follows:\n\n- [b1]: 0.9\n- [b2]: 0.8\n- [b7]: 0.9\n- [b12]: 0.8\n\nThis is a JSON representation of the result: \n\n{\n    \"b1\": 0.9,\n    \"b2\": 0.8,\n    \"b7\": 0.9,\n    \"b12\": 0.8\n}", "Based on the text and the titles of the references, the following papers have the highest likelihood of being the source papers:\n\n- \"A baseline for few-shot image classification\" [b1] - The paper mentions that the authors were inspired by the feature reuse concept of this paper. It is also referred to multiple times throughout the paper.\n- \"Scaling personalized web search\" [b2] - This paper is referenced multiple times in relation to the algorithm used by the authors. It may have largely influenced the method used by the authors.\n- \"Graph attention networks\" [b4] - The authors appear to be using a Graph Neural Network (GNN), this paper could have been a foundational source for their GNN approach.\n- \"Rethinking few-shot image classification: A good embedding is all you need\" [b7] - This paper is also mentioned multiple times and appears to have motivated the authors' approach to their work.\n- \"How powerful are graph neural networks\" [b11] - It is referenced in relation to the GNN approach and was likely influential in the development of the authors' algorithm\n- \"On few-shot node classification in graph metalearning\" [b12] - This paper is referenced in the context of the problem the authors are addressing and the approach they're using. \n\nIn JSON format:\n{\n\"b1\": 0.9,\n\"b2\": 0.8,\n\"b4\": 0.7,\n\"b7\": 0.8,\n\"b11\": 0.7,\n\"b12\": 0.8\n}", "Based on the text and the titles of the references, the following source papers seem to have influenced the given academic paper the most:\n\n1. \"A baseline for few shot image classification\" [b1]\n2. \"Scaling personalized web search\" [b2]\n3. \"Estimating node importance in knowledge graphs using graph neural networks\" [b4]\n4. \"Prototypical networks for few shot learning\" [b5]\n5. \"Explaining the power law degree distribution in a social commerce network\" [b6]\n6. \"Rethinking few shot image classification: a good embedding is all you need\" [b7]\n7. \"Graph few shot learning with attribute matching\" [b9]\n8. \"How powerful are graph neural networks\" [b11]\n9. \"On few shot node classification in graph metalearning\" [b12]\n\nIn JSON format, assuming equal confidence for each source:\n\n{\n\"b1\": 0.125,\n\"b2\": 0.125,\n\"b4\": 0.125,\n\"b5\": 0.125,\n\"b6\": 0.125,\n\"b7\": 0.125,\n\"b9\": 0.125,\n\"b11\": 0.125,\n\"b12\": 0.125\n} \n\nThese results were obtained by looking for key phrases that indicated inspiration from previous work, and considering the novel methods and approaches introduced in the paper. The titles of the papers were also taken into account to ascertain their relevance to the study.", "Based on the text and references provided, the primary challenges outlined in the text are the accurate prediction of labels when they are scarcely available and the highly skewed distribution of real-world graphs. \n\nThe paper proposes a transductive fine-tuning based framework for few-shot node classification, with a GNN encoder pretrained in a supervised contrastive learning manner. \n\nFrom the text, the following references are likely source papers:\n\n- [b1] \"A baseline for few shot image classification\" is mentioned as a source of inspiration for the proposed method with phrases such as \"Inspired by [b1]\". The paper proposes a transductive fine-tuning based framework for few-shot node classification, which seems to be inspired by the methods in [b1]. Confidence Score: 0.9\n\n- [b2] \"Scaling personalized web search\" is frequently referred to and seems to be a source of the methodological approach used in the paper. It appears that the authors have adopted the episodic learning paradigm from [b2] to emulate the test environment for few-shot learning. Confidence Score: 0.8\n\n- [b7] \"Rethinking few shot image classification: a good embedding is all you need\" is mentioned as a source that questions whether the learned representation from meta-learning is responsible for the fast adaption onto the target few-shot tasks in the test phase. It seems to have influenced the authors' approach. Confidence Score: 0.7\n\n- [b12] \"On few-shot node classification in graph metalearning\" appears to be a key source. The authors compare their methods with this work, and it seems to have influenced their decision to propose a meta-trained GNN model. Confidence Score: 0.8\n\n- [b5] \"Prototypical networks for few shot learning\" is used as a reference for the existing meta-learning algorithms. It seems to be a direct source of the paper's methodology. Confidence Score: 0.6\n\n- [b6] \"Explaining the power law degree distribution in a social commerce network\" is cited as a source of the realization that real-world graphs usually follow a highly skewed distribution. Confidence Score: 0.7\n\n- [b11] \"How powerful are graph neural networks\" is used as a reference for the explanation of the effectiveness of GNNs and appears to have influenced the authors' use of GNNs in their proposed method. Confidence Score: 0.6\n\n- [b4] \"Estimating node importance in knowledge graphs using graph neural networks\" is referred as a source to explain the use of GNNs in various domains. Confidence Score: 0.5\n\nThe json format of the results is:\n\n{\n  \"b1\": 0.9,\n  \"b2\": 0.8,\n  \"b7\": 0.7,\n  \"b12\": 0.8,\n  \"b5\": 0.6,\n  \"b6\": 0.7,\n  \"b11\": 0.6,\n  \"b4\": 0.5\n}", "Based on the provided text and the titles of the references, the source papers that inspired the given paper most directly appear to be the following:\n\n1. \"a simple yet effective pretraining strategy for graph few shot learning\" [Unnumbered]\n2. \"guneet singh dhillon pratik chaudhari avinash ravichandran and stefano soatto a baseline for few shot image classification\" [b1]\n3. \"rethinking few shot image classification a good embedding is all you need\" [b7]\n4. \"how powerful are graph neural net\" [b11]\n5. \"on few shot node classification in graph metalearning\" [b12]\n6. \"graph few shot learning with attribute matching\" [b9]\n7. \"estimating node importance in knowledge graphs using graph neural networks\" [b4]\n8. \"jeh and widom glen jeh and jennifer widom scaling personalized web search\" [b2]\n\nReturning the result in JSON format:\n\n```JSON\n{\n    \"Unnumbered\": 0.9,\n    \"b1\": 0.8,\n    \"b7\": 0.85,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b9\": 0.75,\n    \"b4\": 0.8,\n    \"b2\": 0.65\n}\n```\n\nThe confidence score is subjective and based on the analysis of the text, with 1 being complete certainty of the paper's influence, and 0 being no certainty."], "6226c93d5aee126c0fd57ba8": ["Based on the provided text and given criteria, the following references seem to be the most influential for the paper:\n\n1. \"Translating embeddings for modeling multirelational data\" [b2]: The paper's proposed method is based on the well-known KG embedding method TransE, which is the foundation for many entity alignment methods. This could be seen as a source of inspiration for the authors.\n\n2. \"Semisupervised classification with graph convolutional networks\" [b11]: The authors used Graph Convolutional Networks (GCNs) for graph convolution, which is an integral part of the proposed method.\n\n3. \"Learning and development in neural networks: the importance of starting small\" [b7]: The authors have adopted a curriculum learning strategy, which is a concept introduced in this paper.\n\n4. \"Reinforcement learning-based collective entity alignment with adaptive features\" [b38]: This paper is most relevant to the provided paper as it also leverages RL algorithms for entity alignment. Although the focus and implementation are different, it could have provided a starting point for the method proposed in the provided paper.\n\n5. \"Go for a walk and arrive at the answer: reasoning over paths in knowledge bases using reinforcement learning\" [b6]: The authors have drawn insights from reinforcement learning (RL), which is a concept that this paper heavily focuses on.\n\n6. \"Representation learning with contrastive predictive coding\" [b25]: The authors have used a mutual information estimator to estimate the density ratio, which is a concept introduced in this paper.\n\n7. \"Simple statistical gradient-following algorithms for connectionist reinforcement learning\" [b32]: The authors have used the policy gradient algorithm REINFORCE to find the parameters leading to a larger reward, which is a concept taught in this paper.\n\nHere is the result in the requested JSON format:\n\n{\n\"b2\": 0.8,\n\"b11\": 0.7,\n\"b7\": 0.7,\n\"b38\": 0.9,\n\"b6\": 0.7,\n\"b25\": 0.7,\n\"b32\": 0.7\n}", "Based on the provided text and the titles of the referenced papers, the source papers which are directly related to the paper's contributions are:\n\n1. \"deep reinforcement learning for entity alignment\" - The whole paper is based on the idea of using reinforcement learning for entity alignment, and the author specifically mentions the influence of RL in the text.\n2. \"multilingual knowledge graph embeddings for cross lingual knowledge alignment\" - The author mentions the use of EEA (Embedding-based Entity Alignment) methods, which this paper is about.\n3. \"translating embeddings for modeling multirelational data\" - The author specifically mentions the use of TransE, which is the main topic of this paper.\n4. \"graph attention networks\" - The author mentions the use of GATs (Graph Attention Networks), which this paper is about.\n5. \"learning and development in neural networks the importance of starting small\" - The author mentions the use of curriculum learning, which is the main topic of this paper.\n6. \"mutual information neural estimation\" - The author mentions the use of mutual information as an additional feature, which is the main topic of this paper.\n7. \"simple statistical gradientfollowing algorithms for connectionist reinforcement learning\" - The author mentions the use of the policy gradient algorithm REINFORCE, which is the main topic of this paper.\n\nThe \"confidence score\" of these source papers are hard to determine without the full text of the paper, but based on the provided text, they all seem to be directly related to the paper's main contributions.\n\nHere is the answer in json format:\n\n{\n\"1\": 1,\n\"2\": 1,\n\"3\": 1,\n\"4\": 1,\n\"5\": 1,\n\"6\": 1,\n\"7\": 1\n}", "Based on the text and criteria provided, the source papers that this paper draws heavily from are:\n\n- \"Translating embeddings for modeling multi-relational data\" [b2] - This paper introduces the TransE model which is used as a basis for the entity alignment in the given paper. Confidence Score: 0.9\n- \"Multi-channel graph neural network for entity alignment\" [b3] - It is mentioned in the context of existing EEA methods, which this paper seeks to improve upon. Confidence Score: 0.8\n- \"Co-training embeddings of knowledge graphs and entity descriptions for cross-lingual entity alignment\" [b4] - This paper informs the EEA methods the author uses and seeks to improve upon. Confidence Score: 0.8\n- \"Bootstrapping entity alignment with knowledge graph embedding\" [b21] - This paper's method for labeling likely entity alignments informs the author's approach. Confidence Score: 0.9\n- \"Learning and development in neural networks: the importance of starting small\" [b7] - This paper's concept of curriculum learning is adopted in the given paper. Confidence Score: 0.8\n- \"Human-level control through deep reinforcement learning\" [b13] - The given paper applies the reinforcement learning concept from this paper to improve EEA methods. Confidence Score: 0.7\n\nThe above references have been chosen based on the given paper's primary challenges, proposed algorithm, novel methods and approaches, and direct mentions of influence and adoption in the text. \n\nHere is the JSON format of the results:\n\n{\n \"b2\": 0.9,\n \"b3\": 0.8,\n \"b4\": 0.8,\n \"b21\": 0.9,\n \"b7\": 0.8,\n \"b13\": 0.7\n}", "Based on the text and titles of the references, the source papers that most inspire the given paper are:\n\n{\n\"b21\": 0.9, \n\"b2\": 0.85, \n\"b7\": 0.8, \n\"b32\": 0.8, \n\"b12\": 0.75, \n\"b13\": 0.75, \n\"b38\": 0.75, \n\"b5\": 0.7\n}\n\nThe paper heavily relies on the concepts and methods in these papers for developing its core contributions, especially the ones related to reinforcement learning and entity alignment.", "Based on the provided text and the instructions, the source papers most likely to have influenced this work are:\n\n1. \"Translating embeddings for modeling multi-relational data\" [b2] - The authors of the paper have used the TransE method for their KG embedding model, but the paper seems to extend the ideas in TransE and does not merely use it as a tool. Therefore, this paper likely had a significant influence on the current work.\n\n2. \"Co-training embeddings of knowledge graphs and entity descriptions for cross-lingual entity alignment\" [b4] - The paper mentions this source as a significant extension of the work in [b2] and it seems to be closely related to the current work, especially in the context of EA tasks. \n\n3. \"Cross-lingual entity alignment via joint attribute-preserving embedding\" [b20] - The paper directly discusses the work in [b20] and contrasts it with another method, suggesting that the authors were significantly influenced by it.\n\n4. \"Bootstrapping entity alignment with knowledge graph embedding\" [b21] - The paper discusses this source paper in detail, suggesting that it had a significant influence on the current work.\n\n5. \"Human-level control through deep reinforcement learning\" [b13] - The authors mention that their work is inspired by the insights from reinforcement learning, and this paper is one of the cited sources about RL.\n\n6. \"Graph attention networks\" [b26] - The authors mention using GCN and GAT models for graph convolution and this paper is the cited source for GATs.\n\n7. \"Representation learning with contrastive predictive coding\" [b25] - The authors use a mutual information estimator in their methodology and this paper is the cited source for it.\n\n8. \"A benchmarking study of embedding-based entity alignment for knowledge graphs\" [b23] - This source seems to be a comprehensive review of the field and likely provided the authors with a broad foundation for their work.\n\nThe confidence scores for these sources have not been provided in the text, so they cannot be assessed. \n\nTo summarize the results in JSON format:\n\n```\n{\n\"b2\": \"N/A\",\n\"b4\": \"N/A\",\n\"b20\": \"N/A\",\n\"b21\": \"N/A\",\n\"b13\": \"N/A\",\n\"b26\": \"N/A\",\n\"b25\": \"N/A\",\n\"b23\": \"N/A\"\n}\n```\n\nThe \"N/A\" indicates that a confidence score could not be assigned based on the provided information."], "6243ca915aee126c0fbd0aa0": ["{\n    \"b0\": 0.9,\n    \"b14\": 0.9,\n    \"b22\": 0.9,\n    \"b12\": 0.8,\n    \"b16\": 0.8,\n    \"b9\": 0.8,\n    \"b17\": 0.7\n}", "{\n    \"b0\": 0.9,\n    \"b9\": 0.9,\n    \"b12\": 0.9,\n    \"b14\": 0.9,\n    \"b16\": 0.9,\n    \"b22\": 0.9\n}", "{\n  \"b0\": 0.9,\n  \"b12\": 0.9,\n  \"b14\": 0.9,\n  \"b16\": 0.9,\n  \"b22\": 0.9\n}", "{\n  \"b9\": 0.9,\n  \"b12\": 0.9,\n  \"b14\": 0.9,\n  \"b16\": 0.9,\n  \"b22\": 0.9\n}", "{\n    \"b9\": 0.8,\n    \"b12\": 0.9,\n    \"b14\": 0.95,\n    \"b16\": 0.9,\n    \"b22\": 0.95\n}"], "62296c7b5aee126c0f57d7aa": ["Based on the text and titles of the referenced papers, the source papers that inspire the author most are:\n\n1. \"Do transformers really perform bad for graph representation\" (b18): The paper is mainly based on the Graphormer model which was proposed in this source paper. The author reports several design improvements to this model.\n\n2. \"Attention is all you need\" (b15): The paper mentions this source as the basis for the Transformer model, which the Graphormer model is built upon.\n\n3. \"On layer normalization in the Transformer architecture\" (b16): This paper influences the decision to change the placement of layer normalization leading to better results.\n\n4. \"A large scale challenge for machine learning on graphs\" (b8): The dataset PCQM4M from this paper is used and the author made improvements on the results.\n\n5. \"The Open Catalyst OC dataset and community challenges\" (b3): The dataset OC20 from this paper is used to verify the effectiveness of 3D molecular modeling. \n\n6. \"Simple GNN regularisation for d molecular property prediction and beyond\" (b6): The author is inspired by this paper to adopt an auxiliary node-level objective to predict the displacement of each atom.\n\n7. \"What graph neural networks cannot learn: depth vs width\" (b11): The theoretical understanding of Graphormer based on distributed computing theory in this section is inspired by the study of expressiveness of GNNs in this paper.\n\n8. \"On the power of the congested clique model\" (b5): This paper is used to demonstrate that a global receptive field qualitatively improves the expressiveness of the model.\n\nThe result in json format:\n\n```\n{\n\"b18\": 1,\n\"b15\": 0.95,\n\"b16\": 0.9,\n\"b8\": 0.85,\n\"b3\": 0.8,\n\"b6\": 0.75,\n\"b11\": 0.7,\n\"b5\": 0.65\n}\n```", "Based on the text and title of the papers, the source papers which inspire the given paper are:\n\n1. \"Do transformers really perform bad for graph representation\" (b18) - This is the main paper from which the authors have taken inspiration for their work on Graphormer. They have built their improvements on top of the ideas presented in this paper.\n\n2. \"On layer normalization in the transformer architecture\" (b16) - This paper is referred to when the authors discuss the change of placement of layer normalization, which leads to better results on molecular property prediction tasks.\n\n3. \"A large scale challenge for machine learning on graphs\" (b8) - This paper appears to have inspired the application of Graphormer to large-scale molecular modeling datasets.\n\n4. \"Simple GNN regularisation for 3D molecular property prediction and beyond\" (b6) - This paper is referred to when the authors discuss their new approach to 3D molecular modeling.\n\n5. \"The Open Catalyst OC dataset and community challenges\" (b3) - This paper seems to have inspired the authors' application of Graphormer to the Open Catalyst 2020 dataset.\n\n6. \"What Graph Neural Networks cannot learn: Depth vs Width\" (b11) - This paper is referred to in the theoretical understanding of Graphormer, where it is compared to classic message-passing-based GNNs.\n\nThe returned result in the json format is:\n{\n\"b18\": 1,\n\"b16\": 0.9,\n\"b8\": 0.9,\n\"b6\": 0.8,\n\"b3\": 0.8,\n\"b11\": 0.8\n}\nThis result means that b18 is the most relevant source paper, and the papers b16, b8, b6, b3, and b11 have relevance scores of 0.9, 0.9, 0.8, 0.8, and 0.8 respectively.", "Based on the given text and the provided references, the source papers that inspired most of the presented work can be inferred to be:\n\n- \"Do transformers really perform bad for graph representation\" [b18]: The paper's new design improvements are built in the Graphormer framework, which was proposed in this source paper. \n- \"On layer normalization in the transformer architecture\" [b16]: The authors adopt a modification mentioned in this paper which leads to a better generalization performance.\n- \"A large scale challenge for machine learning on graphs\" [b8]: This paper which introduced the PCQM4M dataset is referred to multiple times.\n- \"Simple GNN regularisation for 3D molecular property prediction and beyond\" [b6]: Inspired by this paper, the authors adopt an auxiliary node-level objective.\n- \"The Open Catalyst OC dataset and community challenges\" [b3]: This paper introduced the OC20 dataset used in the study.\n- \"What Graph Neural Networks Cannot Learn: Depth vs Width\" [b11]: The authors use the theoretical perspective from this paper to analyse Graphormer.\n- \"The surprising power of graph neural networks with random node initialization\" [b0]: The authors refer to this paper when discussing the expressiveness of Graphormer.\n\nThe json format is as follows:\n\n{\"b18\": 1, \"b16\": 0.9, \"b8\": 0.9, \"b6\": 0.8, \"b3\": 0.8, \"b11\": 0.7, \"b0\": 0.6}", "Based on the provided text and the titles of the references, the most relevant source papers that inspired the current paper are:\n\n- b18: \"Do transformers really perform bad for graph representation\"\n- b15: \"Attention is all you need\"\n- b16: \"On layer normalization in the transformer architecture\"\n- b8: \"A large scale challenge for machine learning on graphs\"\n- b3: \"The Open Catalyst OC dataset and community challenges\"\n- b14: \"Rotation invariant graph neural networks using spin convolutions\"\n- b6: \"Simple GNN regularisation for 3D molecular property prediction and beyond\"\n- b11: \"What graph neural networks cannot learn: depth vs width\"\n\nThe JSON format of the source papers with confidence scores is:\n\n{\n  \"b18\": 1,\n  \"b15\": 0.9,\n  \"b16\": 0.9,\n  \"b8\": 0.9,\n  \"b3\": 0.8,\n  \"b14\": 0.8,\n  \"b6\": 0.8,\n  \"b11\": 0.8\n}\n\nThe confidence scores are assigned based on the frequency and context of the references in the provided paper text.", "Based on the text and the titles of the references, the most pertinent source papers that inspired this paper are:\n\n1. \"Do Transformers really perform bad for graph representation\" [b18]: The author directly mentions that this paper inspired the Graphormer model, which is the main focus of the paper. Confidence Score: 1\n2. \"On Layer Normalization in the Transformer Architecture\" [b16]: The author discusses the placement of layer normalization in Graphormer, which was inspired by the findings of this paper. Confidence Score: 0.9\n3. \"A large-scale challenge for machine learning on graphs\" [b8]: The author uses the dataset from this paper to evaluate the performance of their model. This paper has been mentioned multiple times in the text. Confidence Score: 0.85\n4. \"The Open Catalyst OC Dataset and Community Challenges\" [b3]: The author used the OC20 dataset from this paper to verify the effectiveness of 3D molecular modeling. Confidence Score: 0.85\n5. \"Simple GNN Regularisation for 3D Molecular Property Prediction and Beyond\" [b6]: The author introduces an auxiliary node-level objective inspired by this paper. Confidence Score: 0.8\n6. \"What Graph Neural Networks Cannot Learn: Depth vs Width\" [b11]: The author discusses the expressiveness of GNNs with discriminative features, which was studied in this paper. Confidence Score: 0.8\n\nIn JSON format:\n\n{\n\"b18\": 1,\n\"b16\": 0.9,\n\"b8\": 0.85,\n\"b3\": 0.85,\n\"b6\": 0.8,\n\"b11\": 0.8\n}"], "624fa8db5aee126c0f3a5be7": ["Based on the given text, the source papers that seem to have influenced the given academic paper the most are:\n\n1. \"b31: Deep unsupervised learning using nonequilibrium thermodynamics\" - The paper repeatedly refers to the Gaussian diffusion model proposed in this paper, indicating that it's foundational to the work being presented.\n\n2. \"b33: Generative modeling by estimating gradients of the data distribution\" and \"b14: Denoising diffusion probabilistic models\" - These two papers are frequently cited together, suggesting that their ideas on denoising and generative modeling have been key to the presented work.\n\n3. \"b34: Score-based generative modeling through stochastic differential equations\" - The authors seem to have adopted the score-based generative modeling technique for their work.\n\n4. \"b12: Classifier-free diffusion guidance\" - This paper is mentioned while discussing improvements in the current work, suggesting that it has directly inspired some of the novel contributions.\n\n5. \"b23: Improved denoising diffusion probabilistic models\" - This is also frequently cited and appears to be related to the core methodological approach of the paper.\n\n6. \"b10: Diffusion models beat gans on image synthesis\" - This paper is cited in relation to the explicit classifier guidance method, suggesting it had a strong impact on the methodology used.\n\nHere's the result in the requested JSON format:\n\n{\n  \"b31\": 0.9,\n  \"b33\": 0.8,\n  \"b14\": 0.8,\n  \"b34\": 0.7,\n  \"b12\": 0.7,\n  \"b23\": 0.7,\n  \"b10\": 0.6\n} \n\nThe confidence scores have been assigned based on the frequency of citation and the direct relevance of the referenced papers to the main contributions of the given paper.", "Based on the text of the paper and the titles of the references, the following references seem to be the most pertinent source papers:\n\n1. \"deep unsupervised learning using nonequilibrium thermodynamics\" [b31]: The paper used the standard formulation of the Gaussian diffusion model, which is first introduced in this source paper.\n2. \"denoising diffusion probabilistic models\" [b14]: The paper's core contributions involve learning to reverse the forward process for generation, which is a technique introduced in this source paper.\n3. \"score based generative modeling through stochastic differential equations international conference on learning representations\" [b34]: This paper was referred multiple times in the text, indicating it has a significant influence on the paper's methodology.\n4. \"diffusion models beat gans on image synthesis\" [b10]: This paper is referred to in the discussion of classifier guidance, suggesting it played a significant role in developing this aspect of the paper's methodology.\n5. \"classifier free diffusion guidance\" [b12]: This paper was explicitly mentioned as influencing the paper's approach to improving sample quality.\n6. \"vivit a video vision transformer\" [b1]: This paper was mentioned in the discussion of the factorized space-time architecture of the model, suggesting it played a significant role in the development of the model's architecture.\n7. \"a connection between score matching and denoising autoencoders\" [b40]: This paper was referred to in the discussion of denoising, suggesting it played a role in the paper's approach to this aspect of the algorithm.\n\nIn JSON format:\n\n{ \"b31\": 0.9, \"b14\": 0.8, \"b34\": 0.8, \"b10\": 0.7, \"b12\": 0.7, \"b1\": 0.6, \"b40\": 0.6}", "Based on the given text and the titles of the references, the most pertinent source papers seem to be:\n\n1. 'Deep unsupervised learning using nonequilibrium thermodynamics' [b31]: The paper specifically mentions the usage of the Gaussian diffusion model, which is a key contribution of [b31]. The authors also indicate that they used the standard formulation of the Gaussian diffusion model with little modification.\n\n2. 'Denoising diffusion probabilistic models' [b14]: This paper is cited multiple times throughout the text, indicating its importance. It is specifically mentioned in context with the training of the denoising model and the discrete time ancestral sampler.\n\n3. 'Variational diffusion models' [b17]: The text mentions the usage of this paper in the background of the diffusion model and also in practice, indicating its importance.\n\n4. 'Score-based generative modeling through stochastic differential equations' [b34]: This paper is also cited multiple times and appears to be a key methodological reference for the paper.\n\n5. 'Classifier-free diffusion guidance' [b12]: The paper discusses the benefits of classifier-free guidance for video diffusion models on both video and image sample quality metrics, which is a contribution of [b12].\n\n6. 'Improved denoising diffusion probabilistic models' [b23]: The paper mentions the usage of cosine schedule from [b23] which is used in defining the -prediction parameterization.\n\nBased on the above, the JSON format answer would be:\n\n{\n  \"b31\": 1,\n  \"b14\": 0.9,\n  \"b17\": 0.9,\n  \"b34\": 0.9,\n  \"b12\": 0.9,\n  \"b23\": 0.8\n} \n\nEach key represents a reference number and the corresponding value indicates the confidence score, where 1 is the highest confidence.", "Based on the analysis of the paper's text, the most pertinent source papers that inspire the author are:\n\n1. \"Deep unsupervised learning using nonequilibrium thermodynamics\" [b31]\n2. \"Denoising diffusion probabilistic models\" [b14]\n3. \"Diffusion models beat GANs on image synthesis\" [b10]\n4. \"Score-based generative modeling through stochastic differential equations international conference on learning representations\" [b34]\n5. \"Vivit a video vision transformer\" [b1]\n6. \"Is space time attention all you need for video understanding\" [b4]\n7. \"Diffusion models beat GANs on image synthesis\" [b10]\n8. \"Classifier-free diffusion guidance\" [b12]\n\nThese papers are indicated by key phrases in the text and are directly linked to the main challenges and methods introduced in the paper. They are directly relevant to the paper's novel contributions.\n\nHere is the result in json format:\n\n{\n  \"b31\": 0.9,\n  \"b14\": 0.85,\n  \"b10\": 0.85,\n  \"b34\": 0.8,\n  \"b1\": 0.8,\n  \"b4\": 0.8,\n  \"b12\": 0.75\n} \n\nThe confidence score is evaluated based on the frequency and contextual relevance of the references in the paper's text.", "Based on the given text and the instructions, the most pertinent source papers appear to be as follows:\n\n- [b31]: 'deep unsupervised learning using nonequilibrium thermodynamics' - The paper frequently cites this source, and it is the foundation of the Gaussian diffusion model that they use. \n- [b14]: 'denoising diffusion probabilistic models' - The paper cites this source multiple times, particularly in relation to the denoising model used.\n- [b34]: 'score-based generative modeling through stochastic differential equations international conference on learning representations' - The paper uses this source to define the forward process and to detail the replacement method for conditional sampling.\n- [b23]: 'improved denoising diffusion probabilistic models' - The paper cites this source in relation to the cosine schedule used in the -prediction parameterization.\n- [b10]: 'diffusion models beat gans on image synthesis' - This source is cited as the original proposal of the explicit classifier guidance method, which the paper adapts.\n- [b12]: 'classifier-free diffusion guidance' - The paper uses this method to improve sample quality in a text-to-video generation setting.\n- [b24]: 'u-net convolutional networks for biomedical image segmentation' - The paper uses the U-net architecture as a basis for their model.\n\nIn the JSON format:\n\n{\n\"b31\": 0.9,\n\"b14\": 0.8,\n\"b34\": 0.8,\n\"b23\": 0.7,\n\"b10\": 0.7,\n\"b12\": 0.7,\n\"b24\": 0.7\n}"], "6243ca9b5aee126c0fbd1cfd": ["Based on the text, the academic paper seems to be heavily influenced by the following source papers:\n\n1. \"graph neural networks are dynamic programmers\" [b27]: The paper frequently refers to the concept of \"algorithmic alignment\" introduced by this source. It also uses the theory of [b27] to predict that graph neural networks (GNNs) algorithmically align with dynamic programming.\n2. \"on a routing problem\" [b0]: The author uses the Bellman-Ford algorithm [b0] as an example to illustrate the connection between GNNs and dynamic programming, indicating a strong reliance on this source.\n3. \"geometric deep learning grids groups graphs geodesics and gauges\" [b3]: The paper uses the definition of GNNs based on [b3] and hopes to achieve a unification between algorithmic reasoning and the geometric deep learning blueprint [b3].\n4. \"how powerful are graph neural networks arxiv preprint\" [b26]: This source is cited as one of the geometric views into how better generalization can be achieved in GNNs.\n5. \"an invitation to applied category theory seven sketches in compositionality\" [b9]: The paper refers to category theory as an attractive tool for making the obvious even more obvious, indicating a methodological inspiration from this source.\n\nJSON format:\n{\n\"b27\": 1,\n\"b0\": 0.9,\n\"b3\": 0.9,\n\"b26\": 0.8,\n\"b9\": 0.7\n}", "Based on the provided text and paper titles, the source papers that seem to have most influenced the paper are:\n\n1. \"graph neural networks are dynamic programmers\" [b0]\n2. \"on a routing problem\" [b27]\n3. \"neural algorithmic reasoners are implicit planners\" [b7]\n4. \"how does a neural network s architecture impact its robustness to noisy labels\" [b12]\n5. \"neural execution of graph algorithms\" [b20]\n6. \"how to transfer algorithmic reasoning knowledge to learn new algorithms\" [b25]\n7. \"what can neural networks reason about arxiv preprint\" [b27]\n\nPlease note that this analysis is based on the paper text, and the confidence scores would require a more in-depth analysis of the paper.\n\nIn JSON format:\n\n{\n    \"b0\": 0.9,\n    \"b27\": 0.85,\n    \"b7\": 0.8,\n    \"b12\": 0.75,\n    \"b20\": 0.7,\n    \"b25\": 0.65\n}", "Based on the text, the source papers that most inspired the author can be identified as follows:\n\n- 'b27': \"What can neural networks reason about?\" is frequently referenced, and it seems to be the primary source that introduced the concept of algorithmic alignment, which is a key theory discussed in the given paper.\n- 'b20': \"Neural execution of graph algorithms\", appears to be a source of inspiration as it demonstrated the use of max aggregation in GNNs for executing pathfinding algorithms, which the author seems to adopt or find influential.\n- 'b12': \"How does a neural network's architecture impact its robustness to noisy labels?\", might be another source paper due to its theoretical contributions to understanding how the choice of aggregator affects extrapolation and noisy-label training.\n- 'b26': \"How powerful are graph neural networks?\", is likely a source paper, as its work on multisets is used in the author's own definition of the pushforward.\n- 'b3': \"Geometric deep learning grids groups graphs geodesics and gauges\", is referenced as the basis for the definition of GNNs used in the paper, which indicates that it could have heavily influenced the author's approach.\n- 'b9': \"An invitation to applied category theory seven sketches in compositionality\", might be another source paper, as it seems to have inspired the usage of concepts from category theory in the given paper.\n\nThe JSON format result would be: {\"b27\": 1, \"b20\": 0.9, \"b12\": 0.8, \"b26\": 0.8, \"b3\": 0.7, \"b9\": 0.7}.", "Based on the text, the most pertinent source papers seem to be:\n\n1. \"What can neural networks reason about\" [b27]: The paper explicitly mentions this work multiple times, stating that it has made key theoretical advances towards the goals of the paper, formalizing the concept of algorithmic alignment and predicting that graph neural networks (GNNs) algorithmically align with dynamic programming.\n\n2. \"Graph neural networks are dynamic programmers\" [None]: Though the reference number is not mentioned, the paper seems to heavily rely on the concepts from this paper, particularly in terms of using graph neural networks for algorithmic reasoning and dynamic programming.\n\n3. \"Geometric deep learning grids groups graphs geodesics and gauges\" [b3]: This paper is referenced when discussing the definition of GNNs used in the paper and also in the context of hoping to open up broader unification between algorithmic reasoning and the geometric deep learning blueprint. \n\n4. \"Neural execution of graph algorithms\" [b20]: This paper is cited as an example of early research in algorithmic reasoning deploying GNNs with aggregators that align to the problem.\n\n5. \"How powerful are graph neural networks\" [b26]: This paper is mentioned in the context of discussing the expressive power of GNNs using multisets and also discussing the aggregator function in the integral transform view.\n\n6. \"An invitation to applied category theory seven sketches in compositionality\" [b9]: This paper is referenced as the source of the applied category theory that is used to draw connections between GNNs and DP.\n\n7. \"On a routing problem\" [b0]: This paper is referenced in the context of the Bellman-Ford algorithm, which is used to demonstrate the connection between GNNs and DP.\n\nBased on these observations, the confidence scores for each paper might be:\n\n{\"b27\": 1, \"None\": 0.95, \"b3\": 0.9, \"b20\": 0.85, \"b26\": 0.8, \"b9\": 0.75, \"b0\": 0.7}\n", "Based on the text and title of references, the source papers that inspired this paper most appear to be:\n\n1. \"graph neural networks are dynamic programmers\" [b27]\n2. \"on a routing problem\" [b0]\n3. \"geometric deep learning grids groups graphs geodesics and gauges\" [b3]\n4. \"neural execution of graph algorithms\" [b20]\n5. \"how does a neural network s architecture impact its robustness to noisy labels\" [b12]\n6. \"combinatorial optimization and reasoning with graph neural networks\" [b4]\n7. \"neural algorithmic reasoners are implicit planners\" [b7]\n\nThis conclusion is drawn from the repeated mentioning and discussion of these papers and their conceptual contributions in the provided text. They are all strongly tied to the core discussions and proposed methodologies of the paper, thus considered as the source papers.\n\nAs a result, the source papers in the json format with confidence scores would be:\n\n{\n\"b27\": 0.93,\n\"b0\": 0.90,\n\"b3\": 0.87,\n\"b20\": 0.84,\n\"b12\": 0.82,\n\"b4\": 0.80,\n\"b7\": 0.78\n}"], "623d33155aee126c0f100f50": ["Based on the given text and the titles of the references, the following references are identified as source papers:\n\n1. \"b2\": \"intriguing properties of contrastive losses\" - Mentioned multiple times in the text and it appears to be a primary source for the paper's contrastive learning methods.\n2. \"b19\": \"deep graph infomax\" - Mentioned directly after a key phrase \"Inspired by the above CL models\" indicating that this paper significantly influenced the current paper's approach.\n3. \"b22\": \"graph contrastive learning with augmentations\" - Mentioned in the context of the current paper's methodology, showing that it was a significant influence.\n4. \"b14\": \"gcc graph contrastive coding for graph neural network pre training\" - Mentioned as a method that the current paper follows, indicating it as a source of inspiration.\n5. \"b7\": \"momentum contrast for unsupervised visual representation learning\" - This paper is referred to in the context of contrasting graph-level embedding, indicating its influence on the current paper.\n6. \"b9\": \"semisupervised classification with graph convolutional networks\" - Mentioned in the context of GNNs used as backbones, indicating its influence on the paper's understanding and application of GNNs.\n\nIn JSON format:\n\n{\n    \"b2\": 0.9,\n    \"b19\": 0.95,\n    \"b22\": 0.8,\n    \"b14\": 0.7,\n    \"b7\": 0.75,\n    \"b9\": 0.6\n}", "Based on the text and the titles of the references, the source papers that the given paper is most inspired by are:\n\n1. \"Intriguing properties of contrastive losses\" [b2]: The paper employs contrastive learning and the InfoNCE loss function which are central concepts in [b2]. \n\n2. \"Deep Graph InfoMax\" [b19]: The paper applies the InfoMax principle to graph representation learning which is a concept that is directly borrowed from [b19].\n\n3. \"Graph contrastive learning with augmentations\" [b22]: The method of enforcing the embedding of positive pair and negative pair to be close and distant in Euclidean space, respectively, is inspired by [b22].\n\n4. \"Momentum contrast for unsupervised visual representation learning\" [b7]: The paper references [b7] while discussing graph-level embedding with momentum encoder.\n\n5. \"Spectral networks and locally connected networks on graphs\" [b1]: The paper uses GNNs as the encoder, which are discussed in [b1].\n\n6. \"Graph contrastive learning automated\" [b23]: This paper describes how to automatically and adaptively select the augmentation for a specific dataset, a concept that appears to be used in the paper.\n\n7. \"Adversarial complementary learning for weakly supervised object localization\" [b25]: The paper proposes a complementary contrastive learning approach inspired by non-maximum suppression which is used in [b25].\n\nThe confidence scores are given as:\n\n{\n    \"b2\": 0.95,\n    \"b19\": 0.90,\n    \"b22\": 0.88,\n    \"b7\": 0.85,\n    \"b1\": 0.80,\n    \"b23\": 0.75,\n    \"b25\": 0.70\n}", "Based on the text of the academic paper and the list of references, the source papers that most inspired the author can be inferred as follows:\n1. \"b2: Intriguing properties of contrastive losses\" - The paper appears to draw heavily on the concepts of contrastive learning and InfoNCE loss, which are central to the method proposed in this reference.\n2. \"b19: Deep graph infomax\" - This paper introduces the application of the InfoMax principle to graph representation learning, a key concept that is built upon in the given paper.\n3. \"b22: Graph contrastive learning with augmentations\" - The given paper seems to follow the method described in this reference to enforce the embedding of positive and negative pairs in Euclidean space.\n4. \"b7: Momentum contrast for unsupervised visual representation learning\" - The concept of using a momentum encoder, mentioned in the given paper, seems to be inspired by this reference.\n5. \"b14: GCC graph contrastive coding for graph neural network pre-training\" - The author appears to use this source as a reference point for contrasting graph-level embedding with a momentum encoder and maintaining the queue of data samples.\n6. \"b25: Adversarial complementary learning for weakly supervised object localization\" - The novel GraphCoCo approach proposed by the author appears to be inspired by the adversarial complementary learning method stated in this reference.\n\nReturning the results in the requested JSON format:\n{\n    \"b2\": 0.90,\n    \"b19\": 0.85,\n    \"b22\": 0.80,\n    \"b7\": 0.75,\n    \"b14\": 0.75,\n    \"b25\": 0.70\n}", "Based on the text provided, the following references appear to have been influential in the development of the paper's contributions:\n\n1. \"GraphCoCo: Graph Complementary Contrastive Learning\" - This is the paper being analyzed. No associated reference number.\n\n2. \"Intriguing properties of contrastive losses\" - This paper inspired the use of contrastive losses in the study. Reference number: b2\n\n3. \"Deep Graph InfoMax\" - This paper provided the principle of maximizing mutual information in graph representation learning. Reference number: b19\n\n4. \"SimCLR: A Simple Framework for Contrastive Learning of Visual Representations\" - This work inspired the graph contrastive learning methods adopted in the paper. Reference number: b2\n\n5. \"GCC: Graph Contrastive Coding for Graph Neural Network Pre-training\" - This paper proposed a method of contrasting graph-level embedding with a momentum encoder. Reference number: b14\n\n6. \"Non-Maximum Suppression in Object Detection\" - This paper's concept of non-maximum suppression inspired the erasing operation in this paper. Reference number: b25\n\n7. \"Graph Contrastive Learning with Augmentations\" - This paper proposed the graph augmentation methods used in the study. Reference number: b22\n\nTherefore, the JSON format is:\n{\"b2\": 0.9, \"b19\": 0.8, \"b14\": 0.7, \"b25\": 0.85, \"b22\": 0.75} \n\nThe confidence score is set between 0 and 1, with 1 being the most confident. Note that the paper \"Intriguing properties of contrastive losses\" and \"SimCLR: A Simple Framework for Contrastive Learning of Visual Representations\" have the same reference number (b2), and were both found to be influential, hence the high confidence score.", "Based on the text and title of the references, the source papers for this academic paper could be:\n\n1. \"graphcoco graph complementary contrastive learning\" - This is the title of the paper itself, therefore it cannot be a source paper.\n2. \"intriguing properties of contrastive losses\" [b2] - Mentioned multiple times in the paper and the method proposed by the authors seems to be directly based on or inspired by it. Confidence Score: 0.9\n3. \"momentum contrast for unsupervised visual representation learning\" [b7] - The authors of the paper referred to this work when talking about contrastive learning and its application to their own research. Confidence Score: 0.8\n4. \"deep graph infomax\" [b19] - Clearly mentioned as inspiration for applying the InfoMax principle to graph representation learning. Confidence Score: 0.95\n5. \"graph contrastive learning with augmentations\" [b22] - The authors followed the method proposed in this paper to enforce the embedding of positive and negative pairs. Confidence Score: 0.85\n6. \"contrastive multi view representation learning on graphs\" [b6] - The authors seem to have used this paper as a basis for their contrastive learning methods. Confidence Score: 0.8\n7. \"on mutual information maximization for representation learning\" [b17] - The authors refer to this work when discussing the gaps between the InfoMax principle and the performance of embeddings in downstream tasks. Confidence Score: 0.7\n\nThe JSON format is:\n{\"b2\": 0.9, \"b7\": 0.8, \"b19\": 0.95, \"b22\": 0.85, \"b6\": 0.8, \"b17\": 0.7}"], "625e1a335aee126c0feca4ba": ["Based on the provided text and titles of the references, the source papers that could have potentially inspired the author of the given paper can be identified as follows:\n\n1. \"Neural message passing for quantum chemistry\" [b42]\n   - The paper frequently refers to message passing neural networks (MPNNs) and this source paper could have provided the foundational knowledge on this topic.\n2. \"A new model for learning in graph domains\" [b43]\n   - The paper mentions the need for a rich embedding of the graph or each node that captures structural properties as well as attribute information, which this source paper likely discusses.\n3. \"Spectral networks and deep locally connected networks on graphs\" [b18]\n   - The paper explicitly mentions spectral graph neural networks and this source paper likely provided the basis for understanding this concept.\n4. \"On the power of color refinement\" [b5]\n   - The paper discusses the Weisfeiler-Leman (WL) algorithm for graph isomorphism testing. This source paper could have provided insights into this algorithm.\n5. \"How powerful are graph neural networks\" [b109]\n   - The paper discusses the representational power of GNNs and this source paper likely provides an in-depth analysis of the capabilities and limitations of GNNs.\n6. \"Approximation ratios of graph neural networks for combinatorial problems\" [b93]\n   - The paper discusses the application of GNNs for combinatorial problems and this source paper likely provides insights into this specific application.\n7. \"The graph neural network model\" [b95]\n   - The paper frequently discusses the workings of graph neural networks and this source paper likely provides a comprehensive understanding of the model.\n\nThe JSON format of the result is:\n\n{\n  \"b42\": 0.9,\n  \"b43\": 0.9,\n  \"b18\": 0.8,\n  \"b5\": 0.8,\n  \"b109\": 0.9,\n  \"b93\": 0.8,\n  \"b95\": 0.9\n}\n\nThe confidence scores are assumed based on the relevance of the source paper's topic to the content of the given paper.", "Based on the given text and reference titles, the following source papers seem to have inspired the given paper:\n\n1. 'theory of graph neural networks representation and learning': The paper discusses topics related to graph neural networks, graph isomorphism testing, and approximation algorithms, which are also covered in the given paper.\n\n2. 'how powerful are graph neural networks': The paper discusses limitations of graph neural networks and the need to improve their representational power, which is also a key theme in the given paper.\n\n3. 'neural message passing for quantum chemistry': The given paper discusses message passing graph neural networks (MPNNs), which is a concept introduced in this source paper.\n\n4. 'a new model for learning in graph domains': The given paper discusses the need for a rich embedding of the graph to capture structural properties, a topic also covered in this source paper.\n\n5. 'spectral networks and deep locally connected networks on graphs': The given paper discusses spectral graph neural networks, which are introduced in this source paper.\n\n6. 'generalization and representational limits of graph neural networks': The paper discusses the representational power of GNNs, a key theme in the given paper.\n\n7. 'approximation by superpositions of a sigmoidal function': The paper discusses the use of multi-layer perceptrons (MLPs) in the most general form, which is also discussed in the given paper.\n\n8. 'neural execution of graph algorithms': The given paper discusses the analogy between the 1-WL algorithm and MPNNs, a concept also covered in this source paper.\n\nHere is the result in JSON format:\n\n{\n    \"b0\": 0.9,\n    \"b109\": 0.95,\n    \"b42\": 0.85,\n    \"b43\": 0.85,\n    \"b18\": 0.8,\n    \"b39\": 0.9,\n    \"b26\": 0.8,\n    \"b103\": 0.8\n}", "Based on the given text and the titles of the references, the most pertinent source papers could be:\n\n1. 'b42': 'neural message passing for quantum chemistry' - The paper often mentions the concept of 'Message passing neural networks (MPNNs)', which is a title this paper is about. Confidence Score: 0.9\n2. 'b43': 'a new model for learning in graph domains' - The given paper deals with a new model for graph domain learning, which aligns with the title of this source paper. Confidence Score: 0.8\n3. 'b46': 'inductive representation learning on large graphs' - The given paper discusses inductive learning on large graphs, suggesting that this could be a relevant source paper. Confidence Score: 0.8\n4. 'b58': 'semi supervised classification with graph convolutional networks' - This paper refers to graph convolutional networks and classification, which are topics discussed in the given paper. Confidence Score: 0.7\n5. 'b76': 'automatic generation of complementary descriptors with molecular graph networks' - This paper refers to graph networks, which are a major topic in the given paper. Confidence Score: 0.7\n6. 'b95': 'the graph neural network model' - This paper's title aligns with the topic of the given paper, which is about graph neural networks. Confidence Score: 0.9\n7. 'b109': 'how powerful are graph neural networks' - This paper's title suggests that it discusses the capabilities of graph neural networks, which is a topic the given paper focuses on. Confidence Score: 0.8\n\nIn JSON format:\n\n{\n\"b42\": 0.9,\n\"b43\": 0.8,\n\"b46\": 0.8,\n\"b58\": 0.7,\n\"b76\": 0.7,\n\"b95\": 0.9,\n\"b109\": 0.8\n}", "Based on the provided text and the titles of the referenced papers, the source papers that most likely inspired the given paper are:\n\n{\"b42\": 0.9, \"b43\": 0.9, \"b46\": 0.8, \"b58\": 0.8, \"b76\": 0.8, \"b95\": 0.9, \"b39\": 0.7, \"b109\": 0.8}\n\nThis selection is based on the presence of key phrases like \"inspired by\", \"motivated by\", and \"following.. we adopt ... to solve the challenge/problem\" in the context of the references. The confidence score is an estimation of how directly relevant the source paper is to the paper's novel contributions.", "Based on the given text and the titles of the references, the source papers that most likely inspired this paper are:\n\n1. \"How powerful are graph neural networks\" [b109]\n2. \"On the power of color refinement\" [b5]\n3. \"The graph neural network model\" [b95]\n4. \"Convolutional neural networks on graphs with fast localized spectral filtering\" [b30]\n5. \"Node vec scalable feature learning for networks\" [b45]\n6. \"Inductive representation learning on large graphs\" [b46]\n7. \"Learning combinatorial optimization algorithms over graphs\" [b28]\n8. \"The expressive power of kth order invariant graph networks\" [b64]\n\nThese papers are all directly related to the core concepts of Graph Neural Networks (GNNs) and their potential limitations. The paper frequently refers to concepts and ideas outlined in these source papers, making them the most likely sources of inspiration.\n\nReturned in JSON format:\n{\n\"b109\": 0.9,\n\"b5\": 0.8,\n\"b95\": 0.8,\n\"b30\": 0.7,\n\"b45\": 0.7,\n\"b46\": 0.7,\n\"b28\": 0.7,\n\"b64\": 0.6\n}"], "62393e7f5aee126c0f126162": ["Based on the provided text, the paper seems to be primarily influenced by the following papers:\n\n1. \"Semi-supervised classification with graph convolutional networks\" [b17]\n2. \"Graphsaint graph sampling based inductive learning method\" [b37]\n3. \"Fastgcn fast learning with graph convolutional networks via importance sampling\" [b1]\n4. \"Stochastic training of graph convolutional networks with variance reduction\" [b2]\n5. \"Cluster gcn an efficient algorithm for training deep and large graph convolutional networks\" [b5]\n6. \"Inductive representation learning on large graphs\" [b11]\n7. \"Improving the accuracy scalability and performance of graph neural networks with roc\" [b15]\n8. \"Neugraph parallel deep neural network computation on large graphs\" [b24]\n\n\nThe returned json format:\n\n```\n{\n\"b17\": 0.9,\n\"b37\": 0.85,\n\"b1\": 0.8,\n\"b2\": 0.8,\n\"b5\": 0.75,\n\"b11\": 0.75,\n\"b15\": 0.7,\n\"b24\": 0.7\n}\n```\n\nIn this json, the key is the reference number and the value is the confidence score, indicating how much the paper appears to be influenced by each source.\n", "Based on the text, the papers that most influenced the authors are:\n\n1. \"Semi-Supervised Classification with Graph Convolutional Networks\" [b17]: This paper is mentioned multiple times as the foundational work on graph convolutional networks (GCNs). However, it is referenced at the beginning of the paper, which may indicate that it provides historical context rather than direct inspiration for the paper's main contributions.\n\n2. \"GraphSAGE: Inductive Representation Learning on Large Graphs\" [b11]: This paper is directly cited as inspiration for the methods used in the paper, specifically the use of neighbor sampling to reduce a full graph into a mini-batch.\n\n3. \"VR-GCN: Stochastic Training of Graph Convolutional Networks with Variance Reduction\" [b2]: This paper is also cited as inspiration for the methods used in the paper, specifically the use of neighbor sampling.\n\n4. \"NeuGraph: Parallel Deep Neural Network Computation on Large Graphs\" [b24], \"ROC: Improving the Accuracy, Scalability, and Performance of Graph Neural Networks with ROC\" [b15], \"CAGNET\" [b31], \"Dorylus: Affordable, Scalable, and Accurate GNN Training with Distributed CPU Servers and Serverless Threads\" [b30], and \"PipeGCN: Efficient Full Graph Training of Graph Convolutional Networks with Pipelined Feature Communication\" [b33]: These papers are all cited as pioneering efforts in the field of distributed GCN training, which is one of the main focuses of the paper.\n\nTherefore, the source papers are:\n- b11: 0.9\n- b2: 0.9\n- b24: 0.8\n- b15: 0.8\n- b31: 0.8\n- b30: 0.8\n- b33: 0.8 \n\nThe confidence scores are assigned based on the degree of direct inspiration and methodological similarity they share with the paper being analyzed.", "Based on the given academic paper, the most pertinent source papers that inspired the authors are:\n\n- \"b17\": \"Semi-supervised classification with graph convolutional networks\" - This paper introduced the concept of Graph Convolutional Networks (GCNs), which is the foundation of this paper. Confidence Score: 0.8\n- \"b11\": \"Inductive representation learning on large graphs\" - This paper proposed GraphSAGE which is mentioned as a key method this paper builds upon. Confidence Score: 0.9\n- \"b2\": \"Stochastic training of graph convolutional networks with variance reduction\" - This paper is also directly referenced as a method that this paper builds upon. Confidence Score: 0.9\n- \"b5\": \"Cluster-GCN: An efficient algorithm for training deep and large graph convolutional networks\" - The paper refers to this work as an alternate method and compares it to their proposed method. Confidence Score: 0.7\n- \"b31\": \"Reducing communication in Graph Neural Network Training\" - This paper is mentioned as one of the works that have demonstrated promising training performance. Confidence Score: 0.7\n- \"b33\": \"PipeGCN: Efficient full graph training of Graph Convolutional Networks with pipelined feature communication\" - This paper is mentioned as one of the works that have demonstrated promising training performance. Confidence Score: 0.7\n- \"b24\": \"NeuGraph: Parallel deep neural network computation on large graphs\" - This paper is mentioned as one of the works that have demonstrated promising training performance. Confidence Score: 0.7\n- \"b15\": \"Improving the accuracy, scalability, and performance of Graph Neural Networks with ROC\" - This paper is mentioned as one of the works that have demonstrated promising training performance. Confidence Score: 0.7\n\nThe returned JSON format would be as follow:\n{\n    \"b17\": 0.8,\n    \"b11\": 0.9,\n    \"b2\": 0.9,\n    \"b5\": 0.7,\n    \"b31\": 0.7,\n    \"b33\": 0.7,\n    \"b24\": 0.7,\n    \"b15\": 0.7\n}", "Based on the given information, the following references are considered as source papers:\n\n1. \"b0: Recent Advances in Graph Partitioning\" - This paper is not directly related to the main topic but provides an essential approach used in BNS-GCN.\n2. \"b1: FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling\" - BNS-GCN extends upon the idea of sampling from this paper.\n3. \"b2: Stochastic Training of Graph Convolutional Networks with Variance Reduction\" - BNS-GCN also utilizes the concept of variance reduction.\n4. \"b5: Cluster GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks\" - BNS-GCN takes inspiration from the concept of subgraph sampling.\n6. \"b11: Inductive Representation Learning on Large Graphs\" - GraphSAGE, a method used in BNS-GCN, is proposed in this paper.\n7. \"b15: Improving the Accuracy Scalability and Performance of Graph Neural Networks with ROC\" - BNS-GCN can also be easily plugged into this partition-parallel training method.\n8. \"b31: Reducing Communication in Graph Neural Network Training\" - BNS-GCN adopts the goal of this paper to reduce communication in GNN training.\n\nSo, the result is:\n{\"b0\": 0.3, \"b1\": 0.9, \"b2\": 0.9, \"b5\": 0.9, \"b11\": 1.0, \"b15\": 0.8, \"b31\": 0.8}", "Based on the given text of the academic paper, the source papers which have inspired the author most are:\n\n- \"Semi-supervised classification with graph convolutional networks\" [b17]: The paper directly mentions GCNs as its foundation and primary method.\n- \"Inductive representation learning on large graphs\" [b11]: The paper mentions GraphSAGE and its adopted approach of reducing a full graph into a mini-batch via neighbor sampling.\n- \"Improving the accuracy scalability and performance of graph neural networks with ROC\" [b15]:  This paper is highlighted as a pioneering effort in \"partition-parallelism\" paradigm, which the current paper also adopts.\n- \"NeuGraph: parallel deep neural network computation on large graphs\" [b24]: Another pioneering work in the \"partition-parallelism\" paradigm.\n- \"Reducing communication in graph neural network training\" [b31]: This paper is another key reference in the paper's discussion of distributed GCN training.\n- \"Dorylus: affordable, scalable and accurate GNN training with distributed CPU servers and serverless threads\" [b30]: This paper is also cited in the context of \"partition-parallelism\" paradigm and has influenced the current paper's approach to distributed GCN training.\n- \"FastGCN: Fast learning with graph convolutional networks via importance sampling\" [b1]: This method is compared to in the analysis of BNS-GCN, indicating its influence on the development of BNS-GCN.\n- \"A fast and high quality multilevel scheme for partitioning irregular graphs\" [b16]: This work is mentioned as the default graph partition algorithm adopted in the proposed BNS-GCN.\n\nThe result in JSON format would be:\n\n{\"b17\": 1, \"b11\": 0.9, \"b15\": 0.9, \"b24\": 0.9, \"b31\": 0.9, \"b30\": 0.9, \"b1\": 0.8, \"b16\": 0.8}"], "6241273e5aee126c0f292ae3": ["Based on the text of the paper, the key references that most likely inspired the work are as follows:\n\n- [b1]: Ext: Towards Extreme Multi-Task Scaling for Transfer Learning - This paper is directly mentioned as an inspiration for the proposed work on multitask prompt-based training. Confidence score: 0.9\n- [b2]: Language Models Are Few-Shot Learners - The success of GPT series especially GPT-3 marked the beginning of prompt's popularization on NLP tasks. Confidence score: 0.95\n- [b48]: Multitask Prompted Training Enables Zero-Shot Task Generalization - The 'Pretrain, Personalized Prompt & Predict Paradigm' (denoted as P5) proposed in this paper is directly inspired by the recent progress in multitask prompt-based training. Confidence score: 0.95\n- [b43]: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - This paper is mentioned as early pioneers on unifying NLP downstream tasks through text-to-text encoder-decoder framework. Confidence score: 0.85\n- [b63]: Finetuned Language Models Are Zero-Shot Learners - This paper has been cited as a source of inspiration for the use of personalized prompts and the training of a sequence-to-sequence model on a variety of recommendation related tasks. Confidence score: 0.9\n- [b68]: One Person, One Model, One World: Learning Continual User Representation without Forgetting - This paper is cited in the context of learning universal user representations which are easily transferrable to downstream tasks. Confidence Score: 0.8\n- [b52]: One-All User Representation for Recommender Systems in E-Commerce - Same as [b68], this paper is cited in the context of learning universal user representations which are easily transferrable to downstream tasks. Confidence Score: 0.8\n\nThe JSON format of the result is:\n{\"b1\": 0.9, \"b2\": 0.95, \"b48\": 0.95, \"b43\": 0.85, \"b63\": 0.9, \"b68\": 0.8, \"b52\": 0.8}", "Based on the provided paper and reference titles, the most relevant source papers are:\n\n{ \"b1\": 0.9, \"b2\": 0.7, \"b43\": 0.8, \"b48\": 0.9, \"b63\": 0.7, \"b29\": 0.7, \"b18\": 0.7, \"b3\": 0.6, \"b30\": 0.6, \"b56\": 0.6}\n\nThis selection prioritizes papers that appear to have influenced the development of the \"Pretrain, Personalized Prompt & Predict Paradigm\" (P5) and the application of multitask prompt-based training in recommendation systems. The confidence score indicates the estimated level of direct relevance to the paper's main contributions, with a higher score suggesting a stronger influence.", "Based on the provided paper and reference titles, the following references appear to be the most relevant:\n\n1. [b1] \"Ext: Towards extreme multi-task scaling for transfer learning\"\n2. [b2] \"Language models are few-shot learners\"\n3. [b43] \"Exploring the limits of transfer learning with a unified text-to-text transformer\"\n4. [b48] \"Multitask prompted training enables zero-shot task generalization\"\n5. [b63] \"Finetuned language models are zero-shot learners\"\n\nThe confidence scores for each of these references might be:\n\n- \"b1\": 0.9\n- \"b2\": 0.95\n- \"b43\": 0.85\n- \"b48\": 0.9\n- \"b63\": 0.95\n\nThis paper seems to be heavily influenced by the work on transfer learning, multi-task learning, and zero-shot learning. The concept of \"prompt-based\" training also seems to be a significant influence, as mentioned in the references [b1], [b48], and [b63].\n", "Based on the text of the paper and the titles of the references, the most pertinent source papers appear to be:\n\n- \"b1: ext towards extreme multi task scaling for transfer learning\"\n- \"b2: language models are few shot learners\"\n- \"b43: exploring the limits of transfer learning with a unified text to text transformer\"\n- \"b48: multitask prompted training enables zero shot task generalization\"\n- \"b63: finetuned language models are zero shot learners\"\n- \"b68: one person one model one world learning continual user representation without forgetting\"\n\nThese papers appear to be most directly related to the novel contributions of the paper, specifically the use of a multi-task learning approach for recommender systems, the use of pretraining, personalized prompts, leveraging language models and zero-shot learning.\n\nIn JSON format with confidence scores (please note that these are subjective and may vary):\n\n{\n  \"b1\": 0.9,\n  \"b2\": 0.85,\n  \"b43\": 0.9,\n  \"b48\": 0.95,\n  \"b63\": 0.8,\n  \"b68\": 0.75\n}", "Based on the provided text of the paper and the titles of the references, the source papers that most inspired this work seem to be:\n\n{ \n\"b1\": 0.9, \n\"b2\": 0.85, \n\"b3\": 0.7, \n\"b29\": 0.7, \n\"b43\": 0.8, \n\"b48\": 0.85, \n\"b52\": 0.7, \n\"b63\": 0.9 \n}\n\nHere's a brief explanation for each:\n\n- \"b1\" (0.9 confidence): The paper mentions \"Inspired by the recent progress in multitask prompt-based training [b1]\". The title of b1 is \"Ext Towards Extreme Multi-task Scaling for Transfer Learning\", which implies a strong connection with the novel contributions of the paper.\n\n- \"b2\" (0.85 confidence): The paper mentions \"The success of GPT series especially GPT-3 [b2] marked the beginning of prompt's popularization on NLP tasks\". The title of b2 is \"Language Models are Few-Shot Learners\", which indicates that this paper had a significant influence on the development of the paper's method.\n\n- \"b3\" (0.7 confidence): The paper mentions \"Recently, deep neural network models [b3] facilitate crossing and combination among even more diverse and sophisticated features\". The title of b3 is \"Neural Collaborative Reasoning\", which suggests it has a methodological similarity with the paper's algorithm.\n\n- \"b29\" (0.7 confidence): The paper mentions \"Several works [b29] attempt to learn universal user representations which are easily transferrable to downstream tasks\". The title of b29 is \"Personalized Transformer for Explainable Recommendation\", which suggests it has a methodological similarity with the paper's algorithm.\n\n- \"b43\" (0.8 confidence): The paper mentions \"Our P5 model utilizes the pretrained T5 checkpoints [b43] as backbone\". The title of b43 is \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\", which implies a strong connection with the novel contributions of the paper.\n\n- \"b48\" (0.85 confidence): The paper mentions \"In contrast, our P5 first takes personalization into an encoder-decoder Transformer model following [b48]\". The title of b48 is \"Multitask Prompted Training Enables Zero-Shot Task Generalization\", which indicates that this paper had a significant influence on the development of the paper's method.\n\n- \"b52\" (0.7 confidence): The paper mentions \"Recently, a line of work [b52] attempt to learn universal user representations which are easily transferrable to downstream tasks\". The title of b52 is \"One All User Representation for Recommender Systems in E-commerce\", which suggests it has a methodological similarity with the paper's algorithm.\n\n- \"b63\" (0.9 confidence): The paper mentions \"Inspired by the recent progress in multitask prompt-based training [b63]\". The title of b63 is \"Fine-Tuned Language Models Are Zero-Shot Learners\", which implies a strong connection with the novel contributions of the paper.\n"], "6271e0e75aee126c0f574720": ["Based on the text and the titles of the references, the source papers that appear to have most inspired this work are:\n\n- \"Problems of the th annual international symposium on computer architecture isca\" [b26]\n- \"Intrinsics for Intel R advanced matrix extensions Intel R AMX instructions\" [b22]\n- \"Volta: Performance and programmability\" [b4]\n- \"NVIDIA A Tensor Core GPU Architecture\" [b51]\n- \"NVIDIA T Tensor Core GPU\" [b52]\n- \"Warp-level matrix multiply-accumulate instructions\" [b53]\n\nThe confidence scores for these papers could be as follows:\n\n{\n\"b26\": 0.9, \n\"b22\": 0.8, \n\"b4\": 0.85, \n\"b51\": 0.9, \n\"b52\": 0.9, \n\"b53\": 0.85\n}\n\nThe confidence scores are based on the level of direct inspiration or motivation indicated by the text, with a higher score indicating a stronger reliance on the source paper.\n\nPlease note that this is a hypothetical analysis and the actual confidence scores may vary based on further analysis of the text.", "Based on the given text and the information inferred from the titles of the references, the source papers that most inspired the paper are:\n\n1. \"NVIDIA: A Tensor Core GPU Architecture\" [b51], Confidence Score: 0.9\n2. \"NVIDIA T Tensor Core GPU\" [b52], Confidence Score: 0.9\n3. \"Google's Tensor Processing Units (TPUs)\" [b26], Confidence Score: 0.8\n4. \"IBM Power 10 MMA unit\" [b66], Confidence Score: 0.8\n5. \"Volta: Performance and Programmability\" [b4], Confidence Score: 0.7\n6. \"Simd: A generalized matrix instruction set for accelerating tensor computation beyond gemm\" [None], Confidence Score: 0.6\n7. \"Intrinsics for Intel\u00ae Advanced Matrix Extensions (Intel\u00ae AMX) Instructions\" [b22], Confidence Score: 0.6\n8. \"Accelerating applications using Edge Tensor Processing Units\" [b20], Confidence Score: 0.5\n\nThis is represented in JSON Format as:\n\n{\n\"b51\": 0.9,\n\"b52\": 0.9,\n\"b26\": 0.8,\n\"b66\": 0.8,\n\"b4\": 0.7,\n\"None\": 0.6,\n\"b22\": 0.6,\n\"b20\": 0.5\n}", "Based on the content of the paper and the titles of the references, the source papers that most likely inspired the given paper are:\n\n1. \"NVIDIA: A Tensor Core GPU architecture\" [b51]\n2. \"NVIDIA T Tensor Core GPU\" [b52]\n3. \"Google's Tensor Processing Units (TPUs)\" [b26]\n4. \"IBM Power 10 MMA unit\" [b66]\n5. \"Introducing the scalable matrix extension for the ARMv8-A architecture\" [b2]\n6. \"Intrinsics for Intel Advanced Matrix Extensions (Intel AMX) instructions\" [b22]\n7. \"SIMD: A Generalized Matrix Instruction Set for Accelerating Tensor Computation Beyond GEMM.\" [None]\n8. \"cuASR: CUDA Algebra for Semirings\" [b23].\n\nThe confidence scores for each source paper, assuming a maximum score of 1, are as follows:\n{ \"b51\": 0.9, \"b52\": 0.8, \"b26\": 0.9, \"b66\": 0.8, \"b2\": 0.7, \"b22\": 0.7, \"None\": 1, \"b23\": 0.8 }", "Based on the text and the list of references, the most likely sources of inspiration for the paper are:\n\n1. \"Procedings of the th annual international symposium on computer architecture isca\" ['b26'] - It is mentioned when discussing the hardware accelerators like MXUs and TPUs.\n\n2. \"Semiring frameworks and algorithms for shortest distance problems\" ['b46'] - It is directly referenced when discussing the semiring-like structure in matrix problems.\n\n3. \"A hybrid systolic dataflow architecture for inductive matrix algorithms\" ['b70'] - It seems to be related to the proposed SIMD 2 architecture, though not directly referenced in the text.\n\n4. \"Hitting the memory wall implications of the obvious\" ['b71'] - Mentioned when discussing the memory-wall problem and how it influenced the research.\n\n5. \"Volta performance and programmability\" ['b4'] - Referenced when discussing GPU architectures and Tensor Cores, which appear to be relevant to the SIMD 2 architecture.\n\n6. \"Intrinsics for Intel r advanced matrix extensions Intel r amx instructions\" ['b22'] - It is mentioned when discussing the upcoming generations of CPU processors.\n\n7. \"Nvidia a tensor core gpu architecture\" ['b51'] - It is referenced when discussing examples of hardware accelerators like Tensor Cores.\n\nThe confidence score for each paper is hard to determine without a more detailed analysis of the text, but a preliminary estimation could be:\n\n{\"b26\": 0.9, \"b46\": 0.9, \"b70\": 0.8, \"b71\": 0.9, \"b4\": 0.8, \"b22\": 0.8, \"b51\": 0.9}\n\nThis is a preliminary analysis and a more detailed reading of the paper might reveal more insights.", "{\"b2\": 0.8, \"b22\": 0.8, \"b26\": 0.9, \"b51\": 0.9, \"b52\": 0.9, \"b66\": 0.8, \"b4\": 0.7, \"b61\": 0.7}"], "6274c91a5aee126c0f71246a": ["Based on the text and the title of the papers, the source papers are likely:\n\n- \"A new Uncertainty-aware Pseudo Label Refinery (UPLR) framework\" is proposed: This refers to the paper itself, so it's not a source paper.\n- \"Bootstrapping entity alignment with knowledge graph embedding\" [b30]: Mentioned in the context of translational methods and iterative optimization.\n- \"Graph attention networks\" [b35]: The paper's Gate Graph Attention Networks is based on the Graph Attention Networks.\n- \"Cross lingual knowledge graph alignment via graph convolutional networks\" [b36]: Mentioned while talking about entity alignment models and their requirements for high-quality entity alignment seed.\n- \"Pseudo label the simple and efficient semisupervised learning method for deep neural networks\" [b13]: The paper's algorithm is built on pseudo-labelling, and this source paper discusses pseudo-labelling.\n- \"Translating embeddings for modeling multi relational data\" [b1], \"Deep graph matching consensus\" [b7], \"A unified model for cross domain and semisupervised named entity recognition in chinese social media\" [b10]: Mentioned in the context of entity alignment, graph interaction divergence, and the alignment model respectively.\n\nThe JSON format representation of the source papers is:\n\n{\n  \"b30\": 0.9,\n  \"b35\": 0.8,\n  \"b36\": 0.8,\n  \"b13\": 0.8,\n  \"b1\": 0.7,\n  \"b7\": 0.7,\n  \"b10\": 0.7\n}", "Based on the text of the academic paper, the source papers that inspire this work the most are:\n\n1. 'Uncertainty aware pseudo label refinery for entity alignment' [None]\n2. 'Translating embeddings for modeling multi-relational data' [b1]\n3. 'Graph attention networks' [b35]\n4. 'Pseudo label the simple and efficient semi-supervised learning method for deep neural networks' [b13]\n5. 'Non-local neural networks' [b40]\n6. 'Deep graph matching consensus' [b7]\n7. 'Bootstrapping entity alignment with knowledge graph embedding' [b30]\n\nHere is the JSON format:\n\n    {\n    \"None\": 0.9,\n    \"b1\": 0.8,\n    \"b35\": 0.8,\n    \"b13\": 0.7,\n    \"b40\": 0.7,\n    \"b7\": 0.7,\n    \"b30\": 0.7\n    } \n\nHere, the confidence score is a measure of how directly each paper contributes to the novel methods and approaches outlined in the paper. A confidence score of 0.9 indicates a very direct contribution, while a score of 0.7 indicates a somewhat less direct, but still significant, contribution.", "Based on the text of the paper and the titles of the references, the source papers that inspired this work the most are likely:\n\n1. \"Uncertainty aware pseudo label refinery for entity alignment\"\n2. \"Translating embeddings for modeling multi-relational data\"\n3. \"Deep graph matching consensus\"\n4. \"Pseudo label the simple and efficient semisupervised learning method for deep neural networks\"\n5. \"Graph attention networks\"\n6. \"Bootstrapping entity alignment with knowledge graph embedding\"\n7. \"Cross lingual entity alignment via graph convolutional networks\"\n8. \"Non local neural networks\"\n\nThese papers are suggested because they are directly linked to the novel methods and approaches the authors introduced to tackle the challenges of entity alignment task. In addition, phrases indicating direct inspiration or motivation such as \"inspired by\", \"motivated by\", \"we use... based on to achieve...\" are associated with these papers. \n\nThe confidence scores of these papers being the source of inspiration for this work can be represented in JSON format as follows:\n\n{\n \"Uncertainty aware pseudo label refinery for entity alignment\": 0.9,\n \"Translating embeddings for modeling multi-relational data\": 0.8,\n \"Deep graph matching consensus\": 0.8,\n \"Pseudo label the simple and efficient semisupervised learning method for deep neural networks\": 0.8,\n \"Graph attention networks\": 0.7,\n \"Bootstrapping entity alignment with knowledge graph embedding\": 0.7,\n \"Cross lingual entity alignment via graph convolutional networks\": 0.7,\n \"Non local neural networks\": 0.7\n} \n\nThe confidence score is high for the first paper \"Uncertainty aware pseudo label refinery for entity alignment\" as it is the title of this paper and it appears to introduce the main method used in this work. The other papers also have high scores as they are referenced multiple times throughout the text and seem to be associated with key methodologies employed in this work.", "Based on the text and titles of the references, the source papers that most likely inspired the given paper are:\n\n{ \"b1\": 0.6,\n  \"b7\": 0.9,\n  \"b13\": 0.8,\n  \"b30\": 0.7,\n  \"b35\": 0.9,\n  \"b36\": 0.7}\n\nExplanation:\n- \"b1\" is mentioned in the Gate Graph Attention Networks section in relation to the relational translation matrix. \n- \"b7\" is referenced in the context of the Gate Graph Attention Networks section and it seems to have influenced how the new vector of the node is obtained.\n- \"b13\" is referenced in the Model Learning from Pseudo-labelling and Building Entity Alignment Seeds sections, indicating that it has majorly influenced the development of the pseudo-labeling approach.\n- \"b30\" is mentioned in the related work section, specifically in relation to the iterative strategy which is used in this paper as well.\n- \"b35\" is mentioned in the Gate Graph Attention Networks section and it seems to be the basis for the development of GateGAT.\n- \"b36\" is mentioned in the related work section, specifically in EA based on graph structures which is a crucial part of this paper.", "Based on the content of the paper and the titles of the references, the most pertinent source papers that inspired this work could be:\n\n{\"b1\": 0.9, \"b7\": 0.8, \"b13\": 0.7, \"b35\": 0.9, \"b30\": 0.7, \"b36\": 0.8, \"b41\": 0.6, \"b10\": 0.6}\n\nExplanation:\n1. \"b1\" (Translating embeddings for modeling multi-relational data): The paper mentions the use of \"TransE\" for modeling triples involved, which is a method proposed in this reference.\n2. \"b7\" (Deep graph matching consensus): It is mentioned that the paper uses a similar approach of using graph matching for entity alignment. \n3. \"b13\" (Pseudo label the simple and efficient semi-supervised learning method for deep neural networks): The paper uses a pseudo-labeling method similar to the one proposed in this reference.\n4. \"b35\" (Graph attention networks): The paper uses a variation of graph attention networks (GAT), which is proposed in this reference.\n5. \"b30\" (Bootstrapping entity alignment with knowledge graph embedding): The concept of using bootstrapping is mentioned as a key technology to improve performance.\n6. \"b36\" (Cross-lingual knowledge graph alignment via graph convolutional networks): The paper uses a similar approach for entity alignment.\n7. \"b41\" (Self-training with the noisy student improves ImageNet classification): The paper mentioned the use of pseudo-labels and the noisy student model for enhancing the performance, which is a concept proposed in this reference.\n8. \"b10\" (A unified model for cross-domain and semi-supervised named entity recognition in Chinese social media): The paper mentions the use of a batch of samples with the highest prediction probability of the model to help enhance its performance, which is similar to the method proposed in this reference."], "626b868a6750f822a0a5e16d": ["Based on the given text and the criteria provided, the following papers seem to have heavily influenced the main contributions of the paper in question:\n\n1. \"Language models are few-shot learners\" [b11]\n2. \"Training compute optimal large language models\" [b47]\n3. \"Unifying vision and language tasks via text generation\" [b19]\n4. \"Perceiver: general perception with iterative attention\" [b53]\n5. \"Scaling laws for neural language models\" [b58]\n\nThe confidence scores are based on the relevance of the paper to the novel contributions of the paper in question. The confidence score ranges from 0 (low relevance) to 1 (high relevance).\n\n{\n    \"b11\": 0.9,\n    \"b47\": 0.8,\n    \"b19\": 0.7,\n    \"b53\": 0.7,\n    \"b58\": 0.7\n}", "After analyzing the text and the context of the provided paper, the following references stood out as the primary source papers:\n\n1. \"Language Models are Few-Shot Learners\" (b11)\n2. \"Unifying Vision and Language Tasks via Text Generation\" (b19)\n3. \"Training Compute Optimal Large Language Models\" (b47)\n4. \"Perceiver: General Perception with Iterative Attention\" (b53)\n5. \"Scaling Up Visual and Vision-Language Representation Learning with Noisy Text Supervision\" (b55)\n6. \"VisualGPT: Data-Efficient Adaptation of Pretrained Language Models for Image Captioning\" (b15)\n7. \"MAGMA: Multimodal Augmentation of Generative Models Through Adapter-Based Fine-Tuning\" (b28)\n\nThe confidence score for each of the papers is as follows:\n\n```json\n{\n    \"b11\": 0.9,\n    \"b19\": 0.7,\n    \"b47\": 0.8,\n    \"b53\": 0.6,\n    \"b55\": 0.7,\n    \"b15\": 0.7,\n    \"b28\": 0.6\n}\n```\n\nThese references were identified based on their direct relevance to the main contributions and methods outlined in the paper, as well as the explicit mention or implication of their inspiration or influence on the work.", "Based on the text and the titles of the references, the most relevant source papers to the given paper seem to be:\n\n1. \"b11: Language models are few-shot learners\" - The paper takes inspiration from large-scale generative language models which are good few-shot learners. Confidence Score: 0.9\n2. \"b47: Oriol Vinyals and Laurent Sifre Training compute optimal large language models\" - The paper uses a 70B state-of-the-art LM as the base for the Flamingo model. Confidence Score: 0.8\n3. \"b53: Perceiver general perception with iterative attention\" - The paper uses a Perceiver-based architecture for processing visual input. Confidence Score: 0.8\n4. \"b19: Unifying vision and language tasks via text generation\" - The paper explores visually conditioned language generation. Confidence Score: 0.7\n5. \"b55: Scaling up visual and vision language representation learning with noisy text supervision\" - The paper mentions contrastive learning which is also used in the given paper. Confidence Score: 0.7\n6. \"b73: VC-GPT: Visual Conditioned GPT for end-to-end generative vision and language pre-training\" - The paper shares numerous ideas with the given paper and uses a similar architecture. Confidence Score: 0.7\n7. \"b75: Categorization and naming in children problems of induction\" - The paper discusses the ability to quickly learn how to perform a new task which is a key aspect of intelligence considered in the given paper. Confidence Score: 0.7\n\nThe JSON format will be:\n```json\n{\n  \"b11\": 0.9,\n  \"b47\": 0.8,\n  \"b53\": 0.8,\n  \"b19\": 0.7,\n  \"b55\": 0.7,\n  \"b73\": 0.7,\n  \"b75\": 0.7\n}\n```", "Based on the text and the titles of the references, the source papers that most likely inspired the given paper are:\n\n{ \"b11\": 0.9, \"b47\": 0.75, \"b53\": 0.7, \"b94\": 0.65, \"b15\": 0.6, \"b73\": 0.55, \"b93\": 0.5, \"b55\": 0.45 }\n\nExplanation:\n- Reference \"b11\" is mentioned multiple times in the text with phrases that indicate its importance, such as \"takes inspiration from recent work in large-scale generative language models (LMs) which are good few-shot learners [b11]\".\n- Reference \"b47\" is also mentioned multiple times in the text and is indicated as the base LM for the largest Flamingo model, making it a central influence.\n- Reference \"b53\" is used to tackle the challenge of supporting both images and videos, indicating it inspired a key part of the paper's methodology.\n- Reference \"b94\" is also indicated as a source of inspiration for large-scale generative language models.\n- Reference \"b15\" is mentioned as having shown the benefit of initializing the weights of a VLM with a pretrained language-only model for data-efficient training.\n- Reference \"b73\" is mentioned in the related work and seems to share some methodological similarities with the paper.\n- Reference \"b93\" is mentioned multiple times and is used as a comparison point, indicating that it likely influenced the paper.\n- Reference \"b55\" is used to tackle the challenge of obtaining heterogeneous training data to induce good generalist capabilities, indicating it inspired a key part of the paper's methodology.", "Based on the given text and titles of the references, the most pertinent source papers are:\n\n1. \"b11: Language models are few shot learners\" - The paper mentions that Flamingo takes inspiration from recent work in large-scale generative language models, which are good few-shot learners, and directly cites this paper. Confidence score: 1.0\n\n2. \"b53: Perceiver: General perception with iterative attention\" - The paper states that to address challenges related to the unifying of single-modal models and supporting images and videos, they use a Perceiver-based architecture. Confidence score: 0.9\n\n3. \"b47: Oriol Vinyals and Laurent Sifre: Training compute optimal large language models\" - The paper mentions that they build upon the Chinchilla family of models, using the 70B parameter Chinchilla model as the base LM for the largest Flamingo model. Confidence score: 0.8\n\n4. \"b15: VisualGPT: Data-efficient adaptation of pretrained language models for image captioning\" - While discussing related work, the paper mentions that VisualGPT showed the benefit of initializing the weights of a VLM with a pretrained language-only model for data-efficient training. Confidence score: 0.7\n\n5. \"b28: Magma: Multimodal augmentation of generative models through adapter-based fine-tuning\" - The paper discusses in the related work that Magma adds bottleneck adapters within the frozen language model, similar to how Flamingo modifies the computational graph of the original language model. Confidence score: 0.7\n\n6. \"b124: Attention is all you need\" - While discussing the history of language modelling, the paper mentions the introduction of Transformers, which the latest LMs build on. Confidence score: 0.6\n\n7. \"b55: Scaling up visual and vision language representation learning with noisy text supervision\" - The paper refers to the contribution of this work while discussing the challenges of obtaining heterogeneous training data. Confidence score: 0.6\n\n8. \"b93: Learning transferable visual models from natural language supervision\" - The paper discusses this work while reviewing relevant related work in the field of joint vision and language modelling. Confidence score: 0.6\n\nSo, the final result is:\n\n{\n\"b11\": 1.0,\n\"b53\": 0.9,\n\"b47\": 0.8,\n\"b15\": 0.7,\n\"b28\": 0.7,\n\"b124\": 0.6,\n\"b55\": 0.6,\n\"b93\": 0.6\n}"], "626754bb5aee126c0fbccbaa": ["{\n    \"b28\": 0.9,\n    \"b6\": 0.85,\n    \"b53\": 0.75,\n    \"b56\": 0.65,\n    \"b3\": 0.6,\n    \"b18\": 0.55,\n    \"b41\": 0.55,\n    \"b51\": 0.5\n}", "{\n    \"b28\": 0.95,\n    \"b6\": 0.9,\n    \"b53\": 0.85,\n    \"b3\": 0.8,\n    \"b18\": 0.75,\n    \"b41\": 0.75,\n    \"b51\": 0.75\n}", "{\n  \"b28\": 0.95,\n  \"b3\": 0.90,\n  \"b6\": 0.85,\n  \"b53\": 0.80,\n  \"b56\": 0.75\n}", "{\n  \"b28\": 0.95,\n  \"b6\": 0.9,\n  \"b53\": 0.85,\n  \"b3\": 0.8,\n  \"b56\": 0.75,\n  \"b41\": 0.7,\n  \"b18\": 0.65\n}", "{\n    \"b28\": 0.9,\n    \"b6\": 0.8,\n    \"b53\": 0.8,\n    \"b3\": 0.7,\n    \"b18\": 0.6,\n    \"b41\": 0.6,\n    \"b51\": 0.6\n}"], "627332775aee126c0f18d585": ["{\n  \"b7\": 0.95,\n  \"b22\": 0.9,\n  \"b24\": 0.85,\n  \"b26\": 0.8,\n  \"b30\": 0.75,\n  \"b37\": 0.75\n}", "{\n    \"b7\": 0.9,\n    \"b22\": 0.8,\n    \"b24\": 0.8,\n    \"b26\": 0.8,\n    \"b30\": 0.9,\n    \"b37\": 0.9\n}", "{\n  \"b7\": 0.9,\n  \"b22\": 0.8,\n  \"b24\": 0.8,\n  \"b26\": 0.8\n}", "{\n    \"b7\": 0.9,\n    \"b22\": 0.85,\n    \"b24\": 0.8,\n    \"b13\": 0.75,\n    \"b26\": 0.7,\n    \"b32\": 0.65,\n    \"b34\": 0.6,\n    \"b17\": 0.55\n}", "{\n  \"b7\": 0.95,\n  \"b10\": 0.9,\n  \"b22\": 0.9,\n  \"b13\": 0.85,\n  \"b24\": 0.8,\n  \"b26\": 0.75\n}"], "621ee1895aee126c0f26af67": ["Based on the text and the titles of the references, the source papers that most inspire this work could be:\n\n1. \"pasca a graph neural architecture search system under the scalable paradigm\" [b0]\n2. \"rethinking graph neural architecture search from message passing\" [b3]\n3. \"scalable graph neural networks via bidirectional propagation\" [b5]\n4. \"sign scalable inception graph neural networks\" [b11]\n5. \"graph neural architecture search\" [b12]\n6. \"predict then propagate graph neural networks meet personalized pagerank\" [b20]\n7. \"one shot graph neural architecture search with dynamic search space\" [b26]\n\nIn more detail:\n\n- [b0]: The paper's title indicates it's the most relevant source paper because it outlines the system \"PaSca\" which is mentioned frequently in the given paper.\n- [b3, b11, b12, b26]: These papers are related to graph neural architecture search, which is a major focus of the given paper.\n- [b5]: The paper discusses scalability in graph neural networks, which is a key challenge outlined in the given paper.\n- [b20]: This paper is mentioned in the context of the proposed new scalable model PaSca-APPNP.\n\nThe confidence score for each source paper is as follows:\n\n{\n\"b0\": 1,\n\"b3\": 0.8,\n\"b5\": 0.8,\n\"b11\": 0.8,\n\"b12\": 0.8,\n\"b20\": 0.7,\n\"b26\": 0.7\n}", "After examining the text and reference titles of the academic paper, the most pertinent source papers that inspired the given work are:\n\n1. b14 - \"inductive representation learning on large graphs advances in neural information processing systems\"\n2. b51 - \"simplifying graph convolutional networks\"\n3. b11 - \"sign scalable inception graph neural networks\"\n4. b67 - \"simple spectral graph convolution\"\n5. b19 - \"semi supervised classification with graph convolutional networks\"\n6. b5 - \"scalable graph neural networks via bidirectional propagation\"\n7. b42 - \"dorylus affordable scalable and accurate gnn training with distributed cpu servers and serverless threads\"\n\nThe selected papers either introduce methods for graph neural networks (GNNs), discuss scalable methods for GNNs, or propose new frameworks for GNNs, all of which are central themes in the provided paper.\n\nThe JSON format of the result is:\n\n{\n  \"b14\": 0.85,\n  \"b51\": 0.8,\n  \"b11\": 0.75,\n  \"b67\": 0.7,\n  \"b19\": 0.65,\n  \"b5\": 0.6,\n  \"b42\": 0.55\n}\n\n(The confidence score is a subjective measure based on the relevance of the source papers to the main contributions of the given paper. The values are hypothetical and for illustrative purposes.)", "Based on the text of the paper, the source papers that inspired the authors the most are as follows:\n\n{ \"b11\": 0.9, \"b51\": 0.9, \"b67\": 0.8, \"b19\": 0.7, \"b14\": 0.7, \"b5\": 0.7, \"b20\": 0.6, \"b42\": 0.6 } \n\nReasoning:\n\n- \"b11\" and \"b51\" are referred to as inspirations for the author's approach to scalable GNNs, indicated by phrases like \"following SGC [b51]\", \"SIGN [b11] proposes to concatenate different iterations of aggregated feature messages\", and \"we find that recently emerging scalable GNN models, such as SGC [b51], SIGN [b11], S 2 GC [b67] and GBP [b5] are special instances in our design space\".\n\n- \"b67\" and \"b5\" are also mentioned in the same context, indicating that they provided significant inspiration.\n\n- \"b19\" is mentioned as an example of the traditional approach that the authors are building upon.\n\n- \"b14\" is referred to multiple times throughout the text, indicating its importance to the author's research.\n\n- \"b20\" is mentioned as the motivation for the post-processing stage of the authors' approach.\n\n- \"b42\" is referred to as a recent work that the authors compare their approach against, indicating that it was a significant point of reference.", "Based on the text and titles of the references, the most pertinent source papers appear to be:\n\n{\"b14\": 1.0,\n\"b19\": 0.9,\n\"b11\": 0.9,\n\"b51\": 0.9,\n\"b67\": 0.8,\n\"b5\": 0.8,\n\"b20\": 0.7,\n\"b8\": 0.6}\n\nb14 - \"inductive representation learning on large graphs advances in neural information processing systems\"\nb19 - \"semi supervised classification with graph convolutional networks\"\nb11 - \"sign scalable inception graph neural networks\"\nb51 - \"simplifying graph convolutional networks\"\nb67 - \"simple spectral graph convolution\"\nb5 - \"scalable graph neural networks via bidirectional propagation\"\nb20 - \"predict then propagate graph neural networks meet personalized pagerank\"\nb8 - \"single and multi objective evolutionary design optimization assisted by gaussian random field metamodels\" \n\nThese papers are most likely to be the sources that inspired the author due to their relevance and the frequency of their citation in the text. The confidence score is estimated based on the context in which these papers are mentioned and their relevance to the main topics of the text.", "Based on the text of the paper, the following references seem to have significantly influenced the work:\n\n1. \"inductive representation learning on large graphs advances in neural information processing systems\" [b14]: This paper is mentioned multiple times, and the authors demonstrate the issues of this method, which motivates them to propose a novel approach. \n\n2. \"scalable graph neural networks via bidirectional propagation\" [b5]: Mentioned as a recent work that the paper's authors are building upon. \n\n3. \"sign scalable inception graph neural networks\" [b11]: Mentioned as a specific architectural design that the authors are trying to improve.\n\n4. \"simplifying graph convolutional networks\" [b51]: This paper is referenced as a direction for scalable GNNs that the authors are following.\n\n5. \"single and multi objective evolutionary design optimization assisted by gaussian random field metamodels\" [b8]: This paper is mentioned as the basis for the multi-objective search algorithm used in the search engine of the proposed system.\n\n6. \"predict then propagate graph neural networks meet personalized pagerank\" [b20]: This paper is mentioned as a method that the authors decouple and get a new scalable model.\n\nBased on these references, the confidence scores can be assigned as follows:\n{ \"b14\" : 0.9, \"b5\" : 0.8, \"b11\" : 0.8, \"b51\" : 0.8, \"b8\" : 0.7, \"b20\" : 0.7 }"], "628749425aee126c0ffee615": ["Based on the provided information and the criteria given, I can identify the following source papers based on the text:\n\n1. \"Using trusted data to train deep networks on labels corrupted by severe noise\" - [b12]\n2. \"Making deep neural networks robust to label noise: a loss correction approach\" - [b26]\n3. \"Neural fine-grained entity type classification with hierarchy-aware loss\" - [b39]\n4. \"Learning with noise: improving distantly supervised fine-grained entity typing via automatic relabeling\" - [b41]\n5. \"Modeling noisy hierarchical types in fine-grained entity typing: a content-based weighting approach\" - [b37]\n6. \"Composite binary losses\" - [b29]\n\nThese papers are inferred based heavily on the content of the academic text provided, the author notes, and the titles of the papers. The text of the paper frequently references these works, indicating a strong reliance on their findings and methodologies.\n\nHere is the result in the json format:\n\n{\"b12\": 0.9, \"b26\": 0.85, \"b39\": 0.8, \"b41\": 0.8, \"b37\": 0.75, \"b29\": 0.7}", "Based on the text, the paper primarily discusses the challenge of dealing with label noises in fine-grained entity typing (FET) and proposes a novel framework (FCLC) for noisy label learning inspired by weighted training and loss correction. \n\nThe following references seem to be the most influential:\n\n1. [b39] 'Neural fine-grained entity type classification with hierarchy-aware loss' - This paper seems to be the source of the backbone model used in the study.\n2. [b12] 'Using trusted data to train deep networks on labels corrupted by severe noise' - The proposed framework in the paper is inspired by the loss correction method discussed in this source.\n3. [b26] 'Making deep neural networks robust to label noise: a loss correction approach' - This paper also appears to be a source of inspiration for the loss correction approach used in the study.\n\nThe other references mentioned in the text do not seem to have a direct impact on the paper's novel contributions or do not appear to be explicitly linked to the paper's main challenges and methods.\n\nSo, the result might be:\n\n{\n\"b39\": 0.9,\n\"b12\": 0.8,\n\"b26\": 0.8\n} \n\nThe confidence scores are subjective and could be adjusted based on further analysis of the paper.", "Based on the provided text and the titles of the references, the source papers of the given paper are:\n\n1. b12: 'Using trusted data to train deep networks on labels corrupted by severe noise' - The paper was inspired by the loss correction method proposed in this paper. Confidence score: 0.9\n\n2. b39: 'Neural finegrained entity type classification with hierarchy-aware loss' - The paper uses the same backbone model as this source paper. Confidence score: 0.9\n\n3. b26: 'Making deep neural networks robust to label noise: a loss correction approach' - The paper was also inspired by the loss correction approach proposed in this paper. Confidence score: 0.8\n\n4. b42: 'Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation' - This paper's findings about the prediction distribution being affected by noisy instances and suffering from confirmation bias were also mentioned. Confidence score: 0.7\n\n5. b30: 'Afet: automatic finegrained entity typing by hierarchical partial label embedding' - This paper might be a potential source paper as the given paper also mentions noisy labels and the importance of feature space. Confidence score: 0.65\n\n6. b41: 'Learning with noise: improving distantly supervised fine-grained entity typing via automatic relabeling' - This paper might be a potential source paper as it also attempts to deal with the problem of noisy labels. Confidence score: 0.65\n\n7. b37: 'Modeling noisy hierarchical types in fine grained entity typing: a content-based weighting approach' - This paper might be a potential source paper as it also discusses the problem of noisy labels. Confidence score: 0.65\n\nThese source papers all seem to have a significant influence on the given paper's main contributions. \n\nThe JSON format is as follows:\n\n{ \"b12\": 0.9, \"b39\": 0.9, \"b26\": 0.8, \"b42\": 0.7, \"b30\": 0.65, \"b41\": 0.65, \"b37\": 0.65 }", "Based on the text and the titles of the references, the source papers of the given paper that most inspired the authors could be:\n\n1. \"Using trusted data to train deep networks on labels corrupted by severe noise\" [b12]: This paper is referred to when discussing the method of loss correction and the estimation of the noise transition matrix. The authors of the current paper extend the ideas from this source to a fine-grained entity typing setting.\n\n2. \"Neural finegrained entity type classification with hierarchy-aware loss\" [b39]: This work seems to have provided the backbone model for the current study, indicating it played a significant role in the development of the method.\n\n3. \"Making deep neural networks robust to label noise: a loss correction approach\" [b26]: This paper is also referred to in the context of loss correction and seems to have influenced the approach of dealing with label noise in the current paper.\n\nNote: The titles of the references don't match the numbers in the text, but I have made an educated guess based on the context. \n\nBased on these observations, the JSON output would be:\n\n{\n\"b12\": 0.9,\n\"b39\": 0.8,\n\"b26\": 0.7\n} \n\nThe confidence scores are assigned based on the degree of influence, where a higher score indicates a stronger influence.", "Based on the text and the titles of the references, the source papers that the author indicates that they inspire them most are:\n\n1. \"Using trusted data to train deep networks on labels corrupted by severe noise\" [b12]\n2. \"Neural fine grained entity type classification with hierarchy aware loss\" [b39]\n3. \"Making deep neural networks robust to label noise: a loss correction approach\" [b26]\n4. \"Composite binary losses\" [b29]\n5. \"Learning with noise: improving distantly supervised fine-grained entity typing via automatic relabeling\" [b41]\n\nThe json format of the result is:\n```\n{\n\"b12\": 0.9,\n\"b39\": 0.8,\n\"b26\": 0.7,\n\"b29\": 0.8,\n\"b41\": 0.7\n}\n```"], "62451c2b5aee126c0f47ab40": ["{\n    \"b18\": 1.0,\n    \"b25\": 0.95,\n    \"b32\": 0.9,\n    \"b2\": 0.85,\n    \"b74\": 0.8\n}", "```json\n{\n  \"b18\": 0.95,\n  \"b25\": 0.9,\n  \"b2\": 0.85,\n  \"b74\": 0.8,\n  \"b32\": 0.75,\n  \"b48\": 0.7\n}\n```", "{\n  \"b18\": 0.95,\n  \"b25\": 0.85,\n  \"b2\": 0.80,\n  \"b74\": 0.75,\n  \"b32\": 0.75\n}", "{\n  \"b18\": 0.95,\n  \"b25\": 0.85,\n  \"b32\": 0.80,\n  \"b74\": 0.80,\n  \"b2\": 0.75\n}", "{\n    \"b18\": 0.95,\n    \"b2\": 0.9,\n    \"b25\": 0.85,\n    \"b74\": 0.8,\n    \"b32\": 0.75\n}"], "6233f88d5aee126c0f94b61a": ["Based on the provided information and the text of the paper, the most pertinent source papers are as follows:\n\n1. \"A minimax approach to supervised learning\" [b9]: The paper frequently cites this work, particularly when discussing its new approach to sample selection, which is inspired by the minimax approach. Confidence score: 0.9.\n\n2. \"Unsupervised data augmentation for consistency training\" [b52]: This paper is mentioned multiple times, especially in relation to the issues with existing DA methods and how they've influenced the development of Glitter. Confidence score: 0.85.\n\n3. \"Masked adversarial text a companion to knowledge distillation\" [b38]: This paper is referenced in relation to task-aware DA methods and their limitations, which the paper's authors seek to address with Glitter. Confidence score: 0.8.\n\n4. \"Conditional bert contextual augmentation\" [b51]: This work is cited as an example of task-aware DA, and seems to be one of the methods that the paper's authors are building upon. Confidence score: 0.75.\n\n5. \"A survey of data augmentation approaches for nlp\" [b10]: This paper is likely a key source as it's cited when discussing the challenges of DA in NLP, which is a main focus of the paper. Confidence score: 0.7.\n\n6. \"Distilling the knowledge in a neural network\" [b14]: This paper is mentioned when discussing teacher-student setups and how Glitter can be applied to them. Confidence score: 0.65.\n\n7. \"Easy data augmentation techniques for boosting performance on text classification tasks\" [b48]: This paper is referenced when discussing various DA methods, and the authors seem to take inspiration from it. Confidence score: 0.6.\n\nThe results in json format are:\n\n```json\n{\n  \"b9\": 0.9,\n  \"b52\": 0.85,\n  \"b38\": 0.8,\n  \"b51\": 0.75,\n  \"b10\": 0.7,\n  \"b14\": 0.65,\n  \"b48\": 0.6\n}\n```", "Based on the text and the approach described, the following references seem to be the most influential:\n\n1. \"A minimax approach to supervised learning\" [b9]: The paper's core algorithm, Glitter, is based on a minimax approach for selecting a subset of samples with maximal expected loss. This indicates a significant influence from this reference in formulating the main algorithm.\n\n2. \"Unsupervised data augmentation for consistency training\" [b52]: This reference is mentioned several times in the context of challenges related to data augmentation methods and their sample efficiency. It seems to have motivated the development of Glitter to address these issues.\n\n3. \"SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out of Domain Robustness\" [b33]: Mentioned in the context of generating augmented samples, suggesting an influence on the data augmentation method used in the paper.\n\n4. \"Not Far Away, Not So Close: Sample-Efficient Nearest Neighbour Data Augmentation via Minimax\" [b19]: This paper appears to have influenced the way the authors thought about sample efficiency in data augmentation.\n\n5. \"MATE-KD: Masked Adversarial Text, a Companion to Knowledge Distillation\" [b38]: This reference is cited while discussing teacher-student settings and data augmentation for knowledge distillation, indicating an influence on the methodologies used in the paper.\n\n6. \"CoDA: Contrast-Enhanced and Diversity-Promoting Data Augmentation for Natural Language Understanding\" [b36]: This reference is cited as a baseline comparison and may have influenced the development of Glitter.\n\n7. \"Learning Data Manipulation for Augmentation and Weighting\" [b16]: This paper is mentioned in the context of task-aware DA methods and likely informed the approach taken in the paper.\n\nIn JSON format, the results would look like this:\n\n{ \"b9\": 0.9, \"b52\": 0.8, \"b33\": 0.7, \"b19\": 0.7, \"b38\": 0.7, \"b36\": 0.6, \"b16\": 0.6 }\n\nThese confidence scores are subjective and estimate the level of direct influence each reference has on the main contributions of the paper. A higher score indicates a greater perceived influence.", "Based on the text of the academic paper and the titles of the referenced papers, the source papers that the author indicates inspired them the most are:\n\n- [b9]: 'a minimax approach to supervised learning' which is mentioned in the context of the minimax approach that Glitter uses for sample selection.\n- [b52]: 'unsupervised data augmentation for consistency training' is referred to multiple times, indicating that it has influenced the use of augmented samples in the proposed method.\n- [b38]: 'mate kd masked adversarial text a companion to knowledge distillation' is mentioned when discussing the use of Glitter in teacher-student setups, suggesting its influence on this aspect of the paper.\n- [b42]: 'improving neural machine translation models with monolingual data', although this reference is used in the context of data augmentation in general, it might be a basic concept of this paper.\n- [b36]: 'coda contrast enhanced and diversity promoting data augmentation for natural language understanding', this paper is compared with the proposed method Glitter, indicating it may have served as a motivation for the author.\n- [b33]: 'ssmba self supervised manifold based data augmentation for improving out of domain robustness', it is mentioned in the context of the augmentation methods that the author used.\n- [b18]: 'tinybert distilling bert for natural language understanding', it also appears in the context of the DA methods, which implies the influence on the author's work.\n\nIn JSON format, with a rough estimation of the confidence score:\n\n{ \n \"b9\": 0.9,\n \"b52\": 0.9,\n \"b38\": 0.8,\n \"b42\": 0.7,\n \"b36\": 0.7,\n \"b33\": 0.7,\n \"b18\": 0.7\n}", "Based on the text provided, the source papers that inspire the author the most seem to be:\n\n- \"A minimax approach to supervised learning\" [b9]: The text directly mentions that the proposed method follows a minimax approach to select a subset of worst-case samples with maximal expected loss during training. This indicates that the paper's central algorithm was directly inspired by [b9].\n- \"Unsupervised data augmentation for consistency training\" [b52]: The paper discusses the limitations of this previous work and proposes improvements, suggesting that it was a significant source of inspiration. \n- \"Masked adversarial text: A companion to knowledge distillation\" [b38]: This is another work whose limitations the paper addresses, indicating it was a source of inspiration.\n- \"Easy data augmentation techniques for boosting performance on text classification tasks\" [b48]: The paper makes use of techniques proposed in [b48], suggesting it was another source of inspiration.\n- \"Self-supervised manifold-based data augmentation for improving out-of-domain robustness\" [b33]: As one of the previous work that the new approach is compared to, this paper seems to be a source of inspiration.\n\nThe confidence score for each source paper is hard to determine exactly, but based on the text, a possible distribution could look as follows:\n\n{ \"b9\": 0.9, \"b52\": 0.8, \"b38\": 0.7, \"b48\": 0.7, \"b33\": 0.6 }\n", "Based on the text of the paper, the key phrases indicating inspiration and the titles of the reference papers, the most pertinent source papers are:\n\n1. [b9]: \"A minimax approach to supervised learning\" - This paper is directly mentioned as inspiration for the main method in the subject paper.\n2. [b52]: \"Unsupervised data augmentation for consistency training\" - This paper is referenced multiple times throughout the text, indicating that its methods were a significant influence on the subject paper.\n3. [b38]: \"MATE-KD masked adversarial text a companion to knowledge distillation\" - The subject paper refers to this paper as an example of a method that is not universal and implies that the authors' work improves upon this.\n4. [b36]: \"CoDA contrast enhanced and diversity promoting data augmentation for natural language understanding\" - The subject paper refers to this paper when discussing the limitations of existing DA methods, implying that it was a point of reference during the development of their own method.\n5. [b33]: \"SSMBA self-supervised manifold based data augmentation for improving out of domain robustness\" - This paper is mentioned in the context of task-agnostic DA methods, which the subject paper seeks to improve upon.\n\nThe confidence scores for each reference are estimated as follows:\n\n1. b9: 0.9\n2. b52: 0.85\n3. b38: 0.8\n4. b36: 0.75\n5. b33: 0.7\n\nThese confidence scores are based on how directly and frequently the subject paper refers to each source paper. \n\nThe final result is:\n{\"b9\": 0.9, \"b52\": 0.85, \"b38\": 0.8, \"b36\": 0.75, \"b33\": 0.7}"], "628c4ce15aee126c0ff596d4": ["Based on the text of the paper and the titles of the references, the source papers that inspired the author are:\n\n1. \"E(3) equivariant graph neural networks\" (b11) - The author's novel model, EGR, is built on the previous work of E(3) equivariant graph neural networks.\n2. \"Independent SE(3)-equivariant models for end-to-end rigid protein docking\" (b34) - The paper mentions the adoption of techniques from this paper in designing the EGR model.\n3. \"Protein complex prediction with AlphaFold-Multimer\" (b35) - The paper has used this method to generate decoy structures for the DeepHomo and EVCoupling datasets which are part of their PSR dataset.\n4. \"Atomic protein structure refinement using all-atom graph representations and SE(3)-equivariant graph neural networks\" (b40) - The author cites this paper when discussing the challenges and interests in using learning-based methods for complex structure refinement.\n5. \"Fast and effective protein model refinement using deep graph neural networks\" (b39) - The author cites this paper when discussing existing methods for protein structure refinement.\n6. \"EquiBind geometric deep learning for drug binding structure prediction\" (b18) - The EGR model combines insights from this paper.\n7. \"3D Euclidean transformations that can now directly be found within neural network layers\" (b50) - The author cites this paper, suggesting it influenced the development of the EGR model.\n8. \"Learning from protein structure with geometric vector perceptrons\" (b24) - Influences the author's use of geometric graphs.\n\nI would return the result in json format as follows:\n{\"b11\": 1, \"b34\": 0.9, \"b35\": 0.9, \"b40\": 0.8, \"b39\": 0.8, \"b18\": 0.8, \"b50\": 0.7, \"b24\": 0.7}", "Based on the text and the author's notes, the source papers that likely inspired this paper are:\n\n- [b11]: 'E(n) equivariant graph neural networks'\n- [b34]: 'Independent SE equivariant models for end-to-end rigid protein docking'\n- [b18]: 'EquiBind geometric deep learning for drug binding structure prediction'\n- [b37]: 'Improved protein structure refinement guided by deep learning based accuracy estimation'\n- [b39]: 'Fast and effective protein model refinement using deep graph neural networks'\n- [b40]: 'Atomic protein structure refinement using all-atom graph representations and SE equivariant graph neural networks'\n\nThe paper primarily focuses on refining protein structures and assessing their quality. It proposes a novel E(3)-equivariant graph deep learning model for this cause. The key method used in this paper is inspired by Equivariant Graph Neural Networks [b11]. Moreover, the paper also mentions EquiBind [b18], EquiDock [b34], and other papers [b37, b39, b40] that use deep learning models to refine complex structures. \n\nPlease note that this is an inference based on the text and the author's notes. The actual source papers might be different and can be confirmed only by the author.\n\nHere is the result in the json format:\n{\n\"b11\": 1,\n\"b34\": 0.9,\n\"b18\": 0.9,\n\"b37\": 0.8,\n\"b39\": 0.8,\n\"b40\": 0.8\n}", "Based on the text, the source papers that inspired the author most are:\n\n{ \"b11\": 1.0, \"b34\": 0.9, \"b18\": 0.9, \"b35\": 0.8, \"b39\": 0.8, \"b51\": 0.7, \"b52\": 0.7, \"b60\": 0.6 }\n\nThe references b11, b34, and b18 are related to the use of E(3)-equivariant graph neural networks, which is the primary method proposed in the paper. The reference b35 discusses the AlphaFold-Multimer, which is mentioned multiple times as a method the authors built upon. References b39, b51, and b52 discuss methods for protein structure refinement and assessment, which is the main topic of the paper. b60 is related to the concept of \"positional corruption\" used in the paper. The confidence scores are based on the frequency of the references and their relevance to the main topic.", "Based on the text of the academic paper, the source papers that most likely inspired the author are:\n\n\"b11\": 'e n equivariant graph neural networks' - As indicated by the phrase \"Our work follows that of [b11] to incorporate E(3)-equivariance in our message passing neural network for 3D structure refinement and quality assessment.\"\n\n\"b34\": 'independent se equivariant models for end to end rigid protein docking' - Because the author referenced the EquiDock from this paper in building the EGR model.\n\n\"b18\": 'equibind geometric deep learning for drug binding structure prediction' \u2013 As the author combined insights from EquiBind from this paper while proposing EGR model.\n\n\"b40\": 'atomic protein structure refinement using all atom graph representations and se equivariant graph neural networks' \u2013 As it points to the exploration of using DL models for all-atom refinement which is a key part of the current paper's approach.\n\n\"b39\": 'fast and effective protein model refinement using deep graph neural networks' \u2013 As it shows the application of DL models for protein structure refinement, which is a key part of the current paper's approach.\n\n\"b35\": 'protein complex prediction with alphafold multimer' \u2013 As this paper was used for generating each target's decoy structure for refinement.\n\n\"b37\": 'improved protein structure refinement guided by deep learning based accuracy estimation' \u2013 As it shows the application of DL models for protein structure refinement, which is a key part of the current paper's approach.\n\n\"b10\": 'benchmarking of structure refinement methods for protein complex models' \u2013 As it examines the limits of existing methods for complex refinement which is a key problem the current paper is trying to address.\n\nIn JSON format, the result would look like this:\n```json\n{\n    \"b11\": 1,\n    \"b34\": 0.9,\n    \"b18\": 0.9,\n    \"b40\": 0.8,\n    \"b39\": 0.8,\n    \"b35\": 0.8,\n    \"b37\": 0.8,\n    \"b10\": 0.7\n}\n```", "Based on the analysis of the text and the guidelines, the source papers that inspired this study are:\n\n1. 'E(3)-equivariant graph neural networks' [b11]: The authors make use of the Equivariant Graph Neural Networks (EGNNs) method mentioned in this paper for their model. The indication comes from the phrase \"We exploit equivariant graph neural networks (EGNNs)\".\n\n2. 'D Equivariant Graph Neural Networks for Protein Model Quality Assessment' [b8]: This paper provides the assessment method for 3D protein structures used in the study. The phrase \"To assess the quality of 3D protein structures\" indicates this.\n\n3. 'Intrinsic Extrinsic Convolution and Pooling for Learning on 3D Protein Structures' [b21]: The paper describes the encoding of protein structural priors within neural network architectures which is used in the study. \n\n4. 'Equivariant Graph Refinement and Assessment of 3D Protein Complex Structures' [None]: This is the main paper itself, which clearly outlines the novel method introduced by the authors - the EGR model.\n\n5. 'Physics-informed deep neural network for rigid body protein-protein docking' [b33]: This paper inspired the development of new DL-based models for rigid body protein-protein docking, which is a significant component of the research.\n\n6. 'Protein Docking Model Evaluation by Graph Neural Networks' [b72]: This paper is used as a benchmark for evaluating the model's ability to estimate the quality of a protein complex.\n\n7. 'Protein Complex Prediction with AlphaFold-Multimer' [b35]: This paper is used for generating each target's decoy structures for refinement, which is a key part of the study's methodology.\n\nThe JSON format of the result will be:\n\n{ \"b11\": 0.9, \"b8\": 0.9, \"b21\": 0.8, \"None\": 1, \"b33\": 0.7, \"b72\": 0.7, \"b35\": 0.8 }"], "62466dd35aee126c0f8b79e7": ["{\n  \"b18\": 0.9,\n  \"b4\": 0.9,\n  \"b17\": 0.85,\n  \"b20\": 0.85\n}", "{\n    \"b18\": 0.9,\n    \"b4\": 0.85,\n    \"b17\": 0.8,\n    \"b20\": 0.8\n}", "{\n  \"b18\": 0.95,\n  \"b4\": 0.9,\n  \"b17\": 0.85,\n  \"b20\": 0.85\n}", "{\n    \"b18\": 0.9,\n    \"b4\": 0.9,\n    \"b17\": 0.8,\n    \"b20\": 0.8\n}", "{\n  \"b18\": 0.9,\n  \"b4\": 0.9,\n  \"b17\": 0.8,\n  \"b20\": 0.8\n}"], "6296d90e5aee126c0f730b92": ["Based on the given text, the source papers that most inspire the current study are:\n\n1. \"Spectral graph theory and its applications\" [b45]: This paper is one of the sources of the spectral analysis that the author uses in their paper. It is mentioned when the author explains the \"right-shift\" phenomenon of spectral energy distribution. The author also uses the spectral localized band-pass filters in graph anomaly detection. However, since it's mentioned in the middle of the text, it's not clear whether it directly influenced the author's main contributions.\n\n2. \"Wavelets on graphs via spectral graph theory\" [b20]: This paper is mentioned multiple times throughout the text and seems to be a primary source of inspiration for the author's novel Beta Wavelet Graph Neural Network (BWGNN). Hammond's graph wavelet theory, which is presumably presented in this paper, forms the basis for the author's new graph neural network architecture.\n\n3. \"Learning arbitrary graph spectral filters via Bernstein approximation\" [b22]: This paper is mentioned in the context of existing GNNs that utilize adaptive filters, which the author argues are not ideal for anomaly detection. The author's novel BWGNN, in contrast, provides a band-pass and spectral-localized solution. \n\n4. \"Data mining concepts and techniques\" [b21]: This paper is mentioned when the author refers to identifying anomalies in the data.\n\n5. \"Rethinking graph neural networks for anomaly detection\" [None]: This paper is indicated by the title, which suggests that it could be a key inspiration for the author's work in rethinking the application of GNNs for anomaly detection.\n\n6. \"Analyzing the expressive power of graph neural networks in a spectral perspective\" [b1]: This paper is mentioned when the author emphasizes the need for a tailored spectral filter as a key component of GNN design. \n\nBased on these mentions, the confidence scores for these papers could be as follows (in a format reference:confidence score):\n\n{\"b45\":0.6, \"b20\":0.9, \"b22\":0.8, \"b21\":0.7, \"None\":0.7, \"b1\":0.7}", "Based on the provided text and the titles of the references, the most relevant source papers for the given paper are:\n\n1. \"Analyzing the expressive power of graph neural networks in a spectral perspective\" (b1) - This paper seems to have played a significant role in inspiring the author's analysis of graph neural networks from a spectral perspective. \n\n2. \"Wavelets on graphs via spectral graph theory\" (b20) - The author has utilized Hammond's graph wavelet theory to develop the new graph neural network architecture, indicating significant reliance on this paper. \n\n3. \"Learning arbitrary graph spectral filters via Bernstein approximation\" (b22) - The mention of spectral filters relates to this paper's title.\n\n4. \"Simplifying graph convolutional networks\" (b50) - This paper seems to have inspired the author's discussion on the limitations of vanilla GNNs.\n\n5. \"Beyond low pass filtering graph convolutional networks with automatic filtering\" (b51) - This paper is mentioned in connection with the frequency responses of existing works, which the author has taken into account while developing the Beta Wavelet Graph Neural Network.\n\n6. \"Graph wavelet neural network\" (b52) - The title suggests that this paper might have been a significant inspiration for the author's proposed Beta Wavelet Graph Neural Network.\n\n7. \"Beta wavelets synthesis and application to lossy image compression\" (b0) - This paper seems to be related to the author's use of Beta wavelets.\n\nThe final results in json format:\n{\n    \"b1\": 0.9,\n    \"b20\": 0.9,\n    \"b22\": 0.8,\n    \"b50\": 0.8,\n    \"b51\": 0.8,\n    \"b52\": 0.9,\n    \"b0\": 0.7\n} \nThe confidence scores are estimated based on how directly the paper's contributions seem to rely on the respective source papers.", "Based on the provided information, the source papers can be identified as follows:\n\n1. \"rethinking graph neural networks for anomaly detection\" - This could be the primary source paper as it broadly addresses the topic of the given paper.\n\n2. \"analyzing the expressive power of graph neural networks in a spectral perspective\" - This paper is likely to be a source as it discusses the expressive power of GNNs, a topic that is covered in the given paper.\n\n3. \"outlier aware network embedding for attributed networks\" - This paper might be a source as it deals with anomaly detection, a central theme in the given paper.\n\n4. \"wavelets on graphs via spectral graph theory\" - Given the specific mention of wavelets in the given paper, this paper could be a source due to its focus on wavelets on graphs.\n\n5. \"learning arbitrary graph spectral filters via bernstein approximation\" - This paper might be a source as it discusses graph spectral filters, which are relevant to the given paper.\n\n6. \"graph convolutional neural networks with complex rational spectral filters\" - This paper might be a source as it discusses complex rational spectral filters in GNNs, a topic that is covered in the given paper.\n\n7. \"simplifying graph convolutional networks\" - This paper might be a source as it discusses simplifying GNNs, a topic that is relevant to the given paper.\n\nThe confidence scores are not provided in the context and hence cannot be determined.\n\nThe result in JSON format would look like this:\n\n{\n  \"b1\": 0.8,\n  \"b2\": 0.7,\n  \"b3\": 0.6,\n  \"b20\": 0.9,\n  \"b22\": 0.7,\n  \"b27\": 0.6,\n  \"b50\": 0.5\n}", "Based on the given text and information provided, the source papers that most likely influenced the current paper can be identified as follows:\n\n1. \"Wavelets on graphs via spectral graph theory\" [b20]: This paper is mentioned multiple times throughout the text, and its graph wavelet theory is explicitly invoked in the development of the new graph neural network architecture. The novel contribution of the paper, the Beta Wavelet Graph Neural Network (BWGNN), is directly based on the theories presented in this work. [Confidence Score: 0.9]\n\n2. \"Analyzing the expressive power of graph neural networks in a spectral perspective\" [b1]: This paper is quoted when the authors discuss the spectral filter as a key component of GNN design. This indicates that the authors may have drawn inspiration from this work when choosing the spectral filter for their GNN model. [Confidence Score: 0.8]\n\n3. \"Beyond low-pass filtering: graph convolutional networks with automatic filtering\" [b51]: This paper is mentioned when the authors discuss the limitations of existing GNN models and their inability to focus on the 'right-shift' effect. The authors then propose their new model (BWGNN) to overcome these limitations, suggesting that they were inspired by this paper to improve the existing models. [Confidence Score: 0.8]\n\n4. \"Learning arbitrary graph spectral filters via Bernstein approximation\" [b22]: This paper is referenced when discussing the frequency responses of existing works and their inability to guarantee band-pass and spectral-localized filters. This suggests that the authors were motivated by this paper to develop a new model (BWGNN) that could guarantee these properties. [Confidence Score: 0.7]\n\n5. \"Graph-based anomaly detection\" [b38]: This paper is cited when the authors highlight the need for further exploration in the field of graph-based anomaly detection. This indicates that this paper could have provided inspiration for the current research. [Confidence Score: 0.7]\n\n6. \"A comprehensive survey on graph anomaly detection with deep learning\" [b33]: This paper is mentioned when the authors discuss the importance of further exploration in the field of graph-based anomaly detection. As this paper provides a comprehensive survey on the subject, it could have played a significant role in shaping the authors' research. [Confidence Score: 0.6]\n\nThe result in the JSON format is:\n\n{\n\"b20\": 0.9,\n\"b1\": 0.8,\n\"b51\": 0.8,\n\"b22\": 0.7,\n\"b38\": 0.7,\n\"b33\": 0.6\n}", "Based on the provided text and author notes, the source papers that inspired the given academic paper appear to be:\n\n1. \"Spectral graph theory and its applications\" [b45]\n2. \"Wavelets on graphs via spectral graph theory\" [b20]\n3. \"Learning arbitrary graph spectral filters via Bernstein approximation\" [b22]\n4. \"Beta wavelets synthesis and application to lossy image compression\" [b0]\n5. \"Analyzing the expressive power of graph neural networks in a spectral perspective\" [b1]\n6. \"Procedures for detecting outlying observations in samples\" [b18]\n7. \"Graph-based anomaly detection\" [b38]\n8. \"A comprehensive survey on graph anomaly detection with deep learning\" [b33]\n\nThe paper in question seems to be primarily inspired by the concepts and methodologies proposed in these papers, as they are directly linked to the main challenges and methods outlined in the paper.\n\n{'b45': 0.9, 'b20': 0.9, 'b22': 0.9, 'b0': 0.9, 'b1': 0.9, 'b18': 0.8, 'b38': 0.8, 'b33': 0.7}"], "62997c0b5aee126c0f77cecc": ["Based on the provided text, the following source papers seem to have heavily inspired the main contributions of this paper:\n\n{ \n\"b42\": 0.9, \n\"b6\": 0.85, \n\"b36\": 0.82, \n\"b22\": 0.8, \n\"b3\": 0.76, \n\"b31\": 0.75, \n\"b1\": 0.7\n}\n\nThese papers seem to have inspired the author's approach to contrastive learning, hard negative sampling, model-based uncertainty measure, and the concept of representativeness. The confidence score is estimated based on the frequency and the context they are mentioned in.", "Based on the text and the author notes, the paper introduces a method for managing negative samples by adjusting their weight in the loss function. This method is based on three principles: the label of negative samples should be inconsistent with the positive sample label, positive and negative samples should be as similar as possible, and negative samples should have diverse characteristics. \n\nThe paper's novel contribution seems to be the introduction of the Uncertainty and Representativeness Mixing (UnReMix) for contrastive training, a method that combines importance scores that capture model uncertainty, representativeness, and anchor similarity. \n\nBased on the text, the most relevant source papers are:\n- 'b3': 'are all negatives created equal in contrastive instance discrimination arxiv preprint'\n- 'b42': 'contrastive learning with hard negative samples'\n- 'b22': 'hard negative mixing for contrastive learning'\n- 'b36': 'active contrastive learning of audio visual video representations'\n- 'b31': 'a domain agnostic strategy for contrastive representation learning'\n- 'b1': 'deep batch active learning by diverse uncertain gradient lower bounds'\n\nSo, the json format result is:\n```json\n{\n  \"b3\": 0.9,\n  \"b42\": 0.88,\n  \"b22\": 0.87,\n  \"b36\": 0.85,\n  \"b31\": 0.85,\n  \"b1\": 0.8\n}\n```", "Based on the content of the paper and the criteria provided, the primary challenges outlined were selecting appropriate informative hard negative examples for the success of contrastive learning. The author proposes a novel method called Uncertainty and Representativeness Mixing (UnReMix) for contrastive training, which combines importance scores that capture model uncertainty, representativeness, and anchor similarity.\n\nGiven the key phrases and the references, the most pertinent source papers appear to be:\n\n1. \"Contrastive learning with hard negative samples\" [b42]: This paper introduces the concept of hard negative samples in contrastive learning, which is a direct inspiration for the author's work.\n2. \"Active contrastive learning of audio-visual video representations\" [b36]: This paper introduces a method for uncertainty-based sampling, which the author uses in their proposed algorithm.\n3. \"Hard negative mixing for contrastive learning\" [b22]: It seems to have inspired the author to consider mixing feature-based importance scores.\n4. \"Boosting contrastive self-supervised learning with false negative cancellation\" [b21]: As the author's method penalizes false hard negatives, this paper seems to be a direct inspiration.\n5. \"A domain-agnostic strategy for contrastive representation learning\" [b31]: This paper's idea of incorporating representativeness in the selection of negative examples seems to have been adopted by the author.\n6. \"AdCo: adversarial contrast for efficient learning of unsupervised representations from self-trained negative adversaries\" [b20]: Mentioned as an example of a technique that maintains a separate global set for negative examples but doesn't adapt to different anchors, this paper seems to have been an inspiration for the author to develop a more flexible method.\n7. \"Un-mix: rethinking image mixtures for unsupervised visual representation learning\" [b46]: This paper's idea of mixing data in both input and prediction levels could have been an inspiration for the author's method.\n\nThe confidence scores are estimated based on the direct relevance of these references to the author's algorithm:\n\n```json\n{\n    \"b42\": 0.9,\n    \"b36\": 0.8,\n    \"b22\": 0.7,\n    \"b21\": 0.8,\n    \"b31\": 0.8,\n    \"b20\": 0.7,\n    \"b46\": 0.7\n}\n``` \n\nThis analysis is based on the content of the paper and the titles of the references. The actual content of the cited papers would provide a more accurate assessment.", "Based on the text and author notes, the source papers with the most influence on the given paper are likely to be the ones most directly related to the novel contributions of the paper, namely the method of adjusting the weight of negative samples in the loss function based on three principles. \n\nThe paper primarily addresses the challenges of selecting appropriate hard negative examples in contrastive learning and proposes a novel method, \"Uncertainty and Representativeness Mixing (UnReMix)\", for tackling these challenges. Hence, the source papers that directly contribute to these aspects should be considered.\n\nBased on the text, the following papers are most directly related to the paper's novel contributions:\n\n1. \"Hard Negative Mixing for Contrastive Learning\" [b22]: This paper is referred to multiple times in the context of hard negative sampling in contrastive learning, a key focus of the given paper. This paper might have inspired the authors to develop their novel method, UnReMix.\n\n2. \"Contrastive Learning with Hard Negative Samples\" [b42]: This paper is also referred to multiple times and is directly relevant to the given paper's focus on hard negative sampling in contrastive learning.\n\n3. \"Boosting Contrastive Self-Supervised Learning with False Negative Cancellation\" [b21]: This paper is mentioned in the context of anchor similarity, which is a key component of the proposed UnReMix method.\n\n4. \"Active Contrastive Learning of Audio-Visual Video Representations\" [b36]: This paper is mentioned in the context of model uncertainty, another key component of the proposed UnReMix method.\n\n5. \"A Simple Framework for Contrastive Learning of Visual Representations\" [b6]: This paper is frequently cited in the context of contrastive learning frameworks, which form the basis of the given paper's research.\n\n6. \"Momentum Contrast for Unsupervised Visual Representation Learning\" [b18]: This paper is also frequently cited in the context of contrastive learning, and it seems to be a key influence on the given paper's research.\n\n7. \"Deep Batch Active Learning by Diverse Uncertain Gradient Lower Bounds\" [b1]: This paper is mentioned in the context of model uncertainty, which is a key part of the proposed UnReMix method.\n\nThe confidence score for each paper can be determined by the frequency and context of its citation in the given paper, as well as its direct relevance to the paper's novel contributions.\n\nThe result may look like this in a JSON format:\n\n```\n{\n    \"b22\": 0.90,\n    \"b42\": 0.85,\n    \"b21\": 0.80,\n    \"b36\": 0.75,\n    \"b6\": 0.70,\n    \"b18\": 0.65,\n    \"b1\": 0.60\n}\n```", "Based on the given information and the procedures for identifying the most pertinent source papers, we can infer the source papers as follows:\n\n1. 'b6': 'a simple framework for contrastive learning of visual representations'\n2. 'b18': 'momentum contrast for unsupervised visual representation learning'\n3. 'b31': 'a domain agnostic strategy for contrastive representation learning'\n4. 'b36': 'active contrastive learning of audio visual video representations'\n5. 'b42': 'contrastive learning with hard negative samples'\n6. 'b1': 'deep batch active learning by diverse uncertain gradient lower bounds'\n7. 'b22': 'hard negative mixing for contrastive learning'\n\nThis list is derived by looking at the references that are associated with key phrases indicating direct inspiration or motivation, as well as those that are relevant to the novel contributions of the paper. The confidence score for each source paper is difficult to infer from the given text and would require a more detailed analysis of the paper's content. \n\nThe JSON format would look like this:\n\n{\n\"b6\": 0.9,\n\"b18\": 0.8,\n\"b31\": 0.7,\n\"b36\": 0.8,\n\"b42\": 0.9,\n\"b1\": 0.7,\n\"b22\": 0.8\n}\n\n(Note: The confidence scores are hypothetical and would require a detailed analysis of the paper's content to determine accurately)"], "622183525aee126c0f23c770": ["Based on the given text and the titles of the references, the source papers that may have strongly influenced the given paper are:\n\n1. \"Exploring simple siamese representation learning\" (b8): This paper is mentioned in the context of the contrastive regularization function used in the given paper. The authors of the given paper seem to have been inspired by the regularization function used in this paper, and then proposed an improved regularization function.\n   \n2. \"Noise against noise: stochastic label noise helps combat inherent label noise\" (b5): This paper is relevant as it is related to the problem of label noise, which is the main challenge addressed by the given paper. It's likely that this paper provided a significant inspiration for the treatment of label noise in the given paper.\n\n3. \"Big self-supervised models are strong semi-supervised learners\" (b6): This paper is cited in the context of the power of contrastive representation learning methods, which is a key focus of the given paper. The approach of this paper may have inspired the authors of the given paper to use contrastive learning.\n\n4. \"A closer look at memorization in deep networks\" (b2): This paper is cited when the authors of the given paper explain the memorization effect of deep networks, which is a significant aspect of the given paper's work. This paper likely influenced the authors to consider the memorization effect in their work.\n\n5. \"Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks\" (b25): This paper is mentioned in the context of the gradient analysis in the given paper. The approach of this paper may have provided significant inspiration for the authors' gradient analysis.\n\n6. \"Normalized loss functions for deep learning with noisy labels\" (b28): This paper is mentioned in the context of the underfitting problem that noise-robust loss functions suffer from. This paper may have influenced the authors' approach to noise-robust loss functions.\n\nBased on the criteria provided, the following references are the most likely sources of inspiration:\n\n```json\n{\n\"b8\": 0.9,\n\"b5\": 0.85,\n\"b6\": 0.8,\n\"b2\": 0.75,\n\"b25\": 0.7,\n\"b28\": 0.65\n}\n```", "Based on the provided text, academic papers that have been indicated as sources of inspiration or direct influence for the given paper are:\n\n- \"Exploring simple siamese representation learning\" [b8]: The authors have indicated that they have used an approach similar to SimSiam, which is a method for learning contrastive representations.\n- \"Noise against noise: stochastic label noise helps combat inherent label noise\" [b5]: The method used in the paper to add Gaussian noise to noisy labels to combat label noise is mentioned, indicating a possible influence.\n- \"Supervised contrastive learning\" [b19]: The authors have indicated that they have been inspired by contrastive learning methods, and this paper is one of the key references on this topic.\n- \"A closer look at memorization in deep networks\" [b2]: The memorization effect of deep networks, which is a central challenge addressed in the paper, is also discussed in this reference.\n- \"Can cross entropy loss be robust to label noise\" [b11]: This paper discusses the issue of cross entropy loss not being robust to label noise, which is a key challenge addressed in the given paper.\n\nTherefore, the JSON format response would look like this:\n\n```\n{\n  \"b8\": 0.9,\n  \"b5\": 0.8,\n  \"b19\": 0.9,\n  \"b2\": 0.8,\n  \"b11\": 0.8\n}\n```\nIn this response, each key is a reference number, and the corresponding value is a confidence score. A higher score indicates a greater degree of influence or relevance to the research in the given paper.", "Based on the text content, the author's note, and the titles of each paper, the source papers that directly influenced this paper are:\n\n1. \"on learning contrastive representations for learning with noisy labels\" [b0]: The paper's core contribution is developing a method for learning contrastive representations in a noisy label environment, which is the topic of this paper.\n\n2. \"a closer look at memorization in deep networks\" [b2]: The paper discusses the memorization effect in deep networks, which is one of the challenges mentioned in the text.\n\n3. \"big self supervised models are strong semi supervised learners\" [b6] and \"exploring simple siamese representation learning\" [b8]: Both papers discuss contrastive learning methods, which are utilized in the paper.\n\n4. \"noise against noise stochastic label noise helps combat inherent label noise\" [b5]: This paper deals with label noise, which is one of the main challenges discussed in the text.\n\n5. \"improving generalization by controlling label-noise information in neural network weights\" [b17]: The paper discusses how controlling label-noise information in neural network weights can improve generalization, which aligns with the algorithm proposed in the text.\n\n6. \"normalized loss functions for deep learning with noisy labels\" [b28]: This paper also discusses learning with noisy labels, which is the main challenge tackled in the text.\n\n7. \"an information theoretic framework for multi view learning\" [b37]: This paper discusses an information theory framework for learning, which appears to be the theoretical basis for the analysis in the text.\n\nThe JSON format is:\n\n{\n\"b0\": 1,\n\"b2\": 0.9,\n\"b6\": 0.8,\n\"b8\": 0.8,\n\"b5\": 0.9,\n\"b17\": 0.8,\n\"b28\": 0.9,\n\"b37\": 0.9\n}", "Based on the given text, the key challenges outlined in the paper are: (1) the expense and time-consuming nature of collecting large-scale, correctly labeled datasets for deep neural networks; (2) the challenge of inevitable label noise when using crowdsourcing-like platforms or nonexpert labelers; (3) the 'memorization effect' of deep networks, in which examples with noisy labels can ultimately be memorized, leading to poor performance; and (4) the issue of underfitting when using noise-robust loss functions.\n\nThe paper's novel contributions include theoretically analyzing the representations induced by a contrastive regularization function, proposing a novel algorithm to learn contrastive representations with noisy labels, and demonstrating that their method can be applied with existing label correction techniques and noise-robust loss functions to further boost performance.\n\nThe sources that seem to have directly inspired and contributed to the paper's novel contributions include:\n\n1. \"Exploring simple siamese representation learning\" [b8]\n2. \"Noise against noise: Stochastic label noise helps combat inherent label noise\" [b5]\n3. \"Big self-supervised models are strong semi-supervised learners\" [b6]\n4. \"Bootstrap your own latent: A new approach to self-supervised learning\" [b15]\n5. \"A closer look at memorization in deep networks\" [b2]\n6. \"Can cross entropy loss be robust to label noise\" [b11]\n7. \"Robust bi tempered logistic loss based on bregman divergences\" [b0]\n\nThe confidence score for each reference paper is as follows:\n\n{\"b8\": 0.9, \"b5\": 0.8, \"b6\": 0.7, \"b15\": 0.7, \"b2\": 0.8, \"b11\": 0.8, \"b0\": 0.7 }", "Based on the provided text and the titles of the references, the source papers that most likely inspired the given paper are:\n\n- \"big self supervised models are strong semi supervised learners\" [b6]\n- \"exploring simple siamese representation learning\" [b8]\n- \"a closer look at memorization in deep networks\" [b2]\n- \"noise against noise stochastic label noise helps combat inherent label noise\" [b5]\n- \"contrastive learning improves model robustness under label noise\" [b13]\n- \"an information theoretic framework for multi view learning\" [b37]\n- \"robust bi tempered logistic loss based on bregman divergences\" [b0]\n\nThe confidence scores for these references can be assigned based on the frequency and context of their mentions in the text. For instance, [b6] and [b8] are referenced in relation to contrastive learning, which is a key aspect of this paper's methodology. Other references like [b2] and [b5] are mentioned in relation to the paper's discussion on label noise and memorization in deep networks, indicating that these papers might have directly contributed to the proposed approach.\n\nThe returned result in the json format would be:\n\n```\n{\n  \"b6\": 0.9,\n  \"b8\": 0.9,\n  \"b2\": 0.8,\n  \"b5\": 0.8,\n  \"b13\": 0.7,\n  \"b37\": 0.7,\n  \"b0\": 0.7\n}\n```\n\nNote: The confidence scores are hypothetical and would need to be refined based on a more detailed analysis of the paper and its references. They are on a scale from 0 to 1, with 1 being the most confident."], "628d9e795aee126c0f9791fd": ["{\n  \"b16\": 0.95,\n  \"b36\": 0.90,\n  \"b45\": 0.85,\n  \"b47\": 0.85\n}", "{\n  \"b16\": 0.95,\n  \"b36\": 0.90,\n  \"b45\": 0.85,\n  \"b46\": 0.85,\n  \"b55\": 0.80,\n  \"b47\": 0.75\n}", "{\n  \"b16\": 0.9,\n  \"b45\": 0.85,\n  \"b36\": 0.8,\n  \"b55\": 0.75\n}", "{\n  \"b16\": 0.9,\n  \"b36\": 0.85,\n  \"b45\": 0.9,\n  \"b46\": 0.9,\n  \"b55\": 0.85\n}", "{\n  \"b16\": 0.95,\n  \"b36\": 0.9,\n  \"b45\": 0.85,\n  \"b46\": 0.85,\n  \"b47\": 0.8,\n  \"b52\": 0.8,\n  \"b55\": 0.75\n}"], "6287492a5aee126c0ffe8231": ["Based on the provided information and the instructions, I will analyze the text to identify the references that are most directly relevant to the paper's novel contributions and the ones that have directly inspired the work.\n\nThe novel contributions of the paper are:\n1. The proposal of a simple yet effective strategy for training neural topic models with pre-training and fine-tuning that uses the same architecture for both stages.\n2. Extensive experiments showing that pre-trained neural topic models significantly outperform baselines in terms of topic coherence and topic diversity.\n3. A demonstration that the proposed approach greatly reduces the amount of training data needed.\n\nFrom the text, we can identify several key phrases that indicate direct inspiration or reliance on previous work:\n- The paper mentions that it proposes a strategy \"to utilize extensive knowledge from large corpora for neural topic modeling with low computational complexity.\" This suggests that any method which has previously attempted to reduce computational complexity while leveraging large corpora could be a source of inspiration.\n- The approach of pre-training a neural topic model on a large corpus and then fine-tuning on the target dataset is mentioned as the main contribution, which suggests that references related to pre-training and fine-tuning methods would be highly relevant.\n\nHere are the source papers that appear to be most central to the paper's contributions based on the provided text and the titles of the references:\n\n```json\n{\n  \"b13\": 0.9, \n  \"b30\": 0.8,\n  \"b1\": 0.7,\n  \"b2\": 0.7,\n  \"b31\": 0.6,\n  \"b23\": 0.6,\n  \"b8\": 0.5,\n  \"b25\": 0.5\n}\n```\n\nExplanation:\n- `b13` (\"Improving neural topic models using knowledge distillation\"): This paper seems to directly influence the proposed method by using PLM to guide the training of a topic model, which is a form of pre-training.\n- `b30` (\"TopicOcean an ever increasing topic model with metalearning\"): This reference suggests the use of transferring knowledge from well-trained models to new corpora, which relates to the pre-training concept.\n- `b1` and `b2` are related to the use of PLM outputs in topic modeling, which is closely related to the incorporation of external knowledge that the paper discusses.\n- `b31` and `b23` are mentioned as related work in neural topic modeling, which the paper builds upon.\n- `b8` (\"BERT pre-training of deep bidirectional transformers for language understanding\"): As the paper discusses the use of PLMs, BERT is a foundational model in this domain.\n- `b25` (\"GloVe global vectors for word representation\"): Mentioned as a comparison in the performance of models based on word embeddings, indicating it might have influenced the discussion on embeddings in the paper.\n\nThe confidence scores are subjective estimates based on the provided information and the context in which the references are mentioned in the text. They indicate the likelihood of each reference being a central source of inspiration for the paper's contributions.", "Based on the information provided, the task at hand is to identify source papers that the given paper relies on heavily or takes inspiration from for its novel contributions. The novel contributions of the paper, as outlined in the text, are centered around a simple yet effective strategy for training neural topic models using pre-training on a large corpus followed by fine-tuning on a specific dataset.\n\nGiven the author notes and the text of the paper, we will focus on papers that are directly related to:\n\n1. Pre-training neural topic models directly on large corpora.\n2. Fine-tuning these models on target datasets.\n3. Achieving significant performance gains in terms of topic coherence and diversity with a reduced need for training data.\n\nFrom the paper text, we can infer that references [b13], [b1], and [b2] are directly related to pre-training and fine-tuning strategies that have influenced the proposed approach. Furthermore, reference [b31] is relevant as it discusses neural variational inference for text processing, which is likely related to the neural topic model architecture used in the paper.\n\nLet's prepare a JSON object with the reference numbers as keys and confidence scores as values, considering the strong methodological similarities, direct inspiration, or motivation phrases, and the direct relevance to the paper's novel contributions:\n\n```json\n{\n  \"b13\": 0.9,\n  \"b1\": 0.9,\n  \"b2\": 0.9,\n  \"b31\": 0.8\n}\n```\n\nThe confidence scores are subjective and based on the information provided. The highest confidence is given to references that are explicitly mentioned in the context of pre-training and fine-tuning strategies. Reference [b31] is also included due to its methodological relevance, albeit with a slightly lower confidence score as it may not be directly mentioned in the context of inspiration or motivation but is still closely related to the architecture of the neural topic model.", "Based on the provided information, we can infer the most pertinent source papers that inspire the given paper, which proposes a method for pre-training and fine-tuning neural topic models with external knowledge. By analyzing the text and the titles of the references, we can identify references that are directly relevant to the paper's novel contributions and highlight those preceded by phrases indicating direct inspiration or motivation.\n\nHere's a JSON-formatted list of the source papers with confidence scores based on the criteria:\n\n```json\n{\n    \"b13\": 0.9,\n    \"b31\": 0.8,\n    \"b23\": 0.8,\n    \"b34\": 0.7,\n    \"b30\": 0.7,\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b25\": 0.5\n}\n```\n\nThe confidence scores indicate the likelihood of each reference being a direct inspiration for the novel contributions stated in the paper.\n\nThe rationale for the selection is as follows:\n\n- `b13`: The paper employed knowledge distillation from a PLM to a student topic model, which aligns closely with the idea of using pre-training to incorporate extensive knowledge for neural topic modeling.\n\n- `b31` and `b23`: Both papers discuss neural topic modeling with variational autoencoders and Wasserstein autoencoders, relevant to the architecture and training objective of the NTM presented in the paper.\n\n- `b34`: This paper presents an adversarial training approach, which is relevant to the discussion of neural topic models that utilize adversarial training methods.\n\n- `b30`: TopicOcean is a method for pre-training LDA-based topic models and transferring knowledge to new corpora, which is conceptually similar to the pre-training approach proposed in the paper.\n\n- `b1` and `b2`: Both papers discuss the use of PLM outputs to enhance topic models, which is related to the motivation behind the paper's methodology.\n\n- `b25`: The GloVe embeddings comparison in the paper suggests that the authors are aware of the limitations of PLMs and the benefits of alternative embedding methods like GloVe.\n\nEach reference is given a confidence score based on its perceived relevance to the novel contributions and methodologies described in the paper. The confidence scores are subjective and based on the author's interpretation of the text.", "Based on the provided information and the methodology outlined, we should look for references that are directly tied to the novel contributions of the paper. According to the author's note and the text of the paper, the novel contributions are as follows:\n\n1. The proposed Pre-trained Neural Topic Model (PT-NTM) directly pre-trains the topic model on a large corpus and then fine-tunes it on a specific dataset.\n2. The approach reduces the computational overhead as the architecture used for pre-training and fine-tuning is the same.\n3. Extensive experiments show that the PT-NTM significantly outperforms baselines in terms of topic coherence and topic diversity.\n4. The approach reduces the amount of training data needed, as demonstrated by the experiments on the NYTimes dataset.\n\nGiven this, we should prioritize finding references that the paper mentions in direct relation to these contributions. We are looking for key phrases indicating inspiration or methodological similarities to previous work.\n\nScanning the paper for such phrases, it appears that references [b13], [b1], [b2], and [b30] are directly related to the novel contributions of the paper:\n\n- [b13] is mentioned as an example of knowledge distillation from a PLM to a topic model, which is a related technique to what the authors are doing but with limitations that the authors aim to overcome.\n- [b1] and [b2] are mentioned in the context of PLMs being used as additional inputs or replacements for BoWs in topic modeling, which is related to the authors' approach but again with computational overhead that the authors aim to reduce.\n- [b30] is related to the concept of pre-training (TopicOcean), but the authors differentiate their PT-NTM by using the same model architecture for both pre-training and fine-tuning, unlike the methods that combine PLMs with topic models in [b30].\n\nConsidering this, here is a JSON-formatted list of the source papers with confidence scores reflecting their relevance to the paper's novel contributions:\n\n```json\n{\n  \"b13\": 0.7,\n  \"b1\": 0.7,\n  \"b2\": 0.7,\n  \"b30\": 0.6\n}\n```\n\nThe confidence scores are subjective estimates based on the information provided. They reflect the direct relevance and influence on the paper's novel contributions. The scores are less than 1 as there is no explicit statement like \"Inspired by [reference]\", but the references are mentioned in the context of related work that the authors are building upon or differentiating from.", "Based on the provided text and the instructions, the task is to identify the source papers that have inspired the novel contributions of the given paper. The paper introduces a Pre-trained Neural Topic Model (PT-NTM) that directly pre-trains the topic model itself on knowledge source corpora and then fine-tunes on the target dataset. The key phrases and relevant references are identified, and the confidence score is assigned based on how directly they appear to have influenced the main contributions of the paper.\n\nHere are the source papers that appear to have directly inspired the novel contributions of the given paper:\n\n```json\n{\n  \"b13\": 0.9,\n  \"b23\": 0.8,\n  \"b30\": 0.7,\n  \"b31\": 0.8,\n  \"b34\": 0.7\n}\n```\n\nExplanation:\n\n- `b13`: This paper is mentioned in the context of using knowledge distillation from a PLM to guide the training of a student topic model. The proposed PT-NTM differs from this approach, but the mention indicates that the authors were inspired to improve upon the computational inefficiency of this method.\n- `b23`: Mentioned in the context of employing a similar approach to impose a Dirichlet prior on topic distributions by minimizing MMD, which is also used in the proposed PT-NTM methodology.\n- `b30`: The TopicOcean method is referenced as an approach to pre-training, but the PT-NTM approach is different in that it does not combine large PLMs with topic models but instead pre-trains the topic model directly. This shows that the authors were influenced by the concept but sought to refine it.\n- `b31`: This paper is discussed in relation to neural topic models using VAEs and the challenges with using a Dirichlet prior, which the PT-NTM has addressed in its methodology.\n- `b34`: Mentioned in the context of adversarial training and extending BAT to incorporate pre-trained word embeddings. The PT-NTM approach differs but seems to build upon the idea of incorporating external knowledge into topic models.\n\nNote: The confidence scores are subjective estimates based on the context in which the references are mentioned and the similarity of methodologies. These scores reflect the perceived influence of the cited works on the novel contributions of the paper."], "62982a9a5aee126c0f6f5f99": ["{\n    \"b7\": 0.9,\n    \"b27\": 0.8,\n    \"b32\": 0.7,\n    \"b17\": 0.6\n}", "{\n    \"b7\": 0.95,\n    \"b27\": 0.9,\n    \"b17\": 0.85,\n    \"b32\": 0.8,\n    \"b53\": 0.75\n}", "{\n  \"b7\": 0.9,\n  \"b27\": 0.9,\n  \"b17\": 0.8,\n  \"b32\": 0.7,\n  \"b53\": 0.7,\n  \"b47\": 0.6\n}", "{\n  \"b7\": 0.9,\n  \"b27\": 0.8,\n  \"b17\": 0.7\n}", "{\n    \"b7\": 0.9,\n    \"b27\": 0.8,\n    \"b17\": 0.7,\n    \"b32\": 0.6\n}"], "626f3dd05aee126c0f8f76a7": ["Based on the text and the provided titles of the references, the source papers of this paper appear to be:\n\n1. \"CogView: Mastering Text to Image Generation via Transformers [b2]\" - This paper is mentioned multiple times and seems to be a direct inspiration for the work. The authors also reference the methodologies and shortcomings of CogView in their work.\n2. \"MaskGIT: Masked Generative Image Transformer [b0]\" - This paper is referred to when discussing the novel methods introduced in the paper, indicating a significant influence.\n3. \"All NLP Tasks Are Generation Tasks: A General Pretraining Framework [b5]\" - The paper mentions their use of the GLM model from this paper.\n4. \"Taming Transformers for High-Resolution Image Synthesis [b6]\" - This paper seems to be an inspiration for the high-resolution aspect of the work.\n5. \"Generative Adversarial Networks [b9]\" - This paper is referenced in the context of the challenges the authors are addressing.\n6. \"Vector Quantized Diffusion Model for Text to Image Synthesis [b10]\" - This is referenced as a similar work that used non-autoregressive methods, indicating it may have influenced the authors' approach.\n7. \"Masked Autoencoders Are Scalable Vision Learners [b11]\" - The paper's use of a bidirectional context indicates influence from this paper.\n\nHere is the json format of the result:\n{\"b2\": 1,  \"b0\": 0.9, \"b5\": 0.85, \"b6\": 0.8, \"b9\": 0.7, \"b10\": 0.65, \"b11\": 0.6}", "Based on the text and the titles of the papers, the source papers that inspired this work are:\n\n{ \"b2\": 1, \"b5\": 0.8, \"b9\": 0.6, \"b11\": 0.7, \"b26\": 0.7, \"b0\": 0.8, \"b20\": 0.6}\n\nExplanation:\n- \"b2\" (CogView: Mastering Text to Image Generation via Transformers) is mentioned multiple times and directly compared to the work in the paper. \n- \"b5\" (All NLP Tasks Are Generation Tasks: A General Pretraining Framework) is mentioned as the inspiration for their pretraining method. \n- \"b9\" (Generative Adversarial Networks) is referenced as part of the historical context, but the authors also mention that their work tries to overcome some of the shortcomings of GANs, suggesting a strong influence.\n- \"b11\" (Masked Autoencoders Are Scalable Vision Learners) and \"b26\" (Attention Is All You Need) are both mentioned in the context of the limitations that their work addresses.\n- \"b0\" (MaskGIT: Masked Generative Image Transformer) is mentioned as a similar work to theirs.\n- \"b20\" (Learning Transferable Visual Models From Natural Language Supervision) is mentioned in the context of the challenges their work addresses.", "Based on the text of the academic paper and the titles of the references, the most relevant source papers that have inspired this work are:\n\n1. \"CogView: Mastering Text to Image Generation via Transformers[b2]\"\n2. \"MaskGIT: Masked Generative Image Transformer[b0]\"\n3. \"Taming Transformers for High Resolution Image Synthesis[b6]\"\n4. \"Vector Quantized Diffusion Model for Text to Image Synthesis[b10]\"\n5. \"Mask-Predict: Parallel Decoding of Conditional Masked Language Models[b8]\"\n6. \"All NLP Tasks Are Generation Tasks: A General Pretraining Framework[b5]\"\n7. \"Generative Adversarial Networks[b9]\"\n8. \"Attention Is All You Need[b26]\"\n\nThe JSON format with confidence scores would be:\n{\n    \"b2\": 1,\n    \"b0\": 0.9,\n    \"b6\": 0.8,\n    \"b10\": 0.8,\n    \"b8\": 0.7,\n    \"b5\": 0.7,\n    \"b9\": 0.6,\n    \"b26\": 0.6\n} \n\nThese papers have significant influence on the main contributions of the paper. They cover topics like text-to-image generation with Transformers, masked generative image transformation, high resolution image synthesis with Transformers, vector quantized diffusion models for text-to-image synthesis, parallel decoding of conditional masked language models, a general pretraining framework for all NLP tasks, the fundamental concept of generative adversarial networks, and the attention mechanism in Transformers.", "Based on the given text and the titles of the references, the most pertinent source papers that inspire the author are:\n\n1. \"CogView: Mastering Text to Image Generation via Transformers [b2]\" - This paper clearly influences the development of the novel model \"CogView2\". The author also uses the \"post-selection method introduced in CogView [b2]\" for their own model. Confidence score: 0.9\n2. \"All NLP Tasks Are Generation Tasks: A General Pretraining Framework [b5]\" - The author's approach to their pretraining framework, Cross-Modal General Language Model (CogLM), is inspired by the General Language Model (GLM) from this paper. Confidence score: 0.8\n3. \"Masked Autoencoders Are Scalable Vision Learners [b11]\" - The author mentions this paper when discussing the importance of bidirectional context, which is a feature of their CogLM. Confidence score: 0.8\n4. \"Generative Adversarial Networks [b9]\" - The author has mentioned GANs a few times throughout the paper and seems to draw inspiration from them for their approach to generation. Confidence score: 0.7\n5. \"MaskGIT: Masked Generative Image Transformer [b0]\" - The author takes inspiration from this paper when discussing the direct super-resolution part of their hierarchical generation process. Confidence score: 0.7\n6. \"Taming Transformers for High-Resolution Image Synthesis [b6]\" - This paper is cited when discussing the tokenization strategy. Confidence score: 0.7\n\nThe JSON result is: {\"b2\":0.9, \"b5\":0.8, \"b11\":0.8, \"b9\":0.7, \"b0\":0.7, \"b6\":0.7}\n", "Based on the text of the paper and the titles of the reference papers, here are the source papers that seem to have inspired the authors most:\n\n1. \"CogView: Mastering Text to Image Generation via Transformers\" [b2]\n2. \"DALL-E: Zero-Shot Text to Image Generation\" [b23]\n3. \"All NLP Tasks are Generation Tasks: A General Pretraining Framework\" [b5]\n4. \"Learning a Deep Convolutional Network for Image Super-Resolution\" [b3]\n5. \"Mask-Predict: Parallel Decoding of Conditional Masked Language Models\" [b8]\n6. \"Masked Autoencoders Are Scalable Vision Learners\" [b11]\n7. \"MaskGIT: Masked Generative Image Transformer\" [b0]\n\nThe confidence scores are provided below in the JSON format:\n\n{\n\"b2\": 1.0,\n\"b23\": 0.9,\n\"b5\": 0.85,\n\"b3\": 0.8,\n\"b8\": 0.75,\n\"b11\": 0.7,\n\"b0\": 0.65\n}"], "628c6264f66cd1000c54e4e9": ["Based on the analysis of the provided paper, the following source papers are most relevant and influential to the work:\n\n{\"b14\": 0.9, \"b15\": 0.8, \"b27\": 0.85, \"b28\": 0.8, \"b40\": 0.75, \"b51\": 0.9, \"b52\": 0.75, \"b53\": 0.7}\n\nExplanation:\nThe paper's main contributions revolve around the new approach called \"Imagen\", which is a text-to-image diffusion model. The novel algorithm is primarily inspired by the power of transformer language models (LMs) [b14] [b51] and high-fidelity diffusion models [b27] [b28]. The authors also indicated that they are inspired by the work of [b15] as they corroborate the results of their text-guided diffusion work. The paper mentioned [b40] and [b52] as part of the prior work that used only image-text data for model training, and their new approach improves upon this by using text embeddings from large LMs [b51] [b14]. The paper also mentioned the work of [b53] as part of concurrent work on the same topic.", "Based on the text, the paper introduces a new method called \"Imagen\" that uses a combination of transformer language models and diffusion models to create high-fidelity, photo-realistic images from text. There are several key phrases and references in the text that indicate strong reliance on previous research for developing the paper's core contributions. Therefore, the source papers that might have inspired this paper are:\n\n{ \n\"b52\": 0.9, \n\"b11\": 0.8, \n\"b56\": 0.7, \n\"b48\": 0.8, \n\"b30\": 0.6, \n\"b73\": 0.6, \n\"b14\": 0.7, \n\"b51\": 0.9, \n\"b27\": 0.8, \n\"b28\": 0.8, \n\"b15\": 0.8, \n\"b40\": 0.8,\n\"b53\": 0.9 \n}\n\nThese papers are related to text-to-image synthesis, image-text contrastive learning, transformer language models, and high-fidelity diffusion models, which are all major components of the \"Imagen\" model introduced in the paper. Further, some of the references are mentioned in the context of direct inspiration or motivation, like 'inspired by [reference]', which indicates a strong influence on the paper's main contributions.", "Based on the text and author notes, the source papers that most likely inspired this work appear to be:\n\n- \"Diffusion models beat GANs on image synthesis\" (b15) due to the mention of using diffusion models for generating high-fidelity images. \n- \"Exploring the limits of transfer learning with a unified text-to-text transformer\" (b51) since the work heavily relies on large transformer language models for understanding text.\n- \"GLIDE: Towards photorealistic image generation and editing with text guided diffusion models\" (b40) as it directly relates to the proposed image generation approach and is directly referenced in the context of motivation. \n- \"Cascaded diffusion models for high fidelity image generation\" (b28) since the paper mentions using cascaded diffusion models for image generation.\n- \"BERT: Pre-training of deep bidirectional transformers for language understanding\" (b14) as the paper mentions using BERT as a text encoder. \n- \"VQGAN+CLIP: Open-domain image generation and editing with natural language guidance\" (b11) as it is related to text-to-image synthesis which is the main focus of the paper.\n- \"Zero-shot text-to-image generation\" (b52) is another paper that is directly relevant to the paper's main contributions.\n\nSo, the result is:\n{\n  \"b15\": 0.9,\n  \"b51\": 0.9,\n  \"b40\": 0.85,\n  \"b28\": 0.8,\n  \"b14\": 0.8,\n  \"b11\": 0.75,\n  \"b52\": 0.75\n}", "Based on the text and given instructions, the following papers seem to be the most influential sources for the given paper:\n\n1. \"Zero-shot text to image generation\" [b52] - This paper is mentioned multiple times and it seems like the author's work builds upon this paper's approach to text-to-image synthesis.\n\n2. \"Diffusion models beat GANs on image synthesis\" [b15] - This paper introduces diffusion models which are a core part of the algorithm proposed in the paper.\n\n3. \"Exploring the limits of transfer learning with a unified text to text transformer\" [b51] - This paper introduces the T5 model which the author uses for text encoding.\n\n4. \"GLIDE towards photorealistic image generation and editing with text guided diffusion models\" [b40] - This paper is also mentioned multiple times and it seems like the author's work builds upon this paper's approach to text-to-image synthesis.\n\n5. \"Cascaded diffusion models for high fidelity image generation\" [b28] - The cascaded diffusion model is used in the author's proposed algorithm.\n\n6. \"Classifier free diffusion guidance\" [b26] - The classifier-free guidance technique mentioned in this paper is used in the author's proposed algorithm.\n\n7. \"High resolution image synthesis with latent diffusion models\" [b56] - This paper is mentioned in the context of the author's work on high resolution image synthesis.\n\n8. \"BERT: Pre-training of deep bidirectional transformers for language understanding\" [b14] - This paper introduces the BERT model which the author explores for text encoding.\n\nThe confidence scores cannot be calculated precisely due to a lack of explicit information on the importance of each reference in the text. However, based on the frequency and context of their mentions, the confidence scores might look something like this:\n\n```json\n{\n    \"b52\": 0.9,\n    \"b15\": 0.8,\n    \"b51\": 0.8,\n    \"b40\": 0.8,\n    \"b28\": 0.7,\n    \"b26\": 0.7,\n    \"b56\": 0.7,\n    \"b14\": 0.6\n}\n```\n\nThese scores are only estimations and might not reflect the exact level of influence each paper had on the given paper.", "Based on the text and the titles of the references, the source papers that seem to have inspired the author the most are:\n\n{'b52': 0.9, 'b40': 0.9, 'b14': 0.8, 'b51': 0.8, 'b27': 0.7, 'b28': 0.7, 'b15': 0.7, 'b53': 0.6} \n\nThese papers are frequently mentioned throughout the text and are directly linked to the core contributions of the paper."], "629c4e2a5aee126c0f6f8d0b": ["{\n  \"b5\": 0.9,\n  \"b8\": 0.85,\n  \"b9\": 0.75,\n  \"b10\": 0.7,\n  \"b11\": 0.7,\n  \"b14\": 0.85,\n  \"b15\": 0.8,\n  \"b16\": 0.8,\n  \"b17\": 0.8\n}", "```json\n{\n    \"b5\": 0.9,\n    \"b6\": 0.8,\n    \"b7\": 0.7,\n    \"b8\": 0.8,\n    \"b9\": 0.7,\n    \"b10\": 0.6,\n    \"b11\": 0.6\n}\n```", "{\n  \"b0\": 0.8,\n  \"b6\": 0.9,\n  \"b7\": 0.7,\n  \"b8\": 0.8,\n  \"b9\": 0.7,\n  \"b19\": 0.8\n}", "{\n  \"b0\": 0.85,\n  \"b8\": 0.8,\n  \"b19\": 0.75,\n  \"b9\": 0.7,\n  \"b6\": 0.65\n}", "{\n  \"b8\": 0.9,\n  \"b19\": 0.8,\n  \"b7\": 0.7,\n  \"b6\": 0.6,\n  \"b9\": 0.6\n}"], "62aa9fb55aee126c0fa5cbb7": ["Based on the text, the following papers have had the most influence:\n\n1. \"Learning to simulate complex physics with graph networks\" [b18]: The paper mentions that their work builds upon this paper, implying a strong influence. The confidence score is 0.9 given the explicit mention.\n\n2. \"D u-net learning dense volumetric segmentation from sparse annotation\" [b6]: The 3D-U-Net referenced in this paper is used in the hybrid model proposed, indicating a significant influence. The confidence score is 0.85 due to explicit mention and usage of the method.\n\n3. \"Geodin geoscience based deep interaction networks for predicting flow dynamics in reservoir simulation models\" [b16]: This paper is mentioned in the context of being a concurrent work, suggesting that it might have had some influence on the authors. The confidence score is 0.7 due to indirect comparison and similarity in approach.\n\n4. \"Group normalization\" [b22]: This work is referenced in the context of being utilized in their model, indicating some level of influence. The confidence score is 0.6 due to being a technique used in their model, though it doesn't seem to be a core part of their approach.\n\n5. \"A deep learning based surrogate model for data assimilation in dynamic subsurface flow problems\" [b19]: This paper is referenced as a method that is insufficient for addressing the challenges of subsurface simulation, suggesting that it influenced the development of their novel method by highlighting its limitations. The confidence score is 0.6 due to indirect comparison and influence in the development of the novel method.\n\n```json\n{\n\"b18\": 0.9,\n\"b6\": 0.85,\n\"b16\": 0.7,\n\"b22\": 0.6,\n\"b19\": 0.6\n}\n```", "Based on the task and the given academic paper, I identify the following papers as the sources that most inspired this work:\n\n1. \"Learning to simulate complex physics with graph networks\" [b18]\n2. \"D U-Net: learning dense volumetric segmentation from sparse annotation\" [b6]\n3. \"Solver in theloop learning from differentiable physics to interact with iterative pde solvers\" [b20]\n4. \"Geodin geoscience based deep interaction networks for predicting flow dynamics in reservoir simulation models\" [b16]\n\nThe confidence scores for these references are:\n\n1. [b18]: 0.9\n2. [b6]: 0.8\n3. [b20]: 0.7\n4. [b16]: 0.7\n\nThe JSON format for the answer is:\n\n{\n\t\"b18\": 0.9,\n\t\"b6\": 0.8,\n\t\"b20\": 0.7,\n\t\"b16\": 0.7\n} \n\nPlease note that the confidence score is based on the occurrence of key phrases and the relevance of other papers to the primary challenges and algorithm proposed in this paper.", "Based on the given academic paper text, the source papers that most influenced the authors appear to be:\n\n1. \"Learning to simulate complex physics with graph networks\" [b18] - The authors directly mention GNS [b18] as a state-of-the-art model for particle-based simulation that their work builds upon. The mention of this paper in the context of their own work's novelty suggests it had a significant influence. \n\n2. \"D U-Net: learning dense volumetric segmentation from sparse annotation\" [b6] - The authors mention the use of 3D-U-Net [b6] to model more global dynamics of pressure in their proposed HGNS model. This implies that [b6] had a significant influence on their work.\n\n3. \"Geodin: geoscience based deep interaction networks for predicting flow dynamics in reservoir simulation models\" [b16] - This paper is mentioned as a concurrent work, which implies that it had a certain influence on the authors' work. \n\n4. \"Interaction networks for learning about objects, relations, and physics\" [b2] - This paper is directly referenced in the context of the authors' work. This implies that it had a significant influence on their work. \n\n5. \"Physics informed deep learning for transport in porous media: Buckley\u2013Leverett problem\" [b9] - This paper is mentioned in the context of physics-based methods for developing surrogate models for subsurface flow, which the authors\u2019 work also focuses on.\n\n6. \"Group normalization\" [b22] - This paper is mentioned in the context of the authors' work on the processor of the Subsurface Graph Neural Network (SGNN). This implies that it had a significant influence on their work.\n\n7. \"A deep learning based surrogate model for data assimilation in dynamic subsurface flow problems\" [b19] - The authors mention this paper in relation to the limitations of current data-driven surrogate models. This suggests that it had a notable influence on their work.\n\nThe JSON format of the results would be:\n\n{\n\"b18\": 0.9,\n\"b6\": 0.85,\n\"b16\": 0.8,\n\"b2\": 0.8,\n\"b9\": 0.75,\n\"b22\": 0.7,\n\"b19\": 0.7\n}", "The source papers that most directly influenced the given paper appear to be:\n\n- \"Learning to simulate complex physics with graph networks\" [b18]\n- \"Development and application of reduced order modeling procedures for subsurface flow simulation\" [b5]\n- \"3D U-Net: Learning dense volumetric segmentation from sparse annotation\" [b6]\n- \"Interaction networks for learning about objects, relations, and physics\" [b2]\n- \"Geodin: Geoscience based deep interaction networks for predicting flow dynamics in reservoir simulation models\" [b16]\n- \"A deep learning based surrogate model for data assimilation in dynamic subsurface flow problems\" [b19]\n\nThe paper directly indicates that it is building upon [b18] by using a similar Graph Network Simulators (GNS) model for particle-based simulation. It also refers to [b5] when discussing previous work in subsurface flow simulation. The paper adopts 3D-U-Net [b6], a convolutional autoencoder for volumetric segmentation, to model the more global dynamics of pressure. The interaction network [b2] is referred to while discussing the application to model the subsurface fluid dynamics. The work [b16] is mentioned to be concurrent to the authors' work. The work [b19] is referred to while discussing previous work on recurrent R-U-Net for learning subsurface surrogate models.\n\nThe json format answer is:\n\n{\n\"b18\": 1.0,\n\"b5\": 0.8,\n\"b6\": 1.0,\n\"b2\": 0.8,\n\"b16\": 0.8,\n\"b19\": 0.8\n}", "Based on the text of the academic paper, the following references appear to be the most influential sources:\n\n1. \"Interaction networks for learning about objects relations and physics\" [b2]\n2. \"D u-net learning dense volumetric segmentation from sparse annotation\" [b6]\n3. \"Learning to simulate complex physics with graph networks\" [b18]\n4. \"Solver in theloop learning from differentiable physics to interact with iterative pde solvers\" [b20]\n5. \"A deep learning based surrogate model for data assimilation in dynamic subsurface flow problems\" [b19]\n6. \"Geodin geoscience based deep interaction networks for predicting flow dynamics in reservoir simulation models\" [b16]\n7. \"Fast multiscale reservoir simulations with pod deim model reduction\" [b24]\n8. \"New frontiers in large scale reservoir simulation\" [b8]\n\nBased on the text of the academic paper, these references seem to be the most directly related to the paper's novel contributions. The paper cites these works as inspiration and bases its approach on the methods detailed in these papers.\n\nHere is the result in json format:\n\n{\n\"b2\": 0.9,\n\"b6\": 0.9,\n\"b18\": 0.9,\n\"b20\": 0.8,\n\"b19\": 0.8,\n\"b16\": 0.8,\n\"b24\": 0.7,\n\"b8\": 0.7\n}"], "6260bd7f5aee126c0fc6bbc3": ["Based on the provided information, the key challenges of the paper are to grasp knowledge about abstract concepts in morals, capture inter-event discourse relations in stories, and align value preferences of stories and morals. The authors propose a new dataset named STORAL for moral story understanding and generation, and a retrieval-augmented algorithm to improve the model performance.\n\nThe following source papers seem to have influenced the paper's main contributions:\n\n1. \"A corpus for understanding and generating moral stories\" - It is most likely one of the main sources of inspiration given the close relation to the paper's topic.\n2. \"BERT: Pre-training of deep bidirectional transformers for language understanding\" - It is mentioned in the context of the retrieval-augmented algorithm, indicating its influence on the proposed method.\n3. \"Towards machine ethics and norms\" - The paper might have taken inspiration from this work, considering the paper's focus on morals.\n4. \"A knowledge-enhanced pretraining model for commonsense story generation\" - This paper's approach might have influenced the proposed algorithm.\n5. \"Story ending generation with incremental encoding and commonsense knowledge\" - The paper's focus on story generation and the close relation to the paper's topic suggest it could be a source of inspiration.\n6. \"Aligning AI with shared human values\" - The alignment of morals and values mentioned in the paper suggests possible inspiration from this source.\n7. \"Moral stories: Situated reasoning about norms, intents, actions and their consequences\" - The focus on moral stories and situated reasoning aligns closely with the paper's topic.\n\nThe JSON format of the result is:\n\n{\"b0\": 1, \"b5\": 0.9, \"b22\": 0.9, \"b14\": 0.8, \"b16\": 0.8, \"b19\": 0.7, \"b6\": 0.7}", "Based on the text provided, the source papers that likely inspired the author's work are:\n\n1. \"A corpus for understanding and generating moral stories\" (b6)\n2. \"Moral Stories: Situated Reasoning about Norms, Intentions, Actions, and their Consequences\" (b6)\n3. \"STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation\" (b0)\n4. \"Story Ending Generation with Incremental Encoding and Commonsense Knowledge\" (b16)\n5. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" (b5)\n6. \"A Diversity-Promoting Objective Function for Neural Conversation Models\" (b26)\n7. \"Hierarchical Neural Story Generation\" (b7)\n\nThese papers were selected based on the key phrases and direct indications of inspiration in the text provided, as well as their relevance to the main contributions of the paper. \n\nThe confidence scores for each paper would need to be calculated based on the strength of these indications and their relevance to the paper's main contributions. For example, if a paper is directly mentioned as an inspiration and is highly relevant to the paper's main contributions, it would receive a high confidence score. If a paper is indirectly indicated or less relevant, it would receive a lower confidence score. \n\nHere is the result in json format:\n{\n\"b6\": 0.9,\n\"b0\": 0.8,\n\"b16\": 0.7,\n\"b5\": 0.6,\n\"b26\": 0.5,\n\"b7\": 0.4\n}", "Based on the text, the source papers that inspired the given paper the most are likely:\n\n1. \"A corpus for understanding and generating moral stories\"\n2. \"STORIUM a dataset and evaluation platform for machine in the loop story generation\"\n3. \"Latent Dirichlet Allocation\"\n4. \"Reading Wikipedia to answer open-domain questions\"\n5. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"\n6. \"Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences\"\n7. \"Hierarchical Neural Story Generation\"\n8. \"Towards Machine Ethics and Norms\"\n\nThese papers are directly linked to the main challenges and novel methods proposed in the given paper. They are associated with the development of the STORAL dataset, the retrieval-augmented algorithm, and the main tasks of understanding and generating moral stories.\n\nIn JSON format:\n\n{\n\"b0\": 0.9,\n\"b1\": 0.8,\n\"b3\": 0.8,\n\"b5\": 0.9,\n\"b6\": 1.0,\n\"b7\": 0.9,\n\"b22\": 0.8\n}", "Based on the provided text and titles of the references. The source papers that most likely inspired the given paper are:\n\n1. \"a corpus for understanding and generating moral stories\" (None)\n2. \"latent dirichlet allocation\" (b1)\n3. \"moral stories situated reasoning about norms intents actions and their consequences\" (b6)\n4. \"bert pre training of deep bidirectional transformers for language understanding\" (b5)\n5. \"hierarchical neural story generation\" (b7)\n6. \"bertgcn transductive text classification by combining gcn and bert\" (b28)\n\nThis is determined by the methods used and concepts discussed in the paper, which are most directly related to these source papers. The paper uses a retrieval-augmented algorithm for understanding and generating moral stories, which links to the first source paper. The concept of latent dirichlet allocation is used for topic modeling, which is directly related to the second source paper. The idea of understanding morals through stories is discussed, which is related to the third source paper. Additionally, the paper employs BERT, a pre-training method for natural language understanding, which is linked to the fourth source paper. The concept of hierarchical story generation is discussed, which is related to the fifth source paper. The paper also uses BERTGCN, a text classification model that combines GCN and BERT, which is linked to the sixth source paper.\n\nThe confidence scores for each source paper are estimated as follows:\n\n- \"a corpus for understanding and generating moral stories\": 0.9\n- \"latent dirichlet allocation\": 0.85\n- \"moral stories situated reasoning about norms intents actions and their consequences\": 0.8\n- \"bert pre training of deep bidirectional transformers for language understanding\": 0.8\n- \"hierarchical neural story generation\": 0.75\n- \"bertgcn transductive text classification by combining gcn and bert\": 0.75\n\nThese scores are based on the relevance of the source paper to the methods and concepts discussed in the paper.\n\nThis information can be returned in the requested JSON format as follows:\n\n{\n    \"None\": 0.9,\n    \"b1\": 0.85,\n    \"b6\": 0.8,\n    \"b5\": 0.8,\n    \"b7\": 0.75,\n    \"b28\": 0.75\n}", "Based on the provided text and the titles of the references, here is a list of source papers that may have inspired the given paper:\n\n1. \"a corpus for understanding and generating moral stories\" - This paper appears to be the foundation of the study as it is the same topic, which is about understanding and generating moral stories [Confidence Score: 1].\n2. \"latent dirichlet allocation\" - The given paper uses LDA for topic modeling of morals [Confidence Score: 0.9].\n3. \"hierarchical neural story generation\" - The paper mentions the concept of story generation which may be inspired by this source [Confidence Score: 0.8].\n4. \"moral stories situated reasoning about norms intents actions and their consequences\" - This source paper's title suggests it's about moral stories which is the focus of the given paper [Confidence Score: 0.9].\n5. \"bert pre-training of deep bidirectional transformers for language understanding\" - The proposed method uses BERT, a model that is often used in NLP tasks [Confidence Score: 0.75].\n6. \"story ending generation with incremental encoding and commonsense knowledge\" - The story generation task in the given paper might be inspired by this source [Confidence Score: 0.8].\n7. \"reading wikipedia to answer open-domain questions\" - The proposed retrieval-augmented algorithm might be inspired by this paper [Confidence Score: 0.75].\n8. \"morality classification in natural language text\" - This paper might be a source as it relates to the classification of morals, which is a task in the given paper [Confidence Score: 0.8].\n\nJSON format:\n{\n\"1\": 1,\n\"2\": 0.9,\n\"3\": 0.8,\n\"4\": 0.9,\n\"5\": 0.75,\n\"6\": 0.8,\n\"7\": 0.75,\n\"8\": 0.8\n}"], "628304515aee126c0f6f0fed": ["{\n  \"b14\": 0.9,\n  \"b50\": 0.8,\n  \"b3\": 0.7,\n  \"b31\": 0.7,\n  \"b13\": 0.6\n}", "{\n    \"b14\": 0.9,\n    \"b50\": 0.85,\n    \"b37\": 0.8,\n    \"b12\": 0.75\n}", "{\n  \"b14\": 0.9,\n  \"b50\": 0.9,\n  \"b37\": 0.8,\n  \"b12\": 0.7\n}", "```json\n{\n  \"b14\": 0.9,\n  \"b50\": 0.8,\n  \"b37\": 0.75\n}\n```", "{\n  \"b14\": 0.9,\n  \"b50\": 0.8,\n  \"b37\": 0.8\n}"], "62c28ae55aee126c0f8a1954": ["Based on the text and the author notes, the source papers that most likely inspired this paper are:\n\n1. \"learning from counterfactual links for link prediction\" - This is the title of the paper itself, but the same phrase is mentioned multiple times throughout the text, indicating that the concept of learning from counterfactual links is a central contribution of the paper.\n2. \"causal inference using potential outcomes design modeling decisions\" [b53] - This paper is directly referenced in the context of discussing causal inference methods, which is a key methodology used in the given paper.\n3. \"counterfactual representation learning with balancing weights\" [b3] - This paper is referenced in the context of discussing counterfactual representation learning, which is another key methodology used in the paper.\n4. \"estimating individual treatment effect generalization bounds and algorithms\" [b54]- This paper is referenced when discussing the estimation of individual treatment effects, which is a key part of the proposed method in the paper.\n5. \"counterfactuals and causal inference\" [b42] - This paper is directly referenced in the context of discussing counterfactual questions, which is a key aspect of the proposed method in the paper.\n\nReference papers in JSON format:\n\n{\n\"b3\": 0.9,\n\"b42\": 0.9,\n\"b53\": 0.9,\n\"b54\": 0.9\n}", "Based on the given text and the additional information provided, it appears that the source papers which inspired the given paper most directly are:\n\n1) \"Modeling co-evolution of attributed and structural information in graph sequence\" (b64)\n2) \"Revisiting semi-supervised learning with graph embeddings\" (b75)\n3) \"Graph contrastive learning with augmentations\" (b78)\n4) \"Counterfactual data augmentation using locally factored dynamics\" (b48)\n5) \"Learning representations for counterfactual inference\" (b24)\n\nThe aforementioned papers are identified based on the text content of the academic paper, and the specific mentions and context of these references in the paper. The author has mentioned the key phrases \"counterfactual link\", \"graph learning model\", \"graph neural networks (GNNs)\", \"counterfactual inference\", \"graph data augmentation\" which are core contributions of these papers. Each of these papers appears to directly influence the methodology and approach of the given paper, particularly in terms of the algorithm proposed and the challenges outlined.\n\nPlease note that this is an inference made based on the provided text and additional information, and the actual inspiration might differ based on the author's personal research and reading.\n\nThe result in the requested JSON format is as following:\n\n{\n\"b64\": 1,\n\"b75\": 0.9,\n\"b78\": 0.9,\n\"b48\": 0.8,\n\"b24\": 0.8\n}", "Based on the provided information and the steps outlined in the instructions, the source papers that inspired the provided paper can be inferred as follows:\n\n1. \"Learning from counterfactual links for link prediction\" [None]: This is the title of the given paper, so it doesn't count as a source paper.\n\n2. \"Counterfactuals and causal inference\" [b42]: Mentioned in the text in relation to the key concept of counterfactual questions, which is a significant part of the paper's methodology.\n\n3. \"Estimating causal effects of treatments in randomized and nonrandomized studies\" [b52]: The paper uses the concept of treatments, which is a key concept in this source.\n\n4. \"Estimating individual treatment effect: Generalization bounds and algorithms\" [b54]: The paper uses individual treatment effect (ITE) as a metric, which is discussed in this source.\n\n5. \"A survey on network embedding\" [b11]: The paper uses the concept of network embedding, which is a key concept in this source.\n\n6. \"Graph convolutional networks\" [b27]: The paper uses graph convolutional networks in its design, which are discussed in this source.\n\n7. \"A comprehensive survey on graph neural networks\" [b71]: The paper uses graph neural networks in its design, which are discussed in this source.\n\n8. \"Learning representations for counterfactual inference\" [b24]: Mentioned in the text in relation to the key concept of learning representations for counterfactual inference.\n\nHere's the inferred result in JSON format:\n\n{\n  \"b42\": 0.9,\n  \"b52\": 0.8,\n  \"b54\": 0.7,\n  \"b11\": 0.8,\n  \"b27\": 0.9,\n  \"b71\": 0.9,\n  \"b24\": 0.7\n}", "Based on the text and the provided author's notes, below are the source papers that most likely inspired the work in the given paper:\n\n- The paper \"learning from counterfactual links for link prediction\" seems to be the main inspiration as it aligns with the main theme of the given paper which is about improving link prediction using counterfactual inference. Confidence score: 1\n\n- The paper \"Node2Vec: Scalable Feature Learning for Networks\" might have inspired the work as it has been mentioned in relation to graph embedding methods used for link prediction. Confidence score: 0.8\n\n- The paper \"Semi-Supervised Classification with Graph Convolutional Networks\" could have been a source of inspiration given its mention in relation to Graph Convolutional Network (GCN) used in the paper. Confidence score: 0.8\n\n- The paper \"Counterfactuals and Causal Inference\" is likely to have influenced the work, as it is mentioned in conjunction with the explanation of counterfactual questions, a key component of the paper's methodology. Confidence score: 0.9\n\n- The paper \"Estimating Individual Treatment Effect: Generalization Bounds and Algorithms\" could have provided a basis for the treatment effect estimation methods used in the paper. Confidence score: 0.7\n\n- The paper \"A Survey on Network Embedding\" might have inspired the work as it has been cited in relation to link prediction techniques. Confidence score: 0.7\n\n- The paper \"DrugBank: a major update to the DrugBank database for\" could have provided a basis for the use of drug-drug interaction network (OGB-DDI) as a benchmark dataset. Confidence score: 0.6\n\n- The paper \"A Comprehensive Survey on Graph Neural Networks\" is likely to have influenced the work, as it is mentioned in relation to the GNN methods used in the paper. Confidence score: 0.9\n\nHere is the result in the json format: \n\n{\"None\": 1, \"b17\": 0.8, \"b27\": 0.8, \"b42\": 0.9, \"b54\": 0.7, \"b11\": 0.7, \"b70\": 0.6, \"b71\": 0.9}", "Based on the paper text provided, the following references seem to have heavily influenced the main contributions of the paper:\n\n1. \"b10\": 'Line graph neural networks for link prediction' - This reference might have influenced the authors since the paper proposes a method for link prediction.\n2. \"b27\": 'Semi supervised classification with graph convolutional networks' - This reference is crucial as the authors are using GCN as their graph encoder in their proposed method.\n3. \"b90\": 'for fair comparison we set the size of node link representations to be of all methods cflp' - The authors compare their work with this paper's method.\n4. \"b71\": 'a comprehensive survey on graph neural networks' - This paper might have provided an understanding of the current state of graph neural networks to the authors.\n5. \"b18\": 'inductive representation learning on large graphs' - This paper might have inspired the authors since they are using a similar approach for graph representation learning.\n6. \"b24\": 'learning representations for counterfactual inference' - This paper is a central resource for the authors, as their main contribution is related to counterfactual inference.\n\nHere is the result in the requested format:\n\n{\n    \"b10\": 0.9,\n    \"b27\": 0.85,\n    \"b90\": 0.9,\n    \"b71\": 0.8,\n    \"b18\": 0.85,\n    \"b24\": 1\n}"], "620f0e735aee126c0fec46ff": ["Based on the text and the provided titles of the referenced papers, the source papers that most inspired this specific paper seem to be:\n\n1. \"A generalization of transformer networks to graphs\" [b3] - The paper discusses the process of applying Transformer models to graphs, which is directly related to the core subject of the given paper.\n2. \"Graphit: Encoding graph structure in transformers\" [b9] - The paper introduces a method to inject structural information derived from graphs into Transformers, which is also reflected in the given paper.\n3. \"Heterogeneous graph transformer for graph-to-sequence learning\" [b21] - The paper discusses a specific use case of applying Transformers to graph-structured data, which aligns with the general theme of the given paper.\n4. \"Masked transformer for neighbourhood aware click through rate prediction\" [b10] - The paper introduces the concept of masking attention matrices with different graph priors, which is also covered in the given paper.\n5. \"Gophormer: Ego graph transformer for node classification\" [b24] - The paper presents a model that improves the attention matrix computation based on graph information, a topic that is also explored in the given paper.\n6. \"Do transformers really perform bad for graph representation arxiv preprint\" [b22] - The paper investigates the performance of Transformers when applied to graph representation, a subject that is directly related to the focus of the given paper.\n\nThe JSON format would be:\n\n```json\n{\n  \"b3\": 0.85,\n  \"b9\": 0.8,\n  \"b21\": 0.75,\n  \"b10\": 0.7,\n  \"b24\": 0.7,\n  \"b22\": 0.65\n}\n```\n\nThe confidence scores are estimated based on the relevance of the topics covered in each source paper to the main contributions of the given paper.", "Based on the text and the methods provided, the source papers that seem to have most inspired the given paper are:\n\n1. 'b19': 'transformer dissection a unified understanding of transformer s attention via the lens of kernel'\n2. 'b7': 'semi supervised classification with graph convolutional networks'\n3. 'b22': 'do transformers really perform bad for graph representation arxiv preprint'\n4. 'b9': 'gr goire mialon dexiong chen margot selosse and julien mairal graphit encoding graph structure in transformers'\n\nThe confidence scores for these papers would be:\n\n{\"b19\": 0.8, \"b7\": 0.7, \"b22\": 0.6, \"b9\": 0.75}", "Based on the analysis of the text, the source papers that inspired the author most are:\n\n1. \"A generalization of Transformer networks to graphs\" [b3]: This paper is mentioned multiple times throughout the text, and is directly related to the paper's main contribution, which is to provide an overview of how Transformers are incorporated into graph-structured data. The paper also discusses how this source paper improves positional embedding from graphs.\n\n2. \"Graph Convolutional Networks\" [b7]: This paper is referenced multiple times throughout the text. It introduces the concept of GNNs, which plays a significant role in the author's discussion on how to incorporate graph information into the vanilla Transformer.\n\n3. \"Graph Residual Network for reviving deep GNNs from suspended animation\" [b23]: The paper is referenced in the context of discussing novel methods and approaches of incorporating GNNs into Transformer architecture. \n\n4. \"Graph Transformer for Graph to Sequence Learning\" [b1]: This paper is used by the author to discuss the method of improved positional embeddings from graphs. \n\n5. \"Do Transformers really perform bad for graph representation\" [b22]: This paper is mentioned multiple times and the author uses methods from this paper for experimental evaluations. \n\n6. \"Graph Transformer for graph to sequence learning\" [b21]: This paper is mentioned as a method of improved attention matrices from graphs.\n\n7. \"Masked Transformer for neighbourhood aware click through rate prediction\" [b10]: The author uses this paper to discuss how to improve attention matrices from graphs.\n\n8. \"Encoding Graph Structure in Transformers\" [b9]: This paper is used by the author to discuss how to incorporate GNNs as auxiliary modules in Transformer.\n\nHere is the result in json format:\n\n{ \"b3\": 0.85, \"b7\": 0.8, \"b23\": 0.75, \"b1\": 0.7, \"b22\": 0.65, \"b21\": 0.6, \"b10\": 0.55, \"b9\": 0.5 }", "Based on the provided information, the following source papers have significantly influenced the given paper:\n\n1. \"A generalization of transformer networks to graphs\" (b3): This paper is mentioned multiple times throughout the text and the authors have adopted some of its methods such as using Laplacian eigenvectors as Positional Embeddings (PE) and applying attention masking mechanism to Graph Neural Networks (GNN). \n\n2. \"Graphit: encoding graph structure in transformers\" (b9): The authors have borrowed the idea of using GNNs as auxiliary modules, and they have also referenced the improvement of attention matrices from this paper.\n\n3. \"Gophormer: ego graph transformer for node classification\" (b24): The authors have adopted the idea of proximity-enhanced multi-head attention (PMA) from this paper.\n\n4. \"Do transformers really perform bad for graph representation\" (b22): From this paper, the authors have adopted the concept of using degree embedding and spatial bias in attention matrices.\n\n5. \"Graph transformer for graph to sequence learning\" (b1): This paper is referred when the authors discuss the usage of learned positional encoding.\n\n6. \"Graph residual network for reviving deep gnns from suspended animation\" (b23): The authors have adopted the idea of using a graph residual term in each attention layer from this paper.\n\nBased on the criteria mentioned, the confidence score for each paper is estimated as follows:\n\n{\n\"b3\": 0.9,\n\"b9\": 0.85,\n\"b24\": 0.8,\n\"b22\": 0.8,\n\"b1\": 0.75,\n\"b23\": 0.75\n}\n", "Based on the text of the paper, the following references seem to be the most pertinent source papers that inspired the author:\n\n1. \"A generalization of transformer networks to graphs\" [b3]: The paper made direct reference to this work while discussing how transformers and its variants have been successfully applied in modeling graphs. It also mentions how this work used Laplacian eigenvectors as positional encoding in Graph Transformer.\n\n2. \"Graph Convolutional Kernel Network (GCKN)\" [b9]: This work was mentioned in relation to the use of GNN as an auxiliary module in transformer architecture. The paper discusses how GraphiT, a model presented in this work, adopts one GCKN layer to produce a structure-aware representation from original features.\n\n3. \"Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel\" [b19]: This paper was referenced in the introduction and in the section discussing transformer architecture in the context of graph structure.\n\n4. \"Graph-BERT\" [b23]: This work was mentioned multiple times in the paper, especially in the discussion about different ways to incorporate graph information into the transformer model. \n\n5. \"Equivariant transformers for neural network-based molecular potentials\" [b18]: This work was referenced in the context of the application of transformer variants in modeling graphs.\n\n6. \"Heterogeneous Graph Transformer for graph-to-sequence learning\" [b21]: This work is mentioned in the section discussing the use of graph neural networks as auxiliary modules in transformer architecture. \n\n7. \"Gophormer: Ego Graph Transformer for Node Classification\" [b24]: The paper refers to this source when discussing improved attention matrices from graphs.\n\nBased on the criteria mentioned, these papers seem to have heavily influenced the main contributions of the reviewed paper. \n\nSo, the result in json format would be:\n\n{\n\"b3\": 0.9,\n\"b9\": 0.85,\n\"b19\": 0.9,\n\"b23\": 0.95,\n\"b18\": 0.85,\n\"b21\": 0.85,\n\"b24\": 0.9\n}"], "628603bc970707000cbf14a8": ["{\n    \"b30\": 0.95,\n    \"b17\": 0.9,\n    \"b14\": 0.85\n}", "{\n    \"b30\": 0.95,\n    \"b17\": 0.9,\n    \"b14\": 0.8\n}", "{\n  \"b30\": 0.95,\n  \"b17\": 0.90,\n  \"b14\": 0.85\n}", "{\n    \"b30\": 1.0,\n    \"b17\": 0.9,\n    \"b14\": 0.8\n}", "{\n  \"b30\": 0.95,\n  \"b17\": 0.9,\n  \"b14\": 0.85\n}"], "628ef0495aee126c0f82d966": ["Based on the text and the titles of the references, the source papers that inspire the author most are likely to be:\n\n1. b12: \"A generalization of Transformer networks to graphs\"\n2. b33: \"Rethinking graph transformers with spectral attention\"\n3. b10: \"Rethinking attention with performers\"\n4. b61: \"Big Bird transformers for longer sequences\"\n5. b58: \"Do transformers really perform badly for graph representation\"\n6. b40: \"Graphit: Encoding graph structure in transformers\"\n7. b14: \"Graph neural networks with learnable structural and positional representations\"\n\nHere is the result in JSON format:\n\n{\n  \"b12\": 0.85,\n  \"b33\": 0.8,\n  \"b10\": 0.75,\n  \"b61\": 0.75,\n  \"b58\": 0.7,\n  \"b40\": 0.7,\n  \"b14\": 0.65\n}\n\nThe confidence scores are estimated based on the frequency and context in which the papers are referenced in the text.", "Based on the text of the paper, the most pertinent source papers seem to be:\n\n- 'b12': \"a generalization of transformer networks to graphs\" \n- 'b33': \"rethinking graph transformers with spectral attention\"\n- 'b58': \"do transformers really perform badly for graph representation\"\n- 'b40': \"graphit encoding graph structure in transformers\" \n- 'b10': \"rethinking attention with performers\"\n- 'b61': \"big bird transformers for longer sequences\"\n- 'b60': \"design space for graph neural networks\"\n\nThese references are mentioned in direct relation to the methodologies and approaches the authors have used in their paper, specifically in the creation and application of their Graph Transformer. \n\nThe confidence scores for each reference might look like this:\n\n{ \"b12\": 0.9, \"b33\": 0.9, \"b58\": 0.85, \"b40\": 0.85, \"b10\": 0.8, \"b61\": 0.8, \"b60\": 0.75 }\n\nThese scores are based on the importance and frequency of each reference in the paper, with higher scores indicating a greater influence on the paper's main contributions.", "Based on the text of the paper and the titles of the references, the source papers that inspired this current paper are:\n\n1) 'A generalization of transformer networks to graphs' [b12]: This paper is referenced multiple times in the text, showing that it was an important source of inspiration. It is mentioned in the context of node identifiability, positional encoding schemes, and the introduction of graph Transformers.\n\n2) 'Rethinking graph transformers with spectral attention' [b33]: This paper is also referenced multiple times, indicating its influence on the current work. It is noted for its role in the development of invariant aggregation of Laplacian's eigenvectors for positional encoding.\n\n3) 'Do transformers really perform badly for graph representation?' [b58]: This paper is mentioned in relation to the use of pair-wise graph distances to define relative positional encodings, which has been a successful approach in large molecular benchmarks.\n\n4) 'Graphit: encoding graph structure in transformers' [b40]: This paper is mentioned as it used relative positional encoding derived from diffusion kernels to modulate attention between nodes.\n\n5) 'Re-thinking attention with performers' [b10]: This paper is mentioned multiple times, indicating its influence on the current work. It is noted for its role in linear global attention mechanisms.\n\n6) 'Big Bird: transformers for longer sequences' [b61]: This paper is also referenced multiple times, indicating its influence on the current work. It is noted for its role in linear global attention mechanisms.\n\n7) 'A better understanding and organization of the PE and SE will aid in building of a more modular architecture and in guiding of the future research' [None]: The title of the paper indicates that it is based on a new understanding and organization of positional and structural encodings (PE and SE), which are essential for graph Transformers.\n\nThe confidence score for each of these papers would be 1, as they are directly referenced in the text and explicitly linked to the key contributions of the current paper.\n\n{\n\"b12\": 1,\n\"b33\": 1,\n\"b58\": 1,\n\"b40\": 1,\n\"b10\": 1,\n\"b61\": 1,\n\"None\": 1\n}", "Based on the text of the paper, the following reference papers seem to have inspired the author the most:\n\n1. 'b12': 'a generalization of transformer networks to graphs' - The author refers to this paper when discussing the introduction of fully-connected graph transformer models and the utilisation of eigenvectors of the graph Laplacian as the node positional encoding (PE). \n\n2. 'b33': 'rethinking graph transformers with spectral attention' - This paper is mentioned multiple times, particularly in relation to the implementation of an invariant aggregation of Laplacian's eigenvectors for the PE and the incorporation of edge features in fully-connected graph transformers.\n\n3. 'b10': 'rethinking attention with performers' - The author adopts the Performer as an exemplary linear attention mechanism. The paper also mentions that Performer scales to graphs with several thousand nodes and demonstrates competitive results even without explicit edge features within the attention module.\n\n4. 'b58': 'do transformers really perform badly for graph representation' - This paper is referenced in the context of using pair-wise graph distances to define relative positional encodings, which has been successful on large molecular benchmarks.\n\n5. 'b14': 'graph neural networks with learnable structural and positional representations' - This paper is mentioned when discussing the incorporation of positional and structural encodings into the proposed graph transformer.\n\n6. 'b61': 'big bird transformers for longer sequences' - BigBird is also adopted as an exemplary linear attention mechanism, similar to the Performer.\n\n7. 'b60': 'design space for graph neural networks' - The author mentions the success of GraphGym, which was presented in this paper, and implements the blueprint within a modular and performant GRAPHGPS package.\n\nIn JSON format, the confidence scores for these references would be:\n\n{\n\"b12\": 0.9,\n\"b33\": 0.9,\n\"b10\": 0.8,\n\"b58\": 0.8,\n\"b14\": 0.7,\n\"b61\": 0.8,\n\"b60\": 0.7\n}", "Based on the text of the paper and the titles of the references, the most pertinent source papers that inspired the authors seem to be:\n\n- 'A generalization of transformer networks to graphs' (b12): The authors refer to this paper multiple times, discussing how their work builds on it. It seems to have informed their design of graph transformers.\n- 'Rethinking attention with performers' (b10): This paper is cited when the authors discuss their adoption of Performer and BigBird as exemplary linear attention mechanisms, indicating it directly influenced their approach.\n- 'Rethinking graph transformers with spectral attention' (b33): The authors reference this paper when discussing the improvements in their work, which implies it had a significant influence on their research.\n- 'Do transformers really perform badly for graph representation?' (b58): This paper is cited as a source of inspiration for using pair-wise graph distances to define relative positional encodings.\n- 'Design space for graph neural networks' (b60): The authors reference this paper when discussing their implementation of the blueprint within a modular and performant GRAPHGPS package, showing its influence on their work.\n\nThe JSON format of the result with confidence score is:\n\n{\n\"b12\": 0.9,\n\"b10\": 0.8,\n\"b33\": 0.85,\n\"b58\": 0.85,\n\"b60\": 0.8\n}\n\nThe confidence scores are subjective and based on the frequency and context of the citations in the paper."], "628d9e795aee126c0f979247": ["Based on the text of the paper and the titles of the references, the source papers that inspired the author the most are likely:\n\n- \"Billion scale pretraining with vision transformers for multi task visual representations\" [b0]: The author mentions using embeddings from this paper for generating candidates to upstream ranking systems. This indicates that the author took inspiration from this paper for their own embedding representations.\n\n- \"Deep neural networks for YouTube recommendations\" [b3]: The author mentions this paper when discussing earlier work that extends to multi-modal features, suggesting that this paper was an inspiration for their own work in multi-modal feature development.\n\n- \"BERT: Pre-training of deep bidirectional transformers for language understanding\" [b4]: The author mentions being inspired by the success of Transformer models like BERT for modality fusion, indicating that this paper was a key inspiration for their work.\n\n- \"Semantic product search\" [b20]: The author contrasts their work with other works that target a single vertical application, such as product search, which is the focus of this paper. This suggests that this paper was an important reference point for the author's work.\n\n- \"Graph convolutional neural networks for web scale recommender systems\" [b32]: The author refers to this work multiple times throughout the paper, indicating that it was a significant inspiration for their own work.\n\n- \"Learning a unified embedding for visual search at Pinterest\" [b33]: The author refers to this work as an example of using multi-task learning to optimize for a single embedding, indicating that this paper was an influential source.\n\n- \"Hash embeddings for efficient word representations\" [b25]: The author mentions applying the hash embedding trick from this paper, indicating that this paper was a direct inspiration for their work.\n\nThe confidence score for each of these references would depend on how closely the author's work follows the methods and approaches outlined in these papers. Without more detailed information about the author's work, it's difficult to assign exact confidence scores. However, given the author's explicit references to these papers, it is reasonable to assume a high confidence score (close to 1) for each. \n\nThe final result in the json format: \n{\"b0\": 0.9, \"b3\": 0.8, \"b4\": 0.9, \"b20\": 0.8, \"b32\": 0.9, \"b33\": 0.9, \"b25\": 0.8}", "Given the provided text and the notes from the author, the relevant source papers are:\n\n1. \"Billion scale pretraining with vision transformers for multi task visual representations\" [b0]: This paper is cited multiple times and it's clear that the approach of multi-task learning and vision transformers have influenced the researched paper.\n\n2. \"Uniter: learning universal image-text representations\" [b1]: This work is cited in the context of the transformer model used for modality fusion, which is a key aspect of the paper.\n\n3. \"Deep neural networks for youtube recommendations\" [b3]: The paper cites this work in the context of multi-modal feature aggregation, which is an important part of their approach.\n\n4. \"Bert: pre-training of deep bidirectional transformers for language understanding\" [b4]: This is likely a source paper as the paper mentions its inspiration from the success of Transformer models like BERT.\n\n5. \"Graph-based multilingual product retrieval in e-commerce search\" [b17]: The paper cites this when discussing embedding representations for e-commerce, indicating that it influenced their approach.\n\n6. \"Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs\" [b18]: This paper is mentioned with respect to their use of approximate nearest neighbor (ANN) search techniques, which is a major component of their methodology.\n\n7. \"Graph convolutional neural networks for web-scale recommender systems\" [b32]: This paper is cited multiple times throughout the text, suggesting it had a significant influence on the methodology.\n\nThe confidence scores are as follows:\n\n{\n\"b0\": 0.9,\n\"b1\": 0.85,\n\"b3\": 0.85,\n\"b4\": 0.85,\n\"b17\": 0.8,\n\"b18\": 0.85,\n\"b32\": 0.9\n}", "Based on the given text of the academic paper, the source papers that most inspired the author could be inferred as follows:\n\n- 'b0': \"billion scale pretraining with vision transformers for multi task visual representations\" is referenced multiple times in the context of the author's work on developing the ItemSage embeddings. It is directly related to the paper's topic of creating embedding representations for product recommendations. Confidence score: 0.9\n\n- 'b32': \"graph convolutional neural networks for web scale recommender systems\" is also referenced several times in the text, and it seems to have a strong influence on the author's work, particularly in the development of the PinSage model for generating image embeddings. Confidence score: 0.8\n\n- 'b20': \"semantic product search\" appears to have inspired the author's work on leveraging multi-faceted product information for shopping recommendations. Confidence score: 0.7\n\n- 'b33': \"learning a unified embedding for visual search at pinterest\" is referenced in relation to the author's work on learning product representations for recommendation systems. Confidence score: 0.7\n\n- 'b1': \"uniter learning universal image text representations\" is referenced in the context of the author's work on the transformer model for modality fusion in learning product representations. Confidence score: 0.6\n\n- 'b3': \"deep neural networks for YouTube recommendations\" is mentioned as an example of previous work that extends to multi-modal features, which the author also does in their work. Confidence score: 0.6\n\n- 'b4': \"bert pre training of deep bidirectional transformers for language understanding\" is mentioned in the context of the author's use of the transformer encoder as the basic building block for learning product embeddings. Confidence score: 0.6\n\n- 'b27': \"progressive layered extraction ple a novel multi task learning mtl model for personalized recommendations\" is mentioned in relation to the author's work on multi-task learning in recommendation systems. Confidence score: 0.5\n\nThe results in the JSON format would be:\n\n{\n\"b0\": 0.9,\n\"b32\": 0.8,\n\"b20\": 0.7,\n\"b33\": 0.7,\n\"b1\": 0.6,\n\"b3\": 0.6,\n\"b4\": 0.6,\n\"b27\": 0.5\n}", "Based on the given text, it appears that the paper is primarily focused on the development of the 'ItemSage' system for product recommendation, utilizing a multi-modal approach that combines image and text information, as well as a multi-task learning regime.\n\nThe following references seems to be the most pertinent sources that inspired the paper:\n\n- 'b0': 'billion scale pretraining with vision transformers for multi task visual representations'. (Confidence Score: 0.9)\n- 'b1': 'uniter learning universal image text representations'. (Confidence Score: 0.8)\n- 'b32': 'graph convolutional neural networks for web scale recommender systems'. (Confidence Score: 0.9)\n- 'b20': 'semantic product search'. (Confidence Score: 0.9)\n- 'b3': 'deep neural networks for youtube recommendations'. (Confidence Score: 0.8)\n- 'b22': 'an overview of multi task learning in deep neural networks'. (Confidence Score: 0.8)\n\nThese references are selected based on their methodological similarity with the approach described in the text, as well as their direct relevance to the novel contributions of the paper. It seems that these sources heavily influenced the development of the 'ItemSage' system and the overall approach of the paper.\n\nThe JSON format of the result:\n{\n\"b0\": 0.9,\n\"b1\": 0.8,\n\"b32\": 0.9,\n\"b20\": 0.9,\n\"b3\": 0.8,\n\"b22\": 0.8\n}", "{\n\"b0\": 0.9,\n\"b1\": 0.8,\n\"b3\": 0.7,\n\"b4\": 0.85,\n\"b20\": 0.8,\n\"b32\": 0.9,\n\"b33\": 0.8\n}"], "62bd48b80cd9e8000cfc9dc5": ["Based on the provided text and the author notes, the paper aims to design a search space at the micro-meso-macro level for more efficient Transformer-like architectures. The paper introduces novel contributions such as a micro-meso-macro search space, Norm-Op-Norm-Act and Hamburger structure, hybrid sampling method, and a One-Shot NAS strategy. \n\nThe paper seems to be heavily influenced by previous works that have utilized similar methodologies or have been directly mentioned within the text. From these, the source papers that seem to have directly inspired the paper are:\n\n1. b6 (\"AutoFormer: Searching Transformers for Visual Recognition\"): This paper is mentioned multiple times and seems to be a significant influence, especially regarding the design of the block of Transformer and the idea of Neural Architecture Search (NAS).\n\n2. b12 (\"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"): Influential in the application of Transformers in the field of Computer Vision.\n\n3. b15 (\"Single Path One-Shot Neural Architecture Search with Uniform Sampling\"): Seems to have inspired the One-Shot NAS strategy proposed in the paper.\n\n4. b54 (\"MetaFormer: Is Actually What You Need for Vision\"): The use of average pooling in the MetaFormer seems to have influenced the design of the search space in this paper.\n\n5. b44 (\"Training Data-Efficient Image Transformers & Distillation through Attention\"): The paper seems to be influenced by the teacher-student distillation strategy proposed in DeiT.\n\n6. b29 (\"Swin-Transformer: Hierarchical Vision Transformer using Shifted Windows\"): Influential in the application of Transformer in different vision tasks such as detection and segmentation.\n\n7. b28 (\"DARTS: Differentiable Architecture Search\"): Influential in the design of the search space and the adoption of architecture parameters to select operations.\n\n8. b53 (\"BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models\"): This paper seems to have influenced the sandwich sampling method in the proposed paper.\n\n```\n{ \n\"b6\": 0.9,\n\"b12\": 0.85,\n\"b15\": 0.85,\n\"b54\": 0.8,\n\"b44\": 0.75,\n\"b29\": 0.75,\n\"b28\": 0.75,\n\"b53\": 0.75\n}\n```", "Based on the paper's text, the following source papers seem to have inspired the authors the most:\n\n1. b29: 'Swin transformer hierarchical vision transformer using shifted windows' - The authors have adopted the multi-stage strategy from this paper in their macro search space design.\n2. b12: 'An image is worth x words transformers for image recognition at scale' - This paper seems to be one of the landmark works that brought transformers into the CV field, heavily influencing the authors.\n3. b44: 'Training data efficient image transformers distillation through attention' - The authors have adopted the token dimension strategy from this paper.\n4. b6: 'Autoformer searching transformers for visual recognition' - The authors have taken inspiration from the block design of Transformer in AutoFormer.\n5. b54: 'Metaformer is actually what you need for vision' - The authors have adopted the idea of using average pooling operation from Metaformer.\n6. b3: 'ProxylessNAS direct neural architecture search on target task and hardware' - The authors have borrowed the block design from MobileNet for their search spaces, which was also used in ProxylessNAS.\n7. b15: 'Single path one shot neural architecture search with uniform sampling' - The authors have used the single-path methods from this paper in their One-Shot NAS strategy.\n8. b34: 'Efficient neural architecture search via parameters sharing' - The authors have borrowed the weight sharing strategy from this paper for their One-Shot NAS strategy.\n\nResult in json:\n{\n  \"b29\": 0.9,\n  \"b12\": 0.9,\n  \"b44\": 0.85,\n  \"b6\": 0.85,\n  \"b54\": 0.85,\n  \"b3\": 0.8,\n  \"b15\": 0.8,\n  \"b34\": 0.8\n}", "Based on the given text, the papers that have most influenced this work are:\n\n- \"An image is worth x words: Transformers for image recognition at scale\" [b12]\n- \"AutoFormer: Searching Transformers for Visual Recognition\" [b6]\n- \"NASViT: Neural Architecture Search for Efficient VisIon Transformers with Gradient Conflict Aware Supernet Training\" [b8]\n- \"DARTS: Differentiable Architecture Search\" [b28]\n- \"Single Path One-Shot: Neural Architecture Search with Uniform Sampling\" [b15]\n- \"MetaFormer: Is Actually What You Need for Vision\" [b54]\n- \"Training data-efficient image transformers distillation through attention\" [b44]\n- \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\" [b29]\n\nThe paper seems to be motivated and inspired by the above papers in its development of a new NAS process that combines searching at different levels of complexity, namely operator, block/cell and stage. This is indicated by phrases such as \"inspired us to search varied normalization and activation functions\" which is directed at [b12] and [b44], \"Following Transformer-like architectures\" directed at [b6] and [b54], and the use of One-Shot NAS which is mentioned in [b15].\n\nHere is the result in json format:\n{\n\"b12\": 0.9,\n\"b6\": 0.85,\n\"b8\": 0.8,\n\"b28\": 0.85,\n\"b15\": 0.9,\n\"b54\": 0.85,\n\"b44\": 0.8,\n\"b29\": 0.75\n}", "Based on the given information, the following references are identified as source papers:\n\n1. \"An Image is worth x words: Transformers for image recognition at scale\" [b12]: The paper mentions Vision Transformer (ViT) as one of the landmark works. The algorithm proposed by the author for architecture search seems to take inspiration from ViT.\n\n2. \"AutoFormer: Searching Transformers for Visual Recognition\" [b6]: The paper mentions Neural Architecture Search (NAS) and specifically mentions AutoFormer. It appears that the author took inspiration from AutoFormer\u2019s approach to NAS.\n\n3. \"DeiT: Training Data-Efficient Image Transformers & Distillation through Attention\" [b44]: DeiT is mentioned as another landmark work. The author seems to have drawn inspiration from DeiT\u2019s data-efficient approach.\n\n4. \"DARTS: Differentiable Architecture Search\" [b28]: The author mentions DARTS while discussing the design of the search space, indicating that DARTS likely influenced the development of their algorithm.\n\n5. \"Single Path One-Shot Neural Architecture Search with Uniform Sampling\" [b15]: The author mentions 'One-Shot NAS' and 'Single-Path methods' which suggest that Single Path One-Shot Neural Architecture Search has influenced the methodology employed in this work.\n\n6. \"NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict Aware Supernet Training\" [b8]: The author references NASViT while discussing works that focus on the design of the block or stage granularity. It is likely that NASViT\u2019s approach to searching ViTs influenced the author\u2019s method.\n\nBased on the provided information, the JSON format of the source papers and their confidence scores (assuming equal confidence for each) would be:\n\n{\n    \"b12\": 1,\n    \"b6\": 1,\n    \"b44\": 1,\n    \"b28\": 1,\n    \"b15\": 1,\n    \"b8\": 1\n}", "Based on the provided text, the following references are likely to be the source papers that most inspired the paper's main contributions:\n\n1. \"AutoFormer: Searching Transformers for Visual Recognition\" [b6]: The paper directly cites AutoFormer when discussing the combination of Transformer and Neural Architecture Search (NAS), indicating that it could have been a primary source of inspiration. \n\n2. \"Neural Architecture Search for Global and Local Image Transformer\" [b5]: The paper cites GLiT when discussing its own strategy of searching ViT from both global and local levels, suggesting a direct influence on the paper's methodology.\n\n3. \"Searching for Efficient Multi-Stage Vision Transformers\" [b26]: The paper cites ViT-ResNAS when discussing the design of the block or stage granularity in its own method, suggesting a strong influence on the paper's approach.\n\n4. \"Single Path One-Shot Neural Architecture Search with Uniform Sampling\" [b15]: The paper cites Single-Path methods when discussing its own methods to reduce the resource consumption, indicating that Single-Path methods have influenced the paper's methodology.\n\n5. \"An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale\" [b12]: This paper is cited when discussing the emergence of landmark works in Transformers, suggesting it could have inspired the paper's focus on Transformer-like architectures.\n\n6. \"DARTS: Differentiable Architecture Search\" [b28]: This paper is cited when discussing the design of search space in NAS, suggesting it could have influenced the paper's design of its search space.\n\nReturned result in JSON format: \n\n{ \n   \"b6\": 0.9,\n   \"b5\": 0.8,\n   \"b26\": 0.8,\n   \"b15\": 0.7,\n   \"b12\": 0.7,\n   \"b28\": 0.7\n}\n\nThe confidence score is a rough estimation of how much each source paper contributed to the main contributions of the given paper. It is assessed based on the frequency and context of the citations in the text. The score is between 0 and 1, with 1 indicating the highest influence."], "62bd48b2cb97d2000c50c6a6": ["Based on the given academic paper, the source papers that inspired the author the most are:\n\n1. \"DARTS: Differentiable Architecture Search\" (Reference: 'b28')\n2. \"Rethinking Architecture Selection in Differentiable NAS\" (Reference: 'b37')\n3. \"ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware\" (Reference: 'b3')\n4. \"Attention Enhances Synaptic Efficacy and the Signal to Noise Ratio in Neural Circuits\" (Reference: 'b2')\n5. \"A Convolutional Neural Network Pruning Method based on Attention Mechanism\" (Reference: 'b39')\n\nThe reasons are:\n\n\"DARTS: Differentiable Architecture Search\" (Reference: 'b28') is mentioned as the paper's primary motivation. The author states that they are concerned with the issues of DARTS and propose an alternative paradigm.\n\n\"Rethinking Architecture Selection in Differentiable NAS\" (Reference: 'b37') is mentioned as the paper that inspired the author's novel paradigm. The author indicates that they learned from this paper that the operation associated with the largest magnitude of architecture parameters does not necessarily result in the highest validation accuracy after discretization.\n\n\"ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware\" (Reference: 'b3') is mentioned as the paper that the author used as a basis to propose their idea of using the attention mechanism to search for the specific choice block at each layer of the target network.\n\n\"Attention Enhances Synaptic Efficacy and the Signal to Noise Ratio in Neural Circuits\" (Reference: 'b2') is mentioned as the paper that inspired the author's idea of using the attention mechanism to guide the neural architecture search.\n\n\"A Convolutional Neural Network Pruning Method based on Attention Mechanism\" (Reference: 'b39') is mentioned as a paper that applied the attention mechanism to prune channels with smaller attention weights. The author states that they were inspired by this fact to reflect the influence of candidate operations on the super-network with the aim of guiding the neural architecture search.\n\nThe confidence score for each reference can be calculated based on the number of times they are referenced and the context in which they are mentioned. The confidence score is a subjective measure and can vary based on interpretation.\n\nBased on the given academic paper, the result in the json format would be:\n\n{\n\"b28\": 0.9,\n\"b37\": 0.8,\n\"b3\": 0.8,\n\"b2\": 0.7,\n\"b39\": 0.7\n}", "The following source papers are inferred from the text:\n\n1. \"DARTS: Differentiable architecture search\" [b28]: The paper frequently mentions this work and its limitations, and the authors propose their work as an alternative. \n\n2. \"Stabilizing differentiable architecture search via perturbation based regularization\" [b6]: The paper mentions this work as a reference for stabilizing the optimization issue, which their work also addresses.\n\n3. \"SGAS: Sequential greedy architecture search\" [b22]: The paper refers to this work as a method of pruning redundant operations, which is also a key aspect of their proposed methodology.\n\n4. \"AttentionNAS: Spatiotemporal attention cell search for video classification\" [b38]: This work is referred to in the context of including the attention module in the search space, which is a key concept in their work.\n\n5. \"Rethinking architecture selection in differentiable NAS\" [b37]: This work is a key reference, as it points out that architecture parameters do not necessarily reflect operation strength, which is a key issue that the authors' work addresses.\n\n6. \"NASNet-A: Learning transferable architectures for scalable image recognition\" [b54]: This work is referenced when discussing the micro search space, which is a key concept in the authors' work.\n\n7. \"A model of saliency-based visual attention for rapid scene analysis\" [b16]: This work is referred to when discussing the attention mechanism, which is a key concept in the authors' work.\n\n8. \"AutoPruner: An end-to-end trainable filter pruning method for efficient deep model inference\" [b29]: This work is referenced when discussing the use of the attention mechanism to prune channels, which is also a key aspect of the authors' proposed methodology.\n\nHere is the json format:\n\n{\n\"b28\": 0.9,\n\"b6\": 0.8,\n\"b22\": 0.8,\n\"b38\": 0.85,\n\"b37\": 0.9,\n\"b54\": 0.85,\n\"b16\": 0.8,\n\"b29\": 0.8\n}", "Based on the text and the author notes, the key source papers seem to be:\n\n1. \"DARTS: Differentiable Architecture Search\" (b28): The paper frequently mentions DARTS and its limitations which motivated the author to propose an alternative paradigm for determining the importance of candidate operations. It is clear that the author's work is heavily influenced by DARTS, and they've used its methodology as a base to improve upon. \n\n2. \"Attention Enhances Synaptic Efficacy and the Signal to Noise Ratio in Neural Circuits\" (b2): This paper appears to be the source of the attention mechanism which is central to the author's proposed method. \n\n3. \"NASNet: Learning Transferable Architectures for Scalable Image Recognition\" (b54): This paper is mentioned in the context of the micro search space which is widely used in later works. It might have been an inspiration for the author to develop his own method.\n\n4. \"Single Path One-Shot Neural Architecture Search with Uniform Sampling\" (b13): This paper is referenced in the context of macro search methods and seems to be one of the previous works the author is building upon.\n\n5. \"Rethinking Architecture Selection in Differentiable NAS\" (b37): This paper is referenced multiple times, highlighting the limitations of existing methods and suggesting the need for a new paradigm, which the author proposes.\n\n6. \"Stabilizing Differentiable Architecture Search via Perturbation-Based Regularization\" (b6): This work is also mentioned quite frequently, indicating that it was influential in the development of the author's methodology.\n\nThe above list is based on the textual analysis, and the importance of these papers in inspiring the author's work.\n\nThe JSON format of the result would be:\n\n{\n\"b28\": 1,\n\"b2\": 0.9,\n\"b54\": 0.8,\n\"b13\": 0.8,\n\"b37\": 0.9,\n\"b6\": 0.8\n}", "Based on the text and the author's notes, the key phrases indicating the source papers that inspire this paper are \"DARTS [b28]\", \"ENAS [b32]\", \"attention mechanism\", \"MobileNet [b14]\", and \"attention weights\". The paper's novel contributions revolve around a new approach to Neural Architecture Search (NAS) that leverages attention mechanisms to indicate the importance of candidate operations, which is a departure from the traditional use of architecture parameters. \n\nHere are the possible source papers:\n\n1. \"DARTS [b28]\" - This paper is referred to multiple times as a source of inspiration. It outlines the original DARTS approach, its limitations, and its methods are used as a comparison for the new approach proposed in this paper. The paper is: \"DARTS: Differentiable Architecture Search\" [b28].\n\n2. \"ENAS [b32]\" - This paper is also referred to as inspiration. The paper is: \"Efficient Neural Architecture Search via Parameters Sharing\" [b32].\n\n3. \"attention mechanism\" - While there isn't a specific paper cited immediately after this phrase, the concept of attention mechanism fundamentally informs the new approach proposed in this paper. Therefore, papers like \"Attention Is All You Need\" might be relevant, but it's not listed in the references provided.\n\n4. \"MobileNet [b14]\" - The paper is: \"Efficient Convolutional Neural Networks for Mobile Vision Applications\" [b14]. The design of MobileNet was borrowed for the macro architecture in this paper.\n\n5. \"attention weights\" - Again, while no specific paper is cited immediately after this phrase, the concept fundamentally informs the new approach proposed in this paper. Papers like \"Channel Pruning via Automatic Structure Search\" might be relevant, but it's not listed in the references provided.\n\nTherefore, the most relevant source papers are b28, b32, and b14. However, without specific papers directly linked to the concepts of \"attention mechanism\" and \"attention weights\", it's hard to definitively include them in the list.\n\nHere's the result in the requested format:\n\n```json\n{\n  \"b28\": 1.0,\n  \"b32\": 0.8,\n  \"b14\": 0.7\n}\n```", "Based on the text and the titles of the references, the main challenges discussed in the paper are: the inability of DARTS's connection weights to reflect operator importance and the inefficiency of the evaluation process which requires sampling a path. This paper introduces a novel method based on attention map to determine operator importance and adds an attention map after each cell to indicate the importance of the operator. The evaluation process does not need to resample the subnetwork.\n\nFollowing the instructions, the given text and the author's notes, the most pertinent source papers seem to be:\n1. 'darts differentiable architecture search' [b28] \n2. 'neural architecture search with reinforcement learning' [b0]\n3. 'sgas sequential greedy architecture search' [b22]\n4. 'stabilizing differentiable architecture search via perturbation based regularization' [b6]\n5. 'rethinking architecture selection in differentiable nas' [b37]\n6. 'neural architecture search with random labels' [b50]\n7. 'learning transferable architectures for scalable image recognition' [b54]\n\nBelow is the result in JSON format:\n\n```\n{\n  \"b28\": 1.0,\n  \"b0\": 0.9,\n  \"b22\": 0.8,\n  \"b6\": 0.8,\n  \"b37\": 0.7,\n  \"b50\": 0.7,\n  \"b54\": 0.6\n}\n```\n\nThese confidence scores are subjective and could vary depending on the interpretation of the text. The paper by DARTS [b28] has the highest score as it is directly addressed and critiqued in the text, and the proposed solution is a direct response to the challenges presented by their approach. The other papers are also mentioned in the context of the novel contributions of this paper and their methods have likely inspired the author's work. The confidence scores are based on how strongly the paper's text indicates reliance or inspiration from each source."], "6253cbd75aee126c0f191b95": ["Based on the analysis of the text and the author's notes, the papers that appear to have influenced this research the most are:\n1. \"Highly accurate protein structure prediction with AlphaFold2 [b30]\" as it is the method used to predict structures for the sequences in large databases.\n2. \"Learning from protein structure with geometric vector perceptrons [b28]\" is mentioned as the method to extract geometric features, followed by a generic autoregressive encoder-decoder Transformer.\n3. \"Generative models for graph-based protein design [b24]\" is referenced as the paper that introduced the approach of treating inverse folding as a sequence-to-sequence problem.\n4. \"Understanding back-translation at scale [b15]\" is cited as the back-translation method used to improve the model.\n5. \"GVP-GNN from [b28]\" is referred to as the model currently state-of-the-art on inverse folding.\n6. \"The coming of age of de novo protein design [b23]\" is mentioned as the central challenge in bioengineering that the paper addresses.\n\nThe confidence scores for each paper are estimated as follows:\n\n{\"b30\": 0.9, \"b28\": 0.85, \"b24\": 0.85, \"b15\": 0.8, \"b23\": 0.75}", "Based on the given text and author notes, the paper seems to be primarily about the design of protein sequences, with a particular focus on the challenge of overcoming limited datasets for training deep learning models. The method proposed involves using a larger dataset (UniRef50) with structures predicted by AlphaFold2, and making use of larger model parameters for improved performance. The paper emphasizes the use of Geometric Vector Perceptron (GVP) layers and the adaptation of an autoregressive encoder-decoder architecture for the task.\n\nFrom the text, it seems that the following papers have directly influenced the work of the authors:\n\n1. \"Highly accurate protein structure prediction with AlphaFold2\" (b30) - It is the source of the AlphaFold2 model that the authors have used to predict protein structures.\n2. \"Learning from protein structure with geometric vector perceptrons\" (b28) - It has provided the GVP model which has been adopted and expanded upon in the paper.\n3. \"Generative models for graph-based protein design\" (b24) - This paper is cited alongside the task that the authors have chosen to focus on: predicting sequences from backbone structures, also known as inverse folding. \n\nWith this information, the following references and confidence scores could be identified in JSON format:\n\n```json\n{\n    \"b30\": 1,\n    \"b28\": 0.9,\n    \"b24\": 0.8\n}\n```\n\nNote: The confidence scores are subjective and based on the information given in the text, with '1' indicating the highest confidence.", "Based on the text and the provided notes, the following source papers appear to have significantly influenced the given paper:\n\n{   \n    \"b24\": 0.95, \n    \"b28\": 0.95, \n    \"b30\": 0.9, \n    \"b54\": 0.85, \n    \"b67\": 0.8 \n}\n\nExplanation: \n\n- The paper frequently refers to [b24] and [b28] when discussing the deep learning approaches and the generative models for predicting sequences for structures, suggesting these papers heavily influenced the methodology used in the given paper. \n\n- Reference [b30] is mentioned in the context of predicting structures for large databases, indicating that it played a significant role in the development of the paper's methodology. \n\n- The paper refers to [b54] when discussing back-translation, a technique that the authors used to improve their model. It suggests this paper provided significant inspiration for the paper's approach.\n\n- The paper refers to [b67] when discussing the GVP-Transformer model architecture, indicating that this paper was a major influence on the paper's methodology. \n\nThe confidence scores are assigned based on the frequency and context of the references in the text, with higher scores indicating a higher degree of influence.", "Based on the text of the academic paper and the given notes, the most influential source papers would be:\n\n1. \"Highly accurate protein structure prediction with AlphaFold2 [b30]\" - The paper's authors used AlphaFold2 to predict structures for 12 million sequences in UniRef50. This clearly indicates a strong reliance on AlphaFold2, making it a significant source of inspiration for the paper.\n\n2. \"Learning from protein structure with geometric vector perceptrons [b28]\" - The authors adopted the GVP-GNN architecture from this source paper to their model, implying a high degree of influence from this paper.\n\n3. \"Understanding back-translation at scale [b15]\" - The authors draw a parallel to the back-translation idea in machine translation, indicating they were inspired by this paper.\n\n4. \"Generative models for graph based protein design [b24]\" - The authors approach inverse folding as a sequence-to-sequence problem, just like the idea proposed in this paper.\n\n5. \"Accurate prediction of protein structures and interactions using a three track neural network [b6]\" - Considering the paper's focus on protein structure prediction, this source paper seems to be influential.\n\n6. \"Learning inverse folding from millions of predicted structures\" - This is the paper's title, suggesting that it's a continuation or expansion of research in this area.\n\nFinal results in JSON format:\n\n{\n  \"b30\": 1,\n  \"b28\": 0.9,\n  \"b15\": 0.8,\n  \"b24\": 0.7,\n  \"b6\": 0.7,\n  \"None\": 0.6\n}", "{\n\"b28\": 0.9, \n\"b30\": 0.9, \n\"b24\": 0.7, \n\"b62\": 0.6, \n\"b3\": 0.6, \n\"b39\": 0.6, \n\"b54\": 0.5\n}"], "620c6b655aee126c0fe29013": ["Based on the paper text and the author notes, the source papers that inspired the paper most are likely to be:\n\n1. \"Mixup: Beyond Empirical Risk Minimization\" (b44)\n2. \"Manifold Mixup: Better Representations by Interpolating Hidden States\" (b32)\n3. \"Deep Learning on Graphs: A Survey\" (b47)\n4. \"A Comprehensive Survey on Graph Neural Networks\" (b38)\n5. \"Graphon Neural Networks and the Transferability of Graph Neural Networks\" (b27)\n6. \"Learning Graphons via Structured Gromov-Wasserstein Barycenters\" (b39)\n7. \"Graph Theory and Additive Combinatorics\" (b49)\n\nThe paper's main focus is on improving the generalization and robustness of Graph Neural Networks (GNNs) through mixing up graph data. The authors propose G-Mixup, a class-level graph data augmentation method, to mix up graph data based on graphons. The methods and approaches proposed in this paper are largely inspired by the concepts from the papers b44, b32, b47, b38, b27, b39, and b49. \n\nThe json format of the result would be:\n\n{\n\"b44\": 0.9,\n\"b32\": 0.8,\n\"b47\": 0.8,\n\"b38\": 0.7,\n\"b27\": 0.7,\n\"b39\": 0.7,\n\"b49\": 0.7\n} \n\nThe confidence score is a subjective estimation of how relevant and influential each reference paper is to the main contributions of the given paper, with 1 being the most relevant and influential.", "Based on the provided paper text and the author notes, the source papers that seem to have most inspired the given paper are:\n\n1. \"g mixup graph data augmentation for graph classification\" - The whole paper is about a method named \"G-Mixup\" for graph classification, which indicates that this paper is directly related to the main contributions of the analyzed paper.\n2. \"stochastic blockmodel approximation of a graphon theory and consistent estimation\" - This paper appears to be the source of the graphon theory that is central to the method proposed in the given paper.\n3. \"large networks and graph limits\" - This paper could be the theoretical basis for the use of graphon in the given paper.\n4. \"mixup beyond empirical risk minimization\" - The Mixup method was mentioned multiple times in the given paper, and it's clear that this paper provided the theoretical basis for the Mixup method.\n5. \"manifold mixup better representations by interpolating hidden states\" - This paper was also mentioned in the context of the Mixup method, suggesting that it was a significant source of inspiration.\n6. \"graph neural networks a review of methods and applications ai open\" - This paper likely provided a broad context for the development of Graph Neural Networks in the given paper.\n7. \"semi supervised classification with graph convolutional networks\" - Given the multiple mentions of Graph Convolutional Networks (GCN) in the paper, this paper is likely a key source.\n\nThe confidence scores for these papers may look like:\n\n```json\n{\n  \"None\": 1,\n  \"b0\": 0.9,\n  \"b22\": 0.8,\n  \"b44\": 0.9,\n  \"b32\": 0.8,\n  \"b38\": 0.7,\n  \"b21\": 0.7\n}\n```", "Based on the text and the titles of the references, the most relevant source papers that have inspired the given academic paper are:\n\n- \"Mixup beyond empirical risk minimization\" [b44]: This paper is mentioned multiple times in the context of how Mixup has been applied in image recognition and natural language processing tasks. The given paper extends Mixup to graph data to improve the generalization and robustness of GNNs.\n\n- \"Graph Neural Networks: A Review of Methods and Applications\" [b50]: This paper is likely a valuable source as it provides an overview of Graph Neural Networks (GNNs), which are the main focus of the given academic paper.\n\n- \"Stochastic blockmodel approximation of a graphon: Theory and consistent estimation\" [b0]: This paper appears to have inspired the approach to graphon estimation in the given paper. \n\n- \"Graph attention networks\" [b31]: This paper seems to have inspired the use of graph neural networks for graph classification.\n\n- \"Manifold Mixup: Better Representations by Interpolating Hidden States\" [b32]: This paper is mentioned in the context of Mixup variants, which suggests that it has influenced the given paper's approach to graph data augmentation.\n\n- \"DropEdge: Towards Deep Graph Convolutional Networks on Node Classification\" [b26]: This paper is mentioned in relation to data augmentation strategies in graph analysis which indicates it has guided the given paper's exploration of graph data augmentation.\n\n- \"Graphon Neural Networks and the Transferability of Graph Neural Networks\" [b27]: Given the given paper's focus on graphons and GNNs, it seems likely that this paper has been influential.\n\nBased on this analysis, the JSON format answer would be:\n\n{\"b44\": 0.9, \"b50\": 0.8, \"b0\": 0.8, \"b31\": 0.7, \"b32\": 0.7, \"b26\": 0.7, \"b27\": 0.7}", "Based on the provided text from the academic paper and the author\u2019s notes, we can identify a few key source papers that heavily influenced the main contributions of the paper:\n\n- The authors were inspired by the concept of graphons, which is a generator of a group of graphs, used in the paper \"Stochastic blockmodel approximation of a graphon theory and consistent estimation\" [b0].\n- The concept of graph mixup, which was proposed in this paper, is inspired by work on data augmentation and mixup techniques such as \"Mixup: Beyond empirical risk minimization\" [b44] and \"Manifold Mixup: Better Representations by Interpolating Hidden States\" [b32].\n- GNNs (Graph Neural Networks) are a central part of the paper's methodology, which is inspired by \"A Comprehensive Survey on Graph Neural Networks\" [b38] and \"Semi-Supervised Classification with Graph Convolutional Networks\" [b21].\n- The paper also seems to be inspired by the work on graph data augmentation such as \"DropEdge: Towards Deep Graph Convolutional Networks on Node Classification\" [b26] and \"Subgraph cropping for graph classification\" [b35].\n\nThe JSON format:\n\n{\n    \"b0\": 1,\n    \"b44\": 0.9,\n    \"b32\": 0.9,\n    \"b38\": 0.8,\n    \"b21\": 0.8,\n    \"b26\": 0.7,\n    \"b35\": 0.7\n} \n\nThe confidence scores are assigned based on the inferred influences of these papers on the current paper. The scores are subjective and indicate the level of direct relevance to the paper's novel contributions.", "Based on the analysis of the paper, the source papers that inspired the author's work can be identified as follows:\n\n1. \"Graph Neural Networks (GNNs) [b38] [b50] [b47] [b40]\" - These papers are mentioned at the very beginning and indicate the foundational work on which the author's research is based. The author uses Graph Neural Networks as a significant part of their research.\n\n2. \"data augmentation (e.g., DropEdge [b26] , Subgraph [b42] [b35])\" - These papers show that the author has taken inspiration from previous data augmentation strategies.\n\n3. \"Mixup [b44] and its variants (e.g., Manifold Mixup [b32])\" - The author's work directly relates to the Mixup technique and its variants, indicating that these papers are a source of inspiration.\n\n4. \"a graphon [b0] is a continuous, bounded and symmetric function\" - This paper explains the concept of graphons, which are a key component of the author's research.\n\n5. \"we propose G-Mixup, a class-level graph data augmentation method, to mix up graph data based on graphons\" - This sentence suggests that the authors' novel contributions are inspired by the combination of concepts from the above-mentioned papers.\n\nBased on the above information, the source papers that most inspired the author are [b38], [b50], [b47], [b40], [b26], [b42], [b35], [b44], [b32], and [b0].\n\nIn the Json format, the reference sources and their confidence scores would be as follows:\n\n{\n    \"b38\": 0.9,\n    \"b50\": 0.9,\n    \"b47\": 0.9,\n    \"b40\": 0.9,\n    \"b26\": 0.8,\n    \"b42\": 0.8,\n    \"b35\": 0.8,\n    \"b44\": 1,\n    \"b32\": 0.9,\n    \"b0\": 1\n}"], "62d8c4565aee126c0f762dba": ["Based on the text, the paper is about a new autoregressive model called NUWA-Infinity for generating arbitrarily-sized high-quality images or videos. The key challenges outlined include the generation of variable-size visual content and the need to model different generation orders and directions for different types of tasks. The authors propose solutions like a Nearby Context Pool and an Arbitrary Direction Controller.\n\nBased on the given text, the following references stand out as potential sources that might have inspired this paper:\n\n1. 'b0': 'masked generative image transformer'\n2. 'b5': 'taming transformers for high resolution image synthesis'\n3. 'b27': 'uwa visual synthesis pre training for neural visual world creation'\n4. 'b29': 'scaling autoregressive models for content rich text to image generation'\n\nThe phrases like \"Compared to diffusion-based approaches [b3] [b6] [b9]\", \"Compared to DALL?E [b18], Imagen [b19] and Parti [b29]\" and \"Compared to NUWA [b27]\" indicate that these papers may have been direct inspirations or benchmarks that the authors were trying to improve upon.\n\nHere is the returned result:\n\n{\n\"b0\": 0.9,\n\"b5\": 0.8,\n\"b27\": 1.0,\n\"b29\": 1.0\n}\n\nThe confidence score is a measure of how strongly the paper appears to rely on the corresponding source based on its text.", "Based on the text and the authors' notes, the paper introduces NUWA-Infinity, a model that uses an autoregressive over autoregressive generation mechanism for generating arbitrarily-sized high-quality images or videos. This novel approach to visual synthesis introduces two new concepts, the Nearby Context Pool (NCP) and the Arbitrary Direction Controller (ADC).\n\nHere are the identified source papers based on the text and criteria provided:\n\n1. 'b0': 'Masked generative image transformer' - The paper mentions that the autoregressive models like 'b0' incorporate a sliding window to enforce dependencies between different patches during inference. However, the proposed model introduces the autoregressive over autoregressive mechanism, which suggests that 'b0' inspired the work but NUWA-Infinity extends upon it. Confidence Score: 0.8\n2. 'b5': 'Taming transformers for high-resolution image synthesis' - The paper mentions that the model is based on an autoregressive over autoregressive generation mechanism and each patch is reconstructed by a pre-trained VQGAN decoder, which is proposed by 'b5'. Confidence Score: 0.9\n3. 'b18': 'Zero-shot text to image generation' - The paper mentions that the proposed model can deal with variable-image size generation task compared to 'b18' which generates images with a fixed resolution, suggesting that 'b18' inspired the work but NUWA-Infinity extends upon it. Confidence Score: 0.8\n4. 'b27': 'UWA visual synthesis pre-training for neural visual world creation' - The paper mentions that the generation quality of NUWA-Infinity has been improved significantly compared to 'b27', suggesting that 'b27' inspired the work but NUWA-Infinity extends upon it. Confidence Score: 0.8\n5. 'b29': 'Scaling autoregressive models for content rich text to image generation' - The paper mentions that the proposed model introduces the autoregressive over autoregressive mechanism into the generation procedure, which suggests that 'b29' inspired the work but NUWA-Infinity extends upon it. Confidence Score: 0.8\n\nBased on the analysis, the result is:\n{\"b0\":0.8, \"b5\":0.9, \"b18\":0.8, \"b27\":0.8, \"b29\":0.8}", "Based on the text, the following references seem to be the source papers that inspired the authors:\n\n1. 'b0': 'Masked Generative Image Transformer' - The paper mentions this as a previous work that tries to solve the problem of generating variable-size visual content using a divide-and-conquer strategy. Besides, it also mentions Mask-Predict models, which could be referring to this paper.\n\n2. 'b5': 'Taming Transformers for High-Resolution Image Synthesis' - This paper is referred to in the context of existing work that uses a divide-and-conquer strategy for generating variable-size visual content.\n\n3. 'b18': 'Zero-Shot Text-to-Image Generation' - The paper mentions DALL-E, which is the subject of this paper, and compares NUWA-Infinity's performance to it.\n\n4. 'b27': 'NUWA Visual Synthesis Pre-training for Neural Visual World Creation' - The paper mentions NUWA as a model that also supports image and video synthesis and then compares NUWA-Infinity's performance to it. \n\n5. 'b29': 'Scaling Autoregressive Models for Content-Rich Text-to-Image Generation' - The paper mentions Parti, which is the subject of this paper, and compares NUWA-Infinity's performance to it.\n\nThe JSON format with confidence scores (0 to 1 scale, with 1 being the most confident) would be:\n```\n{\n\"b0\": 0.9,\n\"b5\": 0.9,\n\"b18\": 1,\n\"b27\": 1,\n\"b29\": 1\n}\n```\nPlease note that the confidence score is subjective and could vary based on how the text is interpreted.", "Based on the text of the paper, it seems that the following references are the source papers that inspired the authors the most:\n\n1. \"b0: Masked Generative Image Transformer\" - This paper is mentioned multiple times in the text, especially in relation to the divide-and-conquer strategy and the autoregressive models.\n\n2. \"b5: Taming Transformers for High Resolution Image Synthesis\" - As the authors' work is heavily focused on high-resolution image synthesis, it is likely that this paper has provided significant inspiration.\n\n3. \"b12: InfinityGAN: Towards Infinite Pixel Image Synthesis\" - This paper is cited when discussing the trend of generating images with higher resolutions, which appears to be a key focus of the authors' work.\n\n4. \"b27: NUWA: Visual Synthesis Pre-training for Neural Visual World Creation\" - This paper is mentioned in the context of autoregressive visual synthesis pre-trained model to support both image and video generation tasks, which aligns with the aims of the authors' work.\n\n5. \"b29: Scaling Autoregressive Models for Content Rich Text to Image Generation\" - The authors' work also uses autoregressive models for content-rich text-to-image generation, indicating that this paper may have been a major source of inspiration.\n\nHere is the result in JSON format:\n\n{\n\"b0\": 0.9,\n\"b5\": 0.8,\n\"b12\": 0.8,\n\"b27\": 0.9,\n\"b29\": 0.8\n}", "Based on the paper's text and the titles of the references, the source papers that inspired the author most are likely to be:\n\n{\"b0\": 0.7, \"b5\": 0.9, \"b18\": 0.8, \"b27\": 1, \"b29\": 0.7}\n\nHere are the reasons:\n- \"b0\": The paper mentions the work of \"Masked Generative Image Transformer\" multiple times and discusses some ideas based on the work of this source paper.\n- \"b5\": The paper uses the idea of a pre-trained VQGAN decoder, which is likely influenced by the source paper \"Taming Transformers for High Resolution Image Synthesis\". \n- \"b18\": The paper compares its work with \"Zero Shot Text to Image Generation\" (DALL?E) multiple times, indicating it is a significant source paper.\n- \"b27\": The paper references its previous work \"NUWA: Visual Synthesis Pre-training for Neural Visual World Creation\" multiple times and builds upon it, indicating it's a main source.\n- \"b29\": The paper compares its approach to \"Scaling Autoregressive Models for Content Rich Text to Image Generation\" (Parti), suggesting it is a crucial source paper. \n\nPlease note that this analysis is based on the provided information and the confidence score is subjective and might vary."], "622eb2495aee126c0f62b12a": ["Based on the paper's text and the given instructions, the source papers of this paper seem to be:\n\n1. BERTopic: Neural Topic Modeling with a Class-Based TF-IDF Procedure - The paper describes a new model, BERTopic, which utilizes the class-based TF-IDF procedure to generate coherent topic representations. \n2. On the Surprising Behavior of Distance Metrics in High Dimensional Space - The paper talks about the challenges of high dimensional data, which is a primary challenge addressed in the paper.\n3. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction - The paper discusses the use of UMAP for dimension reduction of document embeddings, which is a key component of the proposed method in the paper.\n4. Top2Vec: Distributed Representations of Topics - This paper provides inspiration for the clustering approach used in the paper.\n5. HDBSCAN: Hierarchical Density Based Clustering - The paper uses HDBSCAN for clustering, which is also used in the paper.\n6. A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization - The paper uses a variant of TF-IDF for topic representation, which is inspired by this paper.\n\nIn JSON format:\n\n{\n\"b0\": 0.8,\n\"b27\": 0.9,\n\"b23\": 0.9,\n\"b29\": 0.8,\n\"b18\": 0.9\n}", "Based on the given information, the primary challenges outlined in the paper are about the limitations of conventional models such as Latent Dirichlet Allocation and Non-Negative Matrix Factorization. The author proposes a novel method, BERTopic, which uses clustering techniques and a class-based variation of TF-IDF to generate coherent topic representations.\n\nThe source papers that might have inspired this work could be inferred from the text through phrases indicating direct inspiration or motivation:\n\n1. \"Bidirectional Encoder Representations from Transformers (BERT) [b13] and its variations (e.g., [b24] [b26] [b19], have shown great results in generating contextual word-and sentence vector representations.\" \n2. \"[b37] demonstrated the viability of clustering embeddings with centroid-based techniques, compared to conventional methods such as LDA, as a way to represent topics.\"\n3. \"Top2Vec leverages Doc2Vec's word-and document representations to learn jointly embedded topic, document, and word vectors (Angelov, 2020; [b23].\"\n4. \"CTM, for example, demonstrates the advantage of relying on pre-trained language models, namely that future improvements in language models may translate into better topic models [b43].\"\n5. \"BERTopic uses the Sentence-BERT (SBERT) framework [b34].\"\n6. \"UMAP has shown to preservers more of the local and global features of high-dimensional data in lower projected dimensions [b27].\"\n7. \"The reduced embeddings are clustering used HDBSCAN [b27].\"\n8. \"[b1] demonstrated that reducing high dimensional embeddings with UMAP can improve the performance of well-known clustering algorithms, such as k-Means and HDBSCAN, both in terms of clustering accuracy and time.\"\n9. \"The classic TF-IDF procedure combines two statistics, term frequency, and inverse document frequency [b18].\"\n\nFrom the inferences and the context provided, the source papers include:\n\n- b13: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n- b24: BioBERT: A Pre-trained Biomedical Language Representation Model for Biomedical Text Mining\n- b26: RoBERTa: A Robustly Optimized BERT Pretraining Approach\n- b19: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\n- b37: Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics Too\n- b23: Distributed Representations of Sentences and Documents\n- b43: Topic Modelling Meets Deep Neural Networks: A Survey\n- b34: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\n- b27: UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction\n- b1: Considerably improving clustering algorithms using UMAP dimensionality reduction technique: A comparative study\n- b18: A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization\n\nTo represent as a JSON:\n\n{\n  \"b13\": 0.9,\n  \"b24\": 0.8,\n  \"b26\": 0.8,\n  \"b19\": 0.8,\n  \"b37\": 0.9,\n  \"b23\": 0.85,\n  \"b43\": 0.85,\n  \"b34\": 0.9,\n  \"b27\": 0.9,\n  \"b1\": 0.85,\n  \"b18\": 0.85\n}", "Based on the provided text, the following source papers appear to have the most significant influence on the paper's main contributions:\n\n1. \"BERTopic: Neural Topic Modeling with a Class-based TF-IDF Procedure\" - The paper appears to be directly based on the author's own previous work on BERTopic, a topic modeling technique that uses clustering and a class-based variant of TF-IDF. Confidence Score: 1.0\n\n2. \"Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks\" [b34] - The paper uses Sentence-BERT for generating document embeddings, indicating that this work has a direct influence on the methods used in the paper. Confidence Score: 0.9\n\n3. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b13] - The paper mentions the use of BERT and its variations for generating contextual word and sentence representations, suggesting that this work has a foundational influence on the paper's approach. Confidence Score: 0.8\n\n4. \"UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction\" [b27] - The paper uses UMAP for dimensionality reduction of document embeddings, indicating a significant reliance on this work. Confidence Score: 0.8\n\n5. \"HDSCAN: Hierarchical Density-Based Clustering\" [b29] - The paper uses HDBSCAN for document clustering, suggesting that this work has a direct influence on the methods used in the paper. Confidence Score: 0.8\n\n6. \"Top2Vec: Distributed Representations of Topics\" [b23] - The paper mentions Top2Vec as a similar approach to the one proposed, indicating that this work has some influence on the paper. Confidence Score: 0.7\n\nThe reference numbers and confidence scores are represented in the following JSON format:\n\n{\"BERTopic: Neural Topic Modeling with a Class-based TF-IDF Procedure\": 1.0, \"b34\": 0.9, \"b13\": 0.8, \"b27\": 0.8, \"b29\": 0.8, \"b23\": 0.7}", "Based on the information given, the most pertinent source papers for this given paper could be:\n\n1. 'bertopic neural topic modeling with a class based tf idf procedure' - The paper introduces BERTopic, a topic model that leverages clustering techniques and a class-based variation of TF-IDF to generate coherent topic representations, which is directly relevant to the author's work. Confidence Score: 0.9\n\n2. 'on the surprising behavior of distance metrics in high dimensional space' - The paper makes use of document embeddings and clustering in high-dimensional space, which seems to be a key aspect of the author's approach. Confidence Score: 0.7\n\n3. 'considerably improving clustering algorithms using umap dimensionality reduction technique a comparative study' - The author has used UMAP for dimensionality reduction before clustering, which aligns with the method described in this paper. Confidence Score: 0.7\n\n4. 'top vec distributed representations of topics' - The author's paper seems to build upon the techniques presented here, such as clustering embeddings and generating topic representations. Confidence Score: 0.8\n\n5. 'latent dirichlet allocation' - Although the author proposes a new method, they do reference the limitations of conventional models like LDA, suggesting that this work may have informed their research. Confidence Score: 0.6\n\n6. 'bert pre training of deep bidirectional transformers for language understanding' - The author uses BERT for generating document embeddings, a key step in their proposed method, which suggests they may have been inspired by this paper. Confidence Score: 0.8\n\n7. 'umap uniform manifold approximation and projection for dimension reduction' - The author uses UMAP for dimensionality reduction, indicating that this paper could be a source of inspiration. Confidence Score: 0.7\n\n8. 'dynamic topic models' - The author references dynamic topic modeling and proposes a method for modeling how topics evolve over time, suggesting inspiration from this paper. Confidence Score: 0.7\n\nJSON format:\n\n{\"None\": 0.9, \"b0\": 0.7, \"b1\": 0.7, \"b23\": 0.8, \"b8\": 0.6, \"b13\": 0.8, \"b27\": 0.7, \"b7\": 0.7}", "Based on the content of the paper and the context in which certain references are mentioned, the following are the most pertinent source papers:\n\n- \"b37\": The paper mentions that BERTopic builds on top of the clustering embeddings approach used in [b37]. This indicates that [b37] is a significant source paper that influenced the contributions of the current paper. Confidence score: 0.9.\n\n- \"b27\": Reference [b27] is mentioned several times in connection with UMAP and HDBSCAN methods used for dimensionality reduction and clustering respectively in the paper. Confidence score: 0.85.\n\n- \"b34\": [b34] is mentioned in the context of the Sentence-BERT (SBERT) framework used for document embedding. Confidence score: 0.85.\n\n- \"b43\": This paper is indicated as one demonstrating the advantage of relying on pre-trained language models for topic modeling. It could have inspired the use of pre-trained language models in BERTopic. Confidence score: 0.8.\n\n- \"b18\": This paper could have inspired the modification of TF-IDF to represent a term's importance to a topic instead of a document. Confidence score: 0.75.\n\n- \"b2\": This paper is mentioned in the context of Top2Vec, a method similar to the approach used in BERTopic. Confidence score: 0.7.\n\n- \"b23\": [b23] is mentioned in relation to the use of Doc2Vec's word-and document representations, which could have inspired similar usage in BERTopic. Confidence score: 0.7.\n\n{\n\"reference number\": \"confidence score\",\n\"b37\": 0.9,\n\"b27\": 0.85,\n\"b34\": 0.85,\n\"b43\": 0.8,\n\"b18\": 0.75,\n\"b2\": 0.7,\n\"b23\": 0.7\n}"], "628c4ce15aee126c0ff597ca": ["Based on the text of the paper and the titles of the references, the following references appear to be the most influential for this paper:\n\n1. 'b6': 'rethink training of bert rerankers in multi stage retrieval pipeline': The paper mentions this work multiple times and even uses its approach as a stepping stone for their own method.\n\n2. 'b10': 'dense passage retrieval for open domain question answering': The paper cites this work as a significant inspiration for their retrieval model.\n\n3. 'b25': 'attention is all you need': The paper uses the transformer model structure introduced in this paper for their HLATR model.\n\n4. 'b27': 'adversarial retriever ranker for dense text retrieval': The paper mentions this work as an example of a joint optimization approach for the retrieval and reranking stages.\n\n5. 'b11': 'bert pre training of deep bidirectional transformers for language understanding': The paper uses pre-trained language models (PTMs) such as BERT for their retrieval system.\n\n6. 'b23': 'rocketqav a joint training method for dense passage retrieval and passage re ranking': This paper is mentioned as a method for jointly optimizing the retrieval and reranking stage.\n\n7. 'b3': 'pre training methods in information retrieval': This reference might have provided the foundation for the pre-training methods used in the paper.\n\nHere is the final result in JSON format:\n\n{\n  \"b6\": 0.9,\n  \"b10\": 0.8,\n  \"b25\": 0.9,\n  \"b27\": 0.8,\n  \"b11\": 0.7,\n  \"b23\": 0.7,\n  \"b3\": 0.6\n}\nThe confidence scores indicate the level of direct relevance or inspiration each source paper has on the given paper. The scores are subjective and based on the number of mentions and the context in which they are mentioned in the paper.", "Based on the provided text and the titles of the referenced papers, the most relevant source papers that inspired the authors of the given paper can be identified as follows:\n\n- 'b6': \"Rethink Training of BERT Rerankers in Multi-Stage Retrieval Pipeline\": The paper mentions how better retrieval results provide a more enlightening signal to the reranking stage. It also explains that a simple weighted combination of the relevance scores produced by the two stages can further improve overall performance. These points strongly suggest that this paper was a significant source of inspiration.\n\n- 'b25': \"Attention is All You Need\": They specifically mention that they adopt the transformer model structure as introduced in this paper for the Hybrid List Aware Transformer Ranker (HLATR), which strongly suggests this is a source paper.\n\n- 'b10': \"Dense Passage Retrieval for Open-Domain Question Answering\": They mention the emergence of deep pre-trained language models has brought a storm in natural language processing and information retrieval fields, and they use the dense retrieval method, which suggests inspiration from this paper.\n\n- 'b27': \"Adversarial Retriever Ranker for Dense Text Retrieval\": They mention that studies in the second group attend to jointly optimize the two stages by adversarial training, which suggests some influence from this paper.\n\n- 'b23': \"ROCKETQAV: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking\": The given paper discusses joint optimization methods for retrieval and reranking stage, suggesting this paper as a source.\n\nThus, the returned result in the json format would be:\n\n```json\n{\n    \"b6\": 0.8,\n    \"b25\": 0.9,\n    \"b10\": 0.7,\n    \"b27\": 0.6,\n    \"b23\": 0.6\n}\n```\nThe confidence scores are determined based on the level of influence and mention of these papers in the given text, with 'b25' having the highest confidence score due to direct adoption of the transformer model structure mentioned in the paper.", "Based on the provided text and the guidance, the source papers that most likely inspired the authors are:\n\n1. \"Attention is all you need\" [b25]. The authors adopted the transformer model structure introduced in this paper. \n2. \"Rethink training of bert rerankers in multi stage retrieval pipeline\" [b6]. This paper is mentioned multiple times as a reference for both the design of the new method and the challenges faced in this field.\n3. \"Adversarial retriever ranker for dense text retrieval\" [b27]. This paper is referred to as an example of a joint optimization method, which is also adopted in the new method.\n4. \"Dense passage retrieval for open domain question answering\" [b10]. This paper is referenced multiple times and seems to provide inspiration for the new approach.\n5. \"Rocketqav a joint training method for dense passage retrieval and passage re-ranking\" [b23]. This work is mentioned as an example of a joint optimization method, which is also a key feature of the new method.\n6. \"Layer normalization\" [b0]. This paper introduces the LayerNorm operation used in the new method.\n\nThe confidence score for each paper is difficult to determine without access to more detailed context or the full paper. But a possible assignment could be:\n\n```\n{\n  \"b25\": 0.9,\n  \"b6\": 0.8,\n  \"b27\": 0.8,\n  \"b10\": 0.7,\n  \"b23\": 0.7,\n  \"b0\": 0.6\n}\n```\n\nThis assignment assumes that references mentioned more frequently or used in key parts of the new method are more influential, hence have a higher confidence score.", "Based on the provided text and the author's notes, the source papers that most inspired the given paper are:\n\n1. 'b10': 'Dense passage retrieval for open domain question answering'. The paper is heavily referenced and appears to provide the fundamental idea of text retrieval that this paper is based upon.\n2. 'b6': 'Rethink training of bert rerankers in multi stage retrieval pipeline'. This paper seems to be a direct inspiration for the training strategy adopted in this paper.\n3. 'b25': 'Attention is all you need'. The transformer model structure used in this paper is adopted from this source paper.\n4. 'b23': 'RocketQAv: A joint training method for dense passage retrieval and passage re-ranking'. This paper forms the basis for the idea of jointly optimizing the retrieval and reranking stage.\n5. 'b27': 'Adversarial retriever ranker for dense text retrieval'. The paper also uses adversarial training, a method mentioned in this paper.\n6. 'b3': 'Pre-training methods in information retrieval'. This paper appears to be a significant source paper as it provides the basis for the use of pre-trained models in retrieval systems that this paper builds upon.\n\nThe json format of the answer is:\n\n{\n\"b10\": 0.9,\n\"b6\": 0.85,\n\"b25\": 0.8,\n\"b23\": 0.8,\n\"b27\": 0.75,\n\"b3\": 0.75\n}", "Based on the given text and author notes, the most pertinent source papers that inspired this paper are:\n\n1. \"Attention is all you need\" [b25]: The author mentions that they adopted the transformer model structure introduced in this paper. This indicates that this paper provided the fundamental approach for their proposed model.\n\n2. \"Rethink training of BERT rerankers in multi-stage retrieval pipeline\" [b6]: This paper is mentioned in the context of providing a signal to the reranking stage, which implies that it influenced the author's approach to their problem.\n\n3. \"Adversarial retriever ranker for dense text retrieval\" [b27]: This paper is mentioned in the context of joint learning methods for the retrieval and reranking stages, indicating that it played a role in shaping the author's approach.\n\n4. \"BERT: Pre-training of deep bidirectional transformers for language understanding\" [b11]: This paper is mentioned in the context of the emergence of deep pre-trained language models, which have revolutionized natural language processing and information retrieval fields.\n\n5. \"Dense passage retrieval for open domain question answering\" [b10]: This paper is mentioned in the context of being beneficial for down streaming applications such as open domain question answering.\n\n6. \"Pre-training methods in information retrieval\" [b3]: Cited when discussing the different model architectures used in the retrieval stage and reranking stage.\n\nBased on these inferences, the source papers that inspired this paper most are [b25], [b6], [b27], [b11], [b10], and [b3].\n\nThe returned result in json format would be:\n{\"b25\": 1, \"b6\": 0.9, \"b27\": 0.8, \"b11\": 0.7, \"b10\": 0.7, \"b3\": 0.6}\n"], "6292aa4a8c0a46000c95808f": ["Based on the given text and the task requirements, the source papers that seem to have influenced the given paper the most are:\n\n1. \"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume\" [b52]: The authors have mentioned this paper multiple times and have also used its model as the foundation for their method. The authors have also specifically mentioned that they have been \"inspired by\" the successes of CNNs in many vision-related tasks, further indicating the strong influence of this paper.\n\n2. \"FlowNet: Learning Optical Flow with Convolutional Networks\" [b42]: The authors have referenced this paper multiple times and have also discussed the limitations of its methods, which they aim to address in their paper. \n\n3. \"FlowNet2: Evolution of Optical Flow Estimation with Deep Networks\" [b43]: This paper is also mentioned multiple times and the authors have compared their method with it, indicating that it might have influenced their research.\n\n4. \"Densely Connected Convolutional Networks\" [b60]: The authors have used a multilayer Dense CNN in their model, which is inspired by this paper.\n\n5. \"Occlusion-aware unsupervised learning of optical flow\" [b59]: The authors have discussed the limitations of the occlusion detection methods presented in this paper, which might indicate that it has inspired their research on occlusion detection.\n\n6. \"Residual Attention Network for Image Classification\" [b61]: The authors have been inspired by the attention mechanism presented in this paper to capture long-range dependencies of different-scale features in their model.\n\n7. \"Fast R-CNN\" [b67]: The authors have used a robust penalty function in their hybrid training loss, which is inspired by this paper.\n\n8. \"A naturalistic open source movie for optical flow evaluation\" [b68]: The authors have used the database from this paper to test the optical flow estimation performance of their model.\n\nThe confidence score for each paper is not provided in the task, but it can be assumed that the confidence score for these papers is high, since they appear to have directly contributed to the main contributions of the given paper.\n\nExpected output in JSON format:\n{\n \"b52\": 1,\n \"b42\": 1,\n \"b43\": 1,\n \"b60\": 1,\n \"b59\": 1,\n \"b61\": 1,\n \"b67\": 1,\n \"b68\": 1\n}", "Based on the information given, the source papers that most inspired the author are likely:\n\n- \"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume\" [b52]\n- \"Self-Attention-Based Multiscale Feature Learning Optical Flow with Occlusion Feature Map Prediction\" [b61]\n- \"FlowNet: Learning Optical Flow with Convolutional Networks\" [b42]\n- \"Unsupervised Deep Learning for Optical Flow Estimation\" [b58]\n- \"High Accuracy Optic Flow Estimation Based on a Theory for Warping\" [b27]\n- \"Flow Fields: Dense Correspondence Fields for Highly Accurate Large Displacement Optical Flow Estimation\" [b33]\n\nThe confidence scores can be assigned based on the frequency of the reference and its relevance to the main contributions of the paper. For example:\n\n{\"b52\": 0.9, \"b61\": 0.8, \"b42\": 0.8, \"b58\": 0.7, \"b27\": 0.7, \"b33\": 0.7} \n\nThis is based on the assumption that the paper with a higher frequency of reference and relevance to the main contributions would have a higher confidence score. These papers seem to contribute directly to the methods and approaches the author uses to address the challenges in the paper.", "Based on the text of the paper, the key sources and inspirations for the work seem to be:\n\n1. \"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume\" [b52]: This paper is repeatedly mentioned as the backbone of the proposed model, indicating that it is a key source of inspiration. The author uses this method to construct their proposed approach and improve upon it. \n2. \"FlowNet: Learning Optical Flow with Convolutional Networks\" [b42]: This paper is mentioned as the first to use deep learning for optical flow computation. It is likely that this paper was an important foundation for understanding the application of deep learning to optical flow. \n3. \"Densely Connected Convolutional Networks\" [b60]: This paper is mentioned in relation to the construction of the optical flow estimator used in the proposed model. It is likely that the methods in this paper were used as inspiration for this part of the model.\n4. \"Self-supervised learning of optical flow\" [b54]: This paper is mentioned in the context of unsupervised learning optical flow studies, indicating that it may have been a source of inspiration for the author's exploration of self-learning in occlusion prediction. \n\nIn JSON format:\n\n{\n\"b52\": 0.9,\n\"b42\": 0.8,\n\"b60\": 0.7,\n\"b54\": 0.7\n}", "Based on the text and the author's notes, the paper's main challenges are large displacements and motion occlusions in optical flow predictions using convolutional neural networks (CNNs). The paper introduces a self-attention-based multiscale feature learning optical flow computation method with occlusion feature map prediction, which is designed to tackle these challenges. The novel contributions include a self-attention-based multiscale feature learning module, a self-learning-based occlusion prediction module, and a hybrid loss function. \n\nBy looking for key phrases and the methods proposed in the paper, the most likely source papers are:\n\n1. ['b52': 'pwc net cnns for optical flow using pyramid warping and cost volume']: This paper is mentioned multiple times as the foundational method upon which the author's work is built. The author notes that they \"utilize a normal PWC-Net model as the foundation of the proposed method\" and later on, the paper is again cited in relation to the construction of the self-attention-based multiscale feature learning module. \n2. ['b14': 'determining optical flow']: This paper is cited as the origin of the variational optical flow method, which is the dominant method that the author's work is improving upon. \n3. ['b42': 'flownet learning optical flow with convolutional networks']: This paper is mentioned as a significant study that has inspired the deep learning approach to optical flow computations, which is a key aspect of the author's work.\n4. ['b43': 'flownet evolution of optical flow estimation with deep networks']: This paper is also mentioned in relation to the deep learning approach to optical flow computations.\n5. ['b61': 'residual attention network for image classification']: This paper is cited as a key source of inspiration for the self-attention-based multiscale feature learning module.\n6. ['b65': 'back to basics unsupervised learning of optical flow via brightness constancy and motion smoothness']: This paper could be considered as it is related to the author's unsupervised learning of optical flow.\n\nThe JSON format of the result:\n\n{\"b52\": 1, \"b14\": 0.9, \"b42\": 0.8, \"b43\": 0.7, \"b61\": 0.9, \"b65\": 0.7}", "Based on the given text and the context provided, the following are potential source papers for the given academic paper:\n\n1. 'PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume' [b52]: This paper is the most influential source since the proposed solution in the given paper uses the PWC-Net model as the foundation and explicitly mentions this in the text. It adopts its pyramid, warping, and cost volume model and improves upon it to construct new convolutional networks for better optical flow estimation performance. Confidence score: 1.\n\n2. 'FlowNet: Learning Optical Flow with Convolutional Networks' [b42]: This paper is mentioned as the first deep learning-based optical flow model, and the given paper's approach builds upon its concepts. Confidence score: 0.9.\n\n3. 'FlowNet2: Evolution of Optical Flow Estimation with Deep Networks' [b43]: Mentioned in the context of the development of CNN-based optical flow estimation, the given paper's method also relies on these foundational concepts. Confidence score: 0.8.\n\n4. 'Self-supervised Learning of Optical Flow' [b54]: This paper is indirectly referred to while discussing the unsupervised learning of optical flow methods, and the given paper's approach seems to adopt some concepts from this methodology. Confidence score: 0.7.\n\n5. 'UnFlow: Unsupervised Learning of Optical Flow with a Bidirectional Census Loss' [b66]: This paper is also mentioned in the context of unsupervised learning of optical flow methods, and the given paper's approach seems to incorporate some ideas from this paper. Confidence score: 0.6.\n\n6. 'LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation' [b45]: This paper is referenced when discussing the balance between prediction accuracy and network size, indicating that it has potentially influenced the given paper's approach. Confidence score: 0.6.\n\n7. 'Residual Dense Network for Image Super Resolution' [b64]: The concept of residual learning from this paper is utilized in the development of the self-attention-based multiscale feature learning module in the given paper. Confidence score: 0.7.\n\n\nThe result in the JSON format:\n\n{\"b52\": 1, \"b42\": 0.9, \"b43\": 0.8, \"b54\": 0.7, \"b66\": 0.6, \"b45\": 0.6, \"b64\": 0.7}"], "62a013785aee126c0ff695ef": ["Based on the paper's text and the titles of the references, the primary challenges outlined in the paper are the ill-defined pre-training tasks and the limited model capacity in current self-supervised learning methods on molecular graphs. The author proposed a novel self-supervised learning framework, Knowledge-guided Pre-training of Graph Transformer (KPGT), to alleviate these issues. The most pertinent source papers are:\n\n1. [b24]: \"Strategies for pre-training graph neural networks\" - This paper is referenced multiple times, especially when discussing the issues of current self-supervised learning methods on molecular graphs and the pre-training tasks. Confidence score: 0.9\n\n2. [b43]: \"Self-supervised graph transformer on large scale molecular data\" - This paper is mentioned as a comparison point for KPGT and is also related to the use of transformer-based models in self-supervised learning on molecular graphs. Confidence score: 0.8\n\n3. [b9]: \"BERT: Pre-training of deep bidirectional transformers for language understanding\" - This paper is cited when introducing the generative self-supervised learning scheme, which is a key method in KPGT. Confidence score: 0.7\n\n4. [b34]: \"Molecule attention transformer\" - This paper is cited when discussing the use of transformer-based architectures in the self-supervised learning on molecular graphs. Confidence score: 0.7\n\n5. [b62]: \"Graph contrastive learning with augmentations\" - This paper is mentioned when discussing the contrastive methods on molecular graphs, which seem to be relevant to the proposed method. Confidence score: 0.6\n\n6. [b61]: \"Graph contrastive learning automated\" - This paper is also discussed in the context of contrastive methods on molecular graphs. Confidence score: 0.6\n\nThe JSON format of the result is:\n{\n  \"b24\": 0.9,\n  \"b43\": 0.8,\n  \"b9\": 0.7,\n  \"b34\": 0.7,\n  \"b62\": 0.6,\n  \"b61\": 0.6\n}", "Based on the given academic paper's text, the list of source papers that inspired the authors the most are as follows:\n\n{ \"b24\": 0.9, \"b43\": 0.8, \"b62\": 0.7, \"b61\": 0.6, \"b57\": 0.6, \"b31\": 0.6, \"b9\": 0.5, \"b49\": 0.5}\n\nThe values are confidence scores ranging from 0 to 1, where a higher score indicates a stronger influence on the current paper.", "Based on the given paper text and the author's notes, the following source papers have been identified:\n\n1. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b9]: This paper is referenced multiple times in the context of the design of the pre-training tasks for the proposed self-supervised learning framework. The author also explicitly mentions that their generative methods follow the masked-language models in the NLP field, such as BERT.\n\n2. \"Strategies for Pre-training Graph Neural Networks\" [b24]: This paper is referred to multiple times in the context of discussing the issues with current self-supervised learning methods on molecular graphs, particularly the ill-defined pre-training tasks. It seems to have inspired the author's emphasis on the importance of well-defined pre-training tasks.\n\n3. \"Graph Contrastive Learning Automated\" [b61]: This paper is quoted in the discussion of contrastive methods on molecular graphs. It appears to have influenced the author's approach to self-supervised learning on molecular graphs.\n\n4. \"Graph Contrastive Learning with Augmentations\" [b62]: This paper is referenced in the same context as [b61], and appears to have similarly influenced the author's approach.\n\n5. \"Attention is All You Need\" [b49]: This paper is referenced in the description of the Line Graph Transformer (LiGhT) model, which the author describes as being built upon a classic transformer encoder initially proposed in this paper.\n\n6. \"Self-supervised Graph-level Representation Learning with Local and Global Structure\" [b57]: This paper is quoted when the author discusses the other self-supervised learning methods on molecular graphs that have been proposed previously. The author seems to have been inspired by this work in their design of their novel self-supervised learning framework.\n\n7. \"Self-supervised Graph Transformer on Large Scale Molecular Data\" [b43]: This paper is mentioned when the author discusses the methods that incorporate additional knowledge of molecules, and the performance of these methods compared to those that do not. The author appears to have been influenced by this work in their decision to incorporate additional knowledge in their proposed method.\n\n8. \"Molecule Attention Transformer\" [b34]: This paper is referenced when the author discusses the limitations of previous graph transformers and their inability to model the structural information of molecular graphs properly. The author seems to have been inspired by this work in their design of the Line Graph Transformer (LiGhT) model.\n\nThe confidence scores for these papers can be inferred from the context in which they are cited, the relevance of their topics to the paper's contributions, and the frequency of their citation.\n\nBased on these factors, the following JSON object is returned:\n\n```\n{\n  \"b9\": 0.9,\n  \"b24\": 0.8,\n  \"b61\": 0.8,\n  \"b62\": 0.8,\n  \"b49\": 0.85,\n  \"b57\": 0.75,\n  \"b43\": 0.85,\n  \"b34\": 0.75\n}\n```", "Based on the given academic paper text and the author's notes, the source papers that inspire the paper most could be:\n\n- \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b9]: The paper mentions that the generative methods follow the masked language models in the NLP field, which is primarily introduced by BERT. This indicates that BERT had a significant influence on the development of the algorithm proposed in the text. \n\n- \"Strategies for Pre-training Graph Neural Networks\" [b24]: The paper cites this source multiple times and discusses the ill-defined pre-training tasks and the limited model capacity issues in the current self-supervised learning methods on molecular graphs that this source paper discusses. It's clear that this paper played a major role in inspiring the novel methods proposed in the text.\n\n- \"Graph Contrastive Learning with Augmentations\" [b62]: The paper refers to this source as a pioneering work in the field of contrastive methods on molecular graphs.\n\n- \"Self-Supervised Graph Transformer on Large-Scale Molecular Data\" [b43]: The paper cites this source when discussing the success of transformer-based models in the NLP and CV domains. This implies that this source paper had a significant influence on the development of the Graph Transformer proposed in the text.\n\n- \"MolecuLeNet: A Benchmark for Molecular Machine Learning\" [b54]: The paper refers to this source multiple times when discussing the challenges in molecular property prediction, implying that this source paper had a significant influence on the development of the algorithm proposed in the text.\n\n- \"Attention is All You Need\" [b49]: The paper refers to this source when discussing the use of the classic transformer encoder.\n\n- \"Deep Graph Infomax\" [b50]: The paper refers to this source when discussing the contrastive methods on molecular graphs.\n\n- \"Molecule Attention Transformer\" [b34]: The paper refers to this source when discussing the use of distance encoding in the transformer architecture.\n\nBased on the above analysis, the following source papers can be provided:\n\n```\n{\n    \"b9\": 0.8,\n    \"b24\": 0.9,\n    \"b62\": 0.7,\n    \"b43\": 0.8,\n    \"b54\": 0.8,\n    \"b49\": 0.7,\n    \"b50\": 0.7,\n    \"b34\": 0.7\n}\n```\n\nThe confidence scores are roughly estimated based on how directly the source papers appear to have influenced the main contributions of the paper. A higher score indicates a stronger influence.", "Based on the given academic paper's text, the following references seem to have inspired the paper's novel contributions most directly:\n\n1. \"BERT: Pre-training of deep bidirectional transformers for language understanding [b9]\": The author refers to BERT's strategy of masking for pre-training tasks. The influence of the BERT model is further highlighted in the \"Knowledge-guided pre-training strategy\" section of the paper.\n\n2. \"Graph Contrastive Learning with Augmentations [b62]\": This paper is referred to as a pioneering work on contrastive methods for molecular graphs. It's also mentioned in the introduction and related work sections, indicating its influence on the current paper's approach to pre-training GNNs.\n\n3. \"GROVER: Self-supervised graph transformer on large scale molecular data [b43]\": The author compares the performance of their model with GROVER and highlights how their work improves over GROVER by introducing additional knowledge in the pre-training. \n\n4. \"Molecule Attention Transformer [b34]\": This paper's influence is seen in the design of the \"Line Graph Transformer (LiGhT)\" which is a novel part of the proposed method in this paper.\n\n5. \"Strategies for pre-training graph neural networks [b24]\": The author discusses the strategies mentioned in this paper and their limitations. It's used to highlight the novel contributions of the current paper's approach to pre-training tasks.\n\n6. \"Graph Contrastive Learning Automated [b61]\": The author uses this paper for comparison in the experimental results section and refers to it in the related work section, pointing to its influence on the current paper's methodology.\n\nThe confidence scores for these references might be subjectively decided based on how directly they influenced the paper's novel contributions. If a paper's methodology is built directly on the ideas proposed in a reference, the confidence score would be closer to 1.\n\nThe result in JSON format would look like this:\n{\n    \"b9\": 0.9,\n    \"b62\": 0.8,\n    \"b43\": 0.85,\n    \"b34\": 0.7,\n    \"b24\": 0.75,\n    \"b61\": 0.7\n}"], "62e744545aee126c0f33c1ec": ["Based on the text of the paper provided, the primary challenge outlined in the paper is to solve the issue of data noise and skewed data distribution in user-item interaction modeling in the context of recommender systems. The authors propose a novel approach, the Self-Supervised Hypergraph Transformer (SHT), to tackle these challenges. The SHT integrates a hypergraph neural network with a topology-aware Transformer to enhance the robustness and generalization performance of graph-based CF paradigms for recommendation.\n\nBased on the text and the titles of the references, below are the source papers that inspired the authors the most:\n\n1. 'LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation' (b5)\n2. 'Graph Convolutional Matrix Completion' (b0)\n3. 'Neural Graph Collaborative Filtering' (b25)\n4. 'Self-Supervised Graph Learning for Recommendation' (b30)\n5. 'Hypergraph contrastive collaborative filtering' (b32)\n6. 'Self-supervised learning for large scale item recommendations' (b36)\n\nThis inference is made based on the direct mention of these papers in the text and their relevance to the novel method proposed in the paper. These papers deal with similar challenges and methods, namely graph-based recommendation systems, self-supervised learning, and hypergraph-based systems. \n\nThe confidence scores can be assigned based on the frequency of the mention of these methods and their direct relevance to the proposed solution in the paper. However, without more specific information or context, it's not possible to assign precise confidence scores. \n\nHere is the inferred result in JSON format:\n\n{\n\"b5\": 0.85,\n\"b0\": 0.8,\n\"b25\": 0.8,\n\"b30\": 0.75,\n\"b32\": 0.75,\n\"b36\": 0.7\n}", "Based on the text and the titles of the references, the source papers that have inspired this work most directly seem to be:\n\n'b5': 'lightgcn simplifying and powering graph convolution network for recommendation' - The paper mentioned LightGCN as a previous method to simplify graph-based message passing in recommendation models. \n\n'b25': 'neural graph collaborative filtering' - The paper cites NGCF as an example of a method that tries to aggregate neighboring information for recommendation. \n\n'b10': 'dual channel hypergraph collaborative filtering' - The paper talked about different models that construct hypergraph connections to improve recommendation, and this paper might have been used as a reference for that.\n\n'b30': 'self supervised graph learning for recommendation' - The paper mentioned self-supervised learning methods like SGL to improve user representations, which is a key aspect of the proposed method.\n\n'b32': 'hypergraph contrastive collaborative filtering' - The paper also mentioned about leveraging hypergraph to generate contrastive signals to improve the graph-based recommender system, which is similar to the proposed method.\n\n'b37': 'graph convolutional neural networks for web scale recommender systems' - The paper mentioned PinSage, a model built upon the graph convolutional network, for recommendation.\n\nThus, the source papers for this work might be:\n\n- 'lightgcn simplifying and powering graph convolution network for recommendation'\n- 'neural graph collaborative filtering'\n- 'dual channel hypergraph collaborative filtering'\n- 'self supervised graph learning for recommendation'\n- 'hypergraph contrastive collaborative filtering'\n- 'graph convolutional neural networks for web scale recommender systems'\n\nThe confidence scores for each paper are not provided in the text, so it's hard to give an exact estimate. However, based on the frequency and context of their citations, a rough estimate could be:\n\n```json\n{\n  \"b5\": 0.8,\n  \"b25\": 0.8,\n  \"b10\": 0.7,\n  \"b30\": 0.7,\n  \"b32\": 0.7,\n  \"b37\": 0.7\n}\n```\nThe scores are subjective and based on the information provided in the text.", "Based on the text of the paper, the following references seem to be the most pertinent source papers:\n\n1. 'b5': 'LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation'\n2. 'b25': 'Neural Graph Collaborative Filtering'\n3. 'b30': 'Self-Supervised Graph Learning for Recommendation'\n4. 'b32': 'Hypergraph Contrastive Collaborative Filtering'\n5. 'b37': 'Graph Convolutional Neural Networks for Web Scale Recommender Systems'\n\nThese papers were referenced in the context of the methods and approaches that the authors of the current paper took inspiration from and built upon. They all deal with graph-based collaborative filtering, graph neural networks, contrastive learning and self-supervised learning, which are central to the methods proposed in the paper.\n\nThe confidence scores for each paper are subjective and can vary, but a possible assignment could be:\n\n{\"b5\": 0.9, \"b25\": 0.8, \"b30\": 0.85, \"b32\": 0.7, \"b37\": 0.75} \n\nThis implies high confidence in \"b5\" and \"b30\" being key sources of inspiration, with slightly less confidence in the other papers.", "Based on the text and the specific phrases that suggest the use of previous research, the source papers are likely:\n\n1. \"LightGCN [b5] omits the burdensome non-linear transformer during the embedding propagation and improve the recommendation performance.\"\n2. \"To further enhance the graph-based user-item interaction modeling, some follow-up studies propose to learn intent-aware representations with disentangled graph neural frameworks (e.g., DisenHAN [b28]), differentiate behavior-aware embeddings of users with multi-relational graph neural models (e.g., MB-GMN [b33]).\"\n3. \"While there exist a handful of recently developed recommendation methods (SGL [b30] and SLRec [b36]) leveraging self-supervised learning to improve user representations.\"\n4. \"Recent studies have attempted to design various graph neural architectures to model the user-item interaction graphs through embedding propagation. For example, PinSage [b37] and NGCF [b25].\"\n5. \"Several multirelational GNNs are proposed to enhance recommender systems with multi-behavior modeling, including KHGT [b31] and HMG-CR [b34].\"\n\nHence, the source papers that inspired this work can be identified as: b5, b28, b33, b30, b36, b37, b25, b31, and b34.\n\nThe json format of the identified source papers with a confidence score of 1 (since they are directly mentioned in the text) would look like this:\n\n{\n \"b5\": 1,\n \"b28\": 1,\n \"b33\": 1,\n \"b30\": 1,\n \"b36\": 1,\n \"b37\": 1,\n \"b25\": 1,\n \"b31\": 1,\n \"b34\": 1\n}", "Based on the text provided, the papers that most directly influenced the work on the Self-Supervised Hypergraph Transformer (SHT) for recommender systems are:\n\n- \"LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation\" [b5]: This paper is mentioned multiple times in the text and seems to have been foundational in the development of SHT, which also employs a lightweight graph convolutional network.\n- \"Self-Supervised Graph Learning for Recommendation\" [b30]: This paper is mentioned in the context of recent developments in self-supervised learning methods for improving user representations. It is likely that the techniques described in this paper helped to shape the self-supervised learning component of SHT.\n- \"Self-Supervised Auxiliary Learning with Meta Paths for Heterogeneous Graphs\" [b9]: This paper is also mentioned in the context of self-supervised learning and likely influenced the development of SHT's self-supervised learning component.\n- \"Disentangled Graph Collaborative Filtering\" [b27]: The authors mention this paper when discussing recent improvements in user representation. It is possible that techniques from this paper influenced the design of SHT.\n- \"Hypergraph Contrastive Collaborative Filtering\" [b32]: This paper is mentioned in the context of recent advances in graph-based recommender systems. The authors of SHT probably drew on techniques from this paper when developing their own model.\n- \"Dual Channel Hypergraph Collaborative Filtering\" [b10]: This paper is mentioned in the context of recently developed models that construct hypergraph connections to improve relation learning for recommendation, which is a key component of the SHT model.\n\nHere is the result in the required format:\n\n```json\n{\n  \"b5\": 0.9,\n  \"b30\": 0.8,\n  \"b9\": 0.8,\n  \"b27\": 0.7,\n  \"b32\": 0.7,\n  \"b10\": 0.7\n}\n```\n\nThe confidence scores represent a rough estimate of how directly each paper influenced the development of SHT, with a higher score suggesting a stronger influence."], "623be1965aee126c0f37abe2": ["{\n  \"b18\": 0.95,\n  \"b41\": 0.95,\n  \"b3\": 0.85,\n  \"b14\": 0.85\n}", "{\n    \"b41\": 0.9,\n    \"b18\": 0.85,\n    \"b3\": 0.8,\n    \"b14\": 0.75\n}", "{\n    \"b18\": 0.95,\n    \"b41\": 0.95\n}", "{\n    \"b18\": 0.9,\n    \"b41\": 0.9,\n    \"b3\": 0.7,\n    \"b14\": 0.6\n}", "{\n    \"b18\": 0.9,\n    \"b3\": 0.8,\n    \"b14\": 0.8,\n    \"b33\": 0.7,\n    \"b35\": 0.7,\n    \"b41\": 0.9\n}"], "6281b2a35aee126c0ffd514a": ["{\n    \"b22\": 0.9,\n    \"b32\": 0.8,\n    \"b10\": 0.7,\n    \"b30\": 0.7\n}", "{\n    \"b22\": 0.8,\n    \"b10\": 0.8,\n    \"b30\": 0.8,\n    \"b32\": 0.9\n}", "{\n  \"b22\": 0.9,\n  \"b32\": 0.9,\n  \"b10\": 0.85,\n  \"b30\": 0.85\n}", "{\n    \"b22\": 0.9,\n    \"b30\": 0.85,\n    \"b10\": 0.8,\n    \"b32\": 0.75,\n    \"b42\": 0.75\n}", "{\n  \"b22\": 0.9,\n  \"b32\": 0.8,\n  \"b10\": 0.7,\n  \"b30\": 0.7,\n  \"b42\": 0.6\n}"], "624802f36750f84d00082030": ["Based on the text and titles of references, the paper seems to take inspiration from the following source papers:\n\n1. \"Network hygiene, incentives, and regulation: deployment of source address validation in the internet\" [b6]: This paper is mentioned multiple times in the text, and the authors of the current paper clearly state that their work is a continuation and extension of the work done in [b6]. They also compare their findings directly with the results from [b6]. Confidence Score: 1\n\n2. \"Exit from Hell? Reducing the Impact of Amplification DDoS Attacks\" [b21]: The current paper uses a method for detecting SAV non-compliance that was implemented by K\u00fchrer et al. in [b21]. Confidence Score: 0.8\n\n3. \"The Spoofer project: Inferring the extent of source address filtering on the internet\" [b13]: The authors mention that their method of observing SAV adoption is different from the volunteer-based Spoofer project. Confidence Score: 0.7\n\n4. \"Nudge: Improving Decisions about Health, Wealth, and Happiness\" [b30]: Although this is not an academic paper, the concept of 'nudging' mentioned in this book inspired the authors to test the impact of 'social and reciprocity nudges' on compliance. Confidence Score: 0.6\n\n5. \"Using Loops Observed in Traceroute to Infer the Ability to Spoof\" [b22]: This paper is mentioned in the context of methods to detect SAV compliance, suggesting the current paper may have taken some inspiration from it. Confidence Score: 0.5\n\n6. \"A Promise is a Promise: The Effect of Commitment Devices on Computer Security Intentions\" [b33]: This paper seems to have inspired the \"nudging treatments\" section of the current paper. Confidence Score: 0.5\n\n7. \"You've Got Vulnerability: Exploring Effective Vulnerability Notifications\" [b10]: This paper seems to have inspired the \"Security Notification Experiments\" section of the current paper. Confidence Score: 0.5\n\nHere's the result in JSON format:\n\n{\n\"b6\": 1,\n\"b21\": 0.8,\n\"b13\": 0.7,\n\"b30\": 0.6,\n\"b22\": 0.5,\n\"b33\": 0.5,\n\"b10\": 0.5\n}", "Based on the given academic text, the papers that seem to have most inspired or influenced this research are:\n\n1. \"Network hygiene, incentives, and regulation: deployment of source address validation in the internet\" [b6]: This paper is cited multiple times in the text, and it appears that the methodology and findings of this paper have had a significant influence on the current research, especially in the context of source address validation (SAV) and the use of notifications.\n\n2. \"Amplification hell: revisiting network protocols for DDoS abuse\" [b11]: This paper is referenced as a method for observing the lack of SAV adoption, indicating that it has been a significant influence in shaping the current paper's methodology.\n\n3. \"Saving the Internet: Explaining the adoption of source address validation by Internet service providers\" [b12]: This paper also seems to have influenced the current study, especially in regard to the adoption of SAV among network operators.\n\n4. \"The Spoofer project: inferring the extent of source address filtering on the internet\" [b20]: The Spoofer project is referenced several times, indicating its impact on the current research. \n\n5. \"The Cox model\" [b14]: This model is used to quantify the impact of network complexity factors and sociotechnical country level effects on the deployment of SAV, suggesting it has had a critical role in the paper's methodology.\n\n6. \"Exit from hell? Reducing the impact of amplification DDoS attacks\" [b21]: This paper is referenced as a prior study on the effectiveness of security notifications, indicating its influence on the current research.\n\nBased on these findings, the list of source papers along with the confidence scores would be:\n\n{ \n    \"b6\": 0.9,\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b20\": 0.85,\n    \"b14\": 0.7,\n    \"b21\": 0.75\n}", "Based on the provided text and context of the academic paper, the most pertinent source papers that inspired the study can be identified as follows:\n\n1. \"Network hygiene, incentives, and regulation: Deployment of source address validation in the internet\" [b6] - This paper is cited several times throughout the text and seems to have heavily influenced the author's approach and methodology. It also seems to be a key source for the author's understanding of SAV adoption and the effects of notifying operators.\n\n2. \"Exit from Hell? Reducing the Impact of Amplification DDoS Attacks\" [b21] - This source is mentioned for its methodology of identifying networks that do not implement SAV and for its approach of sending notifications to network operators about open resolvers.\n\n3. \"The Spoofer Project: Inferring the extent of source address filtering on the internet\" [b20] - This source has clearly influenced the author's understanding of the Spoofer project and its impact on the SAV adoption.\n\n4. \"Using Loops Observed in Traceroute to Infer the Ability to Spoof\" [b22] - This paper is referred to as a method to detect networks that do or do not implement SAV. \n\n5. \"Tell me you fixed it: Evaluating vulnerability notifications via quarantine networks\" [b23] - This paper seems to have motivated the authors to explore the effectiveness of notifications to operators.\n\n6. \"A Theory of Reciprocity\" [b40] - This paper is cited as a theoretical basis for the use of reciprocity nudges in the study's experimental design.\n\n7. \"Nudge: Improving Decisions about Health, Wealth, and Happiness\" [b30] - This source seems to have inspired the authors to use nudging as a means to motivate network operators to implement SAV.\n\n8. \"The Cox Model\" [b14] - This paper is referred to for its statistical model used to quantify the impact of network factors on the deployment of SAV.\n\nTherefore, based on the provided text and context, the source papers that inspired the study can be listed in a JSON format as follows:\n\n{\n \"b6\": 0.9,\n \"b21\": 0.85,\n \"b20\": 0.8,\n \"b22\": 0.75,\n \"b23\": 0.7,\n \"b40\": 0.65,\n \"b30\": 0.6,\n \"b14\": 0.55\n}\n\nThe confidence scores are assigned based on the relevance and frequency of their mentions in the text, with '0.9' indicating the highest relevance and '0.55' indicating the lowest relevance among the selected sources.", "Based on the text and the titles of the references, the source papers that inspired the author of the given paper the most are:\n\n{\"reference number\": \"b6\", \"confidence score\": 0.9}, \n{\"reference number\": \"b21\", \"confidence score\": 0.8}, \n{\"reference number\": \"b12\", \"confidence score\": 0.8}, \n{\"reference number\": \"b13\", \"confidence score\": 0.7}, \n{\"reference number\": \"b46\", \"confidence score\": 0.7}, \n{\"reference number\": \"b15\", \"confidence score\": 0.6}, \n{\"reference number\": \"b16\", \"confidence score\": 0.6}. \n\nExplanation:\n- Reference \"b6\" titled \"network hygiene incentives and regulation deployment of source address validation in the internet\" seems to be a primary source of inspiration and methodology as the author of the given paper frequently refers to this paper and its findings. \n- Reference \"b21\" titled \"exit from hell reducing the impact of amplification ddos attacks\", \"b12\" titled \"saving the internet explaining the adoption of source address validation by internet service providers\", and \"b13\" titled \"the spoofer project\" are also mentioned multiple times specifically in the context of the methods and approaches used in the given paper. \n- References \"b46\" titled \"spoofing asns\", \"b15\" titled \"challenges in inferring spoofed traffic at ixps\", and \"b16\" titled \"detection classification and analysis of inter domain traffic with spoofed source ip addresses\" are also relevant as they provide the technical background and methodology for detecting IP spoofing and SAV adoption. \n\nThe \"confidence score\" is a subjective measure of how strongly the given paper appears to rely on each source, based on the frequency and context of citations and the apparent relevance of the source paper's topic.", "The source papers that inspired the given academic paper are:\n\n1. \"Network hygiene, incentives, and regulation: deployment of source address validation in the internet\" [b6]\n   The author has directly mentioned this paper as a source of inspiration for his study. The paper focuses on the study of IP spoofing and its impact on network operators. It provides a comprehensive analysis of the deployment of SAV and its impact on the network. The author of the given paper has used this study as a base to develop his own experimental design and methodology.\n\n2. \"Exit from Hell? Reducing the Impact of Amplification DDoS Attacks\" [b21]\n   This paper discusses misbehaving DNS resolvers for detecting networks that do not implement the SAV standard. The given paper has adopted this technique to identify such networks, which indicates the prominent influence of this paper.\n\n3. \"Spoofer project: Inferring the extent of source address filtering on the Internet\" [b13]\n   The author has referred to the Spoofer project multiple times, indicating its influence. This project's findings are used to compare and contrast the results of the author's own study.\n\n4. \"Inferring the deployment of inbound source address validation using DNS resolvers\" [b18]\n   This paper provides a different technique to observe the lack of SAV adoption. The author of the given paper has adopted this technique, indicating its inspiration.\n\n5. \"Challenges in inferring spoofed traffic at IXPs\" [b15]\n   This paper discusses the challenges in implementing the SAV standard, which is a key aspect of the given paper's study. The author has used this paper to understand these challenges better.\n\n6. \"The Cox Model\" [b14]\n   This paper provides the Cox mixed-effects model used by the author to quantify the impact of network complexity factors on the deployment of SAV.\n\n7. \"Behavioral Nudges: Their Mechanisms and Justification\" [b35]\n   This paper discusses the concept of behavioral nudges, which the author has incorporated in his experimental design. The author has used social and reciprocity nudges in the notification system to motivate network operators to implement SAV.\n\nHere is the JSON representation of the confidence scores of the source papers:\n\n{\n  \"b6\": 1,\n  \"b21\": 0.9,\n  \"b13\": 0.8,\n  \"b18\": 0.7,\n  \"b15\": 0.7,\n  \"b14\": 0.7,\n  \"b35\": 0.6\n}\n\nThe confidence score is given on a scale of 0 to 1, with 1 being the most confident in the paper's influence on the given paper."], "63034ea190e50fcafd73e0b0": ["{\n  \"b8\": 0.95,\n  \"b16\": 0.95,\n  \"b17\": 0.95,\n  \"b28\": 0.95,\n  \"b30\": 0.95,\n  \"b39\": 0.85,\n  \"b41\": 0.90\n}", "{\n    \"b8\": 0.9,\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b30\": 0.7,\n    \"b47\": 0.9,\n    \"b41\": 0.7\n}", "{\n  \"b8\": 0.9,\n  \"b16\": 0.85,\n  \"b17\": 0.85,\n  \"b47\": 0.9,\n  \"b41\": 0.85,\n  \"b13\": 0.85,\n  \"b28\": 0.8,\n  \"b39\": 0.8\n}", "{\n    \"b8\": 0.9,\n    \"b16\": 0.9,\n    \"b17\": 0.85,\n    \"b30\": 0.8,\n    \"b47\": 0.95\n}", "{\n    \"b8\": 0.9,\n    \"b16\": 0.85,\n    \"b17\": 0.85,\n    \"b39\": 0.8\n}"], "62c2a9595aee126c0fcf0a32": ["Based on the provided text and the titles of the references, the most pertinent source papers to the given paper are:\n\n1. \"Deep Graph Structure Learning for Robust Representations: A Survey\" [b51]\n2. \"Reliable Graph Neural Networks Via Robust Aggregation\" [b12]\n3. \"Graph Revised Convolutional Network\" [b43]\n4. \"Adversarial Attacks on Graph Neural Networks via Meta Learning\" [b54]\n5. \"Adversarial Attacks on Neural Networks for Graph Data\" [b53]\n6. \"Semi-supervised Classification with Graph Convolutional Networks\" [b23]\n7. \"Graph Contrastive Learning with Augmentations\" [b42]\n\nThe json result would look like this:\n\n{\n  \"b51\": 0.9,\n  \"b12\": 0.85,\n  \"b43\": 0.8,\n  \"b54\": 0.8,\n  \"b53\": 0.8,\n  \"b23\": 0.75,\n  \"b42\": 0.7\n} \n\nNote: The confidence scores are arbitrary and used for illustrative purposes only; they should be adjusted based on the level of influence each reference paper has on the paper in question.", "Based on the given text and the titles of the references, the following papers seem to be the most pertinent source papers:\n\n1. \"adversarial attacks on neural networks for graph data\" [b53] - This paper is directly relevant to the main topic of the paper, which discusses the vulnerability of GNNs to adversarial attacks. Confidence score: 0.9\n2. \"adversarial attacks on graph structured data\" [b6] - Similar to [b53], this paper discusses adversarial attacks on graph data, which aligns with the main topic of the paper. Confidence score: 0.85\n3. \"robust graph convolutional networks against adversarial attacks\" [b48] - This paper directly relates to the author's work on developing a robust GNN against adversarial attacks. Confidence score: 0.8\n4. \"graph revised convolutional network\" [b43] - The author refers to the GRCN in the text, suggesting that this paper had a significant influence on the development of their method. Confidence score: 0.75\n5. \"graph attention networks\" [b34] - The author refers to the graph attention networks in the text, indicating that it has influenced their research. Confidence score: 0.7\n6. \"gnnguard defending graph neural networks against adversarial attacks\" [b44] - This paper is relevant as it discusses defending GNNs against adversarial attacks, which is a key topic of the paper. Confidence score: 0.65\n7. \"contrastive multi view representation learning on graphs\" [b14] - The author discusses using a contrastive method in their research, which is a key topic in this paper. Confidence score: 0.6\n\nThe JSON format of the result is:\n{\"b53\": 0.9, \"b6\": 0.85, \"b48\": 0.8, \"b43\": 0.75, \"b34\": 0.7, \"b44\": 0.65, \"b14\": 0.6}", "Based on the given paper and the titles of the references, the most pertinent source papers that inspired the author's work could be:\n\n1. \"Adversarial attacks on graph neural networks via meta learning\" [b54]: This paper potentially inspired the author's focus on adversarial attacks and robustness of graph neural networks (GNNs).\n\n2. \"Graph structure learning for robust graph neural networks\" [b21]: The concept of graph structure learning for robustness of GNNs might have been influenced by this paper.\n\n3. \"Graph revised convolutional network\" [b43]: The author's design for an advanced GCN might have drawn inspiration from this paper.\n\n4. \"GNNGuard: Defending Graph Neural Networks against Adversarial Attacks\" [b44]: The defense strategies against adversarial attacks in GNNs mentioned in the given paper might have been influenced by this paper.\n\n5. \"Adversarial attacks on graph neural networks via meta learning\" [b54]: The author's interest in adversarial attacks on GNNs might have been inspired by this paper.\n\n6. \"Deep graph infomax\" [b35]: The concept of representation learning in the adversarial scenario via contrastive method could have been drawn from this paper.\n\n7. \"Graph attention networks\" [b34]: The author's usage of graph attention networks in the pipeline might have been influenced by this paper.\n\nResults in JSON format:\n\n```\n{\n    \"b54\": 0.9,\n    \"b21\": 0.8,\n    \"b43\": 0.85,\n    \"b44\": 0.85,\n    \"b35\": 0.8,\n    \"b34\": 0.75\n}\n```\nThe confidence scores are assigned based on the relevance of the source paper's title to the given paper's content.", "Based on the text and the titles of the references, the source papers that most likely inspired the given academic paper are:\n\n1. \"Adversarial attacks on graph neural networks via meta learning\" [b54]: This paper is mentioned multiple times in the context of existing adversarial attacks on graph data and seems to be a key reference in the development of the author's approach to defending against such attacks.\n\n2. \"Graph revised convolutional network\" [b43]: This paper is directly referenced in the context of using learned representations to compute edge weights, a strategy adopted by the authors in their proposed solution.\n\n3. \"Exploring structure adaptive graph learning for robust semi supervised classification\" [b10]: The title of this paper suggests that it might be relevant to the authors' work, given their focus on robustness and structure learning in graph neural networks.\n\n4. \"GNNGuard: Defending Graph Neural Networks against Adversarial Attacks\" [b44]: This paper is mentioned in the context of how edge weights are modeled, which is a central aspect of the authors' proposed solution.\n\n5. \"Adversarial Attack on Graph Structured Data\" [b6]: This paper is cited in the context of the vulnerabilities of Graph Neural Networks to adversarial attacks, which is the main problem the authors are trying to address.\n\n6. \"Graph structure learning for robust graph neural networks\" [b21]: The title suggests that this paper is possibly a key reference for the authors' own work on structure learning for robust graph neural networks.\n\n7. \"Graph Attention Networks\" [b34]: This paper is cited in the context of the success of Graph Neural Networks, which the authors are building upon in their work.\n\n8. \"Contrastive Multi-View Representation Learning on Graphs\" [b14]: This paper seems to be relevant as it deals with contrastive learning, a method used by the authors in their proposed solution.\n\nHere is the result in JSON format:\n{\n    \"b54\": 0.95,\n    \"b43\": 0.9,\n    \"b10\": 0.85,\n    \"b44\": 0.9,\n    \"b6\": 0.85,\n    \"b21\": 0.85,\n    \"b34\": 0.8,\n    \"b14\": 0.85\n}", "Based on the text of the paper, the following references seem to be the most influential:\n\n1. \"b6\": \"Adversarial attack on graph structured data\" - The paper mentions adversarial attacks on GNNs (Graph Neural Networks) multiple times and seems to be focused on defending against these attacks. This indicates that the work of [b6] is a major inspiration for this paper.\n\n2. \"b23\": \"Semi-supervised classification with graph convolutional networks\" - The paper mentions GCN (Graph Convolution Networks) multiple times and seems to be focused on improving their robustness. This suggests that the work of [b23] is a significant inspiration for this paper.\n\n3. \"b44\": \"GNNGuard: Defending Graph Neural Networks against Adversarial Attacks\" - The paper specifically mentions GNN-Guard as a method of modelling edge weights in the context of adversarial attacks. This suggests that [b44] is a direct inspiration for this paper.\n\n4. \"b54\": \"Adversarial attacks on graph neural networks via meta learning\" - The paper explicitly discusses the performance of various models when subjected to MetaAttack, suggesting that [b54] is a direct inspiration for the techniques used in this paper.\n\n5. \"b24\": \"Adaptive Graph Convolutional Neural Networks\" - The paper mentions a modified GCN that enhances the robustness of the original GCN, which is a method presented in [b24]. This shows that [b24] is a direct source of inspiration for this paper.\n\n6. \"b39\": \"Adversarial examples on graph data: Deep insights into attack and defense\" - The paper's discussion about adversarial examples and defense strategies indicates that [b39] is a key source of inspiration.\n\n7. \"b35\": \"Deep Graph Infomax\" - The paper uses contrastive learning method to learn representations, which is a technique presented in [b35]. This indicates that [b35] is a key source of inspiration for this paper.\n\n8. \"b51\": \"Deep graph structure learning for robust representations: A survey\" - The paper discusses methods for refining graph structures, which are discussed in [b51]. This suggests [b51] is a source of inspiration.\n\nThe confidence scores are assigned based on the number of times these papers are cited and the context in which they are mentioned in the text. For example, papers that are discussed in detail or compared to the proposed method are given higher confidence scores.\n\nIn the JSON format:\n```\n{\n  \"b6\": 0.9,\n  \"b23\": 0.9,\n  \"b44\": 0.8,\n  \"b54\": 0.8,\n  \"b24\": 0.7,\n  \"b39\": 0.7,\n  \"b35\": 0.6,\n  \"b51\": 0.6\n}\n```"], "62f07ec290e50fcafde5ad10": ["{\n  \"b29\": 1,\n  \"b27\": 1,\n  \"b59\": 1,\n  \"b86\": 0.9,\n  \"b64\": 0.9\n}", "{\n    \"b29\": 0.95,\n    \"b27\": 0.95,\n    \"b86\": 0.9,\n    \"b59\": 0.85\n}", "{\n  \"b29\": 0.9,\n  \"b27\": 0.9,\n  \"b86\": 0.85,\n  \"b59\": 0.8\n}", "{\n    \"b29\": 0.9,\n    \"b27\": 0.9,\n    \"b86\": 0.8,\n    \"b59\": 0.8\n}", "{\n  \"b29\": 0.95,\n  \"b27\": 0.95,\n  \"b86\": 0.95,\n  \"b59\": 0.9\n}"], "628464625aee126c0faca44e": ["```json\n{\n    \"b8\": 0.9,\n    \"b25\": 0.8,\n    \"b39\": 0.85\n}\n```", "{\n  \"b8\": 0.95,\n  \"b25\": 0.95,\n  \"b39\": 0.9\n}", "```json\n{\n    \"b8\": 0.95,\n    \"b25\": 0.9,\n    \"b39\": 0.85\n}\n```", "{\n  \"b8\": 0.9,\n  \"b25\": 0.9,\n  \"b39\": 0.9\n}", "{\n  \"b8\": 0.95,\n  \"b25\": 0.95,\n  \"b39\": 0.95\n}"], "6303545e90e50fcafd7d3b71": ["Based on the given text and titles of the references, the source papers that inspired the paper most directly are:\n\n1. \"Graph neural networks with node wise architecture\" (directly in the title)\n2. \"Simple and deep graph convolutional networks\" (b3: mentioned in the context of node-wise depth, a key concept in the paper)\n3. \"Principal neighbourhood aggregation for graph nets\" (b5: motivated the usage of a mixture of aggregators for a GNN, which the paper builds upon)\n4. \"Graph neural architecture search\" (b7: the paper seems to build upon this concept)\n5. \"Policy gnn aggregation optimization for graph neural networks\" (b16: studied in the context of node-wise depth)\n6. \"When do gnns work understanding and improving neighborhood aggregation\" (b32: studied in the context of node-wise depth)\n7. \"Search to aggregate neighborhood for graph neural network\" (b38: possibly related to the paper's core concept of searching for the optimal GNN architecture)\n8. \"Auto gnn neural architecture search of graph neural networks\" (b39: possibly related to the paper's core concept of searching for the optimal GNN architecture)\n\nThe result in the json format would be:\n\n{ \"b0\": 1, \"b3\": 0.9, \"b5\": 0.8, \"b7\": 0.8, \"b16\": 0.8, \"b32\": 0.8, \"b38\": 0.8, \"b39\": 0.8 }", "Based on the text and the titles of the references, the most influential source papers for the given paper seem to be:\n\n1. Graph Neural Networks with Node Wise Architecture\n2. Simple and Deep Graph Convolutional Networks\n3. Adaptive Universal Generalized Pagerank Graph Neural Network\n4. Principal Neighbourhood Aggregation for Graph Nets\n5. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\n6. Graph Neural Architecture Search\n7. Inductive Representation Learning on Large Graphs\n8. Policy GNN: Aggregation Optimization for Graph Neural Networks\n\nThe confidence scores for these papers are difficult to assess without access to the full texts, but they could be estimated based on the frequency and importance of their mentions in the given paper. \n\nHere is the result in json format:\n\n{ \n    \"b0\": 0.9, \n    \"b3\": 0.85, \n    \"b4\": 0.9, \n    \"b5\": 0.85, \n    \"b6\": 0.9, \n    \"b7\": 0.85, \n    \"b10\": 0.8, \n    \"b16\": 0.8\n}", "Based on the provided text, the key source papers that have inspired this paper can be characterized as follows:\n\n1. \"Simple and deep graph convolutional networks\" [b3]: This paper is referenced when discussing the concept of node-wise depth in graph neural networks, indicating that it provided a significant influence.\n\n2. \"Adaptive universal generalized pagerank graph neural network\" [b4]: The paper is mentioned when discussing the performance of the new method on diverse datasets, suggesting that it served as a key point of comparison.\n\n3. \"Principal neighbourhood aggregation for graph nets\" [b5]: This paper is referred to multiple times, especially in the discussion about the aggregator controller, indicating that it played a significant role in the development of this aspect of the paper's proposed method.\n\n4. \"Graph neural architecture search\" [b7]: This paper is mentioned in the context of the challenges faced when applying the new method to large-scale graphs, suggesting it was a major source of inspiration for the proposed solution.\n\n5. \"Policy gnn aggregation optimization for graph neural networks\" [b16]: This paper is mentioned multiple times, particularly in relation to the depth controller, suggesting it influenced the development of this aspect of the paper's proposed method.\n\n6. \"When do gnns work understanding and improving neighborhood aggregation\" [b32]: This paper is referenced in the discussion of node-wise depth, indicating it provided a significant influence.\n\n7. \"Design space for graph neural networks\" [b36]: This paper is referred to when discussing the implementation of the new method, suggesting it provided important methodological guidance.\n\n8. \"Search to aggregate neighborhood for graph neural network\" [b38]: This paper is mentioned in the context of the challenges faced when applying the new method to large-scale graphs, indicating it was a major source of inspiration for the proposed solution.\n\nSo, the result in json format is:\n\n{\n    \"b3\": 0.8,\n    \"b4\": 0.7,\n    \"b5\": 0.9,\n    \"b7\": 0.7,\n    \"b16\": 0.8,\n    \"b32\": 0.8,\n    \"b36\": 0.7,\n    \"b38\": 0.7\n}", "Based on the text of the paper, the key source papers that most inspired the author are as follows:\n\n{ \n\"b3\": 0.89,\n\"b5\": 0.98,\n\"b10\": 0.9,\n\"b16\": 0.95,\n\"b32\": 0.93,\n\"b33\": 0.88\n} \n\nThe confidence scores are based on the frequency and context of the citations, with higher scores indicating a stronger influence on the paper's primary contributions.", "Based on the given text of the academic paper and the titles of the references, the source papers that inspired this paper most likely include:\n\n1. \"graph neural networks with node wise architecture\" [b0]: This paper appears to be the primary source of inspiration for the paper under analysis, as it directly discusses the concept of node-wise architecture in graph neural networks.\n\n2. \"simple and deep graph convolutional networks\" [b3]: This paper is likely a source of the methods used in the paper under analysis, as it discusses graph convolution operations.\n\n3. \"adaptive universal generalized pagerank graph neural network\" [b4]: This paper discusses adapting graph neural networks, which is a primary topic of the paper under analysis.\n\n4. \"principal neighbourhood aggregation for graph nets\" [b5]: This paper likely inspired the aggregation methods used in the paper under analysis.\n\n5. \"convolutional neural networks on graphs with fast localized spectral filtering\" [b6]: This paper likely provided inspiration for the methods used in the paper under analysis, as it discusses convolutional neural networks on graphs.\n\n6. \"graph neural architecture search\" [b7]: This paper is likely a source of the architecture search methods used in the paper under analysis.\n\n7. \"inductive representation learning on large graphs\" [b10]: This paper likely provided methods for handling large-scale graphs in the paper under analysis.\n\n8. \"policy gnn aggregation optimization for graph neural networks\" [b16]: This paper likely provided methods for node aggregation optimization.\n\nThe confidence scores are as follows:\n\n{\"b0\": 0.9, \"b3\": 0.8, \"b4\": 0.8, \"b5\": 0.7, \"b6\": 0.7, \"b7\": 0.8, \"b10\": 0.8, \"b16\": 0.7}"], "62d16f8d5aee126c0fd82ddd": ["Based on the text of the academic paper, the key phrases that indicate a strong reliance on previous research for developing the paper's core contributions, and the titles of the references, the most pertinent source papers are:\n\n1. \"Profile-guided optimizations of Linux kernel. Previous work for Linux kernel PGO focuses on creating applicationspecific kernels and re-writing the kernel binary [b24] [b26] [b40] [b41] [b42].\"\n2. \"Profile-guided optimizations of data center applications. Large instruction footprints of modern data center applications make a compelling case for optimizing these applications in a profile-guided manner [b5] [b6] [b9] [b12] [b27].\"\n3. \"Measuring profile similarity. Prior works [b8] [b11] [b28] [b43] on measuring profile similarity and diversity primarily investigate how execution profiles vary across different inputs or versions of an application and focus on each application on its own.\"\n\nThe corresponding titles of the references are:\n\n1. \"optimizing linux kernel with bolt\" - [b24]\n2. \"profile guided specialization of an operating system kernel\" - [b26]\n3. \"system wide compaction and specialization of the linux kernel\" - [b40]\n4. \"an application oriented linux kernel customization for embedded systems\" - [b41]\n5. \"binary rewriting of an operating system kernel\" - [b42]\n6. \"memory hierarchy for web search\" - [b5]\n7. \"asmdb understanding and mitigating front end stalls in warehouse scale computers\" - [b6]\n8. \"clearing the clouds a study of emerging scale out workloads on modern hardware\" - [b9]\n9. \"profiling a warehouse scale computer\" - [b12]\n10. \"softsku optimizing server architectures for microservice diversity scale\" - [b27]\n11. \"autofdo automatic feedback directed optimization for warehouse scale applications\" - [b8]\n12. \"profile inference revisited\" - [b11]\n13. \"predicting program behavior using real or estimated profiles\" - [b28]\n14. \"profmig a framework for flexible migration of program profiles across software versions\" - [b43]\n\nTherefore, the source papers that inspired the author the most are [b24], [b26], [b40], [b41], [b42], [b5], [b6], [b9], [b12], [b27], [b8], [b11], [b28], and [b43].\n\nThe JSON format is:\n\n{\n  \"b24\": 0.8,\n  \"b26\": 0.8,\n  \"b40\": 0.8,\n  \"b41\": 0.8,\n  \"b42\": 0.8,\n  \"b5\": 0.8,\n  \"b6\": 0.8,\n  \"b9\": 0.8,\n  \"b12\": 0.8,\n  \"b27\": 0.8,\n  \"b8\": 0.8,\n  \"b11\": 0.8,\n  \"b28\": 0.8,\n  \"b43\": 0.8\n}", "Based on the text of the paper, the following references are the source papers that inspire the author the most:\n\n1. \"Modern data center applications lose significant performance potential due to frequent I-cache and I-TLB misses.\" - This statement suggests that the paper entitled \"Memory hierarchy for web search\" [b5] and \"asmdb understanding and mitigating front end stalls in warehouse scale computers\" [b6] were influential to the author's research. \n\n2. The author notes that \"Large instruction footprints of modern data center applications make a compelling case for optimizing these applications in a profile-guided manner\" - This indicates that the works \"Profile-guided optimizations of data center applications\" [11, 13, 17, 19-26, 28, 31] have influenced the paper.\n\n3. The concept of a \"universal\" kernel profile seems to be derived from the paper \"Profile-guided optimizations of Linux kernel\" [b4] [b26] [b40] [b41] [b42]. \n\n4. The author's use of similarity metrics has been influenced by the papers \"Measuring profile similarity\" [b8] [b11] [b28] [b43].\n\n5. The author notes that \"we conclude that profile-guided basic-block reordering for the kernel using only one application's profile will still provide a substantial performance benefit for a different application.\" - This suggests that the paper \"Profile-guided basic-block reordering\" [b8] has influenced the author's work.\n\nBased on these observations, the source papers that inspired the author the most are:\n\n{\"b5\": 0.9, \"b6\": 0.9, \"11\": 0.8, \"13\": 0.8, \"17\": 0.8, \"19\": 0.8, \"20\": 0.7, \"26\": 0.7, \"28\": 0.7, \"31\": 0.7, \"b4\": 0.9, \"b26\": 0.9, \"b40\": 0.9, \"b41\": 0.9, \"b42\": 0.9, \"b8\": 0.9, \"b11\": 0.8, \"b28\": 0.8, \"b43\": 0.8}\n\nPlease note that the confidence scores are subjective estimates based on the key phrases and context in the paper.", "Based on the provided information and the text from the academic paper, the source papers that most inspired this paper are:\n\n1. \"memory hierarchy for web search\" [b5]\n2. \"asmdb understanding and mitigating front end stalls in warehouse scale computers\" [b6]\n3. \"autofdo automatic feedback directed optimization for warehouse scale applications\" [b8]\n4. \"clearing the clouds a study of emerging scale out workloads on modern hardware\" [b9]\n5. \"profiling a warehouse scale computer\" [b12]\n6. \"optimizing function placement for large scale data center applications\" [b21]\n7. \"bolt a practical binary optimizer for data centers and beyond\" [b22]\n8. \"lightning bolt powerful fast and scalable binary optimization\" [b23]\n\nThe confidence scores for these references are assumed to be high but cannot be calculated without a specific numerical method. Therefore, the final results in JSON format could be:\n\n{\n  \"b5\": 1,\n  \"b6\": 1,\n  \"b8\": 1,\n  \"b9\": 1,\n  \"b12\": 1,\n  \"b21\": 1,\n  \"b22\": 1,\n  \"b23\": 1\n} \n\nThe confidence score is set to 1 for each reference, indicating a high level of confidence. However, please note that a more refined calculation of the confidence score should be implemented if possible.", "Based on the text, the source papers are:\n\n1. \"memory hierarchy for web search\" [b5]\n2. \"asmdb understanding and mitigating front end stalls in warehouse scale computers\" [b6]\n3. \"clearing the clouds a study of emerging scale out workloads on modern hardware\" [b9]\n4. \"profiling a warehouse scale computer\" [b12]\n5. \"autofdo automatic feedback directed optimization for warehouse scale applications\" [b8]\n6. \"optimizing linux kernel with bolt\" [b4]\n7. \"optimizing function placement for large scale data center applications\" [b21]\n8. \"bolt a practical binary optimizer for data centers and beyond\" [b22]\n\nThese papers are inferred as the sources as they are directly connected with the main contributions of the paper: the optimization of the Linux kernel and profile-guided optimizations (PGO). Also, the terms of \"inspired by\", \"motivated by\", \"following.. we adopt ... to solve the challenge/problem\" are found around these papers. \n\nThe confidence score for each paper based on the number of mentions, the context of the mentions, and their direct relevance to the paper's novel contributions are estimated as:\n\n1. \"memory hierarchy for web search\" [b5] - 0.9\n2. \"asmdb understanding and mitigating front end stalls in warehouse scale computers\" [b6] - 0.85\n3. \"clearing the clouds a study of emerging scale out workloads on modern hardware\" [b9] - 0.8\n4. \"profiling a warehouse scale computer\" [b12] - 0.85\n5. \"autofdo automatic feedback directed optimization for warehouse scale applications\" [b8] - 0.9\n6. \"optimizing linux kernel with bolt\" [b4] - 0.95\n7. \"optimizing function placement for large scale data center applications\" [b21] - 0.8\n8. \"bolt a practical binary optimizer for data centers and beyond\" [b22] - 0.95\n\nSo the answer in the json format is:\n\n{\n\"b5\": 0.9, \n\"b6\": 0.85, \n\"b9\": 0.8, \n\"b12\": 0.85, \n\"b8\": 0.9, \n\"b4\": 0.95, \n\"b21\": 0.8, \n\"b22\": 0.95\n}", "Based on the given text, the source papers that influenced the main contributions of the paper are:\n\n- b5: \"Memory hierarchy for web search\"\n- b6: \"Asmdb understanding and mitigating front end stalls in warehouse scale computers\"\n- b8: \"Autofdo automatic feedback directed optimization for warehouse scale applications\"\n- b13: \"Ripple profile guided instruction cache replacement for data center applications\"\n- b26: \"Binary rewriting of an operating system kernel\",\n- b40: \"Experiences in profile guided operating system kernel optimization\"\n\nThe json format of the result can be: \n\n{\n\"b5\": 0.9, \n\"b6\": 0.9,\n\"b8\": 0.8,\n\"b13\": 0.75,\n\"b26\": 0.7,\n\"b40\": 0.7\n}\n\nThe confidence scores are estimated based on the degree of relevance and influence these papers seem to have on the main contributions of the current paper, with 1 being the highest confidence."], "62393e7f5aee126c0f12607f": ["Based on the information provided and the instructions for identifying source papers, we can infer potential source papers that have likely inspired the given paper. The key novel contributions identified by the author notes are:\n\n1. Using axial attention to capture dependencies between triplets.\n2. Proposing a new method to address class imbalance in the context of long-tail samples.\n3. Employing knowledge distillation to address differences between manually annotated data and distantly supervised data, with the teacher and student models having the same structure but different training data.\n\nFrom the text of the academic paper, we can identify several references that are particularly relevant to these contributions:\n\n- Axial attention is related to the work on axial attention in other domains. Reference [b20] is mentioned in the context of axial attention, which indicates it could be a source paper for the axial attention module proposed in the given paper.\n\n- The Adaptive Focal Loss for addressing class imbalance seems to be inspired by the focal loss introduced in reference [b6]. As the paper proposes an enhancement to this method, it is likely a significant source paper.\n\n- The use of knowledge distillation is discussed in the context of distantly supervised data. The paper mentions [b22] in relation to the distantly supervised data, making it a likely source paper for this aspect of the paper's contributions.\n\n- Reference [b23] is mentioned multiple times, indicating that the DocRED dataset it introduces is central to the work done in the given paper.\n\n- Reference [b30] is mentioned in relation to adaptive thresholding and localized context pooling, which could have inspired the Adaptive Focal Loss method.\n\n- Reference [b27] is cited in relation to using CNN structures for encoding interactions between entity pairs, which the given paper aims to improve upon with axial attention.\n\nHere is the resulting JSON formatted list of source papers with a confidence score between 0 and 1, indicating the relevance to the paper's novel contributions:\n\n```json\n{\n  \"b6\": 0.9,\n  \"b20\": 0.8,\n  \"b22\": 0.8,\n  \"b23\": 1.0,\n  \"b27\": 0.7,\n  \"b30\": 0.7\n}\n```\n\nThe confidence scores are subjective and based on the given paper's text and the author's notes. The highest score (1.0) is given to the reference that provides the dataset central to the study, indicating its direct relevance. Other scores reflect the degree to which each reference appears to have inspired the novel contributions of the paper.", "Based on the information provided, the task is to identify the source papers that most inspired the novel contributions of the given paper. The novel contributions, as indicated by the author notes, are:\n\n1. Use of axial attention to capture dependencies between triplets.\n2. A new method to improve class imbalance under long-tail sample scenarios.\n3. Employing knowledge distillation to address differences between manually labeled data and distantly supervised data, with the teacher and student models being structurally identical.\n\nIn the provided paper text, we can look for explicit mentions of inspiration or methodological similarities. The following references seem to be directly linked to the novel contributions:\n\n- Reference [b20] is mentioned in relation to axial attention, which is a novel contribution of the paper, suggesting methodological similarity.\n- Reference [b6] is mentioned related to focal loss, which is adapted in the paper as \"Adaptive Focal Loss\" to tackle the class imbalance.\n- Reference [b22] is mentioned as prior work that utilizes distantly supervised data but is improved upon with the paper's knowledge distillation approach.\n- Reference [b30] is mentioned with respect to the work related to adaptive thresholding, which could have inspired the proposed \"Adaptive Focal Loss\".\n\nOther references are mentioned as part of the background or related work, but without explicit statements indicating that they inspired the main novel contributions. Therefore, the following JSON object lists the identified references and assigns confidence scores based on the criteria outlined:\n\n```json\n{\n  \"b20\": 0.9,\n  \"b6\": 0.9,\n  \"b22\": 0.8,\n  \"b30\": 0.8\n}\n```\n\nThe confidence scores are subjective estimates indicating how strongly each reference is associated with the novel contributions based on the text's context. They range from 0 (no confidence) to 1 (absolute confidence). References [b20] and [b6] receive a higher confidence score due to their direct mention in relation to key novel methods. Reference [b22] also receives a high score due to its mention in the context of improvement over previous work. Reference [b30] receives a slightly lower score as it is related to a method that is adapted rather than a direct inspiration for a novel contribution.", "Based on the provided author notes and text of the paper, we can infer that the source papers that most likely inspired the novel contributions of the given paper are those related to the following key points:\n\n1. \u4f7f\u7528\u8f74\u5411\u6ce8\u610f\u529b\u6765\u6355\u83b7\u4e09\u5143\u7ec4\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb (Use of axial attention to capture dependencies between triplets): This suggests that papers related to axial attention mechanisms may be relevant, such as [b20] \"Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation\".\n\n2. \u5728\u7c7b\u522b\u4e0d\u5747\u8861\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u51fa\u65b0\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\uff0c\u6765\u89e3\u51b3\u957f\u5c3e\u6837\u672c\u4e0d\u5145\u5206\u60c5\u51b5 (Proposal of a new method to improve the situation of class imbalance, specifically addressing the issue of insufficient long-tail samples): This may refer to methods dealing with class imbalance and long-tail distributions, possibly including [b6] \"Focal Loss for Dense Object Detection\" which introduces Focal Loss to address class imbalance.\n\n3. \u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u7684\u65b9\u5f0f\uff0c\u6765\u89e3\u51b3\u4eba\u5de5\u6807\u8bb0\u6570\u636e\u548c\u8fdc\u7a0b\u76d1\u7763\u6570\u636e\u7684\u5dee\u5f02 (Using knowledge distillation to address the differences between manually annotated data and distantly supervised data): This indicates that papers discussing knowledge distillation and its application to relation extraction could be relevant, such as [b22] \"Entity Structure Within and Throughout: Modeling Mention Dependencies for Document Level Relation Extraction\".\n\nBy evaluating the titles of each reference, the following references seem to be the most central to the novel contributions of the paper:\n\n- [b20]: Axial attention model inspiration.\n- [b6]: Addressing the long-tail class imbalance with a new loss function.\n- [b22]: Knowledge distillation for leveraging distantly supervised data.\n\nHere is the result in the JSON format with confidence scores based on the inferred relevance of each reference:\n\n```json\n{\n  \"b20\": 0.9,\n  \"b6\": 0.9,\n  \"b22\": 0.9\n}\n```\n\nThe confidence scores are high (0.9) for these references since they directly relate to the novel contributions and methodologies mentioned in the author's notes. The actual paper's text might have more references that are relevant, but based on the provided criteria and information, these appear to be the most pertinent source papers.", "Based on the provided information and the task's criteria, the following JSON object is constructed to represent the source papers with their respective confidence scores. The confidence scores are subjective estimations based on the information given and are not calculated by a specific algorithm.\n\n```json\n{\n  \"b23\": 0.9,\n  \"b5\": 0.8,\n  \"b6\": 0.9,\n  \"b22\": 0.95,\n  \"b30\": 0.85,\n  \"b20\": 0.7\n}\n```\n\nExplanation of Confidence Scores:\n\n- `b23` (\"DocRED: A Large-Scale Document-Level Relation Extraction Dataset\"): The paper mentions conducting experiments on this dataset, thus it is highly relevant to their work. Confidence is high (0.9) because it is directly mentioned in the context of the novel contribution related to datasets used.\n  \n- `b5` (\"MRN: A Locally and Globally Mention-based Reasoning Network for Document-Level Relation Extraction\"): Mentioned in the context of prior methods that use CNN structures, which the paper seeks to improve upon with axial attention. Confidence is high (0.8) as it indicates a direct relationship to a challenge they addressed.\n  \n- `b6` (\"Focal Loss for Dense Object Detection\"): The proposed use of Adaptive Focal Loss in the paper suggests a strong methodological influence from the focal loss concept. Confidence is high (0.9) because it is a foundation for their novel loss function.\n  \n- `b22` (\"Entity Structure Within and Throughout: Modeling Mention Dependencies for Document-Level Relation Extraction\"): This reference is given prominence in the context of knowledge distillation and the use of distantly supervised data, directly influencing their novel contributions. Confidence is very high (0.95) as it appears to be a key motivator for their work.\n  \n- `b30` (\"Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling\"): Mentioned in relation to addressing the class imbalance problem and is a foundation for their Adaptive Focal Loss. Confidence is quite high (0.85) due to its methodological similarity.\n  \n- `b20` (\"Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation\"): Referenced in the context of axial attention, which the paper applies to capture dependencies among relation triplets. Confidence is moderate (0.7) because while it influences their approach, it is not as central as others.\n\nThe confidence scores reflect the degree to which each paper seems to have had a direct influence on the novel contributions of the paper being analyzed. The scores are on a scale of 0 to 1, with higher values indicating stronger influence.", "Based on the information provided and the instructions to focus on the novel contributions and methodological similarities, it is possible to infer which source papers may have most directly inspired the work described in the given paper. The novel contributions outlined by the authors are:\n\n1. The use of axial attention to capture dependencies between triplets.\n2. Proposing a new method to improve class imbalance situations, specifically dealing with long-tail samples.\n3. Employing knowledge distillation to address the discrepancy between human-annotated and distantly supervised data, where the teacher model's predictions on distantly supervised data are used as soft labels to pre-train the student model, which is then fine-tuned on human-annotated data.\n\nWhile the text does not directly mention inspiration phrases, we can infer the influences based on the novel contributions and the references mentioned in relation to these contributions.\n\nHere is a JSON format list of the source papers with their corresponding confidence scores:\n\n```json\n{\n  \"b20\": 0.9, // axial attention mechanism directly related to the novel axial attention module proposed.\n  \"b6\": 0.8,  // focal loss method influencing the proposed Adaptive Focal Loss to handle class imbalance.\n  \"b22\": 0.7, // previous work on using distantly supervised data for document-level relation extraction.\n  \"b23\": 0.6, // the dataset used in experiments, which might indicate methodological influence.\n  \"b30\": 0.6  // adaptive thresholding and localized context pooling might have influenced the adaptive components of the proposed method.\n}\n```\n\nPlease note that the confidence scores are relative estimations based on the contribution descriptions and the context in which the references are mentioned. The scores represent how likely each reference is to have directly inspired the novel contributions of the paper. The references b23 and b30 have lower confidence scores than b20 and b6 because they seem to be more about the context and datasets than about the methodology itself, while b22 is specifically about the method of adapting distantly supervised data, which is a key novel contribution of the paper."], "6293c3025d72d8000db42919": ["{\n  \"b10\": 0.95,\n  \"b42\": 0.90,\n  \"b8\": 0.85,\n  \"b46\": 0.85,\n  \"b22\": 0.80\n}", "{\n  \"b10\": 0.95,\n  \"b42\": 0.9,\n  \"b46\": 0.85,\n  \"b8\": 0.8\n}", "{\n  \"b3\": 0.9,\n  \"b12\": 0.8,\n  \"b22\": 0.8,\n  \"b42\": 0.75,\n  \"b46\": 0.7,\n  \"b8\": 0.65\n}", "{\n  \"b10\": 0.95,\n  \"b42\": 0.90,\n  \"b8\": 0.85,\n  \"b22\": 0.80,\n  \"b46\": 0.75\n}", "{\n    \"b8\": 0.9,\n    \"b10\": 0.9,\n    \"b42\": 0.8,\n    \"b46\": 0.8,\n    \"b22\": 0.7\n}"], "62de84a55aee126c0f96fbb9": ["Based on the text of the paper and the author's notes, the author has been inspired by several sources to develop their novel method, SimGCL, for recommendation tasks. The paper primarily challenges the necessity of data augmentation in contrastive learning for recommender systems and proposes a new method that uses InfoNCE loss as a basis.\n\nThe key references that the author appears to have been particularly inspired by include:\n\n1. \"representation learning with contrastive predictive coding\" [b17]: The author specifically mentions the use of InfoNCE loss from this paper as a crucial factor in their method.\n   \n2. \"self-supervised graph learning for recommendation\" [b27]: This paper is frequently referred to as a typical way to apply contrastive learning to recommendation. \n\n3. \"understanding contrastive representation learning through alignment and uniformity on the hypersphere\" [b23]: The author uses the approach from this paper to measure the uniformity of representation, which is a key part of their method.\n\n4. \"explaining and harnessing adversarial examples\" [b7]: The author uses the concept of adding imperceptibly small perturbation (noise) to the input images, which is a key part of their approach.\n\n5. \"a simple framework for contrastive learning of visual representations\" [b4]: This paper is referred to in the context of contrastive learning, which is a major theme of the author's paper.\n\n6. \"lightgcn simplifying and powering graph convolution network for recommendation\" [b8]: This paper is referred to as the backbone of the graph encoder, which plays an important role in the author's method.\n\nBased on the above analysis, the JSON format would look like the following:\n\n{\n  \"b17\": 1,\n  \"b27\": 0.9,\n  \"b23\": 0.9,\n  \"b7\": 0.8,\n  \"b4\": 0.8,\n  \"b8\": 0.8\n}", "Based on the text of the academic paper, the primary challenges outlined in the paper are regarding the necessity of graph augmentations when integrating contrastive learning with recommendation systems, and whether there are more effective and efficient augmentation approaches. The author of the paper proposes a new algorithm, SimGCL, that introduces a graph-augmentation-free contrastive learning method for recommendation systems, which can regulate the uniformity of representation distribution in a smooth way.\n\nThe source papers that inspired the author most, based on the text, may include:\n\n1. \"Learning representations by maximizing mutual information across views\" [b0]: The author cites this paper when discussing how their proposed method helps learn representations invariant to the disturbance factors. The author also uses the idea of maximizing mutual information, which is a key concept in this source paper.\n\n2. \"A simple framework for contrastive learning of visual representations\" [b4]: This paper is cited when the author talks about the resurgence of contrastive learning in deep representation learning. It seems like the author was heavily inspired by the framework proposed in this paper.\n\n3. \"LightGCN: simplifying and powering graph convolution network for recommendation\" [b8]: This paper is referenced multiple times throughout the text, particularly in the context of the graph encoder used in their proposed method. The author also uses this paper's methodology as a baseline for comparison.\n\n4. \"Representation learning with contrastive predictive coding\" [b17]: The author references this paper when discussing the contrastive loss InfoNCE, which learns more uniform user/item representations. This seems to be a key part of the author's proposed method.\n\n5. \"Understand contrastive representation learning through alignment and uniformity on the hypersphere\" [b23]: This paper is referenced when the author discusses how the contrastive loss influences the uniformity of the representation.\n\n6. \"Self-supervised graph learning for recommendation\" [b27]: This paper seems to be a source of inspiration for the author's proposed method, especially when discussing how to apply contrastive learning to recommendation systems.\n\nHere is the return in the json format:\n{\n  \"b0\": 0.8,\n  \"b4\": 0.9,\n  \"b8\": 0.95,\n  \"b17\": 0.85,\n  \"b23\": 0.75,\n  \"b27\": 0.9\n}", "Based on the given text, the paper is primarily focused on the challenges and potential improvements in contrastive learning (CL) for recommendation systems. The authors propose a new method, SimGCL, which is inspired by previous works in the field. The source papers that seem to have influenced the paper's contributions include:\n\n1. \"Learning Representations by Maximizing Mutual Information Across Views\" [b0]: The authors of the current paper mention this as the source of the concept of learning representations invariant to disturbance factors.\n\n2. \"A Simple Framework for Contrastive Learning of Visual Representations\" [b4]: This work is referenced in relation to the major advances CL has led in multiple research fields.\n\n3. \"LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation\" [b8]: The authors use LightGCN as the graph encoder in their proposed method, SimGCL.\n\n4. \"Representation Learning with Contrastive Predictive Coding\" [b17]: This paper is referenced as the source of the contrastive loss InfoNCE, which the authors of the current paper claim to be a key factor influencing recommendation performance.\n\n5. \"Understanding Contrastive Representation Learning Through Alignment and Uniformity on the Hypersphere\" [b23]: The authors mention this paper in relation to the uniformity of the learned representation in CL-based recommendation methods.\n\n6. \"Self-Supervised Graph Learning for Recommendation\" [b27]: This paper is referenced as an example of the typical way to apply CL to recommendation.\n\nThe confidence score for each of these papers could be assessed based on how directly they seem to have influenced the main contributions of the current paper.\n\n{\"b0\": 0.8, \"b4\": 0.9, \"b8\": 0.9, \"b17\": 1.0, \"b23\": 0.8, \"b27\": 1.0}", "Based on the text, the author's main contributions are about the development of SimGCL, a graph-augmentation-free contrastive learning (CL) method for recommendation systems. The author claims that this method can regulate the uniformity of the learned representations in a smoother way compared to other CL methods that use graph augmentations. The author was inspired by adversarial examples and contrastive learning.\n\nThe following papers could be the sources:\n\n1. 'Contrastive learning for debiased candidate generation in large scale recommender systems' [b43]: The author mentions contrastive learning and debiasing in the context of recommendation systems, which is a significant theme in the paper.\n2. 'Self supervised graph learning for recommendation' [b27]: This paper is referenced multiple times when the author discusses the usage of contrastive learning in recommendation systems, and it's also mentioned specifically in the context of using dropout-based graph augmentations, which the author's method aims to replace.\n3. 'Representation learning with contrastive predictive coding' [b17]: This paper seems to be where the CL loss (InfoNCE loss) used by the author's method originates from.\n4. 'A simple framework for contrastive learning of visual representations' [b4]: This paper might have inspired the author's focus on contrastive learning, and is cited multiple times in the text.\n5. 'Bias and debias in recommender system a survey and future directions' [b3]: The author discusses debiasing in the context of recommendation systems, which is also a major theme in this paper.\n6. 'Self supervised learning for recommendation with mutual information maximization' [b44]: This paper might have provided insights into the author's development of a self-supervised learning method for recommendation systems.\n7. 'Explaining and harnessing adversarial examples' [b7]: This paper might have inspired the author's idea of adding random noises to the representations for data augmentation, which is a key part of their method.\n\nThe JSON format result is:\n{\n    \"b43\": 0.8,\n    \"b27\": 0.9,\n    \"b17\": 0.7,\n    \"b4\": 0.7,\n    \"b3\": 0.6,\n    \"b44\": 0.7,\n    \"b7\": 0.8\n}", "{\"b0\": 0.8, \"b4\": 0.85, \"b8\": 0.95, \"b17\": 0.9, \"b23\": 0.7, \"b27\": 0.9}"], "63180bf590e50fcafded784e": ["Based on the text of the paper and the titles of the references, the source papers that appear to have inspired this paper most directly are:\n\n- b3: 'graph contrastive learning with augmentations'\n- b23: 'are graph augmentations necessary simple graph contrastive learning for recommendation'\n- b27: 'lightgcn simplifying and powering graph convolution network for recommendation'\n- b30: 'understanding contrastive representation learning through alignment and uniformity on the hypersphere'\n- b11: 'selfsupervised graph learning for recommendation'\n\nThis is determined by the explicit mentions of these papers in the text, their relevance to the novel contributions of the paper, and their methodological similarity.\n\nHere's the result in JSON format:\n\n{\n\"b3\": 0.9,\n\"b23\": 0.9,\n\"b27\": 0.8,\n\"b30\": 0.8,\n\"b11\": 0.7\n}", "Based on the text, the paper heavily relies on several source papers for its core contributions. The source papers are:\n\n1. \"Graph contrastive learning with augmentations\" (b3) - The author frequently mentions contrastive learning and graph augmentations, indicating a strong reliance on the methodologies proposed in this paper.\n\n2. \"Self-supervised graph learning for recommendation\" (b11) - The author specifically states that his new model, XSimGCL, is based on the self-supervised graph learning model proposed in this paper.\n\n3. \"LightGCN: simplifying and powering graph convolution network for recommendation\" (b27) - The author compares the performance of their model with LightGCN, indicating strong methodological similarity.\n\n4. \"Are graph augmentations necessary? Simple graph contrastive learning for recommendation\" (b23) - The author refers to their preliminary study, which is the basis of the current paper.\n\n5. \"Understanding contrastive representation learning through alignment and uniformity on the hypersphere\" (b30) - The author uses the findings of this paper to explain the distribution of the learned feature.\n\n6. \"Learning spreadout local feature descriptors\" (b36) - The author uses the theoretical justification provided in this paper to explain the uniformity of the learned representation distribution.\n\nIn JSON format, this would look like:\n\n{\"b3\": 0.9, \"b11\": 0.8, \"b27\": 0.7, \"b23\": 1.0, \"b30\": 0.8, \"b36\": 0.8}", "Based on the text and the titles of the references, the source papers that inspire the given paper most seem to be:\n\n1. \"graph contrastive learning with augmentations\" [b3]: The text has a lengthy discussion about graph augmentations and their effectiveness, which suggests that this paper played a significant role in shaping the author's work.\n2. \"selfsupervised graph learning for recommendation\" [b11]: This paper is mentioned multiple times in the text, which indicates it influenced the author's study significantly.\n3. \"learning representations by maximizing mutual information across views\" [b18]: The text discusses the importance of mutual information extensively, suggesting this paper heavily influenced the author's work.\n4. \"representation learning with contrastive predictive coding\" [b21]: This paper is mentioned in the context of the contrastive loss In-foNCE, which is a main component of the author's algorithm.\n5. \"are graph augmentations necessary simple graph contrastive learning for recommendation\" [b23]: The text references this paper when discussing the author's preliminary study and its findings.\n6. \"lightgcn simplifying and powering graph convolution network for recommendation\" [b27]: The text describes this as a conventional recommendation model for comparison, suggesting it was a significant source paper.\n7. \"understanding contrastive representation learning through alignment and uniformity on the hypersphere\" [b30]: This paper is referenced when discussing the properties of pre-training with the In-foNCE loss, suggesting it was a source of inspiration.\n\nTo summarize these results in json format:\n\n{\n    \"b3\": 0.9,\n    \"b11\": 0.9,\n    \"b18\": 0.8,\n    \"b21\": 0.8,\n    \"b23\": 0.9,\n    \"b27\": 0.8,\n    \"b30\": 0.7\n} \n\nThese confidence scores range from 0.7 to 0.9, suggesting a high degree of relevance to the given paper.", "Based on the information provided, it is clear that the paper is heavily influenced by theories and methodologies from the field of contrastive learning, particularly as applied to recommendation systems. The authors have specifically highlighted certain papers and methods that have inspired their work. \n\nThe following are the identified source papers with their respective confidence levels:\n\n1. \"Selfsupervised graph learning for recommendation\" [b11]: Confidence Score - 1.0\n2. \"S rec self supervised learning for sequential recommendation with mutual information maximization\" [b14]: Confidence Score - 0.8\n3. \"Representation learning with contrastive predictive coding\" [b21]: Confidence Score - 0.9\n4. \"Bootstrap your own latent a new approach to selfsupervised learning\" [b7]: Confidence Score - 0.7\n5. \"Are graph augmentations necessary simple graph contrastive learning for recommendation\" [b23]: Confidence Score - 1.0\n6. \"Lightgcn simplifying and powering graph convolution network for recommendation\" [b27]: Confidence Score - 1.0\n7. \"Understanding contrastive representation learning through alignment and uniformity on the hypersphere\" [b30]: Confidence Score - 0.8\n\nThese papers are prioritized as they either directly inspired the paper's major contributions or shared strong methodological similarities with the paper.", "Based on the text of the paper and the titles of the references, the most pertinent source papers seem to be:\n\n- \"A simple framework for contrastive learning of visual representations\" [b4]\n- \"Self-supervised graph learning for recommendation\" [b11]\n- \"Are graph augmentations necessary simple graph contrastive learning for recommendation\" [b23]\n- \"LightGCN simplifying and powering graph convolution network for recommendation\" [b27]\n- \"Understanding contrastive representation learning through alignment and uniformity on the hypersphere\" [b30]\n- \"Learning spreadout local feature descriptors\" [b36]\n\nThe json format of the result would be:\n\n{\n    \"b4\": 1,\n    \"b11\": 0.9,\n    \"b23\": 1,\n    \"b27\": 0.8,\n    \"b30\": 0.8,\n    \"b36\": 0.7\n}"], "628c4ce25aee126c0ff59b39": ["Based on the text, the following source papers seem to be the most influential:\n\n1. \"Graph Isomorphic Neural Networks\" - This paper is directly mentioned as a source of inspiration for the development of the Linear Spectral Graph Neural Network Framework. However, the exact reference number or full title is not provided in the text.\n\n2. \"b8\" - Adaptive Universal Generalized Pagerank Graph Neural Network. The paper's authors mention this work when discussing the ability of Spectral GNNs to work on homophilic and heterophilic graphs. \n\n3. \"b34\" - How powerful are graph neural networks. This work is referenced in the discussion of the expressive power of Spectral GNNs and Graph Isomorphism (GI) testing.\n\n4. \"b9\" - Convolutional neural networks on graphs with fast localized spectral filtering. This paper is mentioned when discussing Spectral GNNs with learnable filters.\n\n5. \"b2\" - Graph neural networks with convolutional arma filters. This paper is mentioned when discussing Spectral GNNs with learnable filters.\n\n6. \"b13\" - Bernnet learning arbitrary graph spectral filters via bernstein approximation. This paper is mentioned when discussing Spectral GNNs with learnable filters.\n\n7. \"b1\" - Analyzing the expressive power of graph neural networks in a spectral perspective. This is inferred from the title and the paper's focus on the expressive power of GNNs.\n\n8. \"b33\" - A comprehensive survey on graph neural networks. This is inferred from the title and the paper's broad discussion of GNNs, which likely provided a foundational understanding for the authors.\n\nThe confidence scores can't be determined from the information given, but these seem to be the most relevant papers based on the text provided.\n\nAnswer in JSON format:\n\n{\n\"Graph Isomorphic Neural Networks\": \"0\",\n\"b8\": \"0.8\",\n\"b34\": \"0.7\",\n\"b9\": \"0.6\",\n\"b2\": \"0.6\",\n\"b13\": \"0.6\",\n\"b1\": \"0.5\",\n\"b33\": \"0.5\"\n}", "Based on the given information, the source papers that inspire the given paper most are:\n\n1. \"Graph Isomorphic Neural Networks\" is directly mentioned in the author's notes as a source of inspiration, but its reference number is not given in the text or the title list.\n2. \"Graph Neural Networks with Convolutional ARMA Filters\" (b2) is not directly indicated as a source of inspiration but its concepts seem to be relevant to the topic of the paper.\n3. \"Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\" (b9) is also not directly indicated as a source of inspiration but its concepts seem to be relevant.\n4. \"BernNet: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation\" (b13) is not directly indicated as a source of inspiration but its concepts seem to be relevant.\n5. \"Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective\" (b1) is indicated to be related to the topic of the paper and may have been a source of inspiration.\n6. \"How Powerful are Spectral Graph Neural Networks\" (None) seems to be the title of the paper itself.\n\nHere is the result in JSON format:\n\n{\n  \"None\": 0.8,\n  \"b2\": 0.7,\n  \"b9\": 0.7,\n  \"b13\": 0.7,\n  \"b1\": 0.7\n}\n\nNote: The \"None\" key refers to the paper itself. Since the title \"Graph Isomorphic Neural Networks\" is not in the list, we could not assign a reference number to it. Similarly, the confidence scores are assigned based on my understanding of the relevance of each paper, and are subjective.", "Based on the analysis of the text and author notes, the following references seem to be the most pertinent source papers:\n\n1. \"Analyzing the expressive power of graph neural networks in a spectral perspective\" [b1]\n2. \"Graph neural networks with convolutional arma filters\" [b2]\n3. \"Adaptive universal generalized pagerank graph neural network\" [b8]\n4. \"Convolutional neural networks on graphs with fast localized spectral filtering\" [b9]\n5. \"Bernnet learning arbitrary graph spectral filters via bernstein approximation\" [b13]\n\nThe JSON format of the result is:\n\n{\"b1\": 0.9, \"b2\": 0.85, \"b8\": 0.8, \"b9\": 0.75, \"b13\": 0.85}", "Based on the text, the most pertinent source papers that have inspired the author are:\n\n1. \"Graph Isomorphic Neural Networks\" - The author states that they were inspired by the ideas in this paper to develop their work. However, the exact reference number for this paper is not provided in the given text.\n\n2. 'b8': \"Adaptive universal generalized pagerank graph neural network\" - The author mentions this paper when discussing the different types of GNNs and their filters.\n\n3. 'b2': \"Graph neural networks with convolutional arma filters\" - This paper is referenced in the discussion of spectral GNNs with learnable filters.\n\n4. 'b13': \"Bernnet learning arbitrary graph spectral filters via bernstein approximation\" - This paper is also mentioned in the discussion of spectral GNNs with learnable filters.\n\n5. 'b9': \"Convolutional neural networks on graphs with fast localized spectral filtering\" - This paper is mentioned while discussing spectral GNNs with learnable filters.\n\n6. 'b34': \"How powerful are graph neural networks\" - This paper is referenced when discussing the expressive power of GNNs.\n\n7. 'b1': \"Analyzing the expressive power of graph neural networks in a spectral perspective\" - This paper is listed under related work and might have contributed to the theoretical framework the author used to analyze their proposed model.\n\nThe confidence score for each paper is difficult to determine without more specific information from the author about the degree to which each paper influenced their work. However, based on the information provided, a hypothetical confidence score might be assigned as follows: \n\n{'b8': 0.8, 'b2': 0.7, 'b13': 0.7, 'b9': 0.7, 'b34': 0.7, 'b1': 0.6}\n\nPlease note that \"Graph Isomorphic Neural Networks\" is likely to have a high confidence score given the author's explicit mention of its influence, but a specific score cannot be assigned without knowing its reference number.", "Based on the author notes and the text of the paper, I can infer that the source papers could be:\n\n1. \"Graph Isomorphic Neural Networks\" - This paper is specifically mentioned in the author notes as having an idea that was borrowed. However, the exact reference number is not provided in the text. \n\n2. b8 - \"Adaptive universal generalized pagerank graph neural network\" - This paper is referred to in the context of spectral GNNs, which is the main topic of the paper.\n\n3. b9 - \"Convolutional neural networks on graphs with fast localized spectral filtering\" - This paper is also mentioned in the context of spectral GNNs.\n\n4. b34 - \"How powerful are graph neural networks\" - This paper is referred to in the context of spatial GNNs, which seems to be a topic relevant to the paper.\n\n5. b13 - \"BernNet learning arbitrary graph spectral filters via Bernstein approximation\" - This paper is mentioned in the context of spectral GNNs with learnable filters, which is relevant to the paper's topic.\n\n6. b2 - \"Graph neural networks with convolutional arma filters\" - This paper is also mentioned in the context of spectral GNNs with learnable filters, which is relevant to the paper's topic.\n\nHere is the JSON format:\n{\"Graph Isomorphic Neural Networks\": 1, \"b8\": 0.9, \"b9\": 0.9, \"b34\": 0.8, \"b13\": 0.7, \"b2\": 0.7}"], "62c28ae45aee126c0f8a182f": ["{\n  \"b10\": 0.95,\n  \"b15\": 0.95,\n  \"b22\": 0.85,\n  \"b24\": 0.85,\n  \"b46\": 0.90,\n  \"b16\": 0.80,\n  \"b23\": 0.75\n}", "{\n    \"b10\": 1.0,\n    \"b15\": 1.0,\n    \"b22\": 0.9,\n    \"b24\": 0.8,\n    \"b46\": 0.9,\n    \"b56\": 0.8\n}", "{\n    \"b10\": 0.9,\n    \"b15\": 0.9,\n    \"b22\": 0.8,\n    \"b23\": 0.8,\n    \"b46\": 0.8\n}", "{\n    \"b10\": 0.95,\n    \"b15\": 0.95,\n    \"b22\": 0.85,\n    \"b24\": 0.8,\n    \"b46\": 0.9\n}", "{\n    \"b10\": 0.95,\n    \"b15\": 0.95,\n    \"b22\": 0.9,\n    \"b24\": 0.9,\n    \"b46\": 0.85\n}"], "62d0db155aee126c0f9f111a": ["Based on the provided text, the following are the source papers that the paper's author indicates as inspiration or builds upon:\n\n1. \"Parameter-Efficient Transfer Learning for NLP\" [b15] - The paper discusses parameter-efficient (PE) transfer learning, including prompt tuning, adapters, and hybrid methods. They employ very few tuning parameters to achieve fine-tuning comparable transfer performance. Confidence Score: 0.9\n2. \"Prefix tuning: Optimizing continuous prompts for generation\" [b23] - The paper introduces the concept of prefix-tuning, which is being used in the paper in question. Confidence Score: 0.9\n3. \"The power of scale for parameter efficient prompt tuning\" [b22] - The paper introduces the concept of parameter efficient prompt tuning which is being used in the paper. Confidence Score: 0.8\n4. \"P-Tuning: Prompt Tuning Can Be Comparable to Fine-Tuning Across Scales and Tasks\" [b27] - The paper discusses P-Tuning v2, which is used in the paper in question. Confidence Score: 0.9\n5. \"Dense Passage Retrieval for Open Domain Question Answering\" [b19] - The paper discusses Dense Passage Retrieval (DPR) Model, which is used as the baseline in the paper in question. Confidence Score: 0.85\n6. \"Attention is all you need\" [b42] - The paper discusses the Transformer architecture, which forms the basis of the parameter-efficient learning methods used in the paper in question. Confidence Score: 0.8\n7. \"BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models\" [b40] - The paper introduces the BEIR benchmark, which is being used in the paper in question. Confidence Score: 0.7\n\nThe json format result is: \n{\n  \"b15\": 0.9,\n  \"b23\": 0.9,\n  \"b22\": 0.8,\n  \"b27\": 0.9,\n  \"b19\": 0.85,\n  \"b42\": 0.8,\n  \"b40\": 0.7\n}", "Based on the text and references of the paper, the most pertinent source papers that the author drew inspiration from are:\n\n1. \"Parameter-Efficient Transfer Learning for NLP\" [b15]\n2. \"The Power of Scale for Parameter-Efficient Prompt Tuning\" [b22]\n3. \"P-Tuning: Prompt Tuning Can Be Comparable to Fine-Tuning across Scales and Tasks\" [b27]\n4. \"Dense Passage Retrieval for Open-Domain Question Answering\" [b19]\n5. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b9]\n6. \"Attention Is All You Need\" [b42]\n7. \"BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models\" [b40]\n\nThese papers are identified based on their relevance to the methods and challenges outlined in the paper, their direct mention in relation to the author's approach and proposals, and the frequency of their citation. The confidence score is determined by the degree of direct inspiration or motivation indicated by the author's language.\n\nHere is the result in json format:\n{\n    \"b15\": 0.8,\n    \"b22\": 0.9,\n    \"b27\": 1.0,\n    \"b19\": 0.7,\n    \"b9\": 0.6,\n    \"b42\": 0.6,\n    \"b40\": 0.7\n}", "Based on the text of the paper and the titles of the references, the source papers that appear to have inspired this paper the most are:\n\n1. \"Dense passage retrieval for open domain question answering\" [b19]: This paper is cited multiple times and seems to be the basis for the Dense Passage Retrieval (DPR) Model used in the research. \n\n2. \"Parameter efficient transfer learning for NLP\" [b15]: The authors of the paper have put a strong emphasis on parameter efficiency which seems to be inspired by this paper.\n\n3. \"The power of scale for parameter efficient prompt tuning\" [b22]: This paper discusses prompt tuning, which is a method used in the study.\n\n4. \"Parameter-Efficient prompt tuning makes generalized and calibrated neural text retrievers\" [None]: The title of this paper matches closely with the subject of the paper.\n\n5. \"P-Tuning: Prompt tuning can be comparable to fine tuning across scales and tasks\" [b27]: The P-Tuning method is used and discussed in the paper.\n\n6. \"Prefix tuning optimizing continuous prompts for generation\" [b23]: The authors of the paper have studied and applied prefix tuning, which is discussed in this paper.\n\nThe JSON format of the result is:\n\n{\n    \"b19\": 1,\n    \"b15\": 0.9,\n    \"b22\": 0.9,\n    \"None\": 0.8,\n    \"b27\": 0.8,\n    \"b23\": 0.8\n}", "Based on the text and titles of the references, the source papers that most inspire this paper are:\n\n1. \"parameter efficient prompt tuning makes generalized and calibrated neural text retrievers\" [None]\n2. \"Dense passage retrieval for open domain question answering\" [b19]\n3. \"P-Tuning: prompt tuning can be comparable to fine tuning across scales and tasks\" [b27]\n4. \"The power of scale for parameter efficient prompt tuning\" [b22]\n5. \"Parameter-Efficient Transfer learning for NLP\" [b15]\n6. \"BEIR: a heterogeneous benchmark for zero-shot evaluation of information retrieval models\" [b40]\n7. \"Prefix tuning: optimizing continuous prompts for generation\" [b23]\n8. \"ColBERT: Efficient and effective passage search via contextualized late interaction over BERT\" [b20]\n\nThis is based on the following observations from the text:\n\n- The authors are inspired by the progress in Parameter-Efficient (PE) transfer learning, including prompt tuning [b23][b52][b22], adapters [b15], and hybrid methods [b16][b51], to study whether and how PE learning can benefit neural text retrieval.\n- They note that P-Tuning v2 [b27] can encourage neural text retrievers to generalize on the cross-domain benchmark BEIR [b40] and OAG-QA.\n- The authors draw on DPR [b19] for their retrieval model, they use it as a baseline to compare the performance of their proposed models.\n- The authors mention the work done on ColBERT [b20], a late-interaction model, indicating that it may have influenced their research.\n\nThe confidence score for each reference is estimated as:\n\n{\"None\": 1, \"b19\": 0.9, \"b27\": 0.9, \"b22\": 0.8, \"b15\": 0.8, \"b40\": 0.7, \"b23\": 0.7, \"b20\": 0.6}", "Based on the text of the paper and the titles of the references, the source papers that inspire the author most are:\n\n- \"b19: Dense passage retrieval for open domain question answering\"\n- \"b23: Prefix tuning optimizing continuous prompts for generation\"\n- \"b27: P tuning prompt tuning can be comparable to fine tuning across scales and tasks\"\n- \"b40: BEIR a heterogeneous benchmark for zero-shot evaluation of information retrieval models\"\n- \"b52: OAG toward linking large scale heterogeneous entity graphs\"\n\nThe JSON format of the result is:\n{\n    \"b19\": 0.9,\n    \"b23\": 0.8,\n    \"b27\": 0.9,\n    \"b40\": 0.8,\n    \"b52\": 0.8\n} \n\nThe confidence score is based on how directly the source papers are related to the novel contributions of this paper. For instance, \"b19: Dense passage retrieval for open domain question answering\" and \"b27: P tuning prompt tuning can be comparable to fine tuning across scales and tasks\" both have a high confidence score because they directly relate to the main topic of the paper, which is about improving the efficiency and generalization of neural text retrieval."], "628749345aee126c0ffeb827": ["Based on the text and the titles of the references, here are the most pertinent source papers that inspired the author:\n\n{\n\"b32\": 0.9,\n\"b33\": 0.9,\n\"b30\": 0.8,\n\"b2\": 0.7,\n\"b18\": 0.6,\n\"b1\": 0.6,\n\"b35\": 0.5,\n\"b10\": 0.5\n}\n\nExplanation:\n\n- \"b32\" and \"b33\" are directly mentioned in the text as the main references for the value taxonomy used in the paper, hence the high confidence scores.\n- \"b30\" is also directly referenced in the text multiple times and its definitions of values are used in the paper.\n- \"b2\" is mentioned as an important reference for the value-based argumentation framework.\n- \"b18\" is the source of the US argument dataset used in the paper.\n- \"b1\" is mentioned in the text as a source that hypothesized the use of values for audience-specific argument generation.\n- \"b35\" is referenced as a source for value-based argumentation.\n- \"b10\" is mentioned as a source that consolidates multiple value schemes, which seems to align with the methodology of the paper.\n \nNote: The confidence score is a subjective estimation based on the information in the text and the titles of the references. It reflects the perceived influence of each reference on the paper's main contributions.", "Based on the given text, the papers that mostly influenced are:\n\n1. \"Identifying the human values behind arguments\": This is the main topic of the paper, where the author introduces a method to identify the values behind arguments, which is the main contribution of the paper. \n2. \"Value-based argumentation\": This paper is directly mentioned as a source of inspiration in the provided text.\n3. \"Developing a meta inventory of human values\": This paper is mentioned when discussing the taxonomy of values used in the study.\n4. \"Refining the theory of basic individual values\": This paper is mentioned as the main source of the refined theory that the research is based on.\n5. \"The nature of human values\": The author uses this paper to define what values are.\n6. \"Practical reasoning using values\": This paper is referred to when discussing how values are used in argumentation.\n7. \"Modeling frames in argumentation\": This paper is mentioned when discussing how values overlap with the idea of framing in communication.\n\nThe confidence scores are assumed to be the same for each paper and are therefore set to 1. The final result in json format is:\n\n{\n\"b0\": 1,\n\"b2\": 1,\n\"b10\": 1,\n\"b33\": 1,\n\"b30\": 1,\n\"b37\": 1,\n\"None\": 1\n}", "Based on the text and the instructions, the following papers are identified as sources that inspired the given paper:\n\n1. \"Identifying the human values behind arguments\" (None)\n2. \"Modeling frames in argumentation\" (b0)\n3. \"Toward audience aware argument generation\" (b1)\n4. \"Value-based argumentation\" (b2)\n5. \"From arguments to key points: Towards automatic argument summarization\" (b4)\n6. \"Persuasion in practical argument using value based argumentation frameworks\" (b5)\n7. \"Developing a meta inventory of human values\" (b10)\n8. \"Refining the theory of basic individual values\" (b33)\n\nHere's the confidence score for each source paper:\n\n{ \"None\": 0.9, \"b0\": 0.8, \"b1\": 0.9, \"b2\": 0.9, \"b4\": 0.8, \"b5\": 0.8, \"b10\": 0.9, \"b33\": 0.9 }", "Based on the given text, the following are the most pertinent source papers:\n\n1. \"The nature of human values\" (b30): This paper is repeatedly cited and its theories are used as the basis for the presented research. It's the source for the definitions of values and the understanding of their importance in argumentation. \n2. \"Refining the theory of basic individual values\" (b33): This paper is referred to for the basis of the value taxonomy used in the research, indicating that it had a significant influence on the presented work.\n3. \"Value-based argumentation\" (b2): This paper is referenced as the source of the concept of using value systems in argumentation frameworks, which is a key part of the presented research.\n4. \"A large scale dataset for argument quality ranking construction and analysis\" (b18): This paper is the source of the data used in the US part of the study, making it an essential reference for the presented research.\n5. \"Learning whom to trust with mace\" (b21): Although not directly mentioned in the text, it may be relevant as the authors might have used this paper's methodology to build their own approach to the problem.\n\nThe JSON format would be: \n\n{\n\"b30\": 1,\n\"b33\": 0.9,\n\"b2\": 0.9,\n\"b18\": 0.9,\n\"b21\": 0.8\n}", "Based on the text provided, the source papers that directly inspired this paper most are:\n\n- \"The nature of human values\" [b30]: This paper is referenced multiple times. The author of the given text heavily relies on definitions and concepts from this source to build their argument, indicating that this source has directly inspired the paper.\n\n- \"Refining the theory of basic individual values\" [b33]: This source is referenced several times as well. The author of the given text refers to this paper as the basis for the refined theory that their research is largely based on.\n\n- \"Value-based argumentation\" [b2]: This source is referenced multiple times in the context of the use of values in formal argumentation, which is a key component of the given paper's content.\n\n- \"A large scale dataset for argument quality ranking construction and analysis\" [b18]: This source is referenced in the context of the dataset used in the given paper, suggesting that this source has influenced the methodology used in the given paper.\n\nBased on the information provided, other sources such as [b1], [b6], [b10], and [b16] have provided secondary inspiration or context, but do not appear to have influenced the main contributions of the paper as directly as the four sources listed above. \n\nTherefore, the JSON format of the result would look like this:\n{\n\"b30\": 0.9,\n\"b33\": 0.8,\n\"b2\": 0.7,\n\"b18\": 0.7\n} \n\nThe confidence score is based on the frequency and context of the references made to each source in the text. A higher score indicates a greater level of direct inspiration from the source."], "621635aa91e011b46d7ce129": ["{\n  \"b4\": 0.95,\n  \"b47\": 0.95,\n  \"b12\": 0.90,\n  \"b32\": 0.85,\n  \"b41\": 0.85,\n  \"b42\": 0.85\n}", "{\n  \"b4\": 0.95,\n  \"b12\": 0.95,\n  \"b47\": 0.95,\n  \"b41\": 0.90,\n  \"b32\": 0.90,\n  \"b42\": 0.85\n}", "{\n  \"b4\": 0.9,\n  \"b12\": 0.9,\n  \"b47\": 0.9\n}", "{\n    \"b4\": 0.95,\n    \"b12\": 0.95,\n    \"b47\": 0.95,\n    \"b41\": 0.9,\n    \"b42\": 0.9,\n    \"b32\": 0.85\n}", "{\n    \"b4\": 0.9,\n    \"b12\": 0.85,\n    \"b42\": 0.8,\n    \"b32\": 0.75,\n    \"b41\": 0.7\n}"], "62feff8d90e50fcafd599888": ["Based on the text and the provided notes, we can identify the following source papers that appear to have most influenced the paper's core contributions:\n\n1. \"debiasing neighbor aggregation for graph neural network in recommender systems\" - This is the main paper which we are analyzing, not a source paper.\n2. \"debiased explainable pairwise ranking from implicit feedback\" [b2] - Mentioned multiple times in the text and appears to have had a significant influence on the development of the proposed approach.\n3. \"unbiased learning to rank with biased feedback\" [b8] - The paper discusses IPS that has been previously applied to mitigate exposure bias, which is a key theme in this paper.\n4. \"lightgcn simplifying and powering graph convolution network for recommendation\" [b4] - This paper's methodology is used as a backbone for the GNN approach in the paper.\n5. \"the self normalized estimator for counterfactual learning\" [b19] - The paper discusses a method to address high variance of IPS which is a key theme in this paper.\n6. \"unbiased pairwise learning from implicit feedback\" [b16] - This paper is mentioned in the context of propensity score calculation, which is a crucial part of the proposed approach.\n7. \"neural graph collaborative filtering\" [b21] - The paper discusses the exploitation of multiple layers with non-linear DNN, which is mentioned as a method employed by GNNs and is relevant to the paper's methodology.\n\nBased on the paper's text, the following JSON object can be returned:\n\n{\"b2\": 0.85, \"b8\": 0.75, \"b4\": 0.90, \"b19\": 0.70, \"b16\": 0.80, \"b21\": 0.70}", "Based on the text of the paper and the titles of the referenced papers, the most pertinent source papers that have inspired the author of this paper are:\n\n1. 'debiasing neighbor aggregation for graph neural network in recommender systems' (b0): This is the main topic of the paper, so it is highly likely that this paper has inspired the author.\n\n2. 'debiased explainable pairwise ranking from implicit feedback' (b2): The author has explained the use of IPS (Inverse Propensity Scoring) in the text, which is a method used in this paper to mitigate exposure bias in recommender systems.\n\n3. 'lightgcn simplifying and powering graph convolution network for recommendation' (b4): This paper is mentioned when the author discusses GNN (Graph Neural Network) which is a key component of their proposed solution.\n\n4. 'unbiased pairwise learning from implicit feedback' (b16): The author has mentioned this paper while discussing IPS, implying its importance to the author's work.\n\n5. 'the self normalized estimator for counterfactual learning' (b19): It is mentioned when the author discusses the drawbacks of IPS, indicating its influence on the author's work.\n\n6. 'graph convolutional neural networks for web scale recommender systems' (b23): It is mentioned when the author discusses the use of GNN for recommender systems, indicating its influence on the author's work.\n\nThe results in json format:\n{\"b0\": 1, \"b2\": 0.9, \"b4\": 0.9, \"b16\": 0.8, \"b19\": 0.8, \"b23\": 0.8}", "Based on the text provided, the source papers that inspired the authors most should be the ones referenced in the sections where the authors outline the challenges and propose their novel algorithm (Neighbor Aggregation via Inverse Propensity, Navip). \n\nAnalyzing the text, the following references are directly linked to the challenges and the novel approach:\n\n1. b2 - Debiasing explainable pairwise ranking from implicit feedback: The authors mention this paper several times when discussing the problem of exposure bias and the use of Inverse Propensity Scoring (IPS) to mitigate it. This paper is crucial as it provides the IPS-based recommender learning method that the authors improve upon.\n\n2. b4 - LightGCN simplifying and powering graph convolution network for recommendation: This is the backbone GNN the authors used throughout their experiment. As such, it is a key foundational paper for their work.\n\n3. b16 - Unbiased pairwise learning from implicit feedback: This paper is mentioned when discussing IPS and its application in acquiring unbiased estimators of performance. \n\n4. b17 - Unbiased recommender learning from missing not at random implicit feedback: Also mentioned in the context of IPS and the problem of exposure bias, indicating that it was an influential source.\n\n5. b22 - Unbiased offline recommender evaluation for missing not at random implicit feedback: This paper is discussed in the context of the problem of exposure bias, underscoring its influence on the authors' work.\n\nTherefore, as per the guidelines, the references most pertinent to the paper's core contributions appear to be b2, b4, b16, b17, and b22.\n\n{\n\"b2\": 0.8,\n\"b4\": 0.8,\n\"b16\": 0.7,\n\"b17\": 0.7,\n\"b22\": 0.7\n}", "Based on the text of the academic paper and the primary challenges outlined, the paper appears to be heavily influenced by the following source papers:\n\n'b1': 'graph convolutional matrix completion' - The paper discusses the use of GNNs, which is a key method used in the given paper.\n'b2': 'debiased explainable pairwise ranking from implicit feedback' - The paper discusses the use of IPS, which is a central method used in the given paper.\n'b4': 'lightgcn simplifying and powering graph convolution network for recommendation' - The paper discusses the use of GNNs, which is a key method used in the given paper.\n'b5': 'neural collaborative filtering' - The paper discusses collaborative filtering, which is a method mentioned in the given paper.\n'b16': 'unbiased pairwise learning from implicit feedback' - The paper discusses the use of IPS, which is a central method used in the given paper.\n'b18': 'recommendations as treatments debiasing learning and evaluation' - The paper discusses debiasing, which is a key challenge outlined in the given paper.\n\nThe resultant JSON format would be: {\"b1\": 0.9, \"b2\": 0.95, \"b4\": 0.9, \"b5\": 0.85, \"b16\": 0.95, \"b18\": 0.9}", "Based on the text and author notes, the following papers seem to be the most pertinent sources for this academic paper:\n\n1. \"LightGCN simplifying and powering graph convolution network for recommendation\" [b4]\n2. \"Graph convolutional matrix completion\" [b1]\n3. \"Debiased explainable pairwise ranking from implicit feedback\" [b2]\n4. \"Unbiased learning to rank with biased feedback\" [b8]\n5. \"Unbiased pairwise learning from implicit feedback\" [b16]\n6. \"The unfairness of popularity bias in recommendation\" [b0]\n\nHere is the result in the JSON format:\n\n{\n  \"b0\": 0.8,\n  \"b1\": 0.9,\n  \"b2\": 1,\n  \"b4\": 1,\n  \"b8\": 0.9,\n  \"b16\": 0.9\n}"], "62cce6795aee126c0f2a7fcc": ["Based on the text and mentioned references in the paper, the following references seem to be the most pertinent source papers:\n\n- \"tensorir an abstraction for automatic tensorized program optimization\" [b1]: This is the core topic of the paper and seems to be the main inspiration for the algorithm proposed by the author.\n- \"tvm an automated end to end optimizing compiler for deep learning\" [b8]: The author mentions this as past work in machine learning compilation which did not handle tensorized programs automatically.\n- \"the hierarchically tiled arrays programming approach\" [b15]: The author refers to this paper several times in the text when explaining the divide and conquer approach.\n- \"a language and compiler for optimizing parallelism locality and recomputation in image processing pipelines\" [b33]: The author compares this approach to the proposed approach in the paper.\n- \"tensor comprehensions framework agnostic high performance machine learning abstractions\" [b40]: This is another past work the author refers to in the context of machine learning compilation.\n- \"unit unifying tensorized instruction compilation\" [b42]: The author seems to be influenced by this paper as it is related to tensorized instruction compilation.\n- \"stripe tensor compilation via the nested polyhedral model\" [b43]: This paper could be a source paper based on its relation to tensor compilation.\n\nThe confidence scores for each source paper are difficult to determine based on the provided text, but all of them seem to be of high relevance.\n\nTherefore, the answer in JSON format would be:\n\n{\n    \"b1\": 1,\n    \"b8\": 0.9,\n    \"b15\": 0.9,\n    \"b33\": 0.8,\n    \"b40\": 0.8,\n    \"b42\": 0.7,\n    \"b43\": 0.7\n}", "Based on the text of the paper and the titles of the references, it seems that the most relevant source papers for this paper are:\n\n1. \"TVM an automated end to end optimizing compiler for deep learning\" [b8]: The paper mentions TVM, a previous work in machine learning compilation, multiple times. It discusses the limitations of TVM and how their proposed TensorIR abstraction addresses these limitations. \n\n2. \"The hierarchically tiled arrays programming approach\" [b15]: The term \"grid\" is mentioned multiple times in the text, which is related to the concept of tiled arrays programming. This suggests that this paper may have inspired the authors in designing the TensorIR abstraction.\n\n3. \"Halide a language and compiler for optimizing parallelism locality and recomputation in image processing pipelines\" [b33]: The authors compare their approach with Halide, suggesting that it has motivated their work.\n\n4. \"Efficient primitives for deep learning\" [b10]: The paper discusses the use of tensorized primitives for optimizing machine learning models, which is a key concept in the paper.\n\n5. \"Arm compute library\" [b2]: The paper mentions this library as an example of a specialized kernel library. This suggests that it may have inspired the authors in developing their method.\n\n6. \"MLIR scaling compiler infrastructure for domain specific computation\" [b24]: This paper might have influenced their work as they refer to it while discussing the approaches taken by existing machine learning compilers.\n\n7. \"Stripe tensor compilation via the nested polyhedral model\" [b43]: The authors refer to this paper when discussing the divide and conquer approach which is a key concept in their paper.\n\nIn JSON format, the references and their confidence scores are:\n\n{\n\"b8\": 0.9,\n\"b15\": 0.8,\n\"b33\": 0.8,\n\"b10\": 0.7,\n\"b2\": 0.7,\n\"b24\": 0.7,\n\"b43\": 0.7\n}", "Based on the text of the paper, the following references seem to be the most pertinent source papers:\n\nb8: TVM, an automated end-to-end optimizing compiler for deep learning \nb33: Halide, a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines\nb46: AMOS, enabling automatic mapping for tensor computations on spatial accelerators with hardware abstraction\nb2: ARM Compute Library\nb10: NVIDIA cuDNN, efficient primitives for deep learning\nb19: Intel MKL-DNN, a kernel library for deep learning \nb31: PyTorch, an imperative style high-performance deep learning library\nb45: Ansor, generating high performance tensor programs for deep learning \n\nThese papers were frequently mentioned and are related to the challenges and solutions outlined in the paper. However, the confidence score for each source paper would require further analysis of the text. \n\nFor example:\n{\n\"b8\": 0.9,\n\"b33\": 0.8,\n\"b46\": 0.85,\n\"b2\": 0.75,\n\"b10\": 0.8,\n\"b19\": 0.75,\n\"b31\": 0.7,\n\"b45\": 0.85\n}", "Based on the text of the paper and the author's notes, the following source papers seem to have inspired the author:\n\n1. \"TVM an automated end to end optimizing compiler for deep learning\" [b8]\n2. \"Tensor comprehensions framework agnostic high performance machine learning abstractions\" [b40]\n3. \"Halide a language and compiler for optimizing parallelism locality and recomputation in image processing pipelines\" [b33]\n4. \"The hierarchically tiled arrays programming approach\" [b15]\n5. \"Efficient primitives for deep learning\" [b10]\n6. \"Intel math kernel library for deep learning networks\" [b19]\n7. \"Stripe tensor compilation via the nested polyhedral model\" [b43]\n8. \"Unit unifying tensorized instruction compilation\" [b42]\n\nBased on the level of inspiration and motivation derived from these papers, I would assign the following confidence scores:\n\n{\"b8\": 0.9, \"b40\": 0.8, \"b33\": 0.7, \"b15\": 0.8, \"b10\": 0.7, \"b19\": 0.6, \"b43\": 0.8, \"b42\": 0.7}\n\nThese confidence scores are based on the amount of detail and emphasis the author places on the concepts discussed in these papers.", "Based on the given text and the authors' notes, the paper introduces an abstraction named TensorIR for automatic tensor program optimization. It proposes a new construct called block and program transformation primitives. \n\nBy analyzing the text, the following references appear to be the most influential:\n\n1. \"b8\": \"tvm an automated end to end optimizing compiler for deep learning\"\n    - The paper mentions TVM as a previous work in machine learning compilation that does not handle tensorized programs automatically. This suggests that the authors take inspiration from TVM but aim to advance its capabilities.\n\n2. \"b33\": \"halide a language and compiler for optimizing parallelism locality and recomputation in image processing pipelines\"\n    - Halide is mentioned as an existing machine learning compiler that takes a bottom-up approach, which the authors are improving upon.\n\n3. \"b15\": \"the hierarchically tiled arrays programming approach\"\n    - This reference is directly linked with the text's mention of a divide and conquer approach, which is a key concept in the authors' proposed methodology.\n\n4. \"b39\": \"polyhedral code generation in the real world\"\n    - The authors mention the domain representations of integer sets and relations, which is a concept discussed in this paper.\n\n5. \"b19\": \"intel math kernel library for deep learning\"\n    - This library is mentioned as an example of specialized kernel libraries that require huge engineering efforts to support, a problem that the authors' work aims to address.\n\n6. \"b10\": \"efficient primitives for deep learning\"\n    - NVIDIA cuDNN is used as an example of specialized kernel libraries, which the authors aim to optimize.\n\nBased on this analysis, I would return the following JSON:\n\n{\n\"b8\": 0.9,\n\"b33\": 0.8,\n\"b15\": 0.8,\n\"b39\": 0.7,\n\"b19\": 0.7,\n\"b10\": 0.7\n}"], "628749495aee126c0fff087a": ["```json\n{\n    \"b14\": 0.9,\n    \"b31\": 0.8,\n    \"b40\": 0.9,\n    \"b42\": 0.8,\n    \"b9\": 0.8\n}\n```", "```json\n{\n  \"b14\": 0.95,\n  \"b31\": 0.95,\n  \"b40\": 0.9,\n  \"b42\": 0.9,\n  \"b9\": 0.9\n}\n```", "```json\n{\n  \"b14\": 0.9,\n  \"b42\": 0.8,\n  \"b9\": 0.8,\n  \"b31\": 0.9\n}\n```", "```json\n{\n  \"b14\": 0.95,\n  \"b31\": 0.9,\n  \"b42\": 0.9,\n  \"b9\": 0.9,\n  \"b40\": 0.85\n}\n```", "```json\n{\n  \"b14\": 0.95,\n  \"b31\": 0.9,\n  \"b42\": 0.85,\n  \"b9\": 0.85,\n  \"b40\": 0.8\n}\n```"], "62bbc3865aee126c0fa68605": ["Based on the text of the paper, the main source papers that have inspired the author's work can be identified as:\n\n1. \"structural entropy guided graph hierarchical pooling\" - This is the main paper that has influenced the author's work, as they have directly mentioned that their work is inspired by structural entropy. This paper would have the highest confidence score.\n2. \"hierarchical graph representation learning with differentiable pooling\" [b30] - This paper is mentioned multiple times and seems to be directly related to the author's topic of graph neural networks and pooling operations.\n3. \"self attention graph pooling\" [b12] - This paper is also mentioned multiple times and may have influenced the author's work on pooling mechanisms in GNNs.\n4. \"graph u nets\" [b8] - The author's methodology seems to be inspired by this paper, as they have adopted a similar encoder-decoder network design for node classification.\n5. \"structural information and dynamical complexity of networks\" [b13] - This paper is referenced in the context of structural entropy, which is a key concept in the author's work.\n6. \"spectral clustering with graph neural networks for graph pooling\" [b2] - This paper seems to have influenced the author's work on node clustering in GNNs.\n\nThe confidence score for each reference paper would be determined by the frequency of its mention in the text, its relevance to the paper's novel contributions, and the author's explicit statements of its influence on their work.\n\nThe output in JSON format would be:\n\n{\n  \"structural entropy guided graph hierarchical pooling\": 1.0,\n  \"b30\": 0.8,\n  \"b12\": 0.8,\n  \"b8\": 0.7,\n  \"b13\": 0.7,\n  \"b2\": 0.6\n}", "Based on the text, the most relevant source papers for this academic paper are:\n\n1. \"structural entropy guided graph hierarchical pooling\" - [None]: The entire work is based on a novel approach to graph hierarchical pooling guided by structural entropy, making this the most central reference.\n2. \"hierarchical graph representation learning with differentiable pooling\" - [b30]: The paper refers to this work as a previous method that it seeks to improve upon.\n3. \"self attention graph pooling\" - [b12]: The paper refers to this work as a previous method that it seeks to improve upon.\n4. \"graph u nets\" - [b8]: The paper refers to this work as a previous method that it seeks to improve upon.\n5. \"structural information and dynamical complexity of networks\" - [b13]: This paper is the source of the concept of structural entropy which is central to the approach proposed in the current paper.\n6. \"decoding topologically associating domains with ultra low resolution hi c data by graph structural entropy\" - [b16]: This paper is also the source of the concept of structural entropy and its application is relevant to the current paper.\n\nHere is the information in JSON format:\n\n```json\n{\n  \"None\": 1,\n  \"b30\": 0.8,\n  \"b12\": 0.8,\n  \"b8\": 0.8,\n  \"b13\": 0.9,\n  \"b16\": 0.9\n}\n```", "Based on the provided paper text, the source papers that seem to be most pertinent are:\n\n1. \"structural entropy guided graph hierarchical pooling\" - Confidence score: 0.9 - This is the main inspiration for the paper's algorithm and methodology as mentioned in the \"Proposed Method\" section.\n2. \"hierarchical graph representation learning with differentiable pooling\" (b30) - Confidence score: 0.8 - It is mentioned multiple times throughout the paper and seems to be a key reference for the issues the paper is aiming to address.\n3. \"self attention graph pooling\" (b12) - Confidence score: 0.7 - The paper mentions this work as an example of previous pooling methods that the authors aim to improve upon.\n4. \"graph u nets\" (b8) - Confidence score: 0.7 - This work is referenced as a method the paper aims to improve upon and as a baseline for their node classification task.\n5. \"adaptive structure aware pooling for learning hierarchical graph representations\" (b20) - Confidence score: 0.6 - It's mentioned as an example of previous pooling methods that the authors aim to improve upon.\n6. \"structural information and dynamical complexity of networks\" (b13) - Confidence score: 0.8 - This paper is the source of the structural entropy concept, which is a key component of the proposed method in the paper.\n\nThe JSON format of the answer is:\n\n{\n  \"None\": 0.9, \n  \"b30\": 0.8, \n  \"b12\": 0.7, \n  \"b8\": 0.7, \n  \"b20\": 0.6, \n  \"b13\": 0.8\n}", "Based on the text, the following papers appear to be the most influential sources for the given paper:\n\n1. \"Structural information and dynamical complexity of networks\" [b13]: The authors of the given paper are directly inspired by the concept of structural entropy from this paper to design their algorithm. Confidence score: 0.9\n\n2. \"Hierarchical graph representation learning with differentiable pooling\" [b30]: The authors of the given paper compared their method with this paper, which suggests that this paper has had a significant influence on theirs. Confidence score: 0.7\n\n3. \"Self-attention graph pooling\" [b12]: This paper is mentioned in the context of the development of GNNs, indicating that it is a significant previous work in the field. Confidence score: 0.6\n\n4. \"Graph U-Nets\" [b8]: This paper is referred to multiple times, and the authors of the given paper have used it as a comparison for their work. Confidence score: 0.6\n\n5. \"Semi-supervised classification with graph convolutional networks\" [b10]: This paper is mentioned as a key point of the convolutional mechanism of GNNs, which is a core part of the given paper. Confidence score: 0.5\n\n6. \"Spectral clustering with graph neural networks for graph pooling\" [b2]: This paper is referred to in the context of the pooling mechanism in GNNs, suggesting it has had a significant influence on the given paper. Confidence score: 0.5\n\nResult in the json format:\n\n{\n\"b13\": 0.9,\n\"b30\": 0.7,\n\"b12\": 0.6,\n\"b8\": 0.6,\n\"b10\": 0.5,\n\"b2\": 0.5\n}", "Based on the analysis of the paper text and the titles of the references, the source papers most likely to have inspired the given paper are:\n\n1. 'structural entropy guided graph hierarchical pooling' - The key concept of structural entropy is mentioned multiple times in the text and motivates the main approach of the paper.\n2. 'hierarchical graph representation learning with differentiable pooling' - The concept of hierarchical graph representation and pooling is central to the paper's method.\n3. 'self attention graph pooling' - The paper refers to pooling mechanisms and this could have been a source of inspiration.\n4. 'adaptive structure aware pooling for learning hierarchical graph representations' - This work might have inspired the structure aware pooling mechanism proposed in the paper.\n5. 'Semi supervised classification with graph convolutional networks' - This work is potentially relevant as the paper uses Graph Convolutional Networks and discusses semi-supervised classification.\n6. 'structural information and dynamical complexity of networks' - Again, the emphasis on structural information and complexity in networks is echoed in the given paper.\n\nThe JSON format of the answer is:\n\n```json\n{\n  \"None\": 0.85,\n  \"b30\": 0.8,\n  \"b12\": 0.7,\n  \"b20\": 0.75,\n  \"b10\": 0.65,\n  \"b13\": 0.8\n}\n```"], "630359f590e50fcafd88dc06": ["Based on the provided information, the following references appear to be the most influential sources for the given paper, as indicated by direct mentions, discussions, and the role they play in the proposed methodology:\n\n1. 'b11': 'causal inference in statistics a primer' - Confidence Score: 0.85\n2. 'b29': 'causality' - Confidence Score: 0.85\n3. 'b24': 'towards deep learning models resistant to adversarial attacks' - Confidence Score: 0.8\n4. 'b23': 'learning causal semantic representation for out-of-distribution prediction' - Confidence Score: 0.8\n5. 'b40': 'recovering latent causal factor for generalization to distributional shifts' - Confidence Score: 0.75\n6. 'b20': 'bridging adversarial robustness and gradient interpretability' - Confidence Score: 0.7\n\nThese references are mentioned in the context of developing and motivating the paper's main contributions, and hence are likely to be the most influential sources.", "Based on the content of the text, the papers that seem to have most inspired the authors are:\n\n1. 'Towards deep learning models resistant to adversarial attacks' - [b24] - Confidence Score: 0.9\n\nThis paper was mentioned multiple times throughout the text in the context of adversarial training (AT), a defense strategy against attacks on deep neural networks (DNNs).\n\n2. 'Causal inference in statistics a primer' - [b11] - Confidence Score: 0.85\n\nThis paper is referenced in the context of explaining the causal mechanisms and relationships between variables in the model.\n\n3. 'Learning causal semantic representation for out-of-distribution prediction' - [b23] - Confidence Score: 0.8\n\nThis paper is mentioned in the context of discussing the role of causal reasoning in constructing a causal graph for understanding attack and defense mechanisms.\n\n4. 'Recovering latent causal factor for generalization to distributional shifts' - [b40] - Confidence Score: 0.75\n\nThis paper is referenced in the context of discussing the role of latent factors in the causality model.\n\n5. 'Intriguing properties of neural networks' - [b42] - Confidence Score: 0.75\n\nThis paper is mentioned in the context of domain shifts and the performance of DNNs on test data.\n\n6. 'Bridging adversarial robustness and gradient interpretability' - [b20] - Confidence Score: 0.7\n\nThis paper is used to motivate the use of gradients as the importance of pixels and generate the mask.\n\n7. 'Causality' - [b29] - Confidence Score: 0.65\n\nThis paper is referenced when discussing the role of causal reasoning in understanding and improving attack and defense mechanisms.\n\n8. 'Adversarial examples make strong poisons' - [b9] - Confidence Score: 0.6\n\nThis paper is mentioned when discussing the effectiveness of poisoning training data to maximize test errors in DNNs. \n\nThe confidence scores are assigned based on the number of times a paper is referenced, the context in which it is referenced, and the direct attribution of inspiration or motivation.", "Based on the given text, the papers that influenced the authors the most are:\n\n1. \"Adversarial examples make strong poisons\" [b9]: It is directly referenced in the context of understanding the effectiveness of delusive attacks and their impact on DNNs.\n\n2. \"A theory of causal learning in children: causal maps and bayes nets\" [b13]: This paper is referred to when discussing the human cognitive system's ability to capture causal relations.\n\n3. \"Adversarial training produces human-perception aligned gradients\" [b20]: This paper is used to motivate the development of the confounder generator in the proposed system.\n\n4. \"Towards deep learning models resistant to adversarial attacks\" [b24]: The authors mention this paper as a primary source for adversarial training, which is a key component of their proposed system.\n\n5. \"Causal inference in statistics: a primer\" [b11]: This paper is cited while discussing the construction of a causal graph, which is a major part of the proposed system.\n\n6. \"Learning causal semantic representation for out-of-distribution prediction\" [b23]: This paper is repeatedly cited when discussing the assumptions and design of the causal graph in the proposed system.\n\n7. \"Causality\" [b29]: This is mentioned multiple times in reference to the development of the causal inference pipeline, a key component of the proposed system.\n\n8. \"Interpretation and identification of causal mediation\" [b30]: It's used to motivate the use of backdoor adjustment for causal intervention.\n\nThe confidence scores of each paper are difficult to precisely quantify without further information about how heavily each paper influenced the specific methodologies and conclusions of the given paper. However, it is clear that these eight papers played a significant role in shaping the authors' research. \n\nThe result is: {\"b9\": 0.8, \"b13\": 0.7, \"b20\": 0.8, \"b24\": 0.9, \"b11\": 0.9, \"b23\": 0.9, \"b29\": 0.9, \"b30\": 0.8}\n", "Based on the text and the provided titles of the references, the source papers that seem most pertinent to the given paper are:\n\n1. \"dice domain attack invariant causal learning for improved data privacy protection and adversarial robustness\": The paper's title matches the main topic of the given text, hinting that it might be the main source of inspiration for the authors. Confidence Score: 0.9\n\n2. \"invariance causality and robustness\": The paper's title matches the main topic of the given text, suggesting it could be a key source of inspiration for the authors. Confidence Score: 0.85\n\n3. \"causal inference in statistics a primer\": The given text refers to causal inference, making this paper a likely source. Confidence Score: 0.8\n\n4. \"recovering latent causal factor for generalization to distributional shifts\": The given text discusses recovering latent causal factors, making this paper a likely source. Confidence Score: 0.75\n\n5. \"a causal view on robustness of neural networks\": The given text discusses the robustness of neural networks from a causal perspective, making this paper a likely source. Confidence Score: 0.7\n\n6. \"adversarially robust generalization requires more data\": The given text discusses the need for more data for adversarially robust generalization, making this paper a likely source. Confidence Score: 0.65\n\n7. \"towards deep learning models resistant to adversarial attacks\": The paper's title matches the main topic of the given text, indicating it could be a key source of inspiration for the authors. Confidence Score: 0.6\n\n8. \"invariant risk minimization\": The given text refers to invariant risk minimization, hinting this paper could be a source. Confidence Score: 0.55\n\nHere is the result in JSON format:\n\n{\n\"1\": 0.9,\n\"2\": 0.85,\n\"3\": 0.8,\n\"4\": 0.75,\n\"5\": 0.7,\n\"6\": 0.65,\n\"7\": 0.6,\n\"8\": 0.55\n}", "Based on the text and the titles of the references, the source papers that most influenced this paper are:\n\n- \"causal inference in statistics a primer\" [b11]: The paper refers to this source when discussing the structural causal model (SCM), indicating that this paper provided a theoretical basis for the research.\n- \"adversarial examples make strong poisons\" [b9]: This paper seems to be directly related to the problem of \"poisoning train data with manual noise\", which is a central issue in the paper.\n- \"towards deep learning models resistant to adversarial attacks\" [b24]: This paper is mentioned in the context of adversarial training (AT), a defense strategy that is a key part of the paper's methodology.\n- \"learning causal semantic representation for out ofdistribution prediction\" [b23]: This source is referred to multiple times, particularly when discussing the causal assumptions of the Domain-attack Invariant Causal Model (DICM).\n- \"improving adversarial robustness via promoting ensemble diversity\" [b27]: This paper is mentioned in the context of the defense methods that require more data or an ensemble of cross-domain models, which is part of the strategy proposed in the paper.\n- \"causality\" [b29]: This source is referred to multiple times, particularly when discussing the concept of causal intervention, which is central to the paper's methodology.\n\nI should note that this is not an exhaustive list, and other references might also have influenced the paper. However, these are the ones that seem most central based on the text.\n\nBased on this, the result in JSON format would be:\n\n{\n    \"b11\": 0.9,\n    \"b9\": 0.8,\n    \"b24\": 0.8,\n    \"b23\": 0.9,\n    \"b27\": 0.7,\n    \"b29\": 0.9\n}\n"], "628748bc5aee126c0ffc3ecd": ["{\n    \"b6\": 0.9,\n    \"b20\": 0.85,\n    \"b3\": 0.8,\n    \"b17\": 0.75,\n    \"b1\": 0.7\n}", "{\n    \"b3\": 0.9,\n    \"b6\": 0.8,\n    \"b20\": 0.7,\n    \"b5\": 0.6\n}", "{\n    \"b6\": 0.95,\n    \"b20\": 0.9,\n    \"b3\": 0.85,\n    \"b17\": 0.8,\n    \"b1\": 0.75,\n    \"b10\": 0.7,\n    \"b8\": 0.65\n}", "{\n    \"b6\": 0.9,\n    \"b3\": 0.8,\n    \"b1\": 0.75,\n    \"b17\": 0.75,\n    \"b20\": 0.7,\n    \"b2\": 0.65,\n    \"b5\": 0.65\n}", "{\n    \"b6\": 0.9,\n    \"b3\": 0.85,\n    \"b20\": 0.8,\n    \"b17\": 0.75,\n    \"b1\": 0.7,\n    \"b8\": 0.65\n}"], "62bab8f95aee126c0f6afb82": ["Based on the text and the references given, the source papers that mostly likely inspired this work are:\n\n```\n{\n\"b4\": 0.9, \n\"b26\": 1, \n\"b18\": 0.8, \n\"b1\": 0.7, \n\"b6\": 0.7, \n\"b15\": 0.6,  \n\"b33\": 0.6\n}\n```\n\nThese papers are mentioned in the text in contexts that suggest they have directly influenced the work of the paper, and align with the key challenges and novel methods proposed by the author. For instance, reference b26 is referred to multiple times in relation to the properties of alignment and uniformity on the hypersphere, which are key to the proposed DirectAU objective in the author's work. Similarly, reference b4 is mentioned in the context of contrastive representation learning, which appears to be a significant methodological influence on the paper. \n\nPlease verify these suggestions with the actual papers, as I can only infer so much from the provided text and titles.", "Based on the text and the provided author notes, the most influential sources for the given paper are likely to be:\n\n- \"Understanding contrastive representation learning through alignment and uniformity on the hypersphere\" [b26]: This paper is frequently referenced and seems to be the main source of inspiration for the alignment and uniformity concepts which are key to the paper's algorithm.\n- \"BPR: Bayesian personalized ranking from implicit feedback\" [b18]: This paper is referenced multiple times and seems to be the basis for the BPR loss, which the authors of the given paper improve upon.\n- \"Simplex: A simple and strong baseline for collaborative filtering\" [b15]: This paper is also referenced multiple times and seems to provide the authors with the fundamental concepts of their approach.\n- \"LightGCN: Simplifying and powering graph convolution network for recommendation\" [b6]: This paper is referenced in the context of explaining how different CF methods have distinct learning trajectories, which is a key part of the paper's analysis.\n- \"Efficient neural matrix factorization without sampling for recommendation\" [b1]: This paper is also referenced in the context of explaining how different CF methods have distinct learning trajectories, which is a key part of the paper's analysis.\n\nBased on these references, the following JSON format could be returned:\n\n```\n{\n    \"b26\": 0.9,\n    \"b18\": 0.85,\n    \"b15\": 0.8,\n    \"b6\": 0.75,\n    \"b1\": 0.75\n}\n```\n\nThese confidence scores are estimates based on the frequency and context of the references in the paper.", "Based on the input text and the author's notes, the paper seems to be heavily inspired by papers focusing on alignment and uniformity in collaborative filtering. Based on the frequent mentions and the context in which these papers are cited, these key sources are:\n\n1. \"Understanding contrastive representation learning through alignment and uniformity on the hypersphere\" (b26)\n2. \"Efficient Neural Matrix Factorization without Sampling for Recommendation\" (b1)\n3. \"LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation\" (b6)\n4. \"BPR: Bayesian Personalized Ranking from Implicit Feedback\" (b18)\n5. \"Improving pairwise learning for item recommendation from implicit feedback\" (b17)\n\nPlease note that the confidence score is a subjective measure and could depend on the specific context and understanding of the paper.\n\nIn JSON format, this might be represented as:\n\n```\n{\n    \"b26\": 0.9,\n    \"b1\": 0.8,\n    \"b6\": 0.7,\n    \"b18\": 0.7,\n    \"b17\": 0.6\n}\n```\n\nHere, a higher score indicates greater confidence that the reference paper had a significant influence on the paper in question.", "Based on the provided information, the paper's key methodological inspiration comes from the idea of alignment and uniformity in the hypersphere, which is a concept proposed in reference [b26]. The author also mentions the BPR loss from [b18] and the contrastive representation learning from [b4]. Several other papers are mentioned in the context of previous research in the field of collaborative filtering and recommender systems, but these papers are not directly linked to the paper's novel contributions and therefore are not considered as source papers. \n\nHere's a list of source papers based on the criteria provided:\n\n1. [b26]: \"Understanding contrastive representation learning through alignment and uniformity on the hypersphere.\" - Confidence score: 0.9\n2. [b18]: \"BPR: Bayesian personalized ranking from implicit feedback.\" - Confidence score: 0.8\n3. [b4]: \"SimCSE: Simple contrastive learning of sentence embeddings.\" - Confidence score: 0.7\n\nThe confidence scores are estimated based on the frequency and context of the references in the text.\n\nThe JSON format for the result:\n\n```\n{\n\"b26\": 0.9,\n\"b18\": 0.8,\n\"b4\": 0.7\n}\n```", "Based on the text, the following references seem to have inspired the author the most:\n\n1. \"Understanding contrastive representation learning through alignment and uniformity on the hypersphere\" - [b26]\n2. \"LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation\" - [b6]\n3. \"Efficient Neural Matrix Factorization without Sampling for Recommendation\" - [b1]\n4. \"BPR: Bayesian Personalized Ranking from Implicit Feedback\" - [b18]\n5. \"Improving Pairwise Learning for Item Recommendation from Implicit Feedback\" - [b17]\n6. \"SimCSE: Simple Contrastive Learning of Sentence Embeddings\" - [b4]\n\nThe json format should be:\n\n{\n  \"b26\": 1,\n  \"b6\": 0.9,\n  \"b1\": 0.8,\n  \"b18\": 0.7,\n  \"b17\": 0.6,\n  \"b4\": 0.5\n}\n\nThis listing assumes that b26 is the most influential source paper to the author with a confidence score of 1, and the influence of other source papers decreases as the score decreases."], "628749125aee126c0ffe0f2d": ["Based on the paper text and the author's note, the source papers which inspire the given paper most are:\n\n1. \"Leveraging passage retrieval with generative models for open domain question answering\" [b7]: This paper is directly mentioned as the foundation for the current paper's model. The current paper's model, FiD-PGN, is built upon FiD and enhances it with a pointer network. \n\n2. \"Get to the point: Summarization with pointer-generator networks\" [b22]: This paper is directly mentioned as the inspiration for the enhancement of the generative model with a pointer network.\n\n3. \"Pointer networks\" [b23]: This paper is also mentioned as the inspiration for the enhancement of the generative model with a pointer network.\n\n4. \"Dense passage retrieval for open-domain question answering\" [b8]: This paper's method was adopted for retriever results in the current paper's model.\n\n5. \"Distilling knowledge from reader to retriever for question answering\" [b6]: The current paper's model adopted the retriever results of FiD-KD, which is an extension of FiD model proposed in this paper.\n\n6. \"BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension\" [b10]: The current paper's model utilized the BART model in their reader encoder, which was mentioned in the Method section.\n\n7. \"Reading Wikipedia to answer open-domain questions\" [b1]: This paper's work was followed by the current paper's model in building a two-stage retriever-reader system. \n\nThe JSON format result is:\n\n{\n\"b7\": 1,\n\"b22\": 0.9,\n\"b23\": 0.9,\n\"b8\": 0.8,\n\"b6\": 0.8,\n\"b10\": 0.7,\n\"b1\": 0.7\n}", "Based on the text, the source papers that influenced the paper's main contributions are:\n\n1. \"Leveraging passage retrieval with generative models for open domain question answering\" (b7): This paper is mentioned several times, indicating it was a key source of inspiration. The authors of the paper being analyzed built their model upon FiD, which was proposed in this source paper.\n\n2. \"Get to the point: summarization with pointer-generator networks\" (b22): The authors specifically state they were \"inspired by the work of [b22]\" and enhanced their model with a pointer network as suggested in this source paper.\n\n3. \"A copy augmented generative model for open domain question answering\" (None): The authors' novel approach of integrating a copying mechanism into their model is a key contribution of this paper, and likely inspired by the methodologies explored in this source paper.\n\n4. \"Reading Wikipedia to answer open-domain questions\" (b1): This paper is mentioned as the pioneering work that modern ODQA approaches typically follow. While it doesn't seem to directly contribute to the paper's key novel contributions, it does provide the foundational retriever-reader pipeline that the paper builds upon.\n\n5. \"Dense Passage Retrieval for Open-Domain Question Answering\" (b8): This paper is mentioned in relation to the two-stage retriever-reader pipeline and is referenced when discussing the effectiveness of extractive models, indicating it likely had a significant influence on the paper's methodologies.\n\nThe returned results in json format, with the key as the \u201creference number\u201d and the value as the \u201cconfidence score\u201d between 0 and 1, would look as follows:\n\n```\n{\n    \"b7\": 0.9,\n    \"b22\": 0.9,\n    \"None\": 0.8,\n    \"b1\": 0.7,\n    \"b8\": 0.7\n}\n```", "Based on the text and the titles of the references, the source papers can be inferred as:\n\n- [b1] 'Reading Wikipedia to Answer Open-Domain Questions'\n- [b7] 'Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering'\n- [b8] 'Dense Passage Retrieval for Open-Domain Question Answering'\n- [b22] 'Get to the Point: Summarization with Pointer-Generator Networks'\n- [b23] 'Pointer Networks'\n\nAnalysis:\n\n- The author states, \"With the pioneering work of DrQA[b1]\". The paper uses DrQA as the base model for their work, indicating strong reliance on this paper.\n\n- The paper's main contribution is a novel approach that combines the advantages of extractive and generative readers, as mentioned, \"we enhance the generative model with a pointer network [b23]\". This indicates that [b23] is a significant source paper.\n\n- The mention of [b7] and [b8] in this context, \"benefiting from the powerful ability of large-scale pre-trained encoderdecoder language models [b10] [b19] and the capability of aggregating information from multiple passages, generative approaches have achieved in general better performance than extractive methods.\" This indicated that these papers inspired the authors to focus on improving generative approaches.\n\n- The author states, \"Inspired by the work of [b22]\" indicating direct inspiration from this paper.\n\nThe JSON format of the results is:\n{\n  \"b1\": 0.9,\n  \"b7\": 0.8,\n  \"b8\": 0.8,\n  \"b22\": 1.0,\n  \"b23\": 0.9\n}", "Based on the text of the paper and the information provided by the author, the source papers that most inspire this paper are:\n\n1. \"A copy augmented generative model for open domain question answering\" [b22]\n2. \"Leveraging passage retrieval with generative models for open domain question answering\" [b7]\n3. \"Dense passage retrieval for open-domain question answering\" [b8]\n4. \"Reading Wikipedia to answer open-domain questions\" [b1]\n5. \"Get to the point: Summarization with pointer-generator networks\" [b22]\n6. \"Pointer networks\" [b23]\n7. \"Distilling knowledge from reader to retriever for question answering\" [b6]\n\nThe confidence scores of these papers are calculated as:\n\n- \"A copy augmented generative model for open domain question answering\" [b22]: 1\n- \"Leveraging passage retrieval with generative models for open domain question answering\" [b7]: 0.9\n- \"Dense passage retrieval for open-domain question answering\" [b8]: 0.8\n- \"Reading Wikipedia to answer open-domain questions\" [b1]: 0.7\n- \"Get to the point: Summarization with pointer-generator networks\" [b22]: 0.6\n- \"Pointer networks\" [b23]: 0.6\n- \"Distilling knowledge from reader to retriever for question answering\" [b6]: 0.5\n\nThe paper is returned in the JSON format as follows:\n\n{\n\"b22\": 1,\n\"b7\": 0.9,\n\"b8\": 0.8,\n\"b1\": 0.7,\n\"b23\": 0.6,\n\"b6\": 0.5\n}", "Based on the text of the academic paper and the titles of the references, the most pertinent source papers seem to be:\n\n1. 'Leveraging passage retrieval with generative models for open domain question answering'[b7]: The paper directly states that their model is built upon the state-of-the-art model FiD, which is referenced in this paper. \n\n2. 'Get to the point: Summarization with pointer-generator networks'[b22]: The authors specifically mention they were inspired by this work to enhance their generative model with a pointer network.\n\n3. 'Pointer Networks'[b23]: This paper is also directly related to the novel method the authors used in their paper.\n\n4. 'Dense passage retrieval for open-domain question answering'[b8]: This paper is referenced in the description of the two-stage retriever-reader pipeline that the authors' model also adopts.\n\n5. 'Reading Wikipedia to answer open-domain questions'[b1]: The authors mention the pioneering work of DrQA, which is presented in this paper.\n\n6. 'Distilling knowledge from reader to retriever for question answering'[b6]: This paper is mentioned when the authors describe the retriever results they adopted in their model.\n\n7. 'BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension'[b10]: This paper is mentioned related to the powerful ability of large-scale pre-trained encoder-decoder language models, which the authors' model also benefits from.\n\n8. 'Natural questions: A benchmark for question answering research'[b9]: This paper is mentioned in the context of the datasets used to evaluate the performance of the authors' model.\n\nJSON format output:\n\n{\n\"b1\": 0.8,\n\"b7\": 1.0,\n\"b8\": 0.8,\n\"b9\": 0.7,\n\"b10\": 0.7,\n\"b22\": 1.0,\n\"b23\": 0.9,\n\"b6\": 0.7\n}"], "62cce67a5aee126c0f2a86f3": ["Based on the provided text, the paper's main challenges include author name disambiguation in digital libraries due to the high number of authors sharing the same names. The author proposes a novel approach leveraging semantic and symbolic representations of titles, sources, and co-authors to tackle this issue. \n\nThe source papers that inspired this work can be identified by checking the sections where the authors discuss their novel contributions and the methods they used to tackle the identified challenges. For this paper, the authors have taken inspiration from previous works that employed different machine learning approaches to tackle the author name disambiguation issue. \n\nBelow are the source papers that seem to have inspired this paper most:\n\n- \"b1\": 'a joint model for word embedding and word morphology' - The authors have used Char2Vec, a character-level embedding, which might have been inspired by this source paper which talks about word embedding and morphology. Confidence Score: 0.8\n\n- \"b2\": 'bert pre training of deep bidirectional transformers for language understanding' - The authors of the current paper have used BERT to capture the semantic representations of sequences of words, which could have been inspired by this source paper. Confidence Score: 0.8\n\n- \"b31\": 'a network embedding based method for author disambiguation' - The authors have leveraged network embedding to exploit the relationship among publications, which could have been inspired by this paper. Confidence Score: 0.7\n\n- \"b9\": 'two supervised learning approaches for name disambiguation in author citations' - The authors have mentioned the use of supervised approaches, and this paper could have been one of their sources. Confidence Score: 0.6\n\n- \"b34\": 'bayesian non exhaustive classification a case study online name disambiguation using temporal record streams' - The authors have mentioned the use of Bayesian Classification which might have been inspired by this paper. Confidence Score: 0.6\n\n- \"b29\": 'author name disambiguation by using deep neural network' - The authors have used deep learning, which could have been inspired by this source paper. Confidence Score: 0.7\n\nThe result in JSON format would look like this:\n{\"b1\": 0.8, \"b2\": 0.8, \"b31\": 0.7, \"b9\": 0.6, \"b34\": 0.6, \"b29\": 0.7}", "Based on the text, the paper appears to be most influenced by the following source papers:\n\n1. b1: 'a joint model for word embedding and word morphology' - The paper mentions using a Char2Vec model, which provides a symbolic representation of words. This method is known to be used in b1 for word embedding and word morphology.\n   \n2. b2: 'bert pre training of deep bidirectional transformers for language understanding' - The paper uses a BERT model to capture the semantic representations of the sequence of words in the title and source of the publication. This method is featured prominently in b2.\n   \n3. b34: 'bayesian non exhaustive classification a case study online name disambiguation using temporal record streams' - The paper discusses the problem of author name ambiguity and its resolution by partitioning papers into homogeneous groups. These concepts are also discussed in b34.\n\n4. b33: 'name disambiguation in anonymized graphs using network embedding' - The paper uses network embedding to exploit the relationship among publications, which is a method also used in b33.\n   \n5. b31: 'a network embedding based method for author disambiguation' - The paper focuses on author disambiguation and uses network embedding for author disambiguation, which is also the focus of b31.\n\nIn json format, the results would be as follows:\n\n{\"b1\": 0.8, \"b2\": 0.9, \"b34\": 0.7, \"b33\": 0.7, \"b31\": 0.8}\n\nNote: The confidence scores are subjective estimates based on the extent of the information provided in the text. A higher score indicates a greater level of perceived influence.", "Based on the text of the paper, the following references seem to have the most significant influence on the paper's core contributions:\n\n1. 'b1': 'a joint model for word embedding and word morphology' - The paper mentions using Char2Vec for representation of co-authors, which is a character-level embedding of words. The source paper for this method can be inferred to be 'b1'. Confidence score: 0.8\n2. 'b2': 'bert pre training of deep bidirectional transformers for language understanding' - BERT model is used to capture the semantic representations of the sequence of words for title and source. This method is likely inspired by 'b2'. Confidence score: 0.9\n3. 'b29': 'author name disambiguation by using deep neural network' - The overall theme of author name disambiguation using neural networks aligns with 'b29'. Confidence score: 0.7\n4. 'b33': 'name disambiguation in anonymized graphs using network embedding' - The paper also seems to be influenced by 'b33' as it mentions using relational information considering three graphs. Confidence score: 0.7\n5. 'b31': 'a network embedding based method for author disambiguation' - The paper uses network embedding method for author disambiguation, which is similar to 'b31'. Confidence score: 0.7\n\nThe JSON representation of the sources would be:\n{\n\"b1\": 0.8,\n\"b2\": 0.9,\n\"b29\": 0.7,\n\"b33\": 0.7,\n\"b31\": 0.7\n}", "Based on the provided text and the titles of the references, the source papers for the given academic paper could be identified as follows:\n\n- 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding' (b2) - The authors mention using the BERT model to capture the semantic representations of the sequence of words. This indicates a strong reliance on this work for developing the paper's core contributions. Confidence Score: 0.9\n- 'Chars2vec: A joint model for word embedding and word morphology' (b1) - The authors mention using Char2Vec for author name representation, which is a significant part of their proposed approach. Confidence Score: 0.8\n- 'WhoIs: Deep Author Name Disambiguation using Bibliographic Data' (None) - Although it's not clear from the text, the title suggests this paper might have been a primary source of inspiration for the given paper. Confidence Score: 0.7\n- 'Author Name Disambiguation in Citations' (b32) - This work could be a source paper as the given paper's main topic is also author name disambiguation. Confidence Score: 0.6\n- 'A Network Embedding Based Method for Author Disambiguation' (b31) - This paper seems to be a source paper as its title suggests it shares methodological similarity with the given paper. Confidence Score: 0.6\n- 'Name Disambiguation in Anonymized Graphs using Network Embedding' (b33) - This paper could be a source paper due to its relevance to the topic and methodological similarity. Confidence Score: 0.6\n\nSo, the final result in JSON format would be:\n\n{\n\"b2\": 0.9,\n\"b1\": 0.8,\n\"None\": 0.7,\n\"b32\": 0.6,\n\"b31\": 0.6,\n\"b33\": 0.6\n}", "Based on the provided text and the titles of the references, the following references seem to be the most relevant source papers for this academic paper:\n\n- \"b1\": 'a joint model for word embedding and word morphology'\n- \"b2\": 'bert pre training of deep bidirectional transformers for language understanding'\n- \"b9\": 'two supervised learning approaches for name disambiguation in author citations'\n- \"b13\": 'using deep learning word embeddings for citations similarity in academic papers'\n- \"b22\": 'semantic author name disambiguation with word embeddings'\n- \"b29\": 'author name disambiguation by using deep neural network'\n- \"b31\": 'a network embedding based method for author disambiguation'\n- \"b33\": 'name disambiguation in anonymized graphs using network embedding'\n\nPlease note that this is an educated guess based on the provided text and the titles of the references.\n\nHere is the JSON format of the answer:\n\n{\n\"b1\": 0.8,\n\"b2\": 0.9,\n\"b9\": 0.85,\n\"b13\": 0.8,\n\"b22\": 0.85,\n\"b29\": 0.8,\n\"b31\": 0.85,\n\"b33\": 0.85\n}"], "62f3220a90e50fcafd115bd6": ["{\n  \"b3\": 0.9,\n  \"b26\": 0.8,\n  \"b1\": 0.7,\n  \"b21\": 0.6,\n  \"b11\": 0.6,\n  \"b27\": 0.6,\n  \"b28\": 0.6\n}", "{\n  \"b3\": 0.9,\n  \"b26\": 0.9,\n  \"b4\": 0.8\n}", "```json\n{\n    \"b26\": 0.9,\n    \"b3\": 0.8,\n    \"b21\": 0.7,\n    \"b11\": 0.7,\n    \"b27\": 0.7,\n    \"b28\": 0.7\n}\n```", "```json\n{\n  \"b3\": 1.0,\n  \"b26\": 0.9,\n  \"b12\": 0.8,\n  \"b21\": 0.7,\n  \"b11\": 0.7,\n  \"b28\": 0.7,\n  \"b27\": 0.7\n}\n```", "{\n  \"b3\": 1.0,\n  \"b26\": 0.9\n}"], "628748e05aee126c0ffd1130": ["Based on the content of the paper and the corresponding references, the source papers that most likely inspired the authors are:\n\n1. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" (Reference: b5)\n2. \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\" (Reference: b19)\n3. \"SyntaxBERT: Improving Pre-trained Transformers with Syntax Trees\" (Reference: b0)\n4. \"Syntax-Enhanced Pre-trained Model\" (Reference: b39)\n5. \"Improving BERT with Syntax-Aware Local Attention\" (Reference: b17)\n6. \"KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation\" (Reference: b33)\n7. \"Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction\" (Reference: b13)\n8. \"Contrastive Self-Supervised Learning\" (Reference: b38)\n\nThe paper's primary algorithm and novel contributions are centered around the use of syntactic information to enhance pre-trained language models, specifically through the use of contrastive learning objectives and syntax-guided pre-training. The above references have been cited in connection with these key aspects and methodologies of the paper.\n\nHere's the result in the required format:\n\n```json\n{\n  \"b5\": 0.9,\n  \"b19\": 0.9,\n  \"b0\": 0.9,\n  \"b39\": 0.8,\n  \"b17\": 0.8,\n  \"b33\": 0.7,\n  \"b13\": 0.7,\n  \"b38\": 0.6\n}\n```\nThe confidence scores are set based on the relevance of each source paper to the main contributions of the paper, with a maximum score of 1 indicating highest relevance.", "Based on the text and the provided titles of the references, the following papers seem to be the most relevant sources that inspired the author:\n\n1. 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding' [b5]: The author uses BERT as a base model and mentions it multiple times in the text, indicating a significant reliance on this work.\n\n2. 'RoBERTa: A Robustly Optimized BERT Pretraining Approach' [b19]: RoBERTa is mentioned alongside BERT as one of the pre-trained transformer-based neural language models used as a foundation for the author's model.\n\n3. 'SyntaxBERT: Improving Pre-trained Transformers with Syntax Trees' [b0]: This paper is explicitly mentioned in the context of previous works that enhance pre-trained language models by adding syntax-driven attention components.\n\n4. 'Syntax Enhanced Pre-trained Model' [b39]: This paper is highlighted in the related work section and is referenced throughout the paper in the context of incorporating syntax-related objectives into the pre-training stage.\n\n5. 'Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction' [b13]: The author's phrase-guided contrastive objective is directly inspired by this paper.\n\n6. 'Improving BERT with Syntax Aware Local Attention' [b17]: This work is mentioned in the context of syntax-guided attention methods and is used in the fine-tuning stage of the author's model.\n\nIn JSON format, the result would look like this:\n\n{\"b5\": 0.9, \"b19\": 0.9, \"b0\": 0.8, \"b39\": 0.8, \"b13\": 0.7, \"b17\": 0.7}", "Based on the provided text and author notes, the source papers that inspired the given paper could be identified as:\n\n1. \"SyntaxBERT: Improving Pre-trained Transformers with Syntax Trees\" (b0) - The author mentioned that previous works enhance pre-trained LMs by adding syntax-driven attention components to the transformer. These efforts are evident in b0, which is a paper that directly discusses enhancing a pre-trained language model with syntax trees.\n\n2. \"Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction\" (b13) - The author directly stated that they were inspired by recent studies, including b13, which prove that a language model's attention heads exhibit syntactic structure akin to constituency grammar.\n\n3. \"Tree-structured attention with hierarchical accumulation\" (b23) - The author made a reference to the paper (b23) while discussing the machine translation task and how it incorporates a tree-structured attention into the transformer for helping encode syntactic information.\n\n4. \"Syntax Enhanced Pre-trained Model\" (b39) - The author highlighted b39 in the context of injecting syntax information into pre-trained LMs. It was stated that some works inject syntactic knowledge into pre-trained LMs via introducing new learning objectives, such as syntax head prediction and dependency distance prediction, which are key themes in b39.\n\n5. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" (b5) - The author referred to BERT several times, indicating its significance as a foundational work for the given paper.\n\nHere is the answer in json format:\n\n{\n\"b0\": 1,\n\"b13\": 0.9,\n\"b23\": 0.8,\n\"b39\": 0.8,\n\"b5\": 0.7\n} \n\nEach number represents the confidence score of the source paper being a significant inspiration for the given paper. The score is between 0 and 1, where 1 indicates the highest confidence.", "The source papers for this academic paper, based on the provided text and titles of the references, are as follows:\n\n- \"Attention is all you need\" [b30]: The paper's algorithm is based on the Transformer model, which is the primary contribution of this paper. Confidence Score: 0.9\n- \"BERT: Pre-training of deep bidirectional transformers for language understanding\" [b5]: The paper refers to BERT and how it does not encode enough syntactic knowledge in its learned representations. Confidence Score: 0.8\n- \"RoBERTa: A robustly optimized BERT pretraining approach\" [b19]: The work utilizes RoBERTa as a base model. Confidence Score: 0.8\n- \"SyntaxBERT: Improving pre-trained transformers with syntax trees\" [b0]: The paper discusses the use of syntax-aware layers, which is similar to the approach used in SyntaxBERT. Confidence Score: 0.7\n- \"Are pre-trained language models aware of phrases? Simple but strong baselines for grammar induction\" [b13]: This paper is directly cited when discussing the phrase-guided contrastive objective. Confidence Score: 0.7\n- \"Syntax-enhanced pre-trained model\" [b39]: This paper is referred to when discussing the introduction of new learning objectives for pre-training. Confidence Score: 0.7\n- \"Supervised contrastive learning\" [b12]: The paper proposes contrastive learning method which is also used in this paper. Confidence Score: 0.7\n\nIn JSON format:\n{\n\"b30\": 0.9,\n\"b5\": 0.8,\n\"b19\": 0.8,\n\"b0\": 0.7,\n\"b13\": 0.7,\n\"b39\": 0.7,\n\"b12\": 0.7\n}\n", "Based on the analysis of the text and the author's notes, the most pertinent source papers to the given paper are:\n- b0: 'syntaxbert improving pre trained transformers with syntax trees' \n- b17: 'improving bert with syntax aware local attention'\n- b2: 'enhancing machine translation with dependency aware self attention'\n- b11: 'what does bert learn about the structure of language'\n- b19: 'roberta a robustly optimized bert pretraining approach'\n- b20: 'from balustrades to pierre vinken looking for syntax in transformer self attentions'\n- b39: 'syntax enhanced pre trained model'\n- b5: 'bert pre training of deep bidirectional transformers for language understanding'\n\nThe JSON format of the result is:\n{\n  \"b0\": 0.9,\n  \"b17\": 0.9,\n  \"b2\": 0.8,\n  \"b11\": 0.8,\n  \"b19\": 0.9,\n  \"b20\": 0.8,\n  \"b39\": 0.9,\n  \"b5\": 0.9\n}\n\nPlease note that the confidence score is a subjective metric based on the relevance of the source paper to the given paper's topic, the number of times it is cited, and the context in which it is cited. The closer the score is to 1, the more relevant the source paper is considered to be."], "628afb4c5aee126c0f04e3aa": ["Based on the text of the academic paper and the titles of the referenced papers, the source papers that appear to have most influenced this work are as follows:\n\n- \"b23: denoising diffusion probabilistic models\": The paper proposes a new approach to trajectory optimization using a trajectory-level diffusion probabilistic model (Diffuser), which is directly inspired by the denoising diffusion probabilistic models.\n\n- \"b49: deep unsupervised learning using nonequilibrium thermodynamics\": This paper seems to be a significant influence as it introduces the iterative sampling process of diffusion models which is adopted in the given paper.\n\n- \"b59: synthesis and stabilization of complex behaviors through online trajectory optimization\": This paper likely inspired the idea of online trajectory optimization that is used in the given paper.\n\n- \"b55: synthesis and stabilization of complex behaviors through online trajectory optimization\": The given paper mentions trajectory optimization which is a concept explored in this source paper.\n\n- \"b57: benchmarking model based reinforcement learning\": The given paper's approach is a learning-based analogue of past work in behavioral synthesis using trajectory optimization, which is a method commonly used in model-based reinforcement learning.\n\n- \"b35: reinforcement learning and control as probabilistic inference tutorial and review\": The given paper refers to a control-as-inference graphical model which is likely influenced by this source paper.\n\nThe confidence score for each reference is assumed to be 0.8.\n\nThe resulting JSON format would be:\n\n{\n\"b23\": 0.8,\n\"b49\": 0.8,\n\"b59\": 0.8,\n\"b55\": 0.8,\n\"b57\": 0.8,\n\"b35\": 0.8\n}", "Based on the given academic paper text and the corresponding title references, the source papers that seem to have inspired this work most directly are:\n\n1. \"planning with diffusion for flexible behavior synthesis\"\n2. \"diffusion models beat gans on image synthesis\"\n3. \"denoising diffusion probabilistic models\"\n4. \"synthesis and stabilization of complex behaviors through online trajectory optimization\"\n5. \"the cross entropy method for optimization\"\n6. \"deep reinforcement learning in a handful of trials using probabilistic dynamics models\"\n7. \"reinforcement learning and control as probabilistic inference tutorial and review\"\n8. \"model based planning with energy based models\"\n\nHere are the key phrases and findings from the paper text that indicate these references as source papers:\n\n1. The paper presents the idea of \"planning with diffusion\", which could be inspired by the paper with the same title. \n\n2. The discussion of the iterative denoising process of diffusion models and the comparison of diffusion models with GANs could be inspired by the paper \"diffusion models beat gans on image synthesis\".\n\n3. The use of denoising diffusion models in the proposed method is likely inspired by the paper \"denoising diffusion probabilistic models\".\n\n4. The reference to trajectory optimization routines suggests that the paper \"synthesis and stabilization of complex behaviors through online trajectory optimization\" could be a source paper.\n\n5. The paper uses the cross-entropy method as an example of simple gradient-free trajectory optimization routines, suggesting that the paper \"the cross entropy method for optimization\" is a source paper.\n\n6. The paper's focus on reinforcement learning and data-driven decision-making could be inspired by the paper \"deep reinforcement learning in a handful of trials using probabilistic dynamics models\".\n\n7. The paper's view of planning as a probabilistic problem could be inspired by the paper \"reinforcement learning and control as probabilistic inference tutorial and review\".\n\n8. The paper's focus on energy-based models for planning could be inspired by the paper \"model based planning with energy based models\".\n\nThe confidence scores for these source papers are subjective and depend on the detailed reading and understanding of the given paper text.\n\nThe JSON format of the result is as follows:\n\n```\n{\n\"b1\": 0.9,\n\"b9\": 0.8,\n\"b23\": 0.9,\n\"b55\": 0.7,\n\"b4\": 0.7,\n\"b8\": 0.8,\n\"b35\": 0.8,\n\"b11\": 0.8\n}\n```\nPlease note that the confidence scores are hypothetical and used for illustrative purposes. They represent a subjective measure of how directly each source paper is seen to contribute to the main contributions of the given paper text.", "Based on the text and author notes, it appears that the paper draws heavily on concepts from model-based reinforcement learning, denoising diffusion probabilistic models, and trajectory optimization. The paper also introduces a novel concept of planning with diffusion and Diffuser, a trajectory-level diffusion probabilistic model. Here are the source papers that seem to have contributed most to the research:\n\n1. \"Denoising Diffusion Probabilistic Models\" [b23]: The paper introduces a novel concept of planning with a learned model, drawing directly from concepts in this paper. It's also explicitly mentioned in the text multiple times. Confidence Score: 1\n\n2. \"Planning with Diffusion for Flexible Behavior Synthesis\" [None]: The title itself explains that the paper is about planning with diffusion, which is a key concept introduced in this paper. Confidence Score: 1\n\n3. \"A Direct Method for Trajectory Optimization of Rigid Bodies Through Contact\" [b45]: The paper discusses learned model being plugged into classical trajectory optimization routines, indicating that it may have been influenced by this paper. Confidence Score: 0.8\n\n4. \"Synthesis and Stabilization of Complex Behaviors Through Online Trajectory Optimization\" [b55]: This paper is mentioned in the context of classical trajectory optimization routines, suggesting that it may have influenced the research. Confidence Score: 0.8\n\n5. \"An Introduction to Trajectory Optimization: How to Do Your Own Direct Collocation\" [b29]: The paper is referenced in the context of classical trajectory optimization routines, indicating its likely influence on the research. Confidence Score: 0.6\n\n6. \"Deep Unsupervised Learning Using Nonequilibrium Thermodynamics\" [b49]: This paper is mentioned twice when the authors discuss the concept of Diffuser, the trajectory-level diffusion probabilistic model. Confidence Score: 0.8\n\n7. \"Benchmarking Model-Based Reinforcement Learning\" [b57]: The authors mention that contemporary model-based reinforcement learning algorithms often inherit more from model-free methods, suggesting that this paper might have informed their approach. Confidence Score: 0.6\n\nThe result is in json format:\n{\"b23\": 1, \"None\": 1, \"b45\": 0.8, \"b55\": 0.8, \"b29\": 0.6, \"b49\": 0.8, \"b57\": 0.6}", "Based on the given text, the most pertinent sources would be:\n\n- [b23]: 'denoising diffusion probabilistic models'\n- [b55]: 'synthesis and stabilization of complex behaviors through online trajectory optimization'\n- [b49]: 'deep unsupervised learning using nonequilibrium thermodynamics'\n- [b38]: 'neural network dynamics for model based deep reinforcement learning with model free fine tuning'\n- [b57]: 'benchmarking model based reinforcement learning'\n- [b29]: 'an introduction to trajectory optimization how to do your own direct collocation'\n- [b4]: 'the cross entropy method for optimization'\n\nThese references are mentioned in the context of the development and application of the proposed approach, indicating they have had a significant influence on the paper's contributions. \n\nIn JSON format:\n\n{\n\"b23\": 0.9,\n\"b55\": 0.9, \n\"b49\": 0.8, \n\"b38\": 0.8, \n\"b57\": 0.7, \n\"b29\": 0.7,\n\"b4\": 0.7\n} \n\nThe confidence score is given on a scale of 0 to 1, where 1 indicates the highest level of confidence.", "Based on the provided text and titles of references, the most pertinent source papers to the given paper are:\n\n- \"Denoising Diffusion Probabilistic Models\" [b23]: This paper seems crucial to the given paper as it outlines the diffusion probabilistic models that the author uses for planning and trajectory optimization.\n\n- \"Deep Unsupervised Learning using Non-equilibrium Thermodynamics\" [b49]: The given paper discusses extensively about diffusion models, and this referenced paper seems to be the one of the primary sources outlining the theory of these models.\n\n- \"Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review\" [b35]: This paper looks to be a key source as it provides the theoretical background for the control-as-inference graphical model used in the given paper.\n\n- \"Planning with Diffusion for Flexible Behavior Synthesis\" [None]: The title of the given paper indicates that it is heavily inspired by this paper.\n\n- \"Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization\" [b55]: This paper seems to be related to the trajectory optimization routines mentioned in the given paper.\n\nBased on the task's requirement, the result in the json format should be: \n\n{\"b23\": 0.9, \"b49\": 0.8, \"b35\": 0.7, \"None\": 0.9, \"b55\": 0.7}\n\nPlease note that these confidence scores are subjective and are just a rough estimate of how much each paper contributed to the main paper. \n\nTo validate these results, the full text of the source papers should be examined to confirm the extent of their influence on the main paper."], "6344dede90e50fcafd24ceec": ["Based on the provided information and the instruction to focus on novel contributions and methodological similarities, we can infer some of the key references that the paper builds upon. The paper seems to be addressing two primary challenges with graph transformers:\n\n1. The use of a uniform node sampling strategy across all nodes, which may not be optimal given the diverse properties of different nodes in a graph.\n2. The limitations of existing attention mechanisms which focus more on local attention and may neglect long-range dependencies and global contexts in graphs.\n\nThe novel contributions of the paper are:\n\n- The proposal of an Adaptive Node Sampling strategy for Graph Transformer (ANS-GT) that uses a multi-armed bandit approach to select the most informative nodes for attention.\n- A hierarchical attention scheme that includes both fine-grained local attention and coarse-grained global attention through graph coarsening to capture long-range dependencies.\n\nNow, let's identify the key references that seem most directly relevant to these novel contributions:\n\n```json\n{\n  \"b2\": 0.9,\n  \"b8\": 0.8,\n  \"b25\": 0.85,\n  \"b40\": 0.7,\n  \"b37\": 0.65,\n  \"b24\": 0.75\n}\n```\n\nExplanation of the confidence scores:\n\n- **b2 (\"contextual bandit algorithms with supervised learning guarantees\")** is given a high confidence score of 0.9 because the paper explicitly mentions modifying the Exp4.P method from this reference to adaptively assign weights to several chosen sampling heuristics.\n- **b8 (\"a generalization of transformer networks to graphs\")** has a confidence score of 0.8 because it directly relates to the generalization of Transformer to graph data, which is a core aspect of the paper's contribution.\n- **b25 (\"graph reduction with spectral and cut guarantees\")** gets a confidence score of 0.85 as it is related to graph coarsening algorithms, which are used in the paper to implement the hierarchical attention scheme.\n- **b40 (\"graph bert only attention is needed for learning graph representations\")** is scored at 0.7 because it discusses restricting the receptive field of nodes which is an aspect the authors seem to be building upon to improve with their contributions.\n- **b37 (\"do transformers really perform bad for graph representation\")** receives a score of 0.65 as it is cited in the context of the performance of transformers on small graphs, particularly with molecular property prediction tasks, which may have informed the development of the novel contributions.\n- **b24 (\"bandit samplers for training graph neural networks\")** has a confidence score of 0.75 as it is related to bandit sampling strategies for GNNs, and the authors propose a multi-armed bandit approach for their adaptive node sampling strategy.\n\nThese references are most closely associated with the introduction of the adaptive node sampling strategy and the hierarchical attention scheme, which are the paper's main contributions. The confidence scores reflect the perceived relevance and direct influence on the paper's novel aspects.", "Based on the provided text and the instructions, we can identify the key references that have directly influenced the novel contributions of the paper. The novel contributions are:\n\n1. Adaptive Node Sampling for Graph Transformer (ANS-GT) - A multi-armed bandit algorithm to adaptively sample nodes for attention.\n2. Hierarchical attention scheme - Incorporates coarse-grained global attention with graph coarsening to help the graph transformer capture long-range dependencies while increasing efficiency.\n\nLet's identify the key references that are directly related to these contributions:\n\n- For the adaptive node sampling strategy, we can see that the paper mentions \"[b2]\" in relation to modifying the Exp4.P method for their ANS-GT. This indicates a direct influence from \"[b2]\" on this novel contribution.\n\n- For the hierarchical attention scheme, the paper discusses \"[b25]\" in connection with graph coarsening algorithms that the authors use to pre-process the input graph. This implies that \"[b25]\" has provided significant inspiration for this contribution.\n\nOther references that seem closely related to the paper's contributions are:\n\n- \"[b32]\" for introducing the Transformer architecture, which is foundational for the work.\n- \"[b6]\" and \"[b7]\" for showcasing the success of Transformers in NLP and CV, which likely inspired the authors to adapt Transformers for graph data.\n- \"[b37]\" as an example of a state-of-the-art Graph Transformer on small graphs, which the authors aim to improve upon for large graphs.\n\nNow, let's compile a list with confidence scores, taking into account the direct relevance and the language indicating inspiration or motivation:\n\n```json\n{\n  \"b2\": 1.0,\n  \"b25\": 1.0,\n  \"b32\": 0.8,\n  \"b6\": 0.7,\n  \"b7\": 0.7,\n  \"b37\": 0.9\n}\n```\n\nThe confidence scores reflect the degree to which each paper appears to have directly inspired the novel contributions of the paper. The scores are subjective estimates based on the information provided, with 1.0 indicating the highest confidence of direct influence and lower scores indicating less direct influence.", "Based on the information provided and the guidance for identifying the most pertinent source papers, we can infer which references are most likely to have inspired the novel contributions of the paper in question. The novel contributions are:\n\n1. Adaptive Node Sampling for Graph Transformer (ANS-GT), which modifies a multi-armed bandit algorithm to adaptively sample nodes for attention.\n2. A hierarchical attention scheme with coarse-grained global attention using graph coarsening to help capture long-range dependencies while increasing efficiency.\n\nLet's identify the key references that are directly linked to these novel contributions:\n\n- For the multi-armed bandit algorithm used in adaptive node sampling, we should look for references associated with multi-armed bandit problems and their applications in node sampling strategies.\n- The hierarchical attention scheme employing graph coarsening would be related to references that discuss graph coarsening methods and their application to Graph Transformers or hierarchical attention mechanisms.\n\nBased on the text provided, we can identify the following references as the most central to the paper's contributions:\n\n- Reference [b2]: \"contextual bandit algorithms with supervised learning guarantees\" is likely to be a key source for the multi-armed bandit aspect of the adaptive node sampling.\n- Reference [b25]: \"graph reduction with spectral and cut guarantees\" could be related to the graph coarsening used in the hierarchical attention scheme.\n- Reference [b8]: \"a generalization of transformer networks to graphs\" is probably relevant to the foundational work on Graph Transformers upon which the paper builds.\n- Reference [b24]: \"bandit samplers for training graph neural networks\" is likely to be relevant to the adaptive node sampling method.\n\nGiven the guidance, we should avoid references that only set the historical context or are tangentially related to the paper's main contributions. Therefore, we will exclude references that discuss the general success of Transformers in different domains, such as [b6], [b7], and [b32], because they are not specific to the novel contributions of the paper.\n\nHere is the information in the requested JSON format, with confidence scores reflecting how strongly each reference is believed to be a source of inspiration:\n\n```json\n{\n  \"b2\": 0.9,\n  \"b8\": 0.8,\n  \"b24\": 0.9,\n  \"b25\": 0.8\n}\n```\n\nThe confidence scores are subjective judgments based on the information provided and could vary with additional context from the full paper.", "Based on the provided information, we can infer the most pertinent source papers for the given paper titled \"hierarchical graph transformer with adaptive node sampling\". The paper outlines two primary challenges: the use of a uniform neighbor sampling strategy and attention mechanisms that focus more on local attention. To address these, the authors propose adaptive node sampling using a multi-armed bandit strategy and hierarchical attention through graph coarsening.\n\nFrom the text, we can identify references directly related to the proposed novel contributions by looking for key phrases that indicate inspiration or methodological similarity. Here are the identified references and their confidence scores based on the criteria:\n\n```json\n{\n  \"b2\": 1.0,  // Directly mentioned in the context of adaptive node sampling (multi-armed bandit strategy).\n  \"b25\": 1.0, // Mentioned in relation to graph coarsening for hierarchical attention.\n  \"b8\": 0.8,  // Reference to a generalization of Transformer networks to graphs, relevant for the context but not directly tied to the novel contributions.\n  \"b37\": 0.8, // Mentioned as a state-of-the-art method for molecular property prediction, which the proposed method aims to outperform.\n  \"b40\": 0.7, // Relevant due to the discussion on receptive field restrictions in Graph Transformers.\n  \"b10\": 0.7, // Mentioned in the context of node sampling strategies in Graph Neural Networks.\n  \"b24\": 0.8, // Directly related to the use of a multi-armed bandit for node sampling in Graph Neural Networks.\n  \"b32\": 0.6  // The foundational Transformer architecture paper, which is a base for the proposed method but not a novel contribution per se.\n}\n```\n\nThe confidence scores reflect the direct relevance of each source paper to the novel contributions of the paper as described in the text. The scores are assigned with a maximum of 1.0 for highly relevant references that the authors clearly state as inspiration or basis for their novel contributions. Lower scores are given to references that are relevant to the background but do not directly influence the novel contributions.", "Based on the information provided and the instructions, we need to identify the key source papers that influenced the novel contributions of the given paper. The paper presents two main challenges in the context of graph transformers:\n\n1. The use of a fixed strategy for neighbor sampling in graph transformers that does not consider the graph's properties.\n2. The limitation of existing attention mechanisms to local attention, neglecting long-range dependencies and global contexts.\n\nThe paper proposes solutions to these challenges:\n\n- For Challenge 1: Adaptive Node Sampling for Graph Transformer (ANS-GT), which utilizes a multi-armed bandit approach to adaptively choose neighbor sampling strategies based on the graph's properties.\n\n- For Challenge 2: A hierarchical attention scheme that combines fine-grained local attention with coarse-grained global attention, using graph coarsening techniques to capture long-range dependencies efficiently.\n\nTo identify the source papers, we look for key phrases indicating direct inspiration or motivation, along with the references that are methodologically similar to the novel contributions of the paper.\n\nBased on the text, the following references are identified as the most pertinent sources that directly influenced the paper's contributions:\n\n- b2: \"contextual bandit algorithms with supervised learning guarantees\" - This paper likely inspired the use of a multi-armed bandit approach for adaptive node sampling (confidence high due to the explicit mention of modifying Exp4.P method from this reference).\n- b25: \"graph reduction with spectral and cut guarantees\" - This work might have influenced the hierarchical attention scheme in the domain of graph coarsening (confidence high as the paper mentions using graph coarsening algorithms from this reference).\n- b8: \"a generalization of transformer networks to graphs\" - This paper seems to be a starting point for the idea of generalizing Transformer to graph data (confidence moderate).\n- b24: \"bandit samplers for training graph neural networks\" - This paper might have provided insights into the use of bandit algorithms for node sampling in GNNs, which could be adapted for the Graph Transformer (confidence moderate).\n\nOther references such as b6, b7, and b32 are foundational works on the Transformer architecture and its application in NLP and CV but are less likely to be the direct source of inspiration for the novel contributions of the paper in question, as they are broad and not specific to the challenges addressed.\n\nReturning the result in JSON format with the reference number as the key and the confidence score as the value:\n\n```json\n{\n  \"b2\": 0.9,\n  \"b25\": 0.9,\n  \"b8\": 0.7,\n  \"b24\": 0.7\n}\n```\n\nNote: The confidence scores are subjective estimates based on the provided text and the task instructions. In practice, a more thorough analysis of the full paper and cited works would be necessary to confirm these assessments."], "628749415aee126c0ffee49c": ["{\n  \"b20\": 0.9,\n  \"b32\": 0.8,\n  \"b37\": 0.8\n}", "{\n  \"b20\": 0.9,\n  \"b32\": 0.8,\n  \"b37\": 0.8\n}", "{\n    \"b20\": 0.9,\n    \"b32\": 0.8,\n    \"b37\": 0.8\n}", "{\n    \"b20\": 0.9,\n    \"b32\": 0.9,\n    \"b37\": 0.9\n}", "{\n  \"b20\": 0.9,\n  \"b32\": 0.8,\n  \"b37\": 0.8,\n  \"b6\": 0.7\n}"], "623004305aee126c0f9b3299": ["Based on the text and the titles of the references, the source papers that highly influenced the main contributions of the paper could be:\n\n{\n\"b11\": 0.9, \n\"b4\": 0.85, \n\"b7\": 0.85, \n\"b18\": 0.7,\n\"b19\": 0.65,\n\"b28\": 0.6,\n\"b31\": 0.6,\n\"b3\": 0.5\n}\n\nIn the text, the author mentioned that the GRAND+ (this paper's proposed method) is inspired by GRAND [b11], hence the confidence score for [b11] is high. The paper also specifically mentioned that they adopted a method from [b4] and [b7], hence the confidence score for these two references are also high. The other references such as [b18], [b19], [b28], [b31], and [b3] are related to the challenges and methods discussed in the paper, hence they are also included but with lower confidence scores.", "Based on the given text and the titles of the references, the source papers that most likely inspired the author the most are:\n\n{\n\"b11\": 0.9,\n\"b4\": 0.8,\n\"b7\": 0.8,\n\"b18\": 0.7,\n\"b19\": 0.7,\n\"b28\": 0.6,\n\"b30\": 0.6\n}\n\nThe paper \"Graph random neural network for semi-supervised learning on graphs\" (b11) is likely the most influential, as the author directly mentions it and proposes an extension of it (GRAND+). The papers \"Scaling graph neural networks with approximate pagerank\" (b4) and \"Scalable graph neural networks via bidirectional propagation\" (b7) are also likely significant due to their relevance to the proposed method. The papers \"Semi-supervised classification with graph convolutional networks\" (b18), \"Predict then propagate: Graph neural networks meet personalized pagerank\" (b19), and \"Simplifying graph convolutional networks\" (b28) could have provided the author with necessary background knowledge and methodologies. \"Graphsaint: Graph sampling based inductive learning method\" (b30) is mentioned in the comparison, indicating it could be a source of inspiration or a benchmark the authors aimed to surpass.", "Based on the text analysis, the primary challenges outlined in the paper are the scalability issues when dealing with large graphs and the generalization of Graph Neural Networks (GNNs). The author addresses these challenges by proposing a new framework, \"GRAND+\", which improves upon the Graph Random Neural Network (GRAND) by introducing efficient approximation techniques for random propagation in a mini-batch manner, and a confidence-aware loss for consistency regularization.\n\nThe source papers that most directly inspired the work on GRAND+ can be identified by key phrases in the text. \n- 'b11': \"To overcome this issue, the graph random neural network (GRAND) [b11] designs graph data augmentation and consistency regularization strategies for GNNs.\" The GRAND+ is a scalability improvement over GRAND, and thus 'b11' is a major source paper.\n- 'b4' and 'b7': \"Inspired by recent matrix approximation-based GNNs [b4] [b7] , GRAND+ adopts an approximation method-Generalized Forward Push (GFPush)-to efficiently calculate the generalized propagation matrix.\" Both 'b4' and 'b7' seem to inspire the key methodology of GRAND+.\n- 'b18': \"The graph convolutional network (GCN) [b18] generalizes the convolution operation into graphs.\" GCN is a fundamental concept for this paper, thus 'b18' is a source paper.\n\nBased on the titles of the references, these papers seem to be the most relevant:\n- 'b11': \"graph random neural network for semi supervised learning on graphs\"\n- 'b4': \"scaling graph neural networks with approximate pagerank\"\n- 'b7': \"scalable graph neural networks via bidirectional propagation\"\n- 'b18': \"semi-supervised classification with graph convolutional networks\" \n\nThe JSON format result is:\n{\n\"b11\": 1,\n\"b4\": 0.9,\n\"b7\": 0.9,\n\"b18\": 0.8\n}", "Based on the provided information, the source papers can be identified as:\n\n1. \"Graph random neural network for semi-supervised learning on graphs\" [b11]: The author mentioned that to overcome the issue of generalization of GNNs, they are inspired by the graph random neural network (GRAND) which designs graph data augmentation and consistency regularization strategies for GNNs. Also, the paper presented the GRAND+ framework to address the scalability limitation of GRAND.\n\n2. \"Scaling graph neural networks with approximate pagerank\" [b4]: This paper is mentioned as having inspired the approximation method-Generalized Forward Push (GFPush)- adopted by GRAND+ in order to efficiently calculate the generalized propagation matrix. \n\n3. \"Scalable graph neural networks via bidirectional propagation\" [b7]: Along with [b4], this paper also inspired the approximation method used in GRAND+.\n\n4. \"Semi-supervised classification with graph convolutional networks\" [b18]: This paper is mentioned multiple times in the provided text and can be inferred as one of the source papers. Though it's not explicitly stated that this paper inspired the author, it is cited while explaining the problem the author is addressing and the solution they are proposing.\n\nBased on this, the result can be returned in the JSON format as:\n{ \"b11\": 1, \"b4\": 0.85, \"b7\": 0.85, \"b18\": 0.75 }", "Based on the text and the titles of the references, the source papers which inspire the given paper most are:\n\n{ \"b11\": 1.0, \"b4\": 0.9, \"b7\": 0.8, \"b18\": 0.7, \"b28\": 0.6, \"b19\": 0.5, \"b30\": 0.4, \"b6\": 0.3 } \n\nExplanation: \n\n- [b11] \"Graph random neural network for semi-supervised learning on graphs\" is the main paper that the authors are extending with their GRAND+ framework. They reference it multiple times and build their work upon it, hence the high confidence score of 1.0.\n- [b4] \"Scaling graph neural networks with approximate pagerank\" and [b7] \"Scalable graph neural networks via bidirectional propagation\" are mentioned as inspiration for the GRAND+ adopts an approximation method, hence the confidence scores of 0.9 and 0.8 respectively.\n- [b18] \"Semi-supervised classification with graph convolutional networks\" and [b28] \"Simplifying graph convolutional networks\" are both cited in the context of the challenges that GNNs face, so they likely provided important background and context for this work, hence the confidence scores of 0.7 and 0.6 respectively.\n- [b19] \"Predict then propagate: graph neural networks meet personalized pagerank\", [b30] \"Graphsaint graph sampling based inductive learning method\", and [b6] \"Fastgcn fast learning with graph convolutional networks via importance sampling\" are all cited as examples of previous work that the authors are improving upon with this paper, hence the confidence scores of 0.5, 0.4, and 0.3 respectively."], "629587485aee126c0fe15059": ["{\n  \"b43\": 0.9,\n  \"b44\": 0.95,\n  \"b53\": 0.8,\n  \"b55\": 0.75\n}", "{\n  \"b44\": 0.9,\n  \"b10\": 0.85,\n  \"b63\": 0.8,\n  \"b51\": 0.75\n}", "{\n    \"b44\": 0.9,\n    \"b53\": 0.85,\n    \"b10\": 0.8,\n    \"b63\": 0.75\n}", "{\n    \"b44\": 0.95,\n    \"b10\": 0.85,\n    \"b63\": 0.8,\n    \"b51\": 0.75\n}", "{\n    \"b44\": 0.9,\n    \"b53\": 0.9,\n    \"b63\": 0.8\n}"], "634d809490e50fcafd4e683f": ["Based on the provided text, the papers that have the most influence on the author's research are:\n\n1. \"a method for estimating the grouping of participants in classroom group work using only audio information\" - This seems to be the main topic of the paper and the primary source of inspiration.\n2. \"renovating assessment for the future design based implementation research for a learning in class monitoring system based on the learning sciences\" [b0] - The paper mentions the use of this method to analyze and visualize the learning process.\n3. \"ibm cloud api docs speech to text\" [b1] - The ASR (Automatic Speech Recognition) used to convert speech data into text in this study likely uses the techniques detailed in this paper.\n4. \"acoustic event localization using a crosspower spectrum phase based technique\" [b14] - The author uses the cross-spectrum phase analysis (CSP) for their research which is mentioned in this paper.\n5. \"discourse parsing a decision tree approach\" [b11] - The author discusses the challenge of estimating groupings by accumulating pairs of utterances that match the context of the texts, a concept which might have been inspired by this paper.\n6. \"a tutorial on spectral clustering\" [b18] - The author uses spectral clustering mentioned in this paper for dividing the students into groups.\n\nTherefore, the final result can be:\n\n{\n\"b0\": 0.85,\n\"b1\": 0.8,\n\"b11\": 0.75,\n\"b14\": 0.9,\n\"b18\": 0.8,\n\"None\": 1\n}", "Based on the provided text, the source papers that seem to inspire the author the most are:\n\n- \"A method for estimating the grouping of participants in classroom group work using only audio information\": The author seems to be heavily relying on this paper as it is directly related to the core contributions of the author's paper.\n- \"Acoustic event localization using a crosspower spectrum phase based technique [b14]\": This paper is directly referenced as the method being used for the proposed approach in the author's paper.\n- \"A tutorial on spectral clustering [b18]\": This paper is mentioned specifically when the author is explaining the methodology used for clustering in the proposed approach, indicating its importance for the work.\n- \"A statistical model based voice activity detection [b15]\": This paper seems to be a source of inspiration for the author when handling the Voice Activity Detection issue in the proposed method.\n- \"A simple but efficient real time voice activity detection algorithm [b16]\": This paper is also mentioned in the context of the Voice Activity Detection, which makes it a potential source of inspiration for the author.\n\nThe other references are mentioned in the context of comparing the author's approach with existing methods, but they don't seem to be direct sources of inspiration for the author's work.\n\nThe result in the JSON format:\n\n{\n  \"None\": 0.9,\n  \"b14\": 0.8,\n  \"b18\": 0.8,\n  \"b15\": 0.7,\n  \"b16\": 0.7\n}\n", "Based on the given text, the source papers of the academic paper are likely as follows:\n\n- \"A method for estimating the grouping of participants in classroom group work using only audio information\": This is likely the main source paper as it directly relates to the novel algorithm proposed in the text that estimates the grouping of participants in classroom group work using only audio information. This paper likely inspired the main method proposed in the text. Confidence Score: 1\n\n- \"Renovating assessment for the future design based implementation research for a learning in class monitoring system based on the learning sciences\" [b0]: This paper is referenced during the explanation of the scenario that the proposed method is applied to. It likely laid the groundwork for the novel contribution of the text. Confidence Score: 0.9\n\n- \"Acoustic event localization using a crosspower spectrum phase based technique\" [b14]: This paper is directly mentioned when the author is explaining the method used in the proposed algorithm. Therefore, it is highly likely that it heavily influenced the main contribution of the text. Confidence Score: 1\n\n- \"A statistical model based voice activity detection\" [b15] and \"A simple but efficient real time voice activity detection algorithm\" [b16]: These two papers are mentioned when the author is explaining the voice activity detection (VAD) technique used in the proposed method. These papers likely influenced this specific aspect of the proposed algorithm. Confidence Score: 0.8\n\n- \"A tutorial on spectral clustering\" [b18]: This paper is directly mentioned when explaining the clustering process of the proposed method. It likely heavily influenced the clustering aspect of the text's main contribution. Confidence Score: 1\n\nThe JSON format of the source papers with confidence scores is:\n\n{\n\"None\": 1,\n\"b0\": 0.9,\n\"b14\": 1,\n\"b15\": 0.8,\n\"b16\": 0.8,\n\"b18\": 1\n}", "Based on the text and titles of the referenced papers, the following references seem to be the most influential for the paper's main contributions:\n\n1. \"a method for estimating the grouping of participants in classroom group work using only audio information\" - This is probably the main source paper as it proposes a similar methodology for grouping participants in a classroom setting.\n2. \"acoustic event localization using a crosspower spectrum phase based technique\" [b14] - The paper extensively uses the cross-spectrum phase analysis (CSP) technique to measure the degree of leaked speech, which is a key part of their proposed method.\n3. \"a tutorial on spectral clustering\" [b18] - The paper uses spectral clustering to group audio tracks into multiple subgroups, which is an integral part of their proposed method.\n4. \"renovating assessment for the future design based implementation research for a learning in class monitoring system based on the learning sciences\" [b0] - This paper is likely a source of inspiration for the author as it promotes a new form of education involving active learning.\n5. \"ibm cloud api docs speech to text\" [b1] - The paper uses automatic speech recognition to convert speech tracks into text, which is a crucial step in their proposed method.\n\nThe confidence score for each reference is inferred based on the frequency and importance of the concepts mentioned in the paper:\n\n{\n\"None\": 1.0,\n\"b14\": 0.9,\n\"b18\": 0.9,\n\"b0\": 0.8,\n\"b1\": 0.8\n}", "Based on the text provided, the source papers that seem to have most directly influenced the work are:\n\n1. 'A method for estimating the grouping of participants in classroom group work using only audio information' - This paper seems to be the most directly related to the work being discussed, as its title suggests a similar methodology and objective. However, it has not been explicitly cited in the text so its influence cannot be confirmed.\n2. 'Acoustic event localization using a crosspower spectrum phase based technique' [b14] - This paper is likely a major source, as the method it proposes (cross-spectrum phase analysis) is adopted in the work being discussed.\n3. 'A statistical model based voice activity detection' [b15] - The work discusses Voice Activity Detection (VAD) and mentions model-based methods, suggesting that this source may have been influential.\n4. 'A simple but efficient real time voice activity detection algorithm' [b16] - Similar to [b15], this paper on voice activity detection is likely to have been a key source.\n5. 'A tutorial on spectral clustering' [b18] - Spectral clustering is a major part of the proposed method in the work being discussed, indicating that this tutorial may have been a significant resource.\n6. 'Multiple emitter location and signal parameter estimation' [b2] - This paper may have influenced the work's approach to estimating speaker location, a key aspect of the problem being addressed.\n\nThe confidence scores for these papers, however, would require further analysis of the paper text to confirm the extent of their influence.\n\nHere is a representation of the results in JSON format:\n\n{\n  \"b14\": 0.9,\n  \"b15\": 0.8,\n  \"b16\": 0.8,\n  \"b18\": 0.7,\n  \"b2\": 0.6\n}"], "630359ec90e50fcafd88ceba": ["Based on the information provided, the source papers that most likely inspired the given academic paper can be identified as follows:\n\n- \"Discriminative jackknife: quantifying uncertainty in deep learning via higher order influence functions\" [b1]: This paper seems to have inspired the main methodology of the given paper, which is to use jackknife resampling to quantify uncertainty in Graph Convolutional Networks (GCN).\n\n- \"Residuals and influence in regression\" [b8]: This paper might have provided the theoretical basis for the use of influence functions in the given paper.\n\n- \"Graph Convolutional Networks on Graphs with Fast Localized Spectral Filtering\" [b10]: This paper likely provided a foundation for the use of GCN in the given paper.\n\n- \"Graph Convolutional Networks for Semi-Supervised Learning\" [b26]: This paper might have been a significant influence as it introduces the use of GCN for semi-supervised learning, which is a concept used in the given paper.\n\n- \"Understanding black-box predictions via influence functions\" [b27]: This paper provides a theoretical background for the use of influence functions to understand the impact of data points on the model.\n\n- \"The Jackknife: A Review\" [b33]: This paper likely provided a comprehensive understanding of jackknife resampling, which is the main method used in the given paper.\n\n- \"Uncertainty estimation using a single deep deterministic neural network\" [b41]: This paper could have inspired the use of a single neural network for uncertainty estimation in the given paper.\n\nHere is the result in JSON format:\n\n{\n\"b1\": 0.9,\n\"b8\": 0.8,\n\"b10\": 0.7,\n\"b26\": 0.85,\n\"b27\": 0.8,\n\"b33\": 0.9,\n\"b41\": 0.8\n}\n\nPlease note that the confidence scores are estimated based on the given information and the actual influence may vary.", "Based on the given text, the most relevant source papers that have inspired the authors are:\n\n{ \"b1\": 0.9, \"b2\": 0.8, \"b33\": 0.8, \"b27\": 0.7, \"b24\": 0.6, \"b39\": 0.5, \"b48\": 0.5, \"b46\": 0.4 }\n\nExplanation: The paper \"b1\" is the most relevant as it presented a frequentist-based method inspired by jackknife resampling, which is the main method used in the given paper. The paper \"b2\" is also crucial as it refers to the jackknife approach. The paper \"b33\" introduces the jackknife methodology, which is a fundamental part of the given paper. The paper \"b27\" is relevant as it introduces the concept of influence functions referred to in the given paper. The paper \"b24\" seems to be linked to the given paper as it is referred to in the context of first-order derivative of GCN. The papers \"b39\" and \"b48\" are relevant as they discuss the uncertainty quantification on a graph neural network, a main topic in the given paper. The paper \"b46\" is important as it discusses the homophily assumption with respect to uncertainty/confidence, which is also mentioned in the given paper.", "Based on the text, the source papers that inspire the authors the most are:\n\n1. \"Discriminative Jackknife: Quantifying Uncertainty in Deep Learning via Higher Order Influence Functions\" [b1]\n2. \"Jackknife After Bootstrap: Standard Errors and Influence Functions\" [b12]\n3. \"Graph Convolutional Network\" (No reference number given in the text, inferred from the context.)\n4. \"Understanding Black-Box Predictions via Influence Functions\" [b27]\n5. \"The Jackknife: A Review\" [b33]\n6. \"Rawls GCN: Towards Rawlsian Difference Principle on Graph Convolutional Network\" [b24]\n7. \"Bayesian Graph Neural Networks with Adaptive Connection Sampling\" [b20]\n8. \"Uncertainty-Aware Semi-Supervised Learning on Graph Data\" [b48]\n\nHere is the result in json format:\n\n{\n    \"b1\": 0.9,\n    \"b12\": 0.9,\n    \"GCN\": 0.8,\n    \"b27\": 0.85,\n    \"b33\": 0.85,\n    \"b24\": 0.8,\n    \"b20\": 0.75,\n    \"b48\": 0.85\n}\n\nThe confidence scores are estimated based on the importance of the key phrases and the relevance of the source papers to the paper's novel contributions. Higher scores indicate stronger influence.", "Based on the given text, the source papers that most directly inspired the author are:\n\n1. \"Discriminative Jackknife: Quantifying Uncertainty in Deep Learning via Higher Order Influence Functions\" [b1]: The author uses concepts from this paper to provide a frequentist-based analysis of GCN uncertainty and the key idea to leverage a jackknife estimator to construct a leave-one-out predictive confidence interval for each node.\n\n2. \"The Jackknife: A Review\" [b33]: The author uses the general principle of jackknife (leave-one-out) resampling outlined in this paper to estimate the GCN parameters without exhaustively re-training GCN.\n\n3. \"Understanding Black Box Predictions via Influence Functions\" [b27]: The author adopts the influence functions to quantify the change in GCN parameters by infinitesimally upweighting the loss of a training node.\n\n4. \"FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling\" [b7]: This paper seems to be referred by the author to demonstrate the importance of optimizing the loss of a node in graph convolutional networks (GCN).\n\n5. \"Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\" [b10]: It appears that the author cites this paper as a source of inspiration for the GCN architecture utilized in their study.\n\n6. \"Semi-supervised Classification with Graph Convolutional Networks\" [b26]: The author refers to this paper when discussing semi-supervised node classification and its role in the creation of their own method.\n\nThe confidence scores for each paper would be determined through further textual analysis and understanding of the relevance of each paper to the novel contributions of the author's work. \n\nThe JSON output may look like this:\n\n```\n{\n\"b1\": 0.9,\n\"b33\": 0.8,\n\"b27\": 0.8,\n\"b7\": 0.7,\n\"b10\": 0.7,\n\"b26\": 0.7\n}\n```\n\nPlease note that the confidence scores are hypothetical and used for illustrative purposes only. The actual scores would need to be determined through a more detailed analysis of the text.", "Based on the text and the relevant key phrases, the main source papers that inspired the given paper are:\n\n1. \"Discriminative Jackknife: Quantifying Uncertainty in Deep Learning via Higher Order Influence Functions\" (b1)\n2. \"Jackknife After Bootstrap: Standard Errors and Influence Functions\" (b12)\n3. \"Understanding Black Box Predictions via Influence Functions\" (b27)\n4. \"The Jackknife: A Review\" (b33)\n5. \"Uncertainty-Aware Semi-Supervised Learning on Graph Data\" (b48)\n\nThe JSON format of the sources and their confidence scores is:\n\n{\n    \"b1\": 0.95,\n    \"b12\": 0.9,\n    \"b27\": 0.85,\n    \"b33\": 0.8,\n    \"b48\": 0.95\n} \n\nThe scores are high as these references were directly mentioned in the context of the novel contributions of the paper, indicating a strong reliance on these works for developing the core contributions of the paper."], "629435a05aee126c0f2fe317": ["Based on the given text and the provided information, the source papers that inspired the authors the most are:\n\n1. 'MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing' [b0]\n2. 'Graph Neural Networks with Convolutional ARMA Filters' [b2]\n3. 'Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective' [b1]\n4. 'When Does a Spectral Graph Neural Network Fail in Node Classification' [b7]\n5. 'Graph Structure Learning for Robust Graph Neural Networks' [b15]\n6. 'All You Need is Low (Rank): Defending Against Adversarial Attacks on Graphs' [b11]\n7. 'On the Relationship Between Heterophily and Robustness of Graph Neural Networks' [b35]\n\nThese papers are selected based on the fact that they are directly related to the novel contributions of the given paper (the development of EvenNet, a spectral GNN that can generalize across graphs of different homophily) and that they are cited with phrases indicating direct inspiration or motivation.\n\nIn JSON format with confidence scores (assuming a uniform distribution of confidence), the references would look like:\n\n{\n\"b0\": 0.14,\n\"b2\": 0.14,\n\"b1\": 0.14,\n\"b7\": 0.14,\n\"b15\": 0.14,\n\"b11\": 0.14,\n\"b35\": 0.14\n}", "Based on the text and author's notes, the source papers that inspired this work are:\n\n1. \"On the relationship between heterophily and robustness of graph neural networks\" [b35]\n2. \"Beyond homophily in graph neural networks current limitations and effective designs\" [b36]\n3. \"Robust graph convolutional networks against adversarial attacks\" [b34]\n4. \"Adaptive universal generalized pagerank graph neural network\" [b8]\n5. \"Graph neural networks with convolutional arma filters\" [b2]\n6. \"Bernnet learning arbitrary graph spectral filters via bernstein approximation\" [b14]\n7. \"When does a spectral graph neural network fail in node classification\" [b7]\n8. \"Structural balance a generalization of heider s theory\" [b5]\n\nThe JSON format that represents these source papers with their reference number as the key and a confidence score of 1 as the value (since these papers are explicitly referenced in the text) is:\n\n{\n\"b35\": 1,\n\"b36\": 1,\n\"b34\": 1,\n\"b8\": 1,\n\"b2\": 1,\n\"b14\": 1,\n\"b7\": 1,\n\"b5\": 1\n}", "Based on the text of the paper and the titles of the references, the most pertinent source papers that inspired this work are:\n\n- \"evennet ignoring odd hop neighbors improves robustness of graph neural networks\" (The paper's main contribution, EvenNet, is directly named after this work)\n- \"when does a spectral graph neural network fail in node classification\" (This work seems to be directly addressing the challenges outlined in this paper)\n- \"beyond homophily in graph neural networks current limitations and effective designs\" (The paper mentions several times the limitations of homophily and heterophily, which is a main focus of this work)\n- \"on the relationship between heterophily and robustness of graph neural networks\" (This work seems to be directly addressing the challenges outlined in this paper)\n- \"graph neural networks with convolutional arma filters\" (The paper mentions several times the use of spectral GNNs and their limitations, which is a main focus of this work)\n- \"robust graph convolutional networks against adversarial attacks\" (The paper mentions several times the problem of adversarial attacks, which is a main focus of this work)\n- \"adversarial attacks on graph neural networks via meta learning\" (The paper mentions several times the problem of adversarial attacks, which is a main focus of this work)\n\nBased on the above analysis, here is the JSON output:\n\n{\n    \"evennet ignoring odd hop neighbors improves robustness of graph neural networks\": 1.0,\n    \"when does a spectral graph neural network fail in node classification\": 0.9,\n    \"beyond homophily in graph neural networks current limitations and effective designs\": 0.9,\n    \"on the relationship between heterophily and robustness of graph neural networks\": 0.8,\n    \"graph neural networks with convolutional arma filters\": 0.7,\n    \"robust graph convolutional networks against adversarial attacks\": 0.7,\n    \"adversarial attacks on graph neural networks via meta learning\": 0.7\n}", "Based on the text and the titles of the references, the most pertinent source papers that inspired the given paper are:\n- b2: 'graph neural networks with convolutional arma filters'\n- b8: 'adaptive universal generalized pagerank graph neural network'\n- b14: 'bernnet learning arbitrary graph spectral filters via bernstein approximation'\n- b21: 'geom gcn geometric graph convolutional networks'\n- b33: 'gnnguard defending graph neural networks against adversarial attacks'\n- b34: 'robust graph convolutional networks against adversarial attacks'\n- b36: 'beyond homophily in graph neural networks current limitations and effective designs'\nThe confidence score can be assigned based on the frequency of these papers being referenced in the text and how close their titles are to the main contributions of the given paper.\n\nThe resulting JSON format would look like this:\n```\n{\n\"b2\": 0.8,\n\"b8\": 0.85,\n\"b14\": 0.8,\n\"b21\": 0.7,\n\"b33\": 0.75,\n\"b34\": 0.7,\n\"b36\": 0.9\n}\n```", "Based on the provided text, the primary challenges outlined in the paper are the robustness issues of Graph Neural Networks (GNNs) and their performance under adversarial attacks and homophily-heterophily inductive learning tasks. The authors propose EvenNet, inspired by the balance theory, to address these challenges.\n\nThe key phrases indicating inspiration are \"inspired by balance theory\", which refers to the balance theory [b5] titled \"Structural balance: a generalization of Heider\u2019s theory\". Thus, this is a significant source paper.\n\nOther source papers identified based on their relevance with the challenges and the novel method proposed include:\n\n1. [b9] \"Convolutional neural networks on graphs with fast localized spectral filtering\" - as it is mentioned in the context of spectral GNNs, which are a key focus of the paper.\n2. [b2] \"Graph neural networks with convolutional ARMA filters\" and [b8] \"Adaptive universal generalized pagerank graph neural network\" - both are mentioned as part of the research into spectral GNNs with learnable polynomial filters, which seems to be a significant aspect of the methodology proposed in the paper.\n3. [b35] \"On the relationship between heterophily and robustness of graph neural networks\" - it established the relationship between graph homophily and structural attacks, which is a primary challenge being addressed in the paper.\n4. [b7] \"When does a spectral graph neural network fail in node classification\" - this paper is referenced during the analysis of graph filters and the transformed homophily degree, which is a crucial aspect of the methodology and analysis in the paper.\n\nThe references [b5], [b9], [b2], [b8], [b35], and [b7] are thus identified as the source papers that have most inspired the current paper.\n\nIn JSON format:\n\n{\n  \"b5\": 0.9,\n  \"b9\": 0.8,\n  \"b2\": 0.8,\n  \"b8\": 0.8,\n  \"b35\": 0.9,\n  \"b7\": 0.8\n}"], "626754c85aee126c0fbcdd75": ["Based on the given text and the provided instructions, the following papers appear to be the most influential to the author's work:\n\n1. **\"SuperGlue: Learning Feature Matching with Graph Neural Networks\" (b30)**: The author mentions that their work is inspired by this paper, and they use some of its methods as a starting point for their own research. They also compare their methods and results with it frequently throughout the text.\n\n2. **\"Routing Transformer (b27)**: The author specifically mentions being inspired by this paper in their methodology for a coarse-to-fine cluster-based GNN.\n\n3. **\"Attention is All You Need (b41)**: Transformer based GNNs from this paper are applied in the author's research.\n\n4. **\"Learning to Match Features with Seeded Graph Matching Network (b5)**: This paper is mentioned in the context of prior works that have used GNNs for feature matching, a key element of the author's research.\n\n5. **\"Efficient Content-Based Sparse Attention with Routing Transformers (b27)**\": The author cites this paper when explaining their own approach to attention in GNNs.\n\n6. **\"Learning Two-View Correspondences and Geometry Using Order-Aware Network (b46)**: This paper is mentioned in the context of deep learning methods for feature matching, a central topic in the author's research.\n\n7. **\"Neighbourhood Consensus Networks (b26)**: The author uses the Dual-softmax operator from this paper in their own matching module.\n\n8. **\"Blockwise Self-Attention for Long Document Understanding (b23)**: This paper is referenced when discussing efforts to reduce the quadratic complexity of attention.\n\nThe confidence scores for each source paper are difficult to determine without a numerical evaluation method. However, based on the frequency and context of mentions, the paper \"SuperGlue: Learning Feature Matching with Graph Neural Networks\" (b30) likely has the highest confidence score, followed by \"Routing Transformer\" (b27) and \"Attention is All You Need\" (b41).\n\nThe final output in JSON format would be:\n\n{\n\"b30\": 0.9,\n\"b27\": 0.8,\n\"b41\": 0.8,\n\"b5\": 0.7,\n\"b27\": 0.7,\n\"b46\": 0.6,\n\"b26\": 0.6,\n\"b23\": 0.6\n}", "Based on the text and author notes, the following references seem to be the most influential for this paper:\n\n1. \"SuperGlue: Learning Feature Matching with Graph Neural Networks\" [b30]: This paper is mentioned multiple times throughout the text as the method the author's propose is developed to build upon and improve the performance of SuperGlue. The authors adopt the idea of feature matching using graph neural networks from this paper but aim to reduce the redundancy and computational complexity. Confidence score: 0.9\n\n2. \"Routing Transformer\" [b27]: The authors mention that their proposed approach of a coarse-to-fine cluster-based GNN is inspired by the Routing Transformer. They apply a similar clustering approach to reduce complexity and improve efficiency. Confidence score: 0.85\n\n3. \"Efficient Content-Based Sparse Attention with Routing Transformers\" [b16]: This paper is mentioned as a source of inspiration for the method of using content-based sparse attention, which is an important part of the authors' approach for efficient feature matching. Confidence score: 0.75\n\n4. \"Learning to Match Features with Seeded Graph Matching Network\" [b5]: This paper is mentioned as one of the works that proposed learning feature matching using graph neural networks, which forms the basis for the authors' approach. Confidence score: 0.7\n\n5. \"Attention is All You Need\" [b41]: This paper is cited as the source of the Transformer model, which is used in the SuperGlue method that the authors build upon. Confidence score: 0.7\n\n6. \"Longformer: The Long Document Transformer\" [b1]: This paper is mentioned in the context of approximating attention by lowering the sequence dimension - a method that the authors argue is not suitable for feature matching, thereby informing their own approach. Confidence score: 0.6\n\n7. \"Rethinking Attention with Performers\" [b8]: This paper is mentioned in the context of approximating attention with kernels - a method that the authors argue is not suitable for feature matching, thereby informing their own approach. Confidence score: 0.6\n\nHere is the result in json format:\n{\"b30\": 0.9, \"b27\": 0.85, \"b16\": 0.75, \"b5\": 0.7, \"b41\": 0.7, \"b1\": 0.6, \"b8\": 0.6}", "Based on the text and the author's notes, the most pertinent source papers that inspired this paper are:\n\n1. \"SuperGlue: Learning feature matching with graph neural networks\" [b30]: The paper directly mentions SuperGlue and proposes a sparser alternative to reduce the propagation of redundant messages, achieving a significant decrease in runtime and memory while preserving matching performance. It is clear that this paper is the main inspiration for the proposed method.\n\n2. \"Routing Transformer\" [b27]: The proposed method is directly inspired by the Routing Transformer. The authors adopt a coarse-to-fine cluster-based GNN to learn the feature matching task with lower redundancy and computational complexity.\n\n3. \"Efficient content-based sparse attention with routing transformers\" [b16]: This work is referenced in the context of content-based sparse attention, which the authors use to design their learned clustering module.\n\n4. \"Longformer: The long document transformer\" [b1]: This paper is mentioned in the context of approximating attention, which is a key aspect of the authors' proposed method.\n\n5. \"Learning to match features with seeded graph matching network\" [b5]: This paper is referenced in the context of learning to match features, which is a key challenge that the authors' paper addresses.\n\n6. \"Rethinking attention with performers\" [b8]: This work is mentioned in the context of approximating attention, which is a key aspect of the authors' paper.\n\nBased on these considerations, the JSON format answer might be:\n\n{\n\"b30\": 1,\n\"b27\": 0.9,\n\"b16\": 0.8,\n\"b1\": 0.7,\n\"b5\": 0.7,\n\"b8\": 0.7\n}\nThis means that the paper [b30] is the most influential source paper with a confidence score of 1, followed by [b27] with a score of 0.9, and so on.", "Based on the text of the paper, the most pertinent source papers that inspired the author are:\n\n1. \"SuperGlue: Learning Feature Matching with Graph Neural Networks\" [b30]: The paper mentions this work multiple times, specifically stating that it builds on its success but proposes a sparser alternative to reduce the propagation of redundant messages. \n\n2. \"Efficient Content-Based Sparse Attention with Routing Transformers\" [b27]: The author directly mentions being inspired by this paper to propose a coarse-to-fine cluster-based GNN to learn the feature matching task with lower redundancy and computational complexity.\n\n3. \"Detector-Free Local Feature Matching with Transformers\" [b36]: This work is mentioned in the context of previous attempts to learn the feature matching task with attention-based GNNs, which the current paper aims to improve on.\n\n4. \"Attention is All You Need\" [b41]: This paper is referenced in the context of the use of Transformer based GNNs in the SuperGlue method and the application of multi-head attention in the proposed method.\n\n5. \"Longformer: The Long Document Transformer\" [b1]: This work is referenced in the context of approximating attention for reducing computational and memory complexity, which is less suitable for feature matching, thereby indicating a motivation to develop a different approach.\n\n6. \"Rethinking Attention with Performers\" [b8]: Similar to [b1], this work is also referred to in the context of reducing attention dimensions, which is deemed less suitable for feature matching.\n\n7. \"Sinkhorn Distances: Lightspeed Computation of Optimal Transport\" [b9]: This paper is referenced in the context of learning-based feature detectors and descriptors, indicating that it may have influenced the methods used in the current paper.\n\nThe confidence scores for each of these references can be subjectively assigned based on the extent of influence inferred from the text.\n\nThe final result in JSON format would be:\n{\n\"b30\": 0.9,\n\"b27\": 0.8,\n\"b36\": 0.7,\n\"b41\": 0.7,\n\"b1\": 0.6,\n\"b8\": 0.6,\n\"b9\": 0.6\n}", "Based on the given text and provided notes, the following papers are likely to have been the most significant sources, inspiring the development of the paper:\n\n1. \"SuperGlue: Learning Feature Matching with Graph Neural Networks\" [b30]: This paper is mentioned multiple times and the paper's proposed method seems to build directly upon the work done in SuperGlue, aiming to improve upon its limitations.\n\n2. \"Routing Transformer: Efficient Content-Based Sparse Attention with Routing Transformers\" [b27]: This paper is directly stated as the inspiration for the proposed method's coarse-to-fine cluster-based GNN.\n\n3. \"Learning to match features with seeded graph matching network\" [b5]: It is mentioned in the context of previous works that have attempted to learn the task of feature matching using GNNs.\n\n4. \"Efficient content-based sparse attention\" [b16]: This work is mentioned as an inspiration for the way they propose clustering.\n\n5. \"Attention is all you need\" [b41]: This paper is frequently referred to throughout the text, especially in the context of applying attention mechanisms in GNNs.\n\n6. \"Longformer: The long document transformer\" [b1]: This paper is mentioned in the context of other works that have aimed to improve the efficiency of attention.\n\n7. \"Blockwise self-attention for long document understanding\" [b23]: This paper is mentioned in the context of works that have attempted to reduce the complexity of attention.\n\n8. \"Rethinking attention with performers\" [b8]: Mentioned regarding efforts to reduce the quadratic complexity of attention.\n\nThe corresponding JSON object:\n\n```\n{\n \"b30\": 1,\n \"b27\": 0.95,\n \"b5\": 0.9,\n \"b16\": 0.85,\n \"b41\": 0.85,\n \"b1\": 0.8,\n \"b23\": 0.75,\n \"b8\": 0.75\n}\n```\n\nThe numbers in this JSON object represent confidence scores, indicating how likely each source paper has deeply influenced the paper's main contributions. The scores are on a scale of 0 (no influence) to 1 (high influence)."], "634f6ae390e50fcafdcb62af": ["Based on the provided text and the instructions, the following references seem to be the most influential sources for the paper:\n\n1. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b1]\n2. \"Language Models are Few-Shot Learners\" [b2]\n3. \"Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge\" [b4]\n4. \"An Open Multilingual Graph of General Knowledge\" [b6]\n5. \"QAGNN: Reasoning with Language Models and Knowledge Graphs for Question Answering\" [b7]\n6. \"GreaseLM: Graph Reasoning Enhanced Language Models for Question Answering\" [b8]\n7. \"A Robustly Optimized BERT Pretraining Approach\" [b17]\n\nThe paper is highly inspired by the use of pre-trained models like BERT[b1] and GPT[b2] for NLP tasks. Also, the paper has used concepts from Freebase[b4] and ConceptNet[b6] for the application of knowledge graphs. \n\nThe methodology of the paper is largely inspired by QAGNN[b7] and GreaseLM[b8] for reasoning with language models and knowledge graphs. \n\nFurther, RoBERTa[b17] is also a key inspiration for the paper, as it is used as a base model for language modeling.\n\nThe confidence score for each reference is as follows:\n\n{ \"b1\": 0.85, \"b2\": 0.8,  \"b4\": 0.8, \"b6\": 0.8, \"b7\": 0.85, \"b8\": 0.9, \"b17\": 0.8 }\n\nPlease note that these are estimations based on the provided text.", "Based on the analysis of the given paper text and author notes, the following are the most relevant sources:\n\n- \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b1]\n- \"GreaseLM: Graph Reasoning Enhanced Language Models for Question Answering\" [b8]\n- \"ERNIE: Enhanced Language Representation with Informative Entities\" [b11]\n- \"ConceptNet: An Open Multilingual Graph of General Knowledge\" [b6]\n- \"TransE: Translating Embeddings for Modeling Multi-relational Data\" [b44]\n- \"DistMult: Embedding Entities and Relations for Learning and Inference in Knowledge Bases\" [b45]\n- \"RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space\" [b46]\n- \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\" [b17]\n\nThe paper seems to be heavily influenced by these works, as it utilizes concepts such as deep bidirectional transformers, graph reasoning, entity representation, and knowledge graphs. The use of various pretraining approaches and embedding models also suggests inspiration from these sources.\n\nIn JSON format, the result would be: \n\n{\n\"b1\": 0.95,\n\"b8\": 0.95,\n\"b11\": 0.9,\n\"b6\": 0.9,\n\"b44\": 0.85,\n\"b45\": 0.85,\n\"b46\": 0.85,\n\"b17\": 0.8\n}\n\nThe confidence scores are subjective and are estimated based on the degree of relevance and the frequency of mention in the paper text.", "Based on the text, the referenced papers which might have inspired this work could be:\n\n{ \n\"b1\": 0.9, \n\"b7\": 0.8, \n\"b8\": 0.9, \n\"b11\": 0.7, \n\"b12\": 0.7, \n\"b17\": 0.6\n} \n\n'b1' is BERT, which is mentioned in the context of the success of pretraining language models, which is a concept utilized in this paper. \n\n'b7' and 'b8' are papers about integrating knowledge graphs and text for question answering, which seems to be a key idea in this paper.\n\n'b11' and 'b12' are papers about self-supervised pretraining and integrating knowledge graphs into language models, which are also used in this paper.\n\n'b17' is RoBERTa, which is mentioned as a baseline that this paper seeks to improve upon. \n\nThe confidence scores are assigned based on how closely the concepts and methods from each paper align with those in this paper. Higher scores indicate a stronger influence.", "Based on the text, the most relevant source papers are:\n\n- \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b1]\n- \"Language Models are Few-Shot Learners\" [b2]\n- \"GreaseLM: Graph Reasoning Enhanced Language Models for Question Answering\" [b8]\n- \"QAGNN: Reasoning with Language Models and Knowledge Graphs for Question Answering\" [b7]\n- \"ERNIE: Enhanced Language Representation with Informative Entities\" [b11]\n- \"KG-BERT: BERT for Knowledge Graph Completion\" [b50]\n- \"TransE: Translating Embeddings for Modeling Multi-relational Data\" [b44]\n\nThese papers are related to the main challenges and novel methods proposed in the paper, which involve the integration of text and knowledge graphs for pre-training to solve reasoning/question-answering tasks, as well as the use of self-supervised tasks and bidirectional models.\n\nNow I will calculate the confidence score for each reference:\n\n- [b1]: This paper is about BERT, a method for pre-training language representations, which the author used as the base model. Confidence Score: 0.9\n- [b2]: GPTs are mentioned as successful models for NLP tasks, which could have inspired the authors. Confidence Score: 0.7\n- [b8]: This paper presents GreaseLM, which the authors used as the architecture for their cross-modal encoder. Confidence Score: 1.0\n- [b7]: This paper proposes bidirectional models for text and KG, which could have inspired the authors. Confidence Score: 0.8\n- [b11]: This paper proposes a method for integrating entity features into LMs, which could have inspired the authors. Confidence Score: 0.7\n- [b50]: This paper proposes a BERT model for knowledge graph completion, which could have inspired the authors. Confidence Score: 0.75\n- [b44]: This paper presents TransE, a method for knowledge graph embedding. It is not directly mentioned in the text, but potentially relevant as it is a seminal paper in the field. Confidence Score: 0.6\n\nPlease note that this is a tentative analysis and the confidence scores are heuristic.\n\nHere is the final result in json format:\n\n{\"b1\": 0.9, \"b2\": 0.7, \"b8\": 1.0, \"b7\": 0.8, \"b11\": 0.7, \"b50\": 0.75, \"b44\": 0.6}", "Based on the text, here are the source papers that mostly likely inspired the author:\n\n- \"bert pre training of deep bidirectional transformers for language understanding\" [b1]: The author mentioned that their work builds upon BERT's success in language processing tasks.\n\n- \"greaselm graph reasoning enhanced language models for question answering\" [b8]: The paper's method is a direct extension of the GreaseLM model.\n\n- \"language models are few shot learners\" [b2]: GPT is mentioned, indicating that the author might have been inspired by the methods proposed in the paper.\n\n- \"ernie enhanced language representation with informative entities\" [b11]: The author criticizes the unidirectional methods of previous works such as ERNIE.\n\n- \"kepler a unified model for knowledge embedding and pre trained language representation\" [b13]: The author also criticizes the shallow methods of previous works such as KEPLER.\n\n- \"translating embeddings for modeling multi relational data\" [b44]: The author uses TransE as part of their link prediction.\n\n- \"embedding entities and relations for learning and inference in knowledge bases\" [b45]: DistMult is also used in their link prediction.\n\nHere is the result in the required format:\n\n```json\n{\n\"b1\": 0.9,\n\"b8\": 0.9,\n\"b2\": 0.8,\n\"b11\": 0.8,\n\"b13\": 0.8,\n\"b44\": 0.8,\n\"b45\": 0.8\n}\n```\n\nPlease note that the confidence scores are estimated based on the author's text and may not completely reflect the actual influence of each paper."], "6369c8cd90e50fcafde87ef2": ["Based on the text of the paper and the titles of the references, the source papers that inspired this work most likely include:\n\n{ \"b6\": 1, \"b17\": 0.9, \"b41\": 0.9, \"b84\": 0.9, \"b66\": 0.8, \"b87\": 0.8, \"b79\": 0.7, \"b83\": 0.7 } \n\nThis is because these papers are cited in key sections of the text that discuss the methods and contributions of the paper, and their titles suggest they are directly related to the topics of few-shot learning, data generation, and meta-learning.", "Based on the text of the paper and the titles of the references, the most relevant source papers that influenced the main contributions of this paper are:\n\n{\n\"b17\": 0.9,\n\"b41\": 0.8,\n\"b84\": 0.8,\n\"b6\": 0.7,\n\"b66\": 0.7,\n\"b79\": 0.6,\n\"b15\": 0.6,\n\"b16\": 0.6\n}\n\nThe paper primarily deals with the development of a method for generating training samples for pretrained language models in few-shot learning tasks. It takes inspiration from the works of [b17], [b41], [b84], [b6] which deal with few-shot learning with PLMs and fine-tuning on few-shot samples. [b66] is important as it introduces the concept of meta-learning for sample weighting which is a key aspect of the proposed method. The idea of data augmentation for consistency training from [b79] influences the approach of generating more samples for training the classifier. The method of learning to learn by gradient descent by gradient descent from [b15] and [b16] is also likely influential in the development of the proposed method.", "Based on the provided text and the titles of the references, the source papers that inspired the given paper most likely are:\n\n1. \"Language models are few-shot learners\" [b6]\n2. \"Making pre-trained language models better few-shot learners\" [b17]\n3. \"Meta-Weight-net: Learning an explicit mapping for sample weighting\" [b66]\n4. \"Plug and play language models: A simple approach to controlled text generation\" [b12]\n5. \"A plug-and-play method for controlled text generation\" [b54]\n6. \"The power of scale for parameter-efficient prompt tuning\" [b36]\n7. \"Tuning language models as training data generators for augmentation enhanced few shot learning\" [b0]\n8. \"GPT3Mix: Leveraging large scale language models for text augmentation\" [b83]\n\nThe confidence scores for these papers could be as follows:\n\n{\n\"b6\": 0.9,\n\"b17\": 0.85,\n\"b66\": 0.8,\n\"b12\": 0.75,\n\"b54\": 0.7,\n\"b36\": 0.65,\n\"b0\": 0.6,\n\"b83\": 0.55\n}", "Based on the given text and the instructions, the source papers that have the most significant influence on this paper are:\n\n1. \"b6: Language models are few-shot learners\": The author frequently refers to this paper and the concept of few-shot learning is central to the author's work.\n2. \"b17: Making pre-trained language models better few-shot learners\": The author indicates that their work is directly inspired by this paper.\n3. \"b66: Meta-weightnet learning an explicit mapping for sample weighting\": The author uses a meta objective similar to the one proposed in this paper.\n4. \"b41: P-Tuning\": The author mentions using similar prompt-based few-shot learning methods.\n5. \"b84: Differentiable prompt makes pre trained language models better few shot learners\": The author adopts similar methods to improve the performance of language models in few-shot learning.\n6. \"b87: Fine-tuning language models from human preferences\": The author fine-tunes autoregressive PLMs, which is a method also used in this paper.\n\nThe confidence scores for these papers would depend on the exact references and the specific ways they are used in the paper, which was not provided in the task. However, based on the frequency and importance of the references to these papers, the confidence scores would likely be relatively high.\n\nThe returned json format would be:\n\n{\n\"b6\": 0.9,\n\"b17\": 0.8,\n\"b66\": 0.7,\n\"b41\": 0.7,\n\"b84\": 0.7,\n\"b87\": 0.7\n}", "Based on the given text and author notes, the following references seem to be the most pertinent source papers:\n\nb6: \"Language models are few-shot learners\" - This paper discusses the concept of few-shot learning with pretrained language models, which is a key aspect of the given paper.\n\nb17: \"Making pre-trained language models better few-shot learners\" - This paper contributes to the discussion of improving few-shot learning with pre-trained language models.\n\nb41: \"P-Tuning\" - This paper introduces the idea of parameterizing prompts as continuous learnable embeddings, which seems to be a method employed in the given paper.\n\nb66: \"Meta-weightnet learning an explicit mapping for sample weighting\" - This paper discusses the concept of meta-learning for sample weighting, which seems to be applied in the given paper.\n\nb84: \"Differentiable prompt makes pre trained language models better few shot learners\" - This paper discusses the idea of improving few-shot learning with pre-trained language models, which is a key aspect of the given paper.\n\nb87: \"Fine-tuning language models from human preferences\" - This paper introduces the idea of fine-tuning language models, which seems to be a method employed in the given paper.\n\nThus, the JSON format of the source papers would be:\n\n{\n\"b6\": 0.9,\n\"b17\": 0.9,\n\"b41\": 0.7,\n\"b66\": 0.8,\n\"b84\": 0.9,\n\"b87\": 0.8\n}\n\nNote: The confidence score is estimated based on the emphasis and frequency of the concepts from each paper in the given text. A higher score indicates a stronger influence or reliance on that paper."], "6304456b90e50fcafd12fe4b": ["{\n  \"b23\": 1.0,\n  \"b20\": 0.9,\n  \"b21\": 0.9,\n  \"b45\": 0.8,\n  \"b63\": 0.8,\n  \"b22\": 0.7\n}", "{\n  \"b23\": 1.0,\n  \"b20\": 0.9,\n  \"b21\": 0.9,\n  \"b16\": 0.8,\n  \"b45\": 0.8\n}", "{\n    \"b23\": 0.9,\n    \"b22\": 0.85,\n    \"b20\": 0.8,\n    \"b21\": 0.75,\n    \"b45\": 0.7,\n    \"b16\": 0.65,\n    \"b31\": 0.6\n}", "{\n  \"b23\": 0.9,\n  \"b22\": 0.85,\n  \"b20\": 0.8,\n  \"b21\": 0.8,\n  \"b45\": 0.75\n}", "{\n  \"b23\": 1.0,\n  \"b22\": 0.9,\n  \"b1\": 0.9\n}"], "637aec2590e50fcafd929667": ["Based on the text, the following references seem to be the most influential:\n\n1. \"bit matrix multiplication for transformers at scale\" [b7]: The authors refer to this paper when discussing the limitations of existing quantization schemes for large language models, indicating that the work has been influential in shaping their own approach. Confidence score: 0.9\n\n2. \"outlier suppression pushing the limit of low bit transformer language models\" [b33]: This paper is referred to when discussing the handling of outliers in activation quantization, suggesting that it has been a significant reference point for the authors. Confidence score: 0.8\n\n3. \"zeroquant efficient and affordable post training quantization for large scale transformers\" [b35]: This paper is also referred to in the discussion of existing quantization methods, implying that it has provided important context for the authors' work. Confidence score: 0.7\n\n4. \"attention is all you need advances in neural information processing systems\" [b30]: This paper is cited in the discussion of the baseline methods, suggesting that it has provided a foundational framework for the authors' work. Confidence score: 0.6\n\n5. \"language models are few shot learners\" [b4]: This paper is mentioned in the introduction as an example of a large language model, suggesting that it has been a key reference in the authors' understanding of the problem space. Confidence score: 0.5\n\n6. \"quantization and training of neural networks for efficient integer arithmetic only inference\" [b13]: This paper is cited multiple times throughout the text, suggesting that it has been influential in shaping the authors' understanding of the quantization process. Confidence score: 0.8\n\nThe JSON result would be:\n\n{\"b7\": 0.9, \"b33\": 0.8, \"b35\": 0.7, \"b30\": 0.6, \"b4\": 0.5, \"b13\": 0.8}", "{\"b7\": 0.9, \"b35\": 0.8, \"b33\": 0.7, \"b37\": 0.6, \"b13\": 0.5, \"b38\": 0.4, \"b27\": 0.3, \"b11\": 0.2, \"b4\": 0.1}", "Based on the text of the paper, the most pertinent source papers that inspired this work are:\n\n1. \"bit matrix multiplication for transformers at scale\" [b7] - This paper is frequently mentioned throughout the text, and it's clear that its methodology has strongly influenced the development of SmoothQuan's algorithm. The author uses it as a comparison and also builds upon it to address the challenges they identified. Confidence score: 0.9\n\n2. \"zeroquant efficient and affordable post training quantization for large scale transformers\" [b35] - This paper is mentioned as a significant reference in the development of the algorithm, and it is clear that the author uses it as a comparison and also builds upon it to address challenges they identified. Confidence score: 0.8\n\n3. \"outlier suppression pushing the limit of low bit transformer language models\" [b33] - This paper is mentioned as a significant reference in the development of the algorithm, and it is clear that the author uses it as a comparison and also builds upon it to address challenges they identified. Confidence score: 0.75\n\n4. \"language models are few shot learners\" [b4] - This paper is mentioned as a significant reference in the introduction and its method has influenced the development of the algorithm. Confidence score: 0.7\n\n5. \"the pile an gb dataset of diverse text for language modeling\" [b10] - This paper is used as a significant reference for the dataset used in the experiments. Confidence score: 0.65\n\n6. \"model opt b opt b\" [b38] - This paper is mentioned as a significant reference in the development of the algorithm and is a model that the author used for testing the proposed method. Confidence score: 0.7\n\nThe returned result in the JSON format is:\n\n{\n    \"b7\": 0.9,\n    \"b35\": 0.8,\n    \"b33\": 0.75,\n    \"b4\": 0.7,\n    \"b10\": 0.65,\n    \"b38\": 0.7\n}", "Based on the text of the paper, the primary challenges outlined in the paper involve optimizing inference for large language models and finding a way to effectively quantize the weights and activations of these models. The algorithm proposed by the authors, SmoothQuant, addresses these challenges. Key phrases such as \"inspired by\", \"motivated by\", \"following.. we adopt ... to solve the challenge/problem\" do not seem to appear in the text. However, several references are tied to the main challenges and methods presented in the paper. Based on this and the titles of the references, the most pertinent source papers appear to be:\n\n1. \"Language models are few shot learners\" [b4]: Mentioned multiple times in the paper, this paper may have provided inspiration for the development of large language models that the authors are trying to optimize.\n2. \"Quantization and training of neural networks for efficient integer arithmetic only inference\" [b13]: This paper is likely a source of inspiration for the quantization process proposed in SmoothQuant.\n3. \"Bit matrix multiplication for transformers at scale\" [b7]: Repeatedly referenced in the paper, this paper seems to be integral to the discussion on quantization and the challenges of activation quantization in large language models.\n4. \"ZeroQuant: Efficient and affordable post-training quantization for large-scale transformers\" [b35]: Given the clear parallel in the goals of ZeroQuant and SmoothQuant, this paper is likely a major source of inspiration.\n5. \"Outlier suppression: Pushing the limit of low-bit transformer language models\" [b33]: This paper seems to be directly related to the primary challenge of handling outliers in activation quantization.\n6. \"GLM-130B: An open bilingual pre-trained model\" [b37]: This paper is referenced in context of a model that was tested with SmoothQuant, suggesting that it directly influenced the research.\n\nThis selection is based on the provided text, and could change with a more complete view of the paper and its references. \n\nIn json format, the result would be:\n\n{\"b4\": 0.9, \"b13\": 0.8, \"b7\": 0.9, \"b35\": 0.9, \"b33\": 0.8, \"b37\": 0.7 }", "Based on the text of the paper and the titles of the references, the source papers that inspire the authors most appear to be:\n\n1. \"Language models are few shot learners\" [b4] - it is mentioned multiple times in the text, implying a strong influence on the work. The paper discusses the performance of large language models on various tasks, which is a key aspect of the current paper.\n\n2. \"Quantization and training of neural networks for efficient integer arithmetic only inference\" [b13] - The authors' work is based on quantization, and this paper could provide foundational knowledge or methods for their research.\n\n3. \"Bit matrix multiplication for transformers at scale\" [b7] - This paper is mentioned multiple times and discusses the issue of quantizing large language model activations, a challenge the current paper aims to address.\n\n4. \"ZeroQuant: Efficient and affordable post-training quantization for large scale transformers\" [b35] - This paper is mentioned as a comparative work, indicating that it has influenced the authors' research.\n\n5. \"Outlier Suppression: Pushing the limit of low bit transformer language models\" [b33] - The authors mention this paper when discussing the difficulties of quantization and their approach to addressing these difficulties.\n\n6. \"SmoothQuant: Accurate and efficient post-training quantization for large language models\" [None] - The paper itself seems to be a significant source, as it contains the main algorithm proposed by the authors.\n\nThe confidence score for each source paper:\n\n{\u201cb4\u201d: 0.9, \u201cb13\u201d: 0.8, \u201cb7\u201d: 0.9, \u201cb35\u201d: 0.8, \u201cb33\u201d: 0.7, \u201cNone\u201d: 1}\n\nThese confidence scores are estimated based on the frequency and context of the references in the text. Please note that these are estimations and may vary depending on further context not provided in the text."], "6350bc6d90e50fcafdecf145": ["Based on the text and abstract of the academic paper, the primary challenges outlined are the heavy reliance on high-quality supervision labels and the limitation of existing self-supervised learning (SSL) methods on homogeneous graphs due to their dataset-specific example generation strategies. The novel method proposed by the author to tackle these challenges is a new SSL approach, SHGP, which requires neither positive examples nor negative examples. This approach includes two modules, Att-HGNN and Att-LPA, which can mutually enhance each other.\n\nThe following references are identified as potential source papers based on the given criteria:\n\n1. \"Heterogeneous Graph Attention Network\" [b32]: The attention-aggregation scheme, which is a core feature of the proposed SHGP method, is based on this paper.\n\n2. \"Near Linear Time Algorithm to Detect Community Structures in Large Scale Networks\" [b21]: This reference is directly linked to the Att-LPA module of the proposed method, which uses the label propagation algorithm (LPA) for structural clustering on HINs.\n\n3. \"Label GCN: An Effective Method for Adding Label Propagation to Graph Convolutional Networks\" [b1]: This paper appears to have influenced the author's approach to integrating label propagation into graph neural networks.\n\n4. \"Graph Contrastive Learning with Augmentations\" [b41]: The author refers to this paper when discussing the limitations of existing SSL methods, specifically their example generation strategies.\n\n5. \"Information-Aware Graph Contrastive Learning\" [b36]: This paper is also mentioned in relation to the limitations of current SSL methods and seems to have influenced the author's proposed solution.\n\n6. \"Heterogeneous Graph Transformer\" [b11]: This paper could have inspired the author's use of HGNN models, which form the basis of the Att-HGNN module in the proposed method.\n\n7. \"Unifying Graph Convolutional Neural Networks and Label Propagation\" [b31]: This paper appears to have influenced the author's method of combining graph neural networks with label propagation.\n\n8. \"Interpretable and Efficient Heterogeneous Graph Convolutional Network\" [b38]: This paper might be a source of inspiration for the author's focus on interpretability and efficiency in the proposed method.\n\nHere is the result in the json format:\n{\n\"b32\": 0.9,\n\"b21\": 0.85,\n\"b1\": 0.8,\n\"b41\": 0.8,\n\"b36\": 0.8,\n\"b11\": 0.75,\n\"b31\": 0.75,\n\"b38\": 0.7\n}\nThe confidence score reflects the degree of influence each source paper likely had on the author's work, with a higher score indicating a stronger influence.", "Based on the text of the academic paper and the titles of the references, the source papers that inspired this work most likely include:\n\n1. \"Heterogeneous Graph Attention Network\" [b32]: The paper proposes a method that includes two modules with the same attention mechanism, a concept that is also central to the Heterogeneous Graph Attention Network (HAN) method. It is reasonable to assume that the attention mechanism in this work was inspired by HAN.\n   \n2. \"Deep Clustering for Unsupervised Learning of Visual Features\" [b2]: This paper discusses the use of clustering in learning, which is a key component of the presented method. \n\n3. \"Label GCN: An Effective Method for Adding Label Propagation to Graph Convolutional Networks\" [b1]: The presented method combines label propagation with graph neural networks, which indicates a possible inspiration from this paper.\n\n4. \"Information Aware Graph Contrastive Learning\" [b36]: The author's method doesn't require positive and negative examples for learning embeddings, which is similar to the approach used in this paper.\n   \n5. \"Heterogeneous Graph Transformer\" [b11]: The paper proposes a method for learning on heterogeneous graphs, which is the main focus of the analyzed paper.\n\n6. \"Graph Contrastive Learning with Augmentations\" [b41]: The analyzed paper discusses the limitations of certain example generation strategies, a topic that is also central to this work. \n\nTherefore, the source papers are:\n\n{\n\"b32\": 0.9,\n\"b2\": 0.8,\n\"b1\": 0.8,\n\"b36\": 0.7,\n\"b11\": 0.7,\n\"b41\": 0.7\n}", "Based on the text of the academic paper, the source papers that inspired this research are likely:\n\n1. \"Label GCN: An Effective Method for Adding Label Propagation to Graph Convolutional Networks\" (b1): The authors propose a method similar to Label GCN in their research, using a module for label propagation.\n\n2. \"Deep Clustering for Unsupervised Learning of Visual Features\" (b2): The authors propose a clustering-based self-supervised learning method, similar to the deep clustering method in this paper.\n\n3. \"Understanding the Difficulty of Training Deep Feedforward Neural Networks\" (b6): The authors use the Xavier uniform distribution for parameter initialization, which is a method proposed in this paper.\n\n4. \"Near Linear Time Algorithm to Detect Community Structures in Large Scale Networks\" (b21): The authors use the label propagation algorithm (LPA) proposed in this paper for structural clustering.\n\n5. \"Heterogeneous Graph Transformer\" (b11): The authors mention HGT as an example of an HGNN model that uses attention-aggregation, which is a key part of their proposed method.\n\n6. \"Graph Transformer Networks\" (b42): Similar to the previous reference, this paper also provides a model of attention-aggregation used in the authors' research.\n\n7. \"Asap: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations\" (b22): While this isn't mentioned directly, it is likely an inspiration due to its focus on hierarchical graph representations, which is an aspect of the authors' model.\n\n{\"b1\": 0.9, \"b2\": 0.8, \"b6\": 0.7, \"b21\": 0.9, \"b11\": 0.8, \"b42\": 0.8, \"b22\": 0.7}", "Based on the given text, the paper's novel contributions include proposing a new self-supervised learning (SSL) method on heterogeneous information networks (HINs), namely SHGP, which does not require positive or negative examples. The method comprises two modules, Att-LPA and Att-HGNN. \n\nThe source papers that inspired or heavily influenced these contributions can be inferred as:\n\n1. 'Label GCN: An effective method for adding label propagation to graph convolutional networks' (b1): This paper might have inspired the use of label propagation in the Att-LPA module.\n2. 'Heterogeneous Graph Transformer' (b11): The authors might have used this paper to design the Att-HGNN module, which can be instantiated as any attention-based HGNN encoders, including HGT.\n3. 'Near linear time algorithm to detect community structures in large scale networks' (b21): The initial pseudo-labels in the method were obtained by using the label propagation algorithm (LPA) proposed in this paper.\n4. 'Interpretable and efficient heterogeneous graph convolutional network' (b38): This paper might have inspired the author to use an attention-aggregation scheme in their model.\n5. 'Contrastive multi-view representation learning on graphs' (b7) and 'Graph contrastive learning with augmentations' (b41): These papers might have influenced the author's understanding of contrastive learning for graphs and its application in SSL.\n6. 'Understanding the difficulty of training deep feedforward neural networks' (b6): The authors might have used the Xavier uniform distribution for initializing the model parameters, which is mentioned in this paper.\n\nIn the JSON format, the answer is: \n{\n\"b1\": 1,\n\"b11\": 1,\n\"b21\": 1,\n\"b38\": 1,\n\"b7\": 0.8,\n\"b41\": 0.8,\n\"b6\": 0.8\n}", "Based on the text and the titles of the references, the source papers that have inspired the current paper most are:\n\n1. b33: 'Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning'\n2. b10: 'GPT-GNN: Generative Pre-training of Graph Neural Networks'\n3. b31: 'Unifying Graph Convolutional Neural Networks and Label Propagation'\n4. b41: 'Graph Contrastive Learning with Augmentations'\n5. b21: 'Near Linear Time Algorithm to Detect Community Structures in Large Scale Networks'\n6. b40: 'Graph Contrastive Learning Automated'\n7. b32: 'Heterogeneous Graph Attention Network'\n\nThese papers are identified based on the text, which indicates that they have direct relevance to the paper's novel contributions. The confidence scores are calculated based on how directly each paper is mentioned or alluded to in the text, with the highest score given to the paper that appears to be the most directly relevant based on the author's words.\n\nHere's the result in the requested JSON format:\n\n{\n    \"b33\": 0.9,\n    \"b10\": 0.8,\n    \"b31\": 0.7,\n    \"b41\": 0.8,\n    \"b21\": 0.7,\n    \"b40\": 0.8,\n    \"b32\": 0.9\n}"], "6346f67490e50fcafd950453": ["Based on the text and the references, the most relevant source papers are likely to be:\n\n1. \"Reconsidering OS memory optimizations in the presence of disaggregated memory\" - This paper is related to the author's work because it also focuses on the challenges of optimizing memory in disaggregated memory systems. It is possible that the author used this paper as a starting point for their research.\n\n2. \"Thermostat: application-transparent page management for two-tiered main memory\" - This paper is directly relevant since the author's paper is also about managing memory in a two-tiered system. The author's work seems to build on this by introducing a new system, HotBox.\n\n3. \"Nimble: Page management for tiered memory systems\" - This paper is likely a key reference because the author's work is a direct response to it. The author critiques the batching method used in Nimble and proposes a different approach.\n\n4. \"Efficient memory disaggregation with InfiniSwap\" - This paper is likely relevant because it discusses memory disaggregation, a key topic in the author's paper. The author may have used this paper to understand the current methods and their limitations.\n\n5. \"HotBox: A novel memory management subsystem for disaggregated memory\" - This paper is likely the author's main contribution, outlining the HotBox system they propose.\n\n6. \"HeteroOS: OS design for heterogeneous memory management in Datacenter\" - This paper might have provided the author with insights on how to manage heterogeneous memory in data centers, which is part of the author's focus.\n\n7. \"Disaggregated memory for expansion and sharing in blade servers\" - This paper is likely relevant because it discusses memory disaggregation in blade servers, a topic closely related to the author's work.\n\nIn JSON format, the result would be:\n\n{\n    \"b1\": 0.9, \n    \"b6\": 0.9, \n    \"b47\": 0.9, \n    \"b23\": 0.8,\n    \"b0\": 1.0,\n    \"b27\": 0.7, \n    \"b32\": 0.7\n}", "Based on the text of the paper and the author's notes, the source papers that inspired the given paper seem to be the following:\n\n- \"Disaggregated memory for expansion and sharing in blade servers\" [b32]: This paper is relevant as it discusses the architecture of disaggregated memory systems, which is the main topic of the present paper.\n- \"Efficient memory disaggregation with infiniswap\" [b23]: This paper is mentioned when discussing methods to transparently access disaggregated memory. \n- \"Can far memory improve job throughput\" [b9]: This paper is referenced when discussing the swap-only approach, which is a key challenge the authors address in the present paper.\n- \"Thermostat: application-transparent page management for two-tiered main memory\" [b6]: This paper is crucial as it discusses the hybrid access mechanism adopted in the present paper.\n- \"Reconsidering OS memory optimizations in the presence of disaggregated memory\" [None]: The title of this paper suggests that it could have significantly influenced the research in the given paper, which focuses on optimizing the performance of disaggregated memory systems.\n- \"Nimble: page management for tiered memory systems\" [b47]: This paper is constantly referred to in the text as the state-of-the-art, which the authors aim to improve upon.\n\nThe confidence scores for these papers might be:\n\n{ \"b32\": 0.9, \"b23\": 0.8, \"b9\": 0.8, \"b6\": 0.9, \"None\": 0.8, \"b47\": 1 }", "Based on the academic paper text and author notes, the paper primarily addresses the challenges of managing disaggregated memory systems, especially those with high latencies. The authors propose a system called HotBox, which is implemented in the Linux kernel and aims to manage distant memory smartly based on page \"hotness\" or frequency of access.\n\nThe following references seem to be the most significant source papers, as they are directly related to the challenges and solutions discussed in this paper:\n\n1. \"A swap-only approach under higher memory latencies is inefficient\" [b9] [b21] [b23] [b41]\n2. \"A hybrid approach has gained increased interest\" [b6] [b14] [b27] [b47]\n3. \"HotBox takes several steps to reduce the costs of disaggregated memory management\" [b39]\n4. \"We implement HotBox in Linux and evaluate it using Intel's Persistent Hybrid Memory Emulation Platform (PMEP)\" [b18]\n5. \"We compare HotBox with state-of-the-art mechanisms, including swapping-based systems such as InfiniSwap\" [b9] [b21] [b23]\n6. \"as well as the recent Nimble system for tiered memory\" [b47]\n\nHere is the list of source papers in json format with confidence score:\n\n{\n \"b9\": 0.75,\n \"b21\": 0.75,\n \"b23\": 0.75,\n \"b41\": 0.75,\n \"b6\": 0.85,\n \"b14\": 0.85,\n \"b27\": 0.85,\n \"b47\": 0.95,\n \"b39\": 0.8,\n \"b18\": 0.9\n}\n\nThe confidence score is assigned based on the level of relevance of the source paper to the main contributions of the given paper. Papers \"b47\" and \"b18\" have the highest confidence score due to their direct relevance to the HotBox system proposed in this paper.", "Based on the content of the paper and the provided reference titles, the source papers that inspired the authors' work the most likely include:\n\n1. 'Reconsidering os memory optimizations in the presence of disaggregated memory'\n2. 'Thermostat applicationtransparent page management for two tiered main memory'\n3. 'Efficient memory disaggregation with infiniswap'\n4. 'Radiant efficient page table management for tiered memory systems'\n5. 'Remote memory in the age of fast networks'\n6. 'Nimble page management for tiered memory systems'\n7. 'Heteroos os design for heterogeneous memory management in datacenter'\n8. 'Disaggregated memory for expansion and sharing in blade servers'\n\nThose papers seem to be the most directly related to the main contributions of this paper, which involved optimizing the performance of disaggregated memory systems and addressing the challenges of managing byte-addressable tiered memory under high latency conditions.\n\nThe JSON format of the answer would be:\n\n{\n\"b0\": 0.8,\n\"b6\": 0.83,\n\"b23\": 0.85,\n\"b28\": 0.8,\n\"b7\": 0.8,\n\"b47\": 0.85,\n\"b27\": 0.82,\n\"b32\": 0.8\n} \n\nThis represents the confidence score for each reference, with higher scores indicating a stronger influence on the authors' work.", "Based on the text of the academic paper and the analysis steps provided, the following source papers seem most pertinent:\n\n1. \"Disaggregated memory for expansion and sharing in blade servers\" [b32]: This paper appears to be a primary source that inspired the author's research on how to optimize the performance of disaggregated memory systems with a hybrid access mechanism.\n   \n2. \"Nimble: page management for tiered memory systems\" [b47]: The paper mentions Nimble as a state-of-the-art work that utilizes the hybrid model for memory management in disaggregated memory systems. However, the author of the paper finds it ill-suited for disaggregated memory systems and offers an alternative approach.\n\n3. \"Thermostat: application-transparent page management for two-tiered main memory\" [b6]: The paper refers to Thermostat as a prior work that has used a hybrid access mechanism. However, the author of the paper has proposed an alternative approach, HotBox, that seeks to address some of the challenges associated with Thermostat's use in disaggregated memory systems.\n\n4. \"Rethinking software runtimes for disaggregated memory\" [b14]: This paper is mentioned in the context of discussing the hybrid model for memory management in disaggregated memory systems. The author of the paper has taken inspiration from this work but has proposed a different approach.\n\n5. \"Efficient memory disaggregation with InfiniSwap\" [b23]: The paper refers to InfiniSwap as a prior work that has used a swap-only approach for memory management in disaggregated memory systems. However, the author of the paper finds this approach inefficient and has proposed an alternative approach.\n\n6. \"Scale-out NUMA\" [b36]: The paper mentions this work in the context of discussing cache-line access in memory management in disaggregated memory systems. The author of the paper has taken inspiration from this work but has proposed a different approach.\n\n7. \"A study of replacement algorithms for a virtual-storage computer\" [b11]: This paper is mentioned in the context of discussing the swap and cache line access models, and the author of the paper uses it to confirm that the hybrid model is indeed essential for achieving high performance under disaggregated memory latencies.\n\n8. \"Radiant: efficient page table management for tiered memory systems\" [b28]: This paper is mentioned in the context of discussing the use of huge pages in memory management. The author of the paper has taken inspiration from this work but has proposed a different approach.\n\nBased on the analysis, the following JSON object can be returned:\n\n{\n    \"b32\": 0.9,\n    \"b47\": 0.85,\n    \"b6\": 0.8,\n    \"b14\": 0.75,\n    \"b23\": 0.7,\n    \"b36\": 0.65,\n    \"b11\": 0.6,\n    \"b28\": 0.55\n}\n\nThe confidence score indicates the level of relevance of each source paper to the main contributions of the given paper. The score is determined based on the frequency of mention, the context of mention, the directness of inspiration or motivation, and the methodological similarity."], "6344dedd90e50fcafd24cdcd": ["{\n    \"b11\": 0.95,\n    \"b13\": 0.95\n}", "```json\n{\n  \"b11\": 0.9,\n  \"b13\": 0.9,\n  \"b27\": 0.8,\n  \"b41\": 0.8,\n  \"b42\": 0.8\n}\n```", "{\n    \"b11\": 0.9,\n    \"b13\": 0.9,\n    \"b27\": 0.8,\n    \"b42\": 0.8\n}", "{\n    \"b11\": 0.9,\n    \"b13\": 0.9,\n    \"b27\": 0.85,\n    \"b42\": 0.8\n}", "{\n  \"b11\": 0.9,\n  \"b13\": 0.9,\n  \"b27\": 0.8,\n  \"b42\": 0.8\n}"], "633e476490e50fcafde590a1": ["{\n  \"b9\": 0.9,\n  \"b31\": 0.85,\n  \"b33\": 0.8\n}", "{\n  \"b6\": 0.95,\n  \"b9\": 0.90,\n  \"b31\": 0.85,\n  \"b33\": 0.85\n}", "{\n    \"b8\": 0.9,\n    \"b9\": 0.9,\n    \"b31\": 0.8,\n    \"b33\": 0.8,\n    \"b6\": 0.7,\n    \"b7\": 0.7\n}", "{\n    \"b31\": 0.9,\n    \"b33\": 0.9,\n    \"b6\": 0.85,\n    \"b7\": 0.85,\n    \"b8\": 0.8,\n    \"b9\": 0.8\n}", "{\n    \"b31\": 0.9,\n    \"b33\": 0.9,\n    \"b8\": 0.85,\n    \"b9\": 0.8,\n    \"b6\": 0.75,\n    \"b7\": 0.75,\n    \"b36\": 0.7,\n    \"b37\": 0.7\n}"], "632812a590e50fcafd1a6a98": ["{\n  \"b16\": 0.9,\n  \"b18\": 0.9,\n  \"b20\": 0.9,\n  \"b21\": 0.8\n}", "{\n  \"b16\": 0.9,\n  \"b18\": 0.95,\n  \"b20\": 0.8,\n  \"b21\": 0.85\n}", "{\n  \"b16\": 0.9,\n  \"b18\": 0.9,\n  \"b20\": 0.8,\n  \"b21\": 0.8\n}", "{\n    \"b18\": 0.9,\n    \"b16\": 0.8,\n    \"b20\": 0.8,\n    \"b21\": 0.75\n}", "{\n    \"b16\": 0.9,\n    \"b18\": 0.9,\n    \"b20\": 0.8,\n    \"b21\": 0.75\n}"], "634e194790e50fcafd24f33e": ["{\n    \"b43\": 0.95,\n    \"b35\": 0.9,\n    \"b79\": 0.9,\n    \"b39\": 0.85,\n    \"b65\": 0.85\n}", "{\n  \"b43\": 0.9,\n  \"b35\": 0.8,\n  \"b79\": 0.9,\n  \"b39\": 0.8,\n  \"b65\": 0.8\n}", "{\n  \"b43\": 0.9,\n  \"b35\": 0.8,\n  \"b79\": 0.9,\n  \"b39\": 0.8,\n  \"b65\": 0.8\n}", "{\n  \"b35\": 0.9,\n  \"b43\": 0.9,\n  \"b79\": 0.9,\n  \"b39\": 0.8,\n  \"b65\": 0.8,\n  \"b14\": 0.7\n}", "```json\n{\n  \"b35\": 0.8,\n  \"b43\": 0.9,\n  \"b79\": 0.85,\n  \"b39\": 0.75\n}\n```"], "62982a9a5aee126c0f6f5ecb": ["Based on the given text, the source papers that inspired the given academic paper are:\n\n1. \"Simple GNN Regularisation for 3D Molecular Property Prediction and Beyond\" [b22]: The paper's core methodology of a self-supervised pre-training objective is inspired by recent advances in noise regularization for graph neural networks (GNNs), as indicated by the phrase \"Inspired by recent advances in noise regularization for graph neural networks (GNNs) [b22]\". The paper also takes inspiration from [b22] for incorporating denoising as an auxiliary task for improving performance.\n\n2. \"A Connection Between Score Matching and Denoising Autoencoders\" [b69], \"Generative Modeling by Estimating Gradients of the Data Distribution\" [b60], \"Improved Techniques for Training Score-Based Generative Models\" [b61]: The paper discusses the well-known connection between denoising and score-matching, indicating that these papers have inspired the algorithmic approach used in the paper.\n\n3. \"Graph Net Simulator (GNS)\" [b49]: The paper employs the Graph Net Simulator (GNS) architecture in its methodology.\n\n4. \"Tailored Activation Transforms (TAT)\" [b79]: The paper incorporates Tailored Activation Transformation (TAT) to Graph Network Simulators (GNS) to boost performance, indicating that this paper has also inspired the methods used in the given paper.\n\n5. \"Strategies for Pre-training Graph Neural Networks\" [b26]: The paper refers to strategies for pre-training using graph data, which could indicate that it has influenced the paper's approach to pre-training.\n\n6. \"GraphMVP\" [b36]: While the paper contrasts its approach with GraphMVP, it seems to draw inspiration from it in treating 3D structure as one view of a 2D molecule for the purpose of upstream contrastive learning.\n\n7. \"Noisy Nodes\" [b22]: The paper's methodology builds on Noisy Nodes, which incorporates denoising as an auxiliary task to improve performance, indicating that it has played a crucial role in shaping the paper's contributions.\n\nPlease find the confidence score of each reference:\n\n- \"Simple GNN Regularisation for 3D Molecular Property Prediction and Beyond\" [b22]: 0.9\n- \"A Connection Between Score Matching and Denoising Autoencoders\" [b69]: 0.8\n- \"Generative Modeling by Estimating Gradients of the Data Distribution\" [b60]: 0.8\n- \"Improved Techniques for Training Score-Based Generative Models\" [b61]: 0.8\n- \"Graph Net Simulator (GNS)\" [b49]: 0.8\n- \"Tailored Activation Transforms (TAT)\" [b79]: 0.8\n- \"Strategies for Pre-training Graph Neural Networks\" [b26]: 0.7\n- \"GraphMVP\" [b36]: 0.7\n- \"Noisy Nodes\" [b22]: 0.9\n\nJSON response: \n\n{\n  \"b22\": 0.9,\n  \"b69\": 0.8,\n  \"b60\": 0.8,\n  \"b61\": 0.8,\n  \"b49\": 0.8,\n  \"b79\": 0.8,\n  \"b26\": 0.7,\n  \"b36\": 0.7\n}", "Based on the provided text, we can find several key phrases that indicate the reliance on previous research for developing the paper's core contributions. \n\nThe references are:\n1. \"Inspired by recent advances in noise regularization for graph neural networks (GNNs) [b22]\"\n2. \"Relying on the well-known connection between denoising and score-matching [b69] [b60] [b24]\"\n3. \"Our work builds on Noisy Nodes [b22]\"\n4. \"We make improvements to a common GNN architecture, in particular showing how to apply Tailored Activation Transformation (TAT) [b79] to Graph Network Simulators (GNS) [b49]\"\n5. \"The closest work to ours is GraphMVP [b36]\"\n\nBased on these clues, it is evident that the papers [b22], [b69], [b60], [b24], [b79], [b49] and [b36] have heavily influenced the main contributions of the paper.\n\nHence, the JSON output would be:\n\n{\n\"b22\": 1,\n\"b69\": 0.8,\n\"b60\": 0.8,\n\"b24\": 0.8,\n\"b79\": 0.9,\n\"b49\": 0.9,\n\"b36\": 1\n} \n\nHere, the confidence score is set based on the frequency of the reference in the text and the strength of the language indicating inspiration or motivation. For instance, \"inspired by\" and \"builds on\" suggest a high level of influence, thus a score of 1, while \"relying on the connection\" suggests a lower, yet significant level of influence, thus a score of 0.8.", "Based on the text of the paper, the following references seem to be the most pertinent source papers:\n\n1. \"b22: Simple gnn regularisation for d molecular property prediction and beyond\" - This paper is directly mentioned as inspiring the primary method of the paper, which is pre-training via denoising in the space of 3D structures. \n\n2. \"b69: A connection between score matching and denoising autoencoders\" and \"b60: Generative modeling by estimating gradients of the data distribution\" - These two papers were used to establish the connection between denoising and score-matching, which is a crucial part of the paper's method.\n\n3. \"b49: Learning to simulate complex physics with graph networks\" - This paper was used as source for a common GNN architecture that the paper improved on.\n\n4. \"b79: Deep learning without shortcuts shaping the kernel with tailored rectifiers\" - This paper was used to introduce a recently published network transformation method called Tailored Activation Transforms (TAT) which was applied on the GNN architecture mentioned above.\n\n5. \"b41: Pubchemqc project a large scale first principles electronic structure database for data driven chemistry\" - This paper is the source of the main dataset used for pre-training in the paper.\n\nTherefore, the result is:\n```json\n{\n    \"b22\": 1,\n    \"b69\": 0.9,\n    \"b60\": 0.9,\n    \"b49\": 0.8,\n    \"b79\": 0.8,\n    \"b41\": 0.75\n}\n```", "Based on the analysis of the text, the author primarily takes inspiration from the following source papers:\n1. \"Simple GNN regularisation for d molecular property prediction and beyond\" [b22] - Confidence Score: 0.8: This paper is directly mentioned as being an inspiration for the denoising approach used in the pre-training of the model, as well as for their method of representation learning.\n2. \"Learning to simulate complex physics with graph networks\" [b49] - Confidence Score: 0.7: The paper's method is heavily reliant on Graph Network Simulators (GNS), which is a concept proposed in this source paper. \n3. \"A connection between score matching and denoising autoencoders\" [b69] - Confidence Score: 0.6: This paper is cited as the source of the link between denoising autoencoders and score matching, which is a pivotal concept in the paper's approach.\n4. \"Generative modeling by estimating gradients of the data distribution\" [b60] - Confidence Score: 0.6: This paper is also cited for the connection between denoising and score matching, which is used to explain the paper's method.\n5. \"Pre training molecular graph representation with d geometry\" [b36] - Confidence Score: 0.5: This paper is cited as the closest work to the authors' work, hence it can be inferred that this work has a significant impact on the paper's contributions.\n6. \"Noisy Nodes: Denoising as an Auxiliary Loss\" [b22] - Confidence Score: 0.4: Although this paper is not explicitly mentioned as a source of inspiration, it is discussed at length in the paper and the authors build upon its concepts in their methods. \n\nThe JSON format of the result:\n{\"b22\": 0.8, \"b49\": 0.7, \"b69\": 0.6, \"b60\": 0.6, \"b36\": 0.5, \"b22 (Noisy Nodes)\": 0.4}", "Based on the text of the paper and the titles of the references, the source papers that most directly influenced the main contributions of this paper are:\n\n1. \"Simple GNN regularisation for d molecular property prediction and beyond\" [b22]: This paper is directly mentioned and the authors build upon it, applying denoising as a pre-training objective for training on another large dataset of molecular structures to learn higher-quality representations. Confidence Score: 1\n\n2. \"Learning to simulate complex physics with graph networks\" [b49]: This paper is mentioned multiple times and the authors improve upon it, showing how to apply Tailored Activation Transformation (TAT) to Graph Network Simulators (GNS). Confidence Score: 0.9\n\n3. \"A connection between score matching and denoising autoencoders\" [b69]: This paper is mentioned as a key theory that forms the basis of the authors' denoising objective. Confidence Score: 0.8\n\n4. \"Generative modeling by estimating gradients of the data distribution\" [b60]: This paper is mentioned in the context of how the denoising objective is equivalent to learning a particular force field. Confidence Score: 0.7\n\n5. \"TorchMD net equivariant transformers for neural network based molecular potentials\" [b65]: This paper is mentioned in the context of the authors' pre-training technique being architecture-agnostic, and they show that it can be applied to enhance performance in both a GNN-based architecture and a Transformer-based one. Confidence Score: 0.7\n\n6. \"Pre training molecular graph representation with d geometry\" [b36]: This paper is the closest work to the authors' paper, where 3D structure is treated as one view of a 2D molecule for the purpose of upstream contrastive learning. The authors' work, however, aims to improve downstream models for molecular property prediction from 3D structures. Confidence Score: 0.7\n\n7. \"Strategies for pre training graph neural networks arxiv learning\" [b26]: This paper is mentioned in the context of the current literature not being designed to handle 3D structural information, focusing instead on 2D graphs. Confidence Score: 0.6\n\nJSON format of the result is:\n{\"b22\": 1, \"b49\": 0.9, \"b69\": 0.8, \"b60\": 0.7, \"b65\": 0.7, \"b36\": 0.7, \"b26\": 0.6}"], "63520de890e50fcafd60f43e": ["{\n    \"b10\": 0.95,\n    \"b44\": 0.90,\n    \"b7\": 0.85,\n    \"b19\": 0.85\n}", "{\n    \"b10\": 0.9,\n    \"b44\": 0.9,\n    \"b7\": 0.8,\n    \"b19\": 0.8,\n    \"b41\": 0.7\n}", "{\n  \"b10\": 0.9,\n  \"b44\": 0.95\n}", "{\n  \"b10\": 0.9,\n  \"b44\": 0.95,\n  \"b7\": 0.8,\n  \"b19\": 0.7\n}", "{\n    \"b10\": 0.9,\n    \"b44\": 0.95,\n    \"b41\": 0.8,\n    \"b19\": 0.7,\n    \"b7\": 0.7\n}"], "6327dda690e50fcafd67dfe7": ["{\n  \"b29\": 1.0,\n  \"b38\": 0.9,\n  \"b34\": 0.8,\n  \"b21\": 0.75,\n  \"b39\": 0.7,\n  \"b47\": 0.7\n}", "{\n    \"b29\": 0.9,\n    \"b39\": 0.8,\n    \"b34\": 0.8,\n    \"b47\": 0.7,\n    \"b21\": 0.6,\n    \"b45\": 0.6\n}", "{\n  \"b29\": 1.0,\n  \"b38\": 1.0,\n  \"b39\": 1.0,\n  \"b34\": 0.9,\n  \"b47\": 0.8\n}", "{\n  \"b29\": 0.85,\n  \"b39\": 0.9,\n  \"b34\": 0.8,\n  \"b47\": 0.75\n}", "{\n    \"b29\": 0.95,\n    \"b39\": 0.95,\n    \"b38\": 0.9,\n    \"b21\": 0.85,\n    \"b47\": 0.8\n}"], "63896cd690e50fcafde7a0f0": ["Based on the paper's text, it appears that the following papers are the most relevant sources:\n\n1. Zhu et al. [b40]: The authors refer to this paper while discussing the limitations of current CLS works and the need for more robust datasets. They also reference it in the context of previous works on end-to-end CLS methods.\n\n2. Bai et al. [b0]: This paper is cited in connection with the limitations of current CLS datasets and the lengths of source documents.\n\n3. Ladhak et al. [b8]: This paper is mentioned in the context of the construction of WikiLingua, which is compared with the authors' dataset in terms of average length of source documents.\n\n4. Perez-Beltrachini and Lapata [b19]: This paper is cited when discussing XWikis and its average length of source documents.\n\n5. Wang et al. [b31]: This paper is referred to while speaking about the dialogue-oriented CLS dataset XSAMSum. It is also cited in the context of end-to-end CLS methods.\n\n6. Cohan et al. [b4]: The authors cite this paper when discussing long-document summarization and the datasets arXiv and PubMed.\n\n7. OPUS-MT [b25]: This paper is mentioned when the authors discuss the machine translation methods used in their research.\n\nThe confidence scores for each of these references would require a more in-depth analysis of the paper's text, but based on their frequency and context of citation, they all seem to have influenced the authors' work to a significant extent.\n\nIn JSON format, the result might look like:\n\n{\n\"b40\": 0.9,\n\"b0\": 0.8,\n\"b8\": 0.7,\n\"b19\": 0.7,\n\"b31\": 0.75,\n\"b4\": 0.85,\n\"b25\": 0.75\n}", "Based on the text and the nature of the problem outlined in the paper, the following references are likely the main sources of inspiration for the author:\n\n- \"b40\": 'ncls neural cross lingual summarization' - This paper is mentioned multiple times throughout the text and it seems to be a central source of inspiration for the paper's approach to Cross-Lingual Summarization (CLS). \n- \"b4\": 'a discourse aware attention model for abstractive summarization of long documents' - This source is referred while discussing long-document summarization, which is a main theme of the paper. \n- \"b31\": 'clidsum a benchmark dataset for cross lingual dialogue summarization' - This paper is mentioned when discussing the methods for CLS, indicating it might have been a source of inspiration for the paper's methodology. \n- \"b2\": 'longformer the longdocument transformer' - The paper refers to this source while discussing the challenges of modeling the long-distance dependencies within long documents, indicating its possible influence on the authors' work. \n- \"b19\": 'models and datasets for cross lingual summarisation' - This source is referred when discussing the construction of XWikis dataset, suggesting its influence on the dataset construction of the paper. \n- \"b32\": 'a survey on cross lingual summarization' - This source is cited in the introduction and related work, indicating its influence on the author's understanding of the Cross-Lingual Summarization field.\n\nReturn in json format:\n{\n\"b40\": 0.9,\n\"b4\": 0.8,\n\"b31\": 0.7,\n\"b2\": 0.7,\n\"b19\": 0.6,\n\"b32\": 0.6\n}", "Based on the given text, the source papers that inspired this work can be identified by looking for references that are directly linked to the methodologies and challenges mentioned in the paper. The paper mainly focuses on the task of Cross-Lingual Summarization (CLS) for long documents, and it mentions the challenges and methods in this field. The paper also mentions that they built and evaluated various summarize-then-translate and end-to-end methods based on their constructed dataset, Perseus. \n\nThe references that seem to be most relevant to this work are:\n\n1. b32: 'a survey on cross lingual summarization'\n2. b31: 'clidsum a benchmark dataset for cross lingual dialogue summarization'\n3. b2: 'longformer the longdocument transformer'\n4. b39: 'sentence centrality revisited for unsupervised summarization'\n5. b40: 'ncls neural cross lingual summarization'\n6. b4: 'a discourse aware attention model for abstractive summarization of long documents'\n\nThe confidence scores for each reference can be estimated as follows:\n\n1. b32: 0.9 - The paper directly refers to this work as a survey on cross-lingual summarization, which is the main topic of the paper.\n2. b31: 0.8 - This work is referred to as a benchmark dataset for cross-lingual dialogue summarization, which is related to the dataset construction in this paper.\n3. b2: 0.8 - The longformer model mentioned in this work might be relevant to handle the long-document CLS task.\n4. b39: 0.7 - The paper mentions the sentence centrality method, which might be used in their summarize-then-translate methods.\n5. b40: 0.9 - This work is about neural cross-lingual summarization, which is directly related to the main task of this paper.\n6. b4: 0.7 - This work is about abstractive summarization for long documents, which is related to the challenges mentioned in the paper.\n\nThe result in json format:\n{\n    \"b32\": 0.9,\n    \"b31\": 0.8,\n    \"b2\": 0.8,\n    \"b39\": 0.7,\n    \"b40\": 0.9,\n    \"b4\": 0.7\n}", "Based on the text of the paper, the most pertinent source papers that inspired the authors' work are:\n\n{\"b0\": 0.8, \"b4\": 0.9, \"b8\": 0.8, \"b31\": 0.7, \"b40\": 0.9, \"b19\": 0.7, \"b11\": 0.6, \"b3\": 0.6, \"b35\": 0.6}\n\nThe authors made several direct references to these papers in the context of their own research, and they were particularly influenced by the work on long-document summarization (b4), the methodology of Neural Cross-Lingual Summarization (b40), the construction of large-scale CLS datasets (b0, b8), and the development of end-to-end CLS methods (b3, b11, b31, b35). The paper by Perez-Beltrachini and Lapata (b19) was also referenced as a source of inspiration for the authors' work. The confidence scores reflect the level of direct influence and relevance these papers have on the authors' work.", "Based on the given text, the following papers seem to have been influential in the development of the paper's content, methodology, and conclusions. The key phrases and context used indicate a strong reliance on these sources:\n\n1. \"Zhu et al. [b40]\" - The paper discusses the focus of CLS work on short texts with reference to the work of Zhu et al. which proposed two CLS datasets. This paper seems to be a significant source of inspiration as it establishes the context for the need of a long-document CLS dataset like Perseus developed by the authors.\n\n2. \"Bai et al. [b0]\" - This paper is also referenced when discussing the current CLS works that focus on short texts, indicating that it was influential in shaping the author's approach to CLS. \n\n3. \"Ladhak et al. [b8]\" - This work is a reference point for comparison when the authors discuss the average length of source documents in various datasets, suggesting it played a role in determining the need for a long-document CLS system.\n\n4. \"Perez-Beltrachini and Lapata [b19]\" - This study is referred to in the same context as Ladhak et al., suggesting it was also influential in the author's understanding of the current state of CLS datasets.\n\n5. \"Wang et al. [b31]\" - This paper is mentioned when discussing the need for a long-document CLS system, suggesting that it influenced the author's decision to develop Perseus.\n\n6. \"Cohan et al. [b4]\" - This paper is cited when discussing long-document summarization, indicating it had an impact on the author's understanding of the challenges involved in summarizing long documents.\n\n7. \"PacSum [b39]\" - This paper is mentioned when discussing the extractor used in the Ext-Trans method, indicating its influence on the author's methodological approach.\n\n8. \"SummaRu. [b14]\" - This paper is referenced in the same context as PacSum, suggesting it also played a significant role in shaping the author's methodological approach.\n\nThe resulting JSON format is:\n\n```\n{\n    \"b40\": 0.9,\n    \"b0\": 0.8,\n    \"b8\": 0.8,\n    \"b19\": 0.8,\n    \"b31\": 0.7,\n    \"b4\": 0.7,\n    \"b39\": 0.7,\n    \"b14\": 0.7\n}\n```"], "62a165475aee126c0f509e38": ["Based on the provided academic paper text and the titles of the references, the source papers that inspire the author most can be identified as follows:\n\n1. [b45] \"Deep graph infomax\" - The author specifically mentions this work when discussing self-supervised learning schemes for graph-structured data, suggesting it had a significant influence. It is also mentioned when discussing various graph contrastive learning methods.\n\n2. [b52] \"Graph contrastive learning with augmentations\" - This paper is also mentioned multiple times, suggesting it was a significant source of inspiration. It is discussed in the context of self-supervised schemes and graph contrastive learning methods.\n\n3. [b14] \"Contrastive multi view representation learning on graphs\" - This paper is mentioned when the author discusses self-supervised learning schemes and contrastive learning methods. This suggests that it was a key source of inspiration for the author's work.\n\n4. [b24] \"Semi-supervised classification with graph convolutional networks\" - This paper is referenced when the author discusses the challenges of self-supervised learning for graph-structured data. The author also refers to it when discussing graph neural networks.\n\n5. [b31] \"Geom GCN: Geometric graph convolutional networks\" - This work is cited when the author discusses efforts to deal with non-homophilous graphs. This suggests that it had a key influence on the author's research.\n\n6. [b56] \"Beyond homophily in graph neural networks: Current limitations and effective designs\" - This paper is mentioned when the author discusses non-homophilous graphs. It appears to have heavily influenced the author's research.\n\nThe confidence scores for these papers are subjective and can vary based on the specific context and interpretation.\n\nBased on the aforementioned analysis, the result can be represented in the following JSON format:\n\n{\n\"b45\": 0.8,\n\"b52\": 0.8,\n\"b14\": 0.7,\n\"b24\": 0.7,\n\"b31\": 0.7,\n\"b56\": 0.7\n}", "Based on the given academic paper's text, the following references are identified as the most relevant source papers:\n\n1. \"Deep Graph Infomax\" [b45]: The paper mentions that it takes motivation from the Deep Graph Infomax method for self-supervised learning.\n2. \"Graph Contrastive Learning with Augmentations\" [b52]: The paper cites this reference multiple times and discusses its method of capturing structure information without directly emphasizing proximity.\n3. \"Contrastive Multi-View Representation Learning on Graphs\" [b14]: This reference is highlighted as a recent work studying self-supervised learning for graph-structured data.\n4. \"Bootstrap Your Own Latent a New Approach to Self Supervised Learning\" [b9]: This method was used to prevent the model from collapsing into trivial solutions.\n5. \"Decoupled Self Supervised Learning for Non Homophilous Graphs\" [None]: The paper itself is a source, as the authors propose a new self-supervised framework and discuss its novel contributions.\n\nHere is the answer in json format:\n\n{\n\"b45\": 0.9,\n\"b52\": 0.8,\n\"b14\": 0.8,\n\"b9\": 0.7,\n\"None\": 1.0\n}", "Based on the text and references, the most relevant source papers are:\n\n1. \"Deep Graph Infomax\" [b45]: This is indicated by phrases such as \"Recent works... have studied graph self-supervised learning\" and \"importance of neighborhood strategies\" which is a key concept in this paper.\n2. \"Graph Contrastive Learning with Augmentations\" [b52]: This paper is referenced multiple times in context of contrastive learning methods and their limitations, indicating it had a significant influence on the current work.\n3. \"Decoupled Self Supervised Learning for Non Homophilous Graphs\" [b0]: This is the paper's own title and appears multiple times in the text, indicating its relevance.\n4. \"Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs\" [b56]: The paper is cited in context of the challenges of non-homophilous graphs and the limitations of current GNN models, indicating its influence on the current work.\n5. \"Contrastive Multi-view Representation Learning on Graphs\" [b14]: Referenced alongside other key papers [b45] and [b52], suggesting it is also a major influence.\n6. \"GraphSAGE\" [b12]: This paper is mentioned as one of the earlier combinations of GNNs and self-supervised learning.\n\nThe confidence scores for each paper would be based on the frequency and context of their citation in the text. For example, [b45] and [b52] are mentioned multiple times and in key contexts, indicating a high confidence score. In contrast, [b14] and [b12] are mentioned less frequently and in less pivotal contexts, suggesting a lower confidence score.\n\nThe result in json format would be:\n\n```json\n{\n    \"b45\": 0.9,\n    \"b52\": 0.9,\n    \"b0\": 1.0,\n    \"b56\": 0.8,\n    \"b14\": 0.7,\n    \"b12\": 0.7\n}\n```", "{\"b2\": 0.8, \"b4\": 0.9, \"b6\": 0.6, \"b14\": 0.8, \"b23\": 0.7, \"b24\": 0.6, \"b45\": 0.7, \"b52\": 0.9}", "Based on the text and the titles of the references, it seems that the most pertinent sources of inspiration for the paper might be:\n\n- \"Deep Graph Infomax\" [b45]: This paper seems to be the main inspiration for the proposed self-supervised learning scheme, as it is mentioned multiple times throughout the text, especially in the context of contrastive learning methods for graph-structured data.\n\n- \"Graph Contrastive Learning with Augmentations\" [b52]: This paper is also frequently mentioned, suggesting that it had a significant influence on the development of the paper's core contributions. It is particularly referenced in discussions of contrastive learning methods and graph self-supervised learning.\n\n- \"Large-scale learning on non-homophilous graphs: new benchmarks and strong simple methods\" [b26]: This paper is mentioned in the context of the challenges posed by non-homophilous graphs and appears to have influenced the author's approach to these challenges.\n\n- \"Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning\" [b9]: This paper is mentioned as an inspiration for the asymmetric encoder architecture used in the paper, which is used to prevent collapse to trivial solutions.\n\n- \"Decoupled Self Supervised Learning for Non Homophilous Graphs\" [None]: If we assume that this paper is the paper being analyzed, it seems that the author was also heavily influenced by their own previous work.\n\nBased on the above analysis, the source papers with their confidence scores are:\n\n{ \"b45\": 0.9, \"b52\": 0.85, \"b26\": 0.75, \"b9\": 0.7, \"None\": 0.65 }"], "62393e7f5aee126c0f1260e9": ["Based on the text of the academic paper and the titles of references, the most pertinent source papers which have directly inspired and motivated the author's work are:\n\n1. \"Controllable paraphrase generation with a syntactic exemplar\" [b3]\n2. \"Electra pre-training text encoders as discriminators rather than generators\" [b4]\n3. \"Neural syntactic preordering for controlled paraphrase generation\" [b10]\n4. \"Adversarial example generation with syntactically controlled paraphrase networks\" [b15]\n5. \"Exploring the limits of transfer learning with a unified text to text transformer\" [b27]\n6. \"Bleurt learning robust metrics for text generation\" [b31]\n\nTherefore, the JSON format answer would be:\n\n```\n{\n\"b3\": 0.9,\n\"b4\": 0.85,\n\"b10\": 0.8,\n\"b15\": 0.9,\n\"b27\": 0.85,\n\"b31\": 0.9\n}\n```\nThe confidence score is estimated based on the frequency and context of the citations in the text.", "Based on the information provided, the following source papers are identified:\n\n1. \"Controllable paraphrase generation with a syntactic exemplar\" [b3]: The paper is directly mentioned in the text and its method is used as a reference in the proposed algorithm. Confidence Score: 0.9\n\n2. \"Electra pre-training text encoders as discriminators rather than generators\" [b4]: This paper\u2019s method is used for the Quality Predictor (QP) model. Confidence Score: 0.85\n\n3. \"Neural syntactic preordering for controlled paraphrase generation\" [b10]: The paper is mentioned when discussing the challenge in selecting appropriate input control values in controlled paraphrase generation. Confidence Score: 0.8\n\n4. \"Adversarial example generation with syntactically controlled paraphrase networks\" [b15]: The paper is mentioned when discussing the challenge of generating high-quality paraphrases and the concept of controlled paraphrase generation. Confidence Score: 0.8\n\n5. \"Exploring the limits of transfer learning with a unified text-to-text transformer\" [b27]: The paper's method, T5-base, is used as the encoder-decoder model in the proposed algorithm. Confidence Score: 0.85\n\n6. \"BLEURT: Learning robust metrics for text generation\" [b31]: The method from this paper is used to measure the semantic similarity between sentences in the proposed algorithm. Confidence Score: 0.85\n\nThe JSON format of the answer is:\n{\n  \"b3\": 0.9,\n  \"b4\": 0.85,\n  \"b10\": 0.8,\n  \"b15\": 0.8,\n  \"b27\": 0.85,\n  \"b31\": 0.85\n}", "Based on the provided text, the most pertinent source papers are:\n\n1. \"Controllable paraphrase generation with a syntactic exemplar\" [b3]\n2. \"Bleurt learning robust metrics for text generation\" [b31]\n3. \"Adversarial example generation with syntactically controlled paraphrase networks\" [b15]\n4. \"Microsoft COCO: common objects in context\" [b18]\n5. \"Open question answering over curated and extracted knowledge bases\" [b7]\n6. \"Generating sentences from disentangled syntactic and semantic spaces\" [b2]\n7. \"Exploring the limits of transfer learning with a unified text to text transformer\" [b27]\n8. \"Electra: Pre-training text encoders as discriminators rather than generators\" [b4]\n\nThe authors of this paper mention explicitly that their syntactic score was inspired by [b15], and they use Bleurt [b31] to measure semantic similarity. They also refer to [b3] when talking about controlled paraphrase generation, and they used Microsoft's COCO dataset [b18] for training their model. The model they use for quality controlled paraphrase generation (QCPG) is based on T5-base [b27] and Electra [b4]. They also mention [b7] and [b2] when talking about downstream tasks that paraphrase generation has been used for.\n\nJson result:\n\n{\n    \"b3\": 1,\n    \"b31\": 1,\n    \"b15\": 1,\n    \"b18\": 0.8,\n    \"b7\": 0.7,\n    \"b2\": 0.7,\n    \"b27\": 0.8,\n    \"b4\": 0.8\n}", "Based on the given text, the following references seem to have inspired the most:\n\n1. \"Controllable paraphrase generation with a syntactic exemplar\" [b3] - The paper mentions that generating high-quality paraphrases can be challenging and improving linguistic diversity while maintaining semantic similarity is a primary objective. This is a key focus in the paper [b3].\n2. \"Electra pre-training text encoders as discriminators rather than generators\" [b4] - The model QCPG, central to the paper, uses Electra as a part of its framework, suggesting an influence from [b4].\n3. \"Transformers to learn hierarchical contexts in multiparty dialogue for span-based question answering\" [b27] - The paper uses T5-base, a transformer model, as its primary model, indicating an influence from [b27].\n4. \"BLEURT: Learning Robust Metrics for Text Generation\" [b31] - The paper uses Bleurt as its semantic similarity measure, indicating an influence from [b31].\n5. \"Adversarial example generation with syntactic controlled paraphrase networks\" [b15] - The paper mentions how the controlled generation approaches have yielded impressive results, which is a focus in the paper [b15].\n\n{\"b3\": 1, \"b4\": 0.8, \"b27\": 0.8, \"b31\": 0.8, \"b15\": 0.75}\n", "Based on the given text, the author's main task in this paper is to develop a new method for controlled paraphrase generation. The author introduces a Quality Controlled Paraphrase Generation model (QCPG) and a method to select the control parameters that optimize the quality of paraphrases.\n\nBased on the task and the proposed method, the author refers to several previous works that contribute to this study. However, only a few of them are directly inspired or motivated by this research.\n\nThe identified source papers are:\n\n1. \"Controllable paraphrase generation with a syntactic exemplar\" [b3]: The author is inspired by this research to measure the syntactic score [b15].\n2. \"Electra pre-training text encoders as discriminators rather than generators\" [b4]: The author uses Electra base model to predict the typical quality values.\n3. \"Adversarial example generation with syntactic controlled paraphrase networks\" [b15]: The author uses the approach of this paper to measure the normalized tree edit distance.\n4. \"BLEURT: Learning Robust Metrics for Text Generation\" [b31]: The author uses the BLEURT score to measure semantic similarity.\n5. \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" [b27]: The author uses the pre-trained T5-base as the encoder-decoder model.\n\nThe confidence scores for these papers are not available in the text. However, based on the frequency and context of the citations, the confidence score can be inferred as follows (in the order of their listing): 0.9, 0.8, 0.9, 0.8, 0.7.\n\nPlease note, the given text does not provide enough information to identify all source papers. Some references are not explicitly linked to the paper's main contributions or methods. Therefore, the list might contain less than 8 source papers.\n\nThe result in the json format is:\n{\"b3\": 0.9, \"b4\": 0.8, \"b15\": 0.9, \"b31\": 0.8, \"b27\": 0.7}"], "630ed16690e50fcafd793a2d": ["Based on the text of the paper, my analysis suggests the following as the source papers:\n\n1. \"[b77] algorithm hardware co design of adaptive floating point encodings for resilient deep learning inference\": This paper has been cited multiple times and seems to be a key inspiration for the development of the ANT framework. \n2. \"[b57] Convolutional neural networks using logarithmic data representation\": This paper is directly referenced in the context of the PoT type which is integrated into the ANT framework.\n3. \"[b65] energy efficient neural network accelerator based on outlier aware low precision computation\": This paper is mentioned in the context of outlier-aware quantization methods and seems to be a key inspiration for the ANT framework.\n4. \"[b72] Bit fusion bit level dynamically composable architecture for accelerating deep neural network\": This paper is referred to in the context of mixed-precision quantization, which the ANT framework is compatible with.\n5. \"[b85] Gobo quantizing attention based NLP models for low latency and energy efficient inference\": This paper is mentioned in the context of intra-tensor adaptivity which is a feature of the ANT framework.\n6. \"[b93] incremental network quantization towards lossless cnns with low precision weights\": This paper is directly referenced in the context of the PoT type which is integrated into the ANT framework.\n\nThis is my analysis based on the paper content and the references cited. The confidence scores are estimated based on the frequency of mentions and the context in which they are cited.\n\nHere is the result in the JSON format:\n\n{\n  \"b77\": 0.9,\n  \"b57\": 0.8,\n  \"b65\": 0.8,\n  \"b72\": 0.7,\n  \"b85\": 0.7,\n  \"b93\": 0.7\n}", "Based on the text of the paper, the following references seem to be the most relevant source papers:\n\n1. 'b65': \"energy efficient neural network accelerator based on outlier aware low precision computation\" - The paper discusses the concept of outlier-aware quantization and special hardware support which is a method that the given paper builds upon and criticizes for its high hardware design complexity.\n\n2. 'b72': \"bit fusion bit level dynamically composable architecture for accelerating deep neural network\" - This paper discusses mixed-precision quantization which the given paper states it is compatible with.\n\n3. 'b45': \"in datacenter performance analysis of a tensor processing unit\" - The paper mentions Google's TPU and its use of 8-bit integer type for inference, which the given paper uses as a basis for comparison.\n\n4. 'b63': \"NVIDIA a tensor core architecture\" - The paper mentions NVIDIA's Tensor Core and its use of FP16 and TF32, which the given paper uses as a basis for comparison.\n\n5. 'b57': \"convolutional neural networks using logarithmic data representation\" - The given paper refers to the PoT (power of two) type proposed in this paper and integrates it into its own framework.\n\n6. 'b77': \"algorithm hardware co design of adaptive floating point encodings for resilient deep learning inference\" - The paper discusses AdaptiveFloat type which is used by the given paper to build its own framework.\n\n7. 'b85': \"gobo quantizing attention based nlp models for low latency and energy efficient inference\" - The paper discusses the concept of low precision int for small values and high precision float or int for large values which the given paper builds upon.\n\n\nHere is the result in json format:\n\n{\n  \"b65\": 0.9,\n  \"b72\": 0.85,\n  \"b45\": 0.8,\n  \"b63\": 0.8,\n  \"b57\": 0.75,\n  \"b77\": 0.75,\n  \"b85\": 0.7\n}", "Based on the given text and the titles of the references, the source papers that have inspired the given paper seem to be:\n\n1. 'b45': 'in datacenter performance analysis of a tensor processing unit' - This is mentioned multiple times and seems to be a key reference for the work. The authors are likely inspired by the hardware used in this paper.\n\n2. 'b63': 'nvidia a tensor core architecture' - This paper is also mentioned multiple times, indicating its importance to the work. It is likely that the authors were inspired by the architecture proposed in this paper.\n\n3. 'b72': 'bit fusion bit level dynamically composable architecture for accelerating deep neural network' - This paper is referenced in the context of mixed-precision quantization, which the authors also employ in their work.\n\n4. 'b65': 'energy efficient neural network accelerator based on outlier aware low precision computation' - This paper is referenced in the context of outlier-aware quantization, a method that the authors criticize and propose an alternative to.\n\n5. 'b77': 'algorithm hardware co design of adaptive floating point encodings for resilient deep learning inference' - This paper is referenced in the context of adaptive floating point encodings, a concept that is central to the authors' proposed ANT framework.\n\n6. 'b57': 'convolutional neural networks using logarithmic data representation' - This paper is referenced as one of the previous works that the authors build upon in their ANT framework.\n\n7. 'b85': 'gobo quantizing attention based nlp models for low latency and energy efficient inference' - This paper is also referenced in the context of outlier-aware quantization and the authors seem to have been inspired by its methods.\n\nHere is the result in JSON format:\n{\n\"b45\": 0.9,\n\"b63\": 0.9,\n\"b72\": 0.8,\n\"b65\": 0.8,\n\"b77\": 0.8,\n\"b57\": 0.7,\n\"b85\": 0.7\n}\n\nNote: The confidence scores are subjective and are made on the basis of the number of times the papers are mentioned and the context in which they are mentioned. A score of 0.9 indicates a very high level of confidence that the paper is a source of inspiration, while a score of 0.7 indicates a moderate level of confidence.", "Based on the text and the criteria given, the primary challenge outlined in the paper is the inefficiencies of traditional numerical data types for deep neural network (DNN) models. The paper proposes an adaptive numeric data type called ANT to tackle this issue. The key phrases that indicate reliance on previous research for developing the paper's core contributions are \"build upon the previous works like PoT (i.e., power of two) type [b57], [b93]\" and \"Compared to previous works that only exploit the intra-tensor adaptivity [b65], [b72], [b85] or inter-tensor adaptivity [b72], [b77], the ANT framework can achieve both with high hardware efficiency.\"\n\nThe source papers that inspired the current work are likely to be:\n\n- \"Power of two\" type [b57], [b93]\n- Intra-tensor adaptivity [b65], [b72], [b85]\n- Inter-tensor adaptivity [b72], [b77]\n\nThe titles of these papers are:\n\n- b57: \"Convolutional neural networks using logarithmic data representation\"\n- b93: \"Incremental network quantization towards lossless cnns with low precision weights\"\n- b65: \"Energy efficient neural network accelerator based on outlier aware low precision computation\"\n- b72: \"Bit fusion bit level dynamically composable architecture for accelerating deep neural network\"\n- b85: \"Gobo quantizing attention based nlp models for low latency and energy efficient inference\"\n- b77: \"Algorithm hardware co design of adaptive floating point encodings for resilient deep learning inference\"\n\nThese papers seem to have a high relevance to the current paper's contributions, as they tackle similar challenges and propose related methods. \n\nThe confidence score of these papers as source papers based on the provided criteria would be:\n\n{b57: 0.9, b93: 0.9, b65: 0.9, b72: 0.9, b85: 0.9, b77: 0.9}\n\nThese confidence scores are high because the text explicitly mentions these papers as inspiration for the current work.", "Based on the text, the following references seem to be the most influential to the paper:\n\n1. \"b65\": 'Energy efficient neural network accelerator based on outlier aware low precision computation' - The paper has proposed a similar outlier-aware quantization method but found it to have a complex and high-cost hardware design. This inspired the authors to develop a more efficient alternative, the ANT framework. \n\n2. \"b72\": 'Bit fusion bit level dynamically composable architecture for accelerating deep neural network' - This paper is mentioned regarding the method of mixed-precision quantization, which the authors also use in their ANT framework.\n\n3. \"b77\": 'Algorithm hardware co design of adaptive floating point encodings for resilient deep learning inference' - The authors leveraged the adaptive float type presented in this paper to reduce quantization errors in tensors with a non-uniform distribution.\n\n4. \"b57\": 'Convolutional neural networks using logarithmic data representation' - This paper introduced the PoT type, which the authors also integrate into their ANT framework.\n\n5. \"b45\": 'In datacenter performance analysis of a tensor processing unit' - The ANT framework is designed to be integrated into existing DNN accelerator architectures, such as the tensor core mentioned in this paper.\n\n6. \"b63\": 'NVIDIA a tensor core architecture' - This paper is referred to multiple times, especially regarding the use of tensor core and its mixed-precision support, which the authors also adopt in their ANT framework.\n\nThe confidence scores for the given references are estimated as follows:\n\n{\n\"b65\": 0.9,\n\"b72\": 0.8,\n\"b77\": 0.8,\n\"b57\": 0.9,\n\"b45\": 0.85,\n\"b63\": 0.85\n}"], "62a2b6955aee126c0f4d8e7b": ["Based on the given paper text, the source papers which author indicates that inspire them most are:\n\n- \"Parameter Efficient Transfer Learning for NLP\" [b17]: This paper seems to be the source of the Adapter method, which is a key method discussed and used in the given paper. The confidence score is 0.9.\n- \"Low Rank Adaptation of Large Language Models\" [b18]: This paper is cited as the source of the LoRA method, another key method discussed and used in the given paper. The confidence score is 0.9.\n- \"AutoFormer: Searching Transformers for Visual Recognition\" [b4]: This paper is the source of the AutoFormer method, which is applied in the given paper for their proposed NOAH method. The confidence score is 0.9.\n- \"An Image is Worth x Words: Transformers for Image Recognition at Scale\" [b9]: The given paper uses Vision Transformer (ViT), which is introduced in this paper. The confidence score is 0.8.\n- \"Learning Transferable Visual Models From Natural Language Supervision\" [b36]: This paper is mentioned in respect to few-shot learning, which is a topic discussed in the given paper. The confidence score is 0.6.\n- \"A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark\" [b47]: The VTAB-1k benchmark used in the given paper is introduced in this paper. The confidence score is 0.8.\n\nThe JSON format of the result is:\n{\"b17\": 0.9, \"b18\": 0.9, \"b4\": 0.9, \"b9\": 0.8, \"b36\": 0.6, \"b47\": 0.8}", "Based on the given text, the most pertinent source papers that inspired the current paper are:\n1. \"Parameter Efficient Transfer Learning for NLP\" [b17]\n2. \"Low Rank Adaptation of Large Language Models\" [b18]\n3. \"Visual Prompt Tuning (VPT)\" [b19]\n4. \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" [b9]\n5. \"AutoFormer: Searching Transformers for Visual Recognition\" [b4]\n\nThe paper indicates these as the primary sources of inspiration as they are frequently cited in the text, especially when discussing the methods and algorithms employed in the current research. The authors also explicitly mention that their approach, NOAH, incorporates methods from these papers and learns from them. \n\nHere is the JSON format:\n{\n\"b17\": 0.9,\n\"b18\": 0.9,\n\"b19\": 0.9,\n\"b9\": 0.85,\n\"b4\": 0.85\n}", "Based on the text of the paper, the most pertinent source papers that the author indicates that inspire them the most are: \n\n- \"Parameter Efficient Transfer Learning for NLP\" [b17]\n- \"Low Rank Adaptation of Large Language Models\" [b18]\n- \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" [b9]\n- \"AutoFormer: Searching Transformers for Visual Recognition\" [b4]\n- \"A Large-Scale Study of Representation Learning with the Visual Task Adaptation Benchmark\" [b47]\n- \"Visualizing and Understanding Convolutional Networks\" [b46]\n\nThe JSON format of the result is:\n```json\n{\n  \"b17\": 1,\n  \"b18\": 0.95,\n  \"b9\": 0.9,\n  \"b4\": 0.85,\n  \"b47\": 0.8,\n  \"b46\": 0.75\n}\n```\nHere, the confidence scores are assigned based on the frequency and the context in which these papers are mentioned in the given text. The score 1 represents the highest confidence and 0.75 the lowest among these selected papers.", "Based on the text analysis, the source papers that inspired this work (in descending order of confidence) are likely:\n\n- \"b9\": \"An image is worth x words transformers for image recognition at scale\" (Confidence: 0.9)\n- \"b19\": No title provided, but frequently cited (Confidence: 0.85)\n- \"b17\": \"Parameter efficient transfer learning for nlp\" (Confidence: 0.8)\n- \"b18\": \"Low rank adaptation of large language models\" (Confidence: 0.8)\n- \"b4\": \"AutoFormer: Searching Transformers for Visual Recognition\" (Confidence: 0.7)\n- \"b47\": \"A Large-Scale Study of Representation Learning with the Visual Task Adaptation Benchmark\" (Confidence: 0.65)\n- \"b51\": \"Learning to Prompt for Vision Language Models\" (Confidence: 0.6)\n\nThe JSON format:\n\n{\n\"b9\": 0.9,\n\"b19\": 0.85,\n\"b17\": 0.8,\n\"b18\": 0.8,\n\"b4\": 0.7,\n\"b47\": 0.65,\n\"b51\": 0.6\n}", "Based on the given text and the titles of the references, the source papers that most likely inspired this paper are:\n\n{'b4': 1.0, 'b17': 0.8, 'b18': 0.8, 'b19': 0.8, 'b9': 0.6, 'b47': 0.6}\n\nExplanation:\n'AutoFormer: Searching Transformers for Visual Recognition' [b4] is mentioned multiple times throughout the text and it seems to be the source of the one-shot NAS algorithm used in this paper, hence it has the highest confidence score.\n'Parameter-Efficient Transfer Learning for NLP' [b17], 'Low-Rank Adaptation of Large Language Models' [b18] and the unnamed paper [b19] are the source of the parameter-efficient tuning methods (Adapter, LoRA, VPT) applied in this paper.\n'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale' [b9] is the source of the Vision Transformer (ViT) model which this paper's approach is mainly applied to.\n'A Large-Scale Study of Representation Learning with the Visual Task Adaptation Benchmark' [b47] is the source of the VTAB-1k benchmark used for evaluating the proposed method in this paper."], "6389d6b590e50fcafdfead12": ["Based on the text and the titles of the references, the most pertinent source papers that inspired the given paper are:\n\n1. 'The renewed case for the reduced instruction set computer: avoiding ISA bloat with macro op fusion for RISC-V' [b6]\n2. 'Implementing optimizations at decode time' [b16]\n3. 'Increasing cache port efficiency for dynamic superscalar microprocessors' [b30]\n4. 'Design of the RISC-V instruction set architecture' [b29]\n5. 'Two techniques to enhance the performance of memory consistency models' [b8]\n6. 'ARM architecture reference manual Armv8-A' [b1]\n7. 'Combining load or store instructions' [b28]\n\nThe JSON format of the result:\n\n{\n\"b6\": 1,\n\"b16\": 0.9,\n\"b30\": 0.8,\n\"b29\": 0.7,\n\"b8\": 0.6,\n\"b1\": 0.5,\n\"b28\": 0.4\n}\n\nThe confidence scores are assigned based on the frequency of the references in the text and their relevance to the paper's novel contributions. The scores are subjective and may vary depending on the interpreter's understanding of the text.", "Based on the academic paper, the source papers that inspired the author most are:\n\n1. \"The renewed case for the reduced instruction set computer avoiding ISA bloat with macro op fusion for RISC v\" (b6)\n2. \"Implementing optimizations at decode time\" (b16)\n3. \"Software optimization guide for AMD Epyc processors\" (b0)\n4. \"ARM Cortex-A core software optimization guide\" (b2)\n5. \"ARM Neoverse N core software optimization guide\" (b3)\n6. \"Design of the RISC-V instruction set architecture\" (b29)\n7. \"Increasing cache port efficiency for dynamic superscalar microprocessors\" (b30)\n8. \"Combining load or store instructions\" (b28)\n\nThe JSON format of this result is:\n\n{\n    \"b6\": 1,\n    \"b16\": 0.9,\n    \"b0\": 0.8,\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b29\": 0.7,\n    \"b30\": 0.6,\n    \"b28\": 0.6\n}\n\nThe confidence score is set based on the frequency and importance of the reference paper in the given text. The paper \"The renewed case for the reduced instruction set computer avoiding ISA bloat with macro op fusion for RISC v\" (b6) seems to be the most influential since it is repeatedly mentioned and directly linked to the main topic of the paper. Other papers are assigned lower confidence scores based on their relevance to the main challenges and methods proposed in the paper.", "Based on the text and the titles of the referenced papers, the source papers that inspire the authors are:\n\n1. \"Software optimization guide for AMD EPYC processors\" (b0)\n2. \"ARM Cortex-A Core Software Optimization Guide\" (b2)\n3. \"The renewed case for the Reduced Instruction Set Computer: Avoiding ISA bloat with Macro-Op Fusion for RISC-V\" (b6)\n4. \"Implementing optimizations at decode time\" (b16)\n5. \"Design of the RISC-V Instruction Set Architecture\" (b29)\n\nAccording to the text, these papers have motivated the authors to focus on \"instruction fusion\" for optimizing pipeline resource utilization, specifically in the context of the RISC-V ISA. The authors discuss the challenges and limitations of existing approaches outlined in these papers and propose novel solutions to address them, such as the Helios microarchitecture.\n\nThe JSON format for these source papers would be:\n\n{\n\"b0\": 0.8,\n\"b2\": 0.8,\n\"b6\": 1,\n\"b16\": 0.9,\n\"b29\": 0.9\n}\n\nNote: The confidence score ranges from 0 to 1 with 1 being the highest confidence.", "Based on the text and the titles of the references, the most relevant source papers that inspired this academic paper are:\n\n- 'b6': 'The renewed case for the reduced instruction set computer avoiding ISA bloat with macro op fusion for RISC V' - The paper uses the RISC-V ISA quite extensively in their research and this reference appears more than once in the text.\n- 'b16': 'Implementing optimizations at decode time' - This paper is mentioned when discussing non-consecutive fusion, a topic that is explored deeply in the paper.\n- 'b1': 'ARM architecture reference manual ARMv a' - This reference is used when discussing architectural fusion and comparing it to microarchitectural fusion, a key topic in the paper.\n- 'b29': 'Design of the RISC-V instruction set architecture' - The RISC-V ISA's design is mentioned throughout the paper and seems to be the foundation for the author's work.\n- 'b28': 'Combining load or store instructions' - This reference is mentioned when discussing microarchitectural fusion, one of the main focuses of the paper.\n\nThe JSON format will be:\n\n{\n\"b6\": 0.9,\n\"b16\": 0.85,\n\"b1\": 0.8,\n\"b29\": 0.8,\n\"b28\": 0.75\n} \n\nThe confidence scores are given based on the frequency and context of appearance of the references in the text.", "Based on the text of the paper, the most pertinent source papers are likely:\n\n1. \"The renewed case for the reduced instruction set computer avoiding ISA bloat with macro op fusion for RISC V\" [b6]: This paper is mentioned several times in the context of fusion and the RISC-V ISA. It seems to be a major inspiration for the work done in the paper.\n\n2. \"Implementing optimizations at decode time\" [b16]: This paper is mentioned in the context of fusing non-consecutive memory accesses, which is a main point of discussion in the paper. \n\n3. \"Increasing cache port efficiency for dynamic superscalar microprocessors\" [b30]: Mentioned in the context of improving cache bandwidth, which seems to be a key point in the paper.\n\n4. \"Arm architecture reference manual armv a\" [b1]: This paper is mentioned several times in the context of the Armv8 architecture and fusion, suggesting it was a major influence on this paper.\n\n5. \"Exploring instruction fusion opportunities in general purpose processors\" [b0]: Mentioned in the context of fusion and seems to be a key reference for the paper.\n\nThe confidence scores for each paper are estimated as follows: \n\n- [b6]: 0.9\n- [b16]: 0.8\n- [b30]: 0.7\n- [b1]: 0.9\n- [b0]: 0.8\n\nThe result in the JSON format: {\"b6\": 0.9, \"b16\": 0.8, \"b30\": 0.7, \"b1\": 0.9, \"b0\": 0.8}"], "63608e5090e50fcafdee1257": ["Based on the text of the paper and the titles of the referenced papers, the source papers that inspired this work most directly appear to be:\n\n1. \"parameter efficient tuning makes a good classification head\"\n2. \"Fine-tuning pretrained language models: weight initializations, data orders, and early stopping\"\n3. \"Fine-tuning can distort pretrained features and underperform out-of-distribution\"\n4. \"LoRA: Low-Rank Adaptation of Large Language Models\"\n5. \"Parameter-efficient transfer learning for NLP\"\n6. \"Prefix tuning: Optimizing continuous prompts for generation\"\n7. \"Overcoming catastrophic forgetting in zero-shot cross-lingual generation\"\n\nThe JSON output would be:\n\n```\n{\n \"None\": 0.9,\n \"b9\": 0.8,\n \"b27\": 0.8,\n \"b21\": 0.8,\n \"b12\": 0.8,\n \"b33\": 0.8,\n \"b55\": 0.8\n}\n```\n\nThe confidence scores are assigned based on the relevance of the topics discussed in the source papers to the main contributions of the given paper. The paper without a reference number (None) appears to be most directly relevant, hence it is assigned the highest confidence score. The rest of the papers are assigned slightly lower scores due to their indirect influence on the paper's contributions.", "Based on the text of the paper, the key challenges outlined are the large changes in the fine-tuning parameters of the last layer of pre-trained models and the importance of classification head initialization. The paper proposes a solution by adjusting the parameters of the classification head through parameter-efficient fine-tuning methods before fine-tuning the backbone and classification head.\n\nThe text indicates that the paper is primarily inspired by the works referenced as [b27], [b33], [b36], [b12], [b65], and [b21]. These references are mentioned in the context of the challenges and proposed solution, suggesting they directly influenced the paper's core contributions.\n\nHere are the titles of these references:\n\nb27: \"Fine-tuning can distort pretrained features and underperform out of distribution\"\nb33: \"Prefix tuning: optimizing continuous prompts for generation\"\nb36: \"GPT understands too\"\nb12: \"Parameter-efficient transfer learning with diff pruning\"\nb65: \"BitFit: simple parameter-efficient fine-tuning for transformer-based masked language models\"\nb21: \"LoRa: Low-rank adaptation of large language models\"\n\nSo, the most pertinent source papers are:\n\n{\"b27\": 1, \"b33\": 1, \"b36\": 1, \"b12\": 1, \"b65\": 1, \"b21\": 1} \n\nEach paper is given a confidence score of 1, indicating a high level of influence on the main contributions of the paper.", "Based on the paper's text and the titles of the references, the source papers that inspired this work most likely include:\n\n1. \"Fine tuning can distort pretrained features and underperform out-of-distribution\" (b27): The paper's methodology was heavily influenced by this work, which introduced the concept of finetuning the head of a pretrained model before finetuning the whole model.\n\n2. \"Parameter efficient transfer learning with diff pruning\" (b12): This work appears to have provided valuable insights into the use of parameter-efficient tuning methods in the fine-tuning process.\n\n3. \"LoRA: Low Rank adaptation of large language models\" (b21): This is likely a key source paper, as it is mentioned in the context of good classification heads and parameter-efficient tuning methods.\n\n4. \"Prefix tuning: Optimizing continuous prompts for generation\" (b33): This paper is directly mentioned as a method that creates good classification heads, making it a probable source of inspiration.\n\n5. \"BitFit: Simple parameter efficient fine tuning for transformer based masked language models\" (b3): This paper is also directly mentioned as an effective method for creating good classification heads.\n\nThe confidence score based on the frequency and context of the citations in the paper's text would look something like this: {\"b27\": 0.9, \"b12\": 0.8, \"b21\": 0.8, \"b33\": 0.8, \"b3\": 0.8}", "Based on the text of the paper, the key phrases indicating inspiration, the challenges outlined, the proposed solution, as well as the titles of the references, the source papers most likely to have inspired this paper are:\n\n{ \"b25\": 0.8, \"b33\": 0.9, \"b21\": 0.9, \"b27\": 0.8, \"b36\": 0.7, \"b65\": 0.9, \"b12\": 0.7, \"b32\": 0.9 }\n\nHere's a brief explanation:\n\n- \"b25\" (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding): The paper mentions fine-tuning pretrained language models and specifically mentions BERT.\n- \"b33\" (Prefix Tuning: Optimizing Continuous Prompts for Generation): The paper discusses using parameter-efficient tuning methods such as Prefix-Tuning to optimize the classification head.\n- \"b21\" (LoRA: Low-Rank Adaptation of Large Language Models): The paper mentions using LoRA as a good classification head. \n- \"b27\" (Fine-tuning Can Distort Pretrained Features and Underperform Out-of-Distribution): The paper discusses the findings of this paper and its influence in the methodology.\n- \"b36\" (GPT Understands, Too): The paper refers to GPT and its understanding, which could have influenced the approach.\n- \"b65\" (BitFit: Simple Parameter-Efficient Fine-Tuning for Transformer-Based Masked Language Models): The paper refers to BitFit as one of the parameter-efficient tuning methods used.\n- \"b12\" (Parameter-Efficient Transfer Learning with Diff Pruning): The paper mentions parameter-efficient tuning methods and this paper could have influenced the methodology.\n- \"b32\" (Measuring the Intrinsic Dimension of Objective Landscapes): The paper refers to these measurement methods, suggesting it could have been an influential source.", "Based on the provided information, the most pertinent source papers are likely:\n\n1. \"Fine-tuning can distort pretrained features and underperform out-of-distribution [b27]\"\n2. \"Parameter efficient transfer learning with diff pruning [b12]\"\n3. \"LoRa: Low Rank adaptation of large language models [b21]\"\n4. \"Prefix Tuning: Optimizing continuous prompts for generation [b33]\"\n5. \"BitFit: Simple parameter efficient fine tuning for transformer based masked language models [b3]\"\n6. \"Parameter-efficient transfer learning for NLP [b19]\"\n\nThe above papers are mentioned in the text in relation to the methods and challenges that the paper is addressing. The author's note indicates that the paper is dealing with the issue of major changes in the parameters of the final layer of pretrained models during fine-tuning, and these papers are relevant to this issue. Particularly, the papers by [b12], [b21], [b33], and [b3] are directly relevant to parameter-efficient fine-tuning methods that the paper introduces.\n\nIn JSON format, it's represented as:\n\n{\n\"b27\": 0.9,\n\"b12\": 0.85,\n\"b21\": 0.85,\n\"b33\": 0.8,\n\"b3\": 0.8,\n\"b19\": 0.75\n}\n\nThe confidence scores are estimated based on the frequency and context in which the papers are mentioned in the text."], "628ef0495aee126c0f82db2f": ["Based on the text and title of the references, the source papers that most likely inspired the author are:\n\n{\"b28\": 0.9, \"b30\": 0.8, \"b17\": 0.7, \"b24\": 0.6, \"b5\": 0.5, \"b15\": 0.5, \"b11\": 0.4}\n\nExplanation:\n\n1. Reference \"b28\" (DocRED: A Large-scale Document-level Relation Extraction Dataset) is highly likely to be a source paper as it is mentioned multiple times throughout the text. Furthermore, the author's work is a direct extension of this paper, addressing the false negative problem in DocRED.\n\n2. Reference \"b30\" (Positionaware Attention and Supervised Data Improve Slot Filling) seems to be a source paper as it is also mentioned multiple times, specifically, it is cited as the base for the TACRED dataset, which is related to the problem addressed in this paper.\n\n3. Reference \"b17\" (Does Recommend-Revise Produce Reliable Annotations? An Analysis on Missing Instances in DocRED) is also likely a source paper due to its direct relevance to the false negative problem in DocRED, a key point discussed by the author.\n\n4. Reference \"b24\" (Document Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation) is cited when discussing re-annotation works on sentence-level relation extraction datasets, which suggests that it may have been an inspiration for the author's approach.\n\n5. Reference \"b5\" (H-FND: Hierarchical False Negative Denoising for Distant Supervision Relation Extraction) and \"b15\" (Knowing False Negatives: An Adversarial Training Method for Distantly Supervised Relation Extraction) are both mentioned when discussing other efforts to address the false negative problem from the model perspective, suggesting that they may have inspired the author's work.\n\n6. Reference \"b11\" (Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction) is mentioned when discussing the re-annotation of test sets of two distantly supervised RE datasets. It seems that this paper inspired some aspects of the author's approach.", "Based on the text, the following references are identified as source papers:\n\n1. \"The TACRED dataset [b30] is constructed by the TAC Knowledge Base Population challenge.\" - This paper is mentioned in the context of describing the construction of a popular relation extraction dataset. The authors refer to this method to highlight the challenges in the current relation extraction datasets.\n\n2. \"The NYT10 [b21] dataset matched the Freebase Knowledge base [b2] to the New York Times corpus [b22].\" - This is mentioned as another example of how popular datasets are constructed, and the authors seem to draw inspiration from this for their work.\n\n3. \"Alt et al. (2020) re-annotated a small amount of challenging samples in the development and test sets of the TA-CRED dataset [b30].\" - The authors seem to draw inspiration from this methodology of re-annotating data to correct false positive samples.\n\n4. \"Most recently, [b17] also identify the false negative issue in DocRED [b28] , and they combat the problem by annotating 96 documents from scratch with two expert annotators.\" - The authors discuss this method as a solution to the false negative problem in DocRED, but they also propose an alternative approach, suggesting that they're inspired by but also improving on this method.\n\n5. \"For the document-level task, we use the ATLOP [b32] as the baseline model, and we use RoBERTa-large [b19] as the encoder for DocRED and XLM-R-base [b8] for Ha-cRED.\" - The authors are utilizing these models for their document-level tasks, indicating these papers have influenced their methodological approach.\n\nHere are the results in JSON format:\n\n{\n\"b30\": 0.9,\n\"b21\": 0.8,\n\"b2\": 0.8,\n\"b22\": 0.8,\n\"b17\": 0.8,\n\"b32\": 0.9,\n\"b19\": 0.9,\n\"b8\": 0.9\n} \n\nThe confidence scores are given based on the extent of reliance and relevance of the source papers to the given paper's methodology and novel contributions.", "Based on the text of the academic paper and the titles of the references, the source papers that inspired the author most in their research appear to be:\n\n{ \n\"b28\": 1,\n\"b5\": 0.9,\n\"b15\": 0.9,\n\"b24\": 0.8,\n\"b30\": 0.7,\n\"b21\": 0.6,\n\"b11\": 0.6,\n\"b17\": 0.6\n}\n\nExplanation:\n\n- The paper \"DocRED: A Large-Scale Document-Level Relation Extraction Dataset\" [b28] is cited multiple times throughout the text, indicating it had a significant influence on the author's work. This paper introduced the original DocRED dataset that the author is seeking to revise and improve.\n- The papers [b5] \"Hierarchical False Negative Denoising for Distant Supervision Relation Extraction\" and [b15] \"Knowing False Negatives: An Adversarial Training Method for Distantly Supervised Relation Extraction\" are also cited as recent efforts addressing the false negative problem from the model perspective, which is a central issue in this work.\n- The paper [b24] \"Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation\" extended the work of Alt et al. (2020) and re-annotated the training, development, and test sets of TACRED. This appears to be relevant to the author's work on revising DocRED.\n- The paper [b30] \"Position-Aware Attention and Supervised Data Improve Slot Filling\" is the source of the TACRED dataset, which is mentioned as a comparison point for the author's work on DocRED.\n- The paper [b21] \"Relation Extraction with Matrix Factorization and Universal Schemas\" provides the NYT10 dataset, another comparison point in the author's work.\n- The paper [b11] \"Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction\" is referenced as a work that has re-annotated the test sets of two distantly supervised RE datasets.\n- The paper [b17] \"Does Recommend-Revise Produce Reliable Annotations? An Analysis on Missing Instances in DocRED\" is cited as a recent work that also identified the false negative issue in DocRED and sought to combat this problem. \n\nThe confidence scores are assigned based on the frequency and context of citation, with a higher score indicating a stronger influence on the author's work.", "Based on the provided paper text, the most pertinent source papers that have inspired the author are:\n\n1. \"DocRED: A Large-scale Document-level Relation Extraction Dataset\" [b28]\n2. \"Revisiting DocRED: Addressing the Overlooked False Negative Problem in Relation Extraction\" [b17]\n3. \"Hierarchical False Negative Denoising for Distant Supervision Relation Extraction\" [b5]\n4. \"Knowing False Negatives: An Adversarial Training Method for Distantly Supervised Relation Extraction\" [b15]\n5. \"TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task\" [b30]\n6. \"An End-to-End Model for Entity-level Relation Extraction Using Multi-instance Learning\" [b10]\n7. \"Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction\" [b11]\n\nThe papers were selected based on the criteria mentioned in the task, as they are frequently cited and directly related to the core contributions of the paper. The author heavily relies on these works to develop their novel contributions, specifically in addressing the false negative problem in relation extraction datasets, proposing an iterative approach involving human in the loop, and revising the DocRED dataset.\n\nIn JSON format, the result would be:\n\n{\n\"b28\": 0.9,\n\"b17\": 0.8,\n\"b5\": 0.8,\n\"b15\": 0.8,\n\"b30\": 0.7,\n\"b10\": 0.7,\n\"b11\": 0.7\n}\n\nThe confidence score is set close to 1 to reflect a high degree of relevance to the given paper's main contributions.", "Based on the provided text, the paper primarily addresses the false negative problem in relation extraction datasets and proposes a state-of-the-art approach to revise the DocRED dataset. The references that seem to have the most influence on this work are the ones that address similar issues or are directly mentioned as part of the approach. These references include:\n\n1. [b28]: \"DocRED: A large-scale document-level relation extraction dataset\". This paper seems to be the main source of inspiration as the author's work directly improves upon the DocRED dataset.\n2. [b17]: \"Does recommend-revise produce reliable annotations? An analysis on missing instances in DocRED\". This paper identifies similar issues with the DocRED dataset and is compared to the author's approach.\n3. [b24]: \"Document-level relation extraction with adaptive focal loss and knowledge distillation\". This paper could have influenced the author's methodological approach.\n4. [b5]: \"H-FND: Hierarchical False Negative Denoising for Distant Supervision Relation Extraction\". It's one of the recent efforts on addressing the false negative problem from the model perspective. \n5. [b11]: \"Manual evaluation matters: Reviewing test protocols of distantly supervised relation extraction\". This paper also re-annotated the test sets of two distantly supervised RE datasets, which could have influenced the author's approach.\n6. [b30]: \"Position-aware attention and supervised data improve slot filling\". This paper is used as the base dataset for TACRED for comparison purpose.\n7. [b15]: \"Knowing False Negatives: An Adversarial Training Method for Distantly Supervised Relation Extraction\". This paper provides a method to denoise the false negative data during training which is in line with the proposed method in this paper.\n8. [b32]: \"An improved baseline for sentence-level relation extraction\". The author uses this as the baseline model for the document-level task.\n\nSo, the result in json format is:\n{\n\"b28\": 1,\n\"b17\": 0.9,\n\"b24\": 0.85,\n\"b5\": 0.8,\n\"b11\": 0.8,\n\"b30\": 0.8,\n\"b15\": 0.75,\n\"b32\": 0.7\n}"], "622819cdd18a2b26c7ab496a": ["Based on the provided paper text, the source papers that most influenced the work are:\n\n1. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b7]: The paper mentions BERT multiple times and discusses its limitations that the authors of the paper are trying to address.\n   \n2. \"XLNet: Generalized Autoregressive Pretraining for Language Understanding\" [b39]: XLNet is mentioned in the paper and the authors differentiate their work from it.\n\n3. \"SpanBERT: Improving Pre-training by Representing and Predicting Spans\" [b14]: This paper is referred to when the authors discuss their proposed method.\n\n4. \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" [b27]: The paper refers to T5 as another method they are comparing their work with.\n\n5. \"UniLMv2: Pseudo Masked Language Models for Unified Language Model Pre-training\" [b1]: The paper mentions UniLM as a work they compare and contrast their model with.\n\n6. \"Language Models Are Few-Shot Learners\" [b3]: This paper is mentioned as the authors discuss the limitations of GPT-3, which is the focus of this paper.\n\n7. \"Improving Language Understanding by Generative Pre-Training\" [b25]: This paper is referred to when discussing the limitations of GPT, which the authors of the paper are trying to address.\n\n8. \"PET: Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference\" [b32]: This paper is referred to when the authors discuss their method of finetuning.\n\nThus, the relevant source papers are: b7, b39, b14, b27, b1, b3, b25, b32. \n\nThe confidence scores for these references would likely be high (around 0.9-1) due to their direct relevance to the paper's main contributions and the frequent references to them throughout the paper text.", "Based on the text and author's notes, the most relevant source papers for this paper are likely:\n\n1. BERT (b7): The paper discusses BERT in detail, highlighting its strengths and weaknesses and how the proposed GLM framework builds upon and differs from BERT.\n\n2. GPT (b25): GPT is mentioned as a model with strong long-text generation capabilities. It's also mentioned as a model that has been influential but has certain limitations that the new GLM model aims to address.\n\n3. T5 (b27): T5 is referenced and compared with the proposed GLM model multiple times. It's clear that T5 is a significant point of reference for this paper.\n\n4. XLNet (b39): The paper discusses XLNet as a model that shares similarities with the proposed GLM model, but also has key differences that the authors of this paper have sought to improve upon.\n\n5. SpanBERT (b14): The paper discusses SpanBERT and its differences with the proposed GLM model, indicating that SpanBERT is a source of inspiration for this paper.\n\n6. UniLM (b9) and UniLMv2 (b1): The paper discusses UniLM and UniLMv2 as models that have tried to unify different pretraining objectives, similar to what the GLM model is trying to achieve. It also discusses the differences between these models and the proposed GLM model.\n\nIn the json format:\n\n{\n   \"b7\":0.9,\n   \"b25\":0.9,\n   \"b27\":0.9,\n   \"b39\":0.9,\n   \"b14\":0.9,\n   \"b9\":0.9,\n   \"b1\":0.9\n}", "Based on the given information, the source papers that inspire the given paper the most are:\n\n1. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b7]\n2. \"XLNet: Generalized Autoregressive Pretraining for Language Understanding\" [b39]\n3. \"T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" [b27]\n4. \"UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-training\" [b1]\n5. \"SpanBERT: Improving Pre-training by Representing and Predicting Spans\" [b14]\n6. \"MASS: Masked Sequence to Sequence Pre-training for Language Generation\" [b35]\n7. \"Exploring Cloze Questions for Few-shot Text Classification and Natural Language Inference\" [b32]\n\nThe confidence score for these references are as follows:\n\n- [b7]: 0.9\n- [b39]: 0.8\n- [b27]: 0.85\n- [b1]: 0.75\n- [b14]: 0.65\n- [b35]: 0.75\n- [b32]: 0.85\n\nThe json format of the result is:\n\n```\n{\n  \"b7\": 0.9,\n  \"b39\": 0.8,\n  \"b27\": 0.85,\n  \"b1\": 0.75,\n  \"b14\": 0.65,\n  \"b35\": 0.75,\n  \"b32\": 0.85\n}\n```", "Based on the given text, the following are the source papers for the given paper:\n\n1. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b7] is mentioned multiple times throughout the text and the author compares their method to it, indicating that their work is heavily inspired by it.\n\n2. \"Improving Language Understanding by Generative Pre-training\" [b25] is referred to when discussing autoregressive models like GPT, which the author's work also incorporates.\n\n3. \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" [b27] or T5 is mentioned multiple times, and the author draws comparisons and contrasts with their own work, indicating its influence.\n\n4. \"UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-training\" [b1] is mentioned in the discussion and analysis section, where the author compares their proposed GLM with UniLM, indicating that it has influenced their work.\n\n5. \"XLNet: Generalized Autoregressive Pretraining for Language Understanding\" [b39] is compared to the proposed GLM, signifying its influence.\n\n6. \"SpanBERT: Improving Pre-training by Representing and Predicting Spans\" [b14] is compared with GLM, indicating it was a source of inspiration.\n\n7. \"Exploiting Cloze Questions for Few-Shot Text Classification and Natural Language Inference\" [b32] is mentioned as the source of inspiration for reformulating NLU classification tasks as generation tasks of blank infilling.\n\nTherefore, the source papers are:\n\n- [b7]: 1\n- [b25]: 0.8\n- [b27]: 0.9\n- [b1]: 0.8\n- [b39]: 0.8\n- [b14]: 0.7\n- [b32]: 0.7\n\nPlease note that the confidence score is subjective and based on the frequency and context of the mention in the text.", "Based on the given text, the source papers of the given paper that inspired the authors most are:\n\n1. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b7]\n2. \"GPT: Improving Language Understanding by Generative Pre-training\" [b25]\n3. \"T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" [b27]\n4. \"SpanBERT: Improving Pre-training by Representing and Predicting Spans\" [b14]\n5. \"XLNet: Generalized Autoregressive Pretraining for Language Understanding\" [b39]\n6. \"UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation\" [b9]\n7. \"UniLMv2: Pseudo Masked Language Models for Unified Language Model Pre-training\" [b1]\n\nThese papers seem to be the most pertinent source papers as they are directly mentioned in the text and have had a significant impact on the research direction and methodology of the paper. The paper discusses the shortcomings of these models and how its proposed method aims to improve upon them. \n\nIn the JSON format, the result would be:\n\n{\n    \"b7\": 0.9,\n    \"b25\": 0.9,\n    \"b27\": 0.9,\n    \"b14\": 0.7,\n    \"b39\": 0.7,\n    \"b9\": 0.7,\n    \"b1\": 0.7\n}\n\nThe confidence scores are subjective and based on the frequency and context of the references in the text. It seems that BERT, GPT, and T5 are mentioned more frequently and in more crucial parts of the text, therefore, they are assigned higher confidence scores."], "623d90d46750f864fe4cafd6": ["Based on the provided text and author notes, the paper mainly discusses a novel approach to handling node classification in graph-structured data, combining Graph Neural Networks (GNNs) and Conditional Random Fields (CRFs) in a model called Structured Proxy Network (SPN). This approach tackles the challenge of maximizing the likelihood function in learning CRFs through an approximate optimization problem. \n\nThe author notes mention the use of the CRF and GNN combination, the use of a proxy problem for simplifying the optimization of CRFs, and a new cost function. It also highlights the benefit of leveraging the relationship between adjacent node labels for better propagation and learning. \n\nBased on these insights, the key source papers that influenced the paper's main contributions can be identified as:\n\n1. \"Discriminative embeddings of latent variable models for structured data\" [b8]\n2. \"A stochastic parts program and noun phrase parser for unrestricted text\" [b6]\n3. \"Conditional graph neural fields\" [b25]\n4. \"GMNN: Graph Markov Neural Networks\" [b30]\n5. \"An introduction to conditional random fields for relational learning\" [b36]\n6.Tree reweighted belief propagation algorithms and approximate ML estimation by pseudo moment matching\" [b43]\n7. \"Graph neural networks\" [b13]\n8. \"Inductive representation learning on large graphs\" [b14]\n\nThese papers seem to offer the most relevant connections to the paper's primary challenges, the proposed algorithm, and novel methods employed to tackle these challenges.\n\nHere is the returned JSON format:\n\n{\n    \"b8\": 0.9,\n    \"b6\": 0.8,\n    \"b25\": 0.9,\n    \"b30\": 0.9,\n    \"b36\": 0.8,\n    \"b43\": 0.9,\n    \"b13\": 0.8,\n    \"b14\": 0.8\n} \n\nThe confidence score ranges from 0.8 to 0.9, indicating a strong influence of these sources on the paper's main contributions.\n", "Based on the text of the paper, the relevant source papers seem to be:\n\n- b14: 'inductive representation learning on large graphs'\n- b17: 'semi supervised classification with graph convolutional networks'\n- b1: 'predicting structured data'\n- b19: 'conditional random fields probabilistic models for segmenting and labeling sequence data'\n- b25: 'conditional graph neural fields iclr submission'\n- b30: 'gmnn graph markov neural networks'\n- b43: 'tree reweighted belief propagation algorithms and approximate ml estimation by pseudo moment matching'\n- b13: 'neural message passing for quantum chemistry'\n\nThese papers are mentioned in relation to the key challenges and methodologies that the given paper addresses. Specifically, papers b14 and b17 appear to be the main inspiration for the author's approach to node classification in the inductive setting with GNNs, while papers b1 and b19 provide the foundational methods for structured prediction. Papers b25 and b30 are the main precursors for defining potential functions with GNNs in CRFs, and b43 introduces the optimization problem as a proxy that the author adapts for learning CRFs. Lastly, paper b13 is mentioned as the method that brings high capacity to GNNs.\n\nBased on these observations, the JSON object with reference numbers and confidence scores (between 0 and 1) would look like:\n\n{\n\"b14\": 0.9,\n\"b17\": 0.9,\n\"b1\": 0.8,\n\"b19\": 0.8,\n\"b25\": 0.75,\n\"b30\": 0.75,\n\"b43\": 0.7,\n\"b13\": 0.7\n}", "Based on the text, the source papers that significantly inspire the given paper are:\n\n- \"neural structured prediction for inductive node classification\" [b14]: The paper discusses the inductive setting for node classification in the context of Graph Neural Networks (GNNs), which is a key topic in the provided text.\n- \"conditional random fields probabilistic models for segmenting and labeling sequence data\" [b19]: The paper introduces the use of Conditional Random Field (CRF), which is a method employed in the provided text to model the joint distribution of node labels.\n- \"probabilistic graphical models principles and techniques\" [b18]: The paper is cited when discussing the limitations of certain optimization methods.\n- \"gmnn graph markov neural networks\" [b30]: This paper is mentioned in the context of defining potential functions with GNNs. \n- \"an introduction to conditional random fields\" [b38]: The paper discusses the maximin game equivalent to likelihood maximization, which is a method used in the provided text.\n- \"graphical models exponential families and variational inference\" [b42]: This source paper is mentioned when discussing the connection between joint and marginal distributions of CRFs.\n- \"tree reweighted belief propagation algorithms and approximate ml estimation by pseudo moment matching\" [b43]: The paper introduces the concept of pseudo moment matching, which is also used in the provided text.\n\nThe JSON formatted answer:\n\n{\n    \"b14\": 0.9,\n    \"b19\": 0.9,\n    \"b18\": 0.7,\n    \"b30\": 0.8,\n    \"b38\": 0.8,\n    \"b42\": 0.8,\n    \"b43\": 0.7\n}\nThe confidence scores are set based on the papers' relevance to the main contributions of the given paper. The scores are subjective and are set to reflect the papers' influence on the given paper.", "Based on the instructions and given the paper's text and author notes, the most pertinent source papers are likely to be:\n\n- \"\u57fa\u4e8e\u4e0a\u4e00\u7bc7\u6587\u7ae0\u7ed3\u5408CRF\u548cGNN\" (Translation: Based on the previous paper combining CRF and GNN)\n- \"\u4f7f\u7528\u4ee3\u7406\u95ee\u9898\u7b80\u5316CRF\u7684\u4f18\u5316\u65b9\u5f0f\" (Translation: Using proxy problem to simplify the optimization of CRF)\n- \"\u66f4\u6362\u4e86\u4ee3\u4ef7\u51fd\u6570\uff0c\u5e76\u4e14\u80fd\u591f\u5229\u7528\u76f8\u90bb\u8282\u70b9\u6807\u7b7e\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4fbf\u4e8e\u66f4\u597d\u4f20\u64ad\u548c\u5b66\u4e60\" (Translation: Changed the cost function, and can use the relationship between adjacent node labels, which is conducive to better dissemination and learning)\n- \"\u4ee5\u6b64\u89e3\u51b3\u4e86\u5904\u7406maximin\u95ee\u9898\u65f6\u8f83\u96be\u6536\u655b\u7684\u60c5\u51b5\" (Translation: This solves the difficult convergent situation when dealing with maximin problems)\n\nBased on the text of the paper, there are several references that are highlighted as significant sources of inspiration or motivation:\n\n- \"Most existing efforts focus on the transductive setting [b17] [b41]\" (This indicates that [b17] and [b41] are important sources, but they appear in the beginning part of the text, so they may not be the most pertinent source papers.)\n- \"In contrast, our approach uses GNNs to parameterize the potential functions in CRFs, which is in a similar vein to [b25]; [b30]; [b23] 2021); [b44]\" (This indicates that [b25], [b30], [b23], and [b44] are important sources that have inspired this paper's approach.)\n- \"Our approach instead solves a proxy problem, which yields a near-optimal solution to the original problem of maximizing likelihood, and thus gets superior results. For [b23] and [b24]\" (This indicates that [b23] and [b24] are significant sources that this paper's approach improves upon.)\n- \"However, these maximin games are often hard to optimize and the lower bounds are often loose. Different from them, we follow [b43] and build an approximate optimization problem as a proxy, which is easier to solve and yields better results.\" (This indicates that [b43] is an important source that this paper's approach follows.)\n\nBased on the titles of the references, the most relevant papers can be inferred to be:\n\n- [b25]: \"conditional graph neural fields iclr submission\"\n- [b30]: \"gmnn graph markov neural networks\"\n- [b23]: \"a flexible generative framework for graph based semi supervised learning\"\n- [b44]: \"semi supervised node classification on graphs markov random fields vs graph neural networks\"\n- [b43]: \"tree reweighted belief propagation algorithms and approximate ml estimation by pseudo moment matching\"\n\nSo, the final list of source papers is likely to be:\n\n- [b25]\n- [b30]\n- [b23]\n- [b44]\n- [b43]\n\nThe confidence score for these references, given the information in the text and author notes, is:\n\n{\n    \"b25\": 0.9,\n    \"b30\": 0.9,\n    \"b23\": 0.8,\n    \"b44\": 0.8,\n    \"b43\": 1.0\n}", "Based on the provided text and instructions, the source papers that inspired this paper the most appear to be:\n\n1. [b14]: \"Inductive representation learning on large graphs\"\n2. [b19]: \"Conditional random fields probabilistic models for segmenting and labeling sequence data\"\n3. [b25]: \"Conditional graph neural fields iclr submission\"\n4. [b30]: \"GMNN: Graph Markov neural networks\"\n5. [b42]: \"Graphical models exponential families and variational inference\"\n6. [b43]: \"Tree reweighted belief propagation algorithms and approximate ML estimation by pseudo moment matching\"\n\nThe JSON format is as follows:\n\n```\n{\n    \"b14\": 0.9,\n    \"b19\": 0.9,\n    \"b25\": 0.8,\n    \"b30\": 0.8,\n    \"b42\": 0.85,\n    \"b43\": 0.85\n}\n```\n\nThe confidence score for each source paper is estimated based on the frequency and context of their citations in the paper. For instance, [b14] and [b19] are both cited in the introduction and are central to the paper's problem formulation and approach, hence their high confidence scores. [b25] and [b30] are both cited in the context of using GNNs to parameterize the potential functions in CRFs, which is a key motivation for the paper. [b42] and [b43] are both cited in the context of constructing a proxy problem for model learning, which is a significant contribution of the paper."], "634d805690e50fcafd4e07bc": ["Based on the text and the provided notes, the source papers that have inspired the given paper are:\n\n- \"Efficient memory disaggregation with InfiniSwap\" [b3]: The paper references this work when discussing existing approaches to memory disaggregation, particularly page-based approaches.\n- \"Using RDMA efficiently for key-value services\" [b11]: The paper discusses this work when outlining the object-based approach to memory disaggregation.\n- \"Compute express link specification revision\" [b17]: The paper heavily leans on the CXL protocol for its core contributions and discusses this specification in detail. \n- \"Compute express link white paper\" [b18]: This is another important source paper, as it provides the foundational understanding of CXL, which is crucial for the contributions of the given paper.\n- \"Advanced configuration and power interface acpi specification version\" [b26]: The paper discusses the use of ACPI in the context of CXL-enabled systems, suggesting it as a source of inspiration.\n- \"The device tree specification\" [b27]: This paper was referenced when discussing how the CXL enumeration was enabled, suggesting that it was a source of inspiration.\n\nIn JSON format:\n\n```\n{\n    \"b3\": 0.9,\n    \"b11\": 0.8,\n    \"b17\": 1.0,\n    \"b18\": 1.0,\n    \"b26\": 0.7,\n    \"b27\": 0.7\n}\n```", "Based on the text, the following references are the source papers that most inspired the given paper:\n\n- \"Efficient memory disaggregation with infiniswap\" [b3]: The paper discusses the page-based approach of memory disaggregation, which is a crucial concept in the given paper.\n- \"Rethinking software runtimes for disaggregated memory\" [b9]: This reference is related to the object-based approach which is also discussed in the given paper.\n- \"Compute express link specification revision\" [b17]: The given paper advocates the use of compute express link (CXL), making this reference crucial.\n- \"Compute express link or cxl what it is and examples\" [b21]: Similar to [b17], this reference is directly related to the use of CXL in the given paper.\n- \"Aifm high performance application integrated far memory\" [b15]: This paper is related to the object-based approach of memory disaggregation, hence, it is one of the sources that inspire the given paper.\n- \"Direct access high performance memory disaggregation with directcxl\" [None]: As the title suggests, the paper is directly related to the research topic of the given paper.\n\nBased on the above analysis, the result in the required format is:\n\n{\n    \"b3\": 0.9,\n    \"b9\": 0.9,\n    \"b17\": 1.0,\n    \"b21\": 1.0,\n    \"b15\": 0.9,\n    \"None\": 1.0\n}\n\nThe confidence score is set to 0.9 for the references that are somewhat related to the research of the paper, and 1.0 for the references that are directly related to the paper's research topic.", "Based on the text and the author's notes, the paper focuses on the development of DIRECTCXL, a memory disaggregation system that directly connects host processor complex and remote memory resources over CXL's memory protocol (CXL.mem). The paper identifies the latency issues with data movement in existing memory disaggregation runtimes, such as RDMA, and proposes DIRECTCXL as a solution that eliminates data copies between the host memory and remote memory, thus improving performance.\n\nFrom the text, we can infer that the following papers are the main sources of inspiration:\n\n1. \"Efficient memory disaggregation with infiniswap\" [b3]\n2. \"Remote regions a simple abstraction for remote memory\" [b4]\n3. \"Thymesisflow a softwaredefined hw sw co designed interconnect stack for rackscale memory disaggregation\" [b6]\n4. \"Mind in network memory management for disaggregated data centers\" [b7]\n5. \"Clio a hardware software codesigned disaggregated memory system\" [b8]\n6. \"Rethinking software runtimes for disaggregated memory\" [b9]\n7. \"Using rdma efficiently for key value services\" [b11]\n8. \"Compute express link specification revision\" [b17]\n\nThe confidence scores are difficult to determine without more explicit connections in the text, but it can be inferred that the papers [b3] and [b17] have the highest influence based on their relevance to the main contributions of the paper.\n\nThe results in JSON format would look like this:\n\n```json\n{\n    \"b3\": 0.9,\n    \"b4\": 0.7,\n    \"b6\": 0.8,\n    \"b7\": 0.7,\n    \"b8\": 0.8,\n    \"b9\": 0.7,\n    \"b11\": 0.8,\n    \"b17\": 0.9\n}\n```\n\nThe confidence scores are normalized between 0 and 1, where 1 indicates the highest confidence. The scores above are estimates based on the paper's text and may not be completely accurate.", "Based on the text and author notes, the primary challenges outlined in the paper revolve around memory disaggregation and the drawbacks of existing approaches like RDMA. The authors propose a novel solution called DIRECTCXL that directly connects host processor complex and remote memory resources over CXL's memory protocol (CXL.mem).\n\nFrom the text, the key source papers that inspired this work include:\n\n1. b3: \"Efficient memory disaggregation with infiniswap\". This is the key paper that discusses the page-based approach to memory disaggregation, which the authors of the current paper categorize as one of the existing approaches.\n\n2. b11: \"Using rdma efficiently for key-value services\". This is the key paper that discusses the object-based approach to memory disaggregation, which the authors of the current paper categorize as the other approach of the existing methods.\n\n3. b17: \"Compute express link specification revision\". This paper introduces the CXL, which is the main technology the authors used for their novel solution.\n\n4. b0: \"Can far memory improve job throughput\". This paper discusses the potential of memory disaggregation, which is the core topic of the current paper.\n\n5. b21: \"Compute express link or cxl what it is and examples\". This paper provides examples of CXL, which could have guided the authors in implementing their CXL-based solution.\n\n6. b10: \"Disaggregating persistent memory and controlling them remotely an exploration of passive disaggregated keyvalue stores\". This paper explores a similar topic of memory disaggregation, particularly focusing on persistent memory.\n\nThe confidence score for these references is as follows:\n\n{\"b3\": 0.95, \"b11\": 0.95, \"b17\": 1, \"b0\": 0.9, \"b21\": 0.9, \"b10\": 0.85}.\n\nThese scores reflect the level of relevance each source paper has to the current paper's main contributions and challenges.", "Based on the text and the notes provided, the paper focuses on memory disaggregation and introduces a new system called DIRECTCXL, which reduces latency by directly connecting the host and memory pool using CXL-mem. It claims to be the first to bring CXL 2.0 into a real system and shows a significant reduction in latency compared to previous RDMA-based methods. \n\nBased on this, the following references seem to be the most influential source papers:\n\n1. [b0]: \"Can Far Memory Improve Job Throughput?\" - This could be a source paper because it provides a background to memory disaggregation, which seems to be the main topic of the paper. \n2. [b17]: \"Compute Express Link Specification Revision\" - This paper discusses CXL, the technology that the authors have used to connect the host and memory pool, making it a significant source paper. \n3. [b23]: \"Minimizing the Hidden Cost of RDMA\" - This reference is important as it discusses RDMA, the previous methodology the authors are improving upon.\n4. [b11]: \"Using RDMA Efficiently for Key-Value Services\" - This paper is also focused on RDMA, which is the technology that the authors are attempting to replace or improve upon with DIRECTCXL.\n5. [b4]: \"Remote Regions: A Simple Abstraction for Remote Memory\" - This paper seems to discuss a similar topic of memory disaggregation, making it a potential source paper.\n\nThe confidence scores are subjective and it's difficult to give a precise score without reading the full papers. However, based on the information given, here is a possible assignment of confidence scores:\n\n{\"b0\": 0.8, \"b17\": 1.0, \"b23\": 0.9, \"b11\": 0.9, \"b4\": 0.7}\n\nThis means that the paper \"Compute Express Link Specification Revision\" ([b17]) is very likely to be a source paper, whereas \"Remote Regions: A Simple Abstraction for Remote Memory\" ([b4]) is somewhat likely to be a source paper."], "621ee1845aee126c0f26a9df": ["{\n    \"b10\": 0.95,\n    \"b11\": 0.95,\n    \"b12\": 0.95,\n    \"b36\": 0.90,\n    \"b37\": 0.90,\n    \"b38\": 0.90,\n    \"b39\": 0.90,\n    \"b40\": 0.90\n}", "{\n    \"b36\": 0.9,\n    \"b37\": 0.8,\n    \"b38\": 0.8,\n    \"b39\": 0.8,\n    \"b40\": 0.8,\n    \"b41\": 0.8,\n    \"b42\": 0.8\n}", "```json\n{\n    \"b10\": 0.9,\n    \"b11\": 0.9,\n    \"b12\": 0.9,\n    \"b36\": 0.95,\n    \"b37\": 0.85,\n    \"b38\": 0.85,\n    \"b39\": 0.85,\n    \"b40\": 0.85\n}\n```", "{\n  \"b10\": 0.9,\n  \"b11\": 0.9,\n  \"b12\": 0.9,\n  \"b36\": 0.85,\n  \"b37\": 0.8,\n  \"b38\": 0.8,\n  \"b39\": 0.8,\n  \"b40\": 0.85\n}", "{\n    \"b10\": 0.9,\n    \"b11\": 0.9,\n    \"b12\": 0.9,\n    \"b36\": 0.8,\n    \"b37\": 0.8,\n    \"b38\": 0.8,\n    \"b39\": 0.8,\n    \"b40\": 0.8\n}"], "63438d2990e50fcafd4ebd66": ["{\n\"b27\": 0.9,\n\"b18\": 0.8,\n\"b43\": 0.7,\n\"b33\": 0.6,\n\"b8\": 0.5,\n\"b35\": 0.4,\n\"b47\": 0.3,\n\"b63\": 0.2\n}", "Based on the provided text, the author's main challenges are: mapping DNN models on NPUs, comparing different mapper algorithms, and analyzing the impact of sparsity on mapper performance. The novel methods introduced to tackle these challenges include quantitatively comparing three categories of mappers and proposing two heuristic techniques: warm-start technique and sparsity-aware technique.\n\nThe references that seem to be the most relevant to these novel contributions and challenges are:\n\n1. 'b18': 'mind mappings enabling efficient algorithm accelerator mapping space search'\n2. 'b27': 'gamma automating the hw mapping of dnn models on accelerators via genetic algorithm'\n3. 'b43': 'timeloop a systematic approach to dnn accelerator evaluation'\n4. 'b33': 'understanding reuse performance and hardware cost of dnn dataflow a data centric approach'\n5. 'b50': 'poor man s bert smaller and faster transformer models'\n6. 'b34': 'maestro a data centric approach to understand reuse performance and hardware cost of dnn mappings'\n7. 'b79': 'to prune or not to prune exploring the efficacy of pruning for model compression'\n\nThe final result in JSON format would be:\n\n```\n{\n    \"b18\": 0.9,\n    \"b27\": 0.85,\n    \"b43\": 0.9,\n    \"b33\": 0.8,\n    \"b50\": 0.8,\n    \"b34\": 0.85,\n    \"b79\": 0.8\n}\n```\n\nThe confidence scores were determined based on the relevance of the reference title to the challenges and novel contributions of the paper.", "Based on the text provided, the author's work is about comparing different mapping algorithms for DNN models on NPUs. The paper's novel contributions include a quantitative comparison of three mapping categories and the introduction of two heuristic techniques to advance the state-of-the-art in map space exploration.\n\nFrom the text, the following papers are identified as main sources of inspiration based on key phrases and the fact that they are directly related to the paper's core contributions:\n\n1. [b43]: \"Timeloop a systematic approach to dnn accelerator evaluation\" - The authors mention this paper multiple times, and their research is based on Timeloop's cost model. This paper seems to have heavily influenced the authors' approach to their research. \n\n2. [b18]: \"Mind mappings enabling efficient algorithm accelerator mapping space search\" - This paper is mentioned when the authors discuss gradient-based search methods. They later use this method in their own research. \n\n3. [b27]: \"Gamma automating the hw mapping of dnn models on accelerators via genetic algorithm\" - This paper is referenced when the authors discuss feedback-based search methods, which they later use in their research.\n\n4. [b34]: \"Maestro a data centric approach to understand reuse performance and hardware cost of dnn mappings\" - This paper is mentioned when the authors discuss the mapping strategies and their importance. It seems to have influenced the authors' understanding of mapping strategies. \n\n5. [b8]: \"Eyeriss v a flexible accelerator for emerging deep neural networks on mobile devices\" - This paper is mentioned when the authors discuss the state-of-the-art DNN accelerators, which is a key topic in their paper. \n\n6. [b35]: \"Maeri enabling flexible dataflow mapping over dnn accelerators via reconfigurable interconnects\" - This paper is mentioned when discussing the flexibility of DNN accelerators, which is a key aspect of the authors' research.\n\nSo, the source papers can be described in the json format as follows:\n{'b43': 0.9, 'b18': 0.85, 'b27': 0.85, 'b34': 0.8, 'b8': 0.8, 'b35': 0.8}", "Based on the provided text and titles of references, the following papers may have been the most influential to this paper:\n\n1. \"demystifying map space exploration for npus\": The paper's main contribution is about exploring the map space for NPUs, which is the main topic of this paper, so this paper should be a key reference.\n2. \"mind mappings enabling efficient algorithm accelerator mapping space search\": The paper's method is about mapping space search which is a direct match with the problem tackled in the paper.\n3. \"maestro a data centric approach to understand reuse performance and hardware cost of dnn mappings\": The paper uses a data centric approach to understand mappings, which may have inspired the paper's method.\n4. \"gamma automating the hw mapping of dnn models on accelerators via genetic algorithm\": This paper provides a genetic algorithm for mapping DNN models, which is similar to the problem the paper is dealing with.\n5. \"timeloop a systematic approach to dnn accelerator evaluation\": The paper uses 'Timeloop' for evaluations, which is directly utilized in this paper.\n\nTherefore, the json format of the results would be:\n\n{\n\"None\": 0.8,\n\"b18\": 0.9,\n\"b34\": 0.9,\n\"b27\": 0.7,\n\"b43\": 0.8\n}\n\nNote: The confidence scores are subjective and just for demonstration. Further analysis may be needed to fine-tune these scores.", "Based on the provided text, the following references seem to be the most relevant source papers that have inspired the given paper:\n\n- [b43]: \"Timeloop: a systematic approach to DNN accelerator evaluation\"\n- [b18]: \"Mind mappings: enabling efficient algorithm accelerator mapping space search\"\n- [b27]: \"Gamma: automating the HW mapping of DNN models on accelerators via genetic algorithm\"\n- [b8]: \"Eyeriss v: a flexible accelerator for emerging deep neural networks on mobile devices\"\n- [b35]: \"MAERI: Enabling flexible dataflow mapping over DNN accelerators via reconfigurable interconnects\"\n- [b2]: \"Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code\"\n\nThese papers have been selected based on the provided text's clear mention of their relevance to the main challenges and methods outlined in the given paper. They are all closely related to the topic of mapping DNN models on accelerators, which is the main focus of the paper.\n\nThe confidence score for each reference is as follows:\n\n{\"b43\": 0.9, \"b18\": 0.85, \"b27\": 0.85, \"b8\": 0.8, \"b35\": 0.8, \"b2\": 0.75}\n"], "63a413f790e50fcafd6d1f93": ["Based on the given paper text and the titles of the references, the most pertinent source papers which inspired the authors are:\n\n1. \"nodeformer a scalable graph structure learning transformer for node classification\": This is the paper we are analyzing, so it does not count as a source.\n2. \"learning discrete structures for graph neural networks [b10]\": This paper is mentioned multiple times and appears to be a significant source of inspiration for the paper's methodology.\n3. \"mixhop higher order graph convolutional architectures via sparsified neighborhood mixing [b0]\": Mentioned in the context of adjusting the input topology as relational bias, and likely informed the development of NODEFORMER's graph structure learning method.\n4. \"categorical reparameterization with gumbel softmax [b15]\": The reparametrization trick mentioned in the paper is clearly inspired by this work.\n5. \"random features for large scale kernel machines [b25]\": This paper's method is used in the kernelized Gumbel-Softmax operator in NODEFORMER.\n6. \"graph attention networks [b35]\": Mentioned as an early attempt that inspired the development of NODEFORMER.\n7. \"graphsaint graph sampling based inductive learning method [b46]\": This paper's method is compared with NODEFORMER's in terms of scalability.\n\nThe JSON format of the result is:\n\n{\n  \"b10\": 0.9,\n  \"b0\": 0.8,\n  \"b15\": 0.8,\n  \"b25\": 0.8,\n  \"b35\": 0.7,\n  \"b46\": 0.7\n}\n\nThe confidence scores are assigned based on the frequency and context of the references in the paper text.", "Based on the given text, the most pertinent source papers for the provided academic paper seem to be:\n\n1. \"Learning discrete structures for graph neural networks\" [b10]: The new method proposed by the author is inspired by this paper. The author's method also incorporates learning optimal graph topology with message passing - a significant challenge outlined in [b10]. The confidence score would be 1.\n\n2. \"A generalization of transformer networks to graphs\" [b8]: The author's proposed model, NODEFORMER, is the first Transformer model that scales all-pair message passing to large node classification graphs. This indicates that [b8] is a significant source of inspiration, thus giving it a confidence score of 1.\n\n3. \"Categorical reparameterization with Gumbel softmax\" [b15]: The author's method uses a reparametrization trick [b15] to allow differentiable learning. This shows that [b15] has a significant influence on the proposed method. The confidence score would be 0.9.\n\n4. \"Random features for large scale kernel machines\" [b25]: The author uses random features [b25] to approximate the kernel function in their proposed method. This shows that [b25] has a significant influence on the method proposed by the author. The confidence score would be 0.9.\n\n5. \"Graph attention networks\" [b35]: The author mentions Graph Attention Networks (GAT) [b35] as an early work, which has influenced the development of their method. The confidence score would be 0.8.\n\n6. \"Implicit graph neural networks\" [b12]: The author's method addresses long-range dependence with shallow layers, which is a challenge outlined in [b12]. The confidence score would be 0.8.\n\nThe JSON output would be:\n{\n    \"b10\": 1,\n    \"b8\": 1,\n    \"b15\": 0.9,\n    \"b25\": 0.9,\n    \"b35\": 0.8,\n    \"b12\": 0.8\n}", "Based on the provided text and titles of the references, the source papers of the given paper are likely as follows:\n\n1. \"Kernelized Gumbel-Softmax operator\": This term is used multiple times throughout the text, and the paper by '[b5]' is titled 'Rethinking Attention with Performers', which may discuss the use of such an operator.\n\n2. \"NODEFORMER\": The paper is titled 'nodeformer a scalable graph structure learning transformer for node classification', and several references discuss similar topics, such as '[b10]' with the title 'Learning Discrete Structures for Graph Neural Networks' and '[b3]' with the title 'Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings'.\n\n3. \"transformer model\": The paper by '[b34]' is titled 'Advances in Neural Information Processing Systems' which may discuss transformer models, a key concept in this paper.\n\n4. \"graph structure learning\": The paper by '[b17]' is titled 'Graph Structure Learning for Robust Graph Neural Networks', which seems to be directly relevant to the novel contributions of the paper.\n\n5. \"Message-passing scheme\": The paper by '[b46]' is titled 'GraphSAINT: Graph Sampling Based Inductive Learning Method' and could be a source paper given the frequent discussions of message passing in the text.\n\n6. \"Relational Bias\": The paper by '[b2]' is titled 'Geometric Deep Learning: Going Beyond Euclidean Data' which could introduce the concept of relational bias used in this paper.\n\nPlease note that these are educated guesses based on the provided text and titles of the references, and the actual source papers might be different. \nThe json format of the result is:\n{\n    \"reference number\": \"confidence score\",\n    \"b5\": 0.9,\n    \"b10\": 0.8,\n    \"b3\": 0.8,\n    \"b34\": 0.7,\n    \"b17\": 0.7,\n    \"b46\": 0.7,\n    \"b2\": 0.7\n}", "Based on the paper text and related titles, the most relevant source papers appear to be:\n\nReference Number: 'b10', Confidence Score: 0.9 - \"Learning discrete structures for graph neural networks\"\nThis paper appears to be a key inspiration for the NODEFORMER model, particularly regarding the use of discrete structures and graph structure learning.\n\nReference Number: 'b34', Confidence Score: 0.85 - \"Advances in neural information processing systems\"\nThe paper mentions the \"spirit of Transformers\" which suggests some inspiration from the Transformer model, a key development in neural information processing systems.\n\nReference Number: 'b35', Confidence Score: 0.8 - \"Graph attention networks\"\nThe paper mentions Graph Attention Networks (GAT) as an early attempt in the field, indicating some influence from this work.\n\nReference Number: 'b5', Confidence Score: 0.8 - \"Rethinking attention with performers\"\nThe use of a kernelized Gumbel-Softmax operator in the NODEFORMER model may be influenced by this paper, which rethinks attention mechanisms.\n\nReference Number: 'b15', Confidence Score: 0.75 - \"Categorical reparameterization with gumbel softmax\"\nThe use of the Gumbel-Softmax operator in the NODEFORMER model suggests direct influence from this paper.\n\nReference Number: 'b3', Confidence Score: 0.7 - \"Iterative deep graph learning for graph neural networks better and robust node embeddings\"\nThis paper appears to be influential in the development of the NODEFORMER model, particularly regarding the learning of graph structures and node embeddings.\n\nReference Number: 'b38', Confidence Score: 0.7 - \"Simplifying graph convolutional networks\"\nThis paper is likely a source of inspiration, particularly with respect to the scalability of the NODEFORMER model.\n\nThe JSON format of the answer is:\n\n{\"b10\": 0.9, \"b34\": 0.85, \"b35\": 0.8, \"b5\": 0.8, \"b15\": 0.75, \"b3\": 0.7, \"b38\": 0.7}", "Based on the given text of the academic paper and the titles of the references, the most relevant source papers that heavily influenced the main contributions of the paper are:\n\n- \"b10: Learning discrete structures for graph neural networks\" \n- \"b46: Graphsaint: Graph sampling based inductive learning method\"\n- \"b5: Rethinking attention with performers\"\n- \"b15: Categorical reparameterization with gumbel softmax\"\n- \"b35: Graph attention networks\"\n- \"b18: Semi-supervised classification with graph convolutional networks\"\n- \"b38: Simplifying graph convolutional networks\"\n\nThe JSON format return would be:\n```\n{\n    \"b10\": 0.9,\n    \"b46\": 0.85,\n    \"b5\": 0.8,\n    \"b15\": 0.8,\n    \"b35\": 0.75,\n    \"b18\": 0.75,\n    \"b38\": 0.7\n}\n```\nThese confidence scores represent how closely each paper's contributions align with the primary challenges and methods outlined in the given academic paper. They are inferred based on the given paper's content and does not represent a definitive quantitative measure."], "632bd2a990e50fcafdb7a24d": ["Based on the provided text and the notes, the paper primarily discusses the challenges faced by existing methods for knowledge-intensive tasks and proposes a new approach - Generate-then-Read (GENREAD) pipeline which uses a large language model to generate contextual documents and then predict the final answer. The paper refers to various source papers that influenced the authors' work. Here are the key source papers and their respective confidence scores:\n\n{\n\"b21\": 0.9, \n\"b17\": 0.8, \n\"b28\": 0.8, \n\"b31\": 0.7, \n\"b3\": 0.7, \n\"b6\": 0.7, \n\"b15\": 0.6\n}\n\nThe paper extensively refers to [b21] and [b17] in discussing the limitations of existing approaches for knowledge-intensive tasks, so these are highly influential sources. [b28] is referred to when discussing the limitations of document retrieval over a large corpus. [b31] and [b3] are referenced in discussing the potential of large language models. [b6] is mentioned in the context of large language models' impressive generative capabilities. Lastly, [b15] is referred to when discussing nucleus sampling, a method to increase the diversity of generated documents.", "Based on the text of the paper, the author's notes, and the titles of each paper, the most pertinent source papers that inspired this paper are:\n\n1. 'generate rather than retrieve large langu age models are strong context generators' [None] - The paper seems to be strongly influenced by this paper as it also proposes to generate rather than retrieve in the context of large language models. Confidence Score: 0.85\n\n2. 'autoregressive entity retrieval' [b7] - The paper discusses using large models to generate identifiers for documents, which is a concept used in this paper as well. Confidence Score: 0.8\n\n3. 'autoregressive search engines generating substrings as document identifiers' [b2] - As with [b7], this paper also deals with generating identifiers, which is a key concept in the provided paper. Confidence Score: 0.8\n\n4. 'language models are few shot learners' [b3] - This paper is likely a source as it discusses the capabilities of large language models, which is a central topic in the provided paper. Confidence Score: 0.75\n\n5. 'leveraging knowledge in multilingual commonsense reasoning' [b11] - This paper is likely a source as it discusses eliciting knowledge from large language models, which is a central topic in the provided paper. Confidence Score: 0.7\n\n6. 'generated knowledge prompting for commonsense reasoning' [b31] - This paper is likely a source as it discusses generating contexts and providing them as additional input, a technique used in this paper as well. Confidence Score: 0.7\n\n7. 'the curious case of neural text degeneration' [b15] - This paper discusses nucleus sampling, a technique used in this paper to increase the diversity of generated documents. Confidence Score: 0.65\n\n8. 'kilt a benchmark for knowledge intensive language tasks' [b37] - This paper could be a source as it describes knowledge-intensive tasks, which is what the paper at hand is dealing with. Confidence Score: 0.6\n\nBased on the evaluation of the text, these are the most relevant source papers that inspired this paper. The confidence scores are given based on the direct relevance of the source paper to the novel contributions of the paper at hand. \n\n{\n\"None\": 0.85,\n\"b7\": 0.8,\n\"b2\": 0.8,\n\"b3\": 0.75,\n\"b11\": 0.7,\n\"b31\": 0.7,\n\"b15\": 0.65,\n\"b37\": 0.6\n}", "Based on the text and the author's note, the following references seem to be sources that have inspired the main contributions of this paper:\n\n1. \"generate rather than retrieve large langu age models are strong context generators\"\n2. \"autoregressive search engines generating substrings as document identifiers\"\n3. \"leveraging passage retrieval with generative models for open domain question answering\"\n4. \"nucleus sampling\"\n5. \"kilt a benchmark for knowledge intensive language tasks\"\n6. \"language models are few shot learners\"\n7. \"chain of thought prompting elicits reasoning in large language models\"\n8. \"retrieval augmented generation for knowledge intensive nlp tasks\"\n\nNow, let's convert these into the required JSON format:\n\n{\n\"b0\": 0.8,\n\"b2\": 0.9,\n\"b17\": 0.95,\n\"b15\": 0.85,\n\"b37\": 0.9,\n\"b3\": 0.95,\n\"b49\": 0.9,\n\"b29\": 0.85\n}\n\nThis is a preliminary analysis and the confidence scores are assumed. Further in-depth analysis of the paper may result in different scores or even different source papers.", "Based on the text of the paper and the given titles of the references, the most pertinent source papers can be identified as:\n\n1. 'b21': 'Dense passage retrieval for open domain question answering' - The author refers to this paper multiple times in the text, notably when discussing the drawbacks of existing methods and when comparing their method to existing dense retrieval approaches. This indicates that this paper heavily inspired the author's work. Confidence score: 0.9\n\n2. 'b17': 'Leveraging passage retrieval with generative models for open domain question answering' - This paper is referenced in the context of current state-of-the-art models and later when discussing the supervised setting. It thus seems to have strongly influenced the author's work. Confidence score: 0.8\n\n3. 'b6': 'Scaling language modeling with pathways' - The author refers to this paper when discussing the breakthrough of large language models in natural language research, indicating that it served as a source of inspiration. Confidence score: 0.7\n\n4. 'b31': 'Generated knowledge prompting for commonsense reasoning' - The author cites this paper when discussing how relevant knowledge can be elicited from large language models, which is a key concept in their work. Confidence score: 0.7\n\n5. 'b2': 'Autoregressive search engines generating substrings as document identifiers' - This paper is referenced when discussing the drawbacks of using auto-regressive language models to generate identifier strings for documents, which has inspired the author to propose a different approach. Confidence score: 0.6\n\n6. 'b15': 'The curious case of neural text degeneration' - This paper is mentioned when the author discusses nucleus sampling, a method they use in their work. Confidence score: 0.6\n\n7. 'b45': 'Transformer memory as a differentiable search index' - This paper is referenced in the context of existing work that has used auto-regressive language models for retrievals, which is a technique the author adopts in their work. Confidence score: 0.5\n\n8. 'b3': 'Language models are few shot learners' - This paper is mentioned multiple times in the context of large language models, indicating that it played a role in shaping the author's work on language models. Confidence score: 0.5\n\nBased on the given information, the JSON format of the result is:\n\n{\n\"b21\": 0.9,\n\"b17\": 0.8,\n\"b6\": 0.7,\n\"b31\": 0.7,\n\"b2\": 0.6,\n\"b15\": 0.6,\n\"b45\": 0.5,\n\"b3\": 0.5\n}", "Given the detailed analysis from the paper and its references, the most pertinent source papers that inspired the authors are:\n\n- 'b3': 'language models are few shot learners' - It is mentioned numerous times in the paper, especially in discussing the performance and generative capabilities of large language models. Confidence Score: 0.9\n- 'b21': 'dense passage retrieval for open domain question answering' - This reference is used to explain the traditional approaches and their shortcomings, suggesting that this paper has inspired the authors to improve upon these methods. Confidence Score: 0.8\n- 'b17': 'leveraging passage retrieval with generative models for open domain question answering'. - This paper is also mentioned multiple times, particularly in the context of state-of-the-art models for generating answers. Confidence Score: 0.8\n- 'b31': 'generated knowledge prompting for commonsense reasoning' - This paper is cited in the context of previous work that has shown that relevant knowledge can be elicited from large language models. Confidence Score: 0.7\n- 'b2': 'autoregressive search engines generating substrings as document identifiers' - The paper looks upon it to discuss the drawbacks of previous works that use generation for query expansion in traditional search engines. Confidence Score: 0.7\n- 'b15': 'the curious case of neural text degeneration' - It's used when discussing nucleus sampling as a method to increase the diversity of generated documents. Confidence Score: 0.7\n- 'b45': 'transformer memory as a differentiable search index' - This paper is referenced when discussing the use of auto-regressive language models to generate identifier strings for documents. Confidence Score: 0.6\n- 'b7': 'autoregressive entity retrieval' - This paper is mentioned in the context of the drawbacks of the use of auto-regressive language models to generate identifier strings for documents. Confidence Score: 0.6\n\nJSON Format:\n{\"b3\": 0.9, \"b21\": 0.8, \"b17\": 0.8, \"b31\": 0.7, \"b2\": 0.7, \"b15\": 0.7, \"b45\": 0.6, \"b7\": 0.6}"], "62fa0d1390e50fcafd246228": ["Based on the text of the paper and the titles of the references, the source papers that most likely inspired this research are:\n\n1. \"Representation learning for attributed multiplex heterogeneous network\" [b0]\n2. \"Fast and accurate network embeddings via very sparse random projection\" [b1]\n3. \"Metapath vec scalable representation learning for heterogeneous networks\" [b3]\n4. \"Magnn metapath aggregated graph neural network for heterogeneous graph embedding\" [b4]\n5. \"Heterogeneous graph propagation network\" [b13]\n6. \"Fast attributed multiplex heterogeneous network embedding\" [b19]\n7. \"Unsupervised attributed multiplex network embedding\" [b22]\n\nThese papers align with the challenges and methods outlined in the paper. The paper primarily deals with the challenges of heterogeneity and multiplexity in network representation learning, and these source papers all discuss methods and algorithms for handling these challenges in various ways. These references also appear in the part of the text that discusses the methods and approaches used to tackle these challenges, indicating a strong reliance on their research.\n\nIn JSON format:\n\n{\n\"b0\": 0.9,\n\"b1\": 0.8,\n\"b3\": 0.85,\n\"b4\": 0.85,\n\"b13\": 0.8,\n\"b19\": 0.9,\n\"b22\": 0.8\n}", "Based on the text of the academic paper and the provided titles of the references, the primary challenges outlined in the paper revolve around capturing the complex, multi-path relationships between various types of nodes in a network, and integrating multi-relationship structural signals and semantic attributes into node embedding learning. The author's proposed solution to these challenges is the Multiplex Heterogeneous Graph Convolutional Network (MHGCN). Key phrases indicating direct inspiration or motivation for this work are found in references to existing heterogeneous network embedding methods, and the limitations of these methods in handling complex real-world networks. The most pertinent source papers for this work are likely those that the author has critically engaged with in the development of the MHGCN. \n\nBased on these criteria, the following references seem to be the most relevant:\n\n- 'b0': 'representation learning for attributed multiplex heterogeneous network' (Confidence Score: 0.9) - This paper is directly related to the topic of the paper and its title suggests a similar focus on representation learning for heterogeneous networks. It's likely this paper provided some inspiration or basis for the author's work.\n\n- 'b4': 'magnn metapath aggregated graph neural network for heterogeneous graph embedding' (Confidence Score: 0.85) - This paper seems to discuss a similar method to the one proposed by the author, which suggests it might have provided a starting point or reference for their work.\n\n- 'b19': 'fast attributed multiplex heterogeneous network embedding' (Confidence Score: 0.85) - The author directly cites this paper when discussing the process of multiplex relation aggregation in their proposed model, suggesting it was a key reference in the development of the MHGCN.\n\n- 'b39': 'heterogeneous graph neural network' (Confidence Score: 0.8) - The author refers to this paper in the context of existing models for heterogeneous graph learning, indicating it was likely a significant source of inspiration for their work.\n\n- 'b22': 'unsupervised attributed multiplex network embedding' (Confidence Score: 0.8) - This paper appears to address a similar problem to the one tackled by the author, and its mention in the context of discussing limitations of existing methods suggests it was a key reference for this work.\n\nThe proposed JSON format result is:\n{\n   \"b0\": 0.9,\n   \"b4\": 0.85,\n   \"b19\": 0.85,\n   \"b39\": 0.8,\n   \"b22\": 0.8\n}", "Based on the given text and the titles of the references, the source papers of this academic paper are likely:\n\n1. \"Multiplex Heterogeneous Graph Convolutional Network\": The title of the paper itself indicates that it is based on this paper. Confidence score: 1.0\n\n2. \"Representation Learning for Attributed Multiplex Heterogeneous Network [b0]\": The paper directly addresses challenges related to representation learning in multiplex heterogeneous networks, which is the core topic of the given paper. Confidence score: 0.9\n\n3. \"MAGNN: Metapath Aggregated Graph Neural Network for Heterogeneous Graph Embedding [b4]\": The given paper mentions the use of a multilayer graph convolution module to automatically capture meta-path information, which is a method used in this paper. Confidence score: 0.8\n\n4. \"Heterogeneous Graph Transformer [b10]\": The given paper also mentions the use of self-attention to model dynamic dependencies among heterogeneous nodes, which aligns with the methodology used in this paper. Confidence score: 0.8\n\n5. \"Fast Attributed Multiplex Heterogeneous Network Embedding [b19]\": The given paper addresses similar challenges and uses a similar approach to this paper. Confidence score: 0.8\n\n6. \"Heterogeneous Graph Attention Network [b29]\": The given paper mentions the use of graph neural models to tackle the challenge of heterogeneous graph learning which is the focus of this paper. Confidence score: 0.8\n\nThe returned result in the JSON format is:\n{\n\"None\": 1.0,\n\"b0\": 0.9,\n\"b4\": 0.8,\n\"b10\": 0.8,\n\"b19\": 0.8,\n\"b29\": 0.8\n}", "Based on the given text, the author mentions several papers as inspirations or comparisons for their work. Here are the main references that the author referred to:\n\n- 'b0': 'representation learning for attributed multiplex heterogeneous network'\n- 'b4': 'magnn metapath aggregated graph neural network for heterogeneous graph embedding'\n- 'b13': 'heterogeneous graph propagation network'\n- 'b19': 'fast attributed multiplex heterogeneous network embedding'\n- 'b36': 'multiplex bipartite network embedding using dual hypergraph convolutional networks'\n\nThe JSON object for this answer is:\n\n{\n\"b0\": 0.9,\n\"b4\": 0.8,\n\"b13\": 0.7,\n\"b19\": 0.8,\n\"b36\": 0.7\n} \n\nThe confidence score for each reference is estimated based on the frequency of their mention and the context in which they are mentioned. Higher scores indicate higher relevance to the paper's contributions.", "Based on the text and the challenges outlined in the paper, the primary source papers that inspired the author's work and algorithm seem to be:\n\n- 'Representation learning for attributed multiplex heterogeneous network' [b0]: The author's proposed algorithm tackles the issue of representation learning on attributed multiplex heterogeneous networks (AMHENs), which appears to take inspiration from the methodology outlined in this source paper.\n\n- 'Metapath vec scalable representation learning for heterogeneous networks' [b3]: The paper mentions the use of meta-paths for capturing complex dependencies in multiplex heterogeneous graphs, a concept that is central to the 'metapath2vec' approach.\n\n- 'Magnn metapath aggregated graph neural network for heterogeneous graph embedding' [b4]: This source paper seems to have inspired the author's approach of aggregating metapath-based dependencies over the multiplex heterogeneous graphs.\n\n- 'Heterogeneous graph transformer' [b10]: The author's work appears to be influenced by the 'HGT' approach of incorporating self-attention in graph-based message passing mechanism for modeling dynamic dependencies among heterogeneous nodes.\n\n- 'Fast attributed multiplex heterogeneous network embedding' [b19]: This paper inspires the author's approach in terms of its spectral graph transformation component to aggregate information from sub-networks by preserving relation-aware node dependencies.\n\nThe following JSON object represents the source papers and their confidence scores:\n\n```\n{\n    \"b0\": 0.9,\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b10\": 0.7,\n    \"b19\": 0.8\n}\n```\n\nThe confidence scores are estimated based on the relevance of the source papers to the author's methodology and the frequency of their citation in the text."], "634d805490e50fcafd4e02c8": ["Based on the text, the following papers seem to have inspired the authors the most:\n\n1. [b5] 'graph learning based arithmetic block identification': The authors of the text follow the experimental setting of this paper for their own experiments. It is also used as a comparison point in the paper's results, indicating its relevance to the paper's contributions. Confidence Score: 0.85\n\n2. [b4] 'deep learning based circuit recognition using sparse mapping and level dependent decaying sum circuit representations': This paper is used as a baseline in the authors' experiments and is cited in the introduction, indicating its relevance to the paper's topic. Confidence Score: 0.7\n\n3. [b13] 'd vae a variational autoencoder for directed acyclic graphs': The authors discuss the limitations of this paper's methods in their own paper and propose an alternative solution, indicating that this paper influenced their work. Confidence Score: 0.7\n\n4. [b15] 'a simple framework for contrastive learning of visual representations': This paper is cited in the section discussing contrastive learning, which is a key component of the authors' proposed algorithm. Confidence Score: 0.7\n\n5. [b23] 'improved deep metric learning with multi class n pair loss objective': This paper is cited in the section discussing the loss function used in the authors' proposed algorithm, indicating its influence on their work. Confidence Score: 0.65\n\n6. [b7] 'inductive representation learning on large graphs': This paper is used as a baseline in the authors' experiments and is cited in the introduction, indicating its relevance to the paper's topic. Confidence Score: 0.65\n\n7. [b14] 'learning representations by maximizing mutual information across views': This paper is cited in the section discussing contrastive learning, which is a key component of the authors' proposed algorithm. Confidence Score: 0.65\n\nReturn in the json format: { \"b5\": 0.85, \"b4\": 0.7, \"b13\": 0.7, \"b15\": 0.7, \"b23\": 0.65, \"b7\": 0.65, \"b14\": 0.65 }", "Based on the text of the paper and the author notes, the most pertinent source papers that seem to have inspired this research are:\n\n1. \"Graph learning based arithmetic block identification\" [b5]: This paper is directly mentioned multiple times in the paper. The authors of the given paper adopt and improve upon the methods outlined in this work, making it a key source of inspiration.\n\n2. \"Deep graph infomax\" [b17]: This paper is mentioned in the context of embedding high-order global contextual features into node representations, which seems to be a method adopted by the authors of the given paper.\n\n3. \"Graph contrastive learning with adaptive augmentation\" [b19]: The concept of contrastive learning adopted in the paper is motivated by the methods proposed in this paper.\n\n4. \"A simple framework for contrastive learning of visual representations\" [b15]: It's indicated that the authors have adopted the successful contrastive learning approach from the computer vision domain to their own work.\n\n5. \"Deep learning based circuit recognition using sparse mapping and level dependent decaying sum circuit representations\" [b4]: This paper is mentioned multiple times as a source of CNN-based methods that the authors of the given paper are improving upon.\n\n6. \"Directed acyclic graph neural networks\" [b12]: The authors of the given paper mention this work as a source of an impressive GNN architecture that they have adopted and customized for their own work.\n\n7. \"High performance graph convolutional networks with applications in testability analysis\" [b0]: This paper is mentioned in the context of proposing an iterative process to insert observation points into gate-level netlists based on the node representations learned by a graph neural network, which is a method that seems to be employed in the given paper.\n\nReturning the result in the requested JSON format:\n\n{\n  \"b5\": 1.0,\n  \"b17\": 0.9,\n  \"b19\": 0.9,\n  \"b15\": 0.8,\n  \"b4\": 0.8,\n  \"b12\": 0.7,\n  \"b0\": 0.7\n}", "Based on the text of the paper and the titles of the references, the source papers that inspired the authors the most are likely:\n\n'b5': 'Graph learning based arithmetic block identification' - The authors mention this paper multiple times and it appears that they have built upon the ideas in this paper to develop their own methodology. The authors reference this paper when discussing the limitations of existing methods and when describing their own novel graph neural network architecture. \n\n'b12': 'Directed acyclic graph neural networks' - This paper is mentioned when the authors are discussing the design of their proposed novel graph neural network architecture. They appear to have been inspired by the message passing scheme used in this paper.\n\n'b14': 'Learning representations by maximizing mutual information across views' - This paper is cited when the authors are discussing the success of contrastive learning and the reasons behind it. It seems that the authors have used the ideas in this paper to develop their own contrastive learning scheme.\n\n'b15': 'A simple framework for contrastive learning of visual representations' - The authors seem to have been inspired by the data augmentation scheme introduced in this paper, as they mention it when discussing the development of their own data augmentation scheme. \n\n'b18': 'Graph contrastive learning with augmentations' - This paper is referenced when the authors are discussing the limitations of existing data augmentation schemes for graph data. It appears that the authors have built on the ideas in this paper to develop their own data augmentation scheme.\n\nBased on these observations, the JSON format of the source papers with confidence scores is:\n\n{\n\"b5\": 0.9,\n\"b12\": 0.8,\n\"b14\": 0.7,\n\"b15\": 0.7,\n\"b18\": 0.8\n}", "Based on the text, the source papers that inspire the author most are:\n\n1. \"Graph learning based arithmetic block identification\" [b5]: The author refers to this paper multiple times, indicating that it has been a source of inspiration, especially when discussing the customized Graph Neural Network (FGNN) and the evaluation of the models' generalization ability in sub-netlist identification. The author's work differs from this paper as they use independent aggregators for vertices of different types. Confidence score: 0.8\n\n2. \"Directed acyclic graph neural networks\" [b12]: This paper is referenced when discussing the unique graph type, directed acyclic graph (DAG), used in the author's work to model the gate-level netlists. The author's work differs from this paper as they claim that the methods in [b12] can only handle structural information of DAGs while omitting the underlying semantics of target graphs. Confidence score: 0.7\n\n3. \"Graph contrastive learning with augmentations\" [b18]: This paper is mentioned when the author introduces the concept of contrastive learning to handle graph data. However, the author argues that the methods in [b18] only take the structural information into account during the data augmentation procedures, which leads to inadequate or even wrong understanding of the target graphs. Confidence score: 0.7\n\n4. \"Deep graph infomax\" [b17]: This paper is cited when discussing the concept of contrastive learning, where the main idea is to capture statistical dependencies of interest and those that do not by separating positive samples from negative samples in the n-dimensional embedding space. The author's work differs from this paper as they carefully design a data augmentation scheme to maximize the mutual information between functionally similar netlists. Confidence score: 0.6\n\n5. \"A simple framework for contrastive learning of visual representations\" [b15]: This paper is mentioned when discussing the data augmentation scheme for contrastive learning. The author's work differs from this paper as they argue that generating augmented views without causing semantic changes is not explicit for graph data unlike for images as shown in [b15]. Confidence score: 0.6\n\nThe JSON format of the result would be: \n\n{\n    \"b5\": 0.8,\n    \"b12\": 0.7,\n    \"b18\": 0.7,\n    \"b17\": 0.6,\n    \"b15\": 0.6\n}", "Based on the text and the author notes, the following papers seem to have inspired the author the most:\n\n1. \"Graph learning based arithmetic block identification\" [b5]: The author directly compares their work with this paper, and it seems like their approach was heavily inspired by the methodology proposed in this paper. They also utilise this paper to evaluate their proposed methodology.\n\n2. \"Deep learning based circuit recognition using sparse mapping and level dependent decaying sum circuit representations\" [b4]: This paper is referred to when discussing the existing works on netlist representation learning. The author criticises the approach used in this paper, which suggests that they used it as a stepping stone to develop their own methodology.\n\n3. \"Directed acyclic graph neural networks\" [b12]: The author refers to this paper when discussing the existing graph neural network techniques. They also mention that their proposed methodology is inspired by the network structure proposed in this paper.\n\n4. \"A simple framework for contrastive learning of visual representations\" [b15]: This paper is referred to when discussing contrastive learning. The author seems to have borrowed the idea of contrastive learning from this paper to develop their own framework.\n\n5. \"Graph contrastive learning with augmentations\" [b18]: The author refers to this paper when discussing the limitations of existing contrastive learning methods for graph data. It seems like they used this paper as a reference to overcome these limitations in their methodology.\n\n6. \"Deep graph infomax\" [b17]: This paper is referred to when discussing the existing methods for handling graph data. The author seems to have taken inspiration from this paper to develop their own contrastive learning-based framework.\n\nSo, the source papers are: \n{ \"b5\": 0.9, \"b4\": 0.85, \"b12\": 0.8, \"b15\": 0.75, \"b18\": 0.7, \"b17\": 0.65 }"], "63a413f790e50fcafd6d24b3": ["Based on the paper's text and the titles of the references, the following references seem to have strongly influenced the main contributions of the paper:\n\n{\n\"b2\": 0.8,\n\"b7\": 0.9,\n\"b8\": 0.9,\n\"b9\": 0.7,\n\"b10\": 0.8,\n\"b11\": 0.7,\n\"b12\": 0.6,\n\"b13\": 0.6\n}\n\nThe confidence scores are subjective and represent how strongly each source paper appears to have influenced the given paper based on its text.", "Based on the paper's text, the following references were identified as sources that inspired the authors:\n\n- b2: \"how powerful are graph neural networks\" \n- b7: \"Invariant rationalization\" \n- b8: \"out of distribution generalization with maximal invariant predictor\" \n- b9: \"invariance principle meets information bottleneck for out of distribution generalization\" \n- b11: \"invariant models for causal transfer learning\" \n- b12: \"gnnexplainer generating explanations for graph neural networks\" \n- b13: \"parameterized explainer for graph neural network\" \n- b14: \"algorithm as a k means clustering algorithm\"\n\nThe confidence scores for these papers are derived from the frequency of their mention and the context in which they were mentioned.\n\n{\n\"b2\": 0.9,\n\"b7\": 0.8,\n\"b8\": 0.95,\n\"b9\": 0.85,\n\"b11\": 0.8,\n\"b12\": 0.75,\n\"b13\": 0.7,\n\"b14\": 0.9\n}", "Based on the given information, the source papers that most inspire the given paper can be inferred as follows:\n\n- b2: 'how powerful are graph neural networks': The mention of Graph Neural Networks (GNNs) as a method for extracting useful graph information indicates that this paper has had a significant influence.\n\n- b8: 'out of distribution generalization with maximal invariant predictor': The focus on invariant learning and OOD generalization under distribution shifts suggests that this paper is a key source.\n\n- b10: 'invariant rationalization': The emphasis on invariant learning for images and texts suggests this paper's influence.\n\n- b11: 'invariant models for causal transfer learning': The assumption made on \u03a6(G) for invariant learning might be inspired by this paper.\n\n- b14: 'algorithm as a k means clustering algorithm': The use of k-means for environment inference indicates that this paper is a source.\n\nHere is the result in the requested JSON format:\n\n{\n\"b2\": 0.9,\n\"b8\": 0.8,\n\"b10\": 0.7,\n\"b11\": 0.85,\n\"b14\": 0.9\n}\n\nThe confidence scores are subjective and based on the relevance as inferred from the given text. A higher score indicates a stronger inferred influence on the given paper.", "Based on the text and titles of the references, the most pertinent sources that inspired this paper appear to be:\n\n1. 'b2': 'how powerful are graph neural networks'\n2. 'b3': 'a meta transfer objective for learning to disentangle causal mechanisms'\n3. 'b8': 'out of distribution generalization with maximal invariant predictor'\n4. 'b9': 'invariance principle meets information bottleneck for out of distribution generalization'\n5. 'b10': 'invariant rationalization'\n6. 'b11': 'invariant models for causal transfer learning'\n7. 'b12': 'gnnexplainer generating explanations for graph neural networks'\n8. 'b14': 'algorithm as a k means clustering algorithm'\n\nThese are the sources that have the most direct relevance to the paper's novel contributions, such as learning invariant graph representations, graph neural networks (GNNs), distribution shifts, clustering algorithms, and invariant learning. They are also strongly connected to the methodological approaches and algorithms used in the paper.\n\nThe JSON format of the references would look like this:\n\n{\n\"b2\": 0.9,\n\"b3\": 0.8,\n\"b8\": 0.8,\n\"b9\": 0.8,\n\"b10\": 0.7,\n\"b11\": 0.7,\n\"b12\": 0.7,\n\"b14\": 0.7\n} \n\nThe confidence scores are subjective and could be adjusted according to the exact influence of each source on the paper.", "Based on the given text and the titles of the references, the source papers that most likely inspired the author are:\n\n- \"b2\": 'how powerful are graph neural networks'\n- \"b8\": 'out of distribution generalization with maximal invariant predictor'\n- \"b9\": 'invariance principle meets information bottleneck for out of distribution generalization'\n- \"b10\": 'invariant rationalization'\n- \"b11\": 'invariant models for causal transfer learning'\n- \"b12\": 'gnnexplainer generating explanations for graph neural networks'\n- \"b13\": 'parameterized explainer for graph neural network'\n\nThe JSON format of the result is:\n\n{ \"b2\": 0.9, \"b8\": 0.9, \"b9\": 0.9, \"b10\": 0.9, \"b11\": 0.9, \"b12\": 0.9, \"b13\": 0.9 } \n\nThese papers are related to the concepts of graph neural networks, invariant learning, out-of-distribution generalization and graph explanation. The high confidence score for these papers indicates that they are likely to have a significant influence on the main contributions of the given paper."], "62d16e8a5aee126c0fd6847a": ["The source papers that inspire the given paper most based on its text are:\n- \"b77: Elastic cuckoo page tables rethinking virtual memory translation for parallelism\": This paper is mentioned multiple times throughout the text and provides the foundation for the Elastic Cuckoo Page Tables (ECPTs) used in the given paper. The author often cites this paper when discussing the ECPT design and its implementation. Confidence score: 1.0\n- \"b87: Hash don't cache the page table\": This paper is also cited multiple times. It provides the basis for the hashed page tables (HPTs), which are a key component of the given paper's proposed solution. Confidence score: 0.9\n- \"b13: Spectlb a mechanism for speculative address translation\": This paper is referenced in the context of discussing the shortcomings of HPTs, which is an important consideration in the given paper's development of a new approach. Confidence score: 0.7\n- \"b15: Accelerating two dimensional page walks for virtualized systems\": This paper is cited when discussing the problem of address translation in virtualized systems, which is a key challenge that the given paper aims to address. Confidence score: 0.7\n- \"b29: Agile paging exceeding the best of nested and shadow paging\": This paper seems to be a key reference in the context of discussing the existing solutions for address translation in a virtualized environment, an important part of the given paper's literature review. Confidence score: 0.6\n- \"b76: Babelfish fusing address translations for containers\": This paper is cited when discussing the concept of parallelism in address translation, which is central to the given paper's proposed solution. Confidence score: 0.6\n\nThe JSON format result is: {\"b77\": 1.0, \"b87\": 0.9, \"b13\": 0.7, \"b15\": 0.7, \"b29\": 0.6, \"b76\": 0.6}", "Based on the task description, the analysis of the text and the author notes, the source papers of the given academic paper are:\n\n1. \"parallel virtualized memory translation with nested elastic cuckoo page tables\" [No reference number, Confidence: 0.9]: This is the main paper under discussion. The entire paper revolves around the idea proposed in this paper and the author indicates that their work is an improvement or adaptation of the ideas from this paper.\n\n2. \"Elastic cuckoo page tables rethinking virtual memory translation for parallelism\" [b77, Confidence: 0.85]: This paper is directly mentioned in the text and the authors suggest that their work is a continuation or improvement of the ideas from this paper.\n\n3. \"accelerating two dimensional page walks for virtualized systems\" [b15, Confidence: 0.8]: This paper is directly mentioned in the text and appears to be a primary source of inspiration for the authors' work on the nested address translation process and its optimization.\n\n4. \"hash don't cache the page table\" [b87, Confidence: 0.75]: This paper is directly mentioned in the text. The authors seem to have drawn inspiration from its ideas on hashed page tables.\n\n5. \"translation caching skip don't walk the page table\" [b12, Confidence: 0.7]: This paper is directly mentioned in the text. It seems to have inspired the authors' work on address translation caching mechanisms.\n\n6. \"do it yourself virtual memory translation\" [b3, Confidence: 0.7]: This paper is directly mentioned in the text. It seems to have inspired the authors' work on creating their own virtual memory translation mechanism.\n\nJSON format return:\n\n{\"b77\": 0.85, \"b15\": 0.8, \"b87\": 0.75, \"b12\": 0.7, \"b3\": 0.7, \"No reference number\": 0.9}", "Based on the provided text, the paper mentions several previous works and research papers, but the ones that appear to have most directly inspired or influenced the work are:\n\n- \"Elastic Cuckoo Page Tables: Rethinking Virtual Memory Translation for Parallelism\" [b77]: The authors of the current paper refer to this work multiple times and base their design on the ECPTs that were proposed in this paper. They also discuss its limitations and how they aimed to overcome them in their work.\n\n- \"Accelerating Two-Dimensional Page Walks for Virtualized Systems\" [b15]: This paper is referenced multiple times as the authors discuss the limitations of existing systems and how they aimed to overcome them.\n\n- \"Revisiting Hardware-Assisted Page Walks for Virtualized Systems\" [b2]: This paper is mentioned in the context of the authors' discussion of existing systems and the challenges they present.\n\n- \"Hashed Page Tables\" [b13, b48, b81, b87]: Several papers discussing hashed page tables are mentioned, indicating that they were a significant influence on the authors' work.\n\n- \"Efficient Virtual Memory for Big Memory Servers\" [b24]: This paper is mentioned in the context of the authors' discussion of the limitations of existing systems and their proposal to overcome these limitations.\n\n- \"Space Efficient Hash Tables with Worst Case Constant Access Time\" [b27]: This paper is cited in the context of the authors' discussion on how they used cuckoo hashing to resolve hash collisions in their design.\n\nThe references seem to be highly related to the main contributions of the paper, and they have been quoted in the context of the authors' development of their solution.\n\nThe \"confidence score\" of these papers being the main sources of inspiration for the current paper can be estimated as follows:\n\n- \"Elastic Cuckoo Page Tables: Rethinking Virtual Memory Translation for Parallelism\" [b77]: 0.9\n- \"Accelerating Two-Dimensional Page Walks for Virtualized Systems\" [b15]: 0.7\n- \"Revisiting Hardware-Assisted Page Walks for Virtualized Systems\" [b2]: 0.7\n- \"Hashed Page Tables\" [b13, b48, b81, b87]: 0.8\n- \"Efficient Virtual Memory for Big Memory Servers\" [b24]: 0.6\n- \"Space Efficient Hash Tables with Worst Case Constant Access Time\" [b27]: 0.6\n\nThe final output in JSON format would be:\n\n{\"b77\": 0.9, \"b15\": 0.7, \"b2\": 0.7, \"b13\": 0.8, \"b48\": 0.8, \"b81\": 0.8, \"b87\": 0.8, \"b24\": 0.6, \"b27\": 0.6}", "Based on the text and the author's note provided, the source papers that inspired this particular paper the most appear to be:\n\n1. \"Hash don't cache the page table\" (b87)\n2. \"Elastic cuckoo page tables: rethinking virtual memory translation for parallelism\" (b77)\n3. \"Parallel Virtualized Memory Translation with Nested Elastic Cuckoo Page Tables\" (None)\n4. \"Revisiting Hardware-Assisted Page Walks for Virtualized Systems\" (b2)\n5. \"Address translation in virtualized environments: a very large part of memory TLB\" (b74)\n6. \"Translation caching: skip, don't walk (the page table)\" (b12)\n\nThe papers above are directly associated with the challenges outlined in the paper and the proposed algorithm. These papers seem to be the most relevant to the paper's novel contributions and methodologies.\n\nThese findings can be represented in a JSON format as follows:\n\n```\n{\n\"b87\": 0.9,\n\"b77\": 0.85,\n\"None\": 0.8,\n\"b2\": 0.77,\n\"b74\": 0.75,\n\"b12\": 0.72\n}\n```\n\nThe above JSON object represents the confidence score from 0 to 1 for each source paper based on its relevance to the given paper. The keys indicate the reference number of each source paper, and the values represent the confidence score.", "Based on the paper content and the titles of the references, the source papers that are most likely to have influenced the current paper are:\n\n{\n\"b77\" : 1,\n\"b13\" : 0.8,\n\"b25\" : 0.7,\n\"b7\" : 0.7,\n\"b15\" : 0.7,\n\"b48\" : 0.6,\n\"b87\" : 0.6,\n\"b34\" : 0.6,\n\"b39\" : 0.6,\n\"b42\" : 0.6\n}\n\nThese papers are highly relevant to the paper's main contributions, specifically in terms of inspiration for the use of hash algorithms and parallel access mechanisms for virtual address translation. The confidence scores are given based on the frequency and context of the references in the text."], "628704275aee126c0f5b583a": ["Based on the provided information and the instructions, I'll identify the source papers that directly inspired the novel contributions of the given paper. These contributions are the two methods proposed to accelerate Page Walking: \n\n1. Reducing the number of page table layers from four to two, which is notably effective in virtualized environments.\n2. Preferentially caching page table entries for applications with frequent TLB misses.\n\nThe paper mentions that the first method doesn't show significant performance improvement natively due to the existence of Page Walker Caches (PWCs), but it does in virtualized environments. It also refers to another paper from ASPLOS (\"Nested Elastic Cuckoo Page Tables\") optimizing translation memory access numbers in virtualized environments, which suggests that this reference might be relevant.\n\nThe second method is described as intuitive and is linked to a paper (CRISP) that increased the instruction scheduling priority for the critical path of memory accesses. By analogy, the given paper increases the cache block priority for the critical path of page table walks.\n\nLet's locate references that directly influenced the novel contributions:\n\n1. Flattening the page table:\n   - References discussing the overhead of traditional multi-level page tables and approaches to flattening them could be the inspiration for this method.\n   - The paper mentions Elastic Cuckoo Hashing, which is a technique for reorganizing page tables to reduce lookup latencies.\n\n2. Caching page table entries:\n   - References focusing on TLB misses and cache optimizations could have inspired this approach.\n   - The mention of CRISP in the author notes indicates a direct inspiration from the techniques used in that paper for prioritizing certain types of memory accesses.\n\nBased on this analysis, let's infer the most pertinent source papers and construct a JSON with the reference number as the key and the confidence score as the value. The confidence score represents how directly relevant the reference is to the paper's novel contributions.\n\n```json\n{\n  \"b47\": 0.9,  // Nested Elastic Cuckoo Page Tables - specifically mentioned as related work in virtualization.\n  \"CRISP\": 0.9, // The paper regarding instruction scheduling priority for memory access critical paths; directly related to the second method.\n  \"b15\": 0.7,  // Accelerating two-dimensional page walks for virtualized systems - related to virtualization and page walks.\n  \"b3\": 0.6,   // Revisiting hardware-assisted page walks for virtualized systems - related to hardware assistance in page walks for virtualized environments.\n  \"b33\": 0.6,  // CSALT - related to cache prioritization mentioned in the context of page table entries.\n  \"b4\": 0.6,   // Compendia - parallel work addressing flattening the page table, which is a similar contribution to the paper's.\n  \"b11\": 0.5,  // Mask - discusses GPU memory hierarchy but could be related to cache prioritization techniques.\n  \"b34\": 0.5   // Prefetched address translation - related to page table prefetching, which could be a part of the caching discussion.\n}\n```\n\nNote: The confidence scores are subjective estimates based on the provided text and author notes. The actual confidence may vary depending on a full reading of the paper and the referenced works. \"CRISP\" is used as a placeholder for the actual reference number, as it was not provided in the list of titles but was explicitly mentioned in the author notes. If \"CRISP\" is a reference in the given list, it should be replaced with the correct reference number.", "Based on the provided text and author notes, the task is to identify the source papers that the given paper relies on most heavily for its contributions. To accomplish this, I will follow the steps outlined in the task:\n\n1. Identify the novel contributions of the paper.\n2. Prioritize methodological similarities with other works.\n3. Look for comparative phrases indicating direct inspiration or motivation.\n\nAfter analyzing the text, here are the paper's novel contributions:\n- The paper proposes two methods to accelerate Page Walking: reducing page table layers from 4 to 2, and preferentially caching page table entries in applications with high TLB miss rates.\n- The paper notes that the performance improvement from reducing page table layers is not significant in native environments due to the presence of Page Walker Caches (PWCs), but it is significant in virtualized environments.\n- The paper establishes a link with another paper, \"Nested Elastic Cuckoo Page Tables,\" which also optimizes virtualized environment translation memory access reductions.\n- The paper suggests a methodological similarity with the work \"CRISP,\" where the priority of instruction scheduling for memory access critical paths is raised. This paper raises the cache block priority for those critical paths.\n\nBased on the novel contributions and methodological similarities, the following references are identified as sources that the paper relies on most heavily:\n\n```json\n{\n  \"b47\": 0.9,\n  \"b3\": 0.8,\n  \"b33\": 0.7,\n  \"b15\": 0.7,\n  \"b40\": 0.6,\n  \"b34\": 0.5,\n  \"b4\": 0.5\n}\n```\n\nConfidence scores are based on the following justifications:\n- **b47 (Elastic Cuckoo Page Tables)**: Directly mentioned in the author's notes as a paper optimizing virtualized environment translation memory access numbers, which is similar to the method proposed in the given paper. High confidence score (0.9).\n- **b3 (Revisiting Hardware-Assisted Page Walks for Virtualized Systems)**: The paper discusses the performance of hardware-assisted page walks in virtualized systems, which is relevant to the novel contributions of the given paper. High confidence score (0.8) due to methodological similarities.\n- **b33 (CSALT)**: Marathe et al.'s work on cache prioritization extension is closely related to the cache prioritization method proposed in the paper. High confidence score (0.7).\n- **b15 (Accelerating Two-Dimensional Page Walks for Virtualized Systems)**: Gandhi et al.'s work on merging 2D page tables into a single dimension is mentioned as a background for reducing page walks, which is a challenge the given paper addresses. High confidence score (0.7).\n- **b40 (Perforated Page Supporting Fragmented Memory Allocation for Large Pages)**: Mentioned in the context of realistic memory fragmentation, which is a challenge that the given paper tries to overcome. Moderate confidence score (0.6).\n- **b34 (Prefetched Address Translation)**: Margaritov et al.'s work on prefetching the lower levels of the page table during higher level accesses has a methodological similarity but is less impactful due to modern PWCs, as mentioned in the paper. Moderate confidence score (0.5).\n- **b4 (Compendia)**: Parallel work addressing flattening the page table, mentioned in the context of related work, and is relevant for comparison. Moderate confidence score (0.5).\n\nThese references are selected because they are directly linked to the novel contributions and methods mentioned in the paper. The confidence scores reflect the degree to which each reference is expected to have inspired the paper's contributions, based on the explicit mentions and the context in which they are discussed.", "Based on the author's notes and the text provided, we can infer the key source papers that have most directly inspired the paper's novel contributions. The author's notes mention two main methods to accelerate Page Walking: 1) reducing the number of page table levels from 4 to 2, and 2) preferentially caching page table entries for applications with frequent TLB misses. The author also mentions a connection with another paper from ASPLOS, \"Nested Elastic Cuckoo Page Tables,\" which optimizes translation memory access in virtualized environments, and a linkage with the CRISP paper regarding prioritization in the cache hierarchy.\n\nGiven the text of the paper, we can look for specific references mentioned in the context of the novel contributions. For example, references to \"flattening the page table,\" \"prioritizing caching page table entries,\" and comparisons with PWCs (Page Walker Caches) are key to understanding which source papers are most relevant.\n\nThe JSON format with \"reference number\" as the key and \"confidence score\" between 0 and 1 as the value is presented below. The confidence score reflects how directly the referenced work is connected to the novel contributions of the paper, based on the author's notes and the paper's content.\n\n```json\n{\n  \"b47\": 0.9,\n  \"b3\": 0.8,\n  \"b4\": 0.7,\n  \"b33\": 0.6,\n  \"b34\": 0.5,\n  \"b15\": 0.8\n}\n```\n\nExplanation of the confidence scores:\n- `b47` (Nested Elastic Cuckoo Page Tables): This paper is directly mentioned in the author's notes as related to optimizing translation memory access in virtualized environments, which is a key part of the novel contributions.\n- `b3` (Revisiting Hardware-Assisted Page Walks for Virtualized Systems): Implied by the author's notes, this paper is relevant due to its focus on hardware-assisted page walks in virtualized systems.\n- `b4` (Compendia): Mentioned in the paper as a parallel work addressing flattening the page table and providing performance comparisons.\n- `b33` (CSALT): Mentioned in the paper as a previous proposal that included cache prioritization extensions, which is related to the novel contribution of cache prioritization.\n- `b34` (Prefetched Address Translation): Mentioned in the paper in the context of prefetching techniques, which may be extended based on the new methods proposed.\n- `b15` (Accelerating Two-Dimensional Page Walks for Virtualized Systems): Mentioned in the introduction as a work that has addressed issues in virtualized environments, which are directly relevant to the paper's contributions.\n\nThe confidence scores are subjective and based on the paper's content and the author's notes, so they may vary depending on the interpretation of how directly each referenced work contributes to the novel aspects of the paper.", "Based on the instructions and the provided text, let's identify the most pertinent source papers that inspired the novel contributions of the given paper. We'll look for phrases indicating direct inspiration or motivation, novel methods, and approaches introduced to tackle the challenges outlined.\n\nPrimary Challenges Outlined:\n1. Traditional page walks do not scale well with large data sets, causing significant TLB misses and delays.\n2. Virtualized environments see an even larger penalty due to the 2D page walk for guest page table translation.\n\nAlgorithm/Methods Proposed:\n1. Flattening the page table to reduce the number of indirections required for a page walk.\n2. Prioritizing caching of page table entries during high TLB miss rates to reduce latency.\n\nNow, let's identify source papers that are directly linked to these challenges and methods:\n\n1. For the challenge of scaling page walks with large datasets and the method of flattening the page table:\n- 'b4': 'compendia reducing virtual memory costs via selective densification' (Similar approach to flattening page tables)\n- 'b47': 'elastic cuckoo page tables rethinking virtual memory translation for parallelism' (Method for restructuring the page table mentioned in the author notes)\n\n2. For the challenge of high TLB misses and the method of prioritizing caching page table entries:\n- 'b33': 'csalt context switch aware large tlb' (Mentions cache prioritization extension that keeps page table entries in the cache)\n\n3. The author notes mention a work from ASPLOS on translation access numbers in virtualized environments, which might be related to 'b47' mentioned above.\n\n4. The author notes also mention a connection to a paper about improving memory access critical paths ('CRISP'):\n- This paper is not listed in the provided references but seems to be a direct inspiration based on the description in the author notes.\n\nBased on the provided text and author notes, here is the JSON format list of source papers with confidence scores:\n\n```json\n{\n  \"b4\": 0.8,\n  \"b33\": 0.8,\n  \"b47\": 0.9\n}\n```\n\nThe confidence scores are subjective estimates based on the degree of methodological similarity and the direct mention of inspiration in the text and author notes. Paper 'b47' has the highest confidence score as it is directly mentioned in the author notes in relation to the novel contributions of the given paper. Papers 'b4' and 'b33' are inferred to be highly relevant based on the similarity of their methods to the novel contributions of this paper. The paper 'CRISP' mentioned in the author notes would have a high confidence score but is not included due to lack of a reference identifier.", "Based on the provided text and instructions, I will analyze the paper to identify the novel contributions and find references that are indicated as inspirational or foundational to the work. The author notes mention two methods for accelerating Page Walking:\n\n1. Reducing the number of page table levels: from 4 levels to 2 levels.\n2. Preferential caching of page table entries for applications with high TLB miss rates.\n\nThe paper mentions that the presence of Page Walker Caches (PWCs) implies that their first method does not significantly improve performance in native scenarios but does in virtualized ones. Their second method is intuitive and relates to another paper (CRISP) that improved the prioritization of instruction scheduling on the critical access path, while the current paper improves the cache block priority on the critical access path.\n\nNow, let's extract the references based on the criteria:\n\n1. The method of flattening the page table by reducing it from 4 levels to 2 levels is a novel contribution. The references that might be related to this contribution are mentioned in the context of virtualized environments and dealing with page table walks. The closest references mentioned in the context of this contribution are [b47] \"Nested Elastic Cuckoo Page Tables\" and [b3] \"Revisiting hardware-assisted page walks for virtualized systems\".\n\n2. The method of preferentially caching page table entries for applications with high TLB miss rates is another novel contribution. The paper that seems to be directly related to this concept, as mentioned in the author notes, is CRISP, which is not listed among the references here. We can infer that the most relevant references related to cache prioritization could be [b35] \"Dead page and dead block predictors cleaning TLBs and caches together\" and [b33] \"CSALT context switch aware large TLB\".\n\n3. The paper mentions \"Elastic Cuckoo Hashing\" [b47] as a technique for organizing the page table to generate lookup addresses without pointer chasing. Although there are implementation challenges mentioned for ECH, it is clear that this work has some relevance to the paper's contributions.\n\n4. The paper also discusses comparative works like \"Compendia\" [b4], which addresses flattening the page table, and \"CSALT\" [b33], which implements cache prioritization.\n\nGiven the information provided and the context of the paper's contributions, here is the JSON format output of the pertinent source papers with confidence scores:\n\n```json\n{\n  \"b47\": 0.8,\n  \"b3\": 0.6,\n  \"b33\": 0.7,\n  \"b35\": 0.5,\n  \"b4\": 0.6\n}\n```\n\nThe confidence scores are subjective estimates based on the paper's text indicating how directly relevant the referenced paper appears to be to the novel contributions of the current paper. The scores reflect the strength of the methodological similarity and the degree to which the current paper builds upon the referenced work."], "628d27f85aee126c0f4de037": ["Based on the given text and instructions, the following JSON object lists the most pertinent source papers with their reference numbers and confidence scores. The confidence score reflects the degree to which the information provided indicates that the given paper was a significant source of inspiration for the authors.\n\n```json\n{\n  \"b14\": 0.95,\n  \"b13\": 0.85,\n  \"b8\": 0.80,\n  \"b6\": 0.75\n}\n```\n\nExplanation:\n\n- \"b14\" (Boomerang: a metadata-free architecture for control flow delivery) is given a high confidence score of 0.95 because the text explicitly states that the current work addresses the key limitation of Boomerang and builds upon it.\n\n- \"b13\" (Confluence: unified instruction supply for scale-out servers) has a confidence score of 0.85 because it describes a state-of-the-art method that the current paper aims to improve upon, particularly in the context of unified front-end prefetching and BTB-directed prefetching.\n\n- \"b8\" (Fetch directed instruction prefetching) receives a confidence score of 0.80 as this is the foundational technique that is being extended in the current work, and the introduction suggests that the new method is an evolution of FDIP.\n\n- \"b6\" (Temporal instruction fetch streaming) has a confidence score of 0.75 because it represents a technique in front-end prefetching that the current work seeks to enhance, as indicated by discussions of metadata storage and temporal streaming's drawbacks. \n\nThe confidence scores are subjective estimates based on the instructions given and the context within the provided text. The references at the beginning of the text that describe historical findings, such as \"b1\", \"b2\", \"b3\", and \"b4\", were not included in the JSON object as per the instructions, as they are not directly related to the novel contributions of the paper.", "Based on the author's notes and the text provided, the source papers that seem to have directly influenced the main contributions of the paper can be identified with a focus on the novel contributions and methodological similarities. The paper builds on the concept of the Boomerang prefetcher introduced in [b14] and further optimizes performance by focusing on the distribution of control flow instructions and a new BTB structure design.\n\nFrom the text, the main novel contributions appear to include:\n1. A new BTB organization that dedicates most capacity to unconditional branches and spatially encoded footprints of their target regions.\n2. The introduction of Shotgun, a unified instruction cache, and BTB prefetcher based on the new BTB organization.\n3. Improvements in storage efficiency and performance/storage trade-offs through different spatial encoding alternatives.\n\nThe primary source paper that the current work extends upon is the ASPLOS 2018 Shotgun paper, as mentioned by the author. This indicates that [b14] \"Boomerang: A Metadata-Free Architecture for Control Flow Delivery\" is a direct source of inspiration for this paper.\n\nThe competitive analysis section indicates that the paper compares its approach against \"Confluence\" [b13] and \"Boomerang\" [b14], suggesting that these papers are relevant to the novel contributions.\n\nTo provide a concise list of source papers most central to the main contributions of this paper, we'll focus on papers mentioning BTB prefetching, FDIP, and the Boomerang prefetcher as these are directly relevant to the paper's novel contributions.\n\nHere is the JSON-formatted list of source papers with confidence scores based on the criteria provided:\n\n```json\n{\n  \"b13\": 0.9,\n  \"b14\": 1.0\n}\n```\n\nConfidence scores are interpreted as follows:\n- A score of 1.0 for [b14] \"Boomerang: A Metadata-Free Architecture for Control Flow Delivery\" as it is explicitly stated as the basis for the novel contribution in the author's notes.\n- A score of 0.9 for [b13] \"Confluence: Unified Instruction Supply for Scale-Out Servers\" because it is mentioned in the competitive analysis and relates closely to the methodological advancements in BTB prefetching, even though it is not explicitly cited as a source of inspiration. It is still highly relevant due to the comparison made.\n\nThe confidence scores are subjective interpretations based on the information provided and could vary depending on the full context of the paper.", "Based on the provided text and author notes, we can infer that the paper extends upon the work introduced in the ASPLOS 2018 paper on the Boomerang prefetching method and proposes enhancements to the BTB structure to optimize front-end performance. The novel contributions include a new BTB organization and the Shotgun prefetcher which focuses on unconditional branches and their spatial footprints.\n\nTo identify the source papers that had the most direct influence on the paper's core contributions, I will parse the text for phrases that indicate a strong reliance on previous research.\n\nThe key phrases and the corresponding source papers identified are:\n- \"To overcome the overwhelming metadata storage costs of temporal streaming, the latest work in relieving the front-end bottleneck leverages fetch-directed instruction prefetching (FDIP) [b8] and extends it with unified prefetching into the BTB [b14].\" (Indicates Boomerang and the concept of FDIP as central inspirations.)\n- \"While Boomerang reduces the prefetcher costs to near zero by leveraging existing in-core structures (BTB and branch direction predictor), it has limited effectiveness on workloads with very large instruction working sets.\" (Indicates the direct expansion of Boomerang's work.)\n- \"The scheme, called Boomerang, discovers BTB misses on the prefetch path and fills them by fetching the appropriate cache blocks and extracting the necessary branch target metadata.\" (Mentions Boomerang as a key predecessor.)\n- \"Based on these insights, this work introduces Shotgun, a BTB-directed front-end prefetcher powered by a new BTB organization specialized for effective prefetching.\" (Introduction of Shotgun as a novel contribution, built upon the Boomerang scheme.)\n\nFrom the information provided, the papers directly relevant to the paper's novel contributions are:\n\n1. \"Fetch Directed Instruction Prefetching\" - [b8]\n2. \"Boomerang: A Metadata-Free Architecture for Control Flow Delivery\" - [b14]\n\nHere is the result in JSON format with confidence scores based on their relevance and frequency of mention in the context of novel contributions:\n\n```json\n{\n  \"b8\": 0.9,\n  \"b14\": 1.0\n}\n```\n\nThe confidence scores reflect the direct relevance and influence of these works on the novel contributions of the paper. The score for [b14] is the highest because the new work is an expansion of the Boomerang method, directly addressing its limitations. The score for [b8] is slightly lower because it's foundational to the concept of FDIP, which Boomerang and the subsequent Shotgun approach build upon.", "Based on the provided information and the instructions, I will identify the source papers that have directly influenced the novel contributions of the paper titled \"Shooting Down the Server Front-End Bottleneck\". To do this, I will first list the novel contributions as outlined in the text:\n\n1. Addressing the key limitation of Boomerang due to limited BTB capacity.\n2. Observing the software behavior and structure (small functions with high spatial locality).\n3. Proposing a new BTB organization that dedicates most capacity to unconditional branches.\n4. Introducing Shotgun, a new BTB-directed prefetcher that outperforms Boomerang by up to 10%.\n5. Exploring different alternatives for spatial encoding of target regions for storage efficiency.\n6. Suggesting future research directions to close the performance gap with an ideal front-end.\n\nNext, I will look for the references that are most directly related to these novel contributions. The author notes that this work is an extension of the ASPLOS 2018 Shotgun paper and builds upon the Boomerang prefetch method proposed in that paper. The key phrase \"\u57fa\u4e8eASPLOS 2018 Shotgun\u8bba\u6587\u62d3\u5c55\u7684Journal\" indicates that the ASPLOS 2018 Shotgun paper is a significant source of inspiration.\n\nAdditionally, the text mentions that the latest work leverages FDIP and extends it with unified prefetching into the BTB (Boomerang). This suggests that the FDIP-related papers (referenced as [b8] and [b14]) are also directly relevant to the novel contributions of this paper.\n\nFinally, let's identify the papers with a direct methodological similarity and influence on the novel contributions, as outlined in the provided text:\n\n1. Boomerang ([b14]): Mentioned as the starting point, which this paper seeks to improve upon.\n2. FDIP ([b8]): The foundational method for BTB-directed prefetching, which is extended by Boomerang and further by this paper.\n3. Confluence ([b13]): As a state-of-the-art temporal streaming prefetcher, it's referenced for comparative purposes and to highlight the improvements made by Shotgun.\n\nBased on these observations, here are the references in JSON format with their confidence scores, reflecting the degree of direct influence on the novel contributions:\n\n```json\n{\n  \"b8\": 0.9,\n  \"b13\": 0.7,\n  \"b14\": 1.0\n}\n```\n\nThe confidence scores reflect the following rationale:\n\n- \"b8\" (FDIP) is given a score of 0.9 because it's foundational to the BTB-directed prefetching method, which this work builds upon.\n- \"b13\" (Confluence) is given a score of 0.7 because it's used for comparison and to show the advancements made by Shotgun.\n- \"b14\" (Boomerang) is given a score of 1.0 because it's the immediate precursor to Shotgun, and this work directly addresses its limitations.\n\nThe other references are either historical or related to the broader context and do not directly influence the novel contributions of this paper. Therefore, they are not included in the JSON output.", "Based on the provided information and the instructions, I will identify the source papers that have most directly inspired the paper titled \"Shooting Down the Server Front-End Bottleneck.\" The author notes indicate that the paper expands on the ASPLOS 2018 Shotgun paper and aims to address performance limitations due to BTB capacity, as previously proposed by Boomerang. The novel contribution is the introduction of a new BTB structure designed to optimize front-end performance by focusing on the distribution of control flow instructions in server workloads.\n\nExtracting important information from the text, it's clear that the paper directly builds upon the concepts introduced in the Boomerang prefetching method and further develops them to address its limitations. The text also references \"Shotgun,\" which seems to be another key contribution to the field and might be the direct predecessor to the paper's proposed method.\n\nBased on the provided titles of the references and the text, here are the most pertinent source papers with their confidence scores, formatted as a JSON object:\n\n```json\n{\n  \"b14\": 1.0,\n  \"b13\": 0.9,\n  \"b8\": 0.8,\n  \"b6\": 0.7,\n  \"b11\": 0.7\n}\n```\n\nExplanation for the scores:\n- b14 (Boomerang): Confidence score of 1.0 as the paper is an extension of the Boomerang prefetching method and seeks to address its limitations.\n- b13 (Confluence): Confidence score of 0.9 because the paper discusses and builds upon the unified prefetching method introduced in Confluence.\n- b8 (Fetch Directed Instruction Prefetching): Confidence score of 0.8 since the paper mentions FDIP and its influence on the development of Boomerang, which is a central topic in the paper.\n- b6 (Temporal Instruction Fetch Streaming): Confidence score of 0.7 as temporal streaming is mentioned as a technique used in prior works, and the paper's contributions are related to overcoming its storage costs.\n- b11 (Phantom BTB): Confidence score of 0.7 because it deals with the concept of a virtualized BTB design, which appears to be relevant to the paper's novel BTB structure.\n\nThe confidence scores are subjective estimations based on the paper's text and should be used as a guideline for how directly each reference is believed to influence the paper's contributions. The references with the highest confidence scores are deemed the most relevant and should be prioritized for deeper analysis."], "632630ff90e50fcafdf67484": ["Based on the text and the author notes, the key challenges outlined in the paper are the limitations of previous sampling techniques and the difficulty of simulating large, multi-threaded applications. The algorithm proposed by the author, the LoopPoint methodology, aims to overcome these limitations by using loop iterations as the unit of work and using a representative simulation region selection methodology. \n\nIn the text, the phrases \"inspired by\", \"motivated by\", \"inspired us\", \"motivated us\", \"take inspiration\", \"the pioneering/previous work\", \"following.. we adopt ... to solve the challenge/problem\", \"we use... based on to achieve...\" weren't found, but there are several references that appear to be linked to the challenges and methods outlined in the paper.\n\nThe source papers that were instrumental in developing the LoopPoint methodology, and thus the most pertinent source papers, are:\n\n1. \"Automatically characterizing large scale program behavior\" [b0]: The paper refers to this work when discussing the LoopPoint methodology and simulation region selection, which suggests it may have been a source of inspiration. Confidence score: 0.8\n2. \"BarriersPoint sampled simulation of multi-threaded applications\" [b11]: The paper outlines the limitations of this methodology and how the LoopPoint methodology aims to overcome these. Confidence score: 0.8\n3. \"TaskPoint sampled simulation of task-based programs\" [b12]: Similar to [b11], this paper also discusses the limitations of this previous work and how LoopPoint improves upon it. Confidence score: 0.8\n4. \"Sampled simulation of multi-threaded applications\" [b10]: This paper is referenced when discussing the speed limitations of time-based sampling methodologies. Confidence score: 0.7\n5. \"Selecting software phase markers with code structure analysis\" [b18]: This paper is mentioned when discussing how to mark region boundaries in the LoopPoint methodology. Confidence score: 0.7\n6. \"PinPlay: a framework for deterministic replay and reproducible analysis of parallel programs\" [b15]: Referred to when discussing the use of constrained pinballs in the LoopPoint methodology. Confidence score: 0.8\n7. \"IPC considered harmful for multiprocessor workloads\" [b16]: This paper is mentioned in the context of discussing the limitations of using IPC to evaluate the performance of multi-threaded workloads. Confidence score: 0.7\n\nPlease note that the confidence scores are estimated based on the information provided in the text and author notes, and are subject to further verification. The JSON format of the result is:\n{'b0': 0.8, 'b11': 0.8, 'b12': 0.8, 'b10': 0.7, 'b18': 0.7, 'b15': 0.8, 'b16': 0.7}.", "Based on the provided paper text and the author's note, the following papers seem to have had the most direct influence on the paper's content and methodology:\n\n1) b0: 'automatically characterizing large scale program behavior'\n2) b1: 'smarts accelerating microarchitecture simulation via rigorous statistical sampling'\n3) b9: 'esesc a fast multicore simulator using time based sampling'\n4) b10: 'sampled simulation of multi threaded applications'\n5) b18: 'selecting software phase markers with code structure analysis'\n6) b15: 'pinplay a framework for deterministic replay and reproducible analysis of parallel programs'\n7) b16: 'ipc considered harmful for multiprocessor workloads'\n8) b21: 'how to use simpoint to pick simulation points'\n\nThe confidence score for each source paper is a subjective measure. However, based on the frequency and context of their citations in the paper, a tentative assignment could be:\n\n{ \"b0\": 0.9, \"b1\": 0.9, \"b9\": 0.8, \"b10\": 0.8, \"b18\": 0.7, \"b15\": 0.7, \"b16\": 0.7, \"b21\": 0.6 }\n\nPlease note that this result is a rough approximation and it's recommended to fine-tune these scores based on a more detailed review of the paper.", "Based on the text provided, the most pertinent source papers that inspired the given paper seems to be:\n\n1) \"automatically characterizing large scale program behavior\" [b0]\n2) \"smarts accelerating microarchitecture simulation via rigorous statistical sampling\" [b1]\n3) \"esesc a fast multicore simulator using time based sampling\" [b9]\n4) \"sampled simulation of multi threaded applications\" [b10]\n5) \"barrierpoint sampled simulation of multi threaded applications\" [b11]\n6) \"taskpoint sampled simulation of task based programs\" [b12]\n7) \"selecting software phase markers with code structure analysis\" [b18]\n8) \"pinplay a framework for deterministic replay and reproducible analysis of parallel programs\" [b15]\n\nThese references were identified based on the key phrases used in the text and their direct relevance to the novel contributions made in the given paper. However, a more thorough analysis may be required to confirm these findings.\n\nIn the JSON format:\n{\n\"b0\": 1,\n\"b1\": 1,\n\"b9\": 1,\n\"b10\": 1,\n\"b11\": 1,\n\"b12\": 1,\n\"b18\": 1,\n\"b15\": 1\n}", "Based on the provided text, the following source papers seem to have most significantly inspired the given paper:\n\n1. \"automatically characterizing large scale program behavior\" - [b0] : The given paper made use of the concept of 'Basic Block Vector' (BBV) and simulation region selection, which are key concepts in [b0]. The paper also refers [b0] multiple times across its text, indicating the strong reliance on it.\n   \n2. \"smarts accelerating microarchitecture simulation via rigorous statistical sampling\" - [b1] : This paper is referred to in relation to the methodology of simulation and the use of statistical sampling, which is a significant part of the given paper's approach.\n\n3. \"esesc a fast multicore simulator using time based sampling\" - [b9] : The given paper discusses the limitations of time-based sampling methodologies like [b9] and proposes a new approach, indicating that [b9] is a significant reference.\n\n4. \"sampled simulation of multi threaded applications\" - [b10] : The given paper discusses the limitations of [b10] and proposes a new approach, indicating that [b10] is a significant reference.\n\n5. \"selecting software phase markers with code structure analysis\" - [b18] : The given paper refers [b18] in relation to the idea of using loop iterations as slices for single-threaded programs, which is a key concept in the given paper's methodology.\n\n6. \"pinballs portable and shareable user level checkpoints for reproducible analysis and simulation\" - [b19] : The given paper refers [b19] in relation to the use of checkpoints within its own methodology.\n\nHere is the output in the required json format:\n\n{\n\"b0\": 1,\n\"b1\": 0.9,\n\"b9\": 0.8,\n\"b10\": 0.8,\n\"b18\": 0.9,\n\"b19\": 0.9\n}", "Based on the provided information, the key references that contributed to the paper's novel contributions appear to be:\n\n- 'b0': 'automatically characterizing large scale program behavior'\n- 'b1': 'smarts accelerating microarchitecture simulation via rigorous statistical sampling'\n- 'b9': 'esesc a fast multicore simulator using time based sampling'\n- 'b10': 'sampled simulation of multi threaded applications'\n- 'b11': 'barrierpoint sampled simulation of multi threaded applications'\n- 'b12': 'taskpoint sampled simulation of task based programs'\n- 'b18': 'selecting software phase markers with code structure analysis'\n- 'b15': 'pinplay a framework for deterministic replay and reproducible analysis of parallel programs'\n\nThe paper mentions these references in the context of developing the LoopPoint methodology, which is a core contribution of the paper. The paper also directly discusses the limitations of the methods proposed in these references and how their work aims to overcome these limitations.\n\nHere is the result in json format:\n\n{\n \"b0\": 0.8,\n \"b1\": 0.8,\n \"b9\": 0.9,\n \"b10\": 0.9,\n \"b11\": 0.75,\n \"b12\": 0.75,\n \"b15\": 0.85,\n \"b18\": 0.85\n} \n\nThe confidence scores are estimated based on the relevance of the reference to the paper's main contributions and the extent of their discussion in the text. The scores are on a scale of 0 to 1, with 1 indicating the highest relevance."], "62d16e8a5aee126c0fd684cf": ["Based on the text of the paper and the reference titles, the following references seem to be the most relevant source papers that likely had the most influence on this paper:\n\n1. \"Data reorganization in memory using 3D-stacked DRAM\" [b0]\n2. \"High Bandwidth Memory (HBM) DRAM\" [b22]\n3. \"DREAM: Dynamic Re-arrangement of Address Mapping to improve the performance of DRAMs\" [b16]\n4. \"Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories\" [b31]\n5. \"Managing DRAM latency divergence in irregular GPGPU applications\" [b10]\n6. \"A bandwidth-aware memory subsystem resource management using non-invasive resource profilers for large CMP systems\" [b26]\n7. \"A case for richer cross-layer abstractions: Bridging the semantic gap with expressive memory\" [b41]\n\nThis is concluded based on the fact that these papers seem to most directly deal with the primary challenges outlined in the paper and the algorithm proposed by the author as they pertain to 3D memory, address mapping, hardware and software mechanisms, and performance optimization. \n\nHere is the result in the json format:\n\n{\n\"b0\": 1,\n\"b22\": 0.9,\n\"b16\": 0.9,\n\"b31\": 0.85,\n\"b10\": 0.85,\n\"b26\": 0.8,\n\"b41\": 0.8\n} \n\nPlease note that the confidence scores are subjective and are based on the information available in the text. Further detailed analysis of the paper and the reference papers could lead to more accurate scores.", "The given paper discusses the concept of a software-defined address mapping (SDAM) that employs a coarse-grained chunk-based address mapping management while achieving fine-grained data placement in hardware to fully exploit the CLP in 3D memory. The paper mentions that it is based on the understanding that a limited number of major variables in a program have a large memory footprint and contribute to the vast majority of external memory accesses in hardware. \n\nBased on the language used and the references provided, we can identify the following source papers:\n\n- 'Data reorganization in memory using 3D stacked DRAM' [b0]: The paper seems to have been inspired by this work as it mentions hardware-only methods that control data placement across channels/banks/rows to improve bandwidth utilization are hardware-based via the direct control of PA-to-HA address mapping. Confidence Score: 0.9\n- 'Compiler directed page coloring for multiprocessors' [b7]: This paper might have inspired the authors as it discusses software-only methods that use OS support to more intelligently place data in physical memory by changing the VA-to-PA translation in virtual memory (VM) according to the access patterns of data structures. Confidence Score: 0.8\n- 'Cache conscious structure layout' [b13]: The term 'cache conscious' is mentioned in the text, which indicates this paper might have played a role in the development of the authors' ideas. Confidence Score: 0.7\n- 'Physical address decoding in Intel Xeon E5 v3 CPUs: A supplemental datasheet' [b19]: This paper could have inspired the authors' exploration of the address mapping and its impact on the utilization of channel-level parallelism (CLP). Confidence Score: 0.7\n- 'Understanding object-level memory access patterns across the spectrum' [b23]: This paper mentions variables/data structures in a program that may be accessed concurrently and use the information to direct the operating system's VM page mapping strategy to place them to physical pages that do not contend for the same cache line, thereby reducing conflict misses. Confidence Score: 0.8\n- 'High Bandwidth Memory (HBM) DRAM' [b22]: This source could have provided necessary technical details for the authors, as it is directly mentioned in the context of 3D-memory. Confidence Score: 0.9\n\nIn JSON format:\n{'b0': 0.9, 'b7': 0.8, 'b13': 0.7, 'b19': 0.7, 'b23': 0.8, 'b22': 0.9}", "Based on the provided text and the guidelines, the source papers that inspired the author most are:\n\n1. \"Data reorganization in memory using 3D stacked DRAM\" [b0] - Confidence: 0.85\n2. \"A software memory partition approach for eliminating bank-level interference in multicore systems\" [b28] - Confidence: 0.8\n3. \"Managing DRAM latency divergence in irregular GPGPU applications\" [b10] - Confidence: 0.85\n4. \"Efficient parallel graph exploration on multi-core CPU and GPU\" [b20] - Confidence: 0.75\n5. \"High Bandwidth Memory (HBM) DRAM\" [b22] - Confidence: 0.9\n6. \"Heterogeneous memory architectures: a HW/SW approach for mixing die-stacked and off-package memories\" [b31] - Confidence: 0.8\n7. \"A bandwidth aware memory subsystem resource management using non-invasive resource profilers for large CMP systems\" [b26] - Confidence: 0.7\n\nThe confidence scores are estimated based on the frequency and context of the citations in the paper text. The higher the score, the more likely the paper was a major source of inspiration for the authors. \n\nHowever, this is an inference based on the text, and the actual level of inspiration may vary. \n\nHere is the result in json format:\n\n{\n\"b0\": 0.85,\n\"b28\": 0.8,\n\"b10\": 0.85,\n\"b20\": 0.75,\n\"b22\": 0.9,\n\"b31\": 0.8,\n\"b26\": 0.7\n}", "Based on the provided text and the author's notes, the following referenced papers seem to have had the most influence on the paper's core contributions:\n\n1. \"Data reorganization in memory using 3D stacked DRAM\" [b0]: This paper appears to be the main source of inspiration as it's mentioned multiple times in the context of hardware-only methods for data placement and the use of bit-shuffle mechanisms in the Address Mapping Unit (AMU).\n\n2. \"A bandwidth aware memory subsystem resource management using non-invasive resource profilers for large CMP systems\" [b26]: The authors seem to have gotten inspiration from this paper to handle the challenge of channel-level parallelism (CLP) and memory bandwidth utilization.\n\n3. \"Compiler directed page coloring for multiprocessors\" [b7]: This paper is referenced in the context of software-only methods for addressing data placement issues. It seems to have influenced the authors' approach to address mapping management at a coarse-grained level.\n\n4. \"Cache-conscious data placement\" [b8]: This paper is also mentioned under software-only methods and appears to be a source of inspiration for the authors' approach to data placement.\n\n5. \"Can't touch this: Software-only mitigation against RowHammer attacks targeting kernel memory\" [b6]: The authors reference this paper when discussing potential security applications of their proposed method.\n\n6. \"High Bandwidth Memory (HBM) DRAM\" [b22]: This paper seems to be a key source of inspiration, as it's mentioned multiple times and appears to have informed the authors' understanding of 3D-memory and its potential for increasing memory bandwidth.\n\n7. \"Boom v2.0: an open-source out-of-order RISC-V core\" [b9]: This paper is mentioned in the context of the authors' implementation of their proposed method and seems to have influenced their choice of hardware.\n\n8. \"Get out of the valley: Power-efficient address mapping for GPUs\" [b29]: This paper is mentioned in the context of hardware-only methods, and it's possible the authors took inspiration from this paper for their approach to address mapping.\n\nThe confidence scores would need to be calculated based on the frequency and context of the references in the text. However, this requires a more detailed analysis than can be provided here.\n\nIn JSON format, the result might look something like this:\n\n{\n   \"b0\": 0.9,\n   \"b26\": 0.8,\n   \"b7\": 0.7,\n   \"b8\": 0.7,\n   \"b6\": 0.6,\n   \"b22\": 0.9,\n   \"b9\": 0.8,\n   \"b29\": 0.7\n}", "Based on the text of the academic paper, the source papers that likely inspired the authors are:\n\n1. \"Data reorganization in memory using 3D-stacked DRAM\" [b0]\n2. \"Compiler-directed page coloring for multiprocessors\" [b7]\n3. \"Cache-conscious data placement\" [b8]\n4. \"Classifying memory access patterns for prefetching\" [b2]\n5. \"A bandwidth aware memory subsystem resource management using non-invasive resource profilers for large CMP systems\" [b26]\n6. \"DREAM: Dynamic Re-arrangement of Address Mapping to improve the performance of DRAMs\" [b16]\n7. \"Get Out of the Valley: Power-Efficient Address Mapping for GPUs\" [b29]\n8. \"High Bandwidth Memory (HBM) DRAM\" [b22]\n9. \"Hybrid Memory Cube (HMC)\" [b36]\n\nThis is based on the following reasoning:\n\n- [b0] is referred to directly in the text, indicating it likely had a significant influence on the research.\n- [b7] and [b8] are mentioned as software-only methods for addressing data placement issues, which the authors aim to improve upon.\n- [b2] could have inspired the machine learning methods the authors use to learn access patterns.\n- [b26] could have inspired the authors' focus on maximizing bandwidth utilization.\n- [b16] and [b29] are hardware-only methods that the authors aim to integrate with software methods for better memory performance.\n- [b22] and [b36] are the key papers about 3D memory, the main focus of the authors' research.\n\nThe confidence score for each source paper can be calculated based on the number of times each paper is mentioned and the context of those mentions. For example, [b0] might have a confidence score of 1.0 as it is clearly central to the research, while [b2] might have a lower confidence score as it is less directly related to the authors' main contributions.\n\nThe JSON output could be:\n\n{\n    \"b0\": 1.0,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b2\": 0.6,\n    \"b26\": 0.7,\n    \"b16\": 0.8,\n    \"b29\": 0.8,\n    \"b22\": 0.9,\n    \"b36\": 0.9\n}"], "628704555aee126c0f5c7953": ["Based on the provided text and instructions, we need to identify the source papers that have directly influenced the novel contributions of the paper in question, which proposes a profile-guided software prefetching method named APT-GET. The paper claims an average execution time speedup of 1.30\u00d7 and outperforms the state-of-the-art software prefetching mechanism [b5] by providing an average 1.25\u00d7 greater speedup.\n\nTo find the most relevant source papers, we must look for key phrases that indicate inspiration or motivation from previous research, along with methodological similarities, comparisons with previous work, and direct influences on the novel contributions of the paper.\n\nHere's the analysis based on the provided information:\n\n1. The paper's novel contribution is a profile-guided software prefetching mechanism (APT-GET) that outperforms [b5].\n2. The paper targets the challenge of optimizing prefetching timeliness by finding the optimal prefetch-distance and prefetch injection site.\n3. The paper uses Last Branch Record (LBR) hardware support [b66] to collect dynamic profiling information.\n4. The paper contrasts the proposed method with static techniques [b5] and points out the limitations of state-of-the-art static prefetching methods.\n\nGiven the emphasis on profiling and the use of hardware support for dynamic information gathering, the key references that seem to have directly influenced the novel contributions of the paper are [b5] for the state-of-the-art software prefetching mechanism and [b66] for the LBR hardware support.\n\nThe following JSON object presents the inferred source papers with confidence scores, assuming a confidence of 1.0 for direct mentions and influence, and lower confidence scores for other references that may have been influential but are not explicitly stated as inspirations in the text:\n\n```json\n{\n  \"b5\": 1.0,\n  \"b66\": 1.0\n}\n```\n\nThese references are prioritized because they are directly involved in the methodology that APT-GET improves upon (reference [b5]) and the hardware feature it leverages (reference [b66]). Since the paper claims to significantly outperform [b5], it is a central reference for understanding the improvements made by the proposed method. Additionally, the use of LBR ([b66]) is crucial for the proposed profiling methodology, which is a key aspect of the paper's contribution. Other references in the text are either setting the context or providing background and are not directly tied to the novel contributions of the paper.", "Based on the provided text and the author's notes, we can infer the following about the paper in question:\n\n1. The paper introduces a novel method called APT-GET, which is a profile-guided mechanism that ensures the timeliness of software prefetch operations.\n\n2. The method is designed to find the optimal values for two key parameters: prefetch-distance and prefetch injection site, leveraging existing hardware support such as Intel's Last Branch Record (LBR).\n\n3. The paper claims a significant performance improvement over state-of-the-art software prefetching mechanisms, particularly when benchmarked against memory-latency-bound applications such as graph workloads.\n\nTo identify the most relevant source papers based on the key phrases and criteria provided, we examine the text for direct mentions of inspiration, motivation, or comparison. Below is the JSON formatted list of source papers, including confidence scores based on their perceived influence on the paper's novel contributions:\n\n```json\n{\n  \"b5\": 0.9,\n  \"b66\": 0.8,\n  \"b35\": 0.7,\n  \"b67\": 0.6,\n  \"b1\": 0.6,\n  \"b23\": 0.5,\n  \"b119\": 0.5\n}\n```\n\nHere's the rationale behind the choices:\n\n- b5: The paper directly compares its approach to the state-of-the-art software prefetching mechanism described in this reference, thus indicating a high level of influence and relevance.\n- b66: The novel approach of APT-GET relies on hardware support from Intel's LBR, which is detailed in this reference, suggesting a strong methodological connection.\n- b35: The paper discusses profiling the latency distribution of delinquent loads, which is a concept likely expanded upon from the ideas presented in this reference.\n- b67: Given that the paper discusses performance analysis tools like `perf stat`, this reference likely provided background on the tools used for the empirical evaluation of the proposed method.\n- b1: The title suggests relevance to software prefetching, and given that the paper is about a profile-guided software prefetching mechanism, it is reasonable to infer some level of influence.\n- b23: Compiler optimizations for data locality are likely relevant to this paper's focus on prefetching, thus it could have provided foundational insights.\n- b119: This reference discusses compiler-driven dependence profiling, which could have been influential in the development of the paper's profiling methodology.\n\nThese confidence scores are speculative and based on the context provided in the text, the titles of the references, and the specific criteria outlined in the task description. The scores suggest the level of direct relevance and influence each reference has on the novel contributions of the paper being analyzed.", "Based on the provided text and author notes, we need to identify the source papers that have directly inspired the novel contributions of the paper, specifically a profile-guided software prefetching method that leverages the hardware-provided Last Branch Record (LBR) to estimate the actual load-to-use time of delinquent loads within loops, aiding decision-making for where to insert prefetch instructions. This method demonstrates a performance improvement over static compiler analysis-based software prefetching, particularly in workloads such as graph processing.\n\nFrom the text, we can infer that [b5] is a key reference since the novel contributions of the paper aim to significantly outperform the state-of-the-art software prefetching mechanism described in [b5]. Other references might be related to the challenges and existing techniques in software prefetching but may not directly inspire the novel contributions of the paper. The text also suggests that the hardware feature LBR ([b66]) is crucial for the proposed method's profiling approach.\n\nBased on the criteria provided, we should prioritize references that are directly linked to the novel contributions of the paper. The confidence score reflects how directly each reference contributed to the paper's novel contributions. Here is the list of source papers in JSON format with confidence scores:\n\n```json\n{\n  \"b5\": 1.0,\n  \"b66\": 0.9\n}\n```\n\nThe confidence score for [b5] is set to 1.0 because it is directly mentioned as the state-of-the-art mechanism that the paper aims to outperform. The confidence score for [b66] is slightly lower at 0.9 because, while it is essential for the proposed method, it is a hardware feature and not a paper that directly inspired the novel algorithmic contributions. If additional references were mentioned in the full text that directly inspired the novel contributions, they would be listed with appropriate confidence scores. However, based on the excerpt provided and the task instructions, these are the relevant source papers.", "Based on the given text and author notes, we can infer that the paper's novel contributions are centered around a profiling-based software prefetching approach that utilizes hardware-provided Last Branch Record (LBR) to infer the actual load-to-use time of delinquent loads within loops. This aids in making decisions about where to insert prefetches. The paper's approach reportedly achieves more than 20% performance improvement over static compiler analysis-based software prefetching methods for benchmark scenarios that are friendly to indirect prefetching, such as graph workloads.\n\nTo identify the source papers that most directly inspired these contributions, we need to look for references cited in the context of the novel contributions and those that are referred to in a way that indicates direct inspiration or methodological similarity.\n\nFrom the provided text, I will extract the most relevant references based on the criteria outlined earlier:\n\n1. The state-of-the-art software data prefetching mechanism that falls significantly short of ideal data prefetcher due to lack of prefetch timeliness: [b5]\n2. The need for dynamic information such as execution time for effective prefetching: [b5]\n3. The inefficacy of existing static software-based solutions and the requirement for profiling to determine prefetch-distance and prefetch injection site: [b5]\n4. The performance impact of prefetching various distances for indirect memory accesses and the variability of optimal prefetch-distance based on work function complexity: [b5]\n5. The use of Intel's hardware prefetchers and their limitations for predicting irregular memory addresses: [b111]\n6. The existing compiler-based automatic prefetch injection schemes, and their limitations: [b5]\n7. The use of Last Branch Record (LBR) feature by Intel CPUs for profile collection: [b66]\n8. The state-of-the-art static techniques for predicting execution time and their associated errors: [b0], [b25], [b54], [b69], [b70], [b83]\n\nGiven these points, we can compile the JSON object:\n\n```json\n{\n  \"b5\": 1.0,\n  \"b111\": 0.8,\n  \"b66\": 0.8,\n  \"b0\": 0.6,\n  \"b25\": 0.6,\n  \"b54\": 0.6,\n  \"b69\": 0.6,\n  \"b70\": 0.6,\n  \"b83\": 0.6\n}\n```\n\nThe confidence scores are subjective and based on the context in which the references are mentioned. Reference [b5] is given the highest score as it is referenced multiple times in direct relation to the paper's contributions. References [b111] and [b66] are given high scores due to their mention in the description of the proposed method. The rest of the references are given slightly lower scores as they pertain to the background and limitations of existing methods, which informed the development of the novel approach but are not directly part of the novel contribution itself.", "Based on the information provided, the task is to determine the source papers that inspired the novel contributions of the given paper, which proposes a \"profile-guided mechanism to ensure the timeliness of software prefetch operations.\" This mechanism, called APT-GET, leverages Last Branch Record (LBR) hardware support to improve the performance of software prefetching for indirect memory accesses and achieves on average a 1.30\u00d7 speedup across evaluated applications.\n\nFrom the provided text, we can infer several key points that will help us identify the most relevant source papers:\n\n1. The paper seeks to address the shortcomings of existing automated software prefetching techniques, which fail to prefetch memory blocks in a timely manner ([b5]).\n2. The proposed method, APT-GET, uses a profiling methodology leveraging existing hardware support (such as Intel's LBR [b66]) to determine optimal prefetch-distance and prefetch injection site.\n3. The paper evaluates the effectiveness of APT-GET against real-world, memory-latency-bound applications and compares it to the state-of-the-art software prefetching mechanism ([b5]).\n\nGiven these points, we should prioritize references that are methodologically similar or have a strong influence on the novel contributions of the paper. Here is the JSON-formatted list of potential source papers with confidence scores based on the provided criteria:\n\n```json\n{\n  \"b5\": 1.0,\n  \"b66\": 0.9\n}\n```\n\nExplanation:\n- Reference [b5], titled \"Software prefetching for indirect memory accesses,\" is directly compared to APT-GET and is likely the most pertinent source paper, given that the novel contributions are designed to outperform this specific prefetching mechanism (confidence score: 1.0).\n- Reference [b66], titled \"An introduction to last branch records,\" is relevant as it provides the hardware support that APT-GET uses for profiling, making it a key enabler of the novel approach (confidence score: 0.9).\n\nOther references are mentioned in the context of the challenges faced by prefetching mechanisms, historical findings, and related work, but they do not seem to be explicitly indicated as a direct inspiration for the novel contributions of APT-GET. Therefore, they are not included in the list."], "6287044a5aee126c0f5c33ff": ["Based on the provided text, it appears that the paper takes influence from multiple sources. The authors explicitly mention and discuss several methods to tackle the problem of improving the reliability of uncertainty estimates in multimodal stress detection models under distribution shifts, including \"Deep ensembles\", \"Focal Loss\", and \"Manifold Mixup\".\n\nThe references directly related to these methods are:\n\n- \"Deep ensembles\" is proposed by [b19]: \"Simple and scalable predictive uncertainty estimation using deep ensembles\"\n- \"Focal Loss\" is defined by [b20]: \"Focal loss for dense object detection\" and its calibration properties are discussed by [b25]: \"Calibrating deep neural networks using focal loss\"\n- \"Manifold Mixup\" is introduced by [b33]: \"Manifold mixup: Better representations by interpolating hidden states\"\n\nBesides these methods, the authors also use the WESAD dataset [b29]: \"Introducing WESAD, a multimodal dataset for wearable stress and affect detection\" to conduct their research. \n\nTherefore, the list of source papers based on the given text would be:\n\n```json\n{\n\"b19\": 1,\n\"b20\": 0.8,\n\"b25\": 0.8,\n\"b33\": 0.9,\n\"b29\": 0.9\n}\n```\n\nThe confidence scores are assigned based on the level of direct influence each paper has on the core contributions of the given paper.", "Based on the text and the given criteria, the source papers that inspired the given academic paper are most likely:\n\n1. \"Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift\" - [b26] - This paper is likely a source because it was mentioned in the context of comparing approaches to improve calibration. It is directly relevant to the paper's core contributions.\n\n2. \"Deep Residual Learning for Image Recognition\" - [b10] - This paper is likely a source because it was mentioned in the context of discussing the architecture used in the paper's study. It directly influenced the methodology of the paper.\n\n3. \"Simple and scalable predictive uncertainty estimation using deep ensembles\" - [b19] - This paper is likely a source because the method it proposed was directly used and evaluated in the paper's study. It directly influenced the methodology of the paper.\n\n4. \"Mixup: Beyond Empirical Risk Minimization\" - [b35] - This paper is likely a source because it was mentioned in the context of discussing regularization methods used in the paper's study. It directly influenced the methodology of the paper.\n\n5. \"Focal Loss for Dense Object Detection\" - [b20] - This paper is likely a source because it was mentioned in the context of discussing the loss function used in the paper's study. It directly influenced the methodology of the paper.\n\n6. \"Manifold Mixup: Better Representations by Interpolating Hidden States\" - [b33] - This paper is likely a source because it was mentioned in the context of discussing the regularization method used in the paper's study. It directly influenced the methodology of the paper.\n\n7. \"Introducing WESAD, a Multimodal Dataset for Wearable Stress and Affect Detection\" - [b29] - This paper is likely a source because the dataset it introduced was used in the paper's study. It directly influenced the methodology of the paper.\n\nThe JSON file would look like this:\n```\n{\n    \"b26\": 1,\n    \"b10\": 1,\n    \"b19\": 1,\n    \"b35\": 1,\n    \"b20\": 1,\n    \"b33\": 1,\n    \"b29\": 1\n}\n```\nThe confidence score is 1 for all references, as they all directly influenced the methodology of the paper and were used in the context of solving the primary challenges outlined in the paper.", "Based on the text, the most pertinent source papers that inspired this work are:\n\n1. \"Towards reliable multimodal stress detection under distribution shift\" - The title of this paper directly reflects the main focus of the given paper's topic, which is to improve the reliability of multimodal stress detection models when the data distribution changes. Confidence Score: 0.9\n\n2. \"Can we ditch feature engineering end to end deep learning for affect recognition from physiological sensor data\" - The given paper's approach of using end-to-end models for stress detection from physiological data aligns closely with this source paper. Confidence Score: 0.6\n\n3. \"Simple and scalable predictive uncertainty estimation using deep ensembles\" - The given paper uses deep ensembles as one of the approaches to improve uncertainty estimates, which originates from this source paper. Confidence Score: 0.8\n\n4. \"Focal Loss for dense object detection\" - The given paper uses Focal Loss as one of the approaches to improve uncertainty estimates, which originates from this source paper. Confidence Score: 0.8\n\n5. \"Manifold Mixup: Better representations by interpolating hidden states\" - The given paper uses Manifold Mixup as one of the approaches to improve uncertainty estimates, which originates from this source paper. Confidence Score: 0.8\n\n6. \"Introducing WESAD, a multimodal dataset for wearable stress and affect detection\" - The given paper uses the WESAD dataset for their experiments, which is introduced in this source paper. Confidence Score: 0.8\n\n7. \"Time series classification from scratch with deep neural networks: A strong baseline\" - The given paper uses time series data and deep neural networks for classification, which aligns with the methods introduced in this source paper. Confidence Score: 0.7\n\n8. \"Algorithms for hyper-parameter optimization\" - The given paper uses a hyperparameter search using a 3-fold cross validation on the training fold, which aligns with the methods introduced in this source paper. Confidence Score: 0.7\n\nReturned result in json format:\n\n{ \"b0\": 0.7, \"b3\": 0.6, \"b19\": 0.8, \"b20\": 0.8, \"b29\": 0.8, \"b33\": 0.8, \"b34\": 0.7, \"none\": 0.9 }", "Based on the text and notes given, the main challenge of the paper is to examine the robustness of multimodal stress detection models when the data distribution of the inputs changes, with a focus on improving the reliability of uncertainty estimates. The paper discusses three popular methods for improving uncertainty estimates in multimodal stress detection tasks: Deep ensembles, Focal Loss, and Manifold Mixup. \n\nFrom the text, the key phrases indicating the source papers can be identified as follows:\n\n1. \"Deep ensembles, proposed by [b19]\" - The paper takes inspiration from this work on deep ensembles for improving calibration on data shifts.\n2. \"Focal Loss can improve calibration [b25]\" - The paper uses insights from this work on the applicability of Focal Loss in improving calibration.\n3. \"In terms of regularization, we use Manifold Mixup [b33]\" - The paper uses the Manifold Mixup method from this work for regularization purposes.\n4. \"we apply deep ensembles, which have shown great success in terms of calibration on data shifts [b26]\" - The paper is inspired by this work that has shown the success of deep ensembles in terms of calibration on data shifts.\n5. \"As argued and empirically shown by [b25]\" - The paper references this work as empirical evidence for the effect of Focal Loss on calibration.\n\nBased on the identified key phrases and their associated references, the following source papers are identified for the given academic paper:\n\n1. \"Simple and scalable predictive uncertainty estimation using deep ensembles [b19]\" - Confidence Score: 0.9\n2. \"Calibrating deep neural networks using focal loss [b25]\" - Confidence Score: 0.9\n3. \"Manifold Mixup: better representations by interpolating hidden states [b33]\" - Confidence Score: 0.8\n4. \"Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift [b26]\" - Confidence Score: 0.8\n\nOutput in JSON format:\n{\n    \"b19\": 0.9,\n    \"b25\": 0.9,\n    \"b33\": 0.8,\n    \"b26\": 0.8\n}", "Based on the given information, the source papers that most likely inspired this paper are:\n\n1. \"Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift\" [b26]: This paper is likely a significant source of inspiration as it directly addresses the challenge of predictive uncertainty under distribution shifts, which is a key focus of the given paper.\n   \n2. \"Simple and scalable predictive uncertainty estimation using deep ensembles\" [b19]: The given paper's method of using deep ensembles for improving uncertainty estimates, which is a key approach used in this source paper.\n\n3. \"On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks\" [b32]: This paper is likely a source paper since the given paper applies Mixup as a regularization method for improving the reliability of uncertainty estimates.\n\n4. \"Calibrating Deep Neural Networks using Focal Loss\" [b25]: This is likely a source paper as the given paper also uses Focal Loss as a method to improve the reliability of uncertainty estimates.\n\n5. \"Towards reliable multimodal stress detection under distribution shift\" [None]: As the given paper also focuses on stress detection under distribution shifts, this unnamed source paper likely inspired it.\n\n6. \"Manifold Mixup: Better Representations by Interpolating Hidden States\" [b33]: This paper is likely a source paper since the given paper applies Manifold Mixup as a regularization method for improving the reliability of uncertainty estimates.\n\n7. \"Introducing WESAD, a multimodal dataset for wearable stress and affect detection\" [b29]: This paper is possibly a source paper as it provides the WESAD dataset used in the given paper.\n\nHere is the result in json format:\n\n{\n    \"b26\": 1,\n    \"b19\": 0.9,\n    \"b32\": 0.9,\n    \"b25\": 0.9,\n    \"None\": 0.8,\n    \"b33\": 0.8,\n    \"b29\": 0.7\n}"], "62d7a7d15aee126c0f3fe03e": ["```json\n{\n  \"b28\": 1.0,\n  \"b59\": 0.9,\n  \"b61\": 0.9,\n  \"b19\": 0.7,\n  \"b74\": 0.6,\n  \"b77\": 0.6,\n  \"b105\": 0.5,\n  \"b106\": 0.5\n}\n```\n\nExplanation:\n\n- `b28` (\"a study of replacement algorithms for a virtual storage computer\"): The paper addresses the challenge of BTB capacity and replacement strategies and refers to Belady's optimal replacement strategy, which is a key method used in the paper for profiling branches to determine which should be cached.\n  \n- `b59` (\"back to the future leveraging belady s algorithm for improved cache replacement\"): This reference is strongly connected to the idea of using Belady's algorithm to inform cache replacement policies.\n\n- `b61` (\"high performance cache replacement using re reference interval prediction rrip\"): Given that the paper discusses replacement policies and the RRIP is mentioned as a prior technique, this reference is likely relevant.\n\n- `b19` (\"exploring predictive replacement policies for instruction cache and branch target buffer\"): This is directly related to the paper's focus on predictive replacement policies for the BTB.\n\n- `b74` (\"twig profile guided btb prefetching for data center applications\"): The paper focuses on BTB prefetching and profiling, which is relevant to the paper's novel contributions.\n\n- `b77` (\"ripple profile guided instruction cache replacement for data center applications\"): This reference is related to profiling for cache replacement, similar to the approach used in the paper.\n\n- `b105` (\"bolt a practical binary optimizer for data centers and beyond\") and `b106` (\"lightning bolt powerful fast and scalable binary optimization\"): These references are related to profile-guided optimizations, which are a part of the proposed solution in the paper.\n\nThe confidence scores reflect the direct relevance of each source paper to the novel contributions of the paper in question. A higher score indicates a stronger direct influence or methodological similarity. The scores are subjective estimates based on the given information and may vary with a deeper analysis of the full text of each cited work.", "Based on the author notes and the provided text, let's analyze the paper step by step to identify the most pertinent source papers that have directly influenced the main contributions of the paper:\n\n1. **Primary Challenges Outlined**:\n   - Data center applications induce significant stalls in the processor pipeline due to large instruction footprints.\n   - The Branch Target Buffer (BTB) capacity is often insufficient, leading to frontend stalls and performance losses.\n\n2. **Algorithm Proposed**:\n   - The authors propose \"Thermometer,\" a novel BTB replacement technique that leverages profiling to determine which branches should be prioritized in the BTB cache.\n   - This technique uses profiling to simulate Belady's optimal replacement strategy offline and tag branches as 'important' based on their hit rate under this strategy.\n\n3. **Key Phrases Indicating Strong Reliance on Previous Research**:\n   - The paper does not explicitly use phrases like \"inspired by\" or \"motivated by,\" but it does mention comparisons with other works and the improvements Thermometer provides over them.\n   - The text mentions several prior works and their limitations, such as the performance of FDIP being limited by BTB misses and existing BTB prefetching mechanisms falling short.\n\n4. **Directly Linked References to Main Challenges and Methods**:\n   - Reference [b28] is cited multiple times in context with the optimal replacement policy, which is central to the authors' method. This suggests that the work related to Belady's algorithm is directly relevant.\n   - References [b19], [b59], and [b61] are discussed in the context of existing BTB replacement policies, and their limitations are highlighted before introducing Thermometer, which implies these works are directly relevant to the authors' contributions.\n\n5. **References Preceded by Phrases Indicating Direct Inspiration or Motivation**:\n   - Although no explicit phrases like \"inspired by [reference]\" are used, the paper's methodology is built upon the comparison and shortcomings of previous BTB replacement techniques.\n\nBased on this analysis and the titles of the references, the source papers that are most central to the main contributions of the paper and have heavily influenced its development are likely:\n\n- [b28]: \"A study of replacement algorithms for a virtual storage computer\"\n- [b19]: \"Exploring predictive replacement policies for instruction cache and branch target buffer\"\n- [b59]: \"Back to the future leveraging belady s algorithm for improved cache replacement\"\n- [b61]: \"High performance cache replacement using re reference interval prediction (rrip)\"\n\nHere is the JSON result with the reference number as the key and the confidence score between 0 and 1 as the value:\n\n```json\n{\n  \"b28\": 0.9,\n  \"b19\": 0.8,\n  \"b59\": 0.8,\n  \"b61\": 0.8\n}\n```\n\nThe confidence scores reflect the direct relevance and influence of these works on the paper's contributions, with a higher score indicating a more central role.", "Based on the provided information, I will identify the source papers that the given paper relies on most for its primary contributions. The novel contributions of the paper are:\n\n1. A characterization of branch behavior in data center applications that takes into account both holistic and transient patterns of branches.\n2. The introduction of the concept of \"branch temperature\" based on the hit-to-taken percentage of a branch under the optimal BTB replacement policy.\n3. The proposal of \"Thermometer,\" a novel BTB replacement technique that leverages profile-guided analysis to classify branches and guide BTB replacement policy.\n\nTo find the most pertinent source papers, I will look for references that are directly related to these contributions and are indicated by phrases that show the authors were inspired by or motivated by previous work.\n\nFrom the text, here are the references that seem to be most relevant:\n\n- *Belady's optimal replacement policy* is mentioned multiple times and is a key part of the strategy for determining which branches to cache. This is foundational for the paper's technique, and the reference for Belady's algorithm is [b28].\n  \n- *Profile-Guided Optimizations (PGO)* are leveraged for their effectiveness in the work, which suggests that references [b32], [b46], and [b105] are quite relevant. However, these are also mentioned more as a general background, so I will look for more direct references.\n\n- *Intel PT* ([b0]) is used for collecting the trace of executed branch instructions, which is a crucial part of the profile-guided analysis.\n\n- References to *prior work on BTB replacement policies* and their limitations are directly critiqued to motivate the novel contributions of the paper. These include references to GHRP [b19], Hawkeye [b59], and SRRIP [b61].\n\n- *ChampSim* ([b4]) is used for simulation purposes in the evaluation of Thermometer, which indicates its importance in demonstrating the practicality of the proposed solution.\n\n- References to *prior work on data center applications* that show the importance of the BTB's hit rate, such as [b74], [b76], [b77], and [b131], are significant as they directly inspire the need for Thermometer.\n\nBased on the information and the methodological similarities, here is the JSON output with the reference number as the key and the confidence score between 0 and 1 as the value:\n\n```json\n{\n  \"b28\": 1.0,\n  \"b0\": 0.9,\n  \"b19\": 0.95,\n  \"b59\": 0.95,\n  \"b61\": 0.95,\n  \"b4\": 0.85,\n  \"b74\": 0.8,\n  \"b76\": 0.8,\n  \"b77\": 0.8,\n  \"b131\": 0.8\n}\n```\n\nI've assigned a confidence score of 1.0 to [b28] as it seems to be the foundational work that the authors build upon. Other references related to PGO and BTB have slightly lower scores, reflecting their direct but perhaps not as foundational influence on the paper's contributions. The references [b74], [b76], [b77], and [b131] are given a confidence score of 0.8 to reflect their relevance to the challenges addressed by the paper, but they are not as central to the novel contributions as [b28] or the methods directly critiqued such as [b19], [b59], and [b61].", "```json\n{\n  \"b28\": 1.0,\n  \"b0\": 0.9,\n  \"b19\": 0.9,\n  \"b59\": 0.9,\n  \"b61\": 0.9,\n  \"b74\": 0.8,\n  \"b82\": 0.8\n}\n```\n\nExplanation:\n- `b28` (\"a study of replacement algorithms for a virtual storage computer\") is mentioned as the source for the optimal replacement policy (Belady's algorithm), which is central to the paper's contribution. Confidence: 1.0.\n- `b0` (\"adding processor trace support to linux\") is related to the profiling process used in the paper's proposed solution. Confidence: 0.9.\n- `b19`, `b59`, and `b61` are all mentioned as prior works on replacement policies that the paper seeks to improve upon with Thermometer. Confidence: 0.9.\n- `b74` (\"twig profile guided btb prefetching for data center applications\") and `b82` (\"blasting through the front end bottleneck with shotgun\") are previous works on BTB prefetching, which the paper's work is built upon and tries to outperform. Confidence: 0.8. \n\nThe confidence scores reflect the centrality of these references to the novel contributions of the paper. They are not strictly quantifiable but are an estimation based on the criteria provided.", "To identify the source papers that most inspired the given paper, we need to analyze the provided text and look for key phrases that indicate reliance on previous research for the development of the paper's contributions. Given the author notes and paper text, we can infer that the novel contributions are related to the profiling of branches within a Branch Target Buffer (BTB) and using Profile-Guided Optimizations (PGO) to guide the replacement strategy.\n\nBased on the text, let's extract the references that seem to have a direct influence on the paper's novel contributions:\n\n1. The use of PGO techniques: [b32], [b46], [b105], [b106]\n2. The concept of holistic and transient reuse distance: [b36], [b61]\n3. The simulation of Belady's optimal replacement policy: [b28], [b60]\n4. The evaluation of BTB replacement policies: [b19], [b59]\n\nConsidering the paper's focus on BTB replacement and the use of profiling to guide this process, we should prioritize papers that discuss similar methodologies or structures. The references related to PGO, reuse distance, and replacement policies are the most likely to have directly influenced the paper's contributions.\n\nWe can now assign a confidence score to these references based on their perceived relevance to the paper's novel contributions:\n\n```json\n{\n  \"b32\": 0.9,\n  \"b46\": 0.9,\n  \"b105\": 0.9,\n  \"b106\": 0.9,\n  \"b36\": 0.8,\n  \"b61\": 0.8,\n  \"b28\": 0.9,\n  \"b60\": 0.9,\n  \"b19\": 0.9,\n  \"b59\": 0.9\n}\n```\n\nThe confidence scores are high for these references because they are either directly mentioned in the context of the novel contributions or are closely related to the methods used in the paper. The exact scores can be fine-tuned based on a more detailed analysis of the paper and the cited works, but the given scores reflect a strong influence based on the available information."], "62d7a7d15aee126c0f3fe05a": ["Based on the text of the academic paper and the provided titles of the references, the source papers that are most likely to have directly inspired this paper are:\n\n1. [b5]: \"an event triggered programmable prefetcher for irregular workloads\" - Confidence score: 0.8\n2. [b22]: \"impulse building a smarter memory controller\" - Confidence score: 0.75\n3. [b82]: \"livia data centric computing throughout the memory hierarchy\" - Confidence score: 0.7\n4. [b94]: \"phi architectural support for synchronization and bandwidth efficient commutative scatter updates\" - Confidence score: 0.85\n5. [b149]: \"minnow lightweight offload engines for worklist management and worklist directed prefetching\" - Confidence score: 0.8\n\nThese papers are related to the topics of cache optimization, data movement, and near-memory computing, which are central to the paper's contributions.", "Based on the text and the author's notes, the source papers for this paper seem to be:\n\n1. \"Spzip architectural support for effective data compression in irregular applications\" [b145]: This paper is likely a source as it discusses cache optimizations with emphasis on data transformations such as decompression, which is a central theme in the given paper.\n\n2. \"Elastic cgras\" [b58]: This paper is mentioned in the context of the proposed architecture for the general-purpose optimization of data movement.\n\n3. \"An event triggered programmable prefetcher for irregular workloads\" [b5]: This paper is likely a source as it discusses programmable engines near caches, which is a concept used in the given paper.\n\n4. \"Decoupled compressed cache: conciliating low tag implementation cost\" [b117]: This paper is likely a source as it discusses cache optimizations with emphasis on data transformations such as decompression, a concept that is also central in the given paper.\n\n5. \"Phi architectural support for synchronization and bandwidth efficient commutative scatter updates\" [b94]: This paper is likely a source as it discusses optimizations for commutative scatter-updates in graphs, which is a concept used in the given paper.\n\n6. \"HATS: efficient and flexible reconfigurable logic for near data processing\" [b91]: This paper is likely a source as it discusses a representative decoupled streaming application, a concept used in the given paper.\n\n7. \"Logtm log based transactional memory\" [b90]: This paper is likely a source as it discusses transactions on non-volatile memory, which is a concept used in the given paper.\n\n8. \"Last-level cache side channel attacks are practical\" [b80]: This paper is likely a source as it discusses cache side-channel attacks, which is a concept used in the given paper.\n\nSo, the source papers could be represented in JSON format as follows:\n\n{\n    \"b145\": 0.9,\n    \"b58\": 0.8,\n    \"b5\": 0.7,\n    \"b117\": 0.9,\n    \"b94\": 0.8,\n    \"b91\": 0.7,\n    \"b90\": 0.7,\n    \"b80\": 0.7\n}\nHere, the keys are the reference numbers and the values are the confidence scores.", "Based on the text and the titles of the references, the most pertinent source papers are:\n\n1. 'b82': 'livia data centric computing throughout the memory hierarchy'\n2. 'b104': 'opportunistic computing in gpu architectures'\n3. 'b106': 'base delta immediate compression practical data compression for on chip caches'\n4. 'b107': 'optimus prime accelerating data transformation in servers'\n5. 'b145': 'spzip architectural support for effective data compression in irregular applications'\n6. 'b149': 'minnow lightweight offload engines for worklist management and worklist directed prefetching'\n7. 'b131': 'ultra elastic cgras for irregular loop specialization'\n8. 'b131': 'single graph multiple flows energy efficient design alternative for gpgpus'\n\nThese papers seem to be the most directly related to the novel contributions of the paper, which revolve around reducing data movement and optimizing cache usage.\n\nHere is the output in JSON format:\n\n{\n    \"b82\": 0.85,\n    \"b104\": 0.80,\n    \"b106\": 0.90,\n    \"b107\": 0.95,\n    \"b145\": 0.88,\n    \"b149\": 0.86,\n    \"b131\": 0.87\n}", "Based on the text and the titles of the references, the most relevant source papers are:\n\n- \"Base delta immediate compression practical data compression for on chip caches\" [b106]\n- \"Data reorganization in memory using 3D stacked DRAM\" [b5]\n- \"Processing in memory the terasys massively parallel pim array\" [b43]\n- \"Exploiting commutativity to reduce the cost of updates to shared data in cache coherent systems\" [b151]\n- \"An event triggered programmable prefetcher for irregular workloads\" [b5]\n- \"Near data processing insights from a micro workshop\" [b10]\n- \"Livia data centric computing throughout the memory hierarchy\" [b82]\n- \"Optimus prime accelerating data transformation in servers\" [b107]\n\nThis selection is made based on key phrases indicating direct inspiration or motivation, references that are directly linked to the primary challenges and the novel methods proposed by the paper, and references that share strong methodological similarities with the paper.\n\nHere is the result in json format:\n\n{\n \"b106\": 0.85,\n \"b5\": 0.8,\n \"b43\": 0.75,\n \"b151\": 0.7,\n \"b5\": 0.65,\n \"b10\": 0.6,\n \"b82\": 0.55,\n \"b107\": 0.5\n}", "Based on the text of the paper and the paper's titles, the source papers that inspired this work most likely include:\n\n1. \"A case for richer cross-layer abstractions: Bridging the semantic gap with expressive memory\" [b136]\n2. \"Livia: Data-centric computing throughout the memory hierarchy\" [b82]\n3. \"Stream-based memory access specialization for general purpose processors\" [b139]\n4. \"A robust main memory compression scheme\" [b35]\n5. \"A high-resolution side-channel attack on last-level cache\" [b67]\n6. \"The jMachine: Multicomputer - An architectural evaluation\" [b97]\n7. \"Talus: A simple way to remove cliffs in cache performance\" [b15]\n\nHere is the result in json format:\n\n{ \"b136\": 0.9, \"b82\": 0.85, \"b139\": 0.8, \"b35\": 0.75, \"b67\": 0.7, \"b97\": 0.65, \"b15\": 0.6 } \n\nThe confidence scores are between 0 and 1, with 1 being the most confident. These scores were assigned based on the relevance of the source paper to the main contributions of the given paper. The papers were selected based on key phrases found in the text, such as \"motivated by\", \"inspired by\", and \"take inspiration\" and the content of the titles."], "62d7a7d15aee126c0f3fe03d": ["{\n  \"b3\": 0.95,\n  \"b69\": 0.85,\n  \"b34\": 0.75\n}", "{\n  \"b3\": 0.95,\n  \"b69\": 0.9,\n  \"b34\": 0.85,\n  \"b42\": 0.8\n}", "{\n    \"b3\": 0.95,\n    \"b69\": 0.9,\n    \"b34\": 0.8,\n    \"b4\": 0.75\n}", "{\n  \"b3\": 0.95,\n  \"b69\": 0.90,\n  \"b34\": 0.85,\n  \"b74\": 0.80\n}", "{\n  \"b3\": 0.95,\n  \"b69\": 0.8,\n  \"b34\": 0.7,\n  \"b4\": 0.65\n}"], "62d7a7d15aee126c0f3fe044": ["Based on the text, the following references appear to be the most directly relevant to the paper's core contributions:\n\n- 'b36': 'sparch efficient architecture for sparse matrix multiplication'\n- 'b34': 'gamma leveraging gustavson s algorithm to accelerate sparse matrix multiplication'\n- 'b29': 'graphpulse an event driven hardware accelerator for asynchronous graph processing'\n- 'b17': 'meet the walkers accelerating index traversals for in memory databases'\n- 'b5': 'coram an in fabric memory architecture for fpga based computing'\n\nThese references are indicated by specific mention throughout the text, and appear to be directly connected to the paper's novel contributions, which include the development of a cache generation system for specific applications and the introduction of a toolflow to cater to DSA-specialization through cache controllers.\n\nThe JSON output would be:\n\n```json\n{\n  \"b36\": 1,\n  \"b34\": 0.9,\n  \"b29\": 0.9,\n  \"b17\": 0.8,\n  \"b5\": 0.7\n}\n```\n\nHere, the confidence score is indicative of how strongly the paper seems to rely on the corresponding reference, with 'b36' being the most heavily relied upon, and 'b5' being the least within this selection.", "Based on the analysis of the text, the source papers that inspired the given paper most likely include:\n\n1. \"X-Cache: A Modular Architecture for Domain Specific Caches\" - It is the title paper itself and a major part of the text is dedicated to explaining X-Cache's methodology and implementation, which suggests that this paper holds a significant influence on the topic. Confidence Score: 1 \n\n2. \"Efficient Execution of Memory Access Phases Using Dataflow Specialization\" [b14] - The given paper discusses the need for a cache for irregular non-linear accesses, which could be inspired by this reference. Confidence Score: 0.9 \n\n3. \"Meet The Walkers: Accelerating Index Traversals for In-memory Databases\" [b17] - The mention of 'walkers' in the given paper context, which is required to traverse data structures, is likely to be inspired by this reference. Confidence Score: 0.9 \n\n4. \"GAMMA: Leveraging Gustavson\u2019s Algorithm to Accelerate Sparse Matrix Multiplication\" [b34] - The paper discusses sparse GEMM, and compares two DSAs from the same family, sparse GEMM: inner-product and Gustavson product, which directly refers to this reference. Confidence Score: 0.9 \n\n5. \"SpArch: Efficient Architecture for Sparse Matrix Multiplication\" [b36] - The paper discusses the SpArch DSA and the implementation of domain-specific caches for it, which could be inspired by this reference. Confidence Score: 0.8 \n\n6. \"GraphPulse: An Event-Driven Hardware Accelerator for Asynchronous Graph Processing\" [b29] - The given paper mentions the creation of domain-specific caches for GraphPulse, which could be inspired by this reference. Confidence Score: 0.8 \n\n7. \"DASX: Hardware Accelerator for Software Data Structures\" [b21] - The paper discusses the DASX DSA and the implementation of domain-specific caches for it, which could be inspired by this reference. Confidence Score: 0.8 \n\n8. \"A Patch Memory System for Image Processing and Computer Vision\" [b6] - The paper mentions that some prior DSAs include cache-like structures that are inextricably tied to the underlying DSA and do not implement the cache controller, which could be inspired by this reference. Confidence Score: 0.7 \n\nHere is the result in JSON format:\n```\n{\n\"None\": 1, \n\"b14\": 0.9, \n\"b17\": 0.9, \n\"b34\": 0.9, \n\"b36\": 0.8, \n\"b29\": 0.8, \n\"b21\": 0.8, \n\"b6\": 0.7\n}\n```", "Based on the text given, the main contributions of the paper are the development of a novel cache generator for DSA applications, the proposal of changing the cache's tag strategy for specific applications, and the creation of an open-source toolkit that allows users to customize cache tags and generate complete RTL code. The paper also addresses the challenges of incorporating an address-based cache in a sparse matrix DSA and introduces several novel ideas, including DSA-specific tags (Metatags), DSA-programmable walking (X-Routine), and DSA-agnostic controller architecture.\n\nFrom this information, the following references seem to have heavily influenced the main contributions of the paper:\n\n1. [b4]: Efficient data supply for hardware accelerators with prefetching and access execute decoupling - The paper refers to a specific walker mechanism required for the cache, which might have been inspired by this reference.\n2. [b36]: Sparch efficient architecture for sparse matrix multiplication - This paper is referenced in relation to the use of sparse data structures and the need for a walker to traverse and preload the cache.\n3. [b34]: Gamma leveraging Gustavson s algorithm to accelerate sparse matrix multiplication - This paper is referred multiple times when discussing the implementation of meta-tags and the specific walking logic for different DSA\u2019s.\n4. [b29]: Graphpulse an event driven hardware accelerator for asynchronous graph processing - This paper is mentioned when discussing the evaluation of X-Cache with different DSAs.\n5. [b17]: Meet the walkers accelerating index traversals for in memory databases - This reference is likely to have influenced the development of the walker mechanism in the paper.\n6. [b5]: Coram an in fabric memory architecture for FPGA based computing - The paper mentions that existing DSAs predominantly work with dense data, and this reference is likely to have inspired the paper's work on sparse data.\n\nHere is the result in the required format:\n{\n    \"b4\": 0.9,\n    \"b36\": 0.9,\n    \"b34\": 0.8,\n    \"b29\": 0.7,\n    \"b17\": 0.7,\n    \"b5\": 0.6\n}", "Based on the text, the following papers seem to be the most relevant sources:\n\n1. Dally et al. [b7]: This paper is mentioned in the context of discussing the importance of locality and efficient data movement in DSAs, suggesting a strong influence on the work's core principles.\n2. Hennessy-Patterson [b13]: This paper is also mentioned in the context of discussing the importance of locality and efficient data movement in DSAs, suggesting a strong influence on the work's core principles.\n3. Sparse data structures [b36]: This paper is directly related to the problem of non-affine data structures and indirectly addressed data structure traversal, which is one of the challenges the paper is addressing.\n4. Indirect-indexes [b29]: This paper is directly related to the problem of non-affine data structures and indirectly addressed data structure traversal, which is one of the challenges the paper is addressing.\n5. Hash tables [b17]: This paper is directly related to the problem of non-affine data structures and indirectly addressed data structure traversal, which is one of the challenges the paper is addressing.\n6. Pioneering work on caches [b6] [b34] [b36]: These papers are mentioned in the context of the paper's discussion on the need for cache controllers and their contribution to DSA specialization.\n7. Previous DSAs [b29] [b34] [b36]: These papers appear to have influenced the development of the meta-tag concept, which is a significant part of the paper's contributions.\n\nThe confidence scores for each paper cannot be determined exactly without a specific model, but it is reasonable to say that they all have a high relevance to the paper's contributions given their direct mention and relation to the challenges addressed by the paper.\n\nIn json format:\n\n{\n    \"b7\": 1,\n    \"b13\": 1,\n    \"b36\": 1,\n    \"b29\": 1,\n    \"b17\": 1,\n    \"b6\": 1,\n    \"b34\": 1\n}", "Based on the content of the paper and the approach it introduces, the source papers that inspired this work most likely include:\n\n{ \n  \"b4\": 0.8,\n  \"b6\": 0.7,\n  \"b29\": 0.8,\n  \"b34\": 0.9,\n  \"b36\": 0.9\n}\nThe confidence scores associated with each source paper reflect the degree of direct relevance to the research presented in this paper, with scores closer to 1 indicating a higher degree of relevance."], "62d7a7d15aee126c0f3fe059": ["Based on the text of the paper, the main challenge outlined is the problem of lukewarm executions of serverless functions resulting in performance degradation. The authors propose Jukebox, a record-and-replay instruction prefetcher to tackle this challenge. \n\nThe following references seem to be the most relevant sources:\n\n1. Reference [b24]: \"nightcore efficient and scalable serverless computing for latency sensitive interactive microservices\" - This paper is directly related to the idea of improving the efficiency of serverless computing which is the core contribution of the paper. \n\n2. Reference [b15]: \"proactive instruction fetch\" -  This paper seems to be a source of inspiration for the Jukebox mechanism as it discusses the concept of instruction prefetching. \n\n3. Reference [b27]: \"shift shared history instruction fetch for lean core server processors\" - This paper, which similarly discusses instruction prefetching for server processors, seems to be another key source.\n\n4. Reference [b28]: \"confluence unified instruction supply for scale-out servers\" - This paper is likely another source of inspiration, given its discussion of instruction supply for servers, a key component of the Jukebox mechanism.\n\n5. Reference [b41]: \"architectural implications of function as a service computing\" - This paper discusses the architectural implications of Function-as-a-Service (FaaS), a concept central to the paper's main contributions.\n\n6. Reference [b42]: \"serverless in the wild characterizing and optimizing the serverless workload at a large cloud provider\" - This paper pertains to the characterization and optimization of serverless workloads, a topic directly relevant to the paper's main contributions.\n\nBased on the above analysis, the reference list in JSON format would look like this:\n\n{\n\"b24\": 0.9,\n\"b15\": 0.85,\n\"b27\": 0.85,\n\"b28\": 0.8,\n\"b41\": 0.8,\n\"b42\": 0.8\n}", "Based on the provided text and the instructions given, the following source papers seem to have inspired the author the most:\n\n1. \"Firecracker: Lightweight virtualization for serverless applications\" - [b1]: The author refers to the high degree of co-residency and interleaving of serverless functions on a server, which is an issue that Firecracker, a lightweight virtualization technology for serverless applications, aims to address.\n\n2. \"Shift: Shared History Instruction Fetch for Lean-Core Server Processors\" - [b27]: The author mentions the use of a record-and-replay instruction prefetcher, a concept that has been proposed in previous works like this one. \n\n3. \"Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider\" - [b42]: The author refers to the high degree of interleaving and the impact of long inter-arrival times, which are issues that this paper explores in the context of serverless workloads.\n\n4. \"Nightcore: Efficient and Scalable Serverless Computing for Latency Sensitive Interactive Microservices\" - [b24]: The author refers to the short running times of serverless functions, a topic that is explored in this paper.\n\n5. \"Analyzing Tail Latency in Serverless Clouds with Stellar\" - [b48]: The author refers to the long delays of booting a new function instance, an issue that this paper analyzes.\n\n6. \"A Top-Down Method for Performance Analysis and Counters Architecture\" - [b51]: The author refers to the use of the Intel Top-Down performance analysis, a method that is outlined in this paper.\n\nSo, the final output in the json format should be:\n{\n\"b1\": 0.9,\n\"b27\": 0.8,\n\"b42\": 0.9,\n\"b24\": 0.7,\n\"b48\": 0.7,\n\"b51\": 0.8\n}\nThe confidence scores are estimated based on the relevance of the source paper to the given paper's main contributions and the frequency of their mentions in the text.", "Based on the given information, the paper emphasizes the challenges associated with lukewarm executions of serverless functions due to high degree of interleaving, and introduces a solution named \"Jukebox\". \n\nThe source papers that influence the main contributions of this paper can be identified based on the key phrases such as \"inspired by\", \"motivated by\", \"based on this finding\", and \"prior works have proposed\". However, such key phrases are not explicitly found in the provided text.\n\nBased on the context and the topics discussed in the paper, the following references seem to be relevant to the main contributions of the paper:\n\n1. \"b1\": 'firecracker lightweight virtualization for serverless applications'\n2. \"b10\": 'the state of serverless'\n3. \"b14\": 'a review of serverless use cases and their characteristics'\n4. \"b24\": 'nightcore efficient and scalable serverless computing for latency sensitive interactive microservices'\n5. \"b15\": 'proactive instruction fetch'\n6. \"b28\": 'confluence unified instruction supply for scale out servers'\n7. \"b42\": 'serverless in the wild characterizing and optimizing the serverless workload at a large cloud provider'\n\nThese references are directly related to the challenges outlined in the paper such as the high degree of interleaving of serverless functions on a server, the lukewarm execution, and the performance degradation. Also, they seem to be related to the proposed solution - Jukebox, a record-and-replay instruction prefetcher for accelerating lukewarm serverless function executions.\n\nThe JSON format of the source papers with an arbitrary confidence score (considering the lack of explicit mention of their influence) is:\n\n{\n    \"b1\": 0.9,\n    \"b10\": 0.9,\n    \"b14\": 0.9,\n    \"b24\": 0.9,\n    \"b15\": 0.9,\n    \"b28\": 0.9,\n    \"b42\": 0.9\n}", "Based on the given text and the list of references, the primary challenges outlined in the paper revolve around the performance degradation of serverless functions due to obliteration of their on-chip microarchitectural working sets between invocations. The author proposes an algorithm called Jukebox to combat these challenges.\n\nThe source papers that seem to inspire this paper the most are:\n\n1. \"lukewarm serverless functions characterization and optimization\" - This paper appears to be the main source of inspiration as it directly deals with the concept of lukewarm serverless functions, which is the primary subject of the paper. \n\n2. \"Firecracker lightweight virtualization for serverless applications\" - This paper might have inspired the use of Firecracker micro VMs in the implementation of the serverless functions.\n\n3. \"proactive instruction fetch\" - This paper might have inspired the record-and-replay mechanism used in Jukebox.\n\n4. \"temporal instruction fetch streaming\" - This paper also appears to have influenced the design of Jukebox, as it also deals with instruction prefetching methods.\n\n5. \"Serverless in the wild characterizing and optimizing the serverless workload at a large cloud provider\" - The paper might have drawn inspiration from this source in terms of understanding the challenges and optimizations of serverless workloads. \n\n6. \"analyzing tail latency in serverless clouds with stellar\" - This paper might have provided insights on analyzing latency in serverless clouds which is a key challenge addressed in the paper.\n\n7. \"A top down method for performance analysis and counters architecture\" - This paper might have influenced the performance analysis method used in the paper.\n\nTherefore, the result can be represented in JSON format as:\n\n```\n{\n    \"b0\": 0.9,\n    \"b1\": 0.75,\n    \"b15\": 0.85,\n    \"b27\": 0.85,\n    \"b42\": 0.8,\n    \"b48\": 0.8,\n    \"b51\": 0.8\n}\n```\n\nPlease note that the confidence scores are subjective and based on the given text. They represent the inferred degree of relevance of each source paper to the main paper.", "Based on the given academic paper text and the analysis criteria, the following references seem to be the most pertinent sources that inspired the author:\n\n1. \"Firecracker: lightweight virtualization for serverless applications\" [b1]\n2. \"The state of serverless\" [b10] and [b11]\n3. \"Proactive instruction fetch\" [b15]\n4. \"Nightcore: efficient and scalable serverless computing for latency-sensitive interactive microservices\" [b24]\n5. \"Serverless in the wild: characterizing and optimizing the serverless workload at a large cloud provider\" [b42]\n6. \"Analyzing tail latency in serverless clouds with Stellar\" [b48]\n\nThese references have been prioritized based on their direct relevance to the paper's topic and their contribution to the development of the paper's core algorithm. They are directly linked to the challenges, methods, and novel contributions discussed in the paper.\n\nThe JSON format for the result would be:\n\n{\n    \"b1\": 0.9,\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b15\": 0.8,\n    \"b24\": 0.9,\n    \"b42\": 0.9,\n    \"b48\": 0.9\n}\n\nThe confidence scores are indicative of the perceived influence of these references on the paper's content, with 1 being the highest possible score."], "62d7a7d15aee126c0f3fe049": ["Based on the challenges outlined in the paper and the novel methods proposed by the authors, the following source papers are identified:\n\n- \"Early address prediction efficient pipeline prefetch and reuse\" [b1] \n- \"Efficient load value prediction using multiple predictors and filters\" [b67]\n- \"Speculative execution via address prediction and data prefetching\" [b28]\n- \"Value Prediction (VP) [b6] [b45] [b57] [b58] [b62] [b64]\"\n- \"Address Prediction (AP) of loads\" [b8] [b28] [b66] [b67]\n- \"Memory Prefetching [b5] [b9] [b10] [b22] [b29] [b32] [b33] [b36] [b40] [b46] [b48] [b49] [b51] [b59] [b61] [b69] [b71] [b72] [b73]\"\n- \"Caching Multi-level caching\" [b34] [b35] [b44] [b52]\n- \"OOO Pipeline\" [b11] [b75] [b81]\n\nThe confidence scores are assigned based on their relevance to the paper's contributions and methodology.\n\nThe JSON format of the result is as follow:\n\n{\n\"b1\": 0.9,\n\"b67\": 0.8,\n\"b28\": 0.85,\n\"b6\": 0.75,\n\"b45\": 0.75,\n\"b57\": 0.75,\n\"b58\": 0.75,\n\"b62\": 0.75,\n\"b64\": 0.75,\n\"b8\": 0.7,\n\"b66\": 0.7,\n\"b5\": 0.65,\n\"b9\": 0.65,\n\"b10\": 0.65,\n\"b22\": 0.65,\n\"b29\": 0.65,\n\"b32\": 0.65,\n\"b33\": 0.65,\n\"b36\": 0.65,\n\"b40\": 0.65,\n\"b46\": 0.65,\n\"b48\": 0.65,\n\"b49\": 0.65,\n\"b51\": 0.65,\n\"b59\": 0.65,\n\"b61\": 0.65,\n\"b69\": 0.65,\n\"b71\": 0.65,\n\"b72\": 0.65,\n\"b73\": 0.65,\n\"b34\": 0.6,\n\"b35\": 0.6,\n\"b44\": 0.6,\n\"b52\": 0.6,\n\"b11\": 0.55,\n\"b75\": 0.55,\n\"b81\": 0.55\n} \n\nThis result is inferred based on the titles of each paper and the information from the author's note.", "Based on the provided text and the titles of the references, the most relevant source papers that inspired the given paper are:\n\n- 'b1': 'early address prediction efficient pipeline prefetch and reuse'\n- 'b66': 'load value prediction via path based address prediction avoiding mispredictions due to conflicting stores'\n- 'b81': 'speculation techniques for improving load related instruction scheduling'\n- 'b52': 'criticality aware tiered cache hierarchy a fundamental relook at multi level cache hierarchies'\n- 'b39': 'uc check characterizing micro operation caches in x processors and implications in security and performance'\n- 'b21': 'intel s th gen core tiger lake soc detailed superfin willow cove and xe lp'\n\nThe JSON format of result is:\n{\"b1\": 0.9, \"b66\": 0.9, \"b81\": 0.8, \"b52\": 0.7, \"b39\": 0.7, \"b21\": 0.7}", "Based on the given paper text and the titles of the references, the following source papers seem to be most pertinent:\n\n1. \"early address prediction efficient pipeline prefetch and reuse\" [b1]\n2. \"speculative execution via address prediction and data prefetching\" [b28]\n3. \"efficient load value prediction using multiple predictors and filters\" [b67]\n4. \"load value prediction via path based address prediction avoiding mispredictions due to conflicting stores\" [b66]\n5. \"speculation techniques for improving load related instruction scheduling\" [b81]\n6. \"register file prefetching\" [None]\n7. \"a highly accurate and energy efficient stride prefetcher\" [b41]\n8. \"efficiently prefetching complex address patterns\" [b69]\n\nThese papers are mentioned in the context of the algorithm proposed by the author, which is Register File Prefetching (RFP), and the challenges it aims to address. \n\nThe json format for the results is:\n\n{\n  \"b1\": 0.9, \n  \"b28\": 0.8,\n  \"b67\": 0.8,\n  \"b66\": 0.85,\n  \"b81\": 0.8,\n  \"None\": 0.9,\n  \"b41\": 0.75,\n  \"b69\": 0.7\n}\n\nThe confidence score is based on my interpretation of how strongly the paper's main contributions were influenced by these references. The higher the score, the more significant the influence.", "Based on the provided information and the criteria outlined, the source papers for the given text are:\n\n1. \"Early address prediction efficient pipeline prefetch and reuse\" [b1] - This paper is explicitly mentioned as a motivation for the presented work and directly pertains to the prefetching technique proposed in this paper. The phrase \"This observation motivates us to focus our attention on prefetching between the L1 data cache and the Register File\" points to this paper as a primary source of inspiration. Confidence Score: 0.9\n\n2. \"Load value prediction via path based address prediction avoiding mispredictions due to conflicting stores\" [b66] - This paper is mentioned in relation to the discussion of load address prediction and its limitations, which informs the novel approach proposed in the analyzed paper. Confidence Score: 0.8\n\n3. \"Efficient load value prediction using multiple predictors and filters\" [b67] - This paper is also discussed in relation to address prediction, and the analyzed paper highlights its limitations as a motivation for their new approach. Confidence Score: 0.8\n\n4. \"Speculation techniques for improving load related instruction scheduling\" [b81] - This paper is referenced multiple times, particularly in relation to the concept of 'speculative scheduling'. It seems to have a strong methodological influence on the paper in question. Confidence Score: 0.7\n\n5. \"Speculative execution via address prediction and data prefetching\" [b28] - This paper is discussed in relation to prefetching techniques and seems to have influenced the approach of the analyzed paper. Confidence Score: 0.7\n\n6. \"Improving data cache performance by pre executing instructions under a cache miss\" [b23] - This paper is discussed in relation to memory prefetching techniques, which is a central theme in the analyzed paper. Confidence Score: 0.7\n\n7. \"Value locality and load value prediction\" [b45] - This paper is referenced in relation to value prediction, a key concept in the analyzed paper's proposed solution. Confidence Score: 0.7\n\n8. \"Criticality aware tiered cache hierarchy a fundamental relook at multi level cache hierarchies\" [b52] - This paper is discussed as a motivator for the novel approach presented in this paper, focusing on the cache hierarchy. Confidence Score: 0.6\n\nThe confidence scores are calculated based on the frequency of reference, how directly the source papers' methodologies and findings influenced the core contributions of the given paper, and the context in which they were mentioned.\n\nThe JSON format result:\n{ \"b1\": 0.9, \"b66\": 0.8, \"b67\": 0.8, \"b81\": 0.7, \"b28\": 0.7, \"b23\": 0.7, \"b45\": 0.7, \"b52\": 0.6 }", "Based on the provided text and the titles of the references, the list of source papers that inspired the author the most can be as follows:\n\n1) 'b1': 'early address prediction efficient pipeline prefetch and reuse': The paper is directly mentioned in the text and the author discusses its limitations which leads to the development of RFP. Confidence Score: 0.9\n\n2) 'b66': 'load value prediction via path based address prediction avoiding mispredictions due to conflicting stores': The paper is directly mentioned in the text and the author discusses its limitations which leads to the development of RFP. Confidence Score: 0.9\n\n3) 'b67': 'efficient load value prediction using multiple predictors and filters': The paper is directly mentioned in the text and the author discusses its limitations which leads to the development of RFP. Confidence Score: 0.9\n\n4) 'b81': 'speculation techniques for improving load related instruction scheduling': The paper is directly mentioned in the text and the author refers to its approach. Confidence Score: 0.8\n\n5) 'b21': 'intel s th gen core tiger lake soc detailed superfin willow cove and xe lp': The paper is mentioned in the text multiple times and the author uses it as a reference for the modern OOO processor. Confidence Score: 0.7\n\n6) 'b55': 'leveraging targeted value prediction to unlock new hardware strength reduction potential': The paper is mentioned in the text and the author discusses how it is similar to the novel method. Confidence Score: 0.7\n\n7) 'b52': 'criticality aware tiered cache hierarchy a fundamental relook at multi level cache hierarchies': The paper is mentioned in the text and the author uses it as a reference for cache hierarchy. Confidence Score: 0.7\n\n8) 'b34': 'high performance cache replacement using re reference interval prediction rrip': The paper is mentioned in the text and the author uses it as a reference for cache management. Confidence Score: 0.6\n\nNote: The confidence scores are based on the direct relevance and explicit mention in the text.\n\nJSON format:\n\n{\n\"b1\": 0.9,\n\"b66\": 0.9,\n\"b67\": 0.9,\n\"b81\": 0.8,\n\"b21\": 0.7,\n\"b55\": 0.7,\n\"b52\": 0.7,\n\"b34\": 0.6\n}"], "634d805b90e50fcafd4e117a": ["{\n\"b12\": 0.9,\n\"b23\": 0.8,\n\"b30\": 0.7,\n\"b32\": 0.8\n}", "Based on the text, the main challenges outlined in the paper are: 1) The large search space for efficient running kernels for a target operator, and 2) The use of machine learning algorithms to search for optimal solutions, which can be time-consuming. To address these challenges, the paper proposes ROLLER, a different optimization kernel construction method.\n\nThe following references were indicated as sources of inspiration or motivation for the contribution of the paper:\n\n1. \"advanced compilers [b12] [b30] [b32] propose to adopt machine learning algorithms\"\n2. \"We have implemented ROLLER on top of TVM [b12] and Rammer [b23]\"\n3. \"Instead of multi-level nested loops, ROLLER treats the computation in a DNN operator as a data processing pipeline...\" This could be inferred as being inspired or motivated by previous work, but no specific references are given in this context.\n\nBased on these key phrases and the novel contributions of the paper, the main source papers appear to be [b12], [b30], [b32], and [b23].\n\nHowever, further analysis of the titles of the papers suggest:\n- [b12] = \"TVM: An automated end-to-end optimizing compiler for deep learning\"\n- [b30] = \"Ansor: Generating high-performance tensor programs for deep learning\"\n- [b32] = \"FlexTensor: An automatic schedule exploration and optimization framework for tensor computation on a heterogeneous system\"\n- [b23] = \"Rammer: Enabling holistic deep learning compiler optimizations with rTasks\"\n\nThese papers seem to be directly related to the deep learning compiler optimization problem addressed in the text. Therefore, the source papers are likely to be [b12], [b30], [b32], and [b23].\n\nIn a JSON format, the confidence scores could be:\n\n{\n  \"b12\": 0.9,\n  \"b30\": 0.9,\n  \"b32\": 0.9,\n  \"b23\": 0.9\n} \n\nThe confidence score is high (0.9) for all four papers because they all appear to be highly relevant to the main contributions of the paper.", "Based on the provided information, the source papers that inspired this work the most could be:\n\n1. \"TVM: An Automated End-to-End Optimizing Compiler for Deep Learning\" [b12]: This paper seems to be the basis for the work done in this paper. The paper frequently refers to TVM and the issues it presents, and the authors develop their work as a response to these issues. \n2. \"ROLLER: Fast and Efficient Tensor Compilation for Deep Learning\" [None]: The title of the paper suggests that it is the primary source of the study. The authors may have used this paper to develop their own methodology.\n3. \"Ansor: Generating High-Performance Tensor Programs for Deep Learning\" [b30]: The paper frequently refers to Ansor and the problems it presents, and the authors develop their work in response to these problems. \n4. \"Rammer: Enabling Holistic Deep Learning Compiler Optimizations with Rtasks\" [b23]: The authors mention that they implemented ROLLER on top of TVM [b12] and Rammer [b23], indicating that they drew inspiration from this paper as well.\n5. \"Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking\" [b18]: This paper might have been used to understand the GPU architecture and build the ROLLER system accordingly.\n6. \"Dissecting the Graphcore IPU Architecture via Microbenchmarking\" [b19]: Similar to [b18], this paper might have been used to understand the IPU architecture and build the ROLLER system accordingly.\n\nThe confidence score for each source is difficult to determine without more context, but based on the frequency and significance of the mentions, the scores could be as follows:\n\n{\"b12\": 0.9, \"None\": 0.7, \"b30\": 0.9, \"b23\": 0.8, \"b18\": 0.7, \"b19\": 0.7}", "Based on the provided information, I infer that the main references that inspired the author are as follows:\n\n{ \n\"b12\": 0.9,\n\"b30\": 0.8,\n\"b23\": 0.7,\n\"b32\": 0.7,\n\"b4\": 0.6,\n\"b0\": 0.6\n}\n\nExplanation:\nThe paper describes a method for constructing efficient kernels for deep neural networks, which is inspired by previous work using machine learning algorithms to search for good solutions. This suggests that the papers referenced by \"b12\", \"b30\", \"b23\", and \"b32\" are likely to be major sources of inspiration. In addition, the author refers to cuDNN [b4] and cuBLAS [b0], suggesting that these are also significant sources. The confidence score is based on the degree to which the paper seems to rely on the ideas and methods proposed in these sources.\n", "Based on the provided text and the list of references, the source papers of the given paper that most significantly inspired the authors could include:\n\n1. \"TVM: An automated end-to-end optimizing compiler for deep learning\" (b12): The given paper seems to build upon the TVM compiler, and the authors used this as a reference point for their work in developing ROLLER. \n\n2. \"Ansor: Generating high-performance tensor programs for deep learning\" (b30): The paper mentions Ansor as the state-of-the-art tensor compiler and makes numerous comparisons to it. This suggests that Ansor was a significant source of inspiration for the authors' work. \n\n3. \"Rammer: Enabling holistic deep learning compiler optimizations with rTasks\" (b23): Rammer was used for the implementation of ROLLER, suggesting that it was a significant source of inspiration for the paper's main contributions. \n\n4. \"Tensor comprehensions: Framework agnostic high-performance machine learning abstractions\" (b26): Although not directly mentioned, this paper could have inspired the authors' work on creating an optimized tensor compiler, given its focus on high-performance machine learning abstractions.\n\n5. \"Halide: A language and compiler for optimizing parallelism, locality and recomputation in image processing pipelines\" (b24): Halide is a language and compiler that is used for optimizing parallelism, which may have influenced the authors' work on ROLLER. \n\n6. \"Learning to optimize tensor programs\" (b13): This paper focuses on the use of machine learning to optimize tensor programs, which is a key challenge that the authors of the given paper are addressing.\n\nThe confidence scores for these source papers could be estimated as follows (assuming that all source papers are equally likely):\n\n```\n{\n\"b12\": 0.90,\n\"b30\": 0.90,\n\"b23\": 0.85,\n\"b26\": 0.85,\n\"b24\": 0.80,\n\"b13\": 0.80\n}\n```\n\nPlease note that these confidence scores are estimates and the actual relevance of each source paper to the given paper may vary."], "62fc5c7b90e50fcafdbca64d": ["{\n    \"b10\": 0.95,\n    \"b17\": 0.95,\n    \"b29\": 0.95,\n    \"b31\": 0.95\n}", "```json\n{\n    \"b10\": 0.9,\n    \"b17\": 0.95,\n    \"b27\": 0.85,\n    \"b31\": 0.9,\n    \"b37\": 0.95,\n    \"b48\": 0.85\n}\n```", "{\n    \"b10\": 0.9,\n    \"b17\": 0.95,\n    \"b27\": 0.85,\n    \"b29\": 0.9,\n    \"b31\": 0.9,\n    \"b37\": 0.95\n}", "{\n    \"b10\": 0.95,\n    \"b17\": 0.90,\n    \"b29\": 0.85,\n    \"b31\": 0.85\n}", "```json\n{\n    \"b17\": 0.85,\n    \"b10\": 0.8,\n    \"b29\": 0.75,\n    \"b31\": 0.75,\n    \"b27\": 0.7\n}\n```"], "634d80f190e50fcafd4ef483": ["{\n    \"b45\": 0.9,\n    \"b41\": 0.9,\n    \"b43\": 0.9,\n    \"b27\": 0.8,\n    \"b12\": 0.8,\n    \"b1\": 0.7,\n    \"b7\": 0.7\n}", "{\n  \"b1\": 0.8,\n  \"b7\": 0.8,\n  \"b12\": 0.95,\n  \"b27\": 0.95,\n  \"b41\": 0.95,\n  \"b42\": 0.9,\n  \"b43\": 0.95,\n  \"b45\": 0.95\n}", "{\n    \"b1\": 0.85,\n    \"b7\": 0.85,\n    \"b12\": 0.9,\n    \"b27\": 0.95,\n    \"b41\": 0.9,\n    \"b43\": 0.9,\n    \"b45\": 0.95\n}", "{\n    \"b45\": 0.9,\n    \"b41\": 0.9,\n    \"b43\": 0.9,\n    \"b1\": 0.8,\n    \"b7\": 0.8,\n    \"b12\": 0.8,\n    \"b27\": 0.8\n}", "{\n    \"b1\": 0.85,\n    \"b7\": 0.85,\n    \"b12\": 0.80,\n    \"b27\": 0.90,\n    \"b41\": 0.80,\n    \"b43\": 0.80,\n    \"b45\": 0.90\n}"], "6237ecc25aee126c0f3bef94": ["Based on the provided text of the academic paper, the primary challenges outlined in the paper revolve around training massive Graph Neural Networks (GNNs), particularly those with higher-order interactions to billions of parameters. The author proposes an algorithm named \"Graph Parallelism\" to tackle these challenges, which distributes the computation across multiple GPUs by splitting up the input graph.\n\nThe source papers which appear to have most influenced the development of this paper's core contributions are:\n\n1. [\"Open Catalyst (OC20) dataset\" - b2]: The author used this dataset to benchmark the proposed algorithm. This indicates that the OC20 dataset is an important source of inspiration for the paper. Confidence score: 0.9.\n\n2. [\"Fast and uncertainty aware directional message passing for non equilibrium molecules\" - b11]: The author scaled up the DimeNet++ architecture from the paper b11. Confidence score: 0.8.\n\n3. [\"GemNet: universal directional graph neural networks for molecules\" - b13]: The author scaled up the GemNet-T architecture, which indicates the importance of the methods and results of this paper. Confidence score: 0.8.\n\n4. [\"Language models are few shot learners\" - b1]: This paper seems to be a source of inspiration for scaling up GNNs, as the author suggests that scaling up GNNs could yield significant performance gains as it does in natural language processing, as shown in paper b1. Confidence score: 0.6.\n\n5. [\"Graph Neural Networks: A review of methods and applications\" - b32]: The GNNs, which are the core of the paper's algorithm, were first introduced in this paper. Confidence score: 0.6.\n\n6. [\"Neighborhood Sampling\" - b8, b31, b9, b17, b26]: These papers are mentioned in the context of previous approaches to scaling GNNs, although their methods do not apply directly to this paper's problem. Confidence score: 0.4.\n\nThe returned result in the JSON format is:\n{\"b2\": 0.9, \"b11\": 0.8, \"b13\": 0.8, \"b1\": 0.6, \"b32\": 0.6, \"b8\": 0.4, \"b31\": 0.4, \"b9\": 0.4, \"b17\": 0.4, \"b26\": 0.4}", "Based on the text and the author's notes, the paper introduces a method for training large-scale graph neural networks (GNNs) using multiple GPUs, and the method is applied to two recent models leading to significant performance improvements. The primary challenges outlined in the paper include the difficulty of scaling GNNs, particularly those with higher-order interactions, and the memory limitations associated with storing and updating large models. \n\nGiven the above, the following references appear to be the most central to the paper's contributions:\n\n1. 'b5': 'a new model for learning in graph domains': This paper introduces GNNs, which is the fundamental model that the authors seek to scale. \n\n2. 'b11': 'fast and uncertainty aware directional message passing for non equilibrium molecules': This paper presents one of the two recent models that the authors apply their method to. \n\n3. 'b13': 'gemnet universal directional graph neural networks for molecules': This paper presents the other recent model that the authors apply their method to. \n\n4. 'b2': 'open catalyst oc dataset and community challenges': This paper provides the dataset the authors use to benchmark their method. \n\n5. 'b7': 'gpipe efficient training of giant neural networks using pipeline parallelism': This paper could be where the authors drew inspiration for their method of distributing computation across multiple GPUs.\n\n6. 'b31': 'distdgl distributed graph neural network training for billion scale graphs': This paper also deals with distributed training of GNNs, so it might have influenced the authors' method.\n\nBased on the full context of the paper, the confidence scores are estimated as follows:\n\n{\n\"b5\": 0.9,\n\"b11\": 0.9,\n\"b13\": 0.9,\n\"b2\": 0.8,\n\"b7\": 0.7,\n\"b31\": 0.7\n} \n\nThese scores indicate a high level of confidence that these papers are the main sources of inspiration for the paper's novel contributions.", "Based on the text of the paper and the provided titles of the references, the source papers that have inspired this work and directly influenced the main contributions appear to be:\n\n1. \"Fast and uncertainty aware directional message passing for non equilibrium molecules\" [b11]\n2. \"GemNet: Universal directional graph neural networks for molecules\" [b13]\n3. \"Open Catalyst (OC20) dataset and community challenges\" [b2]\n4. \"Language models are few shot learners\" [b1]\n5. \"GPipe: Efficient training of giant neural networks using pipeline parallelism\" [b7]\n6. \"Neural message passing for quantum chemistry\" [b3]\n7. \"Graph Neural Networks (GNNs): A review of methods and applications\" [b32]\n\nThe JSON format of the result is:\n\n{\n    \"b11\": 1,\n    \"b13\": 1,\n    \"b2\": 1,\n    \"b1\": 0.8,\n    \"b7\": 0.8,\n    \"b3\": 0.7,\n    \"b32\": 0.7\n}\n\nThe confidence scores are assigned based on the frequency and the context of the references in the text. The scores are all high because these references are all directly related to the paper's novel contributions.", "Based on the given text, the following papers are identified as the most pertinent source papers:\n\n1. \"A new model for learning in graph domains\" [b5]\n2. \"Open Catalyst (OC20) dataset and community challenges\" [b2]\n3. \"Fast and uncertainty aware directional message passing for non-equilibrium molecules\" [b11]\n4. \"GemNet: Universal directional graph neural networks for molecules\" [b13]\n5. \"Language models are few-shot learners\" [b1]\n6. \"Megatron-LM: Training multi-billion parameter language models using model parallelism\" [b23]\n\nHere are the confidence scores for each:\n\n{\n  \"b5\": 0.8, \n  \"b2\": 0.95, \n  \"b11\": 0.9, \n  \"b13\": 0.9, \n  \"b1\": 0.7, \n  \"b23\": 0.7 \n}", "Based on the text of the paper, the key challenges are: scaling the Graph Neural Networks (GNNs) to handle billions of parameters and training them on large molecular datasets. The novel solution proposed by the authors is called \"Graph Parallelism\", which involves distributing the computation of the graph across multiple GPUs, particularly focusing on large models for a dataset of many moderately-sized graphs.\n\nThe source papers directly linked to these challenges and the novel solution proposed are:\n\n1. Graph Neural Networks (GNNs) [b5] [b32] - The foundational work on GNNs is likely the most significant source of inspiration for this paper, as the authors' method is an extension of the GNN model. \n2. The development of methods like neighborhood sampling [b8] [b31] [b9] [b17] [b26] - These papers discuss techniques for scaling small models to large graphs, which is a direct challenge that the authors are trying to address.\n3. DimeNet++ [b11] and GemNet-T [b13] - These are two recent GNN architectures that the authors applied their method to, indicating that these works were crucial in developing their solution.\n4. The Open Catalyst (OC20) dataset [b2] - This dataset was used to benchmark their approach, making it a key source for their research.\n\nBased on the information provided, the source papers are:\n\n{\"b5\": 0.9, \"b32\": 0.9, \"b8\": 0.7, \"b31\": 0.7, \"b9\": 0.7, \"b17\": 0.7, \"b26\": 0.7, \"b11\": 0.8, \"b13\": 0.8, \"b2\": 0.8}\n\nNote: The confidence scores are subjective and are based on the frequency and context in which the papers are mentioned in the text. Higher scores indicate a stronger influence on the research."], "6257c5b25aee126c0f468af6": ["Based on the paper's text, the key challenges outlined are how to model 3D information in molecular structures and how to incorporate the torsion information into graph neural networks. The algorithm proposed by the author is a new message passing scheme called Spherical Message Passing (SMP) for fast and accurate 3D molecular learning. \n\nThe paper directly indicates that it takes inspiration from or builds upon the following papers:\n- [b18]: \"Neural message passing for quantum chemistry\"\n- [b17]: \"Generalization and representational limits of graph neural networks\"\n- [b24]: \"Generative models for graph-based protein design\"\n- [b26]: \"Semi-supervised classification with graph convolutional networks\"\n- [b28]: \"Directional message passing for molecular graphs\"\n\nBased on the novel methods and approaches introduced by the paper, the following papers can also be inferred as sources:\n- [b47]: \"Tensor field networks rotation and translation equivariant neural networks for d point clouds\"\n- [b33]: \"n gram graph simple unsupervised representation for graphs with applications to molecules\"\n- [b41]: \"Schnet a continuous filter convolutional neural network for modeling quantum interactions\"\n\nTherefore, the source papers that heavily influenced the main contributions of the paper are: [b18], [b17], [b24], [b26], [b28], [b47], [b33], and [b41].\n\nIn the json format:\n{\n\"b18\": 0.9,\n\"b17\": 0.85,\n\"b24\": 0.85,\n\"b26\": 0.8,\n\"b28\": 0.8,\n\"b47\": 0.75,\n\"b33\": 0.75,\n\"b41\": 0.75\n}", "Based on the analysis of the text, the source papers that heavily influenced the main contributions of the given paper are:\n\n{\n\"b18\": 0.9, \n\"b28\": 0.8, \n\"b23\": 0.7, \n\"b17\": 0.8, \n\"b24\": 0.7, \n\"b33\": 0.6, \n\"b6\": 0.7\n} \n\nThis result is inferred from the text, where these papers are directly linked to the primary challenges and methods outlined in the paper. The confidence scores are assigned based on the frequency and the context these papers are mentioned in the text.", "Based on the text of the paper and the titles of the references, the source papers that most likely inspired this work are:\n\n1. \"Spherical message passing for 3d molecular graphs\" - This is the title of the paper itself, so it's not a source paper.\n2. \"Geom: Energy-annotated molecular conformations for property prediction and molecular generation\" [b1] - The paper discusses the incorporation of 3D information into graph neural networks, a key aspect of the text.\n3. \"Relational inductive biases, deep learning, and graph networks\" [b2] - This paper seems to have been a significant influence based on its relevance to the topic of graph neural networks.\n4. \"Generalization and representational limits of graph neural networks\" [b17] - This text discusses the importance of torsion angle in graph neural networks, which is a key concept in the text.\n5. \"Neural message passing for quantum chemistry\" [b18] - The text discusses the use of message passing in graph neural networks, a concept that is also central to this paper.\n6. \"Directional message passing for molecular graphs\" [b28] - This paper seems to be highly relevant as it deals with the same topic of message passing for molecular graphs.\n7. \"Equivariant message passing for the prediction of tensorial properties and molecular spectra\" [b42] - The paper discusses the use of message passing in the prediction of molecular properties, which is a key aspect of the text.\n\nHere is the resulting json object:\n\n{\n    \"b1\": 0.8,\n    \"b2\": 0.7,\n    \"b17\": 0.9,\n    \"b18\": 0.9,\n    \"b28\": 1.0,\n    \"b42\": 0.8\n} \n\nIn this json object, each key is the reference number of a source paper and the associated value is the confidence score indicating how likely it is that the paper influenced this work. All scores are between 0 and 1, with 1 being the most confident.", "Based on the text and author's notes, the paper is exploring how to model 3D information of molecular structures, specifically how to integrate rotational information, or torsion, into graph neural networks (GNNs). The paper proposes a novel message-passing scheme known as the Spherical Message Passing (SMP) for efficient 3D molecular learning. The SMP is shown to be able to distinguish almost all 3D graph structures and yields predictions that are invariant to translation and rotation of input graphs. The paper presents SphereNet as the proposed network for molecular learning.\n\nBased on the text, the following papers seem to have inspired or motivated the authors the most:\n\n1. The paper by [b18] appears to have motivated the authors' use of the message passing scheme in GNNs. This paper is mentioned in the context of commonly used architectures for realizing GNNs, which the authors of the analyzed paper aim to develop a novel method for.\n\n2. [b33], [b48], and [b1] are mentioned in the context of representing 3D molecular graphs, which is a key focus of the paper.\n\n3. [b6] is referenced when the authors conduct formal analyses in the spherical coordinate system (SCS), which is important for their method.\n\n4. [b17] is referred to when the authors discuss the importance of considering torsion in given 3D structures for accurate generation of protein sequences and molecules.\n\n5. [b47] and [b13] are mentioned when the authors discuss the challenges with existing methods that usually take coordinates in the Cartesian coordinate system for all atoms as the raw input.\n\n6. [b18] is again mentioned when the authors propose their novel Spherical Message Passing (SMP) scheme.\n\nReferences [b23] and [b28] are also mentioned in relation to choosing appropriate basis functions, but it is not clear from the text how much they have influenced the authors' work.\n\nBased on these observations, the source papers could be:\n\n```\n{\n\"b18\": 0.9,\n\"b33\": 0.8,\n\"b48\": 0.8,\n\"b1\": 0.8,\n\"b6\": 0.8,\n\"b17\": 0.7,\n\"b47\": 0.7,\n\"b13\": 0.7\n}\n```\n\nConfidence scores are assigned based on how closely the papers seem to be related to the novel contributions of the paper. Papers [b18] is given the highest score due to it being mentioned in the context of fundamental concepts (message passing scheme). Other papers are scored slightly lower as it is less clear how much they have influenced the paper's novel contributions.", "Based on the text and titles of the references, the following references might be the source papers:\n\n{\n  \"b2\": 0.85,\n  \"b18\": 0.95,\n  \"b23\": 0.90,\n  \"b28\": 0.93,\n  \"b33\": 0.80,\n  \"b41\": 0.87,\n  \"b45\": 0.80\n}\n\nThe confidence scores are indicated based on the relevance of the reference paper to the main contributions of the given paper. For instance, 'b18: Neural message passing for quantum chemistry' is given a high score as the given paper also focuses on developing a novel message passing method. Similarly, 'b28: Directional message passing for molecular graphs' and 'b23: Forcenet a graph neural network for large scale quantum calculations' are scored high due to their close relevance to the given paper's topic on graph neural networks and molecular learning."], "6257c5b15aee126c0f468a55": ["Based on the text, the paper mainly proposes a new sparse attention strategy named Learning-to-Hash Attention (LHA) to address the limitations of existing Approximate Nearest Neighbors (ANN) based methods for attention sparsification. Given these insights, the most pertinent source papers seem to be:\n\n1. \"Attention is all you need\" [b34] - This is the paper that introduced the transformer model which is the basis of the paper, thus it's an important source. However, it might be considered as a historical finding, thus the confidence score is 0.5.\n2. \"Reformer: the efficient transformer\" [b13] - This paper introduced the ANN-based method called Locality Sensitive Hashing (LSH) which is one of the methods the author's method is trying to improve upon. Confidence score: 1.0.\n3. \"Efficient content-based sparse attention with routing transformers\" [b28] - This paper proposed another ANN-based method using mini-batch spherical k-means which is also one of the methods the author's method is trying to improve upon. Confidence score: 1.0.\n4. \"Efficient attention using asymmetric clustering\" [b8] - This paper introduced the SMYRF method which is also one of the ANN methods the author's method is trying to improve upon. Confidence score: 1.0.\n5. \"A survey on learning to hash\" [b37] - This paper probably provides a theoretical basis for the author's Learning-to-Hash method, thus it's an important source. Confidence score: 1.0.\n6. \"Sparse Sinkhorn Attention\" [b31] - This paper seems to propose a different approach to sparse attention. It may have provided some inspiration for the proposed approach in the paper. Confidence score: 0.8.\n7. \"Predicting attention sparsity in transformers\" [b33] - This paper proposed SparseFinder that learns sparse attention patterns. The paper's method LHA is compared to this method, indicating that it has some impact on the paper's research. Confidence score: 0.8.\n\nIn JSON format, the result is:\n{\n\"b34\": 0.5,\n\"b13\": 1.0,\n\"b28\": 1.0,\n\"b8\": 1.0,\n\"b37\": 1.0,\n\"b31\": 0.8,\n\"b33\": 0.8\n}", "Based on the given text, the paper introduces a novel method called Learning-to-Hash Attention (LHA) to address challenges in attention sparsification. It improves model expressiveness and solves imbalance issues in existing methods like Approximate Nearest Neighbors (ANN) and Locality Sensitive Hashing (LSH) [b13][b8]. The paper also proposes a new metric called attention utility to quantify how well the sparse patterns approximate the full attention. \n\nFrom the text, it seems the work is primarily inspired by the following papers:\n\n1. [b13] - 'Reformer: The Efficient Transformer' \n2. [b8] - 'Efficient Attention using Asymmetric Clustering'\n3. [b28] - 'Efficient Content-Based Sparse Attention with Routing Transformers'\n4. [b38] - 'Cluster Former: Clustering-Based Sparse Transformer for Long Range Dependency Encoding'\n5. [b34] - 'Attention is All You Need'\n6. [b37] - 'A Survey on Learning to Hash'\n\nTherefore, the source papers are:\n\n{\n\"b13\" : 1,\n\"b8\" : 0.9,\n\"b28\" : 0.9,\n\"b38\" : 0.9,\n\"b34\" : 0.8,\n\"b37\" : 0.8\n}", "Based on the text and the titles of the references, the source papers that inspired this paper are likely to be:\n\n1. 'Attention is all you need' (b34)\n2. 'Reformer: the efficient transformer' (b13)\n3. 'Efficient content-based sparse attention with routing transformers' (b28)\n4. 'Locality Sensitive Hashing' (b13 and b8)\n5. 'Learning-to-Hash methods' (b37)\n6. 'Rethinking attention with performers' (b5)\n\nBased on the provided text and titles, there are several references that the authors directly mention or indicate have influenced their work. Although the exact confidence for each source paper cannot be determined without more detailed analysis, these are the papers that seem to have the most direct relevance to the paper's novel contributions.\n\nIn JSON format:\n\n{\n\"b34\": 0.9,\n\"b13\": 0.85,\n\"b28\": 0.85,\n\"b8\": 0.8,\n\"b37\": 0.8,\n\"b5\": 0.75\n} \n\nThis indicates that 'Attention is all you need' (b34) has the highest confidence score at 0.9, followed by 'Reformer: the efficient transformer' (b13) and 'Efficient content-based sparse attention with routing transformers' (b28) at 0.85, and so on.", "Based on the provided text, the author's methodology seems to be primarily influenced by the following references:\n\n- \"Attention is all you need\" (b34): The author's work is based on transforming this architecture.\n- \"Reformer: The Efficient Transformer\" (b13): The author takes inspiration from Reformer's strategy of using Locality Sensitive Hashing (LSH) but aims to tackle its limitations.\n- \"Efficient Content-Based Sparse Attention with Routing Transformers\" (b28): The author also refers to this paper's method of approximating attention patterns with Approximate Nearest Neighbors (ANN) approaches.\n- \"Rethinking Attention with Performers\" (b5): The author mentions using the kernelized attention techniques from this paper to approximate attention utilities.\n- \"A survey on Learning to Hash\" (b37): The author's methodology of Learning-to-Hash Attention (LHA) is influenced by the techniques outlined in this paper.\n- \"Predicting Attention Sparsity in Transformers\" (b33): The author's work is also related to this paper, particularly in terms of learning sparse attention patterns.\n\nSo, the final answer in json format should be:\n\n{\n\"b34\": 1,\n\"b13\": 0.9,\n\"b28\": 0.9,\n\"b5\": 0.8,\n\"b37\": 0.8,\n\"b33\": 0.7\n}", "Based on the text of the academic paper, the following references are identified as the most pertinent sources:\n\n1. \"Attention is all you need\" [b34] appears to be a key reference throughout the paper and is related to the main technique (Transformer architecture and self-attention mechanism) used in the paper. Confidence score: 0.9\n\n2. \"Reformer: The efficient transformer\" [b13] is mentioned multiple times in relation to the solution of approximating full attention matrix with a sparse one and the usage of Locality Sensitive Hashing (LSH). Confidence score: 0.85\n\n3. \"Routing transformer: efficient content-based sparse attention with routing transformers\" [b28] is referenced in relation to the Approximate Nearest Neighbors (ANN) approaches and mini-batch spherical k-means. Confidence score: 0.8\n\n4. \"Efficient attention using asymmetric clustering\" [b8] is mentioned in relation to the improvement of Reformer by introducing asymmetric transformation to queries and keys. Confidence score: 0.75\n\n5. \"Rethinking attention with performers\" [b5] is cited in the context of reducing the computational complexity in the training phase using unbiased kernelized attention techniques. Confidence score: 0.7\n\n6. \"Cluster former: clustering-based sparse transformer for long range dependency encoding\" [b38] is also related to Approximate Nearest Neighbors (ANN) approaches and mini-batch spherical k-means. Confidence score: 0.65\n\n7. \"Learning-to-Hash\" [b37] seems to be the major reference for the novel Learning-to-Hash Attention (LHA) proposed in the paper. Confidence score: 0.9\n\nThe result in json format is:\n\n{\n\"b34\": 0.9,\n\"b13\": 0.85,\n\"b28\": 0.8,\n\"b8\": 0.75,\n\"b5\": 0.7,\n\"b38\": 0.65,\n\"b37\": 0.9\n}"], "62c2a5625aee126c0fcca6f5": ["Based on the text of the paper, the following references seem to be the most relevant:\n\n1. \"b20\": Semi-supervised classification with graph convolutional networks - The paper directly mentions GCN, which was used as a basis for the work presented.\n2. \"b16\": Open graph benchmark datasets for machine learning on graphs - This paper provides the datasets used for testing the algorithm.\n3. \"b14\": Explaining and harnessing adversarial examples - This paper provides the foundation for understanding adversarial examples, which are central to the method proposed in the paper.\n4. \"b30\": Adversarial training for free - This paper's \"free\" adversarial training methods were leveraged in the paper's proposed method, FLAG.\n5. \"b35\": Generalizing to unseen domains via adversarial data augmentation - The paper cited this as an example of using adversarial data augmentation to improve generalization.\n6. \"b15\": Inductive representation learning on large graphs - Mentioned as a method that FLAG complements.\n7. \"b2\": A simple framework for contrastive learning of visual representations - This paper highlights the importance of using diverse data augmentations, which is adopted in the proposed method.\n\nIn the JSON format, the answer would be:\n```\n{\"b20\": 0.9, \"b16\": 0.9, \"b14\": 0.8, \"b30\": 0.8, \"b35\": 0.7, \"b15\": 0.6, \"b2\": 0.6}\n```\nThe confidence score is based on the degree to which the paper's authors indicate that the source paper inspired their work, as well as the relevance of the source paper to the key contributions of the paper.", "Based on the text, the most pertinent source papers that inspired the author are:\n\n1. b20: \"Semi-supervised classification with graph convolutional networks\"\n2. b30: \"Adversarial training for free\"\n3. b16: \"Open graph benchmark datasets for machine learning on graphs\"\n4. b35: \"Generalizing to unseen domains via adversarial data augmentation\"\n5. b25: \"Towards deep learning models resistant to adversarial attacks\"\n\nIn the JSON format, the result would be:\n\n{\n  \"b20\": 0.9,\n  \"b30\": 0.9,\n  \"b16\": 0.8,\n  \"b35\": 0.8,\n  \"b25\": 0.7\n}\n\nThe confidence scores are assigned based on the frequency and context of the references in the text. For instance, references b20, b30, and b35 are mentioned in key sections of the paper discussing the algorithm and methodology. Hence, they are assigned higher confidence scores.", "Based on the text, the most relevant source papers appear to be:\n\n- b20: \"Semi-supervised classification with graph convolutional networks\" - The authors mention this paper while discussing Graph Neural Networks (GNNs), indicating it could have been a significant influence in their own work.\n- b21: \"Imagenet classification with deep convolutional neural networks\" - The authors refer to this paper while discussing data augmentation, which is a key part of their own research.\n- b30: \"Adversarial training for free\" - This paper is mentioned in relation to the proposed FLAG method, indicating that it was a significant influence in the development of their algorithm.\n- b16: \"Open graph benchmark datasets for machine learning on graphs\" - This paper is mentioned multiple times in their work, and it appears that the authors have used the datasets from this paper for their own experiments.\n\nThe json format of the result is:\n\n{\n  \"b20\": 0.9,\n  \"b21\": 0.8,\n  \"b30\": 0.95,\n  \"b16\": 0.85\n}\nThis represents a high confidence that these papers were the most influential sources for the given academic paper.", "Based on the provided text and the instructions, the source papers that inspired the given paper the most are:\n\n1. \"Robust Optimization as Data Augmentation for Large Scale Graphs\" [None]: The title of the paper itself aligns with the main contribution of the given paper, indicating a strong influence. Confidence score: 0.9\n\n2. \"Adversarial Training for Free\" [b30]: The given paper directly talks about leveraging \"free\" adversarial training methods, which indicates a strong influence from this work. Confidence score: 0.85\n\n3. \"Generalizing to Unseen Domains via Adversarial Data Augmentation\" [b35]: The given paper mentions this source as an example of using adversarial perturbations to augment datasets, which aligns with the main contribution of the given paper. Confidence score: 0.8\n\n4. \"Adversarial Examples Improve Image Recognition\" [b39]: This source paper is mentioned in the context of discussing the success of adversarial augmentation in other domains, which suggests its influence on the given paper. Confidence score: 0.75\n\n5. \"Semi-Supervised Classification with Graph Convolutional Networks\" [b20]: This source paper is mentioned as the basis of Graph Neural Networks, which is the main area of study in the given paper. Confidence score: 0.7\n\n6. \"Fast is Better Than Free: Revisiting Adversarial Training\" [b38]: This paper is referred to in the context of discussing heuristics that can further accelerate the adversarial training process, indicating its influence on the given paper. Confidence score: 0.65\n\nThe JSON format of the answer is:\n\n{\n\"None\": 0.9,\n\"b30\": 0.85,\n\"b35\": 0.8,\n\"b39\": 0.75,\n\"b20\": 0.7,\n\"b38\": 0.65\n}", "Based on the text of the paper, the following papers can be identified as the most pertinent source papers:\n\n1. \"robust optimization as data augmentation for large scale graphs\": Based on the text, this paper appears to be the main source of inspiration for the authors. It is mentioned multiple times and the authors seem to base a large part of their methodology on the concepts and techniques presented in this paper. Confidence score: 0.9\n\n2. \"fastgcn fast learning with graph convolutional networks via importance sampling\": This paper is mentioned as a basis for the GCN methodology used in the paper. Confidence score: 0.85\n\n3. \"adversarial training for free\": The authors mention this paper as the source of the \"free\" adversarial training methods they use in their study. Confidence score: 0.8\n\n4. \"a simple framework for contrastive learning of visual representations\": This paper seems to have inspired the authors' use of multi-scale data augmentations. Confidence score: 0.7\n\n5. \"explaining and harnessing adversarial examples\": This paper seems to have guided the authors in their understanding of adversarial examples and how to use them for data augmentation. Confidence score: 0.65\n\n6. \"open graph benchmark datasets for machine learning on graphs\": This paper provides the datasets used in the study. Confidence score: 0.6\n\n7. \"generalizing to unseen domains via adversarial data augmentation\": This paper seems to have influenced the authors' understanding of how adversarial data augmentation can help generalize to out-of-distribution samples. Confidence score: 0.6\n\n8. \"adversarial examples improve image recognition\": This paper appears to have influenced the authors' understanding of the beneficial effects of adversarial examples on model accuracy. Confidence score: 0.55\n\nPlease note that these are inferred based on the text of the paper and the authors' notes, and may not represent the full list of source papers."], "6287492a5aee126c0ffe82dd": ["{\n  \"b23\": 0.9,\n  \"b25\": 0.8,\n  \"b33\": 0.8,\n  \"b39\": 0.7,\n  \"b45\": 0.7,\n  \"b7\": 0.6\n}", "{\n  \"b25\": 0.9,\n  \"b33\": 0.9,\n  \"b39\": 0.8,\n  \"b45\": 0.8,\n  \"b7\": 0.8\n}", "{\n  \"b39\": 0.9,\n  \"b7\": 0.9,\n  \"b45\": 0.85\n}", "{\n  \"b39\": 0.9,\n  \"b45\": 0.85,\n  \"b7\": 0.8\n}", "{\n  \"b25\": 0.85,\n  \"b33\": 0.85,\n  \"b45\": 0.8,\n  \"b23\": 0.75,\n  \"b39\": 0.75,\n  \"b7\": 0.7\n}"], "6327ec7990e50fcafd928963": ["```json\n{\n    \"b0\": 0.9,\n    \"b1\": 0.9,\n    \"b10\": 0.8,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7\n}\n```", "{\n    \"b0\": 0.9,\n    \"b1\": 0.9,\n    \"b9\": 0.8,\n    \"b10\": 0.7,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6\n}", "{\n    \"b0\": 0.9,\n    \"b1\": 0.9,\n    \"b9\": 0.8,\n    \"b10\": 0.7,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6\n}", "{\n  \"b0\": 0.9,\n  \"b1\": 0.9,\n  \"b3\": 0.8,\n  \"b4\": 0.8,\n  \"b5\": 0.8,\n  \"b6\": 0.8,\n  \"b7\": 0.8,\n  \"b8\": 0.8,\n  \"b9\": 0.9,\n  \"b10\": 0.85,\n  \"b12\": 0.75,\n  \"b13\": 0.7,\n  \"b14\": 0.7,\n  \"b15\": 0.7,\n  \"b16\": 0.7\n}", "{\n    \"b0\": 0.9,\n    \"b1\": 0.9,\n    \"b9\": 0.8,\n    \"b10\": 0.7,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6\n}"], "63520de390e50fcafd60ec75": ["Based on the text of the academic paper and the author note provided, the main challenge outlined in the paper is the lack of a semantic latent space in the diffusion model, which makes the generated images uncontrollable. The author proposes an asymmetric reverse process generative method, which helps in controlled high-quality image editing. \n\nThe author indicates that they are inspired by the following references:\n\n1. \"In image synthesis, diffusion models have advanced to achieve state-of-the-art performance regarding quality and mode coverage since the introduction of denoising diffusion probabilistic models [b10]\": The paper is heavily inspired by the work done in reference [b10], as it forms the basis for the author's research.\n\n2. \"Since their deterministic version enables nearly perfect reconstruction of input images [b32]\": The author uses the deterministic version proposed in reference [b32] to enable nearly perfect reconstruction of input images, which is a key part of the author's proposed method.\n\n3. \"On the other hand, generative adversarial networks [b8] inherently provide straightforward image editing in their latent space\": The author draws inspiration from the work done in reference [b8] on generative adversarial networks, which is used for straightforward image editing in their latent space.\n\n4. \"[b25] introduces an additional input to the reverse diffusion process: a latent vector from an original image embedded by an extra encoder. This latent vector contains the semantics to condition the process\": The author's proposed method is also inspired by the additional input introduced in reference [b25] to the reverse diffusion process.\n\nThe following references are also directly related to the author's novel contributions:\n\n5. \"Compared to directly minimizing the cosine distance between the edited image and the target description [b24]\": The author uses the method proposed in reference [b24] to compare the edited image and the target description.\n\n6. \"Inherently, the model should be trained from scratch. In this paper, we propose an asymmetric reverse process (Asyrp) which discovers the semantic latent space of a frozen diffusion model where modification in the space synthesizes various attributes on input images\": The author's proposed method, the asymmetric reverse process, is heavily influenced by the training method proposed in reference [b14].\n\nBased on the above analysis, the answer in json format is:\n\n{\n  \"b10\": 1,\n  \"b32\": 0.8,\n  \"b8\": 0.7,\n  \"b25\": 0.9,\n  \"b24\": 0.7,\n  \"b14\": 0.9\n}\n\nThe confidence scores are estimated based on how directly each reference is related to the author's novel contributions.", "Based on the provided text, the key challenges of the paper are to address the lack of a semantic latent space in diffusion models and to propose a new method for controlled, high-quality image editing. The paper proposes an asymmetric reverse process to discover a semantic latent space in a frozen diffusion model. The paper also introduces a principled design of the generative process for versatile editing and quality boosting. \n\nThe relevant source papers, based on their direct relevance to the paper's novel contributions and the presence of key phrases, might include:\n\n- 'denoising diffusion probabilistic models' [b10]: As the base diffusion model that the paper aims to improve upon.\n- 'diffusion models beat gans on image synthesis' [b6]: Likely to have influenced the comparison between diffusion models and GANs.\n- 'conditioning method for denoising diffusion probabilistic models' [b2]: Probably inspired the proposed method for conditioning in the reverse process.\n- 'diffusionclip text guided image manipulation using diffusion models' [b14]: Likely to have motivated the focus on image manipulation in the paper.\n- 'denoising diffusion implicit models' [b32]: As the deterministic version of the model, which enables nearly perfect reconstruction of input images, mentioned in the paper.\n- 'diffusion autoencoders toward a meaningful and decodable representation' [b25]: Might have inspired the idea of adding an additional input to the reverse diffusion process.\n- 'styleclip textdriven manipulation of stylegan imagery' [b24]: Likely influenced the use of CLIP embedding for image editing.\n\nThe JSON format result might be:\n\n{\n  \"b10\": 0.9,\n  \"b6\": 0.8,\n  \"b2\": 0.85,\n  \"b14\": 0.9,\n  \"b32\": 0.85,\n  \"b25\": 0.8,\n  \"b24\": 0.8\n}\n\nPlease note that the confidence scores are subjective and may vary. These are based on my understanding of the text and the mentioned papers.", "Based on the text of the paper and the provided titles of the references, the most pertinent source papers seem to be:\n\n- \"Denoising diffusion probabilistic models\" [b10]: This paper is often referred to as the foundation for the diffusion models, which is the main topic of the paper.\n- \"Diffusion models beat GANs on image synthesis\" [b6]: The author compares their work with this reference, indicating its relevance.\n- \"StyleGAN nada: clip-guided domain adaptation of image generators\" [b7]: Mentioned as a method for homogeneous editing.\n- \"Diffusion autoencoders: toward a meaningful and decodable representation\" [b25]: This paper is directly referenced as a method that the author builds upon.\n- \"Learning transferable visual models from natural language supervision\" [b26]: This paper's method is used for semantic similarity indication between images and texts.\n- \"Generative adversarial networks\" [b8]: This paper is referenced as a method that provides image editing in their latent space.\n- \"Denoising diffusion implicit models\" [b32]: This paper's method is mentioned as a way to guarantee nearly perfect inversion.\n\nThe confidence score for each paper can be inferred from the context in which they are mentioned in the paper. However, since this analysis is based on the provided text and there might be missing context, the confidence score is subjective and should be further validated.\n\nHere is the result in JSON format:\n\n```json\n{\n    \"b10\": 0.9,\n    \"b6\": 0.8,\n    \"b7\": 0.7,\n    \"b25\": 0.85,\n    \"b26\": 0.8,\n    \"b8\": 0.75,\n    \"b32\": 0.85\n}\n```", "Based on the given text and author notes, the following references can be inferred to have an influential role in the development of this paper:\n\n1. \"denoising diffusion probabilistic models\" [b10] is mentioned multiple times throughout the text and seems to form the basis of the diffusion models discussed in the paper.\n\n2. \"generative adversarial networks\" [b8] is mentioned as an alternative method to the diffusion models, indicating an influential role.\n\n3. \"denoising diffusion implicit models\" [b32] is central to the understanding of the diffusion process and methods used in the paper.\n\n4. \"styleclip textdriven manipulation of stylegan imagery\" [b24] is the basis of the image editing in the latent space, indicating an influential role.\n\n5. \"diffusion autoencoders toward a meaningful and decodable representation\" [b25] is mentioned in the paper as an alternative approach to the diffusion process, indicating a significant influence.\n\n6. \"learning transferable visual models from natural language supervision\" [b26] is mentioned in the context of multi-modal embeddings, indicating a significant influence.\n\n7. \"blended diffusion for text driven editing of natural images\" [b0] is mentioned when discussing the manipulation of images, indicating a significant influence.\n\n8. \"diffusionclip text guided image manipulation using diffusion models\" [b14] is mentioned when discussing the difficulties in manipulating images, indicating a significant influence.\n\nThe json format would be: \n{\n\"b10\": 1.0,\n\"b8\": 0.9,\n\"b32\": 0.9,\n\"b24\": 0.9,\n\"b25\": 0.8,\n\"b26\": 0.8,\n\"b0\": 0.8,\n\"b14\": 0.8\n}", "Based on the provided text and author notes, the following references seem to be the most influential to the paper. \n\n1. \"b10\": \"denoising diffusion probabilistic models\" - The paper uses the denoising diffusion probabilistic models for image synthesis and improvement of quality and mode coverage. Confidence Score: 0.9\n2. \"b32\": \"denoising diffusion implicit models\" - The deterministic version of these allows nearly perfect reconstruction of input images, which is crucial for image editing. Confidence Score: 0.8\n3. \"b14\": \"diffusionclip text guided image manipulation using diffusion models\" - This paper is referred to in the discussion of existing methods for image editing. Confidence Score: 0.8\n4. \"b0\": \"blended diffusion for text driven editing of natural images\" - This paper is also referred to in the discussion of existing methods for image editing. Confidence Score: 0.7\n5. \"b8\": \"generative adversarial networks\" - This work is mentioned as providing straightforward image editing in their latent space. Confidence Score: 0.7\n6. \"b24\": \"styleclip textdriven manipulation of stylegan imagery\" - This work is referred to when discussing the semantic similarity between images and texts. Confidence Score: 0.6\n7. \"b25\": \"diffusion autoencoders toward a meaningful and decodable representation\" - This is referred to as an inspiration for introducing an additional input to the reverse diffusion process. Confidence Score: 0.6\n\nThe above references are determined based on the frequency of their appearance in the text, their relevance to the main contributions of the paper, and their direct link to the challenges and methods proposed in the paper. The confidence scores are determined based on the same factors.\n\nThe JSON format result is: \n```json\n{\n    \"b10\": 0.9,\n    \"b32\": 0.8,\n    \"b14\": 0.8,\n    \"b0\": 0.7,\n    \"b8\": 0.7,\n    \"b24\": 0.6,\n    \"b25\": 0.6\n}\n```"], "63520de890e50fcafd60f4dd": ["Based on the given text and the author notes, the following reference papers are most likely to have inspired the paper:\n\n1. 'b66': 'Adapting language models for zero-shot learning by meta tuning on dataset and prompt collections'\n2. 'b61': 'Chain of thought prompting elicits reasoning in large language models'\n3. 'b39': 'Exploring the limits of transfer learning with a unified text-to-text transformer'\n4. 'b9': 'Scaling language modeling with pathways'\n5. 'b32': 'Training language models to follow instructions with human feedback'\n6. 'b11': 'Training verifiers to solve math word problems'\n\nThese references were chosen based on their relevance to the paper's main contributions, including the development of language models, instruction finetuning, chain-of-thought tasks and scaling up of models and tasks. The references b66, b61, b32 and b11 are directly linked to these aspects and are therefore likely to have heavily influenced the paper. The references b39 and b9 are linked to the paper's focus on language modeling and scaling, making them important sources of inspiration as well. \n\nHere are their confidence scores:\n\n```\n{\n    \"b66\": 0.9,\n    \"b61\": 0.85,\n    \"b39\": 0.8,\n    \"b9\": 0.8,\n    \"b32\": 0.75,\n    \"b11\": 0.75\n}\n```\n\nThese scores are based on the relevance of the references to the paper's main contributions and the frequency of their mention in the text. The higher the score, the more influential the reference is likely to have been.", "Based on the text of the academic paper, the source papers that directly inspired the author are:\n\n{ \n\"b66\": 1, \n\"b61\": 1, \n\"b39\": 0.8,\n\"b9\": 0.8,\n\"b15\": 0.7,\n\"b48\": 0.7,\n\"b10\": 0.6,\n\"b11\": 0.6\n}\n\nThe author refers to these papers, using phrases that suggest they were heavily relied upon in the development of the paper's core contributions. Notably, references 'b66' and 'b61' seem to have had the most direct influence, with clear indications of inspiration or motivation in the text. \n\nNote: Confidence score has been provided based on the frequency at which these references have been cited and the context in which they have been mentioned in the paper.", "Based on the given text and the author notes, the paper seems to be primarily about scaling instruction finetuned language models and improving their performance on a range of tasks, with a particular focus on reasoning abilities. The paper introduces novel methods such as instruction finetuning, chain-of-thought finetuning, and scaling up the number of tasks and the size of the model.\n\nLooking at the references, the following papers are most likely the main sources of inspiration for this paper:\n\n- 'b9': 'Scaling language modeling with pathways': The paper discusses scaling up the model, which is a major theme in the given paper.\n\n- 'b61': 'Chain of thought prompting elicits reasoning in large language models': The paper introduces chain-of-thought finetuning, which is used in the given paper to enhance the model's reasoning abilities.\n\n- 'b59': 'Finetuned language models are zero-shot learners': The paper discusses instruction finetuning, which is a key method used in the given paper.\n\n- 'b39': 'Exploring the limits of transfer learning with a unified text to text transformer': The paper appears to discuss models such as T5 which are mentioned in the given paper.\n\n- 'b66': 'Adapting language models for zero-shot learning by meta tuning on dataset and prompt collections': The paper seems to focus on zero-shot learning, which is a capability that the given paper aims to improve in their models.\n\nThe JSON format for these key source papers with a confidence score is as follows:\n\n{\n\"b9\": 0.9,\n\"b61\": 0.9,\n\"b59\": 0.9,\n\"b39\": 0.8,\n\"b66\": 0.8\n}", "Based on the provided text, it seems the paper is about enhancing the performance of language models through a method called instruction finetuning and chain-of-thought tasks. The authors have scaled up the number of tasks and the size of the model to improve its performance. They have specifically highlighted the work of multiple previous papers in their approach. \n\nHere are the most pertinent source papers that inspired this work:\n\n1. \"scaling instruction finetuned language models\": This paper might be the main inspiration since the title directly indicates a similar approach, i.e., instruction finetuning and model scaling, which is also implemented in the given paper. Confidence score: 0.9\n\n2. \"Training language models to follow instructions with human feedback[b32]\": This paper seems to have directly inspired the instruction finetuning approach used in the given paper. Confidence score: 0.85\n\n3. \"chain of thought prompting elicits reasoning in large language models[b61]\": This paper inspired the chain-of-thought tasks applied in the given paper. Confidence score: 0.85\n\n4. \"exploring the limits of transfer learning with a unified text to text transformer[b39]\": This paper might have influenced the use of transformer models in the given paper. Confidence score: 0.8\n\n5. \"scaling language models methods analysis insights from training gopher[b38]\": This paper might have guided the model scaling approach implemented in the given paper. Confidence score: 0.8\n\n6. \"language models are few shot learners[b5]\": This paper could have inspired the few-shot learning approach used in the given paper. Confidence score: 0.75\n\n7. \"scaling language models with mixture of experts icml[b13]\": This paper might have contributed to the ideas about scaling language models implemented in the given paper. Confidence score: 0.7\n\n8. \"adapting language models for zero shot learning by meta tuning on dataset and prompt collections[b66]\": This paper might have influenced the zero-shot learning approach used in the given paper. Confidence score: 0.7\n\nIn JSON format, the references and their confidence scores are:\n\n{\n    \"b0\": 0.9,\n    \"b32\": 0.85,\n    \"b61\": 0.85,\n    \"b39\": 0.8,\n    \"b38\": 0.8,\n    \"b5\": 0.75,\n    \"b13\": 0.7,\n    \"b66\": 0.7\n}", "Based on the given text, the primary challenge outlined in the paper is to develop models that can generalize to unseen tasks, with a focus on artificial intelligence and natural language processing. The algorithm proposed by the author involves instruction finetuning, increasing the number of tasks and model size, and including Chain-of-thought (CoT) data. \n\nFrom the text, the phrases that indicate reliance on previous research include:\n\n- In natural language processing (NLP), pretrained language models have made significant progress towards this goal, as they can perform tasks given natural language descriptions [b66].\n- Further progress has been made by finetuning language models on a collection of tasks phrased as instructions, which enables models to respond better to instructions and reduces the need for few-shot exemplars [b32] [b59] [b66].\n- Our experiments show that whereas prior instruction finetuning methods that do not include chain-of-thought (CoT; [b61] severely degrade performance on CoT evaluations.\n- Flan-PaLM outperforms PaLM, achieving new state-of-theart on several benchmarks [b15].\n- Flan-T5 outperforms base T5 by double-digit improvements and even outperforms PaLM 62B on some challenging BIG-Bench tasks [b48].\n\nBased on the key phrases, the source papers are:\n\n- \"Adapting language models for zero-shot learning by meta tuning on dataset and prompt collections\" [b66]\n- \"Finetuned language models are zero-shot learners\" [b32]\n- \"Chain of thought prompting elicits reasoning in large language models\" [b61]\n- \"Measuring massive multitask language understanding\" [b15]\n- \"Beyond the imitation game quantifying and extrapolating the capabilities of language models\" [b48]\n\nThe titles of these papers suggest a strong relevance to the novel contributions of the paper, especially in terms of instruction finetuning, multi-task language understanding, and chain-of-thought prompting.\n\nThe result in json format:\n\n{\n\"b66\": 1,\n\"b32\": 0.9,\n\"b61\": 0.9,\n\"b15\": 0.8,\n\"b48\": 0.8\n}"], "6389d6e490e50fcafdff693f": ["Based on the given text and author notes, the most pertinent source papers that have inspired the given paper could be identified as:\n\n1. \"SoftSKU: Optimizing Server Architectures for Microservice Diversity & Scale\" [b19] - This paper is mentioned multiple times throughout the text, indicating its significant influence on the author's work. It discusses the inefficiency of current CPUs in data centers and the necessity to increase on-chip throughput. The author of the given paper seems to have adopted and expanded upon these ideas.\n\n2. \"SIMT X: Extending Single Instruction Multi Threading to Out-Of-Order Cores\" [b60] - As the author's paper proposes using the SIMT model while maintaining OoO execution, this paper seems to be a direct source of inspiration.\n\n3. \"SIMD-on-SIMD Compilers, like Intel ISPC\" [b41] - While this is not a paper, but a tool, it is mentioned in the context of running SIMT-friendly microservices on CPU SIMD units, which is a core concept in the author's paper.\n\n4. \"A Domain Specific Supercomputer for Training Deep Neural Networks\" [b6] - This paper is cited early in the text, indicating its potential influence on the overall approach and motivation of the author's work.\n\n5. \"Rhythm: Harnessing Data Parallel Hardware for Server Workloads\" [b36] - This paper is mentioned in the context of the shortcomings of using GPU hardware to execute data center applications. The author of the given paper seems to have taken these shortcomings into consideration when developing their proposed solution.\n\n6. \"Scale-Out Processors\" [b28] - This paper is cited in the context of increasing on-chip throughput to make better use of resources in data centers, which is a key concept in the author's paper.\n\n7. \"A New Golden Age for Computer Architecture\" [b2] - This paper is cited in the context of the challenges faced due to the slowing of Moore's Law, which seems to be a primary motivator for the author's work.\n\nHere is the answer in JSON format:\n\n{\n  \"b19\": 0.9,\n  \"b60\": 0.8,\n  \"b41\": 0.7,\n  \"b6\": 0.7,\n  \"b36\": 0.7,\n  \"b28\": 0.6,\n  \"b2\": 0.6\n}", "Based on the provided information and the key phrases in the text, the most pertinent source papers that inspired the given paper are:\n\n{\n\"b19\": 0.9,\n\"b22\": 0.8,\n\"b24\": 0.8,\n\"b28\": 0.8,\n\"b35\": 0.9,\n\"b36\": 0.9,\n\"b60\": 0.8,\n\"b63\": 0.8\n}\n\nThese papers discuss relevant topics such as server architectures for microservice diversity, the use of GPU hardware for data center applications, and extending single instruction multi-threading to out-of-order cores. They are mentioned in the context of being built upon or directly compared with in the given paper, indicating a significant level of inspiration or motivation.", "Based on the text of the academic paper and the author's notes provided, the author's research on the Request Processing Unit (RPU) was most likely inspired by the following source papers:\n\n- [b19]: \"SoftSKU: Optimizing Server Architectures for Microservice Diversity Scale\" - The paper discusses the inefficiency of current CPUs in data centers and suggests an increase in the number of threads on-chip is necessary to better use these resources, which is a concept closely related to the idea of the RPU. Confidence score: 0.85\n- [b22]: \"Profiling a warehouse scale computer\" - This paper might have contributed to the understanding of the inefficiencies of current CPUs in data centers. Confidence score: 0.7\n- [b24]: \"Memory hierarchy for web search\" - The author's work on RPU seems to build upon ideas about CPU inefficiencies and memory hierarchies discussed in this paper. Confidence score: 0.65\n- [b36]: \"Rhythm: Harnessing Data Parallel Hardware for Server Workloads\" - The author notes the limitations of GPUs in executing data center applications, which suggests this paper might have been a reference point when developing the RPU. Confidence score: 0.75\n- [b62]: \"General purpose graphics processor architectures\" - The author's work on RPU, which is a merger of CPU and GPU concepts, seems to be inspired by the principles discussed in this paper. Confidence score: 0.7\n- [b63]: \"Stack less SIMT reconvergence at low cost\" - The RPU design appears to incorporate the concept of SIMT reconvergence discussed in this paper. Confidence score: 0.8\n- [b74]: \"Improving GPU Performance via Large Warps and Two-Level Warp Scheduling\" - This paper's discussions on improving GPU performance through warp scheduling could have been an inspiration for the author's RPU design. Confidence score: 0.7\n\nNote: The confidence scores are based on the assumption that the author's work was directly inspired by these papers. The actual influence of each paper may vary. \n\nFormatted in JSON:\n\n{\n\"b19\": 0.85,\n\"b22\": 0.7,\n\"b24\": 0.65,\n\"b36\": 0.75,\n\"b62\": 0.7,\n\"b63\": 0.8,\n\"b74\": 0.7\n}", "Based on the text and the author's notes, the source papers that most directly influenced the main contributions of the paper are:\n\n- b19: \"softsku optimizing server architectures for microservice diversity scale\",\n- b22: \"profiling a warehouse scale computer\",\n- b24: \"memory hierarchy for web search\",\n- b35: \"memcachedgpu scaling up scale out key value stores\",\n- b36: \"rhythm harnessing data parallel hardware for server workloads\",\n- b62: \"general purpose graphics processor architectures\",\n- b74: \"improving gpu performance via large warps and two level warp scheduling\",\n- b93: \"parvec vectorizing the parsec benchmark suite\".\n\nThese papers are identified based on their relevance to the challenges and methods outlined in the text, as well as the language indicating direct inspiration or motivation.\n\nHere are the results in the requested json format:\n\n```json\n{\n    \"b19\": 0.9,\n    \"b22\": 0.85,\n    \"b24\": 0.9,\n    \"b35\": 0.8,\n    \"b36\": 0.9,\n    \"b62\": 0.8,\n    \"b74\": 0.85,\n    \"b93\": 0.8\n}\n```\n\nThe confidence scores are inferred based on the relevance of each source paper to the challenges and methods outlined in the text, as well as the language indicating direct inspiration or motivation.", "Based on the text and the author's notes, there are several references that seem to have contributed significantly to the paper's development. The paper's main contributions appear to be the Request Processing Unit (RPU), a general-purpose architecture customized for microservices, and its co-design with a SIMR-aware software system. The RPU leverages the frontend and memory system design of SIMT processors, while maintaining OoO execution and support for the CPU's ISA and software stack. \n\nKey source papers identified are:\n\n1. b19: \"SoftSKU: Optimizing Server Architectures for Microservice Diversity & Scale\" - It is clear from the text that this paper's focus on optimizing server architectures for microservices and its demonstration that current CPUs are inefficient when executing data center workloads, with many underutilized resources, has had a significant influence on the author's thinking and the development of RPU.\n\n2. b62: \"General Purpose Graphics Processor Architectures\" - This paper seems to have been influential in the development of the RPU's SIMT pipeline and its handling of control flow divergence.\n\n3. b63: \"Stack-less SIMT Reconvergence at Low Cost\" - This paper's discussion of stack-less SIMT reconvergence appears to have influenced the author's design of the RPU's control flow divergence handling.\n\n4. b35: \"MemcachedGPU: Scaling up Scale-out Key-value Stores\" - This paper seems to have been a source of inspiration in the author's approach to leveraging the energy-efficient nature of the GPU's execution model and scalable memory system by the RPU.\n\n5. b36: \"Rhythm: Harnessing Data Parallel Hardware for Server Workloads\" - The author's comparison of the RPU to GPUs and CPUs and the discussion of the challenges of using GPU hardware to execute data center applications appears to be influenced by this paper.\n\n6. b74: \"Improving GPU Performance via Large Warps and Two-level Warp Scheduling\" - The author's choice to implement sub-batch interleaving in the RPU's SIMT pipeline seems to be a direct result of this paper's findings.\n\n7. b103: \"Are Dynamic Memory Managers on GPUs Slow? A Survey and Benchmarks\" - This paper likely influenced the author's approach to memory management in the RPU. \n\nThe result in the json format is:\n{\n    \"b19\": 0.9,\n    \"b62\": 0.9,\n    \"b63\": 0.9,\n    \"b35\": 0.8,\n    \"b36\": 0.8,\n    \"b74\": 0.8,\n    \"b103\": 0.8\n}"], "63a2c50090e50fcafdb97c2f": ["Based on the text and the titles of the referenced papers, the most relevant source papers are:\n\n1. b39: \"Aligning books and movies towards story like visual explanations by watching movies and reading books\". This paper is mentioned several times in the text and seems to form the basis of the research, as it is the main inspiration for the UVLP method proposed in the paper.\n\n2. b38: \"Unsupervised vision and language pre-training via retrieval-based multi granular alignment\". This paper is also mentioned several times and seems to have influenced the UVLP method, especially in terms of data enrichment and the use of multi-granular alignment tasks.\n\n3. b5: \"An image is worth x words transformers for image recognition at scale\". This paper introduces the ViT model, which is mentioned as a promising alternative to object detectors for general vision tasks.\n\n4. b4: \"BERT: pre-training of deep bidirectional transformers for language understanding\". BERT is used to initialize the multimodal encoder and word embeddings in the E2E-UVLP framework.\n\n5. b33: \"E2E VLP: End-to-end vision language pre-training enhanced by visual learning\". This paper likely provided foundational ideas for developing the E2E-UVLP mentioned in the paper.\n\n6. b17: \"OSCAR: Object semantics aligned pre-training for vision language tasks\". This paper is referred to when discussing the masked tag prediction (MTP) pre-training task.\n\n7. b6: \"An empirical study of training end-to-end vision and language transformers\". This paper is specifically mentioned in relation to the limitations of the MTP objective and the need for the novel referring expression matching (REM) task.\n\nSo the returned JSON format should be:\n\n{\"b39\": 1, \"b38\": 0.9, \"b5\": 0.8, \"b4\": 0.7, \"b33\": 0.6, \"b17\": 0.5, \"b6\": 0.4}\n", "Based on the text, the following references could potentially be the source papers:\n\n1. \"end to end unsupervised vision and language pre training with referring expression matching\" [b0]\n2. \"aligning books and movies towards story like visual explanations by watching movies and reading books\" [b39]\n3. \"unsupervised vision and language pre training via retrieval based multi granular alignment\" [b38]\n4. \"seeing out of the box end to end pre training for visionlanguage representation learning\" [b8]\n5. \"an empirical study of training end to end vision and language transformers\" [b6]\n6. \"vilt vision and language transformer without convolution or region supervision\" [b11]\n\nBased on the text, the following references are the most pertinent source papers:\n\n1. \"end to end unsupervised vision and language pre training with referring expression matching\" [b0] - Confidence Score: 0.95\n2. \"aligning books and movies towards story like visual explanations by watching movies and reading books\" [b39] - Confidence Score: 0.9\n3. \"unsupervised vision and language pre training via retrieval based multi granular alignment\" [b38] - Confidence Score: 0.85\n4. \"seeing out of the box end to end pre training for visionlanguage representation learning\" [b8] - Confidence Score: 0.8\n5. \"an empirical study of training end to end vision and language transformers\" [b6] - Confidence Score: 0.75\n6. \"vilt vision and language transformer without convolution or region supervision\" [b11] - Confidence Score: 0.7\n\nResult in JSON format: \n\n{\n\"b0\": 0.95,\n\"b39\": 0.9,\n\"b38\": 0.85,\n\"b8\": 0.8,\n\"b6\": 0.75,\n\"b11\": 0.7\n}\n", "Based on the text and the information given, the main source papers that inspired the author are:\n- [b39]: \"Aligning books and movies towards story-like visual explanations by watching movies and reading books\"\n- [b38]: \"Unsupervised vision and language pre-training via retrieval-based multi-granular alignment\"\n- [b11]: \"VILT: Vision and Language Transformer without Convolution or Region Supervision\"\n- [b5]: \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"\n- [b4]: \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"\n\nThe JSON output is:\n{\n    \"b39\": 0.9,\n    \"b38\": 0.9,\n    \"b11\": 0.8,\n    \"b5\": 0.7,\n    \"b4\": 0.7\n}\nThis is based on the frequency of their mentions in the paper, their direct relevance to the novel contributions of the paper, and the phrases indicating inspiration or adoption of methods from these papers. The confidence scores are based on the strength of these factors.", "Based on the text, the paper presents a solution to the challenge of conducting unsupervised vision-and-language pre-training (VLP) in an end-to-end manner, with a new framework called E2E-UVLP. It introduces a novel pre-training task, Referring Expression Matching (REM), to improve the model's ability to handle complex attributes of objects. The paper also uses Masked Tag Prediction (MTP) and Masked Language Modeling (MLM) in the pre-training tasks. \n\nThe primary source papers can be identified as:\n\n- [b39]: \"Aligning books and movies towards story like visual explanations by watching movies and reading books\" \n- [b38]: \"Unsupervised vision and language pre training via retrieval based multi granular alignment\"\n- [b5]: \"An image is worth x words transformers for image recognition at scale\"\n- [b17]: \"Oscar object semantics aligned pre training for vision language tasks\"\n- [b4]: \"Bert pre training of deep bidirectional transformers for language understanding\"\n\nThese papers were cited in the context of inspiration and adoption of methods, ideas or challenges. However, without access to the full text and citation context in the paper, this is a best guess based on the provided information. \n\nThe result is:\n\n```\n{\n  \"b39\": 0.9,\n  \"b38\": 0.8,\n  \"b5\": 0.8,\n  \"b17\": 0.7,\n  \"b4\": 0.7\n}\n```\nThe confidence scores are ranked based on the frequency of mentions and the context of the mentions in the provided text.", "Based on the task description and the paper text, the most pertinent source papers that inspire this paper are:\n\n1. [b39]: 'Aligning books and movies towards story like visual explanations by watching movies and reading books' - The paper explicitly mentions this work multiple times, indicating strong reliance on its research.\n2. [b38]: 'Unsupervised vision and language pre-training via retrieval-based multi-granular alignment' - This paper is also directly mentioned and discussed in the context of the novel contributions of the paper.\n3. [b5]: 'An image is worth x words transformers for image recognition at scale' - This paper is mentioned in the context of vision pre-trained models, which is one of the methods the paper introduces to tackle the main challenges.\n4. [b4]: 'BERT: Pretraining of deep bidirectional transformers for language understanding' - This paper is mentioned in the context of the used masked language modeling (MLM) task.\n5. [b17]: 'OSCAR: Object semantics aligned pre-training for vision-language tasks' - This paper is mentioned in the context of deriving a masked tag prediction (MTP) pre-training task.\n\nThe JSON representation of the result is:\n\n{\n    \"b39\": 1,\n    \"b38\": 0.9,\n    \"b5\": 0.8,\n    \"b4\": 0.8,\n    \"b17\": 0.7\n}"], "62be66205aee126c0f07d1e0": ["Based on the text and notes provided, the paper seems to focus on evaluating the performance of GitHub Copilot, an AI-based assistant for developers, and comparing it with human solutions on programming tasks. Here are the potential source papers that may have heavily influenced this work:\n\n1. \"Language models are few shot learners\" [b3]: This paper likely inspired the use of large language models for program synthesis, as it introduced GPT-3, the language model Codex (used by Copilot) is based on.\n\n2. \"Evaluating large language models trained on code\" [b4]: This study, which examined the performance of Codex, likely served as a direct precursor and reference point for this paper.\n\n3. \"An empirical evaluation of GitHub Copilot\u2019s code suggestions\" [b24]: This paper likely served as a key reference, as it also evaluated Copilot's performance but did not analyze efficiency or compare with human solutions, which are the main contributions of the current paper.\n\n4. \"Competition level code generation with AlphaCode\" [b20]: This paper might have inspired the comparison of AI-generated code with human solutions.\n\n5. \"Expectation vs experience evaluating the usability of code generation tools powered by large language models\" [b34]: This paper studied how Copilot can help programmers complete a task, which likely informed the current paper's approach to assessing Copilot's utility as an AI assistant for developers.\n\n6. \"Choose your programming copilot a comparison of the program synthesis performance of GitHub Copilot and genetic programming\" [b31]: This paper likely influenced the current work by establishing a precedent for comparing Copilot's performance with other methods.\n\nThe confidence score for each paper is determined based on the relevance to the main contributions of the paper as inferred from the given text:\n\n1. \"Language models are few shot learners\" [b3]: 0.9\n2. \"Evaluating large language models trained on code\" [b4]: 1.0\n3. \"An empirical evaluation of GitHub Copilot\u2019s code suggestions\" [b24]: 0.8\n4. \"Competition level code generation with AlphaCode\" [b20]: 0.7\n5. \"Expectation vs experience evaluating the usability of code generation tools powered by large language models\" [b34]: 0.8\n6. \"Choose your programming copilot a comparison of the program synthesis performance of GitHub Copilot and genetic programming\" [b31]: 0.7\n\nThe result in json format:\n\n{\n    \"b3\": 0.9,\n    \"b4\": 1.0,\n    \"b24\": 0.8,\n    \"b20\": 0.7,\n    \"b34\": 0.8,\n    \"b31\": 0.7\n}", "Based on the text and the instructions, the following references appear to be the source papers that most directly inspired the paper's core contributions:\n\n1. [b4]: \"Evaluating Large Language Models Trained on Code\"\n2. [b3]: \"Language Models are Few-Shot Learners\"\n3. [b10]: \"Solving Linear Algebra by Program Synthesis\"\n4. [b24]: \"An Empirical Evaluation of GitHub Copilot\u2019s Code Suggestions\"\n5. [b33]: \"Solving Probability and Statistics Problems by Program Synthesis\"\n6. [b7]: \"Introduction to Algorithms\"\n7. [b31]: \"Choose Your Programming Copilot: A Comparison of the Program Synthesis Performance of GitHub Copilot and Genetic Programming\"\n\nThe json format would be as follows: \n\n{\n\"b4\": 0.9,\n\"b3\": 0.85,\n\"b10\": 0.8,\n\"b24\": 0.8,\n\"b33\": 0.7,\n\"b7\": 0.7,\n\"b31\": 0.65\n}\n\nThese references are selected based on their relevance to the paper's core contributions and the author's explicit and implicit indications of their influence on the work. The confidence score assigned to each reference reflects its perceived importance in shaping the paper's contributions, with a higher score indicating a stronger influence.", "Based on the provided information, the paper primarily focuses on evaluating the effectiveness of an AI-based assistant, Copilot, in solving fundamental algorithmic problems and comparing its solutions with human solutions. \n\nThe paper seems to take inspiration from various sources that have previously evaluated similar automatic code generation models or specifically evaluated Copilot's solutions' correctness, but it uniquely contributes by comparing Copilot's solutions with human solutions in solving programming tasks. \n\nThe most relevant source papers, based on the text and the key phrases that indicate inspiration or direct reliance, are:\n\n1. \"One such model is Codex [b4] ; a GPT-3 [b3] language model with up to 12 billion parameters which has been fine-tuned with 159 GB of code samples from 54 million GitHub repositories.\" This indicates that the work of [b4] and [b3] directly influenced the design and implementation of Copilot, which is the central focus of this study.\n\n2. \"There are already studies that evaluated the correctness of Copilot's solutions on different tasks [b10] [b33]\". This shows that the authors were aware of the existing research on Copilot's correctness in different tasks, which likely influenced their decision to focus on comparing Copilot's solutions with human solutions.\n\n3. \"Nguyen and Nadi [b24] compared the complexity of Copilot's solutions in different programming languages, besides their correctness.\" This suggests that the work of [b24] directly influenced the authors' decision to evaluate Copilot's solutions' complexity.\n\n4. \"Authors in [b34] conducted a user study to understand how Copilot can help programmers complete a task.\" This indicates that the user study conducted by [b34] might have influenced the authors' decision to compare Copilot's solutions with human solutions.\n\n5. \"Sobania et al. [b31] compared Copilot with a genetic programming (GP)-based approach that achieved good performance in program synthesis.\" This suggests that the work of [b31] might have inspired the authors to evaluate Copilot's performance in program synthesis.\n\nBased on this analysis, the source papers for this study are: [b4], [b3], [b10], [b33], [b24], and [b31]. \n\nIn JSON format:\n{\n  \"b4\": 0.9,\n  \"b3\": 0.9,\n  \"b10\": 0.75,\n  \"b33\": 0.75,\n  \"b24\": 0.8,\n  \"b31\": 0.7\n}", "Based on the text of the paper, the most pertinent source papers that inspired the author are:\n\n- \"Language models are few-shot learners\" [b3]: This paper is about the transformer architecture used in the current paper for automating code generation. Therefore, it is likely to be a significant source of inspiration.\n- \"Evaluating large language models trained on code\" [b4]: This paper is about the Codex model, which also uses the transformer architecture and has been fine-tuned with code samples. The authors of the current paper have specifically mentioned Codex in their work, so this paper is likely to be a key source.\n- \"An empirical evaluation of GitHub Copilot's code suggestions\" [b24]: This paper is about evaluating the correctness of Copilot's solutions and the complexity of Copilot's solutions in different programming languages, which are also the focus of the current paper.\n- \"The potential of artificial intelligence as a method of software developer's productivity improvement\" [b23]: This paper discusses the potential of tools like Copilot to improve the productivity of developers, which is a topic closely related to the current paper.\n\nThe source papers are:\n\n1. \"Language models are few-shot learners\" [b3]\n2. \"Evaluating large language models trained on code\" [b4]\n3. \"An empirical evaluation of GitHub Copilot's code suggestions\" [b24]\n4. \"The potential of artificial intelligence as a method of software developer's productivity improvement\" [b23]\n\nThe JSON format of the source papers with confidence scores are:\n\n{\n\"b3\": 0.9,\n\"b4\": 0.85,\n\"b24\": 0.8,\n\"b23\": 0.75\n}", "Based on the text of the academic paper, the source papers that have significantly inspired the author's work are:\n\n1. \"Language models are few-shot learners\" [b3] - This paper is directly mentioned as the foundation for the Codex model, which is a key part of the study. Confidence score: 0.9\n2. \"Evaluating large language models trained on code\" [b4] - This paper is directly linked to Codex and its performance, which is a central part of the study. Confidence score: 0.9\n3. \"Is GitHub\u2019s Copilot as bad as humans at introducing vulnerabilities in code?\" [b2] - This paper's theme and findings seem to be a direct inspiration for the author's evaluation of Copilot's vulnerability issues. Confidence score: 0.8\n4. \"An empirical evaluation of GitHub Copilot's code suggestions\" [b24] - The author's work appears to be directly motivated by this paper, as it involves a similar empirical evaluation of Copilot. Confidence score: 0.85\n5. \"The potential of artificial intelligence as a method of software developer's productivity improvement\" [b23] - The paper is related to the productivity of developers, which is a key theme of the author's work. Confidence score: 0.75\n6. \"Choose your programming copilot: A comparison of the program synthesis performance of GitHub Copilot and genetic programming\" [b31] - This paper's theme and findings seem to be a direct inspiration for the author's comparison studies of Copilot. Confidence score: 0.8\n\nTherefore, my final result is:\n\n{\n\"b3\": 0.9,\n\"b4\": 0.9,\n\"b2\": 0.8,\n\"b24\": 0.85,\n\"b23\": 0.75,\n\"b31\": 0.8\n}"], "633e477c90e50fcafde5a596": ["Based on the given text and the author notes, the most pertinent source papers that inspired the work can be identified by references that are directly linked to the main challenges and methods proposed in the paper. The paper focuses on using multiple prompts to improve the performance of large language models (LLMs) and proposes a novel method for aggregating the predictions of these prompts. The paper references several previous works to support its arguments and methodologies, however, not all are directly related to its novel contributions. \n\nBy examining the references that are directly related to the work's novel methods and challenges, the following source papers can be identified:\n\n1. \"Language Models Are Few-Shot Learners\" [b0]: This paper is mentioned several times and is directly linked to the challenges and methods discussed in the paper. The author specifically mentions the improvements they made based on the prompts proposed in this paper. \n\n2. \"Training Language Models to Follow Instructions with Human Feedback\" [b3]: The author discusses how this paper complements their goal of improving the prompting performance of off-the-shelf language models without additional fine-tuning.\n\n3. \"Weak Supervision (WS) WS is a powerful framework that learns the accuracies and correlations of multiple noisy sources and aggregates them to produce weak labels for training data\" [b12]. The author uses WS to improve the reliability of prompting with off-the-shelf LLMs and no further training, which is a novel contribution of the paper.\n\n4. \"Multitask Prompted Training Enables Zero-Shot Task Generalization\" [b4]. The author uses this paper to inspire the creation of prompt-chains and the variation of chains through in-context demonstrations and the style of prompt questions.\n\n5. \"Snorkel: Rapid Training Data Creation with Weak Supervision\" [b12]. This paper is mentioned in the context of the author's proposed use of weak supervision (WS) for reliably aggregating predictions, which is a novel contribution of the paper.\n\nBased on the above analysis, the source papers that inspired the author most are:\n\n{ \n    \"b0\": 1,\n    \"b3\": 0.8,\n    \"b12\": 0.9,\n    \"b4\": 0.8\n}", "Based on the text of the paper, the primary challenges outlined are: effective prompts, scalable collection, and prompt aggregation. The authors propose an approach named ASK ME ANYTHING PROMPTING (AMA) to tackle these challenges. \n\nThe paper includes several key phrases indicating reliance on previous research and inspiration from them. Here are the source papers identified based on the mentioned criteria:\n\n1. \"language models are few shot learners\" [b0]\n2. \"on the opportunities and risks of foundation models\" [b1]\n3. \"calibrate before use improving few shot performance of language models\" [b2]\n4. \"multitask prompted training enables zero shot task generalization\" [b4]\n5. \"chain of thought prompting elicits reasoning in large language models\" [b5]\n6. \"snorkel rapid training data creation with weak supervision\" [b12]\n7. \"learning dependency structures for weak supervision models\" [b13]\n\nBased on the provided information, the confidence score for each reference is not directly mentioned in the text. However, given that these references are directly linked to the main contributions and challenges of the paper, they have a high likelihood of being sources of inspiration for the authors.\n\nHere is the answer in the required JSON format:\n\n{\n\"b0\": 0.9,\n\"b1\": 0.8,\n\"b2\": 0.8,\n\"b4\": 0.8,\n\"b5\": 0.8,\n\"b12\": 0.8,\n\"b13\": 0.8\n}", "Based on the provided text, the source papers that inspired the given paper can be determined. The paper primarily deals with the application of large language models (LLMs) for generating a series of prompts and using a probabilistic graphical model for aggregating the results. The challenges identified are related to the effectiveness of prompts, scalability of collection, and prompt aggregation. \n\nAfter analyzing the text and the methods proposed, the most relevant source papers can be identified by looking for phrases indicating inspiration or motivation and cross-referencing them with the provided list of reference titles. The references that appear to be most directly connected to the paper's novel contributions are:\n\n1. \"Language models are few shot learners\" [b0]\n2. \"Multitask prompted training enables zero shot task generalization\" [b4]\n3. \"Chain of thought prompting elicits reasoning in large language models\" [b5]\n4. \"Snorkel rapid training data creation with weak supervision\" [b12]\n5. \"Learning dependency structures for weak supervision models\" [b13]\n6. \"Maieutic prompting logically consistent reasoning with recursive explanations\" [b9]\n\nNote that the references are identified based on their relevance to the paper's challenges and proposed methods, as well as the indications of inspiration or motivation in the text. However, the confidence scores would need to be calculated based on a more quantitative analysis of the text.\n\nThe resulting JSON format with hypothetical confidence scores could look like this:\n\n```json\n{\n    \"b0\": 0.9,\n    \"b4\": 0.85,\n    \"b5\": 0.8,\n    \"b12\": 0.9,\n    \"b13\": 0.8,\n    \"b9\": 0.8\n}\n```\n\nPlease note that the confidence scores are hypothetical and would need to be calculated based on a more quantitative analysis of the text.", "Based on the text, the following papers seem to be the most relevant sources:\n\n1. \"Language models are few shot learners\" [b0]\n2. \"Multitask prompted training enables zero shot task generalization\" [b4]\n3. \"Chain of thought prompting elicits reasoning in large language models\" [b5]\n4. \"Snorkel rapid training data creation with weak supervision\" [b12]\n5. \"Learning dependency structures for weak supervision models\" [b13]\n6. \"What makes good in-context examples for GPT\" [b24]\n7. \"Self consistency improves chain of thought reasoning in language models\" [b25]\n8. \"Language models in the loop incorporating prompting into weak supervision\" [b29]\n\nThe confidence score can be calculated by counting the number of times that a particular paper is cited. For example, if paper [b0] is cited 10 times, the confidence score would be 10/total number of citations.\n\nThe result in the json format should be:\n\n{\"b0\": 0.8, \"b4\": 0.7, \"b5\": 0.7, \"b12\": 0.8, \"b13\": 0.8, \"b24\": 0.7, \"b25\": 0.7, \"b29\": 0.7}", "Based on the text and the context of the paper, the source papers are:\n1. 'Language models are few-shot learners' [b0]\n2. 'ASK ME ANYTHING PROMPTING (AMA)' [None]\n3. 'MAIEUTIC PROMPTING: Logically Consistent Reasoning with Recursive Explanations' [b9]\n4. 'A.I. Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts' [b8]\n5. 'Snorkel: Rapid Training Data Creation with Weak Supervision' [b12]\n6. 'Learning Dependency Structures for Weak Supervision Models' [b13]\n7. 'Multitask Prompted Training Enables Zero-Shot Task Generalization' [b4]\n8. 'Chain of thought: prompting elicits reasoning in large language models' [b5]\n\nThe confidence score of each source paper:\n1. 'Language models are few-shot learners' [b0] : 1.0\n2. 'ASK ME ANYTHING PROMPTING (AMA)' [None] : 1.0\n3. 'MAIEUTIC PROMPTING: Logically Consistent Reasoning with Recursive Explanations' [b9] : 0.8\n4. 'A.I. Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts' [b8] : 0.9\n5. 'Snorkel: Rapid Training Data Creation with Weak Supervision' [b12] : 0.7\n6. 'Learning Dependency Structures for Weak Supervision Models' [b13] : 0.7\n7. 'Multitask Prompted Training Enables Zero-Shot Task Generalization' [b4] : 0.8\n8. 'Chain of thought: prompting elicits reasoning in large language models' [b5]: 0.8\n\nThe JSON format is:\n{\n\"b0\" : 1.0,\n\"None\" : 1.0,\n\"b9\" : 0.8,\n\"b8\" : 0.9,\n\"b12\" : 0.7,\n\"b13\" : 0.7,\n\"b4\" : 0.8,\n\"b5\" : 0.8\n}"], "62afe5495aee126c0f668b42": ["{\n    \"b1\": 0.9,\n    \"b2\": 0.9,\n    \"b4\": 0.8,\n    \"b14\": 0.8,\n    \"b20\": 0.8\n}", "{\n    \"b1\": 0.9,\n    \"b2\": 0.8,\n    \"b14\": 0.85,\n    \"b20\": 0.85\n}", "{\n  \"b1\": 0.9,\n  \"b2\": 0.8,\n  \"b4\": 0.85,\n  \"b14\": 0.75,\n  \"b20\": 0.85\n}", "{\n    \"b1\": 0.95,\n    \"b2\": 0.95,\n    \"b4\": 0.9,\n    \"b14\": 0.8,\n    \"b20\": 0.8\n}", "{\n    \"b1\": 0.9,\n    \"b2\": 0.8,\n    \"b4\": 0.85,\n    \"b14\": 0.75\n}"], "634781fe90e50fcafd2c1a49": ["```json\n{\n    \"b22\": 0.95,\n    \"b24\": 0.9,\n    \"b25\": 0.9,\n    \"b18\": 0.85\n}\n```", "{\n    \"b22\": 0.9,\n    \"b24\": 0.8,\n    \"b25\": 0.8,\n    \"b18\": 0.7\n}", "{\n    \"b22\": 0.9,\n    \"b24\": 0.85,\n    \"b25\": 0.85,\n    \"b18\": 0.8\n}", "{\n    \"b22\": 0.9,\n    \"b24\": 0.85,\n    \"b25\": 0.85,\n    \"b18\": 0.8\n}", "{\n  \"b22\": 0.95,\n  \"b24\": 0.9,\n  \"b25\": 0.9,\n  \"b18\": 0.85\n}"], "62ceb9215aee126c0f4090b0": ["Based on the text of the academic paper and the author notes, the paper proposes a new algorithm for searching the architecture of neural networks for the dilation convolution operator, which takes the size of the model's perception domain, the dilation parameter, and the number of CNN channels as searchable parameters. \n\nThe following references seem to be the most relevant sources that inspired the development of the paper's core contributions:\n\n1. \"Pruning in Time (PIT) a light weight network architecture optimizer for temporal convolutional networks\" [b13]: This is the previous work of the author and is the basis for the current paper. The author extends the work in this paper. Confidence Score: 1\n\n2. \"FBNetV Differentiable Neural Architecture Search for Spatial and Channel Dimensions\" [b9]: The paper uses a similar lightweight approach and focuses on the number of channels, which is also a searchable parameter in the current paper. Confidence Score: 0.8\n\n3. \"MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks\" [b10]: Similar to [b9], this paper also uses a lightweight architecture search approach and optimizes the number of channels. Confidence Score: 0.8\n\n4. \"ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware\" [b12]: This paper also uses the architecture search approach, but it focuses on the 2D CNN model, which is different from the current paper. But the current paper does mention that they take inspiration from this work. Confidence Score: 0.6\n\n5. \"DARTS: Differentiable Architecture Search\" [b26]: This paper proposes a differentiable architecture search method, which has similar principles to the current paper. Confidence Score: 0.6\n\n6. \"Neural Architecture Search with Reinforcement Learning\" [b8]: This paper presents a method of using reinforcement learning to search the architecture of neural networks, which is a similar concept to the current paper. Confidence Score: 0.6\n\n7. \"Scalpel: Customizing DNN Pruning to the Underlying Hardware Parallelism\" [b30]: This paper proposes a structured pruning method, which is similar to the current paper's method of optimizing receptive field and dilation in a structured weight pruning way. Confidence Score: 0.7\n\nReturn the result in the json format: {\"b13\": 1, \"b9\": 0.8, \"b10\": 0.8, \"b12\": 0.6, \"b26\": 0.6, \"b8\": 0.6, \"b30\": 0.7}.", "Based on the text of the paper and the titles of the references, the most pertinent source papers which inspired the authors are likely to be:\n\n1. \"lightweight neural architecture search for temporal convolutional networks at the edge\" [b13]\n2. \"pruning in time pit a light weight network architecture optimizer for temporal convolutional networks\" [b13]\n3. \"an empirical evaluation of generic convolutional and recurrent networks for sequence modeling\" [b6]\n4. \"fbnetv differentiable neural architecture search for spatial and channel dimensions\" [b9]\n5. \"morphnet fast simple resource constrained structure learning of deep networks\" [b10]\n6. \"proxylessnas direct neural architecture search on target task and hardware\" [b12]\n7. \"darts differentiable architecture search\" [b26]\n8. \"tcn mapping optimization for ultra low power time series edge inference\" [b41]\n\nThese papers are all related to the concept of Neural Architecture Search (NAS), which is the main topic of the paper. They all propose different methods or tools for optimizing the architecture of neural networks, which is the main challenge addressed in the paper.\n\nThe confidence score for these papers could be:\n\n{ \"b13\": 1.0, \"b6\": 0.7, \"b9\": 0.8, \"b10\": 0.9, \"b12\": 0.9, \"b26\": 0.7, \"b41\": 0.6 } \n\nThis JSON format represents the confidence score of each reference number. The score ranges from 0 to 1, with a higher score indicating a greater degree of influence or inspiration from the source paper.", "Based on the given text, the source papers that have inspired the author could be identified as follows:\n\n1. 'An empirical evaluation of generic convolutional and recurrent networks for sequence modeling' [b6]: The text refers to the algorithms proposed in this paper as a viable alternative to RNNs for time-series-based tasks. The algorithm has been used as a basis for developing the proposed model.\n\n2. 'Pruning in Time (PIT): a light-weight network architecture optimizer for temporal convolutional networks' [b13]: The author explicitly mentions that the proposed work is an extension of this paper. The author has used the initial approach and extended it to optimize other parameters.\n\n3. 'FBNetV: Differentiable Neural Architecture Search for Spatial and Channel Dimensions' [b9]: The author mentions this paper while discussing the existing Neural Architecture Search (NAS) approaches. It seems to have influenced the direction of the author's proposed tool.\n\n4. 'MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks' [b10]: This paper is also discussed while explaining the lightweight NAS approaches, indicating its influence on the author's work.\n\n5. 'ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware' [b12]: This paper is mentioned as an advanced DNAS that reduces memory requirements, which seems to have inspired part of the author's approach.\n\n6. 'Neural Architecture Search with Reinforcement Learning' [b8]: The paper is mentioned while talking about early NAS tools, which may have influenced the formation of the author's proposed tool.\n\nThe confidence score for each reference paper is difficult to determine without more explicit mention of their influence in the text. However, it is clear that [b13] should have the highest confidence score as it is directly mentioned as the base of the current work.\n\nThe JSON format of the result is:\n{\n\"b6\": 0.8,\n\"b13\": 1,\n\"b9\": 0.7,\n\"b10\": 0.7,\n\"b12\": 0.7,\n\"b8\": 0.6\n}", "{\n\"b6\": 0.9,\n\"b7\": 0.85,\n\"b9\": 1.0,\n\"b10\": 0.95,\n\"b12\": 0.9,\n\"b13\": 1.0\n}", "Based on the provided text and the author's note, the following references can be considered as the direct source papers for the given academic paper:\n\n- 'b6': 'an empirical evaluation of generic convolutional and recurrent networks for sequence modeling': The paper discusses Temporal Convolutional Networks (TCNs), which is a key concept in the given paper.\n\n- 'b9': 'fbnetv differentiable neural architecture search for spatial and channel dimensions': This paper provides a methodology for architecture search, which seems to be a core part of the given paper's method.\n\n- 'b10': 'morphnet fast simple resource constrained structure learning of deep networks': This paper discusses an approach of optimizing the number of channels in each layer, which is a part of the approach used in the given paper.\n\n- 'b13': 'pruning in time pit a light weight network architecture optimizer for temporal convolutional networks': The given paper extends this work by including the optimization of the receptive field and of the number of channels of all convolutional layers in a TCN.\n\nTherefore, the final output in JSON format is:\n\n{\n\"b6\": 0.9,\n\"b9\": 0.85,\n\"b10\": 0.85,\n\"b13\": 1.0\n}"], "63bb859d90e50fcafd06ee17": ["{\n    \"b47\": 0.95,\n    \"b43\": 0.85,\n    \"b49\": 0.85\n}", "{\n    \"b47\": 0.95,\n    \"b49\": 0.95,\n    \"b43\": 0.90,\n    \"b23\": 0.85\n}", "{\n    \"b47\": 1.0,\n    \"b43\": 0.8,\n    \"b49\": 0.8\n}", "{\n  \"b47\": 0.9,\n  \"b43\": 0.9,\n  \"b49\": 0.8\n}", "{\n  \"b47\": 1.0,\n  \"b43\": 0.9,\n  \"b49\": 0.9\n}"], "628749265aee126c0ffe6e73": ["Based on the text and reference titles, the source papers that most inspire this paper could be:\n\n{\n\"b23\": 0.9, \n\"b63\": 0.8, \n\"b62\": 0.8, \n\"b82\": 0.7, \n\"b110\": 0.7, \n\"b111\": 0.6, \n\"b14\": 0.5, \n\"b85\": 0.5\n}\n\nThese papers are selected because they are mentioned in the text and their titles seem relevant to the paper's novel contributions and challenges. The confidence score is based on how directly these papers seem to influence the paper's contributions. Papers b23, b63, b62 are given high scores due to their relevance to dialogue systems in healthcare, which is the paper's main focus. b82, b110, b111 are also related to the healthcare dialogue systems but with slightly less direct relevance, thus have a bit lower scores. b14 and b85 are relevant but not as central to the paper's contributions, thus have lower scores.", "{\"b23\": 0.9, \"b82\": 0.85, \"b110\": 0.85, \"b124\": 0.8, \"b62\": 0.8, \"b63\": 0.8, \"b111\": 0.7}", "Based on the given text and following the outlined strategy, the most relevant source papers that inspired the author are:\n\n1. \"A survey on dialogue systems recent advances and new frontiers\" [b23]: This paper is mentioned directly with the phrase \"through the lens of AI and NLP research and practice have been limited. [b124] and [b23] presented surveys of recent advances in general-domain task-oriented dialogue systems.\". This suggests that it has had a significant impact on the author's work.\n\n2. \"A systematic review of health dialog systems\" [b62]: This paper is referenced in the text with the phrase \"[b111], [b63], and [b62] conducted systematic reviews of dialogue systems deployed in mental health [b111] or general healthcare [b63] [b62] settings.\" This suggests that the author took inspiration from these works.\n\n3. \"Conversational agents in healthcare a systematic review\" [b63]: This paper is referenced in the same context as [b62], hence it is also considered as an inspiration for the author.\n\n4. \"Chatbots and conversational agents in mental health a review of the psychiatric landscape\" [b111]: This paper is referenced together with [b62] and [b63], and therefore is also considered as a source that inspired the author.\n\n5. \"Survey of conversational agents in health expert systems with applications\" [b82]: This paper is mentioned with the phrase \"[b82] [b110] recently reviewed 40 and 47 articles, respectively, covering conversational agents in the healthcare domain.\" This suggests that the author drew inspiration from this work.\n\n6. \"Conversational agents in health care scoping review and conceptual analysis\" [b110]: This paper is mentioned in the same context as [b82], and is therefore considered as a source that inspired the author.\n\nThe confidence score for each of these references, assuming a uniform distribution, would be 1/6 or approximately 0.167.\n\nThe JSON format would be:\n\n{\n\"b23\": 0.167,\n\"b62\": 0.167,\n\"b63\": 0.167,\n\"b111\": 0.167,\n\"b82\": 0.167,\n\"b110\": 0.167\n}", "Based on the text of the academic paper and the titles of the references, the most pertinent source papers that inspire the author are:\n\n- \"A survey on dialogue systems: recent advances and new frontiers\" [b23]: This paper is directly referenced in the text and discusses the general concept of dialogue systems, which is the main topic of the given paper. The confidence score is 0.9.\n\n- \"Conversational agents in healthcare: a systematic review\" [b63]: This paper is also directly referenced and pertains to the use of dialogue systems in the healthcare domain, which is the specific topic of the given paper. The confidence score is 0.85.\n\n- \"Chatbots and conversational agents in mental health: a review of the psychiatric landscape\" [b111]: This paper is relevant especially considering the paper's focus on healthcare. It's not directly mentioned as a source of inspiration, but it's related to the paper's topic. The confidence score is 0.75.\n\n- \"A survey of available corpora for building data-driven dialogue systems\" [b99]: This paper's topic is related to the methodology used in the paper and could have influenced the author's methods. The confidence score is 0.7.\n\n- \"Recent advances and challenges in task-oriented dialog system\" [b124]: This paper's topic is highly relevant to the paper's content and likely influenced the author's work. The confidence score is 0.8.\n\n- \"The design and implementation of Xiaoice, an empathetic social chatbot\" [b126]: This paper could have influenced the author's approach to developing their own dialogue system. The confidence score is 0.7.\n\n{\"b23\": 0.9, \"b63\": 0.85, \"b111\": 0.75, \"b99\": 0.7, \"b124\": 0.8, \"b126\": 0.7}", "Based on the text of the academic paper, the source papers that inspired the author the most could be:\n\n{ \"b23\": 0.8, \"b82\": 0.7, \"b110\": 0.7, \"b62\": 0.7, \"b63\": 0.7, \"b111\": 0.7 }\n\nThese papers seem to be most directly related to the paper's novel contributions, as they deal with the topics of dialogue systems in healthcare, conversational agents in health care, and systematic reviews of health dialog systems, which are the main focus of the paper. \n\nThe confidence score is estimated based on how closely related the source paper appears to be to the primary focus and novel contributions of the paper, considering also the frequency of the references within the text."], "63608e5090e50fcafdee1224": ["Based on the given text and the author's notes, the source papers that appear to have inspired this work most significantly are:\n\n- b2: \"Temporal knowledge graph completion: a survey\"\n- b33: \"Temporal Message Passing for Temporal Knowledge Graph Completion\"\n- b11: \"Learning to Walk Across Time for Interpretable Temporal Knowledge Graph Completion\"\n- b39: \"Automated Machine Learning: Methods, Systems, Challenges\"\n- b9: \"Single Path One-Shot Neural Architecture Search with Uniform Sampling\"\n- b24: \"Modeling Relational Data with Graph Convolutional Networks\"\n- b1: \"Relational Graph Attention Networks\"\n- b28: \"Composition-Based Multi-Relational Graph Convolutional Networks\"\n\nThese papers are the ones that are most directly related to the main contributions of the paper, which involve the use of a temporal knowledge graph (TKG) and the application of neural architecture search (NAS) to design data-specific architectures for TKG completion (TKGC). The papers are also associated with key phrases that suggest direct inspiration or motivation, and they appear to share strong methodological similarities with the paper.\n\nThe given result in JSON format is:\n\n{\n  \"b2\": 0.9,\n  \"b33\": 0.9,\n  \"b11\": 0.9,\n  \"b39\": 0.9,\n  \"b9\": 0.9,\n  \"b24\": 0.9,\n  \"b1\": 0.9,\n  \"b28\": 0.9\n}\n\nThe confidence scores are high (0.9) because these papers are directly relevant to the paper's novel contributions and are associated with key phrases indicating direct inspiration or motivation.", "Based on the paper provided and the author's notes, the source papers are:\n\n- \"Temporal Knowledge Graph Completion: A Survey\" [b2]: The paper uses a temporal knowledge graph (TKG), which is directly mentioned in the text. The TKG forms the basis for the study's data organization.\n- \"Single Path One-Shot Neural Architecture Search with Uniform Sampling\" [b9]: The paper's search algorithm is based on a one-shot NAS method. It is specifically mentioned in the method section, indicating it was influential in developing the paper's core contributions.\n- \"Relational Graph Attention Networks\" [b1], \"Modeling Relational Data with Graph Convolutional Networks\" [b24], \"Composition-Based Multi-Relational Graph Convolutional Networks\" [b28], \"Temporal Message Passing for Temporal Knowledge Graph Completion\" [b33]: These papers are directly related to the design of the spatial and temporal aggregation in the model.\n- \"Learning Sequence Encoders for Temporal Knowledge Graph Completion\" [b7]: This paper seems to be the pioneering work that the paper's approach is based on, especially regarding the discretization of a TKG into multiple static KG snapshots.\n- \"Diachronic Embedding for Temporal Knowledge Graph Completion\" [b8]: The paper mentions this work in the context of existing embedding-based methods which do not explicitly encode local graph structures in TKG, a limitation that this paper aims to overcome.\n  \nThe JSON format of the answer is:\n{\n\"b2\": 1,\n\"b9\": 1,\n\"b1\": 0.8,\n\"b24\": 0.8,\n\"b28\": 0.8,\n\"b33\": 0.8,\n\"b7\": 0.9,\n\"b8\": 0.9\n}", "Based on the given text and the author's note, the primary challenges outlined in the paper involve designing data-specific GNN architectures for Temporal Knowledge Graph Completion (TKGC). The authors proposed a novel method called Search to Pass Messages (SPA) which combines spatial and temporal information in TKGs. This approach includes a generalized framework, search space, search objectives, and a search algorithm.\n\nBy analyzing the text and the author's note, the following references seem to be the source papers that inspired this work:\n\n- \"b2: Temporal knowledge graph completion a survey\": This paper is referred to in the introduction, discussing the concept of Temporal Knowledge Graphs (TKGs), which forms the basis of the present paper's focus on TKGC. Confidence Score: 0.8\n\n- \"b33: Temp: Temporal message passing for temporal knowledge graph completion\": This paper is mentioned as a GNN-based method for TKGC, which the authors of the present paper also focus on. Confidence Score: 0.9\n\n- \"b39: Automated machine learning methods systems challenges\" and \"b10: Automated machine learning methods systems challenges\": These references are mentioned in the context of Neural Architecture Search (NAS), which is utilized in the proposed SPA method. Confidence Score: 0.85\n\n- \"b9: Single path one shot neural architecture search with uniform sampling\": This paper is brought up in the context of the search algorithm used in the SPA method. Confidence Score: 0.85\n\n- \"b24: Modeling relational data with graph convolutional networks\", \"b1: Relational graph attention networks\", \"b28: Composition based multirelational graph convolutional networks\": These papers are referenced as part of the spatial aggregation operations in the search space. Confidence Score: 0.8\n\n- \"b3: Learning phrase representations using RNN encoder-decoder for statistical machine translation\", \"b29: Attention is all you need\": These papers are referenced as part of the temporal aggregation operations in the search space. Confidence Score: 0.8\n\nBased on the above analysis, the JSON format of the source papers along with their confidence scores would be:\n\n{\n\"b2\": 0.8,\n\"b33\": 0.9,\n\"b39\": 0.85,\n\"b10\": 0.85,\n\"b9\": 0.85,\n\"b24\": 0.8,\n\"b1\": 0.8,\n\"b28\": 0.8,\n\"b3\": 0.8,\n\"b29\": 0.8\n}\n", "Based on the provided paper text and the instruction given, the paper is about a novel method called \"Search to PAss messages (SPA)\" for designing data-specific architectures for temporal knowledge graph completion (TKGC). The method combines spatial and temporal information in TKGs and uses an efficient search algorithm.\n\nThe paper mentions the following papers as inspiration and motivation for their work:\n\n1. \"Temporal knowledge graph completion: A survey\" [b2]: This paper is about temporal knowledge graphs, which is the core concept in the paper. It's likely to be a key source paper.\n\n2. \"Search to pass messages for temporal knowledge graph completion\" [None]: The title suggests that this paper is the main inspiration for this work as it shares the same title and likely the same topic. \n\n3. \"Single path one-shot neural architecture search with uniform sampling\" [b9]: This paper is the foundation for the search algorithm used in the paper.\n\n4. \"Neural architecture search of graph neural networks\" [b46]: This paper is about designing GNN architecture which is a crucial part of the paper.\n\n5. \"Relational graph attention networks\" [b1]: This paper is about multi-relational GNNs which is a part of the spatial aggregation module in the paper's method.\n\n6. \"Learning phrase representations using RNN encoder-decoder for statistical machine translation\" [b3]: This paper is about GRU, a sequential model used in the temporal aggregation module in the paper's method.\n\n7. \"Attention is all you need\" [b29]: This paper is about Self-Attention, another sequential model used in the temporal aggregation module in the paper's method.\n\n8. \"DeepGCNs: Making GCNs Go as Deep as CNNs\" [b16]: This paper is about the use of skip connections, a concept used in the layer connection of the paper's method.\n\nThe confidence score for each source paper is estimated to be 1 as they all seem to be highly relevant to the paper's method.\n\nThe resulting source papers in JSON format are:\n\n```\n{\n  \"b2\": 1,\n  \"None\": 1,\n  \"b9\": 1,\n  \"b46\": 1,\n  \"b1\": 1,\n  \"b3\": 1,\n  \"b29\": 1,\n  \"b16\": 1\n}\n```", "Based on the provided text and the author's notes, the papers that inspired this work the most appear to be:\n\n1. \"Temporal knowledge graph completion a survey\" [b2] - The author's work is based on a form of temporal knowledge graph, which this paper appears to provide a survey on.\n2. \"Search to PAss messages(SPA)\" - The author's work proposes a novel method which tries to Search to PAss messages(SPA), indicating that this work inspired their approach.\n3. \"Neural architecture search (NAS)\" [b39] [b10] - The author's work employs neural architecture search, suggesting that these papers provided important inspiration.\n4. \"Temporal message passing for temporal knowledge graph completion\" [b33] - This paper is referenced as a GNN-based method that has been used to solve the TKGC, which is the problem the author's work addresses.\n5. \"Single path one shot neural architecture search with uniform sampling\" [b9] - This paper is referenced for its search algorithm, which the author's work adopts.\n6. \"DARTS differentiable architecture search\" [b18] - This paper is referenced when discussing the search algorithm, indicating it likely provided inspiration for the author's approach.\n\nIn JSON format, this would be:\n\n{\n\"b2\": 1,\n\"SPA\": 0.9,\n\"b39\": 0.9,\n\"b10\": 0.9,\n\"b33\": 0.8,\n\"b9\": 0.8,\n\"b18\": 0.8\n} \n\nThese confidence scores are based on the text and the author's notes provided. The scores indicate how strongly each paper appears to have inspired the work based on its mentions and context within the text."], "6306e8c890e50fcafdebd565": ["Based on the text, the source papers that inspired the author's work most directly could be:\n\n1. \"ADMoe anomaly detection with mixture of experts from noisy labels\" [None]: This seems to be the main paper that the author's work is based on, as the title suggests the use of a Mixture of Experts (MoE) architecture for anomaly detection, which aligns with the author's work. Confidence score: 1.0\n\n2. \"Outlier ensembles: an introduction\" [b1]: The author references the benefit of ensembling in anomaly detection and the importance of diversity in base models. Confidence score: 0.9\n\n3. \"Adaptive Mixtures of Local Experts\" [b14]: This paper is directly related to the Mixture of Experts (MoE) model, a key component of the author's work. Confidence score: 0.9\n\n4. \"Outrageously large neural networks: the sparsely gated mixture of experts layer\" [b28]: The author's work also uses a sparsely gated MoE, as suggested by this paper. Confidence score: 0.9\n\n5. \"Learning from Multiple Noisy Sources\" [b32]: The author's work also involves learning from multiple noisy sources, a central theme in this paper. Confidence score: 0.9\n\n6. \"Deep Learning from Crowds\" [b23]: This paper might have provided inspiration on how to learn from multiple sets of noisy labels, similar to \"crowds\". Confidence score: 0.8\n\n7. \"Taming Sparsely Activated Transformer with Stochastic Experts\" [b43]: This paper might have provided insights on how to handle sparse activation in the MoE model. Confidence score: 0.8\n\nThe JSON format of the answer is: \n{\n \"None\": 1.0,\n \"b1\": 0.9,\n \"b14\": 0.9,\n \"b28\": 0.9,\n \"b32\": 0.9,\n \"b23\": 0.8,\n \"b43\": 0.8\n}", "Based on the text and the author's note, the paper focuses on utilizing multiple noisy labels from different data sources for anomaly detection, with less dependency on manual annotations. The authors base their approach on the Mixture-of-Experts (MoE) architecture to effectively merge multiple noisy labels, considering the characteristics and similarities of different sources. \n\nThe key references that seem to inspire this work are:\n\n- \"Adaptive mixtures of local experts\" [b14]: This paper is likely the source of the main algorithm (MoE) used in the paper, and the authors claim to offer a novel method with the MoE architecture for specialized and scalable learning. \n\n- \"Deep learning from crowds\" [b23]: This paper could have motivated the authors to learn from multiple noisy sources.\n\n- \"Deep learning from multiple noisy annotators as a union\" [b32]: This paper could have inspired the authors' approach to learning from multiple noisy sources but without mapping each set of noisy labels to part of the network.\n\n- \"Outrageously large neural networks: the sparsely gated mixture-of-experts layer\" [b28]: This paper might have inspired the use of sparse MoE for better model scalability.\n\n- \"Who said what: modeling individual labelers improves classification\" [b10]: The paper could have provided insights into learning from noisy labels.\n\n- \"Taming sparsely activated transformer with stochastic experts\" [b43]: As the paper uses a similar approach of sparsely activated transformer with stochastic experts, this could be another source of inspiration for the authors.\n\nFor the JSON format:\n\n{\n  \"b14\": 0.9,\n  \"b23\": 0.8,\n  \"b32\": 0.8,\n  \"b28\": 0.7,\n  \"b10\": 0.7,\n  \"b43\": 0.7\n} \n\nThe confidence score is given assuming that a higher score indicates a greater influence on the paper. \n\nIt is also worth mentioning that these inspirations are inferred based on the provided paper text and the titles of the references, the actual inspirations might vary depending on the full content of these referenced papers.", "Based on the text and the corresponding references, the source papers that most likely inspired the authors are as follows:\n\n{'b14': 0.9, 'b28': 0.9, 'b10': 0.8, 'b32': 0.8, 'b23': 0.7, 'b41': 0.7, 'b1': 0.6, 'b42': 0.6}\n\nExplanation:\n1. 'b14' and 'b28': Mentioned multiple times throughout the text and are key to understanding the Mixture-of-Experts (MoE) model which is central to the paper's method.\n2. 'b10' and 'b32': Referred to in the context of learning from noisy sources, which is a key issue addressed in this paper.\n3. 'b23': Addressed in the context of learning from multiple sources of noisy labels, which is core to the paper.\n4. 'b41': Relevant to the paper's topic of weakly supervised machine learning.\n5. 'b1' and 'b42': Relevant in the context of ensemble learning and anomaly detection, which are key themes in the paper.", "Based on the text of the academic paper, the following references seem to have the most direct influence on the paper's main contributions:\n\n1. \"Mixture-of-experts (MoE) network(s) [b14] [b28]\" and \"ADMoE enhances existing neuralnetwork-based AD algorithms by Mixture-of-experts (MoE) network(s) [b14] [b28]\" suggest that the authors drew significant inspiration from these papers to develop their algorithm.\n\n2. The author also mentions the following papers when discussing the challenges and existing solutions in the field: \"Rodrigues and Pereira 2018; [b10] [b32]\". \n\n3. The mention of \"DeepSAD [b26]\" suggests that this paper was a key source of inspiration for the method used in this paper.\n\n4. The references to \"[b33]\" and \"[b23]\" in the context of discussing methods for learning from multiple sources of noisy labels indicate that these papers were influential in shaping the approach presented in the paper.\n\n5. The references to \"[b39]\" and \"[b31]\" in the context of discussing learning from a single set of noisy labels also suggest that these papers were important source papers.\n\n6. The paper \"[b41]\" is mentioned when discussing the weakly supervised ML, which is a key aspect of the paper's methodology.\n\nIn summary, the source papers that inspired the main contributions of this paper are likely to be: [b14], [b28], [b10], [b32], [b26], [b33], [b23], [b39], [b31], and [b41].\n\nThe JSON format of the answer is:\n\n{\"b14\": 1, \"b28\": 1, \"b10\": 0.9, \"b32\": 0.9, \"b26\": 1, \"b33\": 0.8, \"b23\": 0.8, \"b39\": 0.7, \"b31\": 0.7, \"b41\": 0.6}", "Based on the academic paper text provided, the following source papers are pertinent:\n\n1. \"admoe anomaly detection with mixture of experts from noisy labels\" [None]: This is the paper being analyzed, so it won't be included as a source paper. \n\n2. \"adaptive mixtures of local experts\" [b14]: The authors used Mixture-of-experts (MoE) architecture/layer, which they adapted to their anomaly detection algorithm design for MNLAD. This indicates a strong reliance on this paper.\n\n3. \"outrageously large neural networks the sparsely gated mixture of experts layer\" [b28]: This paper is also mentioned in context of the Mixture-of-experts (MoE) architecture. The authors have used the insights from this paper to improve model scalability for natural language processing (NLP) tasks.\n\n4. \"who said what modeling individual labelers improves classification\" [b10]: They have developed algorithms to learn from noisy sources, which has shown great success. This paper is likely a significant source of inspiration for the current paper's approach to handling noisy data.\n\n5. \"deep learning from multiple noisy annotators as a union\" [b32]: The authors mention that this method allows for direct learning from multiple sets of noisy labels. They also discuss its limitations, which they address in their own work. \n\n6. \"stablemoe stable routing strategy for mixture of experts\" [b7]: This paper is referenced in context of explaining how MoE can help learn a model from diverse sources (e.g., different languages), which is similar to the current paper's approach of learning from multiple sets of noisy labels.\n\n7. \"taming sparsely activated transformer with stochastic experts\" [b43]: This paper is used to explain the concept of MoE and its application in improving model scalability and robustness for MNLAD.\n\n8. \"a brief introduction to weakly supervised learning\" [b41]: The paper is referenced to explain the category of learning the proposed method falls under, which is weakly supervised ML.\n\nThe JSON output would be:\n\n{\"b14\": 0.9, \"b28\": 0.8, \"b10\": 0.8, \"b32\": 0.8, \"b7\": 0.7, \"b43\": 0.7, \"b41\": 0.7}"], "62451c325aee126c0f47b416": ["Based on the text and the author's notes, the source papers that most directly influenced the work are:\n\n1. \"Online knowledge distillation with diverse peers\" [b2]\n2. \"Learning with retrospection\" [b4]\n3. \"Self knowledge distillation with progressive refinement of targets\" [b11]\n4. \"Data distortion guided self distillation for deep neural networks\" [b34]\n5. \"Snapshot distillation teacher student optimization in one generation\" [b35]\n6. \"Revisiting knowledge distillation via label smoothing regularization\" [b36]\n7. \"Be your own teacher improve the performance of convolutional neural networks via self distillation\" [b42]\n\nBased on the text, these papers are directly related to the challenges and algorithm proposed in the given paper, especially the self-distillation method and label smoothing regularization. They are also mentioned in the text with phrases indicating direct inspiration or motivation.\n\nHere is the result in the JSON format:\n\n{\n\"b2\": 0.9,\n\"b4\": 0.8,\n\"b11\": 0.9,\n\"b34\": 0.9,\n\"b35\": 0.8,\n\"b36\": 0.8,\n\"b42\": 0.9\n}\n\nThe confidence score is given based on the degree of relevance mentioned in the text, with higher scores indicating more direct relevance or inspiration.", "Based on the text of the paper and the titles of the references, the following references seem to be most pertinent as source papers:\n\n1. \"Knowledge distillation beyond model compression\" [b23]: The paper mentions the success of Knowledge Distillation (KD) in boosting the generalization ability which might have influenced the authors' work on self-knowledge distillation.\n \n2. \"Online knowledge distillation via collaborative learning\" [b7]: The authors mention online distillation as a method to alleviate the time-consuming preparation of static teachers, and this concept might have influenced them to come up with their method of self-distillation.\n\n3. \"Self knowledge distillation with progressive refinement of targets\" [b11]: The authors mention this paper when discussing existing self KD methods and the challenges they face, indicating that this paper could have been influential in the development of their own method.\n\n4. \"Data distortion guided self distillation for deep neural networks\" [b34]: The authors discuss this method of self-distillation, which utilizes history information to construct a virtual teacher.\n\n5. \"Revisiting knowledge distillation via label smoothing regularization\" [b36]: This paper is mentioned as a method of knowledge distillation and could have influenced the authors' work.\n\n6. \"Deep mutual learning\" [b43]: The paper mentions the success of online KD which was popularized by this research. This could indicate that it was a significant source of inspiration.\n\nIt is important to note that the authors' method, Self-Distillation from Last Mini-Batch (DLB), is a new approach to tackle the challenges and therefore, while these references have influenced their work, the authors have also built upon them to create something new.\n\nInferred answer in JSON format:\n\n{\n\"b23\": 0.85, \n\"b7\": 0.85, \n\"b11\": 0.87, \n\"b34\": 0.85, \n\"b36\": 0.80, \n\"b43\": 0.80\n}", "Based on the given text and author notes, the source papers for this academic paper seem to be:\n\n1. \"Deep Mutual Learning\" [b43]: This paper introduced the concept of online distillation, which is discussed and compared in the given text.\n\n2. \"Self Knowledge Distillation with Progressive Refinement of Targets\" [b11]: Mentioned in the text as a previous method of self-KD, which the authors of the given paper seem to have built upon.\n\n3. \"Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation\" [b42]: This paper is referred to in the text when discussing the challenges of implementing previous self-KD methods.\n\n4. \"Data Distortion Guided Self Distillation for Deep Neural Networks\" [b34]: Referred to in the text as an existing self-KD method that the authors of the given paper aim to improve upon.\n\n5. \"Revisiting Knowledge Distillation via Label Smoothing Regularization\" [b36]: This paper is mentioned when discussing the theoretical analysis of the success of KD.\n\n6. \"Snapshot Distillation: Teacher-Student Optimization in One Generation\" [b35]: This paper is referred to when discussing the limitations of existing self-KD methods, which the authors of the given paper aim to address.\n\n7. \"Deep Residual Learning for Image Recognition\" [b8]: This paper is mentioned in the implementation section, suggesting its methods were used or compared against in the given paper's experiments.\n\nThe JSON format of this output would be:\n{\n  \"b43\": 1,\n  \"b11\": 0.9,\n  \"b42\": 0.9,\n  \"b34\": 0.9,\n  \"b36\": 0.8,\n  \"b35\": 0.8,\n  \"b8\": 0.7\n} \n\nPlease note that the confidence scores are a rough estimation based on the context in which each paper is mentioned in the text.", "Based on the given text, the most pertinent source papers that inspired the author can be identified as follows:\n\n1. \"Online Knowledge Distillation with Diverse Peers\" [b2]: The author refers to the work as a method that has been extended by several subsequent works, indicating its influence on the field and potentially on this paper.\n   \n2. \"Deep Residual Learning for Image Recognition\" [b8]: The author uses the ResNet models for evaluations, suggesting that this paper had a significant impact on their methodology.\n\n3. \"Self Knowledge Distillation with Progressive Refinement of Targets\" [b11]: The author directly compares their work with this paper, indicating its relevance as a source of inspiration.\n\n4. \"Data Distortion Guided Self Distillation for Deep Neural Networks\" [b34]: The author uses this paper to explain the limitations of existing self-KD methods and how their method provides a solution, implying that this paper was a key reference.\n\n5. \"Revisiting Knowledge Distillation via Label Smoothing Regularization\" [b36]: The author uses the paper to explain the concept of knowledge distillation and compare their approach, suggesting its significance in the development of their paper.\n\n6. \"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features\" [b37]: The author uses this paper to assess the compatibility of their method with data augmentation based regularization, indicating its relevance.\n\n7. \"Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation\" [b42]: The author refers to this paper when discussing the limitations of existing self-KD methods, suggesting its influence.\n\nBased on the text, the confidence score for each source paper is estimated as follows:\n\n{\n\"b2\": 0.8,\n\"b8\": 0.9,\n\"b11\": 0.8,\n\"b34\": 0.9,\n\"b36\": 0.85,\n\"b37\": 0.8,\n\"b42\": 0.85\n}", "Based on the text, the following source papers seem to have the most pertinent influence on the paper:\n\n1. \"Deep Mutual Learning\" [b43] - The paper mentions online knowledge distillation, which was introduced in this work, as a significant influence. It also states that this method was further extended by many following works. Confidence score: 0.9\n2. \"Knowledge Distillation by On-the-Fly Native Ensemble\" [b13] - This is one of the papers that extended the line of work introduced in [b43]. Confidence score: 0.8\n3. \"Self Knowledge Distillation with Progressive Refinement of Targets\" [b11] - The paper discusses this method and its epoch-level update frequency, but also highlights its limitations. Confidence score: 0.7\n4. \"Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation\" [b42] - The author mentions this method as a popular formulation of self-KD and discusses its limitations. Confidence score: 0.7\n5. \"Data Distortion Guided Self Distillation for Deep Neural Networks\" [b34] - This method is discussed as a data distortion based self-distillation approach and is compared with DLB. Confidence score: 0.7\n6. \"Snapshot Distillation: Teacher-Student Optimization in One Generation\" [b35] - The paper discusses this method and its use of history information, but also highlights its limitations. Confidence score: 0.7\n7. \"Revisiting Knowledge Distillation via Label Smoothing Regularization\" [b36] - The paper discusses this method as an example of KD used in the regularization domain and compares it to DLB. Confidence score: 0.6\n\nResult in json format:\n{\n  \"b43\": 0.9,\n  \"b13\": 0.8,\n  \"b11\": 0.7,\n  \"b42\": 0.7,\n  \"b34\": 0.7,\n  \"b35\": 0.7,\n  \"b36\": 0.6\n}"], "62725cd25aee126c0fae91f3": ["Based on the provided text and titles of the references, the most pertinent source papers that inspired the given paper can be identified as follows:\n\n1. \"Distilling the knowledge in a neural network\" [b0]: The text mentions this paper while explaining the basic concept of knowledge distillation (KD). This paper introduced the concept of KD which is the primary focus of the given paper.\n\n2. \"Contrastive representation distillation\" [b1]: This paper is referred to when the author discusses different strategies for distillation, specifically in the context of distilling structural knowledge.\n\n3. \"Relational knowledge distillation\" [b2]: Similarly to [b1], this paper is also mentioned when discussing distillation strategies, specifically in relation to transferring mutual relations of data examples from teacher models to student models.\n\n4. \"FitNets: Hints for thin deep nets\" [b5]: This paper is cited when the author talks about multi-spot distillation methods which provide student models with more supervision signals by mining knowledge from multiple layers in the teacher network.\n\n5. \"Routing networks: Adaptive selection of non-linear functions for multi-task learning\" [b27]: This paper is referenced when discussing the concept of routing networks, which is a key component of the proposed spot-adaptive KD strategy.\n\n6. \"Categorical reparameterization with Gumbel softmax\" [b38]: This reference is cited when explaining the optimization algorithm of the proposed method, suggesting that it plays a crucial role in the methodology.\n\n7. \"Knowledge distillation via route constrained optimization\" [b14]: Although not directly mentioned in the text, the title suggests a methodological similarity with the proposed spot-adaptive KD method, which may have influenced the development of this method.\n\nThe confidence scores are assigned based on the frequency and context of the mentions in the text, with a higher score indicating a stronger influence.\n\nHere is the final result in JSON format:\n\n```json\n{\n  \"b0\": 1.0,\n  \"b1\": 0.8,\n  \"b2\": 0.8,\n  \"b5\": 0.8,\n  \"b27\": 0.7,\n  \"b38\": 0.7,\n  \"b14\": 0.6\n}\n```", "Based on the given text, the source papers that most inspire the author of the given paper are:\n\n1. \"Distilling the knowledge in a neural network\" [b0]: This paper is cited multiple times and is clearly a foundational work in the field of knowledge distillation. The given paper's main idea of adopting a teacher-student learning scheme is directly derived from this work. Confidence score: 0.9\n\n2. \"Fitnets: hints for thin deep nets\" [b5]: This paper is referenced when discussing multi-spot distillation and intermediate representations. The proposed method in the given paper seems to adopt a similar approach. Confidence score: 0.8\n\n3. \"Routing networks: adaptive selection of non-linear functions for multi-task learning\" [b27]: This paper is referenced in the context of routing networks, which are a core part of the proposed method in the given paper. The given paper seems to adopt and extend ideas from this work. Confidence score: 0.7\n\n4. \"Categorical reparameterization with Gumbel softmax\" [b38]: This paper is referenced in the context of enabling differentiability of the discrete sampling function, which is a key part of the proposed method. It appears that the methodology from this paper is directly used in the given paper. Confidence score: 0.7\n\n5. \"Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer\" [b6]: This paper is referenced when discussing multi-spot distillation, which appears to be a significant part of the proposed method in the given paper. Confidence score: 0.6\n\n6. \"Contrastive representation distillation\" [b1]: This paper is referenced multiple times, and its method of adopting a contrastive learning approach to distill structural knowledge seems to have influenced the given paper. Confidence score: 0.6\n\n7. \"Knowledge distillation via route constrained optimization\" [b14]: This paper is referenced in the context of knowledge distillation and seems to be related to the proposed method of the given paper in the context of heterogeneous distillation. Confidence score: 0.5\n\nThe above inference is derived based on the frequency and context of citations, and the alignment between the methods proposed in the source papers and the given paper. The confidence scores are subjective and are assigned based on the degree of influence inferred from the given text. \n\nHere is the JSON format of the result:\n\n{\n  \"reference number\": \"confidence score\",\n  \"b0\": 0.9,\n  \"b5\": 0.8,\n  \"b27\": 0.7,\n  \"b38\": 0.7,\n  \"b6\": 0.6,\n  \"b1\": 0.6,\n  \"b14\": 0.5\n}", "The source papers for the given academic paper are as follows:\n\n1. \"Distilling the knowledge in a neural network\" [b0] - This paper is the foundational work on knowledge distillation (KD), which is the main technique used in the given paper. It's mentioned in the context of teaching the basics of KD, which the authors then build upon. Confidence score: 0.9\n2. \"Contrastive representation distillation\" [b1] - This is another paper on KD, which introduces the contrastive learning approach used in the given paper. Confidence score: 0.8\n3. \"FitNets: Hints for thin deep nets\" [b5] - This paper introduces the concept of using intermediate representations from the teacher network, which the authors of the given paper adopt. Confidence score: 0.8\n4. \"Routing networks: Adaptive selection of non-linear functions for multi-task learning\" [b27] - This paper introduces the concept of routing networks, which the authors of the given paper use in their spot-adaptive KD method. Confidence score: 0.8\n5. \"Categorical reparameterization with Gumbel softmax\" [b38] - This paper presents the Gumbel-Softmax trick, which the authors of the given paper use to allow their policy network to make discrete decisions in a differentiable manner. Confidence score: 0.8\n6. \"Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer\" [b6] - This paper introduces the attention transfer technique, which the authors of the given paper mention as a way of improving the performance of student models. Confidence score: 0.7\n7. \"Knowledge transfer via distillation of activation boundaries formed by hidden neurons\" [b9] - This paper is cited as an example of multi-spot distillation, which the authors of the given paper examine and improve upon. Confidence score: 0.7\n\nIn JSON format: {\"b0\": 0.9, \"b1\": 0.8, \"b5\": 0.8, \"b27\": 0.8, \"b38\": 0.8, \"b6\": 0.7, \"b9\": 0.7}", "Based on the paper's text, the most relevant source papers that inspire the author's work are:\n\n1. 'Distilling the knowledge in a neural network' [b0]: Mentioned multiple times as the original paper that proposed knowledge distillation (KD), which is the core concept used in this paper. The authors also use the Kullback-Leibler divergence from this paper as part of their distillation loss function.\n\n2. 'Contrastive representation distillation' [b1]: Mentioned as a method that adopts a contrastive learning approach to distill structural knowledge.\n\n3. 'Relational knowledge distillation' [b2]: Discussed as a method that transfers mutual relations of data examples from teacher models to student models.\n\n4. 'FitNets: Hints for thin deep nets' [b5]: The authors use the knowledge distillation loss proposed by FitNets as part of their distillation loss function.\n\n5. 'Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer' [b6]: Mentioned as a method that improves the performance of a student CNN network by forcing it to mimic the attention maps at different layers of a powerful teacher network.\n\n6. 'Routing Networks: Adaptive selection of non-linear functions for multi-task learning' [b27]: The authors use a routing network, a concept which is closely related to this paper.\n\n7. 'Categorical reparameterization with Gumbel softmax' [b38]: The authors use the Gumbel-Softmax trick for differentiability of the discrete sampling function, a method which is from this paper.\n\nThe confidence scores for the source papers are hard to assess without knowing the exact impact of each paper on the authors' work, but the scores could look something like this:\n\n{\n\"b0\": 0.9,\n\"b1\": 0.8,\n\"b2\": 0.8,\n\"b5\": 0.8,\n\"b6\": 0.8,\n\"b27\": 0.8,\n\"b38\": 0.8\n}", "Based on the given text, the following references appear to be the most influential in the development of the paper's core contributions:\n\n1. \"distilling the knowledge in a neural network\" [b0]\n2. \"contrastive representation distillation\" [b1]\n3. \"relational knowledge distillation\" [b2]\n4. \"fitnets hints for thin deep nets\" [b5]\n5. \"paying more attention to attention improving the performance of convolutional neural networks via attention transfer\" [b6]\n6. \"knowledge transfer via distillation of activation boundaries formed by hidden neurons\" [b9]\n7. \"routing networks adaptive selection of non linear functions for multi task learning\" [b27]\n\nHere is the JSON format of the source papers with their confidence scores:\n\n{\n    \"b0\": 0.9,\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b9\": 0.8,\n    \"b27\": 0.8\n}\n\nThese references have been chosen based on their relevance to the paper's novel contributions, their methodological similarity with the proposed algorithm in the paper, and the explicit indication of their use in the text."], "63ae56ca90e50fcafda968ed": ["Based on the text and titles of the references, the following papers seem to be the most relevant source papers:\n\n1. \"Few-shot learning with retrieval-augmented language models\" [b14] - The paper frequently discusses the concept of \"retrieve-then-read\" which is a central concept in this paper. Confidence: 0.9\n2. \"Bootstrapping reasoning with reasoning\" [b58] - The paper introduces the DSP framework, which is a major contribution of this paper. Confidence: 0.85\n3. \"Automatic chain of thought prompting in large language models\" [b59] - The paper discusses the DSP framework and its application in various settings. Confidence: 0.8\n4. \"Language models are few-shot learners\" [b1] - The paper discusses the use of language models and their limitations, which is a central challenge addressed by this paper. Confidence: 0.7\n5. \"Robust multi-hop reasoning at scale via condensed retrieval\" [b18] - The paper discusses multi-hop reasoning, which is a major method introduced in this paper. Confidence: 0.75\n6. \"Internet-augmented language models through few-shot prompting for open-domain question answering\" [b25] - The paper discusses the use of retrieval models to augment prompts, which is a key method in this paper. Confidence: 0.7\n7. \"Measuring and narrowing the compositionality gap in language models\" [b37] - The paper mentions this reference while discussing the results of their method. Confidence: 0.65\n\nJSON format:\n{\n  \"b14\": 0.9,\n  \"b58\": 0.85,\n  \"b59\": 0.8,\n  \"b1\": 0.7,\n  \"b18\": 0.75,\n  \"b25\": 0.7,\n  \"b37\": 0.65\n}", "Based on the text of the academic paper and the titles of the references, the source papers that most likely inspired this paper are:\n\n- \"b27\": 'latent retrieval for weakly supervised open domain question answering' - This paper seems to be aligned with the DSP framework and concepts mentioned in the analyzed paper.\n- \"b1\": 'language models are few shot learners' - The analyzed paper uses language models and this reference seems to be a key paper in the field.\n- \"b14\": 'few shot learning with retrieval augmented language models' - This paper appears to be directly relevant to the retrieval augmented in-context learning aspect of the analyzed paper.\n- \"b37\": 'measuring and narrowing the compositionality gap in language models' - The analyzed paper discusses language models and retrieval models, and this paper could provide foundational knowledge on language models.\n- \"b58\": 'bootstrapping reasoning with reasoning' - This paper seems to be related to novel methods and approaches in the analyzed paper.\n- \"b59\": 'automatic chain of thought prompting in large language models' - This paper seems to be related to the techniques and methods used in the analyzed paper.\n\nThe result is:\n{\n  \"b27\": 0.9,\n  \"b1\": 0.85,\n  \"b14\": 0.85,\n  \"b37\": 0.8,\n  \"b58\": 0.75,\n  \"b59\": 0.75\n}", "Based on the text and the titles of the referenced papers, the source papers that inspired the author the most appear to be:\n\n{\"b14\": 0.9, \"b18\": 0.85, \"b1\": 0.8, \"b34\": 0.75, \"b59\": 0.7, \"b37\": 0.65}\n\nExplanation:\n\n- b14 (\"few-shot learning with retrieval augmented language models\"): The text frequently discusses retrieval-augmented learning, which suggests that this paper was highly influential.\n- b18 (\"robust multi-hop reasoning at scale via condensed retrieval\"): The paper proposes a technique for multi-hop reasoning, which is a focal point of the paper's contributions.\n- b1 (\"language models are few-shot learners\"): The paper's focus on language models and few-shot learning suggests significant inspiration from this source.\n- b34 (\"training language models to follow instructions with human feedback\"): The paper discusses using prompts to guide language models, which is a key feature of this source.\n- b59 (\"automatic chain of thought prompting in large language models\"): The paper uses a similar approach of prompting in large language models, indicating inspiration.\n- b37 (\"measuring and narrowing the compositionality gap in language models\"): The paper discusses improving language models, which suggests that this source was influential. \n\nThe confidence scores are estimated based on the frequency and context of the references in the text.", "Based on the text and titles of the references, the most pertinent source papers are:\n\n1. \"Few shot learning with retrieval augmented language models\" - [b14]: The paper refers to this work several times, mentioning its limitations and building upon its approach with the DSP framework.\n\n2. \"Automatic chain of thought prompting in large language models\" - [b59]: This paper is cited multiple times, especially when discussing the DSP framework and its application in complex pipelines.\n\n3. \"Open domain question answering goes conversational via question rewriting\" - [b0]: This is referred to when discussing the novel contributions of the paper, particularly in terms of bootstrapping annotations and question rewriting.\n\n4. \"Robust multi hop reasoning at scale via condensed retrieval\" - [b18]: This paper is cited in the context of the retrieval-augmented NLP and in-context learning literatures that the DSP framework systematically unifies.\n\n5. \"Internet augmented language models through few shot prompting for open domain question answering\" - [b25]: This is mentioned when discussing the limitations of \"retrieve-then-read\" pipelines, which the paper aims to address.\n\n6. \"Latent retrieval for weakly supervised open domain question answering\" - [b27]: This paper is referenced when introducing the DSP framework and its ability to gather information from a knowledge corpus.\n\n7. \"Measuring and narrowing the compositionality gap in language models\" - [b37]: This is cited when discussing the gains achieved by their DSP programs against a self-ask pipeline.\n\nThe confidence scores are assigned based on the frequency and context in which these papers are cited in the text. \n\nHere is the result in JSON format:\n{\n    \"b14\": 0.9,\n    \"b59\": 0.85,\n    \"b0\": 0.8,\n    \"b18\": 0.75,\n    \"b25\": 0.7,\n    \"b27\": 0.65,\n    \"b37\": 0.6\n}", "Based on the text and the titles of the references, the source papers that most likely influenced this paper are:\n\n- 'b1': 'language models are few shot learners' - 0.95\n- 'b14': 'few shot learning with retrieval augmented language models' - 0.95\n- 'b25': 'internet augmented language models through few shot prompting for open domain question answering' - 0.9\n- 'b34': 'training language models to follow instructions with human feedback' - 0.85\n- 'b37': 'measuring and narrowing the compositionality gap in language models' - 0.85\n- 'b39': 'language models are unsupervised multitask learners' - 0.8\n- 'b59': 'automatic chain of thought prompting in large language models' - 0.8\n- 'b18': 'robust multi hop reasoning at scale via condensed retrieval' - 0.75\n\nPlease note that the confidence scores are estimated and suggest the level of relevance based on the frequency and context in which these references are mentioned in the text. The papers 'b1', 'b14', 'b25' and 'b34' seem to be particularly important as they address both language models and retrieval models, which are the core topics of this paper."], "63aab708a4a9066abca549f8": ["{\"b1\": 1, \"b2\": 0.9, \"b9\": 0.9, \"b11\": 0.8, \"b15\": 0.8, \"b7\": 0.7, \"b17\": 0.7, \"b18\": 0.6}", "Based on the key phrases and novel methods outlined in the paper, the most influential source papers are:\n\n1. \"Diaformer: Automatic Diagnosis via Symptoms Sequence Generation\" [b1] - The paper discusses the Diaformer model, a sequence generation approach, which influenced the authors' decision to tackle symptom checking as a multi-label classification task, keeping the unordered nature of automatic diagnosis, rather than a multi-class classification task.\n  \n2. \"Supervised Contrastive Learning\" [b7] - The authors used the idea of contrastive learning from this paper to design their multi-task learning strategy, integrating it into the learning of symptom and disease prediction. \n\n3. \"Attention is All You Need\" [b15] - The authors used the Transformer model from this paper to capture the relationship between disease and symptoms in their multi-task learning strategy.\n\n4. \"Graph Attention Networks\" [b16] - The authors adopted the concept of self-attention used in Graph Attention Networks to calculate the attention scores for symptom aggregation in their model.\n\n5. \"Large Scale Object Detection in the Wild from Imbalanced Multi labels\" [b11] - The authors used the concurrent softmax proposed in this paper to tackle the issue of imbalanced multi-label distribution in automatic diagnosis.\n\n6. \"A Simple Framework for Contrastive Learning of Visual Representations\" [b2] - The authors used the idea of contrastive learning from this paper to better distinguish symptoms among different diseases.\n \nTherefore, the source papers are: b1, b7, b15, b16, b11, b2. \n\nIn JSON format, the result is:\n\n{\n\"b1\": 0.9,\n\"b7\": 0.9,\n\"b15\": 0.85,\n\"b16\": 0.85,\n\"b11\": 0.8,\n\"b2\": 0.8\n} \n\nThe confidence score is a subjective evaluation based on the analysis of the text.", "Based on the text and the provided notes, the following papers are likely to be the most influential sources for the given paper:\n\n1. \"Diaformer: automatic diagnosis via symptoms sequence generation\" [b1]: The given paper refers to this as a recent effort and presents its work as an improvement over the Diaformer's approach.\n2. \"Task-oriented dialogue system for automatic diagnosis\" [b17]: The given paper contrasts its approach with this paper's approach, which typically regards the problem of automatic diagnosis as a Markov Decision Process (MDP) and addresses it via reinforcement learning (RL).\n3. \"Task-oriented dialogue system for automatic disease diagnosis via hierarchical reinforcement learning\" [b9]: The given paper also contrasts its method with the hierarchical policy learning strategy of this paper.\n4. \"A simple framework for contrastive learning of visual representations\" [b2]: The given paper discusses the use of contrastive learning, a method from this paper, in its own approach.\n5. \"Supervised contrastive learning\" [b7]: The given paper also uses ideas from this paper in its approach to contrastive learning.\n6. \"Attention is all you need\" [b15]: The given paper uses the Transformer model, introduced in this paper, in its approach.\n7. \"Large scale object detection in the wild from imbalanced multi labels\" [b11]: The given paper refers to this paper while discussing the choice of loss function for their multi-label classification task.\n\nThese inferences are made based on the discussion and comparison of methods, and the direct usage and adaptation of methods from these papers in the given paper's approach. \n\nThe references are returned in the JSON format as follows:\n{\n  \"b1\": 1,\n  \"b17\": 0.9,\n  \"b9\": 0.9,\n  \"b2\": 0.8,\n  \"b7\": 0.8,\n  \"b15\": 0.7,\n  \"b11\": 0.6\n}\n\nThe confidence scores are assigned based on the degree of direct inspiration or motivation indicated by the text.", "Based on the text, the following references are identified as the most pertinent source papers:\n\n1. \"Diaformer: automatic diagnosis via symptoms sequence generation\" [b1]: The paper mentions that this previous work considers automatic diagnosis as a sequence generation task and generates implicit symptoms in an auto-regressive style. However, in contrast, the current paper reformulates the symptom checking as a multi-label classification task.\n\n2. \"A simple framework for contrastive learning of visual representations\" [b2]: The paper mentions that they resort to the idea of contrastive learning, inspired by this work, to differentiate symptoms of different diseases.\n\n3. \"Large scale object detection in the wild from imbalanced multi labels\" [b11]: The paper changes the loss function to concurrent softmax, which is proposed in this work, to better handle the imbalanced multi-label distribution in automatic diagnosis.\n\n4. \"Attention is all you need\" [b15]: The paper uses the transformer model, which is proposed in this work, in their proposed attention-based model.\n\n5. \"Graph attention networks\" [b16]: The paper's attentional pooling to obtain the sequence representation for the final prediction is similar to the self-attention used in Graph Attention Networks.\n\n6. \"Supervised contrastive learning\" [b7]: The paper uses the idea of supervised contrastive learning proposed in this work for the auxiliary disease prediction.\n\nBased on the above analysis, the JSON format answer is:\n{\"b1\": 0.9, \"b2\": 0.8, \"b11\": 0.8, \"b15\": 0.9, \"b16\": 0.7, \"b7\": 0.8}", "Based on the text, the following references most likely inspired the given paper:\n\n1. \"Diaformer: Automatic Diagnosis via Symptoms Sequence Generation\" [b1]: This paper is directly mentioned as a non-RL-based method that the authors compare their method with, indicating a significant influence. \n\n2. \"Task-oriented Dialogue System for Automatic Diagnosis\" [b17]: This is cited multiple times, particularly in the context of RL-based methods for automatic diagnosis and symptom checking, which the authors of the given paper explicitly aim to improve upon.\n\n3. \"Generative Adversarial Regularized Mutual Information Policy Gradient Framework for Automatic Diagnosis\" [b18]: This paper is also referred to multiple times, particularly when discussing the Generative Adversarial Network and reinforcement learning in the context of automatic diagnosis.\n\n4. \"Supervised Contrastive Learning\" [b7]: This paper is cited as the source of the contrastive learning method used in the multi-task design of the proposed model.\n\n5. \"Attention is all you need\" [b15]: This paper is referred to several times as the basis for the Transformer model used in the proposed model.\n\n6. \"Task Oriented Dialogue System for Automatic Disease Diagnosis via Hierarchical Reinforcement Learning\" [b9]: This paper is mentioned as a work that integrates a two-level hierarchical policy learning strategy, which seems to have influenced the authors' approach.\n\n7. \"A Simple Framework for Contrastive Learning of Visual Representations\" [b2]: This paper is referred to when the authors discuss the use of contrastive learning in their model.\n\n8. \"Large Scale Object Detection in the Wild from Imbalanced Multi Labels\" [b11]: This paper is cited as the source of the concurrent softmax used in the proposed model.\n\nThe confidence score for each of these references is difficult to quantify without a more detailed analysis of the text, but it can be assumed to be relatively high given the frequency and context of their citation in the paper.\n\nOutput in JSON format: \n\n{\n  \"b1\": 0.8,\n  \"b17\": 0.8,\n  \"b18\": 0.8,\n  \"b7\": 0.7,\n  \"b15\": 0.7,\n  \"b9\": 0.7,\n  \"b2\": 0.7,\n  \"b11\": 0.7\n}"], "620e302d5aee126c0fadda4d": ["{\n  \"b12\": 0.8,\n  \"b13\": 0.9,\n  \"b15\": 0.85,\n  \"b19\": 0.9,\n  \"b31\": 0.8\n}", "{\n    \"b13\": 0.9,\n    \"b16\": 0.9,\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b29\": 0.9,\n    \"b31\": 0.9,\n    \"b32\": 0.9\n}", "{\n    \"b25\": 0.9,\n    \"b31\": 0.9,\n    \"b29\": 0.85,\n    \"b32\": 0.85\n}", "{\n    \"b13\": 0.9,\n    \"b19\": 0.8,\n    \"b12\": 0.7,\n    \"b31\": 0.9,\n    \"b32\": 0.85\n}", "{\n    \"b12\": 0.8,\n    \"b13\": 0.9,\n    \"b15\": 0.85,\n    \"b19\": 0.8,\n    \"b31\": 0.95\n}"], "625f6bf75aee126c0ffb36b6": ["Based on the text of the paper, the most pertinent source papers are:\n\n1. 'b18': 'enhancing dialogue symptom diagnosis with global attention and symptom graph' - The author mentioned the task of Symptom Label Inference (SLI) and the importance of symptoms in a medical dialogue. This seems to be directly inspired by this reference paper.\n\n2. 'b33': 'task oriented dialogue system for automatic diagnosis' - The author discusses the Diagnosis-oriented Dialogue Policy (DDP) task, which is directly linked to this reference paper.\n\n3. 'b45': 'an end to end progressive multi task learning framework for medical named entity recognition and normalization' - The author proposes a task for Named Entity Recognition (NER), which is directly related to this reference paper.\n\n4. 'b10': 'dr summarize global summarization of medical dialogue by exploiting local structures' - The author mentions the task of Medical Report Generation (MRG), which seems to be inspired by this reference paper.\n\n5. 'b39': 'meddialog large scale medical dialogue dataset' - This paper is mentioned when discussing the medical dialogue corpus, which is a key part of the authors' work.\n\n6. 'b20': 'meddg a large scale medical consultation dataset for building medical dialogue system' - This paper is referred to when discussing existing datasets and their limitations, which the authors' work aims to address.\n\n7. 'b34': 'medical named entity recognition from un labelled medical records based on pre trained language models and domain dictionary' - This paper seems to be related to the approach the authors' took for the Named Entity Recognition (NER) task.\n\nThe json format of the answer is:\n{\"b18\": 0.9, \"b33\": 0.9, \"b45\": 0.8, \"b10\": 0.85, \"b39\": 0.75, \"b20\": 0.8, \"b34\": 0.75}", "Based on the text given, the following references are identified as the main sources of inspiration for the paper:\n\n1. \"b1\": The author cites this source in the introduction section. It suggests that the paper is inspired by the potential of online medical consultation.\n\n2. \"b3\": The author refers to this source when discussing automatic text-based diagnosis. This could be the basis for the automatic medical consultation framework proposed in the paper.\n\n3. \"b10\": This source is referenced in the discussion about medical report generation, one of the tasks proposed in the author's framework.\n\n4. \"b33\": The author cites this source while explaining the task-oriented interaction framework and Diagnosis-oriented Dialogue Policy. \n\n5. \"b18\": This source is referred to while explaining the Symptom Label Inference task.\n\n6. \"b20\": This paper is cited while comparing existing datasets with DialoAMC.\n\n7. \"b29\": The author cites this source when discussing the slot filling task while comparing proposed tasks with existing ones.\n\n8. \"b45\": This source is referred to during the discussion on Named Entity Recognition task.\n\nBased on the above references, the following JSON object can be formed:\n\n{\n    \"b1\": 0.8,\n    \"b3\": 0.7,\n    \"b10\": 0.8,\n    \"b33\": 0.75,\n    \"b18\": 0.7,\n    \"b20\": 0.7,\n    \"b29\": 0.7,\n    \"b45\": 0.7\n}\n\nThe confidence score is given based on how directly each reference is linked to the novel contributions of the paper. For instance, \"b1\" and \"b10\" have the highest scores as they are linked to the core contributions of the paper, i.e., the potential of online medical consultation and the medical report generation task, respectively.", "Based on the text of the paper and the titles of the references, the most pertinent source papers are:\n\n1. \"online medical consultation a review of literature and practice\" [b1]: This paper provides the conceptual basis for the research paper's focus on online medical consultations. The author mentions the potential of online medical consultations in the introduction, which may be influenced by this source paper.\n\n2. \"Medical named entity recognition from unlabelled medical records based on pre-trained language models and domain dictionary\" [b34]: This reference seems to be related to the Named Entity Recognition (NER) task mentioned in the author's research. The author proposes a similar task in their framework for automatic medical consultation.\n\n3. \"using context information for dialog act classification in dnn framework\" [b22]: This paper possibly inspired the Dialogue Act Classification (DAC) task in the author's framework. The author refers to DAC as a vital aspect of analyzing the intent in medical dialogues.\n\n4. \"enhancing dialogue symptom diagnosis with global attention and symptom graph\" [b18]: This reference could have influenced the Symptom Label Inference (SLI) task in the author's research. The author also aims to identify symptoms from dialogue and align them to standardized names.\n\n5. \"dr summarize global summarization of medical dialogue by exploiting local structures\" [b10]: This paper likely inspired the Medical Report Generation (MRG) task in the author's research. The author refers to MRG as a way to record and convey medical information.\n\n6. \"task oriented dialogue system for automatic diagnosis\" [b33]: This paper appears to have influenced the Diagnosis-oriented Dialogue Policy (DDP) task proposed by the author. The author also aims to learn the optimal policy for symptom-based automatic disease diagnosis.\n\n7. \"meddialog large scale medical dialogue dataset\" [b39]: The author's reference to this paper indicates that it might have inspired their collection and annotation of the DialoAMC corpus for their research.\n\nThese inferences are made based on the text of the paper and the titles of the references. The exact confidence in these sources could only be verified by further detailed analysis of both the paper and the references. The JSON format of the result is:\n\n{\n  \"b1\": 0.9,\n  \"b34\": 0.8,\n  \"b22\": 0.8,\n  \"b18\": 0.8,\n  \"b10\": 0.8,\n  \"b33\": 0.8,\n  \"b39\": 0.8\n}", "Based on the text of the paper, the key challenges identified relate to the lack of unified design of frameworks and tasks for automatic medical consultation as well as the lack of benchmark datasets to support research and application development in this area. The author proposes two frameworks to support automatic medical consultation: dialogue understanding and task-oriented interaction. They also introduce DialoAMC, a large-scale annotated medical dialogue corpus. \n\nBased on the mentions in the text and the relevance to the challenges and solutions proposed in the paper, the following references seem to be the most pertinent source papers:\n\n1. \"Online medical consultation: a review of literature and practice\" [b1]: This paper is mentioned in the introduction, setting the stage for the research in the paper. It may contain foundational ideas that inspired the current research. \n\n2. \"Task-oriented dialogue system for automatic disease diagnosis\" [b33]: This paper is mentioned multiple times in connection with the task-oriented interaction framework proposed by the author.\n\n3. \"Enhancing dialogue symptom diagnosis with global attention and symptom graph\" [b18]: This paper is mentioned in relation to the symptom label inference task within the dialogue understanding framework.\n\n4. \"Dr. summarize: global summarization of medical dialogue by exploiting local structures\" [b10]: This paper is mentioned in relation to the medical report generation task within the dialogue understanding framework.\n\n5. \"Medical named entity recognition from unlabelled medical records based on pre-trained language models and domain dictionary\" [b34]: This paper is mentioned in relation to the named entity recognition task within the dialogue understanding framework.\n\n6. \"Using context information for dialog act classification in DNN framework\" [b22]: This paper is mentioned in relation to the dialogue act classification task within the dialogue understanding framework.\n\n7. \"MedDialog: large-scale medical dialogue dataset\" [b39]: This paper is mentioned in relation to the construction of the DialoAMC corpus.\n\nBased on the context, these references seem to be the most directly relevant to the paper's novel contributions. \n\nThe source papers in JSON format with confidence scores:\n{\n\"b1\": 0.9,\n\"b33\": 0.9,\n\"b18\": 0.85,\n\"b10\": 0.85,\n\"b34\": 0.8,\n\"b22\": 0.8,\n\"b39\": 0.9\n}", "Based on the given text and the author's notes, it appears that the key challenges were to build a framework for automatic medical consultation and propose several tasks to cover the entire procedure. The authors designed two frameworks, namely, dialogue understanding and task-oriented interaction, which are supported by DialoAMC, a large-scale annotated medical dialogue corpus. \n\nThe authors were inspired by and used the methodologies from the following papers:\n\n1. \"Medical named entity recognition from un labelled medical records based on pre trained language models and domain dictionary\" [b34]: It seems the authors used the methodology of this paper to propose a Named Entity Recognition (NER) task under their dialogue understanding framework.\n2. \"Using context information for dialog act classification in dnn framework\" [b22]: It appears that the authors were inspired by this paper to propose a Dialogue Act Classification (DAC) task under their dialogue understanding framework.\n3. \"Extracting symptoms and their status from clinical conversations\" [b7]: This paper seems to have influenced the authors' decision to incorporate a Symptom Label Inference (SLI) task under their dialogue understanding framework.\n4. \"Dr summarize global summarization of medical dialogue by exploiting local structures\" [b10]: This paper appears to have inspired the authors to propose a Medical Report Generation (MRG) task under their dialogue understanding framework.\n5. \"Task-oriented dialogue system for automatic diagnosis\" [b33]: The authors appear to have been inspired by this paper to propose a Diagnosis-oriented Dialogue Policy (DDP) task under their task-oriented interaction framework.\n6. \"MedDialog large scale medical dialogue dataset\" [b39]: This paper seems to have influenced the authors to design DialoAMC, their large-scale annotated medical dialogue corpus.\n7. \"Meddg a large scale medical consultation dataset for building medical dialogue system\" [b20]: This paper appears to have inspired the authors in their design of DialoAMC and their overall task design.\n\nTherefore, the source papers are likely [b34], [b22], [b7], [b10], [b33], [b39], and [b20].\n\nIn JSON format, with confidence scores:\n\n{ \"b34\": 0.9, \"b22\": 0.9, \"b7\": 0.9, \"b10\": 0.9, \"b33\": 0.9, \"b39\": 0.8, \"b20\": 0.8 }"], "63bcd73690e50fcafdefa108": ["{\n  \"b18\": 0.95,\n  \"b17\": 0.90,\n  \"b9\": 0.85,\n  \"b10\": 0.85,\n  \"b12\": 0.85,\n  \"b16\": 0.85,\n  \"b37\": 0.80\n}", "{\n  \"b18\": 1.0,\n  \"b17\": 0.9,\n  \"b7\": 0.85,\n  \"b37\": 0.85,\n  \"b36\": 0.8,\n  \"b0\": 0.75\n}", "{\n    \"b18\": 1.0,\n    \"b17\": 0.9,\n    \"b36\": 0.85,\n    \"b37\": 0.85,\n    \"b0\": 0.75,\n    \"b7\": 0.75,\n    \"b16\": 0.7\n}", "{\n  \"b18\": 1.0,\n  \"b17\": 0.9,\n  \"b7\": 0.8,\n  \"b37\": 0.8\n}", "{\n  \"b18\": 0.95,\n  \"b17\": 0.85,\n  \"b37\": 0.8,\n  \"b16\": 0.75,\n  \"b36\": 0.7\n}"], "6215a4242c356815940385b3": ["Based on the text of the paper, the author indicates that their work is directly inspired by the following papers:\n\n1. \"Early bird gcns graph network co optimization towards more efficient gcn training and inference via drawing early bird lottery tickets\" [b1]\n2. \"The lottery ticket hypothesis finding sparse trainable neural networks\" [b4]\n3. \"Drawing early bird tickets toward more efficient training of deep networks\" [b26]\n4. \"G cos gnn accelerator co search towards both better accuracy and efficiency\" [b28]\n5. \"Semi supervised classification with graph convolutional networks\" [b12]\n6. \"Graph attention networks\" [b20]\n7. \"How powerful are graph neural networks\" [b23]\n8. \"Inductive representation learning on large graphs\" [b8]\n\nThe JSON format of the answer is:\n{\"b1\": 1, \"b4\": 1, \"b26\": 1, \"b28\": 1, \"b12\": 0.8, \"b20\": 0.8, \"b23\": 0.8, \"b8\": 0.8} \n\nThe confidence score is set to 1 for the papers that are directly indicated by the author as their inspiration. For the papers that are not explicitly mentioned as inspiration but are directly linked to the challenges and methods discussed in the paper, a lower confidence score of 0.8 is assigned.", "Based on the given text, the papers that most inspired the author's work can be identified as follows:\n\n1. \"The lottery ticket hypothesis: finding sparse trainable neural networks\" [b4]: This paper is mentioned several times in the text and it appears to be the main inspiration for the \"early-bird lottery ticket hypothesis\" central to the author's work. The author's approach of identifying 'early-bird tickets' in GCN graphs seems to be directly motivated by the methods proposed in this paper.\n\n2. \"Drawing early bird tickets: towards more efficient training of deep networks\" [b26]: Again, this paper's ideas seem to form the basis of the author's early-bird ticket finding in GCNs. The author has acknowledged the extension of this work's finding to GCN models.\n\n3. \"Semi-supervised classification with graph convolutional networks\" [b12]: This is one of the key papers that the author repeatedly refers to while explaining the concept and applications of GCNs. It is likely that the methods and findings of this paper play a big role in the author's work.\n\n4. \"Graph attention networks\" [b20]: The author refers to this paper while discussing methods to improve the performance of GCNs. It is possible that the author's methods to boost GCN training efficiency are influenced by this paper.\n\n5. \"G COS: GNN accelerator co-search towards both better accuracy and efficiency\" [b28]: The author has mentioned this paper several times, and it seems to have influenced the development of their co-sparsification framework for GCN graphs and networks.\n\n6. \"Deep compression: compressing deep neural networks with pruning, trained quantization, and huffman coding\" [b9]: This paper is mentioned in the context of the author's co-sparsification framework. The magnitude-based pruning method used by the author seems to be inspired by this paper.\n\n7. \"A unified lottery ticket hypothesis for graph neural networks\" [b1]: This paper is referred to in the context of the author's findings of early-bird tickets in GCN graphs and networks. It probably inspired the author's exploration of this phenomenon in GCNs.\n\n8. \"I-GCN: a graph convolutional network accelerator with runtime locality enhancement through islandization\" [b7]: This paper could have inspired the author's ideas on improving the efficiency of GCN training and inference.\n\nThe confidence scores for each source paper are difficult to estimate without more specific information about the author's degree of reliance on each paper. However, based on the number of times each paper is mentioned and the context in which it is discussed, we can infer that papers [b4] and [b26] probably have the highest confidence scores, followed by [b12] and [b28], and so on.\n\nThe result in json format would be:\n{\n\"b4\": 1,\n\"b26\": 0.95,\n\"b12\": 0.9,\n\"b20\": 0.85,\n\"b28\": 0.85,\n\"b9\": 0.8,\n\"b1\": 0.75,\n\"b7\": 0.7\n}", "Based on the provided paper text, the following references seem to be the most influential:\n\n1. \"b4\": 'The lottery ticket hypothesis: finding sparse trainable neural networks'\n2. \"b26\": 'Drawing early bird tickets: toward more efficient training of deep networks'\n3. \"b12\": 'Semi-supervised classification with graph convolutional networks'\n4. \"b28\": 'G cos gnn accelerator co search towards both better accuracy and efficiency'\n5. \"b19\": 'Degree quant: quantization aware training for graph neural networks'\n6. \"b13\": 'Semi-supervised classification with graph convolutional networks'\n7. \"b23\": 'How powerful are graph neural networks'\n\nThe paper text directly refers to these works as inspirations or bases for the methods used in the paper, and they are directly linked to the novel contributions and primary challenges outlined in the paper.\n\nIn the JSON format with the confidence score (assuming a scale of 0 to 1, with 1 being the highest confidence):\n\n{\n  \"b4\": 0.9,\n  \"b26\": 0.8,\n  \"b12\": 0.7,\n  \"b28\": 0.7,\n  \"b19\": 0.6,\n  \"b13\": 0.6,\n  \"b23\": 0.6\n}", "Based on the text and the titles of the references, the source papers that have significantly influenced the given paper are:\n\n1. \"The lottery ticket hypothesis: finding sparse trainable neural networks\" [b4]\n2. \"Drawing early bird tickets: toward more efficient training of deep networks\" [b26]\n3. \"EarlyBERT: efficient BERT training via early bird lottery tickets\" [b2]\n4. \"A unified lottery ticket hypothesis for graph neural networks\" [b1]\n5. \"Semi-supervised classification with graph convolutional networks\" [b12]\n6. \"Graph attention networks\" [b20]\n7. \"Inductive representation learning on large graphs\" [b8]\n8. \"G-CoS: GNN accelerator co-search towards both better accuracy and efficiency\" [b28]\n\nHere is the result in JSON format:\n\n```json\n{\n  \"b4\": 1,\n  \"b26\": 1,\n  \"b2\": 0.95,\n  \"b1\": 0.95,\n  \"b12\": 0.9,\n  \"b20\": 0.85,\n  \"b8\": 0.8,\n  \"b28\": 0.75\n}\n```\nThis result indicates that the paper [b4] and [b26] have the highest influence on the given paper with a confidence score of 1, followed by [b2] and [b1] with a score of 0.95, and so on.", "Based on the provided text, the following source papers seem to have inspired the given paper:\n\n1. \"The lottery ticket hypothesis finding sparse trainable neural networks\" [b4]: The author explicitly states in the text that they take inspiration from the lottery ticket hypothesis that was successful in DNN compression and seek to apply it to GCNs. Confidence Score: 0.9.\n\n2. \"Drawing early bird tickets toward more efficient training of deep networks\"[b26]: This paper is mentioned multiple times in the text as a source of inspiration for the early bird tickets concept and its application in GCNs. Confidence Score: 0.9.\n\n3. \"G cos gnn accelerator co search towards both better accuracy and efficiency\"[b28]: This paper is cited multiple times in the text and seems to be a key source paper for the development of the co-sparsification framework and the concept of GCN graph sparsification. Confidence Score: 0.8.\n\n4. \"Semi supervised classification with graph convolutional networks\"[b12]: This paper is mentioned in the text as the source of the GCN algorithm used in the author's work. Confidence Score: 0.7.\n\n5. \"A comprehensive survey on graph neural networks\"[b21]: This paper could have provided the authors with a broad foundation in the field of graph neural networks. Confidence Score: 0.6.\n\n6. \"An analysis of deep neural network models for practical applications\"[b0]: This paper is cited as a comparison in computational cost between DNNs and GCNs. Confidence Score: 0.6.\n\n7. \"Graph attention networks\"[b20]: This paper is used as a reference for further expanding the spatial GCNs' capability. Confidence Score: 0.7.\n\n8. \"How powerful are graph neural networks\"[b23]: This paper is cited as a theoretical basis for understanding the expressiveness of GCNs. Confidence Score: 0.6.\n\nThe final output in JSON format would be:\n\n{\n\"b4\": 0.9,\n\"b26\": 0.9,\n\"b28\": 0.8,\n\"b12\": 0.7,\n\"b21\": 0.6,\n\"b0\": 0.6,\n\"b20\": 0.7,\n\"b23\": 0.6\n}"], "63a1751790e50fcafd1f4880": ["Based on the provided text, the following references may have had the most influence on the paper's main contributions:\n\nReference \"b34\" [A Graph-Free Multi-Hop Reading Comprehension: A Select to Guide Strategy]: This paper is mentioned multiple times throughout the text, indicating that its method had a significant influence on the current paper. The author's model design was directly inspired by the design of S2G [b34] and FE2H [b18]. The authors also compared their model to S2G [b34] multiple times. The confidence score is 0.9.\n\nReference \"b18\" [From Easy to Hard: Two-Stage Selector and Reader for Multi-Hop Question Answering]: This paper is also mentioned multiple times and appears to have had a significant influence on the current paper. Similar to b34, the current paper's model design was directly inspired by the design of S2G [b34] and FE2H [b18]. The confidence score is 0.9.\n\nReference \"b39\" [Robust Machine Reading Comprehension by Learning Soft Labels]: This paper appears to have inspired the authors' approach to label smoothing for machine reading comprehension tasks. The confidence score is 0.8.\n\nReference \"b37\" [HotpotQA: A Dataset for Diverse, Explainable Multi-Hop Question Answering]: This paper is mentioned multiple times as the dataset the authors used to evaluate their model. The confidence score is 0.7.\n\nReference \"b11\" [Towards a Better Understanding of Label Smoothing in Neural Machine Translation]: This paper is mentioned in the context of label smoothing, indicating that it may have influenced the current paper's approach to this technique. The confidence score is 0.7.\n\nReference \"b30\" [Rethinking the Inception Architecture for Computer Vision]: This paper is mentioned in the context of label smoothing, indicating that it may have influenced the current paper's approach to this technique. The confidence score is 0.7.\n\nThe JSON format of the result is:\n{\"b34\": 0.9, \"b18\": 0.9, \"b39\": 0.8, \"b37\": 0.7, \"b11\": 0.7, \"b30\": 0.7}", "Based on the text, the papers that most seem to inspire the author are:\n\n1. \"Rethinking the inception architecture for computer vision\" [b30]\n2. \"HotpotQA a dataset for diverse explainable multi hop question answering\" [b37]\n3. \"A graph free multi hop reading comprehension a select to guide strategy\" [b34]\n4. \"From easy to hard two stage selector and reader for multi hop question answering\" [b18]\n5. \"Robust machine reading comprehension by learning soft labels\" [b39]\n\nThe json format result is:\n{\n\"b30\": 0.9,\n\"b37\": 0.9,\n\"b34\": 0.8,\n\"b18\": 0.7,\n\"b39\": 0.7\n}", "Based on the text, the main challenges outlined in the paper are the application of label smoothing in Multi-hop question answering (MHQA) tasks, and the development of a novel label smoothing method, F1 Smoothing, for machine reading comprehension (MRC) tasks. \n\nIn the paper, the author mentions the following references with a clear indication of direct inspiration or motivation:\n\n1. \"Motivated by the design of S2G [b34] and FE2H [b18], we propose a simpler model called C2FM that does not include an additional attention module.\"\n2. \"Previous research [b39] has explored various label smoothing methods for machine reading comprehension, including normal label smoothing and word overlap smoothing.\"\n\nTherefore, the source papers that inspired the author the most are:\n\n- b34: \"A Graph-Free Multi-Hop Reading Comprehension: A Select to Guide Strategy\"\n- b18: \"From Easy to Hard: Two-Stage Selector and Reader for Multi-Hop Question Answering\"\n- b39: \"Robust Machine Reading Comprehension by Learning Soft Labels\"\n\nFurthermore, the author cited the following papers while explaining and proposing their label smoothing methods:\n\n1. \"Label smoothing is a regularization technique that was introduced in computer vision to improve classification accuracy on ImageNet [b30].\"\n2. \"Label smoothing has been widely adopted in a variety of natural language processing tasks, including speech recognition (Chorowski and Jaitly, 2017), document retrieval [b25], dialogue generation [b28] and neural machine translation [b11] [b22] [b12].\"\n3. \"In recent years, label smoothing has also been applied to span-related tasks such as machine reading comprehension and named entity recognition [b41].\"\n\nTherefore, the additional source papers that inspired the author are:\n\n- b30: \"Rethinking the Inception Architecture for Computer Vision\"\n- b25: \"Weakly Supervised Label Smoothing\"\n- b28: \"Similarity-Based Label Smoothing for Dialogue Generation\"\n- b11: \"Towards a Better Understanding of Label Smoothing in Neural Machine Translation\"\n- b22: \"Semantic Label Smoothing for Sequence-to-Sequence Problems\"\n- b12: \"Generalizing Back Translation in Neural Machine Translation\"\n- b41: \"Boundary Smoothing for Named Entity Recognition\"\n\nIn JSON format, the results are:\n\n{\n  \"b34\": 1,\n  \"b18\": 1,\n  \"b39\": 1,\n  \"b30\": 0.8,\n  \"b25\": 0.8,\n  \"b28\": 0.8,\n  \"b11\": 0.8,\n  \"b22\": 0.8,\n  \"b12\": 0.8,\n  \"b41\": 0.8\n}\n\n(Note: The confidence score is set to 1 for the papers that the author directly states as motivation and 0.8 for the papers that the author used to explain their methods.)", "Based on the content of the academic paper, the source papers which inspired this paper are likely to be:\n\n1. \"Rethinking the Inception Architecture for Computer Vision\" [b30]: This paper introduced the label smoothing concept, which is a key method discussed in the provided paper.\n2. \"Towards a better understanding of Label Smoothing in Neural Machine Translation\" [b11]: This paper applied label smoothing in machine translation, which is relevant to the provided paper's application of label smoothing in Machine Reading Comprehension (MRC).\n3. \"Does Label Smoothing Mitigate Label Noise?\" [b21]: This paper explored the effectiveness of label smoothing, which is directly relevant to the provided paper's research objective.\n4. \"A Simple yet Strong Pipeline for HotpotQA\" [b13]: This paper proposed a model for multi-hop question answering, which is a primary challenge that the provided paper addresses.\n5. \"A Graph-Free Multi-hop Reading Comprehension: A Select-to-Guide Strategy\" [b34]: This paper is directly mentioned as an inspiration for the design of the proposed model in the provided paper.\n6. \"From Easy to Hard: Two-Stage Selector and Reader for Multi-hop Question Answering\" [b18]: This paper is directly mentioned as an inspiration for the design of the proposed model in the provided paper.\n7. \"Robust Machine Reading Comprehension by Learning Soft Labels\" [b39]: This paper explored label smoothing methods for machine reading comprehension, which is directly relevant to the provided paper's research objective.\n\nHere is the returned json:\n\n{\n\"b30\": 0.9,\n\"b11\": 0.85,\n\"b21\": 0.85,\n\"b13\": 0.75,\n\"b34\": 0.9,\n\"b18\": 0.9,\n\"b39\": 0.85\n}", "Based on the given text analysis, the source papers that inspired this paper most likely are:\n\n- 'b34': 'a graph free multi hop reading comprehension a select to guide strategy'\n- 'b18': 'from easy to hard two stage selector and reader for multi hop question answering'\n- 'b39': 'robust machine reading comprehension by learning soft labels'\n- 'b30': 'rethinking the inception architecture for computer vision'\n- 'b23': 'when does label smoothing help'\n- 'b11': 'towards a better understanding of label smoothing in neural machine translation'\n\nTo convert this to the requested format:\n\n```\n{\n\"b34\": 0.9,\n\"b18\": 0.8,\n\"b39\": 0.7,\n\"b30\": 0.6,\n\"b23\": 0.5,\n\"b11\": 0.6\n}\n```\n\nPlease note that the confidence scores are arbitrary and should be adjusted based on the specific context and degree of influence each source paper had on the given paper."], "63d9d87b90e50fcafd580980": ["Based on the paper text and the provided notes, the source papers that influenced the main contributions of the given paper are:\n\n1. \"Few-Shot Object Detection via Feature Reweighting\" [b15]: This paper is mentioned multiple times and seems to be a foundation for the given paper's work on few-shot object detection. The given paper's authors propose a different aggregation scheme from the one used in [b15], indicating that they were inspired by this work but sought to improve upon it.\n2. \"Meta R-CNN: Towards General Solver for Instance-Level Low-Shot Learning\" [b38]: This paper is referenced when the authors of the given paper discuss building a strong meta-learning baseline. The authors also propose different aggregation schemes from the ones used in [b38], indicating that they were inspired by this work but sought to improve upon it.\n3. \"Deep Variational Metric Learning\" [b24]: This paper is referenced when the authors of the given paper discuss variational feature learning, which is a main component of their proposed method.\n4. \"Variational Feature Disentangling for Fine-Grained Few-Shot Classification\" [b37]: This paper is referenced when the authors of the given paper discuss variational feature learning, which is a main component of their proposed method.\n5. \"DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection\" [b26]: This paper is referenced when the authors of the given paper discuss decoupling classification and regression tasks, indicating that they were inspired by this work.\n6. \"Frustratingly Simple Few-Shot Object Detection\" [b32]: This paper is referenced when the authors of the given paper discuss the performance of their proposed method.\n\nBased on the information provided, the confidence scores for these papers are difficult to determine precisely. However, a rough estimation could be:\n\n```\n{\n    \"b15\": 0.9,\n    \"b38\": 0.9,\n    \"b24\": 0.8,\n    \"b37\": 0.8,\n    \"b26\": 0.8,\n    \"b32\": 0.7\n}\n```\n\nThis is just a rough estimation based on the frequency and context of the references in the given paper's text. The confidence scores might be higher or lower depending on more specific criteria.", "Based on the text and analysis, the source papers that inspired this paper are:\n\n- \"Meta R-CNN towards general solver for instance level low shot learning\" [b38]\n- \"Few-shot object detection via feature reweighting\" [b15]\n- \"Variational feature disentangling for fine-grained few shot classification\" [b37]\n- \"Deep variational metric learning\" [b24]\n\nHere is the confidence score for each:\n\n{\n\"b38\": 1,\n\"b15\": 0.8,\n\"b37\": 0.8,\n\"b24\": 0.8\n}", "Based on the text given, the following source papers seem to have most directly influenced the authors:\n\n1. \"Meta R-CNN towards general solver for instance level low shot learning\" [b38]\n2. \"Few shot object detection via feature reweighting\" [b15]\n3. \"Deep variational metric learning\" [b24]\n4. \"Variational few shot learning\" [b39]\n5. \"Defrcn decoupled faster r cnn for few shot object detection\" [b26]\n6. \"Frustratingly simple few shot object detection\" [b32]\n\nThe list is in JSON format as follows:\n\n{\n    \"b38\": 0.9,\n    \"b15\": 0.85,\n    \"b24\": 0.8,\n    \"b39\": 0.8,\n    \"b26\": 0.75,\n    \"b32\": 0.75\n} \n\nThe confidence scores are subjective and represent how directly the source papers seem to influence the main contributions of the paper based on the provided text. They do not reflect the actual impact of the source papers on the field.", "Based on the provided paper, the most pertinent source papers that have inspired the work are:\n\n1. \"Meta R-CNN towards general solver for instance level low shot learning\"\n2. \"Deep variational metric learning\"\n3. \"Variational feature disentangling for fine grained few shot classification\"\n4. \"Frustratingly simple few shot object detection\"\n5. \"Few shot object detection via feature reweighting\"\n6. \"Variational few shot learning\"\n7. \"Decoupled Faster R-CNN for few shot object detection\"\n8. \"Meta-learning to detect rare objects\"\n\nThe confidence scores for each of these sources are as follows:\n\n```\n{\n\"b38\": 0.9,\n\"b24\": 0.8,\n\"b37\": 0.8,\n\"b32\": 0.8,\n\"b15\": 0.7,\n\"b39\": 0.7,\n\"b26\": 0.8,\n\"b33\": 0.6\n}\n```\n\nThe source papers were identified based on key phrases such as \"inspired by\", \"motivated by\", \"inspired us\", \"motivated us\". Further, the papers were selected based on their relevance to the novel methods and approaches outlined in the paper. The confidence scores are given based on how directly these sources are related to the paper's contributions and how often they are referred to in the text.", "Based on the text and the titles of the references, the most pertinent source papers are likely to be:\n\n1. \"few shot object detection via variational feature aggregation\": This is the paper's title, indicating that it's the main work of this paper.\n\n2. \"Meta r cnn towards general solver for instance level low shot learning[b38]\" : The paper's method is built on a strong meta-learning baseline based on Meta R-CNN [b38], which strongly indicates that it has been heavily influenced by this work.\n\n3. \"transformation invariant few shot object detection[b19]\": The paper refers to this work when discussing the feature space augmentation. \n\n4. \"few shot object detection via feature reweighting[b15]\": This paper is mentioned when discussing the early attempts in FSOD with meta-learning architectures.\n\n5. \"deep variational metric learning[b24]\": The paper uses this work to justify the use of common distributions to model intra-class variance.\n\n6. \"variational feature disentangling for fine grained few shot classification[b37]\": The paper refers to this work when discussing the transformation of support features into class distributions with VAEs.\n\n7. \"defrcn decoupled faster r cnn for few shot object detection[b26]\": Mentioned in the context of decoupling classification and regression tasks.\n\nSo, the result should be:\n\n{\n\"b38\": 0.9,\n\"b19\": 0.8,\n\"b15\": 0.7,\n\"b24\": 0.7,\n\"b37\": 0.7,\n\"b26\": 0.6\n}\n"], "62ce38205aee126c0f18bb75": ["Based on the provided text, the source papers that have inspired the authors the most can be inferred from the direct references in the text:\n\n1. \"[b4]\" - This paper is mentioned in connection with the calibration of language models, which is a key component of the authors' research. Confidence Score: 0.9\n2. \"[b19]\" - This paper is mentioned multiple times, suggesting that its findings have significantly informed the authors' research. Confidence Score: 1.0\n3. \"[b1]\" - This paper is referenced in the description of the models used in the study, implying a significant methodological influence. Confidence Score: 0.8\n4. \"[b5]\" - This paper is mentioned in the context of the authors' exploration of multiple choice formats, suggesting that it has informed their methodological approach. Confidence Score: 0.7\n5. \"[b13]\" - This paper is listed as being very similar to the authors' work, indicating that it may have served as an inspiration or point of comparison. Confidence Score: 0.8\n6. \"[b11]\" - This paper is also listed as being very similar to the authors' work, indicating its influence. Confidence Score: 0.8\n\nThe JSON format:\n\n{\"b4\": 0.9, \"b19\": 1.0, \"b1\": 0.8, \"b5\": 0.7, \"b13\": 0.8, \"b11\": 0.8}", "Based on the text, the following references seem to be the most influential sources for the paper:\n\n1. \"training a helpful and harmless assistant with reinforcement learning from human feedback [b1]\"\n2. \"training verifiers to solve math word problems [b3]\"\n3. \"how can we know when language models know on the calibration of language models for question answering [b8]\"\n4. \"truthful ai developing and governing ai that does not lie [b4]\"\n5. \"linguistic calibration through metacognition aligning dialogue agent responses with expected correctness [b13]\"\n6. \"teaching models to express their uncertainty in words [b11]\"\n7. \"scaling language models methods analysis insights from training gopher [b19]\"\n\nThese references are mentioned in the context of the paper's novel contributions and methods, and they appear to be directly relevant to the paper's core research.\n\nThe JSON format of the result is:\n\n{\n  \"b1\": 0.8,\n  \"b3\": 0.8,\n  \"b8\": 0.85,\n  \"b4\": 0.7,\n  \"b13\": 0.7,\n  \"b11\": 0.9,\n  \"b19\": 0.9\n}\n\nThe confidence scores are assigned based on the degree to which the reference appears to have influenced the paper's contributions. Higher scores represent a higher level of influence. These scores are subjective and are based on my interpretation of the text.", "Based on the text of the paper, the main challenges outlined are evaluating the ability of a language model to assess its own outputs and predictions, and improving this ability through various methods. The source papers that appear to have directly inspired or significantly influenced the methods and approaches used in the paper are:\n\n1. \"training a helpful and harmless assistant with reinforcement learning from human feedback [b1]\": This paper seems to be directly related to the methods used in the current paper, particularly in terms of using reinforcement learning to train AI systems.\n2. \"truthful ai developing and governing ai that does not lie [b4]\": This paper discusses the concept of truthful AI, which is a key aspect of the current paper's investigation into AI self-evaluation.\n3. \"can you trust your model s uncertainty evaluating predictive uncertainty under dataset shift [b18]\": The current paper's focus on language model's ability to predict their own correctness is related to the uncertainty evaluation discussed in this paper.\n4. \"scaling language models methods analysis insights from training gopher [b19]\": This paper discusses the scaling of language models, which could be relevant to the current paper's examination of large language models.\n5. \"self consistency improves chain of thought reasoning in language models [b20]\": The concept of self-consistency is directly related to the current paper's investigation into language models' self-evaluation capabilities.\n6. \"teaching models to express their uncertainty in words [b11]\": This paper's focus on expressing model uncertainty in words seems relevant to the current paper's examination of language models' self-evaluation abilities.\n\nTherefore, the most pertinent source papers are: b1, b4, b18, b19, b20, and b11.\n\nIn json format:\n{\n\"b1\": 0.95,\n\"b4\": 0.9,\n\"b18\": 0.85,\n\"b19\": 0.8,\n\"b20\": 0.85,\n\"b11\": 0.8\n}", "Based on the text of the paper, the most pertinent source papers are:\n\n1. \"Training a helpful and harmless assistant with reinforcement learning from human feedback\" [b1]: This paper is directly relevant as it discusses the training setup for the language models used in the primary study, as well as the concept of calibration in language models.\n\n2. \"Truthful ai developing and governing ai that does not lie\" [b4]: This paper is central to the topic of honesty and calibration in AI systems, which is the main focus of the primary paper.\n\n3. \"How can we know when language models know on the calibration of language models for question answering\" [b8]: This paper is directly related to the study of calibration in language models, particularly in the context of question answering.\n\n4. \"Linguistic calibration through metacognition aligning dialogue agent responses with expected correctness\" [b13]: This paper's concept of metacognition/self-evaluation in language models aligns with the primary paper's goal of studying self-evaluation in large language models.\n\n5. \"Scaling language models methods analysis insights from training gopher\" [b19]: This paper is relevant as it discusses the calibration of a wide range of models on the diverse BIG Bench suite of tasks, which is used in the primary paper.\n\n6. \"Self consistency improves chain of thought reasoning in language models\" [b20]: This paper is relevant due to its discussion of self-consistency in language models, which is related to the primary paper's concept of models evaluating their own samples.\n\nThe JSON format is {\"b1\": 1, \"b4\": 0.9, \"b8\": 0.9, \"b13\": 0.8, \"b19\": 0.8, \"b20\": 0.8}.", "Based on the content of the paper, the key challenges it addresses, and the text provided, the source papers that inspired this research can be inferred. The key phrases indicating reliance on previous research include \"calibration improves with model size and few-shot prompting\", \"Good calibration opens up the possibility for using models to evaluate the accuracy of their own outputs\", \"we find that language models can easily learn to perform well at evaluating P(IK)\", and \"calibration improves with model size, and it also improves when we pass from zero-shot to few-shot evaluation\".\n\nThe source papers identified from these phrases include:\n\n1. \"language models mostly know what they know\": This paper likely inspired the current study's exploration of language model's ability to self-evaluate their output's accuracy. The phrase \"we find that language models can easily learn to perform well at evaluating P(IK)\" indicates a reliance on this source paper.\n\n2. \"Training a helpful and harmless assistant with reinforcement learning from human feedback\": This paper could have contributed to the methodological approach adopted in the current study, as evidenced by the phrase \"calibration improves with model size and few-shot prompting\".\n\n3. \"truthful ai developing and governing ai that does not lie\": This research could have been a source of inspiration, given the current study's focus on ensuring the honesty of AI systems.\n\n4. \"scaling language models methods analysis insights from training gopher\": This paper likely inspired the current study's focus on how calibration improves with model size.\n\nPlease note that the confidence score for each source paper is an estimate, based on the key phrases identified in the text and the inferred relevance to the study's challenges and contributions.\n\nThe result in the JSON format is:\n\n{\n    \"None\": 0.8, \n    \"b1\": 0.7, \n    \"b4\": 0.6, \n    \"b19\": 0.9\n}"], "629587465aee126c0fe149f5": ["Based on the given text, the following references appear to be the most pertinent source papers:\n\n- [b3]: \"Language Models are Few Shot Learners\" - The paper's central focus is on fine-tuning the GPT-3 model, which was introduced in this source paper. It is likely that the methods and challenges discussed in the paper draw heavily from this seminal work. Confidence score: 0.9\n- [b10]: \"On Calibration of Modern Neural Networks\" - The paper discusses the concept of model calibration extensively, which suggests a significant influence from this source paper that focuses on the same topic. Confidence score: 0.85\n- [b24]: \"Posterior Calibration and Exploratory Analysis for Natural Language Processing Models\" - The authors' approach to calibrating the model's uncertainty expressions seems to be influenced by this source paper. Confidence score: 0.8\n- [b27]: \"Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift\" - The paper's efforts to evaluate the model's calibration under distribution shifts appears to be inspired by this source paper. Confidence score: 0.8\n\nPlease note that the confidence scores are estimated based on the paper's text and do not represent absolute certainty.\n\nIn JSON format, the references and their confidence scores would be:\n{\n   \"b3\": 0.9,\n   \"b10\": 0.85,\n   \"b24\": 0.8,\n   \"b27\": 0.8\n}", "Based on the provided information, it seems that the paper focuses on improving the reliability of language model responses by enabling models like GPT-3 to generate natural language expressions of confidence in their answers. The paper introduces a new test, CalibratedMath, to evaluate this capability. \n\nUpon scanning the text and the titles of the references, the relevant source papers that seem to have inspired this work are:\n\n- \"Language models are few-shot learners\" [b3] - This is the foundational paper for GPT-3.\n- \"On calibration of modern neural networks\" [b10] - This is related to the concept of model calibration, which is central to the paper.\n- \"Teaching models to express their uncertainty in words\" [None] - Judging by the title, this paper seems directly related to the main objective of the presented paper.\n- \"Truthful AI: Developing and governing AI that does not lie\" [b8] - This may be related to the paper's aim to make models \"honest\".\n- \"Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift\" [b27] - This pertains to the concept of testing how well calibration generalizes under distribution shift.\n- \"Posterior calibration and exploratory analysis for natural language processing models\" [b24] - This paper seems to be related to the topic of model calibration in the context of natural language processing.\n\nIn the json format, the result would be:\n{\n    \"b3\": 1,\n    \"b10\": 0.9,\n    \"None\": 0.9,\n    \"b8\": 0.8,\n    \"b27\": 0.8,\n    \"b24\": 0.7\n}\n\nThese confidence scores are arbitrary and can be adjusted based on a more thorough reading of the documents.", "Based on the text of the paper, the source papers that inspired the authors most extensively are likely:\n\n1. \"Language models are few shot learners\" [b3]\n2. \"Training compute optimal large language models\" [b13]\n3. \"Teaching models to express their uncertainty in words\" (Cannot find its reference number)\n4. \"Calibration of pre trained transformers\" [b7]\n5. \"Truthful AI: Developing and governing AI that does not lie\" [b8]\n6. \"On calibration of modern neural networks\" [b10]\n7. \"Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift\" [b27]\n\nThe confidence score for each reference can be given as follows:\n{\"b3\": 0.9, \"b13\": 0.85, \"b7\": 0.85, \"b8\": 0.8, \"b10\": 0.8, \"b27\": 0.75}", "Based on the provided text, the following papers seem to be the most relevant sources:\n\n1. \"Language models are few shot learners\" [b3]. This paper is mentioned multiple times and seems to be the base model (GPT-3) that the authors are improving upon. Confidence score: 0.9\n\n2. \"On calibration of modern neural networks\" [b10]. This paper is mentioned in the context of previous work on calibration, which is a key aspect of the paper's subject. Confidence score: 0.8\n\n3. \"Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift\" [b27]. This paper is mentioned in the context of testing calibration which is a significant part of the authors' approach. Confidence score: 0.75\n\n4. \"Truthful AI: developing and governing AI that does not lie\" [b8]. This paper is mentioned in the context of training models in verbalized probability for honesty, which is one of the novel contributions of the authors. Confidence score: 0.75\n\n5. \"Measuring how models mimic human falsehoods\" [b19]. This paper is mentioned as being related to the issue of models producing false statements or \"hallucinations\". Confidence score: 0.7\n\n6. \"Calibration of pre-trained transformers\" [b7]. This paper is mentioned in the context of previous work on calibration, which is a key aspect of the paper's subject. Confidence score: 0.65\n\n7. \"A general language assistant as a laboratory for alignment\" [b1]. This paper is mentioned in the context of training models in verbalized probability for honesty, which is one of the novel contributions of the authors. Confidence score: 0.6\n\n8. \"Posterior calibration and exploratory analysis for natural language processing models\" [b24]. This paper is mentioned in the context of previous work on calibration, which is a key aspect of the paper's subject. Confidence score: 0.6\n\nThe JSON format of the result is: \n\n{\n    \"b3\": 0.9,\n    \"b10\": 0.8,\n    \"b27\": 0.75,\n    \"b8\": 0.75,\n    \"b19\": 0.7,\n    \"b7\": 0.65,\n    \"b1\": 0.6,\n    \"b24\": 0.6\n}", "Based on the text of the paper and the titles of the references, the source papers that inspired the given paper the most are:\n\n1. \"Language models are few-shot learners\" [b3]: This paper is mentioned several times in the text and it seems to have strong methodological similarities with the given paper. The given paper also deals with language models and their performance on tasks.\n\n2. \"On calibration of modern neural networks\" [b10]: This paper is referenced when discussing calibration, a key topic in the given paper.\n\n3. \"Posterior calibration and exploratory analysis for natural language processing models\" [b24]: This paper is related to the calibration of language models, a major challenge outlined in the given paper.\n\n4. \"Verbalized probability has the expressive power of natural language and so could express continuous distributions\" [b14]: This reference seems to be directly related to the given paper's novel contribution of using verbalized probability.\n\n5. \"Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift\" [b27]: This paper seems to be related to the challenge of evaluating models under distribution shifts, which is mentioned in the given paper.\n\n6. \"Truthful AI: Developing and governing AI that does not lie\" [b8]: This paper is mentioned in the context of training models in verbalized probability, a novel method proposed in the given paper.\n\nThe confidence score for each source paper is estimated based on how directly they seem to contribute to the given paper's main contributions:\n\n{\"b3\":0.9, \"b10\":0.8, \"b24\":0.85, \"b14\":0.75, \"b27\":0.7, \"b8\":0.7}"], "629587475aee126c0fe14c42": ["Based on the given text, the source papers that inspired the given paper are:\n\n- \"Language models are few-shot learners\" [b2]: The paper refers to few-shot learning ability with the natural language prompts, indicating it was inspired or influenced by this work.\n- \"Making pre-trained language models better few-shot learners\" [b11]: This paper is referred to in the context of the generalization ability of prompt learning with retrieval and association.\n- \"KnowPrompt: Knowledge-aware prompt tuning with synergistic optimization for relation extraction\" [b4]: This paper is referred to as the baseline for information extraction tasks, suggesting it was a source of inspiration.\n- \"Does learning require memorization? A short tale about a long tail\" [b9]: This paper is referenced in the discussion of long-tail theory and the limitations of rote learning, indicating it was influential in the development of the given paper's approach.\n- \"What neural networks memorize and why: Discovering the long tail via influence estimation\" [b10]: This work is cited in conjunction with the above paper, suggesting it also played a role in shaping the given paper's methodology.\n- \"The power of scale for parameter efficient prompt tuning\" [b27]: This paper is referenced in the discussion of the limitations of rote memorization, indicating it was influential in the development of the given paper's approach.\n\nIn JSON format:\n\n{\n\"b2\": 0.9,\n\"b11\": 0.9,\n\"b4\": 0.8,\n\"b9\": 0.8,\n\"b10\": 0.8,\n\"b27\": 0.8\n}", "Based on the text provided, the most pertinent source papers that inspired the given paper are identified as follows:\n\n1. \"Making Pre-trained Language Models Better Few-shot Learners\" [b11]\n2. \"KnowPrompt: Knowledge-Aware Prompt Tuning with Synergistic Optimization for Relation Extraction\" [b4]\n3. \"Does Learning Require Memorization? A Short Tale about a Long Tail\" [b9]\n4. \"What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation\" [b10]\n5. \"Efficient Nearest Neighbor Language Models\" [b14]\n6. \"Neurosymbolic Language Modeling with Automaton Augmented Retrieval\" [b0]\n7. \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" [b29]\n\nThese papers have been identified based on the context and key phrases in the text that indicate reliance on their findings, methodologies, and ideas. The confidence scores for each of the identified source papers can be calculated based on the frequency and strength of their mentions in the text, as well as their relevance to the main contributions of the given paper.\n\nThe result in JSON format:\n\n```json\n{\n  \"b11\": 0.9,\n  \"b4\": 0.9,\n  \"b9\": 0.8,\n  \"b10\": 0.8,\n  \"b14\": 0.7,\n  \"b0\": 0.7,\n  \"b29\": 0.7\n}\n```", "Based on the given information, the main challenges outlined in the paper are: \n1. The limitations of rote memorization in prompt learning with pre-trained language models (PLMs).\n2. The struggle of vanilla prompt learning to memorize atypical instances or overfit shallow patterns with low-shot data.\n3. The need for a balance between generalization and memorization in PLMs.\n\nThe algorithm proposed by the author is the retrieval-augmented framework based on prompt learning (RETROPROMPT). \n\nThe phrases indicating that the author was inspired by other works are \"Motivated by these, we endeavor to improve the generalization ability of prompt learning with retrieval and association\" and \"We further adopt self-influence [b24] as our memorization scoring function\".\n\nThe source papers that directly influenced the paper's novel contributions are:\n1. b11: \"Making pre-trained language models better few-shot learners\" - As it is the one that is referred when discussing the limitations of rote memorization and the idea of prompt learning.\n2. b24: \"Understanding black box predictions via influence functions\" - As it is the one that is referred when discussing the memorization scoring function in RETROPROMPT.\n\nThe final result is {\"b11\": 0.8, \"b24\": 0.7}. This means the paper \"Making pre-trained language models better few-shot learners\" has a confidence score of 0.8 and the paper \"Understanding black box predictions via influence functions\" has a confidence score of 0.7 as sources that most inspire the author.", "Based on the text of the paper and the provided titles of the references, the source papers that have inspired this paper most are:\n\n1. \"Prompt learning for fine-grained entity typing\" [b6]: The paper discusses \"prompt learning\", which directly relates to the topic of this paper. Confidence score: 0.8\n2. \"Making pre-trained language models better few-shot learners\" [b11]: This paper also works on improving pre-trained language models, which aligns with the goal of this paper. Confidence score: 0.9\n3. \"KnowPrompt: Knowledge-aware prompt tuning with synergistic optimization for relation extraction\" [b4]: This paper introduces the concept of \"knowledge-aware prompt tuning\", which is similar to the \"retrieval-augmented prompt learning\" concept introduced in this paper. Confidence score: 0.8\n4. \"Knowledgeable prompt tuning: Incorporating knowledge into prompt verbalizer for text classification\" [b17]: The paper also talks about incorporating knowledge into prompt learning, which is similar to the approach used in this paper. Confidence score: 0.8\n5. \"Billion-scale similarity search with GPUs\" [b18]: This research might have inspired the retrieval process of the knowledge-store in this paper. Confidence score: 0.7\n6. \"Efficient nearest neighbor language models\" [b14]: This paper might have inspired the use of kNN for guiding the training process and cloze-style prediction in this paper. Confidence score: 0.7\n\nThe json format of the results is:\n\n{\n    \"b6\": 0.8,\n    \"b11\": 0.9,\n    \"b4\": 0.8,\n    \"b17\": 0.8,\n    \"b18\": 0.7,\n    \"b14\": 0.7\n}", "Based on the provided text and the guidelines, the most pertinent source papers that inspired the given paper are:\n\n1. \"Language models are few shot learners\" [b2]: The paper mentions the achievement of pre-trained language models in few-shot/zero-shot learning ability, which is a key challenge addressed in this paper.\n2. \"Making pre-trained language models better few-shot learners\" [b11]: The paper mentions the struggle of prompt learning to memorize atypical instances, which is a problem the authors aim to tackle. This paper seems to have introduced an approach to handling this issue.\n3. \"KnowPrompt: Knowledge aware prompt tuning with synergistic optimization for relation extraction\" [b4]: The paper discusses a novel retrieval-augmented framework for prompt learning, which seems to build upon the knowledge-aware prompt tuning introduced in this paper.\n4. \"Efficient nearest neighbor language models\" [b14]: The paper introduces a kNN-based probability for cloze-style prediction, which seems to be inspired by the approach outlined in this paper.\n5. \"An empirical study of memorization in nlp\" [b58]: The paper discusses the limitations of rote memorization in prompt learning, which is a key challenge addressed in this study.\n6. \"Does learning require memorization? A short tale about a long tail\" [b9]: This paper discusses the long-tail theory and the limitations of rote memorization, which are key challenges addressed in the study.\n\nThe JSON format would be:\n\n{\n\"b2\": 0.9,\n\"b11\": 0.85,\n\"b4\": 0.85,\n\"b14\": 0.8,\n\"b58\": 0.75,\n\"b9\": 0.75\n}"], "624fa8db5aee126c0f3a5b79": ["Based on the text and author notes, the following source papers seem to be the most influential:\n\n1. \"Contrastive Language-Image Pretraining (CLIP)\" [b24] - This paper is cited multiple times and the methods proposed by the author are clearly based on the techniques developed in this paper, especially in terms of how they use large-scale pretraining knowledge and the approach to learning multi-modal knowledge. Confidence Score: 0.9\n\n2. \"CLIP4Clip\" [b21] - This paper is also referenced multiple times and the author's method is inspired by how they develop temporal ensemble modules to aggregate sequential CLIP frame features into a global one, and how they use cross-modal contrastive learning. Confidence Score: 0.8\n\n3. \"DCR\" [b27] - The author's method uses token-wise interaction between frames and words, which was proven effective by DCR. Confidence Score: 0.7\n\n4. \"FROZEN\" [b1] - The author refers to this paper when discussing methods that use separate text and video embedding extractors. Confidence Score: 0.6\n\n5. \"DRL\" [b27] - It is mentioned when discussing the state-of-the-art method for frame-word interaction. Confidence Score: 0.6\n\nBased on the titles of the papers, we can also infer the influence of the following papers:\n\n6. \"Hierarchical Transformer with Momentum Contrast for Video Text Retrieval\" [b18] - The hierarchical exploration of video-sentence, clip-phrase, and frame-word interactions in the author's method aligns with the hierarchical transformer concept in this paper. Confidence Score: 0.7\n\nFinally, the author's notes also imply that the author's method is a novel combination of token-wise and Dot product, which is not directly mentioned in the source papers but might have been inspired by several works.\n\nThe JSON result is: {\"b24\": 0.9, \"b21\": 0.8, \"b27\": 0.7, \"b1\": 0.6, \"b18\": 0.7}", "Based on the text, the key phrases indicating direct inspiration or motivation, and the overall context of the paper, the most pertinent source papers for \"HunYuan_tvr for text video retrieval\" are:\n\n1. \"Contrastive Language-Image Pretraining (CLIP)\" [b24] - The paper frequently refers to the CLIP model as an important foundation for its own method. The CLIP model is used as the initial visual and text encoders for HunYuan_tvr, which indicates a strong reliance on this paper. \n\n2. \"CLIP4Clip\" [b21] - This paper is mentioned several times as an important precursor to the current work. The HunYuan_tvr method improves upon CLIP4Clip's approach by exploring hierarchical cross-modal interactions rather than simply summing up frame representations.\n\n3. \"DCR\" [b27] - DCR is referred to as a method that proves the value of token-wise interaction between frames and words, which is a key concept in the HunYuan_tvr method as well. \n\n4. \"FROZEN\" [b1] and \"CLIP4clip\" [b21] - They are mentioned as examples of the video-sentence-interaction-based methods, which are a category of methods relevant to the HunYuan_tvr method.\n\n5. \"DRL\" [b27] - It is mentioned as the state-of-the-art frame-word-interaction-based method which is a category of methods relevant to the HunYuan_tvr method.\n\n6. \"CAMoE\" [b3] - This paper is mentioned in the context of methods that also transfer the knowledge from the image-text pre-trained model CLIP to solve the video-text retrieval task, which is a similar approach to HunYuan_tvr.\n\nThe confidence score for each source paper can be inferred based on their relevance to the novel contributions of \"HunYuan_tvr for text video retrieval\". Here, we assign a confidence score of 1 to [b24] and [b21] as they seem to be the most influential. The score for [b27] is 0.8, [b1] is 0.7, and [b3] is 0.6.\n\nThe result in json format:\n{\n    \"b24\": 1,\n    \"b21\": 1,\n    \"b27\": 0.8,\n    \"b1\": 0.7,\n    \"b3\": 0.6\n}", "Based on the information provided and the analysis of the text, the source papers that most inspired the given paper are:\n\n- Reference \"b24\": \"Learning transferable visual models from natural language supervision\". This paper is mentioned multiple times in the text and it is the basis for the CLIP model that the authors use and build upon. The confidence score might be 0.9.\n- Reference \"b21\": \"CLIP4Clip\". This paper is also mentioned multiple times in the text, and the method it proposes is directly used and compared to in the given paper. The confidence score might be 0.85.\n- Reference \"b27\": \"Disentangled Representation Learning for Text-Video Retrieval\". This paper is referenced in the context of explaining the method that the given paper builds upon. The confidence score might be 0.8.\n- Reference \"b3\": \"Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss\". This paper is mentioned in the context of other methods that the given paper compares itself to. The confidence score might be 0.75.\n\nThe json format of the above result is:\n\n{\"b24\": 0.9, \"b21\": 0.85, \"b27\": 0.8, \"b3\": 0.75}", "Based on the text of the paper, the following source papers are identified:\n\n1. \"Contrastive Language-Image Pretraining (CLIP)\" [b24]: The paper heavily relies on the CLIP model for its novel method called HunYuan_tvr. It is clear from the text that HunYuan_tvr leverages CLIP as the initial visual and text encoders, similar to [b21] [b27]. The paper also mentions that the CLIP model is used in other methods [b21] [b9] [b27] [b3] that the author's method is compared with, which indicates that CLIP can be seen as a source paper.\n\n2. \"CLIP4Clip\" [b21]: CLIP4Clip is mentioned multiple times in the paper, and it has developed temporal ensemble modules which is a concept used in the author's method. The author's method is also compared with CLIP4Clip, indicating a significant influence.\n\n3. \"DCR\" [b27]: DCR is another paper that has influenced the author's work, as the author's method is directly compared to it. The paper mentions how DCR proves the token-wise interaction between frames and words reveals more fine-grained cross-modal knowledge, a concept that is used in the author's method.\n\n4. \"FROZEN\" [b1]: FROZEN and the author's method both treat an image as a single-frame video and design a curriculum learning schedule to train the model on both image and video datasets.\n\n5. \"CAMoE\" [b3]: CAMoE is mentioned in the paper as a method that has made significant strides in the field and has proposed a multi-stream Corpus Alignment network with single gate Mixture-of-Experts (CAMoE) and a novel Dual Softmax Loss (DSL) to further improve the retrieval performance. \n\n6. \"DRL\" [b27]: DRL is mentioned as a state-of-the-art method which proposes a Weighted Token-wise Interaction (WTI) to explore the fine-grained clues between sentence tokens and video frame embeddings and a Channel DeCorrelation Regularization (CDCR) to reduce feature redundancy from a micro-view. This concept is also used in the author's method.\n\nBased on the preceding analysis, the confidence score for each source paper is as follows:\n\n- [b24]: 0.9\n- [b21]: 0.8\n- [b27]: 0.8\n- [b1]: 0.7\n- [b3]: 0.7\n- [b27]: 0.7", "Based on the given text, the source papers that inspired the author the most are:\n\n1) \"Contrastive Language-Image Pretraining (CLIP)\" [b24] - The author has stated that their method, HunYuan_tvr, leverages CLIP as the initial visual and text encoders, and this reference is directly linked to the main challenge and method of the paper.\n\n2) \"CLIP4Clip\" [b21] - The author mentions that their work is based on CLIP, and recent methods aim to transfer the well-pretrained image-text knowledge from text-image to text-video, such as CLIP4Clip. \n\n3) \"DCR\" [b27] - The author mentions that unlike DCR, which proves that the token-wise interaction between frames and words reveals more fine-grained cross-modal knowledge, their method explores hierarchical cross-modal interactions.\n\n4) \"Weighted Token-wise Interaction (WTI)\" [b27] - The author uses token-wise interactions to capture dense correlations between multi-tokens, which is similar to the approach used in WTI.\n\nThe confidence scores are difficult to assess without further context and analysis. However, based on the frequency and manner in which these sources are mentioned, a tentative scoring could be:\n\n{ \"b24\": 0.9, \"b21\": 0.8, \"b27\": 0.75 } \n\nThese scores reflect the direct relevance of these papers to the author's work, with higher scores indicating a stronger influence."], "634d805690e50fcafd4e07d5": ["Based on the text, the following references are likely to be the source papers:\n\n1. \"investigating managed language runtime performance why javascript and python are x and x slower than c yet java and go can be faster\" - This paper seems to be the most important one as it is directly related to the main topic of the text. The author is using the same topic of comparing different languages runtime performance and trying to explain why some languages can be faster than the others.\n\n2. \"myths of enterprise python\" - This paper is related to the performance of Python, which is one of the languages that the author is discussing in the text.\n\n3. \"what cases is java faster than c\" - The author is using the same topic of comparing Java and C language performance.\n\n4. \"why discord is switching from go to rust\" - This paper is relevant because Go is one of the languages that the author is discussing in the text.\n\n5. \"the state of the octoverse\" - This paper provides a broader context for the popularity of different programming languages, which is a relevant factor when discussing their performance.\n\n6. \"don't get caught in the cold, warm up your jvm understand and eliminate jvm warm up overhead in data parallel systems\" - This paper is directly related to the performance of Java, which is one of the languages that the author is discussing in the text.\n\n7. \"renaissance benchmarking suite for parallel applications on the jvm\" - This paper is related to the performance of Java, which is one of the languages that the author is discussing in the text.\n\nHere is the result in JSON format:\n\n{\n  \"reference number\": [\"b33\", \"b0\", \"b22\", \"b36\", \"b55\", \"b68\"],\n  \"confidence score\": [1, 0.8, 0.8, 0.8, 0.8, 0.8]\n}", "Based on the text of the academic paper and the nature of the author's notes, the most likely source papers that have inspired this research are:\n\n1. 'Investigating managed language runtime performance: Why JavaScript and Python are x and x slower than C, yet Java and Go can be faster' (This seems to be the most direct source of inspiration, as the author's research seems to be a deep dive into the performance of managed languages)\n2. 'Myths of Enterprise Python' (The author's study seems to be partly motivated by debunking or confirming certain popular ideas about language performance, which aligns with the focus of this paper)\n3. 'Why do std::string operations perform poorly?' (This paper's focus on the performance of specific language features appears to align with the author's research)\n4. 'Why Discord is switching from Go to Rust' (The author's research includes a focus on Go's performance, which seems to be directly inspired by this paper's exploration of the same topic)\n5. 'The benefits and costs of writing a POSIX kernel in a high level language' (This paper's exploration of high-level language performance in system-level programming could have inspired the author's research)\n6. 'Dynamic Vertical Memory Scalability for OpenJDK Cloud Applications' (The author's research includes a focus on memory management in managed languages, which seems to be directly inspired by this paper's exploration of the same topic in the context of OpenJDK)\n7. 'Don't Get Caught in the Cold, Warm Up Your JVM: Understand and Eliminate JVM Warm Up Overhead in Data-Parallel Systems' (The author's research includes a focus on startup overhead, which seems to be directly inspired by this paper's exploration of the same topic in the context of JVM)\n\nHere is the result in the required format:\n\n{\n    \"b33\": 0.9,\n    \"b0\": 0.8,\n    \"b29\": 0.8,\n    \"b36\": 0.8,\n    \"b45\": 0.7,\n    \"b56\": 0.7,\n    \"b55\": 0.7\n}", "Based on the information provided in the text and the analysis of the content, the source papers that most likely inspired the authors are:\n- \"investigating managed language runtime performance why javascript and python are x and x slower than c yet java and go can be faster\"\n- \"myths of enterprise python\"\n- \"most popular languages on github\"\n- \"the state of developer ecosystem\"\n- \"the state of the octoverse\"\n- \"why discord is switching from go to rust\"\n- \"why we switched from python to go\"\n\nThe JSON format of the result is:\n{\n\"b0\": 0.9, \n\"b13\": 0.85, \n\"b31\": 0.85, \n\"b33\": 0.85, \n\"b36\": 0.8, \n\"b39\": 0.8, \n\"None\": 1\n}", "Based on the text and author's note, the following references seem to be the source papers for this academic paper:\n\n1. \"The State of the Octoverse\"\n2. \"Myths of Enterprise Python\"\n3. \"Don't get caught in the cold, warm up your JVM: Understand and eliminate JVM warm up overhead in data-parallel systems\"\n4. \"Renaissance: Benchmarking Suite for Parallel Applications on the JVM\"\n5. \"Benchmarking Usability and Performance of Multicore Languages\"\n6. \"Innovations for Java Running in Containers\"\n7. \"Characterizing Serverless Platforms with ServerlessBench\"\n\nThe confidence scores are given based on the level of influence they seem to have on the paper's methodology and findings.\n\n{\n\"b33\": 0.8,\n\"b0\": 0.7,\n\"b55\": 0.7,\n\"b63\": 0.6,\n\"b59\": 0.6,\n\"b68\": 0.5,\n\"b73\": 0.5\n}", "Based on the academic paper text and the provided guidelines, the most pertinent source papers that inspired the authors are:\n\n1. \"Myths of Enterprise Python\" [b0]\n2. \"Go memory ballast: How I learnt to stop worrying and love the heap\" [b4]\n3. \"Optimizing a Golang service to reduce over CPU\" [b16]\n4. \"Our journey to type checking 4 million lines of Python\" [b17]\n5. \"Why Discord is switching from Go to Rust\" [b36]\n6. \"Project Tungsten: Bringing Apache Spark closer to bare metal\" [b19]\n7. \"The State of Serverless\" [b32]\n\nThese papers are chosen based on the key phrases indicating strong reliance on previous research for developing the paper's core contributions and the relevance to the challenges and methods proposed in the paper. \n\nThe confidence score for each reference paper is based on the relevance and importance of their contribution to the current paper. The score could be:\n- \"Myths of Enterprise Python\" [b0]: 0.85\n- \"Go memory ballast: How I learnt to stop worrying and love the heap\" [b4]: 0.8\n- \"Optimizing a Golang service to reduce over CPU\" [b16]: 0.8\n- \"Our journey to type checking 4 million lines of Python\" [b17]: 0.75\n- \"Why Discord is switching from Go to Rust\" [b36]: 0.9\n- \"Project Tungsten: Bringing Apache Spark closer to bare metal\" [b19]: 0.8\n- \"The State of Serverless\" [b32]: 0.85\n\nThis results in the following JSON format:\n\n```\n{\n\"b0\": 0.85,\n\"b4\": 0.8,\n\"b16\": 0.8,\n\"b17\": 0.75,\n\"b36\": 0.9,\n\"b19\": 0.8,\n\"b32\": 0.85\n}\n```"], "62281ae45aee126c0f7aa8a8": ["Based on the text of the paper and the titles of the references, the most pertinent source papers are:\n\n1. 'incorporating hierarchy into text encoder a contrastive learning approach for hierarchical text classification'\n2. 'bert pre training of deep bidirectional transformers for language understanding'\n3. 'hierarchy aware global model for hierarchical text classification'\n4. 'hierarchy aware label semantics matching network for hierarchical text classification'\n5. 'contrastive self supervised learning for language understanding'\n6. 'a simple framework for contrastive learning of visual representations'\n7. 'do transformers really perform badly for graph representation'\n8. 'generating natural language adversarial examples'\n\nThese papers are relevant to the author's work as they discuss the use of contrastive learning, hierarchical text classification, and transformers, which are central to the author's approach.\n\nHere is the final result in the requested JSON format:\n\n```json\n{\n    \"b0\": 0.8,\n    \"b6\": 0.9,\n    \"b4\": 0.85,\n    \"b3\": 0.85,\n    \"b14\": 0.8,\n    \"b33\": 0.8,\n    \"b37\": 0.9\n}\n```\nThe confidence scores are subjective and represent how closely related the source paper is to the main contributions of the given paper. A score close to 1 indicates a strong influence.", "Based on the text, the following source papers seem most pertinent:\n\n1. \"incorporating hierarchy into text encoder a contrastive learning approach for hierarchical text classification\" (b1)\n2. \"hyperbolic interaction model for hierarchical multi label classification\" (b2)\n3. \"hierarchy aware label semantics matching network for hierarchical text classification\" (b3)\n4. \"htcinfomax a global model for hierarchical text classification via information maximization\" (b5)\n5. \"a simple framework for contrastive learning of visual representations\" (b4)\n6. \"hierarchical transfer learning for multi label text classification\" (b1)\n7. \"generating natural language adversarial examples\" (b0)\n8. \"self guided contrastive learning for bert sentence representations\" (b14)\n\nThis list of source papers is primarily determined by phrases such as \"inspired by\", \"motivated by\", \"adopt\", as well as the relevance of their titles and the context they are mentioned in the paper.\n\nThe confidence of each source paper:\n{\n\"b1\": 0.9,\n\"b2\": 0.8,\n\"b3\": 0.9,\n\"b5\": 0.85,\n\"b4\": 0.9,\n\"b1\": 0.9,\n\"b0\": 0.7,\n\"b14\": 0.85\n} \n\nThe confidence score is determined by the frequency of the reference in the text and the relevance of the reference paper's title to the paper's challenges and the proposed algorithm.", "The source papers of the given academic paper based on the provided text and author notes are as follows:\n\n1. \"Hierarchy aware global model for hierarchical text classification\" [b37]: This source appears to be the primary inspiration for the paper, as it is mentioned multiple times throughout the text. The paper's novel method of incorporating hierarchy representation directly into the text encoder seems to draw heavily from this source. Confidence Score: 0.9\n\n2. \"A cognitive structure learning model for hierarchical multi label text classification\" [b27]: This paper is referenced in the text in relation to previous studies on hierarchical text classification. It appears to have influenced the authors' decision to use a hierarchy-guided contrastive learning approach. Confidence Score: 0.8\n\n3. \"Contrastive learning for visual representations\" [b4]: This source, which discusses the use of contrastive learning in computer vision, seems to have influenced the authors' decision to adopt contrastive learning for their hierarchical text classification task. Confidence Score: 0.7\n\n4. \"Generative natural language adversarial examples\" [b0]: This paper, which discusses the building of challenging samples, is referenced in the section discussing contrastive learning. This suggests that it may have inspired the authors' approach to constructing high-quality positive examples. Confidence Score: 0.7\n\n5. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b6]: This paper is referenced as the chosen text encoder for the authors' methodology, suggesting it had a significant influence on their approach. Confidence Score: 0.7\n\n6. \"Do Transformers Really Perform Badly for Graph Representation\" [b33]: This paper appears to have influenced the authors' decision to use a Graphormer for their graph encoder. Confidence Score: 0.7\n\n7. \"The New York Times Annotated Corpus\" [b22]: This paper is included in the reference list, and although it is not directly referenced in the text, it is possible that this is a data source for the authors' experiments. Confidence Score: 0.6\n\nThe confidence scores are based on the frequency of references to the source papers in the given text and the relevance of the source papers to the central methods and contributions of the given paper.\n\nThe JSON format of the output:\n\n    {\n    \"b37\": 0.9,\n    \"b27\": 0.8,\n    \"b4\": 0.7,\n    \"b0\": 0.7,\n    \"b6\": 0.7,\n    \"b33\": 0.7,\n    \"b22\": 0.6\n    }", "Based on the text and the hints provided, the source papers that seem to have greatly inspired the author are:\n\n{\"b37\": 0.85, \"b14\": 0.8, \"b33\": 0.75, \"b35\": 0.7, \"b3\": 0.65, \"b2\": 0.6, \"b11\": 0.55} \n\nReference \"b37\" is mentioned multiple times in the text and is also linked to key methodological points. \"b14\" and \"b11\" are referred to when discussing contrastive learning, which is a central methodology in this paper. The authors also take inspiration from \"b33\" in their use of Graphormer. The paper \"b35\" influences the authors' approach to text features. \"b3\" and \"b2\" are key references when discussing the interaction between text and structure. The confidence scores reflect the inferred level of influence of each paper, with \"b37\" having the highest influence.", "Based on the given text and the titles of the references, the most relevant source papers are:\n\n1. \"Hierarchy-aware global model for hierarchical text classification\" [b37]: This paper is cited multiple times, indicating its significance. The authors of our paper heavily depend on the methodology proposed in this paper, particularly the concept of encoding the holistic label structure directly by a structure encoder for improving performance.\n\n2. \"Incorporating hierarchy into text encoder a contrastive learning approach for hierarchical text classification\" [b1]: This paper seems to be the inspiration for the main approach used in our paper, particularly the incorporation of hierarchy into the text encoder and the use of contrastive learning.\n\n3. \"Do transformers really perform badly for graph representation\" [b33]: This paper is mentioned in the context of the Graphormer used for encoding label hierarchy in the study. The authors adopt this method for their own work.\n\n4. \"A cognitive structure learning model for hierarchical multi label text classification\" [b27]: This paper is cited in the context of previous work on the problem at hand. The authors seem to build upon its findings, but also differentiate their own approach.\n\n5. \"Label-based attention for hierarchical multi-label text classification neural network expert systems with applications\" [b35]: The authors of our paper refer to this study in the context of extracting text features according to different hierarchy levels, which aligns with their own approach.\n\n6. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b6]: This paper is the source of the BERT model used as the text encoder in our study.\n\nThe confidence scores for these papers are based on the number of mentions and the context in which they are mentioned in the text.\n\nThe JSON output would be:\n\n{\n\"b37\": 0.9,\n\"b1\": 0.9,\n\"b33\": 0.8,\n\"b27\": 0.7,\n\"b35\": 0.7,\n\"b6\": 0.7\n}"], "63aa965790e50fcafd008008": ["Based on the text of the paper and the titles of the references, the source papers that most directly influenced this paper might be:\n\n1. \"ER-SQL: Learning Enhanced Representation for Text-to-SQL Using Table Contents\" [b3]\n2. \"X-SQL: Reinforce Schema Representation with Context\" [b4]\n3. \"RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers\" [b7]\n4. \"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task\" [b8]\n5. \"Seq-SQL: Generating Structured Queries from Natural Language Using Reinforcement Learning\" [b9]\n6. \"Semantic Parsing Natural Language into SPARQL: Improving Target Language Representation with Neural Attention\" [b5]\n7. \"Knowledge Graph Question Answering via SPARQL Silhouette Generation\" [b6]\n\nThe paper gives a clear indication that it was heavily influenced by the Text-to-SQL task and the existing models and methods used for that task. The papers listed above all seem to directly contribute to this area of research. The paper is also very likely to have been influenced by the work on Text-to-SPARQL, given the similarities between SQL and SPARQL and the aims of the paper's proposed Text-to-CQL task.\n\nThe confidence score for each of these references could be:\n\n{ \"b3\": 0.8, \"b4\": 0.7, \"b7\": 0.7, \"b8\": 0.9, \"b9\": 0.8, \"b5\": 0.6, \"b6\": 0.6 }", "Based on the text and the criteria given, the following are the most pertinent source papers:\n\n1. \"spcql a semantic parsing dataset for converting natural language into cypher\" - This paper is clearly the main source for the current paper, as it introduces the task Text-to-CQL and the dataset SpCQL, which are the core subjects of the current paper.\n\n2. \"lgesql line graph enhanced text to sql model with mixed local and non local relations\" - This paper is mentioned as a solution for complex query scenarios in the Text-to-SQL task, which seems to have inspired the current paper to tackle similar complex scenarios in the Text-to-CQL task.\n\n3. \"rat sql relation aware schema encoding and linking for text to sql parsers\" - This paper is mentioned in the context of the Text-to-SQL task, which is the most similar task to the current paper's Text-to-CQL task.\n\n4. \"spider a large scale human labeled dataset for complex and cross domain semantic parsing and text to sql task\" - This dataset seems to have inspired the creation of the dataset SpCQL for the Text-to-CQL task in the current paper.\n\n5. \"seq sql generating structured queries from natural language using reinforcement learning\" - This paper is mentioned as a representative of the Text-to-SQL task, which is the base task for the current paper's Text-to-CQL task.\n\nThe result in json format:\n\n{\n    \"b0\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.9,\n    \"b9\": 0.7\n} \n\nThe confidence scores are subjective and may vary. However, these scores reflect the apparent influence of each source paper on the current paper, based on the text provided.", "Based on the text of the academic paper and the titles of the references, the source papers that most inspire this paper likely include:\n\n- 'b8': 'spider a large scale human labeled dataset for complex and cross domain semantic parsing and text to sql task'\n- 'b9': 'seq sql generating structured queries from natural language using reinforcement learning'\n- 'b4': 'x sql reinforce schema representation with context corr abs'\n- 'b7': 'rat sql relation aware schema encoding and linking for text to sql parsers'\n\nThe paper explicitly mentions the inspiration of 'b8' and 'b9' in the \"Related Work and Existing Datasets\" section, where the authors base the design of their Text-to-CQL dataset on 'b8' and 'b9'. The paper also references 'b4' and 'b7' when discussing typical solutions to the Text-to-SQL task, indicating that the authors might have been inspired by the methodologies in these papers when developing their Text-to-CQL task.\n\nHere are the results in json format:\n\n{\n\"b8\": 0.9,\n\"b9\": 0.9,\n\"b4\": 0.85,\n\"b7\": 0.85\n}", "Based on the provided text and given instructions, the most pertinent source papers would be:\n\n- 'b8': 'spider a large scale human labeled dataset for complex and cross domain semantic parsing and text to sql task' - This paper is directly mentioned in the text and is the base for the design of the Text-to-CQL dataset. The author also compared the challenges of Text-to-CQL with the challenges of Text-to-SQL, which the paper 'b8' is about. \n- 'b9': 'seq sql generating structured queries from natural language using reinforcement learning' - This paper is also directly mentioned in the text as the primary dataset used in single-table Text-to-SQL research which the author's work is inspired by.\n- 'b3': 'er sql learning enhanced representation for text to sql using table contents' - This paper is mentioned as a typical solution for single-table Text-to-SQL tasks, which is the inspiration for the author's Text-to-CQL task.\n- 'b4': 'x sql reinforce schema representation with context corr abs' - This paper is mentioned as another typical solution for single-table Text-to-SQL tasks, which is the inspiration for the author's Text-to-CQL task.\n- 'b7': 'rat sql relation aware schema encoding and linking for text to sql parsers' - This paper is mentioned as a solution for multi-table Text-to-SQL tasks, which is another inspiration for the author's Text-to-CQL task.\n- 'b0': 'lgesql line graph enhanced text to sql model with mixed local and non local relations' - This paper is mentioned as another solution for multi-table Text-to-SQL tasks, which is another inspiration for the author's Text-to-CQL task.\n\nThe confidence score for each source paper is based on the frequency and significance of its mention within the text. The confidence score ranges from 0 (low confidence) to 1 (high confidence). \n\nThe source papers and their confidence scores are:\n\n- 'b8': 0.9\n- 'b9': 0.8\n- 'b3': 0.7\n- 'b4': 0.7\n- 'b7': 0.65\n- 'b0': 0.65\n\nThe results in JSON format would be:\n```\n{\n  \"b8\": 0.9,\n  \"b9\": 0.8,\n  \"b3\": 0.7,\n  \"b4\": 0.7,\n  \"b7\": 0.65,\n  \"b0\": 0.65\n}\n```", "Based on the provided text, the key phrases and the titles of references, the source papers are:\n\n1. 'lgesql line graph enhanced text to sql model with mixed local and non local relations' (b0): This paper is referred as a representative example of Seq2Seq framework, which has been used for solving complex query scenarios in Text-to-SQL, a task very similar to Text-to-CQL proposed by the author. \n   \n2. 'er sql learning enhanced representation for text to sql using table contents' (b3): It is mentioned as a typical solution for Text-to-SQL, which is the most similar work to Text-to-CQL. Therefore, it is likely that the author drew inspirations from this paper.\n\n3. 'x sql reinforce schema representation with context corr abs' (b4): The author discusses this paper as a sketch-based approach for single-table Text-to-SQL task, which might have some influence on their Text-to-CQL task, especially when they were defining their task and building the dataset.\n\n4. 'rat sql relation aware schema encoding and linking for text to sql parsers' (b7): It is used as an example of Seq2Seq framework, which could be a methodology reference for the author's task.\n\n5. 'spider a large scale human labeled dataset for complex and cross domain semantic parsing and text to sql task' (b8): This paper is referred as it provides a direction for mainstream research in complex query scenarios, which inspired the creation of Text-to-CQL.\n\n6. 'seq sql generating structured queries from natural language using reinforcement learning' (b9): This paper could be a source as it is a previous work on Text-to-SQL, the most similar task to the author's Text-to-CQL, and it might have some influence on the author's work.\n\nHence, the result is:\n{\n'b0': 0.9,\n'b3': 0.9,\n'b4': 0.8,\n'b7': 0.9,\n'b8': 1.0,\n'b9': 0.8\n}"], "63eef09c90e50fcafda0d2a1": ["{\n    \"b24\": 0.9,\n    \"b33\": 0.8,\n    \"b27\": 0.8,\n    \"b11\": 0.7,\n    \"b16\": 0.9,\n    \"b17\": 0.9,\n    \"b3\": 0.9\n}", "{\n  \"b24\": 0.9,\n  \"b33\": 0.85,\n  \"b27\": 0.8,\n  \"b3\": 0.95,\n  \"b16\": 0.9,\n  \"b17\": 0.9\n}", "{\n  \"b24\": 0.9,\n  \"b33\": 0.9,\n  \"b27\": 0.85,\n  \"b3\": 0.8,\n  \"b14\": 0.75\n}", "{\n    \"b24\": 0.95,\n    \"b33\": 0.95,\n    \"b27\": 0.9,\n    \"b11\": 0.9,\n    \"b16\": 0.85,\n    \"b17\": 0.85,\n    \"b3\": 0.8\n}", "{\n    \"b24\": 0.9,\n    \"b33\": 0.9,\n    \"b27\": 0.8,\n    \"b3\": 0.85,\n    \"b16\": 0.8,\n    \"b17\": 0.8\n}"], "63eef09c90e50fcafda0cfa0": ["{\n  \"b38\": 0.9,\n  \"b3\": 0.85,\n  \"b14\": 0.75,\n  \"b15\": 0.75,\n  \"b29\": 0.75,\n  \"b12\": 0.7\n}", "{\n    \"b38\": 0.95,\n    \"b3\": 0.9,\n    \"b14\": 0.85,\n    \"b15\": 0.85,\n    \"b29\": 0.85\n}", "```json\n{\n    \"b38\": 0.95,\n    \"b3\": 0.9,\n    \"b14\": 0.85,\n    \"b15\": 0.85,\n    \"b29\": 0.85,\n    \"b12\": 0.8,\n    \"b56\": 0.75\n}\n```", "{\n    \"b38\": 0.95,\n    \"b3\": 0.9,\n    \"b29\": 0.85,\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b56\": 0.75,\n    \"b16\": 0.7\n}", "{\n    \"b38\": 0.95,\n    \"b3\": 0.9,\n    \"b14\": 0.85,\n    \"b15\": 0.85,\n    \"b29\": 0.85,\n    \"b56\": 0.8,\n    \"b12\": 0.75,\n    \"b53\": 0.75\n}"], "62620f1c5aee126c0f686d0d": ["Based on the text of the academic paper and the titles of the references, the source papers that most inspire the author could be:\n\n{\"b14\": 0.9, \"b19\": 0.8, \"b48\": 0.8, \"b26\": 0.7, \"b16\": 0.7, \"b29\": 0.6}\n\nExplanation:\n- \"b14\" (\"Graph random neural networks for semi-supervised learning on graphs\") and \"b19\" (\"Improving neural networks by preventing co-adaptation of feature detectors\") are mentioned frequently in the paper, especially in the context of being influential in the development of the author's own algorithms.\n- \"b48\" (\"DropEdge: Towards deep graph convolutional networks on node classification\") is also mentioned as a key influence in the development of the author's \"DropMessage\" method.\n- \"b26\" (\"Semi-supervised classification with graph convolutional networks\") and \"b16\" (\"Neural message passing for quantum chemistry\") are discussed in the context of GNNs and their use in various tasks. These papers could have inspired the author's work on GNNs.\n- \"b29\" (\"Gated graph sequence neural networks\") is referenced in the paper as a key contribution to the field of GNNs, which may have informed the author's own work.\n\nThe confidence scores are assigned based on how directly the papers seem to contribute to the main contributions of the author's paper. High scores are given to papers that are discussed in the context of the author's own methods and algorithms, indicating a strong influence. Lower scores are given to papers that are discussed more generally, indicating a less direct but still significant influence.", "Based on the text provided, the following papers are likely to have inspired the author's work:\n\n1. \"Graph random neural networks for semi supervised learning on graphs\" [b14]\n2. \"DropEdge: Towards deep graph convolutional networks on node classification\" [b48]\n3. \"Graph attention networks\" [b53]\n4. \"Graph convolutional neural networks for web scale recommender systems\" [b64]\n5. \"Dropout: A simple way to prevent neural networks from overfitting\" [b19]\n\nThe paper mentions these works directly and discusses the concept of \"random dropping methods\" which are central to the author's contribution of DropMessage. Also, the papers [b14], [b48], [b53], [b64] and [b19] are related to Graph Neural Networks (GNNs) and their training, which is the main topic of the given paper. The paper [b19] is related to Dropout, a technique used by the author in the paper.\n\nThe paper [b19] is the earliest work among selected papers, so it probably had the most influence on the author.\n\nThus, I would return the following result:\n\n{\"b14\": 0.9, \"b48\": 0.9, \"b53\": 0.9, \"b64\": 0.9, \"b19\": 1.0}", "Based on the provided text, the most pertinent source papers are likely to be:\n\n- \"DropEdge: Towards Deep Graph Convolutional Networks on Node Classification\" [b48]: The paper repeatedly mentions random dropping methods, and DropEdge is mentioned as one of these methods. The challenges regarding overfitting, over-smoothing, and non-robustness that this paper seeks to address are also mentioned in the provided text. \n\n- \"Graph Random Neural Networks for Semi-Supervised Learning on Graphs\" [b14]: This is another paper mentioned in the context of random dropping methods, a key part of the proposed solution in the text. \n\n- \"Semi-Supervised Classification with Graph Convolutional Networks\" [b26]: This paper is frequently referred to throughout the text and seems to be particularly related to the downstream tasks the paper seeks to address.\n\n- \"Neural Message Passing for Quantum Chemistry\" [b16]: This paper is referred to in the context of the message-passing schema adopted by GNNs, a key part of the algorithm proposed in the text.\n\n- \"Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors\" [b19]: This paper is mentioned multiple times in the context of random dropping methods, and its authors are also credited with introducing Dropout, a method the text discusses extensively.\n\n- \"Spectral Networks and Locally Connected Networks on Graphs\" [b6], \"Graph Attention Networks\" [b53], and \"Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\" [b12]: These papers are all cited in the context of GNNs, the subject matter of the text, and could be important sources.\n\nSo, the final result should be:\n```json\n{\"b48\": 0.9, \"b14\": 0.9, \"b26\": 0.8, \"b16\": 0.8, \"b19\": 0.8, \"b6\": 0.7, \"b53\": 0.7, \"b12\": 0.7}\n```\nThe confidence scores are subjectively assigned based on the frequency and context of the citations in the text. The papers [b48] and [b14] are given the highest scores due to their direct relevance to the main topic of random dropping methods. The score gradually decreases when the correlation is less direct. Please note, this is just an educated guess based on the limited information given and the actual source papers could be different.", "Based on the text of the paper, the main contribution of the authors is the development of a novel method called \"DropMessage\". They propose this method to address the challenges of overfitting, oversmoothing and non-robustness in Graph Neural Networks (GNNs). DropMessage is designed to perform dropping operations on the message matrix, allowing the same node to propagate different messages to its different neighbors. Furthermore, the authors unify existing dropping methods into their framework and provide a theoretical analysis of the effectiveness of these methods.\n\nBased on these insights and considering the \"inspiration\" phrases in the text, the most relevant source papers are: \n- Dropout [b19]: This paper is mentioned multiple times and it's clear that the authors were inspired by this work as they compare their method with Dropout, indicating that they have drawn heavily from it.\n- DropEdge [b48]: This paper is also repeatedly cited and compared with the new method. The authors discuss the limitations of this method and propose their method as an improvement.\n- DropNode [b14]: This paper is likewise cited multiple times and compared with the new method. It appears to be one of the major inspirations for the development of DropMessage.\n- Graph Neural Networks [b53]: This paper is referred to when discussing the context of the problem being addressed and the existing state of the art. It seems to have provided the foundational understanding for the authors.\n- Random dropping methods [b54]: This paper is cited in the theoretical explanation of the effectiveness of random dropping methods. It seems to have provided key theoretical insights that contributed to the development of the new method.\n- Graph Convolutional Networks [b26]: This paper is cited when discussing the application of GNNs on various downstream tasks. It seems to have inspired the authors in terms of application scenarios of their new method.\n\nIn terms of confidence scores, I would estimate:\n- b19: 1.0\n- b48: 0.9\n- b14: 0.9\n- b53: 0.8\n- b54: 0.85\n- b26: 0.75\n\nThese are the papers that the authors seem to have relied on most heavily in developing their method. However, the exact confidence scores could be different depending on a more detailed analysis of the paper. \n\nThe JSON format of the result is:\n{\n\"b19\": 1.0,\n\"b48\": 0.9,\n\"b14\": 0.9,\n\"b53\": 0.8,\n\"b54\": 0.85,\n\"b26\": 0.75\n}", "Based on the text of the paper and the provided titles of references, the most relevant source papers that inspired this work are likely:\n\n1. \"Dropout: a simple way to prevent neural networks from overfitting\" [b52]: The paper frequently discusses the concept of random dropping, which is a strategy that was first introduced in this paper.\n\n2. \"DropEdge: towards deep graph convolutional networks on node classification\" [b48]: This is another key paper that introduced a form of random dropping in the context of graph neural networks (GNNs).\n\n3. \"Graph random neural networks for semi-supervised learning on graphs\" [b14]: This paper is also frequently mentioned in the context of random dropping methods for GNNs.\n\n4. \"Graph attention networks\" [b53]: This paper is mentioned in the context of GNNs and may have influenced the design of the proposed algorithm.\n\n5. \"Graph convolutional neural networks for web scale recommender systems\" [b64]: This paper could have provided inspiration in terms of the practical applications of GNNs.\n\n6. \"Pairnorm: tackling oversmoothing in GNNs\" [b66]: This paper discusses the problem of oversmoothing in GNNs, which is a challenge that this paper also addresses.\n\nThe confidence score for all these papers would be high (e.g., 0.8 - 1.0) as they are directly related to the methods and challenges discussed in the text.\n\nThe JSON format of the result would be:\n\n{\n\"b52\": 1,\n\"b48\": 0.9,\n\"b14\": 0.9,\n\"b53\": 0.8,\n\"b64\": 0.8,\n\"b66\": 0.8\n}"], "63a413f690e50fcafd6d1a7a": ["Based on the text, the most pertinent source papers that inspired the author are:\n\n1. \"SimGNN a neural network approach to fast graph similarity computation\" [b0]: The author's work is directly based on the GNN encoder and matching model approach outlined in this paper. The author also uses this paper as a contrast to their novel method, indicating a strong reliance on this source. Confidence Score: 0.9.\n\n2. \"Learning based efficient graph similarity computation via multi scale convolutional set matching\" [b2]: The author states that their work is a continuation of the state-of-the-art GNN-based GSC models, which includes this paper. Confidence Score: 0.8.\n\n3. \"Graph Matching Networks for learning the similarity of graph structured objects\" [b15]: This paper is also listed as one of the state-of-the-art GNN-based GSC models that the author builds upon. Confidence Score: 0.8.\n\n4. \"Slow learning and fast inference efficient graph similarity computation via knowledge distillation\" [b21]: The author addresses the limitations of the EGSC approach proposed in this paper and seeks to improve upon it, indicating a strong reliance on this source. Confidence Score: 0.9.\n\n5. \"A graph distance metric based on the maximum common subgraph\" [b5]: The author refers to the GED metric, which is a key component of their novel method. This paper is cited as a source that provides a popular graph similarity measure, making it a pertinent source. Confidence Score: 0.8.\n\n6. \"A Neural Network Approach to Fast Graph Similarity Computation\" [b0] and \"Interpretable Graph Similarity Computation via Differentiable Optimal Alignment of Node Embeddings\" [b7]: These papers are referred to as state-of-the-art GNN-based GSC models that contain two sequential submodules, which the author's work is based on. Confidence Score: 0.8.\n\n7. \"How powerful are graph neural networks\" [b31]: This paper is cited as a source that provides the expressive power in distinguishing graph structures, which seems to be a fundamental concept in the author's work. Confidence Score: 0.8.\n\nBased on the above analysis, the source papers in JSON format are:\n\n{\n\"b0\": 0.9,\n\"b2\": 0.8,\n\"b15\": 0.8,\n\"b21\": 0.9,\n\"b5\": 0.8,\n\"b7\": 0.8,\n\"b31\": 0.8\n}", "Based on the given text and the notes, the source papers that inspired the author the most seem to be:\n\n1. 'efficient graph similarity computation with alignment regularization' \n2. 'simgnn a neural network approach to fast graph similarity computation'\n3. 'learning based efficient graph similarity computation via multi scale convolutional set matching'\n4. 'interpretable graph similarity computation via differentiable optimal alignment of node embeddings'\n5. 'graph matching networks for learning the similarity of graph structured objects'\n6. 'multilevel graph matching networks for deep graph similarity learning'\n7. 'slow learning and fast inference efficient graph similarity computation via knowledge distillation'\n\nThe text of the paper mentions methods such as GNN encoder, alignment regularization, and multi-scale GED discriminator, which seem to be directly inspired by the papers mentioned above. These papers also appear to address the challenges outlined in the paper and propose similar algorithms.\n\nThe final JSON format with reference number and confidence score might look like this:\n\n{\n\"b0\": 0.9,\n\"b2\": 0.85,\n\"b7\": 0.85,\n\"b15\": 0.8,\n\"b17\": 0.75,\n\"b21\": 0.75,\n\"None\": 1\n}", "{\n\"b0\": 0.9,\n\"b2\": 0.85,\n\"b7\": 0.7,\n\"b15\": 0.7,\n\"b17\": 0.75,\n\"b21\": 0.8,\n\"b31\": 0.65\n}", "Based on the text of the academic paper and the titles of the references, the source papers that may have inspired this work the most could be:\n\n1. \"simgnn a neural network approach to fast graph similarity computation\" [b0]: The paper mentions this work when discussing the existing GNN-based GSC models. It's likely that this work inspired the current paper as it represents a state-of-the-art approach in the field.\n\n2. \"learning based efficient graph similarity computation via multi scale convolutional set matching\" [b2]: This paper is also mentioned in the context of GNN-based GSC models. It might have motivated the authors to develop their model.\n\n3. \"graph matching networks for learning the similarity of graph structured objects\" [b15]: This paper is referred to when discussing the matching model in GNN-based GSC models. It might have inspired the authors in the development of their matching model.\n\n4. \"efficient graph similarity computation with alignment regularization\" [b1]: Although this paper is not directly mentioned in the text, the title suggests a strong connection to the current paper's topic, specifically in relation to the alignment regularization approach used.\n\n5. \"slow learning and fast inference efficient graph similarity computation via knowledge distillation\" [b21]: This work is referred to when discussing the recent approach to speed up similarity learning. It might have inspired the authors to improve on this approach.\n\n6. \"combinatorial learning of graph edit distance via dynamic embedding\" [b29]: This paper is not directly mentioned but the title suggests a strong connection to the current paper's topic, specifically in regard to graph edit distance.\n\nHere is the json format:\n{\n    \"b0\": 0.9,\n    \"b2\": 0.8,\n    \"b15\": 0.9,\n    \"b1\": 0.7,\n    \"b21\": 0.9,\n    \"b29\": 0.8\n}\nPlease note that the confidence scores are subjective and based on the information provided in the text of the academic paper and the titles of the references.", "Based on the text, the source papers that inspire the given paper most are likely:\n\n1. \"SimGNN a neural network approach to fast graph similarity computation\" [b0]\n2. \"Learning based efficient graph similarity computation via multi scale convolutional set matching\" [b2]\n3. \"Interpretable graph similarity computation via differentiable optimal alignment of node embeddings\" [b7]\n4. \"Graph matching networks for learning the similarity of graph structured objects\" [b15]\n5. \"Multilevel graph matching networks for deep graph similarity learning\" [b17]\n6. \"Slow learning and fast inference efficient graph similarity computation via knowledge distillation\" [b21]\n\nThe paper proposes a separated neural structure in order to balance between accuracy and speed, and the authors were motivated to design a task-agnostic matching model, which is reflected in [b2], [b7], [b15], [b17] and [b21]. The GNN-based GSC models, the major methods this paper uses, are clearly inspired by [b0].\n\n{\n\"b0\": 1,\n\"b2\": 0.9,\n\"b7\": 0.9,\n\"b15\": 0.9,\n\"b17\": 0.9,\n\"b21\": 1\n}"], "637c3dcf90e50fcafd77c35b": ["{\n    \"b60\": 0.95,\n    \"b28\": 0.9,\n    \"b48\": 0.9,\n    \"b30\": 0.85,\n    \"b63\": 0.8,\n    \"b91\": 0.75\n}", "{\n  \"b60\": 1.0,\n  \"b28\": 0.9,\n  \"b48\": 0.9,\n  \"b91\": 0.95\n}", "{\n  \"b60\": 0.95,\n  \"b28\": 0.9,\n  \"b48\": 0.9,\n  \"b63\": 0.85,\n  \"b46\": 0.75,\n  \"b30\": 0.7,\n  \"b91\": 0.7\n}", "{\n  \"b60\": 0.95,\n  \"b28\": 0.9,\n  \"b48\": 0.9,\n  \"b91\": 0.85\n}", "{\n  \"b60\": 1.0,\n  \"b28\": 0.9,\n  \"b48\": 0.9,\n  \"b91\": 0.95\n}"], "634d805b90e50fcafd4e1168": ["{\n  \"b0\": 0.9,\n  \"b31\": 0.9,\n  \"b51\": 0.9,\n  \"b49\": 0.85,\n  \"b52\": 0.85,\n  \"b26\": 0.8,\n  \"b33\": 0.8\n}", "{\n  \"b0\": 0.9,\n  \"b6\": 0.8,\n  \"b13\": 0.8,\n  \"b31\": 0.8,\n  \"b33\": 0.8,\n  \"b49\": 0.8,\n  \"b51\": 0.8,\n  \"b52\": 0.8\n}", "{\n  \"b0\": 0.9,\n  \"b31\": 0.8,\n  \"b51\": 0.8,\n  \"b26\": 0.7,\n  \"b49\": 0.7\n}", "{\n  \"b0\": 0.9,\n  \"b6\": 0.8,\n  \"b13\": 0.8,\n  \"b31\": 0.85,\n  \"b33\": 0.85,\n  \"b49\": 0.8,\n  \"b51\": 0.9,\n  \"b52\": 0.85\n}", "{\n  \"b0\": 0.8,\n  \"b31\": 0.9,\n  \"b51\": 0.9,\n  \"b49\": 0.8\n}"], "62d4cf305aee126c0fa9ab5a": ["{\n    \"b22\": 0.9,\n    \"b7\": 0.85,\n    \"b12\": 0.9\n}", "{\n  \"b22\": 0.9,\n  \"b7\": 0.9,\n  \"b12\": 0.85\n}", "{\n  \"b7\": 0.9,\n  \"b22\": 0.9,\n  \"b12\": 0.85\n}", "{\n    \"b2\": 0.95,\n    \"b7\": 0.90,\n    \"b12\": 0.90,\n    \"b22\": 0.90,\n    \"b24\": 0.85\n}", "{\n    \"b22\": 0.9,\n    \"b12\": 0.9,\n    \"b7\": 0.9\n}"], "63dcdb422c26941cf00b6094": ["{\n  \"b18\": 0.95,\n  \"b5\": 0.85,\n  \"b30\": 0.85,\n  \"b13\": 0.7,\n  \"b35\": 0.7,\n  \"b3\": 0.65,\n  \"b26\": 0.6\n}", "{\n    \"b18\": 0.9,\n    \"b30\": 0.85,\n    \"b5\": 0.75,\n    \"b13\": 0.7,\n    \"b35\": 0.7\n}", "{\n    \"b18\": 0.9,\n    \"b30\": 0.9,\n    \"b5\": 0.8,\n    \"b13\": 0.7,\n    \"b35\": 0.7,\n    \"b3\": 0.6\n}", "{\n    \"b18\": 0.9,\n    \"b30\": 0.85,\n    \"b5\": 0.8,\n    \"b13\": 0.75,\n    \"b35\": 0.75,\n    \"b19\": 0.7,\n    \"b29\": 0.7\n}", "{\n    \"b18\": 0.95,\n    \"b5\": 0.90,\n    \"b30\": 0.90,\n    \"b13\": 0.85,\n    \"b35\": 0.85\n}"], "633cf5cf90e50fcafd772e24": ["{\n  \"b5\": 0.9,\n  \"b18\": 0.8,\n  \"b23\": 0.7,\n  \"b20\": 0.7,\n  \"b58\": 0.7,\n  \"b3\": 0.8\n}", "{\n    \"b3\": 0.9,\n    \"b5\": 0.75,\n    \"b18\": 0.85,\n    \"b23\": 0.8,\n    \"b20\": 0.8,\n    \"b58\": 0.8,\n    \"b41\": 0.75\n}", "{\n  \"b3\": 0.9,\n  \"b18\": 0.9,\n  \"b5\": 0.9,\n  \"b23\": 0.8,\n  \"b20\": 0.8,\n  \"b58\": 0.8\n}", "{\n    \"b3\": 0.95,\n    \"b5\": 0.9,\n    \"b18\": 0.85,\n    \"b23\": 0.9,\n    \"b41\": 0.8\n}", "{\n    \"b5\": 0.8,\n    \"b18\": 0.9,\n    \"b23\": 0.7,\n    \"b41\": 0.75\n}"], "635b486790e50fcafd32f8b1": ["Based on the text, the source papers for this academic work can be identified as following:\n\n- \"Towards Debiasing Sentence Representations\" [b38]: This paper is referred to multiple times, especially when discussing the novel method the authors propose for attenuating gender bias, MABEL, which is compared to Sent-Debias. The authors also mention Sent-Debias as one of the primary baselines they compare to in their evaluations.\n- \"FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders\" [b10]: This paper is referenced multiple times, especially when discussing MABEL's contrastive learning framework. The authors consider FairFil as a significant baseline and mention it as one of the methods most similar to MABEL.\n- \"Debiasing Pre-Trained Contextualized Embeddings\" [b34]: This paper is referenced as another primary baseline in the authors' evaluations and is mentioned in the discussion of task-agnostic approaches. \n- \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b20]: This paper is referenced when discussing the use of a masked language modeling objective in MABEL.\n- \"SIMCSE: Simple Contrastive Learning of Sentence Embeddings\" [b23]: This paper is referenced when discussing the entailment-based contrastive loss part of their training method, which was inspired by this work.\n- \"On Measuring and Mitigating Biased Inferences of Word Embeddings\" [b17]: This paper is referenced when discussing methods for evaluating biases and is mentioned in the discussion of task-agnostic approaches.\n- \"Measuring and Reducing Gendered Correlations in Pre\"[b62]: This paper is referenced when discussing counterfactual data augmentation.\n\nThe confidence scores for these source papers are as following in json format:\n\n{\n\"b38\": 0.9,\n\"b10\": 0.9,\n\"b34\": 0.8,\n\"b20\": 0.7,\n\"b23\": 0.7,\n\"b17\": 0.6,\n\"b62\": 0.6\n}\n\nThese confidence scores are based on the frequency of reference, the relevance to the novel contributions of the paper, and the methodological similarity to the paper.", "Based on the text and the titles of the references, the most pertinent source papers that inspired the given paper are:\n\n- \"measuring and reducing gendered correlations in pre\" [b62]: This paper seems to have inspired the approach of using counterfactual data augmentation on the entailment pairs. The given paper also uses similar data augmentation technique by swapping gender words in both premise and hypothesis sentences.\n  \n- \"debiasing pre trained contextualised embeddings\" [b34]: This paper appears to have inspired the given paper's method of debiasing pre-trained language models. The given paper also follows a similar approach by proposing a method to attenuate gender bias in pre-trained language models.\n  \n- \"simcse simple contrastive learning of sentence embeddings\" [b23]: This paper seems to have inspired the given paper's method of using a contrastive loss for training. The given paper uses a similar contrastive loss that encourages the inter-association of entailment pairs.\n  \n- \"on measuring social biases in sentence encoders\" [b40]: This paper seems to have inspired the given paper's method of measuring bias. The given paper discusses the limitations of the SEAT metric used in this paper, and proposes a different method for bias evaluation.\n  \n- \"fairfil contrastive neural debiasing method for pretrained text encoders\" [b10]: This paper seems to have inspired the given paper's debiasing method. Like the given paper, it also uses a contrastive learning approach for debiasing.\n  \n- \"a large annotated corpus for learning natural language inference\" [b6]: This paper seems to have provided the NLI dataset used in the given paper for extracting entailment pairs.\n\nThe confidence score for each reference is estimated based on the frequency and context of their citation in the text.\n\nBased on this analysis, the JSON format result can be:\n\n{\n  \"b62\": 0.85,\n  \"b34\": 0.8,\n  \"b23\": 0.75,\n  \"b40\": 0.7,\n  \"b10\": 0.7,\n  \"b6\": 0.65\n}", "Based on the academic paper's text, the main challenge outlined is the issue of gender bias in pre-trained language models. The authors propose a solution, MABEL, which uses entailment pairs from supervised natural language inference datasets to attenuate the gender bias. \n\nLooking at the text, several key phrases indicate source papers that the authors drew inspiration from:\n\n1. \"Inspired by [b23]\" in the section \"Training Objective\", suggests the authors were inspired by the work of [b23] when developing the contrastive loss function for MABEL.\n\n2. In the section \"Baselines & Implementation Details\", the authors mention [b38], [b34] and [b10] as primary baselines for their work, indicating these papers might have contributed to the development of MABEL.\n\n3. The authors also mention [b20] when discussing the optional masked language modeling loss, suggesting influence from this work.\n\n4. The phrase \"Following [b20]\" appears in the section \"masked language modeling loss\", indicating that the authors adopted the approach of [b20] to develop this component of their algorithm.\n\n5. The authors mention [b62] when discussing the pre-processing step of their data augmentation, suggesting that this paper influenced their methodology.\n\nBased on these key phrases and the context in which they appear, the following source papers are likely to have directly inspired the authors' work:\n\n1. [b23] - 'simcse simple contrastive learning of sentence embeddings'\n2. [b38] - 'towards debiasing sentence representations'\n3. [b34] - 'debiasing pre trained contextualised embeddings'\n4. [b10] - 'fairfil contrastive neural debiasing method for pretrained text encoders'\n5. [b20] - 'bert pre training of deep bidirectional transformers for language understanding'\n6. [b62] - 'measuring and reducing gendered correlations in pre'\n\nIn JSON format, the result could be represented as follows:\n\n{\"b23\": 1, \"b38\": 0.8, \"b34\": 0.8, \"b10\": 0.8, \"b20\": 0.9, \"b62\": 0.7}\n", "Based on the text and the titles of the references, the most pertinent source papers seem to be:\n\nb20: \"bert pre training of deep bidirectional transformers for language understanding\" - The paper frequently mentions BERT, which indicates that it has significantly influenced their work. The authors also describe the use of a masked language modeling loss, which is a key component of the BERT model.\n\nb23: \"simcse simple contrastive learning of sentence embeddings\" - This paper is directly mentioned as an inspiration for the contrastive loss used in the paper.\n\nb6: \"a large annotated corpus for learning natural language inference\" - The paper mentions using entailment pairs from NLI datasets, and this paper is about a large dataset for learning natural language inference.\n\nb38: \"towards debiasing sentence representations\" - The paper mentions the use of Sent-Debias as a baseline, indicating that this work was a significant comparison point.\n\nb10: \"fairfil contrastive neural debiasing method for pretrained text encoders\" - FairFil is mentioned as a relevant task-agnostic, contrastive learning approach which is similar to MABEL.\n\nb62: \"measuring and reducing gendered correlations in pre\" - The paper uses counterfactual data augmentation mentioned in this paper. \n\nThe confidence scores are estimated based on the frequency and context of the mentions.\n\n{\n\"b20\": 0.9,\n\"b23\": 0.8,\n\"b6\": 0.8,\n\"b38\": 0.7,\n\"b10\": 0.7,\n\"b62\": 0.7\n}", "Based on the text, the most pertinent source papers inspiring this paper could be:\n\n- b20: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: This is the backbone model used in the paper and is mentioned multiple times throughout.\n- b23: SimCSE: Simple Contrastive Learning of Sentence Embeddings: The paper's contrastive learning approach is inspired by this work.\n- b38: Towards Debiasing Sentence Representations: This work is mentioned as a baseline and seems to have a similar aim. \n- b34: Debiasing Pre-trained Contextualised Embeddings: This paper appears to have a similar goal and is also used as a baseline.\n- b10: FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders: The paper's task-agnostic, contrastive learning approach is similar to the one used in this work.\n- b6: A Large Annotated Corpus for Learning Natural Language Inference: This paper is used for the NLI data, which is a key part of the proposed approach.\n- b62: Measuring and Reducing Gendered Correlations in Pre-trained Language Models: This paper's method of counterfactual data augmentation is used in the approach.\n\nThe confidence score for each of these papers would be:\n\n{ \"b20\": 0.9, \"b23\": 0.9, \"b38\": 0.8, \"b34\": 0.8, \"b10\": 0.8, \"b6\": 0.8, \"b62\": 0.9 }"], "628749485aee126c0fff0290": ["Based on the provided text and instructions, we first identify the novel contributions of the paper and then look for the references which seem to be most directly related to these contributions. The novel contributions are:\n\n1. A method for mitigating biases in pretrained language models (PLMs) without using external corpora.\n2. An approach that uses cloze-style prompts to identify biases within PLMs.\n3. An algorithm (Auto-Debias) for automatically generating biased prompts.\n4. A distribution alignment loss function (equalizing loss) for fine-tuning the language model to minimize the bias.\n\nThe references most directly related to these novel contributions are those mentioned immediately after key phrases indicating inspiration, methodological foundations, or direct comparison. From the paper, we can identify the following key references:\n\n- [b29] and [b35] are mentioned in the context of being motivated by previous work on language model prompting and assumptions about fair NLP systems.\n- [b13], [b25], and [b20] are referenced as previous efforts to debias PLMs using external corpora, which the authors aim to improve upon.\n- [b15] and [b33] are mentioned in relation to adversarial training, which relates to the methodology of the proposed Auto-Debias technique.\n\nGiven the emphasis on not using external corpora and the method of using prompts to identify biases, as well as the proposed algorithm and loss function, the most central references appear to be [b29], [b35], [b13], [b25], [b20], [b15], and [b33].\n\nNow, we will provide a JSON format output with the reference number as the key and a confidence score between 0 and 1 based on the direct relevance to the paper's novel contributions:\n\n```json\n{\n  \"b29\": 0.9,\n  \"b35\": 0.9,\n  \"b13\": 0.8,\n  \"b25\": 0.8,\n  \"b20\": 0.8,\n  \"b15\": 0.7,\n  \"b33\": 0.7\n}\n```\n\nThe confidence scores are subjective estimates based on the context in which each reference is mentioned within the paper. References [b29] and [b35] are given the highest scores due to their direct mention in the motivation for the novel approach. The rest are scored slightly lower as they inform the methodology but are not the sole focus of the paper's contributions.", "Based on the provided text and criteria for identifying source papers that directly inspired the main contributions of the paper, we can infer the following:\n\n1. The novel contribution of the paper is the debiasing method \"Auto-Debias,\" which uses automated biased prompts to identify and mitigate biases in pretrained language models (PLMs) without relying on external corpora.\n2. The paper specifically mentions motivation from previous work on language model prompting to probe biases in PLMs (e.g., [b34], [b29], [b32]).\n3. Key phrases indicating direct inspiration are \"Motivated by this,\" referring to the use of cloze-style prompts to probe and identify biases.\n4. The paper addresses the challenges of identifying and mitigating biases in PLMs, particularly focusing on gender and racial biases, without the need for external corpora.\n\nGiven these points, we can extract the following references as likely sources of direct inspiration:\n\n- \"Language models as knowledge bases\" ([b34]): This paper might have inspired the authors to use language model prompting to probe the internal knowledge of language models, including biases.\n- \"On measuring social biases in sentence encoders\" ([b29]): This work is directly related to measuring biases, which is a central aspect of the paper's contribution.\n- \"Probing toxic content in large pre-trained language models\" ([b32]): This could be related to the probing method used by the authors to identify biases.\n- \"Perturbation sensitivity analysis to detect unintended model biases\" ([b35]): The authors mention an assumption from this paper regarding the independence of scores from the choice of identities, which is related to their debiasing objective.\n\nBelow are the identified references in JSON format with confidence scores based on the provided text and the criteria set by the user:\n\n```json\n{\n  \"b34\": 0.7,\n  \"b29\": 0.9,\n  \"b32\": 0.7,\n  \"b35\": 0.8\n}\n```\n\nPlease note that the confidence scores are subjective estimates based on the degree of relevance and the explicit mentions within the text. The scores reflect the likelihood that the reference is a primary source of inspiration for the paper's novel contributions. The confidence scores are on a scale from 0 to 1, with 1 being the most confident.", "Based on the provided text and the author's notes, we can infer that the paper introduces a novel debiasing technique for pre-trained language models (PLMs) named \"Auto-Debias\". This technique uses automated biased prompt generation followed by a distribution alignment loss function to mitigate biases without the need for an external corpus. The models focused on are BERT, ALBERT, and RoBERTa.\n\nThe paper mentions several key phrases indicating direct inspiration or adaptation from prior work:\n\n1. \"Motivated by\" the recent work in language model prompting to probe and identify biases in PLMs.\n2. \"Inspired by\" the assumption that a fair NLP system should produce scores independent of identity choices mentioned in the text ([b35]).\n3. The debiasing method \"resembles adversarial training\" ([b15], [b33]).\n4. The paper differentiates its approach from other debiasing strategies that rely on external corpora ([b13], [b25], [b20], [b24]).\n5. It also mentions comparative phrases such as \"Unlike [reference], we...\" and \"Improving upon [reference], our method...\" when discussing the differences between their method and existing techniques ([b13], [b25], [b7], [b20]).\n\nConsidering the criteria for selecting the most central references that heavily influenced the main contributions of the paper, we can identify the following source papers:\n\n- [b35]: Perturbation Sensitivity Analysis to Detect Unintended Model Biases\n- [b15]: Explaining and Harnessing Adversarial Examples\n- [b33]: Practical Black Box Attacks against Machine Learning\n- [b13]: He Is Very Intelligent, She Is Very Beautiful: On Mitigating Social Biases in Language Modeling and Generation\n- [b25]: Towards Debiasing Sentence Representations\n\nThese references are directly linked to the novel contributions of the paper and are mentioned in the context of inspiration, motivation, or differentiation from their approach.\n\nNow, we can provide a list of source papers in JSON format with a confidence score between 0 and 1:\n\n```json\n{\n  \"b35\": 0.9,\n  \"b15\": 0.8,\n  \"b33\": 0.8,\n  \"b13\": 0.9,\n  \"b25\": 0.9\n}\n```\n\nThe confidence scores reflect the direct relevance and influence these works have on the paper's novel contributions, based on the presence of key phrases indicating inspiration or adaptation. The highest scores are assigned to references that are explicitly mentioned as motivational or foundational to the paper's approach.", "```json\n{\n  \"b29\": 0.9,\n  \"b35\": 0.9,\n  \"b13\": 0.8,\n  \"b25\": 0.8,\n  \"b20\": 0.8,\n  \"b34\": 0.7\n}\n```\n\nExplanation:\n\n- **b29**: High confidence as the paper discusses probing biases in PLMs and uses cloze-style prompts, which is aligned with the method described in the provided paper text.\n\n- **b35**: High confidence because it talks about fairness in NLP systems, which is a key motivation for the paper's proposed method.\n\n- **b13**: High confidence due to the focus on mitigating social biases in language modeling and generation, which is similar to the debiasing challenges addressed.\n\n- **b25**: High confidence because it explores debiasing sentence representations, which relates to the paper's context of debiasing PLMs without external corpora.\n\n- **b20**: High confidence since it discusses debiasing pre-trained contextualized embeddings, directly relevant to the focus of the paper's novel contributions.\n\n- **b34**: Moderate confidence because it relates to the use of language models as knowledge bases, and while it's not directly about debiasing, the probing technique is relevant to the paper's approach. \n\nThese references are prioritized based on their methodological similarities to the paper in question and the explicit mentions of motivation or inspiration in the text. The confidence scores are subjective estimates based on how directly related the source papers seem to be to the novel contributions of the given paper.", "Based on the provided text and instructions, we need to identify the key references that inspired the novel contributions of the paper, specifically focusing on the aspects of bias identification and mitigation in pre-trained language models (PLMs) using automated biased prompts and distribution alignment loss function (Auto-Debias).\n\nHere are the steps according to the instructions:\n\n1. **Novel Contributions Identification**: The novel contributions of the paper are the introduction of an automated biased prompt generation method and a distribution alignment loss function to debias PLMs without the use of external corpora.\n\n2. **Methodological Similarity**: We prioritize references that also focus on debiasing PLMs and those that might have used similar methods such as automated prompt generation or distribution alignment.\n\n3. **Comparative Phrases**: The paper states, \"Motivated by this, instead of referring to any external corpus, we directly use cloze-style prompts...\" This indicates that the work builds upon and diverges from methods that use external corpora for debiasing.\n\nBased on these criteria, we can now infer the most influential references:\n\n```json\n{\n  \"b29\": 0.8, // Directly mentioned as motivation for probing biases in PLMs.\n  \"b35\": 0.8, // Mentioned regarding the assumption that a fair NLP system should produce scores independent of identity choices, which underpins the paper's methodology.\n  \"b13\": 0.7, // Related to debiasing methods and their limitations, which the paper aims to address.\n  \"b25\": 0.7, // Mentioned in the context of using external corpora for debiasing, which the paper's method does not require.\n  \"b20\": 0.6, // Related to fine-tuning PLMs for debiasing, similar to the novel contribution of the paper.\n  \"b34\": 0.5, // Related to probing language models, which is part of the method used in the paper.\n  \"b30\": 0.5  // Discusses the effectiveness of debiasing techniques, relevant to the paper's contributions.\n}\n```\n\nI have assigned confidence scores based on the direct mentions of motivation, the relevance of the reference to the novel contributions, and the methodological similarity. The scores are somewhat subjective but are guided by the criteria outlined in the instructions. The references selected are less than eight, in accordance with the task requirements."], "627cdc8e5aee126c0f50229e": ["{\n    \"b33\": 0.9,\n    \"b5\": 0.85,\n    \"b44\": 0.85,\n    \"b9\": 0.8,\n    \"b2\": 0.75,\n    \"b30\": 0.7,\n    \"b41\": 0.65,\n    \"b53\": 0.6\n}", "{\n  \"b33\": 0.8,\n  \"b44\": 0.9,\n  \"b5\": 0.75,\n  \"b31\": 0.75\n}", "{\n  \"b33\": 0.9,\n  \"b5\": 0.9,\n  \"b44\": 0.9,\n  \"b41\": 0.85,\n  \"b20\": 0.75,\n  \"b30\": 0.75,\n  \"b37\": 0.75,\n  \"b31\": 0.75\n}", "{\n  \"b9\": 0.9,\n  \"b33\": 0.9,\n  \"b5\": 0.8,\n  \"b31\": 0.8,\n  \"b44\": 0.8,\n  \"b41\": 0.85,\n  \"b0\": 0.75\n}", "{\n    \"b9\": 0.9,\n    \"b33\": 0.9,\n    \"b44\": 0.85,\n    \"b5\": 0.75,\n    \"b31\": 0.75,\n    \"b37\": 0.75,\n    \"b2\": 0.7\n}"], "62d16e895aee126c0fd68310": ["Based on the text provided, the source papers that inspired this paper are:\n\n1. \"AutoFDO: Automatic Feedback Directed Optimization for Warehouse Scale Applications\" [b22]: This paper is referenced multiple times to discuss the effectiveness of profile-guided optimizations, which this paper then applies to network code.\n\n2. \"Dynamic Compilation and Optimization of Packet Processing Programs\" [b80]: The current paper builds upon the idea of dynamic compilation proposed in this paper.\n\n3. \"Policy-driven optimizer for DPDK-based OpenFlow software switches\" [eSwitch] [b69]: This paper is mentioned as a comparison point in the results section, indicating that the current paper is building upon or challenging the work done in this source paper.\n\n4. \"Packet metadata management optimizer for DPDK-based software data planes\" [PacketMill] [b34]: Similar to [b69], this paper is used as a comparison in the results section, indicating its relevance.\n\n5. \"A Service Agnostic Software Framework for Fast and Efficient In-kernel Network Services\" [b66]: The paper discusses the need for tracking packet-level dynamics, which is a concept also mentioned in [b66].\n\n6. \"Decoupling Algorithms and Optimizations in Network Functions\" [b6]: This paper is referenced multiple times in the context of improving performance of packet-processing programs.\n\n7. \"Fast, Effective Dynamic Compilation\" [b7]: The paper discusses the benefits of dynamic compilation, a concept also explored in [b7].\n\nThe corresponding confidence scores would be:\n\n```json\n{\n    \"b22\": 0.9,\n    \"b80\": 0.9,\n    \"b69\": 0.8,\n    \"b34\": 0.8,\n    \"b66\": 0.8,\n    \"b6\": 0.9,\n    \"b7\": 0.8\n}\n```\n\nThese confidence scores are based on the frequency and context of the references in the text, with higher scores indicating a stronger influence on the research in the paper.", "Based on the text of the paper and the available references, the source papers that most likely inspired this paper are:\n\n1. \"AutoFDO: Automatic Feedback Directed Optimization for Warehouse Scale Applications\" [b22]: This paper is directly mentioned in the text, with the authors discussing its shortcomings and building upon it for their own approach. Confidence: 0.9\n\n2. \"Policy-driven Optimization of P4 Pipeline\" [b5]: This paper is mentioned in reference to policy-driven optimization, an approach that the authors also appear to use. Confidence: 0.8\n\n3. \"Fast, Effective Dynamic Compilation\" [b7]: This paper is mentioned in relation to dynamic compilation, a concept central to the paper. Confidence: 0.8\n\n4. \"Decoupling algorithms and optimizations in network functions\" [b6]: This paper is frequently mentioned throughout the text, suggesting it was a significant influence. Confidence: 0.9\n\n5. \"eSwitch: Policy-driven Optimizer for DPDK-based OpenFlow Software Switches\" [b69]: This paper is mentioned as a comparative work, which suggests that it has been a source of inspiration. Confidence: 0.8\n\n6. \"PacketMill: Toward Per-core Gbps Networking\" [b34]: This paper is directly compared to the author's work, suggesting that it was an influence. Confidence: 0.7\n\n7. \"Katran: A High-performance Layer Load Balancer\" [b44]: This paper is mentioned as a real-world application of the author's work, suggesting that it was a source of inspiration. Confidence: 0.7\n\n8. \"Optimizing Dynamically Dispatched Calls with Run-time Type Feedback\" [b43]: This paper is mentioned in relation to dynamic optimization, a concept central to the paper. Confidence: 0.8\n\nThe answer in json format:\n\n{\n    \"b22\": 0.9,\n    \"b5\": 0.8,\n    \"b7\": 0.8,\n    \"b6\": 0.9,\n    \"b69\": 0.8,\n    \"b34\": 0.7,\n    \"b44\": 0.7,\n    \"b43\": 0.8\n}", "Based on the text of the academic paper, the source papers that inspired the author the most can be identified as:\n\n1. \"AutoFDO: Automatic Feedback Directed Optimization for Warehouse Scale Applications\" [b22]\n2. \"Fast, Effective Dynamic Compilation\" [b7]\n3. \"Dynamic Compilation and Optimization of Packet Processing Programs\" [b80]\n4. \"Dynamo: A Transparent Dynamic Optimization System\" [b10]\n5. \"Unsupervised Dynamic Optimization for Network Code\" [b6]\n6. \"Policy-Driven Optimization of P4 Pipeline\" [b5]\n7. \"Data Plane Specialization for High Performance OpenFlow Software Switching\" [b69]\n8. \"PacketMill: Toward Per-Core Gbps Networking\" [b34]\n\nThese papers were frequently cited in the text and they are related to the key challenges and novel methods proposed by the author.\n\nHere is the result in the json format:\n\n{\n\"b22\": 0.9,\n\"b7\": 0.85,\n\"b80\": 0.9,\n\"b10\": 0.85,\n\"b6\": 0.8,\n\"b5\": 0.8,\n\"b69\": 0.8,\n\"b34\": 0.8\n}", "Based on the text of the paper, the strong reliance on previous research, and the novel methods mentioned by the author, the source papers that inspired the author most are:\n\n1. \"AutoFDO: Automatic Feedback Directed Optimization for Warehouse-Scale Applications\" [b22]\n2. \"Dynamic Compilation and Optimization of Packet Processing Programs\" [b80]\n3. \"Decoupling Algorithms and Optimizations in Network Functions\" [b6]\n4. \"Policy-Driven Optimization of P4 Pipeline\" [b5]\n5. \"PacketMill: Toward Per-Core Gbps Networking\" [b34]\n6. \"Fast, Effective Dynamic Compilation\" [b7]\n7. \"Katran: A High-Performance Layer Load Balancer\" [b44]\n\nHowever, this is a primary inference and may need to be verified by examining the full text in detail.\n\nThe JSON format of the source papers with confidence scores is:\n\n{\n  \"b22\": 0.9,\n  \"b80\": 0.85,\n  \"b6\": 0.8,\n  \"b5\": 0.8,\n  \"b34\": 0.8,\n  \"b7\": 0.75,\n  \"b44\": 0.75\n}\n\nThe confidence scores are estimated based on the frequency and context of the citation in the text.", "Based on the analysis of the paper and the provided references, the source papers that inspired this work the most could be:\n\n1. \"domain specific run time optimization for software data planes\" [None]\n2. \"fast effective dynamic compilation\" [b7]\n3. \"autofdo automatic feedback directed optimization for warehouse scale applications\" [b22]\n4. \"dynamic compilation and optimization of packet processing programs\" [b80]\n5. \"dataplane specialization for high performance openflow software switching\" [b69]\n6. \"the case for a flexible low level backend for software data planes\" [b25]\n7. \"llvm a compilation framework for lifelong program analysis and transformation\" [b57]\n8. \"policy driven optimization of p pipeline\" [b5]\n\nThe confidence scores for each paper could be as follows {None: 1, 'b7': 0.9, 'b22': 0.85, 'b80': 0.95, 'b69': 0.8, 'b25': 0.8, 'b57': 0.8, 'b5': 0.7}. These papers are either directly referenced in the text as influencing the development of the proposed algorithm or seem to have significant methodological overlap based on their titles."], "63dcdb422c26941cf00b604a": ["{\n  \"b21\": 0.9,\n  \"b52\": 0.8,\n  \"b50\": 0.8,\n  \"b22\": 0.7\n}", "{\n    \"b21\": 0.95,\n    \"b52\": 0.9,\n    \"b22\": 0.85,\n    \"b50\": 0.8\n}", "{\n  \"b21\": 0.95,\n  \"b52\": 0.9,\n  \"b50\": 0.85,\n  \"b22\": 0.8,\n  \"b58\": 0.75\n}", "{\n    \"b21\": 0.95,\n    \"b52\": 0.9,\n    \"b50\": 0.85,\n    \"b22\": 0.85,\n    \"b58\": 0.8\n}", "{\n    \"b21\": 0.95,\n    \"b52\": 0.85,\n    \"b50\": 0.80,\n    \"b22\": 0.75,\n    \"b58\": 0.75\n}"], "640fe64790e50fcafd9e237f": ["{\n  \"b7\": 0.95,\n  \"b12\": 0.95,\n  \"b13\": 0.9\n}", "{\n  \"b7\": 0.95,\n  \"b12\": 0.95,\n  \"b13\": 0.9\n}", "```json\n{\n  \"b7\": 0.9,\n  \"b12\": 0.9,\n  \"b13\": 0.8,\n  \"b29\": 0.9\n}\n```", "{\n  \"b7\": 0.95,\n  \"b12\": 0.95,\n  \"b13\": 0.9\n}", "{\n  \"b7\": 0.95,\n  \"b12\": 0.9,\n  \"b13\": 0.9\n}"], "628749355aee126c0ffec021": ["{\n    \"b17\": 0.9,\n    \"b45\": 0.9,\n    \"b3\": 0.85,\n    \"b62\": 0.8,\n    \"b41\": 0.8,\n    \"b28\": 0.8,\n    \"b9\": 0.75,\n    \"b43\": 0.75\n}", "{\n    \"b17\": 0.9,\n    \"b3\": 0.9,\n    \"b45\": 0.9,\n    \"b62\": 0.9,\n    \"b28\": 0.9,\n    \"b41\": 0.9,\n    \"b9\": 0.9,\n    \"b43\": 0.9\n}", "{\n    \"b17\": 0.95,\n    \"b3\": 0.9,\n    \"b45\": 0.9,\n    \"b62\": 0.85,\n    \"b28\": 0.85,\n    \"b41\": 0.85,\n    \"b9\": 0.8,\n    \"b43\": 0.8\n}", "{\n    \"b62\": 0.95,\n    \"b41\": 0.95,\n    \"b28\": 0.95,\n    \"b10\": 0.9,\n    \"b9\": 0.85,\n    \"b43\": 0.85\n}", "{\n    \"b17\": 0.9,\n    \"b45\": 0.9,\n    \"b3\": 0.8,\n    \"b62\": 0.9,\n    \"b41\": 0.9,\n    \"b28\": 0.9,\n    \"b9\": 0.9,\n    \"b43\": 0.9\n}"], "6287045b5aee126c0f5ca192": ["Based on the text, the following references seem to have directly influenced the main contributions of the paper:\n\n1. \"b8\": 'a graph partitioning algorithm for edge or vertex balance' - This paper has been mentioned multiple times throughout the text, suggesting that it might have been a significant source of inspiration. The paper appears to directly build upon the B-GRAP method presented in this source.\n\n2. \"b18\": 'spinner scalable graph partitioning in the cloud' - This paper is mentioned in the context of approaches that the authors consider and build upon.\n\n3. \"b24\": 'rank degree an efficient algorithm for graph sampling' - This paper is referred to when the authors discuss their approach to graph sampling in initialization, suggesting this methodology was influential in the development of their own method.\n\n4. \"b20\": 'near linear time algorithm to detect community structures in large scale networks' - This paper is referenced when discussing the label propagation approach, which is a key component of the algorithm proposed in the paper.\n\n5. \"b5\": 'recent advances in graph partitioning' - This paper is mentioned in the context of graph partitioning algorithms, indicating that the authors might have used it as a source of information on existing methods.\n\nThe confidence scores for each of these sources could be assigned based on the frequency and context of their mentions in the paper. \n\nInference in JSON format:\n\n{\n  \"b8\": 0.9,\n  \"b18\": 0.8,\n  \"b24\": 0.8,\n  \"b20\": 0.8,\n  \"b5\": 0.7\n}", "Based on the text and the titles of the references, the source papers that influenced the given paper most directly appear to be:\n\n1. \"A graph partitioning algorithm for edge or vertex balance\" [b8]: This paper is repeatedly referenced and appears to be the main inspiration for the algorithm proposed in the given paper. The authors of the given paper extend the B-GRAP algorithm proposed in this paper.\n2. \"Rank degree an efficient algorithm for graph sampling\" [b24]: The given paper makes use of a rank degree sampling method proposed in this paper.\n3. \"Spinner scalable graph partitioning in the cloud\" [b18]: The given paper discusses this work in the section on related work and compares its own method to Spinner.\n4. \"Multi dimensional balanced graph partitioning via projected gradient descent\" [b0]: This paper is mentioned in the context of discussing graph partitioning approaches, and it could have influenced the given paper's approach to balancing the graph.\n5. \"Distributed balanced partitioning via linear embedding\" [b1]: This paper is similarly mentioned in the context of discussing balancing in graph partitioning, and it could have provided inspiration for the given paper's approach to distributed computation.\n\nThe JSON format of the result is: \n{\"b8\": 1, \"b24\": 0.9, \"b18\": 0.8, \"b0\": 0.6, \"b1\": 0.6}", "Based on the analysis of the provided academic paper text, the most pertinent sources that have inspired or influenced the author's work can be identified as follows:\n\n1. b8: 'A graph partitioning algorithm for edge or vertex balance'\n2. b18: 'Spinner: scalable graph partitioning in the cloud'\n3. b5: 'Recent advances in graph partitioning'\n4. b24: 'Rank Degree: an efficient algorithm for graph sampling'\n5. b20: 'Near linear time algorithm to detect community structures in large scale networks'\n6. b14: 'Multilevel graph partitioning schemes'\n\nThe confidence scores for each identified source paper would be as follows (in JSON format):\n\n```json\n{\n  \"b8\": 1,\n  \"b18\": 0.9,\n  \"b5\": 0.8,\n  \"b24\": 0.8,\n  \"b20\": 0.7,\n  \"b14\": 0.7\n}\n```\n\nThese scores are estimated based on the frequency and context of the references to each source paper in the text.", "Based on the text, the following papers can be identified as the source papers:\n\n1. 'A graph partitioning algorithm for edge or vertex balance' [b8]: The paper's main contributions are built upon the techniques introduced in this paper. The authors extend B-GRAP, a multi-objective and scalable Balanced GRAph Partitioning algorithm. They also define new sampling-based heuristics named RD-B-GRAP (Random Degree) and HD-B-GRAP (High Degree) to study the behavior of the propagation.\n\n2. 'Rank degree an efficient algorithm for graph sampling' [b24]: The authors' strategy for seed initialization in HD-B-GRAP is inspired by the graph Rank Degree sampling method proposed in this paper. \n\n3. 'Spinner scalable graph partitioning in the cloud' [b18]: The paper is referenced when discussing the use of label propagation approach for graph partitioning. The authors also mention how they use a sample of the graph to initiate the label propagation process, similar to this paper.\n\nHere are the references with their confidence scores:\n\n{\n    \"b8\": 0.9,\n    \"b24\": 0.8,\n    \"b18\": 0.7\n}\n\nThe confidence scores are estimated based on the frequency of the paper's citations and the context in which they're mentioned.", "Based on the text, the source papers that heavily influenced the main contributions of the paper are:\n\n1. b8 - \"a graph partitioning algorithm for edge or vertex balance\" \n2. b18 - \"spinner scalable graph partitioning in the cloud\"\n3. b24 - \"rank degree an efficient algorithm for graph sampling\"\n\nThe paper's main contributions revolve around improving graph partitioning by incorporating sampling techniques. The paper extends the label propagation approach proposed by b8 and uses a similar approach to b18 by running the algorithms on different large graphs. The sampling technique used in the paper is inspired by the rank degree algorithm proposed by b24.\n\nIn json format:\n\n{\n    \"b8\": 1,\n    \"b18\": 0.8,\n    \"b24\": 0.8\n}"], "62708f615aee126c0fa6920a": ["Based on the text of the academic paper and the provided titles of the references, the most pertinent source papers that inspired the author are:\n\n1. \"SimCSE simple contrastive learning of sentence embeddings\" (b16): The author based his work on SimCSE and proposed DCLR to reduce the impact of negative sampling bias in sentence representation contrastive learning. The SimCSE model is used as a complementary model to produce weights for each negative representation. The confidence score is 1.0.\n\n2. \"How contextual are contextualized word representations comparing the geometry of bert elmo and gpt embeddings\" (b14): This is referred to when discussing the anisotropy problem of PLMs (Pretrained Language Models), which is one of the challenges the paper addresses. The confidence score is 0.7.\n\n3. \"BERT pre-training of deep bidirectional transformers for language understanding\" (b13): BERT-based PLMs are used for generating sentence representations, which indicates the influence of this paper on the work. The confidence score is 0.7.\n\n4. \"Virtual adversarial training a regularization method for supervised and semisupervised learning\" (b34): The author is inspired by VAT to design a non-uniformity loss maximization objective to produce gradients for improving the noise-based negatives. The confidence score is 0.8.\n\n5. \"A simple framework for contrastive learning of visual representations\" (b10): This paper is the foundational work of contrastive learning which is the main method used in the paper. The confidence score is 0.9.\n\n6. \"Contrastive curriculum learning for sequential user behavior modeling via data augmentation\" (b6): The paper is relevant to the author's use of contrastive learning and data augmentation strategies. The confidence score is 0.6.\n\n7. \"Adversarial training methods for semisupervised text classification\" (b33): This paper is relevant to the author's use of adversarial perturbations and its effectiveness on text classification. The confidence score is 0.6.\n\nThe returned result in the json format is: {\"b16\": 1.0, \"b14\": 0.7, \"b13\": 0.7, \"b34\": 0.8, \"b10\": 0.9, \"b6\": 0.6, \"b33\": 0.6}", "Based on the provided text, the following are the source papers that inspired the given paper:\n\n1. \"SimCSE: Simple Contrastive Learning of Sentence Embeddings\" [b16]\n2. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b13]\n3. \"Roberta: A Robustly Optimized BERT Pretraining Approach\" [b29]\n4. \"Virtual Adversarial Training: A Regularization Method for Supervised and Semi-supervised Learning\" [b34]\n5. \"ADAM: A Method for Stochastic Optimization\" [b24]\n6. \"How Contextual Are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings\" [b14]\n7. \"A Simple Framework for Contrastive Learning of Visual Representations\" [b10]\n\nThe corresponding confidence scores can be given based on the frequency of these references in the text and their direct links to the novel contributions and methods of the paper.\n\nResult in JSON format:\n\n{\n    \"b16\": 0.9,\n    \"b13\": 0.8,\n    \"b29\": 0.8,\n    \"b34\": 0.8,\n    \"b24\": 0.7,\n    \"b14\": 0.7,\n    \"b10\": 0.7\n}", "Based on the given academic paper text and author notes, the source papers that inspired the author most can be identified as follows:\n\n1. \"SimCSE: Simple Contrastive Learning of Sentence Embeddings\" [b16]\n2. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b13]\n3. \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\" [b29]\n4. \"How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings\" [b14]\n5. \"Contrastive Learning\" [b48]\n6. \"Virtual Adversarial Training: A Regularization Method for Supervised and Semi-supervised Learning\" [b34]\n\nThe confidence scores for each reference are as follows, with a higher score indicating a stronger influence on the paper's content:\n\n{ \"b16\": 0.9, \"b13\": 0.8, \"b29\": 0.75, \"b14\": 0.7, \"b48\": 0.65, \"b34\": 0.6 }", "Based on the provided information, we can conclude that the following papers directly inspired the author:\n1. [b16] 'simcse simple contrastive learning of sentence embeddings': This paper is directly mentioned multiple times and has clearly influenced the work. It is specifically mentioned in the context of the work's methodology.\n2. [b13] 'bert pre training of deep bidirectional transformers for language understanding': This paper is referenced multiple times in the context of the author's work, indicating a strong influence.\n3. [b46] 'consert a contrastive framework for self supervised sentence representation transfer': This paper is mentioned in the context of the author's work, suggesting its influence.\n4. [b14] 'how contextual are contextualized word representations comparing the geometry of bert elmo and gpt embeddings': It is mentioned in the text in relation to the author's work.\n5. [b48] 'freelb enhanced adversarial training for natural language understanding': This paper is mentioned in the author's work in relation to the method they propose.\n6. [b33] 'adversarial training methods for semisupervised text classification': This paper is referenced in the context of the author's work, indicating its influence.\n\nThe confidence scores are not provided, but the ranking order indicates the relative influence based on the number of mentions and the context of the mentions.\n\nThe output in JSON format:\n\n```\n{\n    \"b16\": 0.9,\n    \"b13\": 0.85,\n    \"b46\": 0.8,\n    \"b14\": 0.75,\n    \"b48\": 0.7,\n    \"b33\": 0.65\n}\n```\n\nThe confidence scores are set subjectively based on the mention frequency and the context of the mention.", "Based on the given text, the paper proposes a novel approach, DCLR, for reducing the sampling bias in contrastive learning of unsupervised sentence representations. The proposed method introduces noise-based negatives and an instance weighting method to tackle this challenge. The source papers that might have inspired this work could be inferred from the text where the authors have mentioned that they have adopted or built upon a certain method proposed in these papers. \n\nHere are the source papers identified with their confidence scores:\n\n```\n{\n    \"b16\": 0.8,\n    \"b14\": 0.7,\n    \"b48\": 0.6,\n    \"b46\": 0.5,\n    \"b13\": 0.5,\n    \"b33\": 0.4\n}\n```\n\nThe paper 'b16' titled 'SimCSE: Simple Contrastive Learning of Sentence Embeddings' is identified with the highest confidence score (0.8) as the paper explicitly mentions that the DCLR framework adopts the SimCSE as the complementary model. \n\nThe paper 'b14' titled 'How Contextual Are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings' is identified with the confidence score (0.7) as it discusses the anisotropy problem of PLMs, which is one of the challenges addressed in DCLR.\n\nThe paper 'b48' titled 'freelb enhanced adversarial training for natural language understanding' is identified with the confidence score (0.6) as the DCLR approach is inspired by VAT, a method mentioned in this paper. \n\nThe paper 'b46' titled 'ConSErt: A Contrastive Framework for Self-Supervised Sentence Representation Transfer' is identified with a confidence score (0.5) as it is mentioned in the context of existing works that DCLR follows in adopting in-batch representations as negatives.\n\nThe paper 'b13' titled 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding' is identified with a confidence score (0.5) as DCLR uses BERT-based PLMs to generate sentence representations.\n\nThe paper 'b33' titled 'Adversarial Training Methods for Semi-Supervised Text Classification' is identified with a confidence score (0.4) as it discusses virtual adversarial training (VAT), which is a method used in DCLR to optimize noise-based negatives."], "63fd715990e50fcafd146bdb": ["Based on the given text and the titles of the references, the following source papers seem to have significantly inspired the author's work:\n\n{ \n\"b1\": 0.85,\n\"b3\": 0.85,\n\"b7\": 0.9,\n\"b9\": 0.8,\n\"b13\": 0.9,\n\"b17\": 0.8,\n\"b18\": 0.9,\n\"b20\": 0.85\n}\n\nThe confidence score is assigned based on the frequency and context of citation in the text. The higher the score, the stronger the influence of the referenced paper on the author's work.\n\nHere is the explanation:\n\n\"b1\": \"A survey on few-shot learning in natural language processing\" is referenced when discussing the challenges of few-shot learning.\n\n\"b3\": \"Meta-learning for few-shot natural language processing: a survey\" is cited when discussing the methods used for overcoming the challenges of few-shot learning, such as meta-learning.\n\n\"b7\": \"Language models are few-shot learners\" is mentioned multiple times when discussing the use of language models for text generation, which is a key part of the author's proposed method.\n\n\"b9\": \"PTR: prompt tuning with rules for text classification\" is mentioned when discussing the recent methodological developments in few-shot learning.\n\n\"b13\": \"A survey of data augmentation approaches for NLP\" is referenced when discussing the importance of data augmentation in NLP, which is integral to the author's work.\n\n\"b17\": \"A survey on data augmentation for text classification\" is referenced when discussing the limitations of existing data augmentation methods.\n\n\"b18\": \"Recent advances in natural language processing via large pre-trained language models: a survey\" is mentioned when discussing the advent of large language models (LLMs), which the author's work also utilizes.\n\n\"b20\": \"Training language models to follow instructions with human feedback\" is cited when discussing the training of the language model ChatGPT, which the author uses in their method.", "Based on the given text and the titles of the references, the source papers that most likely influenced the main contributions of the paper are:\n\n{\"b0\": 0.95, \"b7\": 0.99, \"b20\": 0.98, \"b55\": 0.95, \"b57\": 0.90, \"b11\": 0.85, \"b13\": 0.80, \"b17\": 0.75}", "Based on the provided text and the titles of the references, the source papers that inspired the current paper the most are:\n\n1. \"Language models are few shot learners\" (b7) - The paper mentions that it was inspired by the success of applying language models in text generation and proposes a new data augmentation method named ChatAug, which leverages ChatGPT to generate auxiliary samples for few-shot text classification.\n2. \"Training language models to follow instructions with human feedback\" (b20) - The paper mentions that the training of ChatGPT follows that of Instruct-GPT, which utilizes reinforcement learning with human feedback (RLHF), thus enabling it to produce more informative and impartial responses to input.\n3. \"Is ChatGPT a general-purpose natural language processing task solver?\" (b55) - The paper refers to a study by Qin et al. about the versatility of ChatGPT in NLP tasks.\n4. \"BERT: Pre-training of deep bidirectional transformers for language understanding\" (b5) - The paper discusses the use of BERT as a pre-trained language model as a common approach in NLP and for fine-tuning with limited samples.\n\nThe JSON format of the result is:\n\n{\n    \"b7\": 1,\n    \"b20\": 0.8,\n    \"b55\": 0.7,\n    \"b5\": 0.6\n}", "Based on the text and the title of the references, the most pertinent source papers are as follows:\n\n1. 'b0': 'generalizing from a few examples a survey on few shot learning' - This paper is about few-shot learning, a concept that is key to the paper. The author mentions this topic several times in the text. Confidence score: 0.9\n2. 'b7': 'language models are few shot learners' - This paper is about language models being few shot learners, which is the same as the topic of the paper. The author mentions this topic multiple times in the text. Confidence score: 0.9\n3. 'b20': 'training language models to follow instructions with human feedback' - The author specifically mentions that the training of ChatGPT, the model used in the paper, follows the training method described in this paper. Confidence score: 0.85\n4. 'b13': 'a survey of data augmentation approaches for nlp' - This paper is referenced multiple times when the author discusses data augmentation, a theme that is central to the whole paper. Confidence score: 0.8\n5. 'b55': 'is chatgpt a general purpose natural language processing task solver' - This paper is about ChatGPT, the model used in the paper. The author mentions that ChatGPT has been evaluated on a comprehensive set of NLP tasks. Confidence score: 0.8\n6. 'b5': 'bert pretraining of deep bidirectional transformers for language understanding' - This paper is mentioned when the author discusses pre-trained language models, which is a key concept in the paper. Confidence score: 0.75\n7. 'b46': 'improving language understanding by generative pre training' - This paper is referenced when the author discusses pre-training and the improvement of language understanding, which are important concepts in the paper. Confidence score: 0.7\n\nThe relevant source papers in JSON format are:\n\n{\n \"b0\": 0.9,\n \"b7\": 0.9,\n \"b20\": 0.85,\n \"b13\": 0.8,\n \"b55\": 0.8,\n \"b5\": 0.75,\n \"b46\": 0.7\n}", "Based on the paper text analysis, the following references seem to have directly influenced the paper's main contributions:\n\n1. \"Inspired by the success of applying language models in text generation, we propose a new data augmentation method named ChatAug, which leverages ChatGPT to generate auxiliary samples for few-shot text classification.\" This suggests that [\"b7\": \"Language Models are Few-Shot Learners\"] and [\"b20\": \"Training Language Models to Follow Instructions with Human Feedback\"] are two main source papers.\n\n2. \"In NLP, there are several types of data augmentation methods. Traditional text-level data augmentation methods rely on direct operations on the existing sample base... More recent methods utilize language models to generate reliable samples for more effective data augmentation, including backtranslation and word vector interpolation in the latent space.\" This suggests that [\"b11\": \"EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\"] and [\"b14\": \"Improving Neural Machine Translation Models with Monolingual Data\"] are also source papers.\n\n3. \"However, the performance of these methods is still intrinsically limited by the data quality and quantity in both the source and target domains... Data augmentation is usually model-agnostic and involves no change to the underlying model architecture, which makes this approach particularly practical and applicable to a wide range of tasks.\" This suggests that [\"b12\": \"A Closer Look at Feature Space Data Augmentation for Few-Shot Intent Classification\"] is another source paper.\n\n4. \"Many FSL methods have shown promising results in overcoming this challenge in various tasks... Existing FSL methods mainly focus on improving the learning and generalization capability of the model via better architectural design, leveraging pre-trained language models as the basis and then finetuning it using limited samples with meta-learning or prompt-based methods.\" This suggests that [\"b0\": \"Generalizing from a Few Examples: A Survey on Few-Shot Learning\"] and [\"b10\": \"Towards Unified Prompt Tuning for Few-Shot Text Classification\"] are source papers.\n\nBased on the above analysis, the confidence scores could be assigned as follows:\n\n{\"b7\": 1, \"b20\": 1, \"b11\": 0.9, \"b14\": 0.9, \"b12\": 0.9, \"b0\": 0.9, \"b10\": 0.9}"], "62b52c635aee126c0f459d22": ["Based on the text and the reference titles, the source papers that have likely inspired this given paper most directly are:\n\n{\n\"b4\": 0.9,\n\"b5\": 0.7,\n\"b7\": 0.7,\n\"b10\": 0.9,\n\"b54\": 0.7,\n\"b11\": 0.7,\n\"b12\": 0.7,\n\"b14\": 0.7,\n\"b15\": 0.7,\n\"b16\": 0.7\n}\n\nThe confidence scores are reflective of the direct relevance and influence of the source papers on the given paper's novel contributions. For instance, papers such as \"Offline reinforcement learning with implicit Q learning\" [b4] and \"Chai: a chatbot AI for task-oriented dialogue with offline reinforcement learning\" [b10] are likely to have been significant sources of inspiration, considering the given paper's emphasis on introducing a novel offline RL method for language models.", "Based on the content of the paper, the key phrases indicating the direct inspiration from previous work, and the novel contributions the paper introduces, the most pertinent source papers seem to be:\n- [b4]: \"offline reinforcement learning with implicit q learning\"\n- [b5]: \"conservative q learning for offline reinforcement learning\"\n- [b10]: \"chai a chatbot ai for task oriented dialogue with offline reinforcement learning\"\n- [b11]: \"human centric dialog training via offline reinforcement learning\"\n- [b12]: \"sequence tutor conservative fine tuning of sequence generation models with kl control\"\n- [b7]: \"decision transformer reinforcement learning via sequence modeling\"\n- [b14]: \"learning to write with cooperative discriminators\"\n\nWe can also infer that the following papers may have influenced the paper's contributions based on the titles and the context they are mentioned in the text:\n- [b3]: \"offline reinforcement learning tutorial review and perspectives on open problems\"\n- [b46]: \"advantage weighted regression simple and scalable off policy reinforcement learning\"\n\nThe json format of the result is: \n{\"b4\": 0.9, \"b5\": 0.8, \"b10\": 0.7, \"b11\": 0.7, \"b12\": 0.7, \"b7\": 0.7, \"b14\": 0.7, \"b3\": 0.6, \"b46\": 0.5} \n\nEach key represents the reference number and value represents the confidence score of the paper being a source paper. The confidence score is subjective and is based on the context in which each paper is mentioned in the text.", "Based on the text, the source papers that inspire this given paper most could be:\n\n{ \n  \"b4\": 0.9, \n  \"b10\": 0.8, \n  \"b11\": 0.8, \n  \"b12\": 0.8, \n  \"b3\": 0.7, \n  \"b5\": 0.7, \n  \"b14\": 0.7, \n  \"b16\": 0.7 \n}\n\nHere, the confidence score represents the relevance of a source paper to the given paper, with 1 being the most relevant and 0 being the least relevant.", "Based on the criteria and the provided text, the following references seem to have inspired the author most:\n\n1. \"b3\" - Offline reinforcement learning tutorial review and perspectives on open problems: The author appears to be directly influenced by the challenges and guidelines presented in this paper to develop the novel offline RL method.\n   Confidence Score: 0.9\n\n2. \"b4\" - Offline reinforcement learning with implicit q learning: This paper's method is directly adopted and extended in the main contribution of the author. It is the source of the novel offline RL algorithm ILQL.\n   Confidence Score: 0.95\n\n3. \"b5\" - Conservative q learning for offline reinforcement learning: The author adopts the conservatism loss term from this paper, which is a crucial part of the proposed ILQL method.\n   Confidence Score: 0.9\n\n4. \"b10\" - Chai a chatbot ai for task oriented dialogue with offline reinforcement learning: The author uses this paper to highlight the challenges and limitations of prior offline RL approaches for language tasks which the ILQL method aims to overcome.\n   Confidence Score: 0.8\n\n5. \"b15\" - Fudge controlled text generation with future discriminators: The author refers to this paper as a comparative study to highlight the benefits of their proposed ILQL method.\n   Confidence Score: 0.8 \n\n6. \"b54\" - Learning cooperative visual dialog agents with deep reinforcement learning: The author uses the Visual Dialogue dataset from this paper for evaluating their method and also refers to the methods presented in this paper for comparison.\n   Confidence Score: 0.85\n\nThe returned result in json format:\n\n{\n    \"b3\" : 0.9,\n    \"b4\" : 0.95,\n    \"b5\" : 0.9,\n    \"b10\" : 0.8,\n    \"b15\" : 0.8,\n    \"b54\" : 0.85\n}\n", "Based on the text and the author's notes, the following source papers are the most relevant:\n\n1. \"offline reinforcement learning with implicit q learning\" [b4]: This paper is directly mentioned as the foundation for the proposed algorithm ILQL which is a novel offline RL method based on dynamic programming. The explicit mention of this paper and the clear reliance on its methodology gives it a high confidence score. Confidence score: 1.\n\n2. \"conservative q learning for offline reinforcement learning\" [b5]: The authors refer to this paper when explaining the conservatism loss term added to their Q-function. Confidence score: 0.9.\n\n3. \"offline reinforcement learning as one big sequence modeling problem\" [b6]: This paper is mentioned in the context of combining supervised learning with the general utility optimization power of online reinforcement learning methods, which is a key aspect of the proposed ILQL algorithm. Confidence score: 0.8.\n\n4. \"decision transformer reinforcement learning via sequence modeling\" [b7]: The paper is mentioned alongside [b6] in the context of combining supervised learning and reinforcement learning. It may have influenced how the authors designed their method. Confidence score: 0.7.\n\n5. \"chai a chatbot ai for task oriented dialogue with offline reinforcement learning\" [b10]: This paper was referred to when discussing the limitations of prior offline RL approaches for language tasks, which likely influenced the development of ILQL. Confidence score: 0.75.\n\n6. \"human centric dialog training via offline reinforcement learning\" [b11]: Similarly to [b10], this paper was referred to when discussing the limitations of existing methods, likely providing motivation for the authors' approach. Confidence score: 0.7.\n\n7. \"sequence tutor conservative fine tuning of sequence generation models with kl control\" [b12]: This paper is mentioned in the context of prior offline RL approaches for language tasks, which likely provided some inspiration for the authors' ILQL approach. Confidence score: 0.7.\n\nReturn in json format:\n{\n\"b4\": 1,\n\"b5\": 0.9,\n\"b6\": 0.8,\n\"b7\": 0.7,\n\"b10\": 0.75,\n\"b11\": 0.7,\n\"b12\": 0.7\n}"], "6417d04190e50fcafd83de21": ["Based on the provided text and the titles of the references, the source papers that most inspired the authors can be inferred as follows:\n\n- \"b6: Iterative deep graph learning for graph neural networks better and robust node embeddings\": The authors have utilized the idea of iterative deep graph learning for enhancing the robustness of node embeddings. The reference is directly related to the main contribution of the paper and seems to be a significant source of inspiration. Confidence score: 0.8\n\n- \"b17: Graph structure learning for robust graph neural networks\": The authors have focused on enhancing the robustness of GNNs using graph structure learning, which aligns with the main challenges and methods outlined in the paper. Confidence score: 0.9\n\n- \"b19: Structural information and dynamical complexity of networks\": The authors used the structural entropy theory suggested in this paper to effectively measure the information embedded in an arbitrary graph and structural diversity. Confidence score: 0.9\n\n- \"b45: AMGCN: Adaptive multi-channel graph convolutional networks\": The authors might have been inspired by the idea of adaptive graph convolutional networks to enhance the robustness of GNNs. Confidence score: 0.7\n\n- \"b62: Deep graph structure learning for robust representations a survey\": The authors have used deep graph structure learning to enhance the robustness of GNNs, which aligns with the main challenges and methods outlined in the paper. Confidence score: 0.8\n\nOverall, the papers b6, b17, b19, b45, and b62 seem to be the most pertinent source papers that inspired the authors. These papers are closely related to the main contributions of the paper, focusing on enhancing the robustness of GNNs using graph structure learning and structural entropy.\n\nThe result in JSON format:\n\n{\"b6\": 0.8, \"b17\": 0.9, \"b19\": 0.9, \"b45\": 0.7, \"b62\": 0.8}", "Based on the text and the titles of the references, the source papers that seem to inspire the given paper the most are:\n\n1. \"Iterative deep graph learning for graph neural networks better and robust node embeddings\" [b6]\n2. \"Structural information and dynamical complexity of networks\" [b19]\n3. \"Decoding topologically associating domains with ultra low resolution hi c data by graph structural entropy\" [b21]\n4. \"Graph structure learning for robust graph neural networks\" [b17]\n5. \"Graph Structure Learning with Variational Information Bottleneck\" [b34]\n6. \"DropEdge: Towards Deep Graph Convolutional Networks on Node Classification\" [b35]\n7. \"Position-aware structure learning for graph topology imbalance by relieving under reaching and over squashing\" [b40]\n8. \"AM-GCN: Adaptive Multi-channel Graph Convolutional Networks\" [b45]\n\nI chose these papers because they seem to have the most direct relevance to the novel contributions of the given paper, which include the development of a graph structure learning framework called SE-GSL that leverages structural entropy for optimization, the use of a hierarchical encoding tree for graph structuralization, and a sampling-based method for graph reconstruction.\n\nJSON format:\n\n```\n{\n  \"b6\": 0.9,\n  \"b19\": 0.9,\n  \"b21\": 0.8,\n  \"b17\": 0.8,\n  \"b34\": 0.7,\n  \"b35\": 0.7,\n  \"b40\": 0.7,\n  \"b45\": 0.7\n}\n```", "Based on the given text and the titles of the references, the source papers of the given paper are:\n\n1. \"Structural information and dynamical complexity of networks\" (b19): This paper introduced the structural entropy theory which is the foundation of the proposed SE-GSL framework.\n\n2. \"Decoding topologically associating domains with ultra low resolution hi c data by graph structural entropy\" (b21): This paper further developed the structural entropy theory and its application in hierarchical clustering, which is a key concept in the proposed method.\n\n3. \"Low model interpretability\" (b14): This paper is mentioned in relation to the problem of model interpretability, which is a challenge that the proposed method is trying to address.\n\n4. \"Graph structure learning with variational information bottleneck\" (b34): This paper is about graph structure learning which is what the proposed method is about.\n\n5. \"Adaptive multi channel graph convolutional networks\" (b45): This paper is mentioned in relation to the fusion of node features and topological features, which is a crucial step in the proposed method.\n\n6. \"Graph information bottleneck\" (b49): This paper is about the graph information bottleneck problem, which is indirectly related to the main challenge that the proposed method is trying to address.\n\nThe JSON format of the result is:\n\n{\n \"b19\": 1.0,\n \"b21\": 0.9,\n \"b14\": 0.8,\n \"b34\": 0.9,\n \"b45\": 0.8,\n \"b49\": 0.7\n}", "Based on the given text and titles of the references, the following papers seem to be the most relevant sources:\n\n1. \"Iterative deep graph learning for graph neural networks better and robust node embeddings\" [b6] - The paper mentions using this work to enhance the robustness of node representation. Confidence Score: 0.8\n\n2. \"Graph structure learning for robust graph neural networks\" [b17] - The paper's goal of improving the resilience and robustness of node representation aligns with the motivations of this work. Confidence Score: 0.7\n\n3. \"Structural information and dynamical complexity of networks\" [b19] - This paper directly influences the proposed methodology of SE-GSL, as it's used to marry the structural entropy and encoding tree theory with GSL. Confidence Score: 0.9\n\n4. \"Decoding topologically associating domains with ultra low resolution hi c data by graph structural entropy\" [b21] - This paper contributes to the proposed methodology of SE-GSL in terms of structural entropy theory. Confidence Score: 0.8\n\n5. \"Graph structure learning with variational information bottleneck\" [b34] - The paper draws inspiration from this work for its graph structure learning. Confidence Score: 0.7\n\n6. \"Dropedge towards deep graph convolutional networks on node classification\" [b35] - This paper could have contributed to the development of the sample-based graph reconstruction method in SE-GSL. Confidence Score: 0.6\n\n7. \"Revisiting semi-supervised learning with graph embeddings\" [b52] - This paper could have provided key insights for the development of the proposed method. Confidence Score: 0.7\n\nThe results in the JSON format would be:\n\n{\n\"b6\": 0.8,\n\"b17\": 0.7,\n\"b19\": 0.9,\n\"b21\": 0.8,\n\"b34\": 0.7,\n\"b35\": 0.6,\n\"b52\": 0.7\n}", "Based on the given text of the paper and the titles of the references, the source papers of the given paper that the author indicates inspire them most are:\n\n1. \"Iterative deep graph learning for graph neural networks: better and robust node embeddings\" [b6]\n2. \"Graph structure learning for robust graph neural networks\" [b17]\n3. \"Structural information and dynamical complexity of networks\" [b19]\n4. \"Graph structure learning with variational information bottleneck\" [b34]\n5. \"Position-aware structure learning for graph topology imbalance by relieving under reaching and over squashing\" [b40]\n6. \"Graph information bottleneck\" [b49]\n7. \"Deep graph structure learning for robust representations: a survey\" [b62]\n8. \"Graph contrastive learning with adaptive augmentation\" [b63]\n\nTo be noted that the above source papers have been selected based on the phrases in the given academic text that indicate the use of the methodologies and approaches from these papers. For instance, phrases such as \"inspired by structural entropy theory [b19]\", \"exploits the hierarchical structure of the encoding tree [b19]\", \"the optimization objective to the ?-level tree with a hyperparameter [b40]\", \"optimize the graph structure with community preservation by investigating the structural entropy of deduction [b40]\", \"graph reconstruction from the hierarchy [b34]\", \"graph structure learning [b17]\", \"robust graph neural networks [b6]\", \"the graph structure is evolutionary [b49]\", \"graph structure learning with variational information bottleneck [b34]\", \"adaptive augmentation [b63]\", and \"graph contrastive learning [b62]\".\n\nThe confidence scores for these source papers are estimated as follows:\n\n{ \"b6\": 0.9, \"b17\": 0.9, \"b19\": 1.0, \"b34\": 0.9, \"b40\": 1.0, \"b49\": 0.8, \"b62\": 0.8, \"b63\": 0.8 }"], "628d1ea25aee126c0f3e9734": ["Based on the provided text of the academic paper, the source papers that seem to inspire this work most are:\n\n1. \"translating embeddings for modeling multi relational data\" [b1]: The paper frequently refers to the TransE model and its limitations, indicating that the authors have been heavily influenced by this work.\n2. \"knowledge graph embedding by translating on hyperplanes\" [b23]: The paper mentions TransH model and its limitations, showing that this is a source of inspiration.\n3. \"learning entity and relation embeddings for knowledge graph completion\" [b13]: The paper discusses the TransR model and its limitations, which suggests it has influenced the authors' work.\n4. \"knowledge graph embedding via dynamic mapping matrix\" [b9]: The paper discusses the TransD model and its limitations, which suggests it has influenced the authors' work.\n5. \"representation learning of knowledge graphs with hierarchical types\" [b25]: This paper is mentioned as it discusses hierarchical types, which seems to be a significant part of the authors' approach.\n6. \"transrhs a representation learning method for knowledge graphs with relation hierarchical structure\" [b31]: This paper is mentioned as it discusses relation hierarchical structure which is relevant to the proposed method.\n\nThe result in json format would be:\n\n{\n    \"b1\": 0.9,\n    \"b23\": 0.8,\n    \"b13\": 0.8,\n    \"b9\": 0.8,\n    \"b25\": 0.7,\n    \"b31\": 0.7\n} \n\nThe confidence score is higher for \"translating embeddings for modeling multi relational data\" [b1] because it is the most frequently mentioned and discussed source paper. The other papers have slightly lower confidence scores as they are mentioned less frequently but still seem to be significant sources of inspiration for the authors.", "Based on the provided text and the task instructions, the following papers could be the main sources of inspiration for the given academic paper:\n\n1. \"TransE: Translating embeddings for modeling multi-relational data\" [b1]: The author has built upon the translation distance model proposed in this paper. The author has also drawn comparisons with TransE and mentioned its limitations, indicating that it was a significant point of reference during the development of their work.\n\n2. \"TransH: Knowledge graph embedding by translating on hyperplanes\" [b23]: The author has compared their work with TransH and mentioned its shortcomings, implying that it served as a reference point for their research.\n\n3. \"TransR: Learning entity and relation embeddings for knowledge graph completion\" [b13]: This paper is another point of reference for the author's work, with the author comparing their model with TransR and mentioning its limitations.\n\n4. \"TransD: Knowledge graph embedding via dynamic mapping matrix\" [b9]: This paper seems to be another source of inspiration, with the author comparing their model with TransD and discussing its limitations.\n\n5. \"RESCAL: A three way model for collective learning on multi-relational data\" [b16]: The semantic matching model approach mentioned in this paper seems to have influenced the author's work.\n\n6. \"ComplEx: Complex embeddings for simple link prediction\" [b20]: This paper's approach to semantic matching appears to have had an impact on the author's research.\n\n7. \"SSE: Semantically smooth embedding for knowledge graphs\" [b5]: This paper, which deals with ontology models, seems to have influenced the author's work, particularly in terms of incorporating semantic information into the knowledge embedding process.\n\n8. \"TKRL: Representation learning of knowledge graphs with hierarchical types\" [b25]: The author seems to have taken inspiration from this paper, as they've discussed the limitations of TKRL and how their own model addresses these limitations.\n\nHere is the result in the requested JSON format:\n\n{\n    \"b1\": 0.9,\n    \"b23\": 0.85,\n    \"b13\": 0.85,\n    \"b9\": 0.85,\n    \"b16\": 0.8,\n    \"b20\": 0.8,\n    \"b5\": 0.8,\n    \"b25\": 0.8\n}", "Based on the text of the paper and the titles of the references, the following references seem to be the source papers that inspired the author the most:\n\n1. \"Translating embeddings for modeling multi-relational data\" [b1]\n2. \"Knowledge graph embedding via dynamic mapping matrix\" [b9]\n3. \"Learning entity and relation embeddings for knowledge graph completion\" [b13]\n4. \"Representation learning of knowledge graphs with hierarchical types\" [b25]\n5. \"Knowledge graph embedding by translating on hyperplanes\" [b23]\n6. \"Knowledge graph embedding: a survey of approaches and applications\" [b24]\n7. \"TransRHS: a representation learning method for knowledge graphs with relation hierarchical structure\" [b31]\n\nThese references are selected based on the relevance of their titles to the research topic, the frequency of their citations in the text, and the importance of their contributions to the algorithm proposed in the paper.\n\nHere is the output in JSON format:\n\n{\n  \"b1\": 0.9,\n  \"b9\": 0.85,\n  \"b13\": 0.8,\n  \"b25\": 0.75,\n  \"b23\": 0.8,\n  \"b24\": 0.85,\n  \"b31\": 0.9\n}\n\nThe confidence scores are estimated based on the relevance and frequency of the citations. Higher score means the paper is more likely to be a source paper that inspired the author.", "Based on the given text of the academic paper, the following source papers seem to have inspired the author the most:\n\n1. \"Translating embeddings for modeling multi-relational data\" [b1] \n2. \"Knowledge graph embedding by translating on hyperplanes\" [b23]\n3. \"Learning entity and relation embeddings for knowledge graph completion\" [b13]\n4. \"Knowledge graph embedding via dynamic mapping matrix\" [b9]\n5. \"Complex embeddings for simple link prediction\" [b20]\n6. \"Representation learning of knowledge graphs with hierarchical types\" [b25]\n7. \"TransRHS a representation learning method for knowledge graphs with relation hierarchical structure\" [b31]\n8. \"Knowledge graph embedding with hierarchical relation structure\" [b32]\n\nThe reason for choosing these papers as sources of inspiration is due to the fact that these papers are extensively used and referenced throughout the paper. They appear to be directly related to the challenges and the novel algorithm being proposed in the paper. The author's novel method seems to be inspired by and improving upon the methods proposed in these source papers.\n\nHere is the list in the requested json format:\n\n{\n \"b1\": 0.9, \n \"b23\": 0.85, \n \"b13\": 0.8, \n \"b9\": 0.75, \n \"b20\": 0.7, \n \"b25\": 0.65, \n \"b31\": 0.6, \n \"b32\": 0.55\n}\n\nThe confidence scores assigned, on a scale of 0-1, are based on the frequency and relevance of the reference in the paper, with a higher score indicating a more significant influence on the paper's contributions. Please note that these scores are approximate and based on the information provided in the paper text.", "Based on the given text, the following references appear to have most heavily influenced the main contributions of the paper:\n\n- \"b1: translating embeddings for modeling multi relational data\": The paper specifically mentions the TransE model, which comes from this paper, as a basis for their work and inspiration for their TransO model.\n\n- \"b9: knowledge graph embedding via dynamic mapping matrix\": The paper mentions and critiques this model, suggesting it was influential in shaping their approach.\n\n- \"b23: knowledge graph embedding by translating on hyperplanes\": Mentioned as a model the paper improves upon, indicating it was a source of inspiration.\n\n- \"b25: representation learning of knowledge graphs with hierarchical types\": This paper is about hierarchical types, which is a key aspect of the proposed model.\n\n- \"b31: transrhs a representation learning method for knowledge graphs with relation hierarchical structure\": This paper also focuses on hierarchical structures, a key aspect of the proposed model.\n\nIt's important to note that the paper also mentions several other models and approaches, but these appear to be the most directly relevant to the paper's primary contributions. \n\nThe returned JSON format is: \n{ \n    \"b1\" : 0.9, \n    \"b9\" : 0.8, \n    \"b23\" : 0.8, \n    \"b25\" : 0.75, \n    \"b31\" : 0.75\n}"], "63dcdb422c26941cf00b6413": ["Based on the context provided, the paper seems to be heavily influenced by the following references:\n\n1. \"b10\": \"Single path one-shot neural architecture search with uniform sampling\"\n2. \"b19\": \"Universal statistics of Fisher information in deep neural networks mean field approach\"\n3. \"b27\": \"The spectrum of the Fisher information matrix of a single-hidden layer neural network\"\n4. \"b36\": \"Transfer learning with neural automl\"\n5. \"b40\": \"Design space for graph neural networks\"\n6. \"b42\": \"Taskonomy disentangling task transfer learning\"\n7. \"b45\": \"Auto gnn neural architecture search of graph neural networks\"\n\nThe JSON output would be:\n\n{\n\"b10\": 0.95,\n\"b19\": 0.90,\n\"b27\": 0.90,\n\"b36\": 0.85,\n\"b40\": 0.85,\n\"b42\": 0.80,\n\"b45\": 0.80\n}\n\nEach of these references is associated with a confidence score between 0 and 1, which represents how strongly the paper seems to rely on the concepts and methods introduced in these references. The confidence score is determined by the mention and context in the provided text.", "Based on the given text and author notes, the following references seem to be the most pertinent sources that inspired the paper:\n\n1. \"b5\": \"ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware\" - The paper mentions the use of AutoML to reduce the human effort required for architecture design and points to this work as an example of successful application of AutoML.\n\n2. \"b10\": \"Single Path One-Shot Neural Architecture Search with Uniform Sampling\" - The paper refers to AutoML techniques considering each task independently, which is also a concept discussed in this work.\n\n3. \"b19\": \"Universal statistics of Fisher Information in Deep Neural Networks: Mean Field Approach\" - The paper discusses the use of Fisher Information Matrix, which aligns with the method used in this paper to measure the features of each task.\n\n4. \"b40\": \"Design Space for Graph Neural Networks\" - This paper seems to be a key inspiration as it discusses the task-dependence of the best GNN design, a concept that is heavily discussed in this paper.\n\n5. \"b36\": \"Transfer Learning with Neural AutoML\" - The paper discusses the idea of transferring knowledge across tasks, which is a key concept in this paper.\n\n6. \"b0\": \"Task2Vec: Task Embedding for Meta-Learning\" - The paper discusses the use of task embeddings, a concept that is used in this paper to enable knowledge transfer.\n\nThe source papers in json format would be:\n\n{\n\"b5\": 0.8,\n\"b10\": 0.7,\n\"b19\": 0.9,\n\"b40\": 1.0,\n\"b36\": 0.8,\n\"b0\": 0.9\n}", "Based on the given information, the source papers that inspired the author the most are:\n\n1. \"AUTOTRANSFER: AUTOML with Knowledge Transfer: An Application to Graph Neural Networks\" - The paper is directly named after this source and its content heavily revolves around the concepts outlined by it.\n2. \"Design space for Graph Neural Networks\" - The author references this paper when discussing the necessity of their proposed solution, indicating that it serves as a strong source of inspiration.\n3. \"Task2Vec: Task Embedding for Meta-Learning\" - The author references this paper as a key concept in developing their task embeddings.\n4. \"Neural Architecture Search with Reinforcement Learning\" - The author references this paper when discussing automated machine learning (AutoML), which is a central theme of their paper.\n5. \"GraphNAS: Graph Neural Architecture Search with Reinforcement Learning\" - This source is referenced as an example of a challenge the author's solution is designed to address.\n6. \"Auto GNN: Neural Architecture Search of Graph Neural Networks\" - Much like the previous source, this paper's central challenge is addressed by the author's proposed solution, indicating its inspirational role.\n7. \"Fisher Information Matrix (FIM)\" - Although not directly referenced, the concept of the FIM is central to the author's proposed solution, indicating it as an important source.\n\nThe confidence scores for each paper can be given as: \n\n{\"b47\": 0.95, \"b40\": 0.90, \"b0\": 0.85, \"b10\": 0.80, \"b45\": 0.85, \"FIM\": 0.90, \"None\": 1} \n\nThese scores are based on the frequency and context of each source's mention in the text, as well as their relevance to the paper's main contributions.", "Based on the text, the source papers that inspired this research could be:\n\n1. \"AUTOTRANSFER: AUTOML WITH KNOWLEDGE TRANSFER AN APPLICATION TO GRAPH NEURAL NETWORKS\" - This paper is highly related to the research topic and could be a direct inspiration.\n2. \"FISHER INFORMATION MATRIX (FIM)\" - The author uses the FIM to measure and feature the tasks, which is a significant part of the methodology.\n3. \"GRAPHNAS: GRAPH NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING\" - This paper was mentioned multiple times and could have motivated the research.\n4. \"AUTOGNN: NEURAL ARCHITECTURE SEARCH OF GRAPH NEURAL NETWORKS\" - The author mentions this paper as an existing solution that their research improves upon.\n5. \"RANDOM SEARCH FOR HYPER PARAMETER OPTIMIZATION\" - This paper was referenced in the context of the author's search algorithm, suggesting it influenced their approach.\n6. \"TASK2VEC: TASK EMBEDDING FOR META LEARNING\" - The author uses task embeddings in their methodology, suggesting inspiration from this paper.\n7. \"DESIGN SPACE FOR GRAPH NEURAL NETWORKS\" - This paper was mentioned in the context of the author's approach to design spaces, indicating it may have inspired their research.\n\nThe JSON format of the answer would be:\n\n{\n\"b0\" : 0.9,\n\"b1\" : 0.9,\n\"b9\" : 0.8,\n\"b45\" : 0.8,\n\"b10\" : 0.7,\n\"b42\" : 0.7,\n\"b40\" : 0.7\n}\n\nNote: The confidence scores are subjective and they indicate the likelihood that each paper is a source of inspiration for the research, where 1 is the most confident and 0 is the least confident.", "Based on the paper text and author notes, the most pertinent source papers appear to be:\n\n1. \"AUTOTRANSFER: AUTOML WITH KNOWLEDGE TRANSFER: AN APPLICATION TO GRAPH NEURAL NETWORKS\" - The paper directly builds upon the concepts and methodologies presented in this paper, as it is mentioned multiple times throughout the text. In particular, it appears to heavily influence the proposed AUTOTRANSFER solution.\n\n2. \"GraphNAS: Graph Neural Architecture Search with Reinforcement Learning\" - This paper is also repeatedly referenced, particularly in relation to the current challenges and limitations of AutoML for graph learning tasks.\n\n3. \"Efficient Neural Architecture Search via Parameters Sharing\" - The current paper discusses the successes of this work, implying that it may have served as a source of inspiration.\n\n4. \"Design Space for Graph Neural Networks\" - This paper is cited as a key reference that indicates the task-dependent nature of the best GNN design, which directly informs the current paper's contributions.\n\n5. \"Task Vec: Task Embedding for Meta Learning\" - This paper's task embedding concept seems to have been incorporated into the current paper's proposed solution.\n\n6. \"Transfer Learning with Neural AutoML\" - This paper's technique of transferring knowledge across tasks appears to be a significant source of inspiration for the current paper's novel contributions.\n\n7. \"The Spectrum of the Fisher Information Matrix of a Single-Hidden Layer Neural Network\" - The current paper uses the Fisher Information Matrix (FIM) concept from this paper to construct task features and further learn task embeddings.\n\nBased on the above analysis, the json format of the source papers and their confidence scores is:\n\n{\n\"b47\": 0.9, \n\"b9\": 0.85, \n\"b29\": 0.8, \n\"b40\": 0.8, \n\"b0\": 0.75, \n\"b36\": 0.75, \n\"b27\": 0.7\n}"], "6310233d90e50fcafdc2f5b3": ["Based on the text and the titles of the references, the most relevant source papers that have inspired the paper are:\n\n1. \"Dense passage retrieval for open domain question answering [b21]\" - This paper is mentioned in the context of inspiring the authors to propose an efficient BI-encoder.\n2. \"Learning dense representations for entity retrieval [b15]\" - This paper is also referenced when discussing the use of a bi-encoder.\n3. \"Scalable zero-shot entity linking with dense entity retrieval [b56]\" - The authors have explicitly stated that they were inspired by the success of this work in entity linking.\n4. \"Named entity recognition with bidirectional LSTM-CNNs [b4]\" - This paper is mentioned when the authors describe their novel method of using span-based contrastive learning for NER, which differs from the methods used in this paper.\n5. \"Spanner: named entity recognition as span prediction [b13]\" - This paper is also mentioned in the context of the authors' novel method.\n6. \"Learning a similarity metric discriminatively, with application to face verification [b6]\" - This paper is cited when the authors discuss the origins of the use of bi-encoder. \n\nIn the json format, the result is:\n\n{\n\"b21\": 1,\n\"b15\": 0.9,\n\"b56\": 1,\n\"b4\": 0.8,\n\"b13\": 0.8,\n\"b6\": 0.7\n}", "Based on the text of the paper and the titles of the references, the source papers for this paper seem to be:\n\n- \"Dense passage retrieval for open-domain question answering\" [b21] - The paper specifically mentions being inspired by this work for open-domain question answering. Confidence score: 0.9\n- \"Learning dense representations for entity retrieval\" [b56] - This paper is mentioned as inspiration for entity linking. Confidence score: 0.9\n- \"Named entity recognition with bidirectional LSTM-CNNS\" [b4] - This paper is referenced as a previous work that formulates NER as a classification problem, which the authors of this paper aim to differ from. Confidence score: 0.7\n- \"Spanner: Named entity recognition as span prediction\" [b13] - Similarly, this work is also mentioned as a previous approach to NER that the authors aim to differ from. Confidence score: 0.7\n- \"Signature verification using a Siamese time delay neural network\" [b2] - This work is mentioned as the origin of the use of bi-encoder, which is a crucial part of the proposed approach in this paper. Confidence score: 0.8\n- \"Learning a similarity metric discriminatively with application to face verification\" [b6] - This work is also mentioned in connection with the use of bi-encoder. Confidence score: 0.8\n- \"Named entity recognition with partially annotated training data\" [b34] - This paper is mentioned in connection with the issue of false negative noises when training data is partially annotated, which this paper aims to tackle. Confidence score: 0.7\n\nIn JSON format, the results would be:\n\n{\n  \"b21\": 0.9,\n  \"b56\": 0.9,\n  \"b4\": 0.7,\n  \"b13\": 0.7,\n  \"b2\": 0.8,\n  \"b6\": 0.8,\n  \"b34\": 0.7\n}", "Based on the text of the paper and the titles of the references, the most pertinent source papers that inspired the given paper are as follows:\n\n- \"Dense passage retrieval for open-domain question answering [b21]\": The authors explicitly mention that their work is inspired by the success of this paper in the field of open-domain question answering.\n- \"Scalable zero-shot entity linking with dense entity retrieval [b56]\": This paper is mentioned right alongside [b21] as an inspiration for the proposed model.\n- \"Learning a similarity metric discriminatively with application to face verification [b6]\": The use of bi-encoder in the paper dates back to this work, which implies a significant influence.\n- \"Learning dense representations for entity retrieval [b15]\": This paper is mentioned in the context of various tasks where the bi-encoder architecture has been used.\n- \"BERT: Pre-training of deep bidirectional transformers for language understanding [b9]\": The authors use BERT as the base for their model, which indicates its substantial influence on the work.\n- \"Spanner: Named entity re-recognition as span prediction [b13]\": The authors distinguish their work from this paper, which implies that it has been a significant reference point for the development of their algorithm.\n- \"Learning dense representations for entity retrieval [b19]\": This paper is mentioned in the context of various tasks where the bi-encoder architecture has been used.\n\nHere is the result in JSON format:\n\n{\n  \"b21\": 1,\n  \"b56\": 0.9,\n  \"b6\": 0.8,\n  \"b15\": 0.8,\n  \"b9\": 0.7,\n  \"b13\": 0.7,\n  \"b19\": 0.6\n}\n\nThese confidence scores are based on the degree of explicit reference to these papers in the text, the context in which they are mentioned, and their relevance to the main contributions of the paper.", "Based on the provided text and titles of the references, I identified the following source papers:\n\n{ \"b21\": 0.9, \"b56\": 0.9, \"b62\": 0.9, \"b4\": 0.8, \"b33\": 0.8, \"b27\": 0.8, \"b13\": 0.8, \"b8\": 0.7, \"b1\": 0.7 }\n\nThe \"b21\", \"b56\", and \"b62\" papers are the most influential as they are directly mentioned as inspirations for the paper's approach. The \"b4\", \"b33\", \"b27\", and \"b13\" papers are also influential as they are compared to the paper's novel approach in contrastive learning for NER. The \"b8\" and \"b1\" papers are included as they discuss challenges the paper addresses such as training data that is partially annotated. The scores indicate the confidence with which these papers are considered influential, with 1 being the highest confidence.", "Based on the paper's text and the titles of the references, the following source papers seem to have most inspired the authors:\n\n1. \"Dense passage retrieval for open-domain question answering\" [b21]: The authors mention that their work is influenced by the success of this paper in open-domain question answering.\n2. \"Scalable zero-shot entity linking with dense entity retrieval\" [b56]: The authors mention that their work is inspired by the success of this paper in entity linking.\n3. \"Named entity recognition with bidirectional LSTM-CNNs\" [b4]: This paper is mentioned as an example of existing work that formulates NER as a classification problem, which the authors of the given paper diverge from.\n4. \"Spanner: named entity recognition as span prediction\" [b13]: This paper is mentioned as an example of existing work that formulates NER as a classification problem, which the authors of the given paper diverge from.\n5. \"Learning dense representations for entity retrieval\" [b15]: This paper is mentioned in the context of the use of bi-encoder, which is a key component of the authors' approach.\n6. \"Attention is all you need\" [b49]: This paper is mentioned as the source of the Transformer model, which is used in the authors' bi-encoder.\n7. \"BERT: pre-training of deep bidirectional transformers for language understanding\" [b9]: This paper is mentioned as the source of the BERT model, which is used in the authors' bi-encoder.\n\nHere is the result in the requested format:\n\n{\n\"b21\": 1,\n\"b56\": 1,\n\"b4\": 0.8,\n\"b13\": 0.8,\n\"b15\": 1,\n\"b49\": 0.9,\n\"b9\": 0.9\n}"], "6389d6fe90e50fcafdffc634": ["Based on the text of the paper, the most pertinent source papers seem to be:\n\n1. \"bingo spatial data prefetcher\" [b1] and \"accurately and maximally prefetching spatial data access patterns with bingo\" [b10]. These papers are mentioned multiple times in the text and it's clear they have had a significant influence on the paper's research and methodology, especially in the design of the prefetcher and the pattern merging strategy.\n\n2. \"spatial memory streaming\" [b8]. This is another significant source paper which the author's prefetcher is based on and it's also used to explain the background of their research.\n\n3. \"pythia a customizable hardware prefetching framework using online reinforcement learning\" [b6]. This paper is compared with the author's prefetcher in the experiment results, indicating it's a big influence on their work.\n\n4. \"pangloss a novel markov chain prefetcher\" [b4] and \"best offset hardware prefetching\" [b2]. Both papers are related to the algorithm proposed by the author.\n\n5. \"mcf homepage\" [b19]. This paper is directly referenced in the discussion of Observation 3.\n\nBased on these findings, I would suggest the following JSON output:\n\n{\n\"b1\": 0.9,\n\"b10\": 0.8,\n\"b8\": 0.85,\n\"b6\": 0.8,\n\"b4\": 0.7,\n\"b2\": 0.7,\n\"b19\": 0.75\n}\n\nThe confidence scores assigned to each reference number reflect the estimated influence of each source on the paper's main contributions. These scores are subjective and could be adjusted based on a deeper analysis of the paper and its references.", "Based on the text and titles of the references, the main source papers seem to be:\n\n1. \"bingo spatial data prefetcher\" [b1]\n2. \"pythia a customizable hardware prefetching framework using online reinforcement learning\" [b6]\n3. \"spatial memory streaming\" [b8]\n4. \"dspatch dual spatial pattern prefetcher\" [b12]\n5. \"mcf homepage\" [b19]\n6. \"not quite my temp matching prefetches to memory access times\" [b13]\n\nThese papers are mentioned in the text as sources of algorithms and methods that have directly influenced the author's research. They all deal with prefetching, which is the main topic of the paper. \n\nHere is the result in JSON format:\n\n```\n{\n    \"b1\": 0.9,\n    \"b6\": 0.9,\n    \"b8\": 0.8,\n    \"b12\": 0.8,\n    \"b19\": 0.7,\n    \"b13\": 0.7\n}\n```\n\nThe confidence scores are based on the frequency and context of their mentions in the text. Note that the actual confidence scores may vary depending on more specific criteria or further analysis of the paper.", "Based on the given text and the title of each reference, the following source papers can be identified:\n\n1. \"bingo spatial data prefetcher\" [b1]\n2. \"spatial memory streaming\" [b8]\n3. \"pythia a customizable hardware prefetching framework using online reinforcement learning\" [b6]\n4. \"accurately and maximally prefetching spatial data access patterns with bingo\" [b10]\n5. \"effective hardware based data prefetching for high performance processors\" [b31]\n6. \"efficiently prefetching complex address patterns\" [b32]\n7. \"efficient metadata management for irregular data prefetching\" [b5]\n\nThese papers are most directly related to the paper's novel contributions and have been indicated as sources of inspiration or motivation in the text. \n\nHere is the result in JSON format:\n\n{\n    \"b1\": 0.9,\n    \"b8\": 0.8,\n    \"b6\": 0.8,\n    \"b10\": 0.8,\n    \"b31\": 0.75,\n    \"b32\": 0.75,\n    \"b5\": 0.7\n} \n\nThe confidence score is given based on the extent of direct relevance to the paper's core contributions and the frequency of their mention in the text.", "Based on the information given in the text and the titles of the references, the source papers that most likely inspired the paper are:\n\n- \"bingo spatial data prefetcher\" [b1]\n- \"pythia a customizable hardware prefetching framework using online reinforcement learning\" [b6]\n- \"spatial memory streaming\" [b8]\n- \"accurately and maximally prefetching spatial data access patterns with bingo\" [b10]\n- \"dspatch dual spatial pattern prefetcher\" [b12]\n- \"not quite my temp matching prefetches to memory access times\" [b13]\n- \"mcf homepage\" [b19]\n\nThe confidence scores are based on the frequency of the references and the relevance of the titles to the 'Pattern Merging Prefetcher (PMP)' proposed by the author. The confidence score for each reference is:\n\n- [b1] = 0.85\n- [b6] = 0.75\n- [b8] = 0.8\n- [b10] = 0.9\n- [b12] = 0.7\n- [b13] = 0.65\n- [b19] = 0.6\n\nJSON format:\n\n{\n\"[b1]\": 0.85,\n\"[b6]\": 0.75,\n\"[b8]\": 0.8,\n\"[b10]\": 0.9,\n\"[b12]\": 0.7,\n\"[b13]\": 0.65,\n\"[b19]\": 0.6\n}", "Based on the text, the most pertinent source papers that inspired the given paper are:\n\n1. \"bingo spatial data prefetcher\" [b1]\n2. \"pythia a customizable hardware prefetching framework using online reinforcement learning\" [b6]\n3. \"spatial memory streaming\" [b8]\n4. \"dspatch dual spatial pattern prefetcher\" [b12]\n\nThe JSON format result is:\n\n{\n    \"b1\": 1,\n    \"b6\": 0.8,\n    \"b8\": 1,\n    \"b12\": 0.8\n}"], "63881b9290e50fcafd3db3f8": ["Based on the provided text and references, the primary challenges outlined in the paper are the memory limitations on deploying deep neural networks (DNNs) on microcontroller units (MCUs). The authors propose a novel method, a partial execution compiler, to address this challenge. The method involves splitting operators along the channel dimension, eliminating the need for full memory allocation for operators, and thus reducing memory usage. This approach is combined with execution-pruning and Quantization Aware Training (QAT) for further memory reduction.\n\nThe phrase \"inspired by\" was found in the text, suggesting the possible influence of another paper. The paper in question is \"MobileNet-v2 [b25]\", which first described a memory-efficient trick to execute the inverted residual block (IRB), a concept the authors in the given text have expanded on to develop their partial execution compiler.\n\nThe text also mentions \"MCUNet-v2 [b21]\", indicating this work inspired the authors by avoiding layer-by-layer execution in favor of executing memory-intense layers using one input tile (\"patch\") at a time. However, the authors propose a different approach, suggesting this paper might have been a motivation rather than a direct source. \n\nAlso, there is a discussion of \"TFLM [b7]\", \"TVM [b5]\", and other works like \"[b17]\". These papers seem to be related to the methods the authors are using or comparing their work against. \n\nBased on the text, the following source papers seem to be most pertinent:\n\n- \"MobileNet-v2 [b25]\": Confidence Score - 1.0 (Direct Inspiration)\n- \"MCUNet-v2 [b21]\": Confidence Score - 0.8 (Motivation)\n- \"TVM [b5]\": Confidence Score - 0.5 (Comparison)\n- \"TFLM [b7]\": Confidence Score - 0.5 (Comparison)\n- \"[b17]\": Confidence Score - 0.5 (Comparison)\n\nPlease note that these confidence scores are subjective and based on the interpretation of the text. The actual influence of these papers on the author's work might be different.\n\nIn JSON format:\n\n```\n{\n\"b25\": 1.0,\n\"b21\": 0.8,\n\"b5\": 0.5,\n\"b7\": 0.5,\n\"b17\": 0.5\n}\n```", "Based on the text of the paper, the primary challenges outlined are the high memory usage of neural network inference and the inability to deploy models with large activation tensors on microcontrollers. The paper proposes to tackle these challenges with a novel model compiler, PEX, which executes operators partially, allowing for more memory-efficient network execution schedules. \n\nThe paper mentions that the idea of partial execution was first described in MobileNet-v2 [b25] as a trick to execute the inverted residual block (IRB), which was a significant source of inspiration for the authors. This can be inferred from the sentence \"A particular instance of this was first described in MobileNet-v2 [b25] as a memory-efficient trick to execute the inverted residual block (IRB).\"\n\nThe paper also indicates that their approach is different from the one proposed in MCUNet-v2 [b21], which executes memory-intense layers using one input tile at a time. This can be inferred from the sentence \"MCUNet-v2 [b21] forgoes layer-by-layer execution in favour of executing memory-intense layers using one input tile (\"patch\") at a time; we discuss this execution style further on.\" \n\nMoreover, the authors mention that they were inspired by the methods developed for model parallelism such as those in [b9], [b10] and [b32], from which they borrowed the idea of splitting computation to save memory.\n\nTherefore, the most pertinent source papers seem to be [b25], [b21], [b9], [b10] and [b32].\n\nThe json format result is:\n{\n\"b25\": 0.9,\n\"b21\": 0.8,\n\"b9\": 0.7,\n\"b10\": 0.7,\n\"b32\": 0.7\n}", "Based on the text of the academic paper, the source papers that mostly inspired the author in developing the paper's core contributions are as follows:\n\n- The first source paper is \"Memory-efficient patch-based inference for tiny deep learning\" [b21]. The author mentioned this paper when discussing the technique of executing memory-intense layers using one input tile at a time, which is a method that this paper aims to improve upon.\n\n- The second source paper is \"MobileNet-v2 inverted residuals and linear bottlenecks\" [b25]. The author specifically mentioned that the idea of partial execution was first described in this paper. This paper seems to have significantly influenced the new method proposed by the author.\n\n- The third source paper is \"Neural networks on microcontrollers: Saving memory at inference via operator reordering\" [b17]. The author mentioned this paper when discussing methods that have been developed to reduce SRAM usage of neural networks.\n\n- The fourth source paper is \"TVM: End to end optimization stack for deep learning\" [b5]. The author mentioned this paper when discussing model compilers for various targets, including microcontroller platforms, which seems to be relevant to the paper's contributions.\n\nThe JSON format answer is as follows:\n\n{ \"b21\": 0.9, \"b25\": 0.95, \"b17\": 0.85, \"b5\": 0.85 } \n\nThe confidence score for each source paper is estimated based on the level of influence it seems to have on the author's work, with a score closer to 1 indicating a higher level of influence.", "Based on the given text, the most pertinent source papers that heavily influenced the main contributions of the paper are:\n\n1. \"TVM: End-to-end optimization stack for deep learning\" [b5]: This paper seems to be the main inspiration for developing the PEX model compiler, which is a core contribution of the paper.\n2. \"Memory-efficient patch-based inference for tiny deep learning\" [b21]: The author of the paper compares the PEX model compiler with the method presented in this paper, and it seems that it has motivated the author to develop a more memory-efficient model.\n3. \"Neural networks on microcontrollers: Saving memory at inference via operator reordering\" [b17]: This paper seems to have motivated the part of the research related to memory efficiency.\n4. \"MobileNet V2: Inverted residuals and linear bottlenecks\" [b25]: This paper seems to have inspired the partial execution concept which is a key contribution of the paper.\n\nHere is the result in the JSON format:\n\n{\n\"b5\": 0.9,\n\"b21\": 0.8,\n\"b17\": 0.7,\n\"b25\": 1\n} \n\nThe confidence scores are subjective and are assigned based on the degree of influence perceived from the text of the paper. The scores can range from 0 to 1, with 1 indicating a high level of influence.", "Based on the text provided, the following source papers seem to have been the most influential in the development of the paper's core concepts and methodologies:\n\n1. \"Deep learning on microcontrollers\": The paper heavily discusses the challenges with deploying DNN models on MCUs, particularly the memory limitations. This paper is likely to have provided the foundational understanding of the challenges in this area. (Reference: b23, confidence score: 0.9)\n\n2. \"Neural networks on microcontrollers saving memory at inference via operator reordering\": The paper's approach to reducing memory usage via partial execution appears to have been influenced by the strategies outlined in this paper. (Reference: b17, confidence score: 0.85)\n\n3. \"Memoryefficient patch based inference for tiny deep learning\": The paper mentions this source when comparing its approach of executing operators partially to produce and/or consume one feature/channel of data at a time to the patch-based method. (Reference: b21, confidence score: 0.8)\n\n4. \"TVM end to end optimization stack for deep learning\": The paper seems to use some of the optimization techniques mentioned in this source paper. (Reference: b5, confidence score: 0.75)\n\n5. \"Mobilenet v inverted residuals and linear bottlenecks\": The paper mentions this source when discussing the concept of partial execution. (Reference: b25, confidence score: 0.7)\n\n6. \"Neural network architectures for deploying tinyml applications on commodity microcontrollers\": It seems like this paper was a source of inspiration for the author in terms of model discovery and compression. (Reference: b3, confidence score: 0.65)\n\nThe JSON format of the output is:\n\n```\n{\n  \"b23\": 0.9,\n  \"b17\": 0.85,\n  \"b21\": 0.8,\n  \"b5\": 0.75,\n  \"b25\": 0.7,\n  \"b3\": 0.65\n}\n```"], "640fe64790e50fcafd9e2811": ["{\n    \"b0\": 0.9,\n    \"b19\": 0.9,\n    \"b20\": 0.8,\n    \"b28\": 0.8,\n    \"b2\": 0.7,\n    \"b27\": 0.7,\n    \"b45\": 0.6\n}", "{\n  \"b19\": 0.95,\n  \"b20\": 0.95,\n  \"b28\": 0.95,\n  \"b0\": 0.90,\n  \"b2\": 0.85\n}", "{\n  \"b0\": 0.9,\n  \"b19\": 0.9,\n  \"b20\": 0.85,\n  \"b22\": 0.8,\n  \"b28\": 0.85,\n  \"b47\": 0.75\n}", "{\n    \"b19\": 0.95,\n    \"b0\": 0.9,\n    \"b2\": 0.85,\n    \"b28\": 0.8,\n    \"b20\": 0.75,\n    \"b47\": 0.7\n}", "{\n  \"b0\": 0.9,\n  \"b19\": 0.9,\n  \"b20\": 0.9,\n  \"b28\": 0.9,\n  \"b2\": 0.8,\n  \"b27\": 0.7,\n  \"b45\": 0.7\n}"], "63d9d87390e50fcafd57e29e": ["Based on the text of the academic paper and the information given about the references, the source papers that inspired the author the most appear to be:\n\n1. \"depgraph towards any structural pruning\" - The paper's title and the content suggest that this is a primary source of inspiration for the study. Confidence Score: 0.9\n\n2. \"the transitive reduction of a directed graph\" - This paper appears to be a key reference as it provides the theoretical foundation for the 'Dependency Graph' concept. Confidence Score: 0.8\n\n3. \"residual connection requires the output of two convolutional layers to share the same number of channels and thus forces them to be pruned together\" - This paper is directly mentioned in the main text and plays a key role in the algorithm proposed by the author. Confidence Score: 0.7\n\n4. \"gate decorator global filter pruning method for accelerating deep convolutional neural networks\" - This paper seems to be a key source for the 'structural pruning' concept and the author's novel contributions. Confidence Score: 0.65\n\n5. \"group fisher pruning for practical network compression\" - This paper is mentioned in the text as a comparative work, which suggests it might have influenced the research. Confidence Score: 0.6\n\n6. \"towards efficient model compression via learned global ranking\" - The author mentioned the challenge of model compression, making this paper a likely source. Confidence Score: 0.55\n\n7. \"pruning filters for efficient convnets\" - This paper might be a source as it is directly related to the topic of the paper, i.e., pruning filters for network acceleration. Confidence Score: 0.5\n\n8. \"network pruning via performance maximization\" - This paper might have influenced the author's method of network pruning for improved performance. Confidence Score: 0.45\n\nThe result in the JSON format would be:\n{\"depgraph towards any structural pruning\": 0.9, \"the transitive reduction of a directed graph\": 0.8, \"residual connection requires the output of two convolutional layers to share the same number of channels and thus forces them to be pruned together\": 0.7, \"gate decorator global filter pruning method for accelerating deep convolutional neural networks\": 0.65, \"group fisher pruning for practical network compression\": 0.6, \"towards efficient model compression via learned global ranking\": 0.55, \"pruning filters for efficient convnets\": 0.5, \"network pruning via performance maximization\": 0.45}", "Based on the given paper text and the titles of the references, the source papers that seem to have inspired this paper the most are:\n\n1. \"depgraph towards any structural pruning\" - This paper clearly aligns with the topic of the given paper and seems to have provided a significant amount of inspiration for it. Confidence Score: 0.9\n\n2. \"the transitive reduction of a directed graph\" - This paper seems to have provided the theoretical basis for the proposed algorithm in the given paper. Confidence Score: 0.7\n\n3. \"centripetal sgd for pruning very deep convolutional networks with complicated structure\" - This paper appears to have significantly influenced the discussion on pruning in the given paper. Confidence Score: 0.7\n\n4. \"resnet can be pruned introducing network purification and unused path removal p rm after weight pruning\" - This paper seems to have provided inspiration for the context of pruning in the given paper. Confidence Score: 0.7\n\n5. \"group fisher pruning for practical network compression\" - This paper seems to have contributed to the given paper's discussion on grouping in pruning. Confidence Score: 0.7\n\n6. \"importance estimation for neural network pruning\" - This paper appears to be relevant to the given paper's discussion on the importance of pruning. Confidence Score: 0.7\n\nThe JSON format of the result is:\n\n{\n  \"b0\": 0.7,\n  \"b3\": 0.7,\n  \"b36\": 0.7,\n  \"b30\": 0.7,\n  \"b38\": 0.7,\n  \"None\": 0.9\n}\n\nNote: The confidence scores are estimated based on the relevance of the topic and the phrases used in the given paper.", "Based on the provided text, the papers that most likely inspired this work are:\n\n1. \"depgraph towards any structural pruning\" - Confidence Score: 0.9\n2. \"the transitive reduction of a directed graph\" - Confidence Score: 0.8\n3. \"pruning filters for efficient convnets\" - Confidence Score: 0.7\n4. \"resnet can be pruned introducing network purification and unused path removal p rm after weight pruning\" - Confidence Score: 0.7\n5. \"an image is worth x words transformers for image recognition at scale\" - Confidence Score: 0.6\n6. \"long short term memory supervised sequence labelling with recurrent neural networks\" - Confidence Score: 0.6\n7. \"deep residual learning for image recognition\" - Confidence Score: 0.6\n8. \"group fisher pruning for practical network compression\" - Confidence Score: 0.6\n\nThe confidence scores are based on the frequency and context of references in the text, with higher scores indicating a stronger connection to the paper's main contributions.\n\nHere is the result in JSON format:\n{\n\"depgraph towards any structural pruning\": 0.9,\n\"the transitive reduction of a directed graph\": 0.8,\n\"pruning filters for efficient convnets\": 0.7,\n\"resnet can be pruned introducing network purification and unused path removal p rm after weight pruning\": 0.7,\n\"an image is worth x words transformers for image recognition at scale\": 0.6,\n\"long short term memory supervised sequence labelling with recurrent neural networks\": 0.6,\n\"deep residual learning for image recognition\": 0.6,\n\"group fisher pruning for practical network compression\": 0.6\n}", "Based on the text of the paper and the titles of the references, the most pertinent source papers that inspired the paper are:\n\n1. \"depgraph towards any structural pruning\" - The paper has the same title as the research, suggesting that it may be the main source of inspiration. Confidence score: 0.8\n\n2. \"the transitive reduction of a directed graph\" - The paper mentions the concept of \"Dependency Graph\" and \"transitive reduction\" which might have been inspired by this paper. Confidence score: 0.6\n\n3. \"pruning filters for efficient convnets\" - The paper frequently discusses pruning, a concept central to this paper. Confidence score: 0.6\n\n4. \"network pruning via performance maximization\" - The paper's focus on network pruning and performance maximization aligns with the research's objectives. Confidence score: 0.6\n\n5. \"channel pruning for accelerating very deep neural networks\" - The concept of network pruning and acceleration is discussed in the paper, which might have been inspired by this paper. Confidence score: 0.5\n\n6. \"group fisher pruning for practical network compression\" - The paper mentions \"grouped parameters\" and \"structural pruning\", which are key themes in the research. Confidence score: 0.6\n\n7. \"resnet can be pruned introducing network purification and unused path removal p rm after weight pruning\" - The paper uses ResNet for evaluation and discusses concepts of pruning, which might have been inspired by this paper. Confidence score: 0.5\n\nThe returned result in the json format is:\n{\n\"b0\": 0.6,\n\"b3\": 0.6,\n\"b20\": 0.6,\n\"b30\": 0.6,\n\"b36\": 0.5,\n\"b62\": 0.5,\n\"None\": 0.8\n}", "Based on the text, the following source papers have inspired the author the most:\n\n1. \"depgraph towards any structural pruning\" - This is the primary paper from which the authors base their work. The authors propose a dependency graph (DepGraph) to explicitly model the interdependency between layers in neural networks, which is essential for their task of structural pruning. This paper directly contributes to the novel contributions of the authors' work. Confidence Score: 1.0\n\n2. \"centripetal sgd for pruning very deep convolutional networks with complicated structure\" (b3) - The authors refer to the concept of structural pruning in this paper and use it to develop their method. Confidence Score: 0.8  \n\n3. \"residual learning for image recognition\" (b16) - The authors refer to the residual connection in their paper which requires the output of two convolutional layers to share the same number of channels and thus forces them to be pruned together. Confidence Score: 0.8\n\n4. \"densely connected convolutional networks\" (b22) - The authors refer to the dependency in modern models which can be infinitely complex, this paper seems to have significant influence on their work. Confidence Score: 0.7\n\n5. \"pruning filters for efficient convnets\" (b27) - The authors discuss the manually-designed grouping scheme from this paper and explain its limitations, which motivated them to develop their own novel method. Confidence Score: 0.6\n\n6. \"group fisher pruning for practical network compression\" (b30) - The authors were inspired by the pilot works proposed in this paper to resolve the complicated relationship between layers and leverage the grouping property in order to improve structural pruning performance. Confidence Score: 0.7\n\n7. \"an image is worth x words transformers for image recognition at scale\" (b9) - The authors apply their proposed method to several popular architectures including the Vision Transformer from this paper, this indicates that they were influenced by it. Confidence Score: 0.6\n\n8. \"long short term memory supervised sequence labelling with recurrent neural networks\" (b11) - This paper is another source of inspiration for the authors as they apply their method to LSTM for text classification. Confidence Score: 0.6\n\nThe final result in JSON format is:\n\n{\n\"None\": 1.0,\n\"b3\": 0.8,\n\"b16\": 0.8,\n\"b22\": 0.7,\n\"b27\": 0.6,\n\"b30\": 0.7,\n\"b9\": 0.6,\n\"b11\": 0.6\n}"], "6424fe3390e50fcafd78b58e": ["Based on the text and the author notes, the following papers seem to be the most pertinent sources for the given paper:\n\n1. \"stochastic blockmodel approximation of a graphon theory and consistent estimation\" [b0]: The paper specifically mentions the use of graphons, which is a key component of the proposed W2PGNN framework.\n\n2. \"strategies for pre training graph neural networks\" [b14]: This paper is mentioned in the context of the problem the authors are addressing - the challenge of determining when to pre-train GNNs.\n\n3. \"bert pre training of deep bidirectional transformers for language understanding\" [b4]: This paper is referred to as an example of successful pre-training in other fields, which the authors are trying to emulate in the graph pre-training context.\n\n4. \"gpt gnn generative pre training of graph neural networks\" [b15]: This paper is referenced as an example of recent advances in pre-training GNNs.\n\n5. \"pre training graph neural networks for generic structural feature extraction\" [b16]: The paper mentions this work in the context of fine-tuning strategies to enhance downstream performance.\n\n6. \"gcc graph contrastive coding for graph neural network pre training\" [b31]: This paper is referred to as an example of cross-domain pre-training strategies.\n\n7. \"transferability properties of graph neural networks\" [b35]: This paper is mentioned in the section discussing the limitations of previous attempts to measure the transferability of GNNs, which is a key problem the authors are addressing.\n\nHere are the reference papers in the required JSON format:\n\n{\n  \"b0\": 0.9,\n  \"b14\": 0.85,\n  \"b4\": 0.8,\n  \"b15\": 0.85,\n  \"b16\": 0.8,\n  \"b31\": 0.85,\n  \"b35\": 0.8\n}\n\nThe confidence scores are estimated based on the context in which these papers are mentioned in the text, with higher scores indicating a stronger influence on the main contributions of the paper.", "Based on the given information, the primary challenge outlined in the paper is to determine when to pre-train Graph Neural Networks (GNNs), and the proposed solution is a framework called W2PGNN. The paper also mentions various strategies and models used in the context of pre-training and fine-tuning GNNs.\n\nAnalyzing the text and the titles of the references, the most relevant source papers could be:\n\n1. \"strategies for pre training graph neural networks\" [b14]: The paper refers to various pre-training and fine-tuning strategies, which could have been inspired by this source.\n2. \"graph contrastive learning with augmentations\" [b51]: This could be a source of inspiration for the paper as it deals with graph pre-training models.\n3. \"gpt gnn generative pre training of graph neural networks\" [b15]: The paper mentions generative mechanisms, which could have been inspired by this source.\n4. \"towards effective and generalizable fine tuning for pre trained molecular graph models\" [b46]: This paper could be a source as it also deals with fine-tuning strategies for GNNs.\n5. \"graphons: A new structural primitive for understanding complex networks\" [b0]: This paper could have provided the theoretical foundation for using graphons in the proposed method.\n\nThe JSON format for the result would be:\n{\n    \"b14\": 0.8,\n    \"b51\": 0.7,\n    \"b15\": 0.9,\n    \"b46\": 0.7,\n    \"b0\": 0.9\n}\n\nPlease note that these are educated guesses based on the given text and should be verified for accuracy. The confidence scores are subjective and can vary based on a deeper understanding of the content.", "The source papers that inspire the author most based on the text are:\n\n- \"b14: strategies for pre training graph neural networks\": The author specifically mentions this work when discussing the issue of negative transfer in graph pre-training. The phrase \"especially when the graphs used for pre-training are not necessarily from the same domain as the downstream data [b14] [b31]\" indicates that this paper heavily influenced their thinking on the issue of domain discrepancy in graph pre-training. Confidence: 0.9\n\n- \"b31: gcc graph contrastive coding for graph neural network pre training\": This paper is also cited in the context of negative transfer in graph pre-training, indicating its influence on the author's understanding of the problem space. Confidence: 0.9\n\n- \"b0: stochastic blockmodel approximation of a graphon theory and consistent estimation\": This paper is mentioned in the context of explaining the concept of graphons, which are a key part of the proposed W2PGNN framework. Confidence: 0.8\n\n- \"b24: learning to pre train graph neural networks\": The author cites this work when discussing the current research landscape, stating that \"recent efforts focus on what to pre-train and how to pre-train, i.e., design/adopt graph pre-training models with a variety of self-supervised tasks to capture different patterns [b24] [b31] [b51]\". This suggests that this paper has significantly influenced their understanding of the current state of graph pre-training research. Confidence: 0.8\n\n- \"b51: graph contrastive learning with augmentations\": This paper is cited alongside b24 and b31, indicating that it has also played a role in shaping the author's understanding of the current state of graph pre-training research. Confidence: 0.8\n\n- \"b47: learning graphon autoencoders for generative graph modeling\": The author cites this paper when discussing the use of the Gromov-Wasserstein distance in their proposed framework, suggesting that it has influenced their methodological approach. Confidence: 0.7\n\nBased on the aforementioned analysis, the JSON format answer is:\n\n{\n\"b14\": 0.9,\n\"b31\": 0.9,\n\"b0\": 0.8,\n\"b24\": 0.8,\n\"b51\": 0.8,\n\"b47\": 0.7\n}", "Based on the text, the following are the source papers that most inspired the author:\n\n1. \"Strategies for pre-training graph neural networks\" [b14] \n2. \"Bert: Pre-training of deep bidirectional transformers for language understanding\" [b4]\n3. \"Stochastic blockmodel approximation of a graphon: Theory and consistent estimation\" [b0]\n4. \"GPT-GNN: Generative pre-training of graph neural networks\" [b15]\n5. \"Graph contrastive learning with augmentations\" [b51]\n6. \"GCC: Graph contrastive coding for graph neural network pre-training\" [b31]\n7. \"Transferability properties of graph neural networks\" [b35]\n8. \"Graph contrastive learning automated\" [b50]\n\nThe confidence scores for each source paper would be determined based on how directly they contributed to the novel contributions of the author's own paper, and this would typically require a more detailed analysis of the text. However, the above papers are those that were most directly mentioned in the context of the author's work. \n\nPlease note that the author of the paper heavily refers to the work in [b14] and [b4], which suggests a strong influence from these papers on the author's methodology. The other papers are inferred based on the key phrases and their context within the text.", "Based on the text and the references provided, the most pertinent source papers that inspired the given paper are:\n\n1. \"strategies for pre training graph neural networks\" [b14]: The paper discusses the issue of when to pre-train, which is a significant focus of the given paper. It seems that the approach proposed in the given paper is a response or an extension to the challenges outlined in this paper. Confidence Score: 0.9\n\n2. \"gpt gnn generative pre training of graph neural networks\" [b15]: This paper is also closely related to the concept of pre-training GNNs, which is a central theme of the given paper. Confidence Score: 0.85\n\n3. \"gcc graph contrastive coding for graph neural network pre training\" [b31]: The paper appears to deal with similar problems and uses similar methods as the given paper. The given paper seems to build upon this work. Confidence Score: 0.8\n\n4. \"learning to pre train graph neural networks\" [b24]: The paper deals with the concept of learning to pre-train, which is closely related to the topic of the given paper. Confidence Score: 0.75\n\n5. \"gromov wasserstein averaging of kernel and distance matrices\" [b30]: The given paper seems to use the methods proposed in this paper for approximating the feasibility of pre-training. Confidence Score: 0.7\n\n6. \"transferability properties of graph neural networks\" [b35]: This paper seems to be directly related to the concept of transferability discussed in the given paper. Confidence Score: 0.65\n\n7. \"when does self supervision help graph convolutional networks\" [b52]: This paper seems to provide important context and background for the given paper's discussion of self-supervision in GNNs. Confidence Score: 0.6\n\nThe result in json format is:\n\n{\n\"b14\": 0.9,\n\"b15\": 0.85,\n\"b31\": 0.8,\n\"b24\": 0.75,\n\"b30\": 0.7,\n\"b35\": 0.65,\n\"b52\": 0.6\n}"], "62bab8f95aee126c0f6afca5": ["Based on the text and the title of references, the following papers appear to be the most pertinent source papers:\n\n1. \"latent augmentation for better graph self supervised learning\" - The paper is directly about the topic of the paper and is likely to be one of the primary sources of inspiration.\n2. \"modals modality agnostic automated data augmentation in the latent space\" - This paper is likely a source as the paper discusses the application of data augmentation in the latent space.\n3. \"deep wiener deconvolution wiener meets deep learning for image deblurring\" - This paper is mentioned as a motivation for introducing the classical deconvolutional technique, wiener kernel, into GDN.\n4. \"node vec scalable feature learning for networks\" - As the paper discusses graph SSL, this source could have contributed to the development of the method.\n5. \"contrastive multi view representation learning on graphs\" - The paper talks about contrastive learning which is a key component of the method.\n6. \"deconvolutional networks on graph data\" - This paper is likely a source as it discusses deconvolutional networks which are a component of the method.\n7. \"graph self supervised learning a survey\" - This paper provides a survey on the topic of the paper and is likely to be one of the primary sources of inspiration.\n8. \"variational graph auto encoders\" - The paper discusses graph autoencoders which is a component of the method.\n\nHere is the json format of the result:\n\n{\n\"b0\": 0.9,\n\"b1\": 0.85,\n\"b3\": 0.8,\n\"b6\": 0.75,\n\"b7\": 0.7,\n\"b16\": 0.65,\n\"b19\": 0.75,\n\"b12\": 0.7\n}", "Based on the provided text and titles of references, the source papers that most likely inspired this work are:\n\n1. \"Latent augmentation for better graph self supervised learning\"\n2. \"Modals modality agnostic automated data augmentation in the latent space\"\n3. \"Dataset augmentation in feature space\"\n4. \"Deep wiener deconvolution wiener meets deep learning for image deblurring\"\n5. \"Deconvolutional networks on graph data\"\n6. \"Graph self-supervised learning a survey\"\n7. \"Symmetric graph convolutional autoencoder for unsupervised graph representation learning\"\n8. \"Graph contrastive learning with adaptive augmentation\"\n\nNow, I will generate the confidence score for each of the source papers:\n\n{ \"b1\": 0.8, \"b2\": 0.7, \"b3\": 0.9, \"b16\": 0.85, \"b19\": 0.75, \"b23\": 0.8, \"b51\": 0.9 }", "Based on the given paper text and titles of references, the most pertinent source papers that inspire the author are:\n\n1. \"latent augmentation for better graph self supervised learning\" - This paper seems to be the central theme of the given paper. The author is discussing self-supervised learning in graphs and also discusses augmentation in the latent space. The confidence score for this paper would be 0.9 as it is highly relevant to the main topic of the paper.\n\n2. \"modals modality agnostic automated data augmentation in the latent space\" - This paper is likely to be another source of inspiration for the author as it discusses data augmentation in the latent space, which is a method used in the given paper. The confidence score for this paper would be 0.8 due to the methodological similarity.\n\n3. \"deep wiener deconvolution wiener meets deep learning for image deblurring\" - This paper introduces the Wiener deconvolution technique that the author adopted to solve the problem of image deblurring. The confidence score for this paper would be 0.7 because of the direct use of the technique in the paper.\n\n4. \"latent adversarial training of graph convolutional networks\" - This paper may have influenced the author's decision to use latent adversarial training in Graph Neural Networks. The confidence score for this paper would be 0.7 as the author uses the technique in the paper.\n\n5. \"contrastive multi view representation learning on graphs\" - This paper might be a source of inspiration as it discusses contrastive learning, which the author mentioned as a dominant technique in SSL. The confidence score for this paper would be 0.7 due to the methodological similarity.\n\n6. \"variational graph auto encoders\" - This paper is likely a source of inspiration as it introduces the concept of Graph Auto Encoders (GAE), which the author used in the given paper. The confidence score for this paper would be 0.7 due to the use of GAE in the paper.\n\n7. \"graph self-supervised learning a survey\" - This paper might have been used by the author for understanding the different methods in SSL for graph data. The confidence score for this paper would be 0.6 due to its comprehensive review on the topic which is related to the given paper.\n\n8. \"graph contrastive learning with adaptive augmentation\" - This paper might be an inspiration to the author as it talks about contrastive learning with adaptive augmentation, which is a method used in the given paper. The confidence score for this paper would be 0.6 due to the methodological similarity.\n\nThe return result in the json format is: \n```\n{\n    \"latent augmentation for better graph self supervised learning\": 0.9,\n    \"modals modality agnostic automated data augmentation in the latent space\": 0.8,\n    \"deep wiener deconvolution wiener meets deep learning for image deblurring\": 0.7,\n    \"latent adversarial training of graph convolutional networks\": 0.7,\n    \"contrastive multi view representation learning on graphs\": 0.7,\n    \"variational graph auto encoders\": 0.7,\n    \"graph self-supervised learning a survey\": 0.6,\n    \"graph contrastive learning with adaptive augmentation\": 0.6\n}\n```", "Based on the text and the task's guideline, the most pertinent source papers of this paper seem to be:\n\n- 'b2': 'dataset augmentation in feature space' - The paper discusses injecting simple augmentations (e.g., Gaussian noise) on latent graph representations. This topic is directly relevant to the dataset augmentation in feature space, which is discussed in the source paper. Confidence score: 0.95\n- 'b3': 'deep wiener deconvolution wiener meets deep learning for image deblurring' - The paper mentions the introduction of the classical deconvolutional technique, wiener kernel, into GDN, which is discussed in this source paper. Confidence score: 0.9\n- 'b12': 'variational graph auto encoders' - The paper discusses graph autoencoders, which are a central topic of this source paper. Confidence score: 0.8\n- 'b16': 'deconvolutional networks on graph data' - The paper discusses graph deconvolutional network, which is a central topic of this source paper. Confidence score: 0.9\n- 'b23': 'symmetric graph convolutional autoencoder for unsupervised graph representation learning' - The paper discusses graph representation learning, which is a central topic of this source paper. Confidence score: 0.85\n- 'b41': 'self-supervised learning of graph neural networks: a unified review' - The paper discusses self-supervised learning of graph neural networks, which is a central topic of this source paper. Confidence score: 0.8\n\nThis result in JSON format:\n\n```\n{\n\"b2\": 0.95,\n\"b3\": 0.9,\n\"b12\": 0.8,\n\"b16\": 0.9,\n\"b23\": 0.85,\n\"b41\": 0.8\n}\n```\n\nThis analysis is based on the text provided and it's possible that a more accurate determination could be made with the full text of the paper.", "Based on the text of the paper and the provided titles of the references, the source papers that seem to have most inspired the authors are:\n\n1. \"latent augmentation for better graph self supervised learning\" [b0]\n2. \"modals modality agnostic automated data augmentation in the latent space\" [b1]\n3. \"dataset augmentation in feature space\" [b2]\n4. \"deep wiener deconvolution wiener meets deep learning for image deblurring\" [b3]\n5. \"latent adversarial training of graph convolutional networks\" [b10]\n6. \"deconvolutional networks on graph data\" [b16]\n7. \"barycentric remez algorithms for best polynomial approximation in the chebfun system\" [b22]\n8. \"symmetric graph convolutional autoencoder for unsupervised graph representation learning\" [b23]\n\nThe \"confidence score\" for each of these references would be:\n\n```json\n{\n    \"b0\": 0.9,\n    \"b1\": 0.85,\n    \"b2\": 0.8,\n    \"b3\": 0.9,\n    \"b10\": 0.85,\n    \"b16\": 0.95,\n    \"b22\": 0.8,\n    \"b23\": 0.9\n}\n```\n\nThese confidence scores are based on the frequency and context of the references within the paper. The references that are directly associated with key methodologies and approaches (such as \"latent augmentation\", \"wiener deconvolution\", \"adversarial training\", and \"graph deconvolutional networks\") are given higher confidence scores."], "6419208e90e50fcafda928aa": ["Based on the text of the paper, the challenges outlined by the authors are primarily about the over-smoothing issue in deep Graph Neural Networks (GNNs), and the paper proposes a more formal and tractable definition of over-smoothing through an axiomatic construction. They also present empirical observations on several existing methods that attempt to mitigate over-smoothing, pointing out that simply keeping the node-similarity measure constant is not sufficient to construct well-performing deep GNNs. \n\nThe source papers that appear to be directly related to these challenges and methods include:\n- [b35]: \"Graph coupled oscillator networks\"\n- [b36]: \"Gradient gating for deep multi-rate learning on graphs\"\n- [b50]: \"Dirichlet energy constrained learning for deep graph neural networks\"\n- [b47]: \"PairNorm: Tackling oversmoothing in GNNs\"\n- [b49]: \"Towards deeper graph neural networks with differentiable group normalization\"\n\nIn the paper, the authors also extend their definition to continuous-time GNNs and discuss this emerging field. The source papers related to this topic include:\n- [b33]: \"Graph neural ordinary differential equations\"\n- [b52]: \"Beyond homophily in graph neural networks: current limitations and effective designs\"\n\nThe source papers listed above are directly tied to the challenges and methods discussed in the paper, and are therefore likely to be the most influential sources for the authors.\n\nThe json format of the result is: \n```\n{\n  \"b35\": 0.9,\n  \"b36\": 0.8,\n  \"b50\": 0.8,\n  \"b47\": 0.7,\n  \"b49\": 0.7,\n  \"b33\": 0.8,\n  \"b52\": 0.8\n}\n```", "The source papers that inspired the given paper based on its text would be:\n\n1. \"On the bottleneck of graph neural networks and its practical implications\" [b0] - The paper mentions 'graph bottlenecks' as one of the issues impairing the performance of deep GNNs, which is a concept originally proposed in this source paper.\n2. \"A note on over smoothing for graph neural networks\" [b4] - This paper focuses on the over-smoothing phenomenon and has inspired the author to delve deeper into the topic.\n3. \"A survey on oversmoothing in graph neural networks\" [b7] - This survey paper seems to be a guiding force for the author's research on over-smoothing.\n4. \"Graph Neural Networks as Gradient Flows\" [b13] - This paper seems to have inspired the author's discussion on the change of GNN dynamics.\n5. \"PDE GCN: Novel architectures for graph neural networks motivated by partial differential equations\" [b14] - This paper has a strong influence on the author's work dealing with PDE-inspired architectures.\n6. \"Dirichlet energy constrained learning for deep graph neural networks\" [b50] - This paper discusses the use of Dirichlet energy as a measure for over-smoothing, which is a key aspect of the author's work.\n7. \"Gradient Gating for Deep Multi-Rate Learning on Graphs\" [b36] - This paper seems to influence the author's work as it provides a method that successfully mitigates over-smoothing and increases the expressive power of the model.\n\nHere is the JSON format of the results:\n{\n  \"b0\": 0.9,\n  \"b4\": 0.9,\n  \"b7\": 0.8,\n  \"b13\": 0.8,\n  \"b14\": 0.8,\n  \"b50\": 0.9,\n  \"b36\": 0.9\n}", "Based on the analysis of the paper text, the primary challenge outlined in the paper is the phenomenon of over-smoothing in Graph Neural Networks (GNNs) and the authors propose various strategies to mitigate this issue. The authors mentioned several papers that inspired their work. Here are the source papers that inspired them most:\n\n1. \"Dirichlet energy constrained learning for deep graph neural networks\" [b50]: As per the text, the authors took inspiration from the method of measuring over-smoothing using Dirichlet energy and optimizing a GNN within a constrained range of the underlying layer-wise Dirichlet energy. \n\n2. \"Pairnorm tackling oversmoothing in gnns\" [b47]: The authors cite this paper while discussing normalization and regularization methods to reduce over-smoothing in GNNs. \n\n3. \"Graph coupled oscillator networks\" [b35]: This paper is cited multiple times as an example of changing GNN dynamics to mitigate over-smoothing.\n\n4. \"Gradient gating for deep multi rate learning on graphs\" [b36]: The authors refer to this work as a successful example of a GNN model that mitigates over-smoothing without sacrificing its expressive power.\n\n5. \"Simple and deep graph convolutional networks\" [b8]: This work is referred to when discussing 'residual connections' strategy for mitigating over-smoothing.\n\n6. \"Neural sheaf diffusion a topological perspective on heterophily and oversmoothing in gnns\" [b1]: This work is referred while discussing the extension of their definitions and strategies to continuous-time GNNs.\n\n7. \"A note on over smoothing for graph neural networks\" [b4]: This work is cited multiple times as a reference for over-smoothing in GNNs, its measurement, and its implications. \n\nReturn in JSON format:\n{\"b50\": 0.9, \"b47\": 0.85, \"b35\": 0.9, \"b36\": 0.85, \"b8\": 0.8, \"b1\": 0.75, \"b4\": 0.8}", "Based on the given text, the following references seem to have directly influenced the paper's core contributions:\n\n- 'b35': 'graph coupled oscillator networks' - The paper discusses this work and details how it has influenced their approach in understanding the dynamics of deep GNNs and how to overcome the over-smoothing issue. Confidence score: 0.9\n- 'b36': 'gradient gating for deep multi rate learning on graphs' - The paper cites this work as a successful example of a method that mitigates over-smoothing while maintaining its expressive power. Confidence score: 0.9\n- 'b50': 'dirichlet energy constrained learning for deep graph neural networks' - The paper uses this work's method of using the Dirichlet energy as a measure for over-smoothing. Confidence score: 0.8\n- 'b47': 'pairnorm tackling oversmoothing in gnns' - The authors discuss the PairNorm method but indicate it is not as effective as other methods in maintaining performance with increasing layers. Confidence score: 0.7\n- 'b49': 'towards deeper graph neural networks with differentiable group normalization' - The authors reference this work while discussing how normalizing within groups of the same labeled nodes could mitigate over-smoothing. Confidence score: 0.7\n- 'b52': 'beyond homophily in graph neural networks current limitations and effective designs' - This paper is referenced multiple times and seems to inform the authors' understanding of the over-smoothing issue, particularly in the context of heterophilic graph data. Confidence score: 0.7\n- 'b4': 'a note on over smoothing for graph neural networks' - The authors cite this work multiple times as an important resource in understanding the over-smoothing issue. Confidence score: 0.6\n\nThe JSON format of the result is:\n{\"b35\": 0.9, \"b36\": 0.9, \"b50\": 0.8, \"b47\": 0.7, \"b49\": 0.7, \"b52\": 0.7, \"b4\": 0.6}", "Based on the text and the titles of the referenced papers, the following references seem to be the source papers which inspired the author the most:\n\n1. \"towards deep graph convolutional networks\" [b34]: This paper is referenced in the context of discussing implicit regularization by adding noise to the optimization process, which is a technique used by the authors to mitigate over-smoothing.\n\n2. \"pairnorm tackling oversmoothing in gnns\" [b47]: This paper is mentioned when discussing normalization methods to address over-smoothing. The authors use a similar approach to normalize node features.\n\n3. \"graph coupled oscillator networks\" [b35]: Referenced multiple times throughout the text, this paper seems to have heavily influenced the author's approach to changing GNN dynamics to mitigate over-smoothing.\n\n4. \"simple and deep graph convolutional networks\" [b8]: This paper is mentioned in the context of adding residual connections to deep GNNs, a tactic utilized by the authors to address over-smoothing.\n\n5. \"neural sheaf diffusion a topological perspective on heterophily and oversmoothing in gnns\" [b1]: This paper is cited in the context of discussing the extension of the authors' definition of over-smoothing to continuous-time GNNs.\n\n6. \"gradient gating for deep multi rate learning on graphs\" [b36]: This paper is discussed as one of the successful methods that are able to mitigate over-smoothing while maintaining its expressive power on the task.\n\nBased on the above analysis, the confidence score for each reference could be as follows:\n\n{\n\"b34\": 0.8,\n\"b47\": 0.9,\n\"b35\": 1,\n\"b8\": 0.8,\n\"b1\": 0.7,\n\"b36\": 0.9\n}"], "63dc7b5290e50fcafdc836c3": ["Based on the text and the titles of the references, the source papers that inspire the author most are:\n\n1. \"Cluster gcn: an efficient algorithm for training deep and large graph convolutional networks\" (b4)\n2. \"Graphsaint: graph sampling based inductive learning method\" (b28)\n3. \"Gnnautoscale: scalable and expressive graph neural networks via historical embeddings\" (b9)\n4. \"Stochastic training of graph convolutional networks with variance reduction\" (b1)\n\nThese papers are mentioned in the context of developing the author's novel subgraph-wise sampling method, Local Message Compensation (LMC). They discuss methods for training GNNs on large-scale graphs, sampling techniques, and the use of historical embeddings, which are all key components of the author's proposed algorithm.\n\nHere is the JSON output:\n\n{\n\"b4\": 0.9,\n\"b28\": 0.85,\n\"b9\": 0.8,\n\"b1\": 0.75\n}\n", "Based on the text and the titles of the references, the most pertinent source papers are:\n\n1. \"Cluster gcn an efficient algorithm for training deep and large graph convolutional networks\" (b4): This paper seems to be a key predecessor to the proposed method, introducing the concept of subgraph-wise sampling methods, which the current paper builds upon. \n\n2. \"Gnnautoscale scalable and expressive graph neural networks via historical embeddings\" (b9): This paper introduces the use of historical embeddings to generate messages outside a sampled subgraph, which is also used in the current paper's method.\n\n3. \"Graphsaint graph sampling based inductive learning method\" (b28): This paper also seems to have contributed to the development of the current paper's method, as it introduces another subgraph-wise sampling method.\n\n4. \"Stochastic training of graph convolutional networks with variance reduction\" (b1): This paper highlights the issue of inaccurate mini-batch gradients slowing down convergence speeds, which the current paper addresses.\n\n5. \"Minimal variance sampling with provable guarantees for fast training of graph neural networks\" (b5): This paper seems to have contributed to the theoretical underpinnings of the current paper's method, as it provides provable guarantees of fast training.\n\n6. \"Graphfm improving large scale gnn training via feature momentum\" (b27): This paper also seems to have provided inspiration to the current paper's method, as it also uses node embeddings in the mini-batch to alleviate the staleness problem.\n\nSo, the json return is: \n{\n\"b4\": 1,\n\"b9\": 0.9,\n\"b28\": 0.8,\n\"b1\": 0.7,\n\"b5\": 0.6,\n\"b27\": 0.5\n} \nThe confidence scores are assigned based on the inferred level of influence each paper had on the development of the current paper's method.", "Based on the given text, the following references seem to be the most relevant:\n\n1. b19: 'Deep learning on graphs' - This paper is likely a key source as it directly relates to the subject of the paper: training Graph Neural Networks (GNNs).\n2. b9: 'GNN AutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings' - LMC is compared to this method in the text, suggesting it was a significant inspiration.\n3. b4: 'Cluster GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks' - It's mentioned as a method that constructs subgraphs based on a sampled mini-batch, which relates to the proposed LMC method.\n4. b12: 'Inductive representation learning on large graphs' - This work is referenced in the context of nodewise sampling methods, which is a key challenge addressed by LMC.\n5. b5: 'Minimal Variance Sampling with Provable Guarantees for Fast Training of Graph Neural Networks' - This paper appears to have provided inspiration for LMC's approach to handling the neighbor explosion problem.\n\nPlease note, this is an educated guess based on the provided text and may not be entirely accurate.\n\nIn the JSON format:\n{\n\"b19\": 0.9,\n\"b9\": 0.8,\n\"b4\": 0.7,\n\"b12\": 0.7,\n\"b5\": 0.7\n}", "Based on the provided information and the text of the academic paper, the source papers which inspired the author are:\n\n{\"b1\": 0.9, \"b5\": 0.9, \"b4\": 0.8, \"b9\": 0.9, \"b28\": 0.8, \"b19\": 0.7, \"b30\": 0.7, \"b27\": 0.8}\n\nThese papers are identified based on their direct relevance to the novel contributions of the given paper, their methodological similarities, and their explicit mention in the text with key phrases indicating the author drew inspiration or motivation from them.", "Based on the text and references provided, the source papers that seem to inspire the given paper most directly are:\n\n- \"Cluster gcn an efficient algorithm for training deep and large graph convolutional networks\" ['b4']\n- \"Gnnautoscale scalable and expressive graph neural networks via historical embeddings\" ['b9']\n- \"Graphsaint graph sampling based inductive learning method\" ['b28']\n- \"Stochastic training of graph convolutional networks with variance reduction\" ['b1']\n- \"Fastgcn fast learning with graph convolutional networks via importance sampling\" ['b2']\n- \"Minimal variance sampling with provable guarantees for fast training of graph neural networks\" ['b5']\n\nThe JSON format result with confidence scores is:\n\n{\n\"b4\": 0.9,\n\"b9\": 0.9,\n\"b28\": 0.8,\n\"b1\": 0.7,\n\"b2\": 0.7,\n\"b5\": 0.7\n}\n\nThe confidence scores are given on a scale of 0 to 1, with 1 indicating the highest degree of influence on the given paper. The scores are based on the frequency and the context in which these references are mentioned in the paper."], "64363413cf18aa11d8b55a06": ["```json\n{\n  \"b17\": 0.95,\n  \"b7\": 0.90,\n  \"b9\": 0.85,\n  \"b1\": 0.80\n}\n```", "{\n  \"b17\": 0.9,\n  \"b7\": 0.8\n}", "{\n    \"b17\": 0.95,\n    \"b7\": 0.9,\n    \"b9\": 0.85\n}", "{\n    \"b17\": 0.95,\n    \"b7\": 0.75,\n    \"b9\": 0.70\n}", "{\n  \"b17\": 0.95,\n  \"b7\": 0.85,\n  \"b9\": 0.80,\n  \"b1\": 0.75\n}"], "642525e790e50fcafdfdd202": ["Based on the author's notes and the provided excerpt from the paper, we are looking for source papers that have directly influenced the novel contributions of the paper. Specifically, the paper mentions a novel approach to generating search space for auto-scheduling in AI compilers by taking into account hardware constraints, and a constraint-based genetic algorithm (CGA) for searching in an irregular search space. We need to identify the papers that inspired these new methods.\n\nFrom the provided text, we can isolate some key phrases indicating direct inspiration or motivation:\n\n1. \"To address the above challenges, in this paper, we propose an automatically constrained exploration-based approach, i.e., Heron, to generate high-performance libraries for DLAs.\"\n\n2. \"During constrained space generation, the first challenge of generating accurate constraints is addressed by conducting static analysis on compute.\"\n\n3. \"During constrained space exploration, the second challenge of exploring the irregular space is addressed by a novel constraint-based genetic algorithm (CGA).\"\n\n4. \"The key is to automatically (rather than manually) enforce massive sophisticated while accurate constraints through the entire program generation including constrained space generation and constrained space exploration.\"\n\n5. \"To evaluate Heron, we conduct extensive experiments on 3 representative DLAs, i.e., NVIDIA TensorCore, Intel DL Boost, and TVM Versatile Tensor Accelerator (VTA) [b54], with a large number of benchmarks (including operators and networks).\"\n\nBased on these indications and the titles of the references, we can extract the following references as the most relevant to the paper's novel contributions:\n\n- AutoTVM [b25]: Mentioned in comparison with Heron, indicating that the paper's approach is an improvement over AutoTVM's methods.\n- Ansor [b70]: Similarly, this paper is used as a comparison to show the advancements made by Heron.\n- AMOS [b71]: Also mentioned in comparison, which suggests that it has a methodological similarity to Heron.\n- AKG [b69]: Mentioned as a state-of-the-art approach that Heron outperforms, which indicates that it may have inspired some aspects of the novel contributions.\n- NVIDIA TensorCore [b10]: While not a paper, it is a hardware platform used to evaluate Heron, which is essential for understanding the paper's context.\n- Intel DL Boost [b6]: Like TensorCore, it is a hardware platform crucial for the evaluation of Heron's effectiveness.\n- TVM Versatile Tensor Accelerator (VTA) [b54]: Another hardware platform used for evaluation, which may have contributed to the development of Heron.\n\nNow, let's provide the result in the requested JSON format with confidence scores. The confidence score is a subjective assessment of how strongly the paper seems to be influenced by the referenced work, based on the context provided:\n\n```json\n{\n    \"b25\": 0.9,\n    \"b70\": 0.9,\n    \"b71\": 0.9,\n    \"b69\": 0.8,\n    \"b10\": 0.7,\n    \"b6\": 0.7,\n    \"b54\": 0.7\n}\n```\n\nThe confidence scores are high (0.7-0.9) because these papers and hardware platforms are directly compared to the novel contributions of the paper, indicating a strong influence or relevance.", "Based on the author notes and the text of the paper provided, the novel contributions of the paper include an improved method for generating operator schedule strategies in the AI compiler's auto-schedule domain. The paper proposes a static analysis method to automatically generate a search space according to hardware accelerator constraints, leading to an irregular search space. To search this irregular candidate space, the paper introduces a constraint-based genetic algorithm.\n\nFollowing the given instructions, we'll look for references that are directly linked to these main challenges and methods. Let\u2019s identify the pertinent source papers with confidence scores indicating their relevance.\n\n1. The paper criticizes AutoTVM and similar exploration-based approaches for generating low-quality search spaces on DLAs, indicating that [b25] (AutoTVM), [b70] (Ansor), and [b71] (AMOS) are related works but are being improved upon. This suggests that they are important references, but they are not directly the inspiration for the novel contributions.\n\n2. The paper introduces a novel constraint-based genetic algorithm (CGA) for searching the irregular search space. The references [b36] (Genetic algorithms in search optimization and machine learning), [b40] (Constrained optimization via genetic algorithms), and [b45] (A review of constraint handling techniques for evolution strategies) likely relate to the genetic algorithm and constraint handling methods that inspired the CGA.\n\n3. The paper highlights the performance gains of Heron over state-of-the-art automatic generation approaches and hand-tuned libraries. While this comparison is crucial, it does not necessarily imply that the compared methods inspired the novel contributions.\n\n4. The references [b54] (A hardware/software blueprint for flexible deep learning specialization) and [b66] (Unit unifying tensorized instruction compilation) might be related to DLAs and the compilation process, which the paper aims to improve, but the text does not suggest that they directly inspired the novel contributions.\n\nGiven the information in the task and the lack of explicit \"inspired by\" or similar phrases, we can infer that the most likely source papers for the novel contributions of the proposed method are those related to genetic algorithms and constraint handling. Hence, the confidence scores are based on the relevance to the novel contributions described in the paper:\n\n```json\n{\n  \"b25\": 0.6,\n  \"b36\": 0.9,\n  \"b40\": 0.9,\n  \"b45\": 0.9,\n  \"b70\": 0.6,\n  \"b71\": 0.6\n}\n```\n\nThe scores reflect a higher confidence for references directly related to genetic algorithms and constraints (b36, b40, b45), and a lower confidence for those references that represent previous works that the proposed method aims to improve upon (b25, b70, b71). Other references might be contextually relevant but are not explicitly tied to the novel contributions of the paper.", "Based on the information provided and the guidelines for identifying the source papers that most inspired the given paper, we can analyze the text to identify the key references. Here is the step-by-step analysis:\n\n1. **Identify Novel Contributions**: The paper proposes an algorithm called \"Heron\" for generating high-performance libraries for Deep Learning Accelerators (DLAs). The novel contributions are:\n   - Automatic generation of sophisticated constraints\n   - Constraint-based genetic algorithm (CGA)\n   - Comprehensive evaluation on NVIDIA TensorCore, Intel DL Boost, and TVM VTA\n\n2. **Primary Challenges Outlined**:\n   - The large and nearly all invalid program candidates search space due to DLAs architectural constraints\n   - The difficulty in accurately constraining the search space\n   - The need for an efficient exploration algorithm for irregular constrained spaces\n\n3. **Algorithm Proposed**: \n   - Static analysis method to generate a search space based on hardware accelerator constraints\n   - Constraint-based genetic algorithm to search in an irregular space\n\n4. **Key Phrases and References**:\n   - \"compared to 4 state-of-the-art automatic generation paradigms including exploration-based approaches (i.e., AutoTVM [b25], Ansor [b70], and AMOS [b71]) and a polyhedral compiler (i.e., AKG [b69])\" - This indicates that [b25], [b70], [b71], and [b69] are related works but potentially not the direct inspiration.\n   - \"To embrace the diversity of DLAs, high-performance software libraries with well-established APIs\" - While this mentions the context, no direct inspiration is indicated here.\n   - \"To address the above challenges, in this paper, we propose an automatically constrained exploration-based approach\" - This indicates that the proposed approach is a response to the limitations of existing methods but does not specify a direct inspiration.\n   - \"Constraint-based genetic algorithm. We formulate the space exploration as optimizing a constrained optimization problem with a novel constraint-based genetic algorithm\" - This suggests a novel contribution but does not cite a specific inspiration.\n\n5. **Re-evaluate by Emphasizing Methodological Similarity**:\n   - No explicit mentions of direct methodological inspiration or motivation phrases such as \"inspired by\" or \"motivated by\" are present in the provided text.\n\n6. **Comparative Phrases**:\n   - The paper compares its results with AutoTVM, Ansor, AMOS, and AKG, indicating they are relevant but mainly in the context of performance comparison rather than direct inspiration.\n\nGiven the lack of explicit phrases indicating direct inspiration, and based on the descriptions and comparative context, we can infer that the references which are most likely to be the source papers for the novel contributions of the proposed \"Heron\" approach are those related to the automatic generation of search spaces and genetic algorithms within the context of DLAs.\n\nBased on the analysis, here is the JSON format result of the most central references that heavily influenced the main contributions of the paper, with confidence scores reflecting the likelihood of direct inspiration:\n\n```json\n{\n  \"b25\": 0.5,\n  \"b70\": 0.5,\n  \"b71\": 0.5,\n  \"b69\": 0.5\n}\n```\n\nThe confidence scores are set to 0.5 due to the lack of explicit inspiration phrases and the fact that the paper primarily compares its approach to these works. The chosen references are based on their relation to the field of auto-schedule and automatic generation, as well as the genetic algorithm approach which is central to the paper's contributions.", "Based on the provided text and author notes, the task is to identify the source papers that most inspired the contribution of the paper. The paper introduces a method for generating improved algorithm schedule strategies in the field of AI compiler auto-scheduling. The authors recognized that existing auto-schedule methods did not consider actual hardware constraints, leading to the generation of many useless candidate schedules and an enormous search space. To solve this, they proposed a static analysis method that automatically generates a search space based on hardware accelerator constraints. This resulted in an irregular search space, which required a novel constraint-based genetic algorithm to search effectively. The work was evaluated on TensorCore, Intel DL Boost, and TVM VTA hardware architectures, outperforming AutoTVM, AMOS, and others.\n\nFollowing the outlined criteria, I will analyze the provided text to extract references that are directly linked to the novel contributions and methods of the paper:\n\n1. The paper introduces \"Heron,\" which is an automatically constrained exploration-based approach for generating high-performance libraries for DLAs.\n\n2. The problem of large, low-quality search spaces in existing exploration-based approaches like AutoTVM [b25], Ansor [b70], and AMOS [b71] is identified.\n\n3. The paper mentions the constraint-based genetic algorithm (CGA) as a novel aspect of their method.\n\n4. The paper compares the performance of Heron to other approaches, including AutoTVM [b25], Ansor [b70], AMOS [b71], and a polyhedral compiler AKG [b69].\n\n5. The paper mentions the use of a cost model similar to AutoTVM's [b25] but with different features from defined variables of the Constraint Generation step.\n\nBased on the novel contributions and methods identified, here is a JSON-formatted list of source papers with their confidence scores:\n\n```json\n{\n  \"b25\": 0.9,\n  \"b70\": 0.9,\n  \"b71\": 0.9,\n  \"b69\": 0.8\n}\n```\n\nThe confidence scores are high for references [b25], [b70], and [b71] because they are directly compared with the proposed method and their limitations are addressed in the novel approach. Reference [b69] is given a slightly lower confidence score because it is mentioned as a comparison but not as an inspiration for the novel method. The paper also seems to draw from the general landscape of auto-scheduling in AI compilers, but specific phrases indicating direct inspiration or motivation from other works are not explicitly mentioned in the provided text.", "Based on the information provided and following the instructions given, I will analyze the text and the author's notes to identify the source papers that inspired the novel contributions of the paper. Here are the steps taken to come up with the result:\n\n1. Identify the novel contributions of the paper:\n   - A new method for operator schedule strategy improvement in AI compilers within the auto-schedule domain.\n   - A static analysis method that automatically generates search space based on hardware accelerator constraints.\n   - A constraint-based genetic algorithm to search through irregular candidate spaces.\n   - Evaluation of the work on TensorCore, Intel DL Boost, and TVM VTA showing superior results compared to AutoTVM and AMOS.\n\n2. Search for key phrases indicating inspiration or motivation from other works:\n   - The paper does not explicitly state phrases like \"inspired by\" or \"motivated by\" in the provided text.\n\n3. Infer references based on the novel contributions and the text:\n   - The authors mention the limitations of existing methods like AutoTVM ([b25]), Ansor ([b70]), and AMOS ([b71]), suggesting these as starting points for understanding the problem space but also as limitations that their work aims to overcome.\n   - The mention of \"exploration-based compilation approaches\" and references like [b16], [b24], [b49], [b66], [b70], [b71], [b72] imply they are related to the broader context but may not directly inspire the novel contributions.\n   - The proposed constraint-based genetic algorithm (CGA) might be related to works on genetic algorithms and constraint handling ([b36], [b30], [b31], [b40], [b42], [b44], [b45], [b57], [b59], [b60], [b61], [b68]).\n\n4. Consider the references directly linked to the main challenge and methods introduced in the paper:\n   - The improvement over AutoTVM ([b25]), Ansor ([b70]), and AMOS ([b71]) is directly mentioned, which implies these works are closely related to the paper's core contributions.\n   - References related to genetic algorithms and constraint optimization ([b30], [b31], [b40], [b42], [b44], [b45]) may be considered for the novel CGA approach, but it's not clear from the text which specific ones were directly influential.\n\n5. Limit the selection to the most central references:\n   - Since the paper aims to improve upon AutoTVM, Ansor, and AMOS, and introduces a new type of genetic algorithm to address the irregular search space issue, the most central references are likely [b25], [b70], and [b71].\n\nHere is the result in JSON format, with confidence scores reflecting the degree of direct relevance and influence on the paper's novel contributions:\n\n```json\n{\n  \"b25\": 0.9,\n  \"b70\": 0.9,\n  \"b71\": 0.9\n}\n```\n\nThe confidence scores are high because the paper explicitly compares its results to those obtained using AutoTVM, Ansor, and AMOS, and aims to address the shortcomings of these methods. The lack of explicit \"inspired by\" phrases in the text makes it necessary to infer the most relevant works based on the context and the paper's stated goals and contributions."], "63aaa48a90e50fcafd27b0fa": ["Based on the provided text, the source papers that the author indicates inspire them most are:\n\n1. Ref [b8] - \"Efficient scheduling of irregular network structures on cnn accelerators\" - The text mentions that the author's work proposes an automatic mapping framework named AutoMap which addresses the issue of finding the optimal strategy when facing continuously evolving and complex network topology. The reference to [b8] here suggests that this paper might have provided some inspiration for developing the framework.\n\n2. Ref [b9] - \"Smaug end to end full stack simulation infrastructure for deep learning workloads\" - This paper is also referenced alongside [b8] in the context of automatic mapping frameworks, indicating that it might have played a role in inspiring the author's work.\n\n3. Ref [b5] - \"Sticker an energy efficient multi sparsity compatible accelerator for convolutional neural networks in nm cmos\" - This paper is cited as one that employs a layerwise mapping strategy, which the author of the text criticizes for causing excessive data movements. This suggests that the author was influenced by this paper in developing a more efficient strategy.\n\n4. Ref [b15] - \"Shidiannao shifting vision processing closer to the sensor\" - This paper is cited in the context of a manual static memory allocation strategy that the author describes as causing excessive memory usage. It's reasonable to assume that this paper influenced the author to propose a dynamic memory allocator (DMA) for better performance.\n\n5. Ref [b14] - \"An efficient hardware design for accelerating sparse cnns with nas based models\" - This paper is cited as one that employs a layerwise mapping strategy, which the author of the text criticizes for causing excessive data movements. This suggests that the author was influenced by this paper in developing a more efficient strategy.\n\n6. Ref [b11] - \"Pytorch an imperative style high performance deep learning library\" - This paper is cited in the context of prevalent deep learning frameworks. It might have inspired the author in terms of how to structure the execution flow of a DNN.\n\nThe confidence score for each source paper is difficult to determine without detailed information on the extent to which each paper influenced the author's work. However, it is likely that the confidence scores would be higher for the papers that the author criticizes or builds upon directly in their work (e.g., [b5], [b15], [b14]) compared to the papers that are simply mentioned in the context of existing research (e.g., [b8], [b9], [b11]).\n\nThe final list of source papers in JSON format is:\n\n{\n\"b8\": 0.7,\n\"b9\": 0.7,\n\"b5\": 0.9,\n\"b15\": 0.9,\n\"b14\": 0.9,\n\"b11\": 0.7\n}", "Based on the text and the titles of the references, the source papers that most likely inspired this work are:\n\n- 'Efficient scheduling of irregular network structures on CNN accelerators' [b8]\n- 'NVDLA: Deep learning accelerator' [b3]\n- 'TVM: An automated end-to-end optimizing compiler for deep learning' [b10]\n- 'SMAUG: End-to-end full-stack simulation infrastructure for deep learning workloads' [b9]\n- 'An efficient hardware design for accelerating sparse CNNs with NAS based models' [b14]\n- 'Shidiannao: Shifting vision processing closer to the sensor' [b15]\n\nLooking at the content of the academic paper text, these references are mentioned in the context of the challenges identified by the author and the methodologies they have proposed. Additionally, these papers are linked to the main challenges and methods introduced by the paper, and appear to be highly relevant to the paper's novel contributions. \n\nAccording to the information given, the JSON format of the relevant references and their confidence scores (inferred from their relevancy in the text) would be:\n\n{\n\"b8\": 0.9,\n\"b3\": 0.8,\n\"b10\": 0.8,\n\"b9\": 0.7,\n\"b14\": 0.7,\n\"b15\": 0.7\n}", "Based on the given text and the criteria specified, the most pertinent source papers that inspired the given paper are likely to be:\n1. 'Efficient scheduling of irregular network structures on CNN accelerators' [b8]\n2. 'SMAUG: End-to-end, full-stack simulation infrastructure for deep learning workloads' [b9]\n3. 'TVM: An automated end-to-end optimizing compiler for deep learning' [b10]\n4. 'An efficient hardware design for accelerating sparse CNNs with NAS-based models' [b14]\n5. 'SHiDiannao: Shifting vision processing closer to the sensor' [b15]\n6. 'TimeLoop: A systematic approach to DNN accelerator evaluation' [b26]\n7. 'Dory: Automatic end-to-end deployment of real-world DNNs on low-cost IoT MCUs' [b27]\n8. 'Maestro: A data-centric approach to understand reuse, performance and hardware cost of DNN mappings' [b29]\n\nThe confidence score for each of these papers is difficult to determine without access to the full paper text, but they would all likely score highly based on their relevance to the paper's main contributions and methods.\n\nIn JSON format, the results would look like this:\n{\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b10\": 0.9,\n    \"b14\": 0.8,\n    \"b15\": 0.9,\n    \"b26\": 0.8,\n    \"b27\": 0.9,\n    \"b29\": 0.9\n}", "Based on the given text and criteria, the most pertinent source papers are:\n\n1. \"Efficient scheduling of irregular network structures on cnn accelerators\" [b8]: The paper's authors discuss the importance of appropriate mapping strategies to utilize the on-chip hardware resources effectively, which is a challenge they address in their work.\n   \n2. \"TVM: an automated end-to-end optimizing compiler for deep learning\" [b10]: The authors mention the difficulty of finding the optimal strategy for complex and evolving network topology, an issue they propose to solve with their automatic mapping framework named AutoMap. \n\n3. \"An efficient hardware design for accelerating sparse cnns with nas based models\" [b14]: The paper discusses how previous works employed a layer-wise mapping strategy that didn't consider data reuse between layers, leading to excessive data movements. This is a problem the authors aim to solve in their paper.\n\n4. \"Shidiannao: shifting vision processing closer to the sensor\" [b15]: This paper is referenced in the context of memory allocation being a significant challenge in group-wise mapping method. The authors of the given paper propose a dynamic memory allocator to address the issues brought about by manual static memory allocation.\n\n5. \"Efficient scheduling of irregular network structures on cnn accelerators\" [b8]: This paper is referenced again when discussing CG Partition, indicating its relevance to the proposed method.\n\n6. \"Sail: a deep learning-based system for automatic gait assessment from tug videos\" [b16]: The authors discuss the deployment of DNN models on edge devices like mobile phones, which is a challenge they address in their paper.\n\nThe confidence scores for each paper could be evaluated based on the frequency and context of their mentions in the paper, which would require a more detailed analysis. A potential output in the requested JSON format could look like this:\n\n```\n{\n    \"b8\": 0.8,\n    \"b10\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.6\n}\n```\nIn this hypothetical output, the confidence scores are higher for the papers that are referenced more frequently or in relation to the main challenges and methods outlined in the paper.", "Based on the given text, the following papers can be identified as the main sources that inspired the author:\n\n{\n\"b8\": 0.9, \n\"b9\": 0.8, \n\"b10\": 0.7, \n\"b5\": 0.7, \n\"b15\": 0.6, \n\"b14\": 0.6, \n\"b26\": 0.5,\n\"b27\": 0.5\n}\n\nThe author directly mentions the challenges in previous works [b8] and [b9] and proposes solutions to them. He also mentions the limitations of the methods used in [b5], [b14] and [b15]. The author's framework is also compared with the methods used in [b8], [b9], [b10], [b26] and [b27], indicating a strong reliance on these papers."], "6427029c90e50fcafd5d6bd8": ["Based on the given text of the paper, its author's notes, and the titles of the references, the most pertinent source papers that inspired the paper are:\n\n1. \"netdimm low latency near memory network interface architecture\" [b2]: Mentioned in the context of datacenter networks and their evolution.\n\n2. \"rambda rdma driven acceleration framework for memory intensive s scale datacenter applications\" [None]: Author's note indicates coherent accelerator interface and ring buffer interface are key ideas in the paper. \n\n3. \"compute express link cxl\" [b32]: The paper mentions a cache-coherent accelerator (cc-accelerator) connected to a cache-coherent off-chip interconnect (cc-interconnect) such as CXL.\n\n4. \"fast and general distributed transactions using rdma and htm\" [b160]: It is in the context of a server with a standard RDMA NIC (RNIC) and a cache-coherent accelerator (cc-accelerator) for efficient network and application processing.\n\n5. \"smart remote memory\" [b144]: It is mentioned in the context of Smart NICs use in datacenter networks.\n\n6. \"data direct i o ddio\" [b69]: The paper mentions the optimization of device-memory-cache interaction inside a single machine.\n\n7. \"data direct i o characterization for future i o system exploration\" [b3]: This paper is related to the optimization of device-memory-cache interaction inside a single machine.\n\nBased on the text, these papers appear to be the most directly influential to the given paper's contributions, although further analysis of the full text of these source papers would be necessary for a more accurate determination. \n\nThe json format of the confidence score is:\n\n{\n \"b2\": 0.85,\n \"None\": 1,\n \"b32\": 0.8,\n \"b160\": 0.8,\n \"b144\": 0.7,\n \"b69\": 0.7,\n \"b3\": 0.7\n}", "Based on the content of the paper and the titles of the references, the most likely sources of inspiration for this paper are:\n\n1. \"RAMBDA: RDMA Driven Acceleration Framework for Memory Intensive s- Scale Datacenter Applications\" [b0]\n2. \"DART: Divide and Specialize for Fast Response to Congestion in RDMA-based Datacenter Networks\" [b165]\n3. \"RDMA over Commodity Ethernet at Scale\" [b54]\n4. \"Using One-Sided RDMA Reads to Build a Fast, CPU-efficient Key-value Store\" [b110]\n5. \"RDMA is Turing Complete, We Just did not Know it Yet\" [b135]\n6. \"Kernel RDMA Support for Datacenter Applications\" [b154]\n7. \"KV-Direct: High Performance In-Memory Key-Value Store with Programmable NIC\" [b93]\n8. \"Design Guidelines for High Performance RDMA Systems\" [b76]\n\nIn JSON format, the results would be:\n\n{\n\"b0\": 0.9,\n\"b165\": 0.8,\n\"b54\": 0.8,\n\"b110\": 0.7,\n\"b135\": 0.7,\n\"b154\": 0.7,\n\"b93\": 0.8,\n\"b76\": 0.7\n} \n\nThese scores are based on the assumption that the paper is heavily influenced by the work done in these references. The higher the score, the more influential the reference is to the paper.", "Based on the paper's text and the provided references, the following source papers appear to have most directly influenced the primary contributions of the paper:\n\n1. \"RAMBDA: RDMA-driven acceleration framework for memory-intensive ?s-scale datacenter applications\" [b29]: This paper directly describes the coherent accelerator that the authors utilized in their research. Confidence score: 0.9\n\n2. \"Efficient SmartNIC offload of a distributed file system with pipeline parallelism\" [b83]: This reference provides important context for the authors' work by outlining the limitations of Smart NICs and the potential benefits of accelerators. Confidence score: 0.8\n\n3. \"Fast and General Distributed Transactions Using RDMA and HTM\" [b160]: This paper highlights the potential of RDMA in improving the efficiency of distributed transactions, which is a critical aspect of the authors' work. Confidence score: 0.75\n\n4. \"Compute Express Link (CXL)\" [b32]: As a source that describes the cache-coherent off-chip interconnect used in the authors' work, this paper is an important reference. Confidence score: 0.7\n\n5. \"Chain Replication for Supporting High Throughput and Availability\" [b155]: The authors mention this paper when discussing a replicated transaction processing system, indicating its influence on their work. Confidence score: 0.65\n\n6. \"Deep learning recommendation model for personalization and recommendation systems\" [b114]: This paper is mentioned in relation to deep learning recommendation model inference, one of the applications evaluated in the author's work. Confidence score: 0.6\n\n7.  \"RDMA over Commodity Ethernet at Scale\" [b54]: This paper describes the RDMA, which is a key technology used in the authors' work. Confidence score: 0.6\n\n8. \"Characterizing and optimizing remote persistent memory with RDMA and NVM\" [b161]: This paper provides important context for the authors' work by outlining the potential of RDMA and NVM in improving memory access. Confidence score: 0.55\n\nIn the JSON format, the result can be presented as follows:\n{\"b29\": 0.9, \"b83\": 0.8, \"b160\": 0.75, \"b32\": 0.7, \"b155\": 0.65, \"b114\": 0.6, \"b54\": 0.6, \"b161\": 0.55}", "Based on the text and author notes, the most relevant source papers appear to be:\n\n1. \"b29: enzian an open general cpu fpga platform for systems software research\" - This paper discusses the concept of a cache-coherent accelerator (cc-accelerator) that can efficiently serve memory-intensive applications, which is a central part of the paper's proposed solution, RAMBDA. \n\n2. \"b38: farm fast remote memory\" - This paper outlines the use of lockless ring buffers to facilitate inter-machine communications, an idea that is adopted in the paper's solution. \n\n3. \"b83: linefs efficient smartnic offload of a distributed file system with pipeline parallelism\" - This paper seems to have influenced the paper's approach to using Smart NICs for more efficient processing and network interaction.\n\n4. \"b26: using one sided rdma reads to build a fast cpu efficient key value store\" - This paper outlines the use of one-sided RDMA operations, a method also employed in the paper's solution.\n\n5. \"b108: hyperplane a scalable low latency notification accelerator for software data planes\" - This paper seems to have inspired the paper's proposal of a coherence-assisted notification mechanism, called cpoll.\n\n6. \"b68: intel xeon gold p processor\" - This paper might have influenced the paper's envision of a server with a standard RDMA NIC (RNIC) and a cc-accelerator.\n\nHere is the confidence score for each source paper:\n\n{\"b29\": 0.9, \"b38\": 0.8, \"b83\": 0.7, \"b26\": 0.8, \"b108\": 0.8, \"b68\": 0.7}\n\nPlease note that these are the possible source papers based on the text, author notes, and the title of the papers. Further analysis may be required to definitively identify the source papers.", "Based on the provided text and using the mentioned criteria, the following papers seem to be the main sources of inspiration for the given paper:\n\n- \"RAMBDA: RDMA Driven Acceleration Framework for Memory-Intensive \u03bcs-Scale Datacenter Applications\" [b120]: This is the title of the paper, but it's mentioned in the reference list too. The author mentions using a cache-coherent accelerator (cc-accelerator) and a RDMA NIC (RNIC), which is the main idea of RAMBDA. The author also mentions this paper frequently in the text. Confidence score: 0.9\n- \"NetChain: Scale-Free Sub-RTT Coordination\" [b117]: The author uses a similar approach to NetChain in their system. Confidence score: 0.8\n- \"Prism: Rethinking the RDMA Interface for Distributed Systems\" [b18]: The author uses a similar approach to Prism in their system. Confidence score: 0.8\n- \"Storm: a fast transactional dataplane for remote data structures\" [b117]: The author uses a fast and efficient mechanism to notify the cc-accelerator of requests, exploiting its access to coherence information, which is similar to Storm. Confidence score: 0.8\n- \"HyperPlane: A Scalable Low-Latency Notification Accelerator for Software Data Planes\" [b108]: The author uses a coherence-assisted notification mechanism, called cpoll, which is similar to the QWAIT in HyperPlane. Confidence score: 0.75\n- \"Using One-Sided RDMA Reads to Build a Fast, CPU-Efficient Key-Value Store\" [b110]: The author uses one-sided RDMA operations for communications, which is similar to the approach used in this paper. Confidence score: 0.7\n- \"Farm: Fast remote memory\" [b38]: The author uses a similar approach to Farm for inter-machine communication. Confidence score: 0.7\n\nThe result in json format: \n\n{\n\"b120\": 0.9,\n\"b117\": 0.8,\n\"b18\": 0.8,\n\"b117\": 0.8,\n\"b108\": 0.75,\n\"b110\": 0.7,\n\"b38\": 0.7\n}"], "63a910a390e50fcafd2a8a6e": ["{\n  \"b26\": 0.9,\n  \"b17\": 0.8,\n  \"b18\": 0.8,\n  \"b13\": 0.7,\n  \"b14\": 0.7,\n  \"b16\": 0.7\n}", "{\n  \"b26\": 0.95,\n  \"b17\": 0.85,\n  \"b18\": 0.85,\n  \"b13\": 0.75,\n  \"b14\": 0.75,\n  \"b16\": 0.75\n}", "{\n  \"b26\": 0.95,\n  \"b17\": 0.85,\n  \"b18\": 0.85,\n  \"b13\": 0.75,\n  \"b14\": 0.75,\n  \"b16\": 0.75\n}", "{\n    \"b26\": 0.9\n}", "{\n    \"b26\": 0.95,\n    \"b17\": 0.85,\n    \"b18\": 0.85,\n    \"b13\": 0.75,\n    \"b14\": 0.75,\n    \"b16\": 0.75\n}"], "634f6ae490e50fcafdcb6525": ["Based on the provided text, the main challenges that the paper seeks to address are the limitations of loop-oriented scheduling in deep learning compilers, such as scheduling challenges, limited optimization support, and long tuning time. The paper proposes a new approach, the task-mapping-oriented programming paradigm, to address these issues. It also introduces a new DNN compiler called Hidet based on these ideas.\n\n1. \"Deep learning compilers such as Tensorflow-XLA [b34] and TVM [b8] automatically generate kernels through a compilation process for the given operators. Various schedulers such as Ansor [b46] and AutoTVM [b9] are used to schedule the kernels during compilation to achieve high performance.\" \nThis indicates that the works of Tensorflow-XLA [b34], TVM [b8], Ansor [b46], and AutoTVM [b9] might have inspired the work in the paper.\n\n2. \"The computation definition of an operator only defines how each element of the output tensor is computed mathematically, and the schedule defines the way the execution is performed, such as the loop order and thread binding [b8] [b33].\" \nThis suggests that the works of TVM [b8] and Halide [b33] might have influenced the paper's approach to computation definition and scheduling.\n\n3. \"This paradigm simplifies the development of tensor programs without sacrificing the ability to express optimizations requiring fine-grained program manipulation.\" \nThis suggests that the paper is inspired by the idea of simplifying the development of tensor programs, which is a concept that could have been influenced by multiple sources.\n\n4. \"The proposed ideas also apply to other accelerators such as CPUs and TPUs [b23].\" This indicates the work of TPUs [b23] might have influenced the paper.\n\n5. \"We implement a new DNN compiler called Hidet based on the proposed ideas.\"\nThis suggests that the original work has been inspired by the concept of DNN compilers, which could have been influenced by multiple sources.\n\n6. \"our work is similar to Microsoft's Rammer, which also abstracts computation into tasks.\" This indicates that the work of Rammer might have influenced the paper.\n\nIn conclusion, the most pertinent source papers to the given paper based on the text could be Tensorflow-XLA [b34], TVM [b8], Ansor [b46], AutoTVM [b9], Halide [b33], TPUs [b23], and Rammer.\n\nThe final result in JSON format could be:\n{\n\"b34\": 0.8,\n\"b8\": 0.8,\n\"b46\": 0.8,\n\"b9\": 0.8,\n\"b33\": 0.7,\n\"b23\": 0.7,\n\"Rammer\": 0.8\n}", "Based on the text and reference titles, the source papers are:\n\n- 'hidet task mapping programming paradigm for deep learning tensor programs' [confidence score: 0.8]\n- 'tvm an automated end to end optimizing compiler for deep learning' [confidence score: 0.9]\n- 'learning to optimize tensor programs' [confidence score: 0.8]\n- 'taso optimizing deep learning computation with automatic generation of graph substitutions' [confidence score: 0.7]\n- 'cutlass cuda template library for dense linear algebra at all levels and scales' [confidence score: 0.6]\n- 'halide a language and compiler for optimizing parallelism locality and recomputation in image processing pipelines' [confidence score: 0.7]\n- 'ansor generating high performance tensor programs for deep learning' [confidence score: 0.9]\n- 'flextensor an automatic schedule exploration and optimization framework for tensor computation on heterogeneous system' [confidence score: 0.7]\n\nResult in json format:\n{\n  \"hidet task mapping programming paradigm for deep learning tensor programs\": 0.8,\n  \"tvm an automated end to end optimizing compiler for deep learning\": 0.9,\n  \"learning to optimize tensor programs\": 0.8,\n  \"taso optimizing deep learning computation with automatic generation of graph substitutions\": 0.7,\n  \"cutlass cuda template library for dense linear algebra at all levels and scales\": 0.6,\n  \"halide a language and compiler for optimizing parallelism locality and recomputation in image processing pipelines\": 0.7,\n  \"ansor generating high performance tensor programs for deep learning\": 0.9,\n  \"flextensor an automatic schedule exploration and optimization framework for tensor computation on heterogeneous system\": 0.7\n}", "Based on the text, the potential source papers that inspired the current work are:\n\n1. \"TVM: An automated end-to-end optimizing compiler for deep learning\" [b8] (0.8)\n2. \"Learning to optimize tensor programs\" [b9] (0.7)\n3. \"Halide: A language and compiler for optimizing parallelism, locality and recomputation in image processing pipelines\" [b33] (0.65)\n4. \"CUDA programming platform\" [b30] (0.6)\n5. \"Double buffering optimization\" [b5] (0.55)\n6. \"TensorIR: An abstraction for automatic tensorized program optimization\" [b16] (0.5)\n7. \"Ansor: Generating high performance tensor programs for deep learning\" [b46] (0.75)\n\nNote: The numbers in brackets are reference numbers, and the numbers after are confidence scores. The source papers are chosen based on the explicit mentioning of these works in the text and their direct contribution to the novel features of the paper.", "Based on the text of the paper, the source papers that inspired this research can be identified as follows:\n\n1. \"Halide a language and compiler for optimizing parallelism locality and recomputation in image processing pipelines\" [b33]: This reference is mentioned as the original proposer of the idea of decoupling computation definition and scheduling, which is further adopted by the author in this paper.\n\n2. \"TVM an automated end to end optimizing compiler for deep learning\" [b8]: This paper frequently mentions the limitations of TVM and its loop-oriented scheduling primitives, which motivates the author's work on a new paradigm for writing efficient tensor programs.\n\n3. \"Learning to optimize tensor programs\" [b9]: This paper directly motivates the author to optimize the tuning time of the scheduling process.\n\n4. \"CUTLASS CUDA template library for dense linear algebra at all levels and scales\" [b24]: This paper is mentioned as an example of kernel libraries and as a source of optimization techniques that the author's method aims to implement.\n\n5. \"Rammer enabling holistic deep learning compiler optimizations with rtasks\" [b29]: The paper's work is compared with Rammer, which also abstracts computation into tasks.\n\n6. \"Ansor generating high performance tensor programs for deep learning\" [b46]: This paper is mentioned in the context of comparing different schedulers, and the author aims to outperform it in terms of performance and tuning time.\n\n7. \"cudnn efficient primitives for deep learning\" [b10]: The paper mentions cuDNN as an example of kernel libraries that the proposed method aims to outperform.\n\n8. \"Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions\" [b41]: The paper discusses the limitations of Tensor Comprehensions and how their method aims to address them.\n\nThe confidence score of each reference is assumed to be 0.9 for each source paper, as there is strong evidence in the text that these papers directly inspired the research.\n\nThe results in JSON format are:\n```\n{\n\"b33\": 0.9,\n\"b8\": 0.9,\n\"b9\": 0.9,\n\"b24\": 0.9,\n\"b29\": 0.9,\n\"b46\": 0.9,\n\"b10\": 0.9,\n\"b41\": 0.9\n}\n```", "Based on the text and titles of the references, the following source papers likely inspired the author:\n\n{\"b33\": 0.9, \"b8\": 0.85, \"b46\": 0.8, \"b9\": 0.75, \"b10\": 0.7, \"b24\": 0.65, \"b19\": 0.6} \n\nExplanation:\n- \"b33\" (Halide: A language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines) and \"b8\" (TVM: An automated end-to-end optimizing compiler for deep learning) are highly relevant because the author discusses the idea of decoupling computation definition and scheduling, which was originally proposed by Halide and later adopted by TVM.\n- \"b46\" (Ansor: Generating high-performance tensor programs for deep learning) and \"b9\" (Learning to optimize tensor programs) are also strongly related to the paper's topic of scheduling and optimizing tensor programs.\n- \"b10\" (cuDNN: Efficient primitives for deep learning) and \"b19\" (Basic linear algebra on NVIDIA GPUs) are likely sources of inspiration because the author refers to their highly optimized hand-crafted kernels, even though these are hard to generalize to other input shapes, new operators, and kernel fusion patterns.\n- \"b24\" (CUTLASS: CUDA Template Library for Dense Linear Algebra at All Levels and Scales) is mentioned as a template-based library that can achieve competitive performance on many input shapes by dynamically tuning the optimization hyper-parameters, but they do not reduce the complexity of writing tensor programs for new operators and only provide limited fusion capability."], "64264f7b90e50fcafd68e145": ["{\n    \"b6\": 1.0,\n    \"b10\": 0.9,\n    \"b20\": 0.9,\n    \"b7\": 0.8,\n    \"b25\": 0.8,\n    \"b36\": 0.8\n}", "{\n    \"b6\": 0.95,\n    \"b17\": 0.8,\n    \"b20\": 0.8,\n    \"b10\": 0.8,\n    \"b7\": 0.75\n}", "{\n  \"b6\": 0.95,\n  \"b7\": 0.9,\n  \"b10\": 0.85,\n  \"b20\": 0.8,\n  \"b25\": 0.8,\n  \"b26\": 0.75\n}", "{\n  \"b6\": 0.95,\n  \"b7\": 0.90,\n  \"b10\": 0.90,\n  \"b20\": 0.90,\n  \"b25\": 0.85,\n  \"b26\": 0.85,\n  \"b36\": 0.80\n}", "{\n  \"b6\": 0.95,\n  \"b10\": 0.85,\n  \"b20\": 0.80,\n  \"b7\": 0.75,\n  \"b17\": 0.70\n}"], "634d809c90e50fcafd4e77f0": ["Based on the provided text, the source papers inspiring the given research can be identified as follows:\n\n1. \"TaxoExpan: Self-Supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network\" [b13]: The authors frequently refer to this paper, and it seems to have inspired the current work significantly, especially in terms of challenges and methods related to taxonomy expansion.\n\n2. \"Pre-train, prompt, and predict: a systematic survey of prompting methods in natural language processing\" [b10]: The authors have adopted the concept of 'prompt tuning' from this paper, suggesting it has significantly influenced their methodology.\n\n3. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b3]: The authors have used BERT as their inner language model, indicating its influence on their work.\n\n4. \"Graph Representation Learning: A Survey\" [b1]: This paper is mentioned in the context of leveraging the random walk algorithm, showing its contribution to the framework used in the current paper.\n\n5. \"Taxonomy completion via triplet matching network\" [b16]: This paper is mentioned multiple times in the context of prompt-based methods and hypernym generation, suggesting it has influenced the current work.\n\n6. \"Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference, and Disease Name Recognition\" [b4]: The authors mention the idea of infusing knowledge into the LM, which seems to be inspired by this paper.\n\nThe following JSON object represents the source papers and the confidence scores:\n\n{\n\"b13\": 0.9,\n\"b10\": 0.8,\n\"b3\": 0.7,\n\"b1\": 0.7,\n\"b16\": 0.8,\n\"b4\": 0.7\n}\n\nThese scores are subjective and are based on the frequency of mentions and the importance of the concepts borrowed from each paper.", "Based on the given text, the following references appear to be the most relevant:\n\n1. b1: \"Graph representation learning: A survey\" - This appears to be the source of the random walk algorithm used to generate self-supervision signals, which is a key part of the proposed method. \n2. b13: \"TaxoExpan: Self-supervised taxonomy expansion with position-enhanced graph neural network\" - This paper seems to be referred to a lot, indicating that it likely inspired the authors. It's the source of other taxonomy expansion methods that the authors compare their method to.\n3. b14: \"Enquire one's parent and child before decision: Fully exploit hierarchical structure for self-supervised taxonomy expansion\" - This is another paper that discusses existing methods for taxonomy expansion that the authors compare their method to.\n4. b16: \"Taxonomy completion via triplet matching network\" - This paper mentioned several times, which focuses on the prompt-based methods that inspired the authors' approach.\n\nThe confidence scores for these papers could be as follows (on a scale of 0 to 1):\n\n- b1: 0.9\n- b13: 0.7\n- b14: 0.6\n- b16: 0.8 \n\nThe JSON output:\n```json\n{\n    \"b1\": 0.9,\n    \"b13\": 0.7,\n    \"b14\": 0.6,\n    \"b16\": 0.8\n}\n```", "Based on the text provided, the source papers that most likely inspired the author are:\n\n1. \"TaxoExpan: Self-Supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network\" [b13]\n2. \"Enquire One\u2019s Parent and Child Before Decision: Fully Exploit Hierarchical Structure for Self-Supervised Taxonomy Expansion\" [b14]\n3. \"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\" [b16]\n4. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b3]\n5. \"Graph Representation Learning: A Survey\" [b1]\n6. \"Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference, and Disease Name Recognition\" [b4]\n\nThe above papers are inferred based on the key phrases indicating their direct influence on the paper's core contributions, and their methodological similarities as well as novel contributions to the respective field.\n\nThe result will be:\n\n{ \"b1\": 0.8, \"b3\": 0.9, \"b4\": 0.7, \"b13\": 1, \"b14\": 1, \"b16\": 0.95}\n \nThis indicates the source papers and their influence level, where 1 is the highest influence level and 0 is the lowest.", "Based on the text of the academic paper and the titles of the given references, the source papers that most inspire the author are likely to be:\n\n- \"TaxoExpan: Self-Supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network\" [b13]\n- \"Enquire One's Parent and Child Before Decision: Fully Exploit Hierarchical Structure for Self-Supervised Taxonomy Expansion\" [b14]\n- \"Taxonomy completion via Triplet Matching Network\" [b16]\n- \"Graph Representation Learning: A Survey\" [b1]\n- \"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\" [b10]\n\nThese references are chosen because the author specifically mentions their methods (e.g., prompt tuning, random walk algorithm, and taxonomy expansion) and their influence on the proposed framework, TaxoPrompt. The paper refers to these source papers when discussing the methodology and the novel contributions, indicating their direct relevance to the research.\n\nHere is the final result in the requested JSON format:\n\n{\n    \"b13\": 0.9,\n    \"b14\": 0.9,\n    \"b16\": 0.8,\n    \"b1\": 0.7,\n    \"b10\": 0.7\n}\nThe confidence scores are subjective and represent the inferred level of influence each source paper has on the given paper, with 1 being the highest possible score.", "Based on the text and title analysis of the academic paper, the top references that likely inspired this work are:\n\n1. \"TaxoExpan: Self-supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network\" [b13]: The paper mentions this work multiple times and it appears that the proposed method, TaxoPrompt, is an extension or improvement of the TaxoExpan method. \n\n2. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b3]: The BERT model is used as the base model in TaxoPrompt, indicating a significant reliance on this work.\n\n3. \"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\" [b10]: This paper seems to be the source of the 'prompt' method used in TaxoPrompt, which is a key method in the paper's algorithm.\n\n4. \"Graph Representation Learning: A Survey\" [b1]: The random walk method used in TaxoPrompt is touted in this paper as an effective method for capturing the global structure information of graphs.\n\n5. \"Enhancing Taxonomy Completion with Concept Generation via Fusing Relational Representations\" [b15]: This previous work is mentioned in the context of local substructure modeling in taxonomies, which is something TaxoPrompt improves upon.\n\n6. \"A Robustly Optimized BERT Pretraining Approach\" [b9]: This paper is likely a source of inspiration as the TaxoPrompt uses a variation of BERT for its algorithm.\n\n7. \"Taxonomy Completion via Triplet Matching Network\" [b16]: This paper is likely a source of inspiration as it is mentioned several times in the text, and the TaxoPrompt method seems to be an improvement or extension of the methods described in this paper.\n\nIn conclusion, the source papers for this academic paper are likely [b13], [b3], [b10], [b1], [b15], [b9], and [b16] with the following confidence scores:\n\n```json\n{\n\"b13\": 0.9,\n\"b3\": 0.85,\n\"b10\": 0.8,\n\"b1\": 0.75,\n\"b15\": 0.7,\n\"b9\": 0.65,\n\"b16\": 0.6\n}\n```\nThis is just an estimation and the actual inspiration may vary."], "63dcdb422c26941cf00b642d": ["Based on the text provided, it can be inferred that the following references are most pertinent to the paper:\n\n1. \"b22\": 'sgas sequential greedy architecture search'\n2. \"b5\": 'neural architecture search on imagenet in four gpu hours a theoretically inspired perspective'\n3. \"b27\": 'neural architecture search without training'\n4. \"b34\": 'efficient neural architecture search via parameter sharing'\n5. \"b31\": 'a tale of evil twins adversarial inputs versus poisoned models'\n6. \"b15\": 'badnets identifying vulnerabilities in the machine learning model supply chain'\n7. \"b18\": 'model reuse attacks on deep learning systems'\n8. \"b41\": 'neural cleanse identifying and mitigating backdoor attacks in neural networks'\n\nThe confidence score for each reference can be assigned as follows:\n\n```\n{\n    \"b22\": 0.9,\n    \"b5\": 0.85,\n    \"b27\": 0.8,\n    \"b34\": 0.85,\n    \"b31\": 0.75,\n    \"b15\": 0.7,\n    \"b18\": 0.7,\n    \"b41\": 0.75\n}\n```\n\nThis is based on their relevance to the paper's novel contributions and the frequency of their citations in the text.", "Based on the text of the paper and the titles of the references, the source papers that most likely inspired this paper are:\n\n1. \"The dark side of AutoML towards architectural backdoor search\"\n2. \"Neural architecture search without training\"\n3. \"Neural architecture search on ImageNet in four GPU hours: A theoretically inspired perspective\"\n4. \"NAS-Bench: Benchmarking NAS algorithms for architecture topology and size\"\n5. \"Towards practical deployment stage backdoor attack on deep neural networks\"\n6. \"Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks\"\n7. \"A tale of evil twins: Adversarial inputs versus poisoned models\"\n8. \"Backdoor Attacks on Deep Learning Systems Using Data Poisoning\"\n\nThe selection is primarily based on the content of the paper, which discusses the development and evaluation of a new backdoor attack method leveraging neural architecture search (NAS), and the titles of the references that suggest a strong relation to the topic. \n\nThe result in json format is:\n\n{\n  \"b0\": 1,\n  \"b27\": 0.9,\n  \"b5\": 0.9,\n  \"b12\": 0.8,\n  \"b35\": 0.8,\n  \"b41\": 0.8,\n  \"b31\": 0.7,\n  \"b15\": 0.7\n}", "Based on the text, the source papers that inspired the author the most can be identified as:\n\n- \"Zero cost proxies for lightweight NAS\" (b0): The author uses a NAS (Neural Architecture Search) based approach, and this paper could have provided insights into efficient methods for conducting NAS.\n- \"Neural architecture search on ImageNet in four GPU hours: a theoretically inspired perspective\" (b5): The author refers to the Neural Tangent Kernel (NTK), which was used as a metric in their research. This paper could have provided the theoretical and practical knowledge to implement NTK in their work.\n- \"Neural architecture search without training\" (b27): The author mentioned the use of NAS without training, which indicates that they may have used techniques or methodologies proposed in this paper.\n- \"Input aware dynamic backdoor attack\" (b29): The author's work is based on input-aware triggers, which is a central topic in this paper.\n- \"NAS-Bench: Benchmarking NAS algorithms for architecture topology and size\" (b12): The author uses the NATS-Bench search space in their work, which is likely inspired by this paper.\n- \"Regularized evolution for image classifier architecture search\" (b36): The author uses a variant of regularized evolution in their work, which could be inspired by this paper.\n\nThe JSON format of the result is:\n\n{\n  \"b0\": 0.85,\n  \"b5\": 0.95,\n  \"b27\": 0.90,\n  \"b29\": 1.0,\n  \"b12\": 0.85,\n  \"b36\": 0.80\n}", "Based on the provided text, the paper primarily discusses the concept of EVAS (Exploitable and Vulnerable Arch Search), a backdoor attack that leverages Neural Architecture Search (NAS). Here are the possible source papers:\n\n1. \"Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective\" [b5] - This paper is mentioned multiple times, and the authors' approach is based on Neural Tangent Kernel (NTK), which is a concept discussed in this paper.\n\n2. \"NAS Bench: Extending the Scope of Reproducible Neural Architecture Search\" [b12] - The authors' experiments are conducted on the NATS-Bench search space, which is likely introduced in this paper.\n\n3. \"Neural Architecture Search without Training\" [b27] - This paper is directly cited when the authors discuss their method of searching for exploitable architectures without having to train the target model or trigger generator.\n\n4. \"Sgas: Sequential Greedy Architecture Search\" [b22] - This paper is mentioned multiple times as a benchmark or a source of comparison. The authors' experiments are compared to the models produced by SGAS.\n\n5. \"Input-Aware Dynamic Backdoor Attack\" [b29] - The authors of the analyzed paper adopt input-aware triggers, a concept probably introduced in this paper.\n\n6. \"Neural Tangent Kernel: Convergence and Generalization in Neural Networks\" [b17] - The concept of NTK, which the authors heavily relied on in their method, is likely introduced and detailed in this paper.\n\n7. \"Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks\" [b22] - The authors compare the effectiveness of their method with Fine-Pruning, a defense against backdoor attacks.\n\nHere is the information in the requested JSON format:\n```json\n{\n    \"b5\": 0.9,\n    \"b12\": 0.8,\n    \"b27\": 0.9,\n    \"b22\": 0.8,\n    \"b29\": 0.8,\n    \"b17\": 0.7,\n    \"b22\": 0.7\n}\n```\nThe confidence scores are subjective and based on the frequency of mentions, the importance of the concepts introduced in these papers to the authors' method, and the context in which they are mentioned.\n", "Based on the text, the author mentions several papers that inspire them. From those mentioned, the following references seem to be most pertinent:\n\n1. \"Neural architecture search on imagenet in four gpu hours a theoretically inspired perspective\" (\"b5\"): The author uses a similar approach to determining the vulnerability of an architecture.\n2. \"NAS-Bench: Benchmarking NAS algorithms for architecture topology and size\" (\"b12\"): The author uses the NAS-Bench search space in their experiment.\n3. \"Neural architecture search without training\" (\"b27\"): The author uses a similar approach to search for exploitable architectures without training.\n4. \"SGAS: Sequential greedy architecture search\" (\"b22\"): The author mentions this paper multiple times, indicating that it might be a source of inspiration for this work.\n5. \"Input-aware dynamic backdoor attack\" (\"b29\"): The author adopts input-aware triggers, which is a concept from this paper.\n\nBased on the information provided, the confidence score for each paper is hard to determine without additional context. However, all these papers seem to be central to the author's research, so a rough estimation might look something like this:\n\n```json\n{\n  \"b5\": 0.9,\n  \"b12\": 0.8,\n  \"b27\": 0.85,\n  \"b22\": 0.9,\n  \"b29\": 0.8\n}\n```\n\nThis is just a rough estimation and the actual value could be different based on the specific influence of each paper on the author's work."], "64250fee90e50fcafdb2d890": ["Based on the provided text and author notes, it seems that the main challenges outlined in the paper are the issues with the status quo of coverage instrumentation and collection, and the paper proposes a new approach that relies on a compiler to lower common automated coverage metrics to a single cover primitive that can be easily implemented for a wide range of different simulators.\n\nThe source papers that are directly relevant to the paper's novel contributions are:\n\n1. \"Chisel constructing hardware in a Scala embedded language\" [b4]: The paper indicates that the authors implemented their approach for the Chisel hardware construction language and the FIRRTL compiler.\n\n2. \"Reusability is FIRRTL ground hardware construction languages compiler frameworks and transformations\" [b11]: The authors mention that they write FIRRTL compiler passes to instrument the IR with automated coverage metrics, which is a key part of their novel approach.\n\n3. \"FIRRTL simulator called treadle\" [b17]: The authors mention that they provide support for a FIRRTL simulator called treadle.\n\n4. \"Efficiently exploiting low activity factors to accelerate RTL simulation\" [b5]: This paper is cited in the context of existing simulators that lack support for collecting and reporting automated coverage metrics, which is one of the issues that the authors' novel approach aims to address.\n\n5. \"FireSim FPGA-accelerated cycle exact scale out system simulation in the public cloud\" [b12]: The authors mention that they provide support for the FireSim simulator, indicating that their approach can be implemented for a wide range of different simulators.\n\n6. \"Verilator\" [b24]: This paper is also mentioned in the context of the authors' approach being implemented for a wide range of different simulators.\n\n7. \"Coverage measurement and analysis\" [b19]: This paper is cited in the context of the authors' discussion of coverage metrics, which are a key aspect of their novel approach.\n\nAll these references seem to be directly relevant to the paper's novel contributions and are preceded by phrases indicating direct inspiration or motivation.\n\nTherefore, the json format of the result is: \n\n{\n\"b4\": 1,\n\"b11\": 0.9,\n\"b17\": 0.8,\n\"b5\": 0.8,\n\"b12\": 0.8,\n\"b24\": 0.8,\n\"b19\": 0.8\n}", "Based on the text and the titles of the references, the source papers for this given paper are potentially:\n\n1. \"Chisel: Constructing Hardware in a Scala Embedded Language\" [b4]\n2. \"Reusability is FIRRTL Ground: Hardware Construction Languages, Compiler Frameworks, and Transformations\" [b11]\n3. \"FireSim: FPGA-Accelerated Cycle-Exact Scale-Out System Simulation in the Public Cloud\" [b12]\n4. \"Functional Verification Coverage Measurement and Analysis\" [b19]\n\nThe JSON result is:\n{\"b4\": 0.9, \"b11\": 0.85, \"b12\": 0.8, \"b19\": 0.75}\n\nThe confidence scores are based on how directly the papers seem to contribute to the novel contributions of the given paper. They are subjective and should be used as a general guide rather than an absolute measure of relevance.", "Based on the given text and author notes, the following references seem to be the most influential:\n\n1. \"b4\": 'chisel constructing hardware in a scala embedded language'\n2. \"b11\": 'reusability is firrtl ground hardware construction languages compiler frameworks and transformations'\n3. \"b12\": 'firesim fpga accelerated cycle exact scale out system simulation in the public cloud'\n4. \"b24\": 'verilator'\n\nThe phrases such as \"we implemented our approach for the Chisel hardware construction language and the FIRRTL compiler\", \"as well as a formal tool for trace generation\" and \"we also provide support for a FIRRTL simulator called treadle, for the ESSENT simulator, the FPGA-accelerated FireSim simulator as well as a formal tool for trace generation\" indicate the direct inspiration and reliance on these sources.\n\nThe JSON format of the result would be:\n{ \"b4\": 1, \"b11\": 1, \"b12\": 1, \"b24\": 1 }", "After carefully reading the entire text and evaluating the titles of each reference, the source papers that appear to be the most relevant to the paper's methodology and contributions are:\n\n1. 'b4': 'Chisel constructing hardware in a scala embedded language' - This paper appears to be a key source as the current paper constantly refers to Chisel language, which is the main subject of this reference. The authors also mention that they have implemented their approach for Chisel.\n\n2. 'b11': 'Reusability is FIRRTL ground hardware construction languages compiler frameworks and transformations' - This paper is directly related to the current paper's methodology as it discusses FIRRTL, a key component of the approach used in the current paper. \n\n3. 'b12': 'FireSim FPGA accelerated cycle exact scale out system simulation in the public cloud' - This reference is mentioned in the context of the application of the proposed method, making it a key source paper.\n\n4. 'b19': 'Functional verification coverage measurement and analysis' - This paper appears to be a key source as it discusses coverage metrics, a central topic in the current paper.\n\n5. 'b24': 'Verilator' - Although the title is not provided, the context of the text suggests that this paper presents the Verilator simulator, which is used in the current paper's methodology.\n\n6. 'b26': 'A golden age of hardware description languages applying programming language techniques to improve design productivity' - The current paper's methodology appears to be built on the concepts discussed in this paper, making it a key source.\n\nThe JSON format of the source papers with confidence scores is as follows:\n\n{ \"b4\": 0.95, \"b11\": 0.95, \"b12\": 0.9, \"b19\": 0.9, \"b24\": 0.85, \"b26\": 0.85 }", "Based on the provided text of the paper, the author is focused on improving the dynamic verification process of pre-silicon verification of digital circuit designs. The author's work is inspired by the challenges faced in the existing coverage instrumentation and collection, including lack of support for collecting and reporting automated coverage metrics by most open-source or innovative research simulators, custom implementation of metrics making merging difficult, and new hardware languages lacking support for source-level coverage metrics. The author proposes a new approach that relies on a compiler to lower common automated coverage metrics to a single cover primitive that can be easily implemented for a wide range of different simulators. The author's work seems to be heavily influenced by the Chisel hardware construction language and the FIRRTL compiler.\n\nBased on the provided information, the source papers that inspired the author's work most could be:\n\n{\n\"b4\": 0.8,\n\"b11\": 0.8,\n\"b5\": 0.7,\n\"b12\": 0.7,\n\"b17\": 0.6,\n\"b24\": 0.6\n}\n\nThe referenced papers b4 and b11 have a higher confidence score as they are directly related to the Chisel hardware construction language and the FIRRTL compiler which are central to the author's proposed solution. The papers b5, b12, b17, and b24 also seem relevant as they mention the challenges faced in the existing simulation platforms that the author aims to address."], "643e0acf0746dc40e3418ed8": ["Based on the provided text and the instruction, we can infer the most pertinent source papers that inspired the paper's novel contributions by looking for phrases indicating direct inspiration or motivation, as well as identifying the novel methods and approaches introduced in the paper. The novel contribution of the paper is the Multi-view Graph Encoder (MVGE) framework to encode different signals in graph representation learning and the use of multiple pretext tasks to capture the diversity of signals within graphs.\n\nThe key phrases that indicate direct inspiration or motivation in the provided text are \"Motivated by the above discussed limitations with one pretext task\" and \"inspired by the approaches that explore the combination of multiple pretext tasks to learn different signals\".\n\nThe references directly linked to these phrases and the novel contributions of the paper are as follows:\n\n- b21: \"Graph Representation Learning beyond Node and Homophily\" - This is the author's previous work and it is directly mentioned as an inspiration for the current paper, indicating that it heavily influenced the main contributions.\n\n- b9: \"Contrastive Multi-view Representation Learning on Graphs\" - This paper is mentioned in relation to the exploration of multiple pretext tasks, which is a key component of the paper's novel contribution.\n\n- b15: \"Multi-scale Contrastive Siamese Networks for Self-supervised Graph Representation Learning\" - Similar to b9, this reference is associated with the idea of using multiple pretext tasks for learning.\n\n- b2: \"Beyond Low Frequency Information in Graph Convolutional Networks\" - This reference discusses the importance of high-frequency information in graph convolutional networks, which is relevant to the paper's focus on capturing both high and low-frequency signals.\n\n- b48: \"Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs\" - This paper discusses the limitations of the homophily assumption in graph neural networks, which is a central theme of the paper.\n\nBased on the information provided, these references are the ones that appear to have the most direct relevance and influence on the novel contributions of the paper. The confidence scores are subjective assessments of how directly each reference is linked to the paper's contributions based on the provided text.\n\nHere is the list in JSON format with hypothetical confidence scores:\n\n```json\n{\n  \"b21\": 0.95,\n  \"b9\": 0.85,\n  \"b15\": 0.85,\n  \"b2\": 0.80,\n  \"b48\": 0.80\n}\n```\n\nPlease note that the confidence scores are illustrative and would need to be determined more precisely through a thorough reading and analysis of both the source and the referenced papers.", "Based on the provided text and instructions, we need to identify the source papers that have directly inspired or motivated the novel contributions of the given paper. The novel contributions of the paper are related to:\n\n1. The development of a Multi-view Graph Encoder (MVGE) framework to encode different signals in graphs.\n2. The design of two novel pretext tasks for capturing high and low-frequency signals within node features.\n3. The proposal of a method to integrate embeddings from multiple perspectives into one uniform semantic space.\n\nWe will focus on references that are linked with phrases indicating inspiration or motivation, and those that are methodologically similar or compared to the proposed methods. We are instructed to ignore references that describe historical findings and are not directly related to the paper's novel contributions.\n\nLet's analyze the text:\n\nThe phrase \"Motivated by the above-discussed limitations with one pretext task\" indicates that the proposed MVGE framework is a response to the limitations of existing methods. This suggests that references associated with the limitations of the pretext task may have inspired the MVGE.\n\nThe introduction of the paper mentions \"Our previous work, PairE [b21]\" which tries to encode both low-frequency and high-frequency signals. This direct mention of the authors' previous work suggests that it played a significant role in leading to the development of MVGE.\n\nAdditionally, the section on \"MULTI-VIEW GRAPH ENCODER MVGE\" states \"MVGE is inspired by the approaches that explore the combination of multiple pretext tasks to learn different signals [b9] [b15] [b16] [b21].\" This explicitly indicates the references that inspired the MVGE.\n\nGiven this information, we can infer the following references are most pertinent:\n\n- Reference [b21]: Directly mentioned as the authors' previous work, indicating a strong influence on the MVGE framework.\n- Reference [b9]: Specifically mentioned as inspiration for combining multiple pretext tasks.\n- Reference [b15]: Mentioned as inspiration for the combination of multiple pretext tasks.\n- Reference [b16]: Also mentioned as inspiration for learning different signals.\n\nWe can provide these references in the requested JSON format with a confidence score, representing our confidence in the relevance and influence of each reference based on the text:\n\n```json\n{\n  \"b21\": 1.0,\n  \"b9\": 0.9,\n  \"b15\": 0.9,\n  \"b16\": 0.8\n}\n```\n\nThe confidence scores are subjective estimates based on the explicit mentions of inspiration in the text and the apparent relevance to the novel contributions of the paper. The highest confidence score is given to the authors' previous work, which is likely to have the most direct influence. Other scores are slightly lower to reflect that while they are mentioned as inspirations, they might not have as direct an influence as the authors' own prior work.", "Based on the provided excerpt and instructions, let's identify the source papers that most directly inspire the novel contributions of the given paper. The novel contributions are:\n\n1. Multi-view Graph Encoder (MVGE) framework to encode different signals in graphs.\n2. New pretext tasks designed for capturing high and low-frequency signals within node features.\n3. A method for integrating embeddings from multiple perspectives into a coherent semantic space.\n\nThe provided excerpt mentions the following key phrases and references:\n\n- \"Motivated by the above discussed limitations with one pretext task, this paper proposes...\" - This indicates that the new paper proposes something novel, motivated by limitations of existing work.\n- \"Our previous work, PairE [b21] tries to encode both low-frequency and high-frequency signals...\" - This suggests that [b21] is a direct precursor to the current work.\n\nBased on these criteria, the following references are identified as most relevant to the novel contributions of the paper:\n\n- [b21]: \"Graph representation learning beyond node and homophily\" seems to be a direct precursor to the novel MVGE framework as the authors mentioned it in the context of their previous work.\n- [b2]: \"Beyond low frequency information in graph convolutional networks\" appears relevant to the novel contribution of capturing different types of signals within node features.\n- [b9]: \"Contrastive multi view representation learning on graphs\" might have inspired the multi-view approach for learning different signals.\n- [b15]: \"Multi scale contrastive siamese networks for self supervised graph representation learning\" could have influenced the design of pretext tasks for capturing high and low-frequency signals.\n- [b48]: \"Beyond homophily in graph neural networks current limitations and effective designs\" is likely relevant as it discusses the limitations of GRL under the homophily assumption, which the novel contributions aim to address.\n- [b38]: \"Augmentation free graph contrastive learning\" could be relevant to the method of integration of embeddings from multiple perspectives, as it deals with contrastive learning without augmentation, which is a form of multi-perspective learning.\n\nBased on the confidence of their relevance, the results in the JSON format are as follows:\n\n```json\n{\n  \"b21\": 1.0,\n  \"b2\": 0.8,\n  \"b9\": 0.8,\n  \"b15\": 0.7,\n  \"b48\": 0.7,\n  \"b38\": 0.6\n}\n```\n\nThe confidence scores are subjective estimations based on the provided instructions and the context in which the references are mentioned within the paper. The key paper [b21] is given the highest score due to it being directly mentioned as the authors' previous work, which directly precedes the novel contributions of the current paper. Other references are scored slightly lower as they seem to be influential but not as directly connected to the paper's novel contributions as [b21].", "To find the most pertinent source papers that heavily influenced the main contributions of the given paper, we need to parse through the text and locate key phrases that indicate a strong reliance on previous research. Based on the author notes and the text of the paper, let's identify the source papers that have inspired the novel contributions regarding the Multi-view Graph Encoder (MVGE) framework for unsupervised Graph Representation Learning (GRL) with a focus on capturing both low-frequency and high-frequency signals in graphs.\n\nFrom the text, the following phrases indicate a direct inspiration from previous work:\n1. \"Motivated by the above discussed limitations with one pretext task, this paper proposes a novel Multi-view Graph Encoder(MVGE) framework...\"\n2. \"Our previous work, PairE [b21] tries to encode both low-frequency and high-frequency signals among connected nodes by employing node pairs as basic embedding units.\"\n3. \"MULTI-VIEW GRAPH ENCODER MVGE is inspired by the approaches that explore the combination of multiple pretext tasks to learn different signals [b9] [b15] [b16] [b21].\"\n\nBased on these phrases, we can infer that the following references were directly relevant to the paper's novel contributions:\n\n- Reference [b21] is mentioned as the authors' previous work that tries to encode both low-frequency and high-frequency signals, which is a direct precursor to the MVGE framework.\n- References [b9], [b15], and [b16] are cited as inspiration for exploring the combination of multiple pretext tasks to learn different signals, which is a key aspect of the MVGE framework.\n\nNow, to provide a concise list of source papers with a confidence score, let's format the results in the requested JSON format:\n\n```json\n{\n  \"b21\": 1.0,\n  \"b9\": 0.8,\n  \"b15\": 0.8,\n  \"b16\": 0.8\n}\n```\n\nThe confidence score reflects the direct mention and relevance of the source papers to the novel contributions of the paper. Reference [b21] is given the highest score because it is the authors' own previous work and directly relates to the novel MVGE framework. Other references ([b9], [b15], [b16]) are given slightly lower scores as they inspired the approach but may not be as central as [b21] to the development of MVGE.", "Based on the provided text and instructions, we need to identify key phrases that indicate a strong reliance on previous research for developing the paper's core contributions. The novel contributions of the paper are the Multi-view Graph Encoder (MVGE) framework, which addresses the limitation of existing Graph Representation Learning (GRL) methods that are designed under a strong homophily assumption, and the introduction of multiple novel pretext tasks to capture different types of signals in graphs.\n\nLooking at the text provided, the key phrases that indicate inspiration or motivation from previous works are:\n\n- \"Motivated by the above discussed limitations with one pretext task\"\n- \"Our previous work, PairE [b21] tries to encode both low-frequency and high-frequency signals among connected nodes by employing node pairs as basic embedding units.\"\n- \"Inspired by the approaches that explore the combination of multiple pretext tasks to learn different signals [b9] [b15] [b16] [b21].\"\n\nBased on these phrases, we can infer that references [b9], [b15], [b16], and [b21] are directly relevant to the paper's novel contributions. The author mentions their own previous work, PairE [b21], which also informs the development of the MVGE framework. References [b9], [b15], and [b16] are mentioned in the context of exploring the combination of multiple pretext tasks, which is a central theme of the MVGE framework.\n\nWe should not include references that are only providing background information on the challenges in the field, such as the historical findings or the general limitations of current GRL methods under the homophily assumption.\n\nThe confidence scores are subjective and depend on the context in which the references are mentioned. Based on how directly related they are to the key contributions of the paper, we can assign the following confidence scores:\n\n```json\n{\n    \"b9\": 0.8,\n    \"b15\": 0.8,\n    \"b16\": 0.8,\n    \"b21\": 1.0\n}\n```\n\nReference [b21] is given the highest confidence score because it is the author's previous work and is likely to have a strong influence on the current paper. The other references are given slightly lower confidence scores because, while they are mentioned as inspiration, they are not the author's own work and thus may have a slightly less direct impact on the novel contributions of the paper."], "634cc7a390e50fcafd162fef": ["Based on the text and the titles of the references, the identified source papers that have most inspired the given paper are:\n\n1. \"revisiting heterophily for graph neural networks\" - This is the title of the paper itself, indicating it is the primary source of the discussion and analysis. Confidence: 1.0\n\n2. \"beyond homophily in graph neural networks current limitations and effective designs\" [b44] - Mentioned multiple times in the context of the comparison of the performance of the proposed method with the existing methods. Confidence: 0.9\n\n3. \"adaptive universal generalized pagerank graph neural network\" [b7] - It is mentioned in the context of the development of the Adaptive Channel Mixing (ACM) GNN framework. Confidence: 0.8\n\n4. \"complete the missing half augmenting aggregation filtering with diversification for graph convolutional networks\" [b30] - It is also mentioned in the context of the development of the ACM GNN framework. Confidence: 0.8\n\n5. \"non local graph neural networks\" [b27] - Referenced in the context of discussing the heterophily problem. Confidence: 0.7\n\n6. \"simplifying graph convolutional networks\" [b40] - The given paper uses the SGC [b40] model for empirical evaluation and comparison on synthetic graphs. Confidence: 0.7\n\n7. \"semi supervised classification with graph convolutional networks\" [b18] - This paper's work (GCN [b18]) is also used for empirical evaluation and comparison on synthetic graphs. Confidence: 0.7\n\n8. \"mixhop higher order graph convolutional architectures via sparsified neighborhood mixing\" [b0] - Mentioned in the context of defining homophily metrics. Confidence: 0.6\n\nHere is the json format:\n{\n\"b44\": 0.9,\n\"b7\": 0.8,\n\"b30\": 0.8,\n\"b27\": 0.7,\n\"b40\": 0.7,\n\"b18\": 0.7,\n\"b0\": 0.6\n}\n", "Based on the provided text and the titles of the references, the most pertinent source papers that inspired the author are:\n\n1. b44: 'Beyond homophily in graph neural networks current limitations and effective designs'\n2. b7: 'Adaptive universal generalized pagerank graph neural network'\n3. b42: 'Two sides of the same coin heterophily and oversmoothing in graph convolutional neural networks'\n4. b18: 'Semi supervised classification with graph convolutional networks'\n5. b30: 'Complete the missing half augmenting aggregation filtering with diversification for graph convolutional networks'\n6. b27: 'Non local graph neural networks'\n7. b40: 'Simplifying graph convolutional networks'\n8. b15: 'Bernnet learning arbitrary graph spectral filters via Bernstein approximation'\n\nThese papers are mentioned in the context of developing the paper's core contributions and addressing the challenges outlined in the paper. \n\nThe confidence score for each reference is estimated as:\n\n{\"b44\": 0.9, \"b7\": 0.85, \"b42\": 0.85, \"b18\": 0.8, \"b30\": 0.8, \"b27\": 0.75, \"b40\": 0.75, \"b15\": 0.7}", "Based on the given text, the following are the papers that seem to have inspired the author the most:\n\n1. \"Beyond homophily in graph neural networks current limitations and effective designs\" [b44]: The author mentions this paper multiple times, and it seems to form a significant part of the discussion on heterophily and its impact on GNNs. It is also used as a basis for comparison in the section on empirical evaluation and comparison on synthetic graphs.\n\n2. \"Complete the missing half augmenting aggregation filtering with diversification for graph convolutional networks\" [b30]: The approach of combining aggregation, diversification, and identity channels in the proposed Adaptive Channel Mixing (ACM) framework is likely inspired by this paper.\n\n3. \"Adaptive universal generalized pagerank graph neural network\" [b7]: This paper is also mentioned multiple times and is likely a source of inspiration for the ACM framework.\n\n4. \"Semi supervised classification with graph convolutional networks\" [b18]: This paper is cited in the context of the aggregation operator used in ACM, suggesting that it may have influenced the author's approach.\n\n5. \"Bernnet learning arbitrary graph spectral filters via bernstein approximation\" [b15]: The paper is mentioned in the context of ACM, suggesting that it has inspired the author's development of the ACM framework.\n\n6. \"Relational inductive biases deep learning and graph networks\" [b2]: This paper is cited in relation to the homophily assumption, which forms a key part of the author's discussion on the limitations of existing GNNs.\n\nThe confidence score for each reference can be estimated as follows:\n\n```json\n{\n\"b44\": 0.9,\n\"b30\": 0.85,\n\"b7\": 0.85,\n\"b18\": 0.8,\n\"b15\": 0.8,\n\"b2\": 0.75\n}\n```\n\nThe confidence score is based on how often and in what context each paper is mentioned in the text. Higher scores are given to papers that are mentioned multiple times or in important contexts (such as the development of the paper's main contributions), while lower scores are given to papers that are mentioned less often or in less significant contexts.", "Based on the paper text and the titles of the references, the source papers that most directly inspired the given paper's core contributions are:\n\n1. 'Revisiting Heterophily for Graph Neural Networks' [None]: The given paper directly references this work in the context of addressing the heterophily problem, which is a main theme in the given paper. Confidence Score: 0.9\n\n2. 'Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs' [b44]: This work is cited multiple times in the context of the heterophily problem and in comparison with the proposed algorithm. Confidence Score: 0.85\n\n3. 'Non-local Graph Neural Networks' [b27]: Mentioned in the context of the novel ACM framework proposed in the paper. Confidence Score: 0.8\n\n4. 'Adaptive Universal Generalized Pagerank Graph Neural Network' [b7]: This paper is cited multiple times in the context of the heterophily problem and the proposed ACM framework. Confidence Score: 0.85\n\n5. 'BernNet: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation' [b15]: This paper is referenced multiple times in the context of the proposed ACM framework and its difference with existing methods. Confidence Score: 0.8\n\n6. 'Complete the Missing Half: Augmenting Aggregation Filtering with Diversification for Graph Convolutional Networks' [b30]: This paper is mentioned in the context of diversification operation in the proposed ACM framework. Confidence Score: 0.8\n\n7. 'Beyond Low Frequency Information in Graph Convolutional Networks' [b3]: This paper is referenced in the context of high-frequency graph signals, and their importance in addressing heterophily. Confidence Score: 0.75\n\nThe JSON format result is:\n\n{\n  \"None\": 0.9,\n  \"b44\": 0.85,\n  \"b27\": 0.8,\n  \"b7\": 0.85,\n  \"b15\": 0.8,\n  \"b30\": 0.8,\n  \"b3\": 0.75\n}", "Based on the text of the academic paper, the source papers that inspire the given paper most are:\n\n1. [b44]: \"Beyond homophily in graph neural networks current limitations and effective designs\"\n2. [b27]: \"Non-local graph neural networks\"\n3. [b7]: \"Adaptive universal generalized pagerank graph neural network\"\n4. [b15]: \"Bernnet learning arbitrary graph spectral filters via bernstein approximation\"\n5. [b30]: \"Complete the missing half augmenting aggregation filtering with diversification for graph convolutional networks\"\n6. [b18]: \"Semi-supervised classification with graph convolutional networks\"\n7. [b38]: \"An adaptive filter bank equalizer for speech enhancement\"\n\nThese papers are selected based on the key phrases indicating their contribution to the development of the given paper's core contributions and methods.\n\nHere is the result in the json format:\n\n{\n    \"b44\": 1,\n    \"b27\": 0.95,\n    \"b7\": 0.9,\n    \"b15\": 0.85,\n    \"b30\": 0.8,\n    \"b18\": 0.75,\n    \"b38\": 0.7\n} \n\nPlease note that the confidence score is based on the frequency and importance of each paper's mention in the given paper's text."], "643e0ad10746dc40e3419478": ["{\n    \"b36\": 0.9,\n    \"b34\": 0.9,\n    \"b8\": 0.8,\n    \"b6\": 0.7,\n    \"b2\": 0.7,\n    \"b31\": 0.6\n}", "{\n    \"b36\": 0.9,\n    \"b34\": 0.85,\n    \"b8\": 0.8,\n    \"b6\": 0.75,\n    \"b12\": 0.75,\n    \"b2\": 0.7\n}", "{\n  \"b36\": 0.95,\n  \"b34\": 0.95,\n  \"b8\": 0.85,\n  \"b6\": 0.8\n}", "{\n    \"b36\": 0.95,\n    \"b34\": 0.95,\n    \"b8\": 0.90\n}", "{\n    \"b36\": 0.9,\n    \"b34\": 0.9,\n    \"b8\": 0.8\n}"], "629041ac5aee126c0fb5da8a": ["{\n  \"b26\": 0.9,\n  \"b27\": 0.85,\n  \"b29\": 0.85\n}", "{\n  \"b26\": 0.95,\n  \"b27\": 0.9,\n  \"b29\": 0.9\n}", "{\n    \"b26\": 0.9,\n    \"b27\": 0.85,\n    \"b29\": 0.85,\n    \"b28\": 0.8,\n    \"b16\": 0.75,\n    \"b18\": 0.75\n}", "{\n  \"b26\": 0.9,\n  \"b27\": 0.9,\n  \"b29\": 0.9\n}", "```json\n{\n  \"b28\": 0.9,\n  \"b27\": 0.9,\n  \"b29\": 0.9,\n  \"b18\": 0.8,\n  \"b16\": 0.8,\n  \"b26\": 0.7\n}\n```"], "63e312ef90e50fcafdc191fd": ["Based on the key phrases mentioned in the text, the most relevant source papers to the given paper are:\n\n1. \"b46\" - Beyond text Incorporating metadata and label structure for multi-label document classification using heterogeneous graphs\n2. \"b58\" - Match metadata aware text classification in a large hierarchy\n3. \"b32\" - Parabel partitioned label trees for extreme classification with application to dynamic search advertising in www\n4. \"b43\" - Correlation networks for extreme multi-label text classification\n5. \"b23\" - OAG-BERT towards a unified backbone language model for academic knowledge services\n6. \"b59\" - Metadata induced contrastive learning for zero-shot multi-label text classification in www\n7. \"b37\" - An overview of Microsoft Academic Service (MAS) and applications\n\nHere is the result in JSON format:\n\n{\n\"b46\": 0.9,\n\"b58\": 0.8,\n\"b32\": 0.7,\n\"b43\": 0.6,\n\"b23\": 0.8,\n\"b59\": 0.7,\n\"b37\": 0.6\n}\n\nThe confidence scores are assigned based on the frequency and importance of the source papers in the text. The papers with key phrases indicating direct inspiration, such as \"inspired by [reference]\", are given higher confidence scores. The scores range from 0 to 1, with 1 indicating the highest confidence. The scores are subjective and may vary depending on the interpretation of the text.", "Based on the text and the titles of the references, the source papers that most likely inspired the given paper are:\n\n1. \"An overview of Microsoft Academic Service (MAS) and applications\" [b37]: This paper is referenced multiple times and is the source of the metadata used in the given paper. It also aligns with the given paper's focus on scientific literature tagging. Confidence score: 0.9.\n   \n2. \"Beyond text: Incorporating metadata and label structure for multi-label document classification using heterogeneous graphs\" [b46]: This paper is also referenced multiple times and directly corresponds to the given paper's topic of incorporating metadata for document classification. Confidence score: 0.9.\n   \n3. \"OAG-BERT: Towards a unified backbone language model for academic knowledge services\" [b23]: This paper presents a language model specifically for academic text and metadata, aligning with the given paper's methodological approach. Confidence score: 0.8.\n   \n4. \"Match: Metadata-aware text classification in a large hierarchy\" [b58]: This paper addresses the same challenges as the given paper, focusing on metadata-aware text classification. Confidence score: 0.7.\n   \n5. \"Parabel: Partitioned label trees for extreme classification with application to dynamic search advertising\" [b32]: This paper introduces the Parabel classifier, which is used in the given paper's methodology. Confidence score: 0.7.\n\nThe resulting json format is:\n\n{\"b37\": 0.9, \"b46\": 0.9, \"b23\": 0.8, \"b58\": 0.7, \"b32\": 0.7}", "{\"b46\": 1, \"b58\": 1, \"b44\": 0.9, \"b56\": 0.9, \"b59\": 0.9, \"b32\": 0.8, \"b43\": 0.8, \"b23\": 0.8, \"b3\": 0.6, \"b16\": 0.6, \"b47\": 0.6, \"b51\": 0.6, \"b55\": 0.6}", "Based on the text and the titles of the references, the most pertinent source papers are:\n\n1. \"b32\": \"parabel partitioned label trees for extreme classification with application to dynamic search advertising in www\"\n2. \"b43\": \"correlation networks for extreme multi label text classification\"\n3. \"b23\": \"oag bert towards a unified backbone language model for academic knowledge services\"\n4. \"b58\": \"match metadata aware text classification in a large hierarchy\"\n5. \"b46\": \"beyond text incorporating metadata and label structure for multi label document classification using heterogeneous graphs\"\n6. \"b37\": \"an overview of microsoft academic service mas and applications\"\n\nThese papers seem to have directly inspired the methods and approaches used in this paper, as they are mentioned in the context of addressing the challenges and developing the algorithm proposed by the authors.\n\nThe confidence score for these references are as follows (in the format of \u201creference number\u201d: \u201cconfidence score\u201d):\n\n{\"b32\": 0.9, \"b43\": 0.9, \"b23\": 0.8, \"b58\": 0.85, \"b46\": 0.8, \"b37\": 0.75}", "Based on the text of the paper and the titles of the references, the source papers that inspired the given paper most likely include:\n\nReference 'b23': 'OAG-BERT: Towards a Unified Backbone Language Model for Academic Knowledge Services'. The paper's method is heavily based on the OAG-BERT model, which is mentioned multiple times in the text.\n\nReference 'b43': 'Correlation Networks for Extreme Multi-label Text Classification'. This paper is cited when discussing the application of the Transformer model in multi-label text classification, a core aspect of the given paper.\n\nReference 'b32': 'Parabel: Partitioned Label Trees for Extreme Classification with Application to Dynamic Search Advertising in www'. Parabel is one of the main classifiers adopted in the paper.\n\nReference 'b44': 'MeshProbeNet: A Self-Attentive Probe Net for MeSH Indexing'. It is mentioned in the context of previous studies on scientific literature tagging.\n\nReference 'b58': 'MATCH: Metadata-Aware Text Classification in A Large Hierarchy'. As the given paper is focused on the effect of metadata on scientific literature tagging, this reference seems to be a key source of inspiration.\n\nReference 'b46': 'Beyond Text: Incorporating Metadata and Label Structure for Multi-label Document Classification Using Heterogeneous Graphs'. This is another paper that is focused on the effect of metadata, which is the main topic of the given paper.\n\nThe confidence score for each of these references can be determined by the frequency and context of their mentions in the given paper's text.\n\nThe result in json format would be:\n{ \"b23\": 1, \"b43\": 0.8, \"b32\": 0.9, \"b44\": 0.7, \"b58\": 0.9, \"b46\": 0.9}"], "6344dede90e50fcafd24d1af": ["Based on the text of the paper and the author's notes, the source papers that most inspired this work are:\n\n1. \"Improving language models by retrieving from trillions of tokens\" [b2]\n2. \"Language models are few-shot learners\" [b3]\n3. \"Pre-training of deep bidirectional transformers for language understanding\" [b6]\n4. \"Sentence BERT: Sentence embeddings using siamese BERT networks\" [b23]\n5. \"MPNET: Masked and permuted pre-training for language understanding\" [b27]\n6. \"Billion-scale similarity search with GPUs\" [b14]\n\nThe confidence scores for these papers would be:\n\n{\n\"b2\": 0.9,\n\"b3\": 0.8,\n\"b6\": 0.7,\n\"b23\": 0.8,\n\"b27\": 0.7,\n\"b14\": 0.7\n}\n\nThese papers are directly related to the main challenges and methods of the current paper, including large scale text de-duplication, BERT pre-training, sentence embedding, and similarity search. They are also directly linked to the paper's novel contributions, including the development of a large de-duplication dataset and the use of neural methods for robust textual de-duplication. These papers are not historical findings and are directly relevant to the paper's topic.", "Based on the text, the following source papers seem to have significantly influenced the main contributions of the given paper:\n\n- \"b2\": \"Improving language models by retrieving from trillions of tokens\" - The paper mentions this source when discussing the need for better understanding of test set leakage in language models.\n\n- \"b23\": \"Sentence BERT: Sentence embeddings using siamese BERT networks\" - The paper mentions drawing inspiration from this work for their own bi-encoder method.\n\n- \"b14\": \"Billion scale similarity search with GPUs\" - The paper mentions this source in the context of making neural deduplication highly scalable.\n\n- \"b30\": \"R reinforced ranker reader for open domain question answering\" - The paper mentions this source in the context of discussing the literature on open domain retrieval and question answering.\n\n- \"b18\": \"Deduplicating training data makes language models better\" - The paper mentions this source in the context of discussing the problems of duplication in training data.\n\nThe confidence scores are subjective and require a deeper understanding of the paper's content. However, one could tentatively assign higher confidence scores to \"b23\" and \"b14\" as they are directly tied to the methods used in the paper, and lower confidence scores to \"b2\", \"b30\", and \"b18\" which are more indirectly related.\n\nThe result in json format would then be:\n\n{\n\"b2\": 0.6,\n\"b23\": 0.9,\n\"b14\": 0.9,\n\"b30\": 0.6,\n\"b18\": 0.6\n}", "Based on the text of the paper and the author's note, the most pertinent source papers are:\n\n1. [b2] \"Improving language models by retrieving from trillions of tokens\" - The text mentions this source as a motivation for their work on retrieval-enhanced transformer model and the role of test set leakage in language models.\n\n2. [b23] \"Sentence BERT: Sentence embeddings using siamese BERT networks\" - The text directly mentions that their bi-encoder method was inspired by Sentence BERT (S-BERT) and used an S-BERT pre-trained bi-encoder as their base language model.\n\n3. [b18] \"Deduplicating training data makes language models better\" - This paper is mentioned in the context of the impact of duplication in training data on model performance, which is a central challenge this paper is addressing.\n\n4. [b14] \"Billion-scale similarity search with GPUs\" - The paper discusses using this method for massive scale similarity search, making it scalable and efficient.\n\n5. [b16] \"Dense passage retrieval for open-domain question answering\" - The paper mentions drawing inspiration from this source for their reranking approach.\n\n6. [b31] \"Scalable zero-shot entity linking with dense entity retrieval\" - Although not mentioned explicitly, this paper may have influenced the method of entity retrieval used in this study.\n\n7. [b10] \"Dimensionality reduction by learning an invariant mapping\" - The study has used S-BERT's online contrastive loss implementation which is based on this referenced paper.\n\nTherefore, the JSON format of the result should be: \n\n```\n{ \n  \"b2\": 0.9,\n  \"b23\": 0.9,\n  \"b18\": 0.8,\n  \"b14\": 0.85,\n  \"b16\": 0.85,\n  \"b31\": 0.75,\n  \"b10\": 0.8\n}\n```\n\nThe confidence scores are based on the level of influence the source papers seem to have on the given paper. Papers directly mentioned as inspirations or motivations have higher confidence scores.", "Based on the text and the titles of the references, the following references seem to be the most relevant sources of inspiration for the given paper:\n\n- \"b2: improving language models by retrieving from trillions of tokens\": The paper uses a retrieval-enhanced transformer model, which indicates that it has used this paper as a basis for developing its own model.\n  \n- \"b3: language models are few shot learners\": This paper is mentioned in the context of training data and the use of language models in the paper, showing its relevance.\n\n- \"b18: deduplicating training data makes language models better\": The paper discusses the importance of de-duplication in training data, which is a key theme in the given paper.\n\n- \"b23: sentence bert sentence embeddings using siamese bert networks\": The paper explicitly states that it has drawn inspiration from this paper for its bi-encoder method.\n\n- \"b14: billion scale similarity search with gpus\": The paper discusses the use of GPUs for similarity search, which is used in the given paper for de-duplication.\n\n- \"b16: dense passage retrieval for open domain question answering\": The re-ranking model used in the paper draws inspiration from the literature on open domain retrieval and question answering, which includes this paper.\n\n- \"b30: r reinforced ranker reader for open domain question answering\": This paper is also part of the literature on open domain retrieval and question answering, and therefore likely contributed to the development of the re-ranking model.\n\nTherefore, the JSON format output will be:\n\n{\n  \"b2\": 0.9,\n  \"b3\": 0.8,\n  \"b18\": 0.9,\n  \"b23\": 1.0,\n  \"b14\": 0.8,\n  \"b16\": 0.8,\n  \"b30\": 0.7\n}", "Based on the text of the paper and the notes provided, the following references appear to have influenced the paper's main contributions:\n\n{ \n\"b2\": 0.75, \n\"b3\": 0.6, \n\"b14\": 0.85, \n\"b15\": 0.6, \n\"b18\": 0.65, \n\"b23\": 0.8, \n\"b24\": 0.75, \n\"b30\": 0.6\n}\n\nThe confidence scores indicate the level of influence each source paper had on the given paper, with 1 being the most influential and 0 the least. The paper seems to draw heavily from [b14], [b23] and [b24], as they provided key methodological inspiration for the paper's novel contributions. The other papers also seem to have influenced the paper, although to a lesser extent."], "633ba44790e50fcafdfe4b50": ["Based on the provided text, the key papers that seem to have inspired this work are:\n\n- \"Training Graph Neural Networks with Layers\" [b29]. The paper mentions this work in the context of discussing various methods for efficient GNN training. This paper could have informed the author's understanding of GNN architecture design.\n\n- \"Sign: Scalable Inception Graph Neural Networks\" [b14]. This paper is mentioned as one of the works that use MLPs to accelerate GNNs. This could have inspired the author's idea to leverage MLPs for GNN training.\n\n- \"Graph MLP: Node Classification without Message Passing in Graph\" [b20]. This work is also mentioned in the context of using MLPs for GNN acceleration. The author's approach of transferring weights between MLP and GNN models may have been inspired by this work.\n\n- \"Graph Convolutional Networks: Algorithms, Applications, and Open Challenges\" [b66]. This paper could have provided the author with a comprehensive understanding of GNNs, informing their approach to the problem.\n\n- \"Inductive Representation Learning on Large Graphs\" [b16]. The author's work speeds up the training of the GraphSAGE model, which was proposed in this paper.\n\n- \"GraphSAINT: Graph Sampling Based Inductive Learning Method\" [b63]. The author's work also speeds up the training of the GraphSAINT model, which was proposed in this paper.\n\n- \"Cluster GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks\" [b5]. The author's work also speeds up the training of the ClusterGCN model, which was proposed in this paper.\n\n- \"Strategies for Pre-training Graph Neural Networks\" [b18]. Although the author's method differs from pre-training, this paper could have provided useful insights into the initial training of GNNs.\n\nBased on this, the following is the JSON output:\n\n{\n\"b29\": 0.85,\n\"b14\": 0.85,\n\"b20\": 0.9,\n\"b66\": 0.8,\n\"b16\": 0.8,\n\"b63\": 0.8,\n\"b5\": 0.8,\n\"b18\": 0.7\n}", "{\"b14\": 0.9, \"b29\": 0.7, \"b51\": 0.7, \"b65\": 0.9}", "Based on the text, the author uses an algorithm, training method, or approach previously proposed in other papers. The author identifies the core contributions of the paper as the use of MLPs to accelerate GNNs, the transfer of weights between MLP and GNN models, and the initialization of GNNs with the weights of its converged PeerMLP. The author uses phrases such as \"inspired us,\" \"we adopt,\" and \"Motivated by this,\" indicating reliance on previous research. \n\nThe following references seem to be the most relevant:\n\n- [b65]: \"MLPs are used to accelerate GNNs [b65] by decoupling GNNs to node features learning and graph structure learning.\" The reference [b65] seems to be directly linked to the main challenge and method of the paper. According to the title, \"graph less neural networks teaching old mlps new tricks via distillation\", this paper seems to be about using MLPs in neural networks, directly related to the core contribution of the paper.\n\n- [b29]: \"Recently, several approaches for more efficient GNNs training have been proposed, including novel architecture design [b51] [b60] [b29].\" The reference [b29] is listed along with a discussion about efficient GNN training methods. According to the title, \"training graph neural networks with layers\", this paper seems to be about training GNNs, which is related to the core contribution of the paper.\n\n- [b14]: \"MLPs are used to accelerate GNNs [b65] [b14] [b20] by decoupling GNNs to node features learning and graph structure learning.\" The reference [b14] seems to be directly linked to the main challenge and method of the paper. According to the title, \"sign scalable inception graph neural networks\", this paper seems to be about scalable GNNs, which is related to the core contribution of the paper.\n\n- [b20]: \"MLPs are used to accelerate GNNs [b65] [b14] [b20] by decoupling GNNs to node features learning and graph structure learning.\" The reference [b20] seems to be directly linked to the main challenge and method of the paper. According to the title, \"graph mlp node classification without message passing in graph\", this paper seems to be about using MLPs in GNNs for node classification, which is related to the core contribution of the paper.\n\n- [b51]: \"Recently, several approaches for more efficient GNNs training have been proposed, including novel architecture design [b51] [b60] [b29].\" The reference [b51] is listed along with a discussion about efficient GNN training methods. According to the title, \"simplifying graph convolutional networks\", this paper seems to be about improving the efficiency of GNNs, which is related to the core contribution of the paper.\n\n- [b60]: \"Recently, several approaches for more efficient GNNs training have been proposed, including novel architecture design [b51] [b60] [b29].\" The reference [b60] is listed along with a discussion about efficient GNN training methods. According to the title, \"l gcn layer wise and learned efficient training of graph convolutional networks\", this paper seems to be about efficient training of GNNs, which is related to the core contribution of the paper.\n\nThe confidence score for each reference is given as follows:\n- b65: 0.9\n- b29: 0.85\n- b14: 0.85\n- b20: 0.85\n- b51: 0.8\n- b60: 0.8\n\nThe returned JSON format result is:\n{\n\"b65\": 0.9,\n\"b29\": 0.85,\n\"b14\": 0.85,\n\"b20\": 0.85,\n\"b51\": 0.8,\n\"b60\": 0.8\n}", "Based on the text of the paper, the key phrases indicating inspiration and the titles of the references, the source papers are:\n\n1. [b29]: \"Training Graph Neural Networks with Layers\" (Confidence Score: 0.9) - The paper mentions novel architecture design as a method for efficient GNNs training, and [b29] seems to be a paper that could be related to this.\n\n2. [b65]: \"Graph-Less Neural Networks: Teaching Old MLPs New Tricks via Distillation\" (Confidence Score: 0.85) - The paper refers to using MLPs to accelerate GNNs and this paper seems to be directly related to that concept.\n\n3. [b16]: \"Inductive Representation Learning on Large Graphs\" (Confidence Score: 0.75) - The paper mentions GraphSAGE, which is a method proposed in this source paper.\n\n4. [b63]: \"GraphSAINT: Graph Sampling Based Inductive Learning Method\" (Confidence Score: 0.75) - The paper mentions GraphSAINT, which is a method proposed in this source paper.\n\n5. [b5]: \"Cluster GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks\" (Confidence Score: 0.75) - The paper mentions ClusterGCN, which is a method proposed in this source paper.\n\n6. [b27]: \"Semi-Supervised Classification with Graph Convolutional Networks\" (Confidence Score: 0.65) - The paper mentions Graph Convolutional Networks (GCN), which is a method proposed in this source paper.\n\n7. [b14]: \"SIGN: Scalable Inception Graph Neural Networks\" (Confidence Score: 0.65) - The paper refers to using MLPs to accelerate GNNs and this paper seems to be directly related to that concept.\n\n8. [b51]: \"Simplifying Graph Convolutional Networks\" (Confidence Score: 0.55) - The paper mentions novel architecture design as a method for efficient GNNs training, and [b51] seems to be a paper that could be related to this.\n\nThe JSON format of the output would be:\n```json\n{\n  \"b29\": 0.9,\n  \"b65\": 0.85,\n  \"b16\": 0.75,\n  \"b63\": 0.75,\n  \"b5\": 0.75,\n  \"b27\": 0.65,\n  \"b14\": 0.65,\n  \"b51\": 0.55\n}\n```", "{\"b29\": 0.9, \"b65\": 0.8, \"b14\": 0.7, \"b20\": 0.9, \"b16\": 0.6, \"b63\": 0.6, \"b5\": 0.6, \"b15\": 0.5}"], "628d9e805aee126c0f979841": ["Based on the provided text, it appears that the paper's main contributions revolve around the development of the RetroMAE model, which is a novel masked autoencoding framework for pre-training retrieval-oriented language models. The RetroMAE model introduces key techniques such as asymmetric network structure, asymmetric masking ratios, and decoding enhancement.\n\nHere are the most pertinent source papers based on the references in the text:\n\n- \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [b4]\" - Confidence Score: 0.9\n  The BERT model is mentioned as the backbone of the encoder used in RetroMAE. \n\n- \"Autoencoding via Transformers for Sentence Embedding Learning [b21]\" - Confidence Score: 0.8\n  This paper seems to have inspired the auto-encoding strategy used in RetroMAE.\n\n- \"Condenser: A Pre-training Architecture for Dense Retrieval [b6]\" - Confidence Score: 0.7\n  RetroMAE simplifies the existing auto-encoding based methods like Condenser.\n\n- \"XLNet: Generalized Autoregressive Pretraining for Language Understanding [b35]\" - Confidence Score: 0.7\n  The two-stream attention from this paper is used in the enhanced decoding of RetroMAE.\n\n- \"Masked Language Model [b6]\" and \"Replaced Token Detection [b2]\" - Confidence Score: 0.6\n  These methods are mentioned as part of the existing auto-encoding strategies that RetroMAE differs from.\n\n- \"Product Quantization for Nearest Neighbour Search [b12]\" - Confidence Score: 0.5\n  This paper is mentioned as a method for conducting dense retrieval efficiently, which is a key challenge that RetroMAE addresses.\n\nThe confidence scores are assigned based on the frequency and context in which each reference paper is mentioned, as well as the relevance to the primary challenges and solutions outlined in the paper. Please note that these scores are estimated and may require further refinement. \n\nThe returned result in the requested format is: \n{\n  \"b4\": 0.9,\n  \"b21\": 0.8,\n  \"b6\": 0.7,\n  \"b35\": 0.7,\n  \"b2\": 0.6,\n  \"b12\": 0.5\n}", "{\"b21\": 0.9, \"b2\": 0.8, \"b6\": 0.7, \"b35\": 0.65, \"b5\": 0.6, \"b4\": 0.5}", "Based on the text of the academic paper, the following references seem to be the main source papers that inspired the author:\n\n{\"b4\": 0.9, \"b21\": 0.9, \"b6\": 0.8, \"b35\": 0.7, \"b2\": 0.6, \"b32\": 0.6, \"b34\": 0.5, \"b28\": 0.5}\n\nExplanation:\n1. \"b4\": BERT is mentioned several times in the text and it is clear that it has heavily influenced the development of RetroMAE.\n2. \"b21\": SEED is mentioned as a specific method for data augmentation in the context of RetroMAE.\n3. \"b6\": SimCSE and Condenser are mentioned in terms of their techniques and strategies that RetroMAE is building upon.\n4. \"b35\": XLNET's method of two-stream attention is mentioned as an inspiration for RetroMAE.\n5. \"b2\": DiffCSE is mentioned in terms of its token replacement prediction task, which seems to have influenced the design of RetroMAE.\n6. \"b32\": TS-DAE is mentioned in the context of its utility for unsupervised sentence embedding learning, which is a key feature of RetroMAE.\n7. \"b34\": LaPraDoR is mentioned as a retrieval-oriented pretraining method that RetroMAE aims to improve upon.\n8. \"b28\": T5 is mentioned as a backbone of dual-encoders, which is a key feature of RetroMAE.", "Based on the text of the paper and the titles of the references, the most pertinent source papers that inspired the given academic paper can be identified as follows:\n\n- 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding' (b4): The paper mentions BERT as a backbone of the proposed model and refers to it multiple times in the text, indicating that the author likely drew inspiration from this work.\n- 'RoBERTa: A Robustly Optimized BERT Pretraining Approach' (b20): RoBERTa is also mentioned as a backbone of the model, implying it was a source of inspiration for the author.\n- 'Dense Passage Retrieval for Open-Domain Question Answering' (b14): This is mentioned in the context of the development of dense retrieval tasks, suggesting it likely influenced the author's work.\n- 'Auto-encoding' (b6): This paper is mentioned in the context of the auto-encoding strategy used in the present study, indicating it likely played a role in the development of the present paper.\n- 'Less is More: Pre-training a Strong Siamese Encoder Using a Weak Decoder' (b21): The author explicitly states that they adopt an asymmetric model structure, similar to the one used in this paper, suggesting it was a direct source of inspiration.\n- 'Masked Language Modeling' (b2): This paper is mentioned in the context of the masked auto-encoding process used in the present study, indicating it likely influenced the author's work.\n- 'XLNet: Generalized Autoregressive Pretraining for Language Understanding' (b35): This is mentioned in the context of two-stream attention, which is applied in the present study, suggesting it likely played a role in the development of the present paper.\n\nBased on the above analysis, the JSON format result is:\n\n{\n\"b4\": 0.9,\n\"b20\": 0.8,\n\"b14\": 0.8,\n\"b6\": 0.7,\n\"b21\": 0.9,\n\"b2\": 0.7,\n\"b35\": 0.8\n}", "Based on the text, the RetroMAE model seemed to draw inspiration from several papers. The papers are indicated by phrases such as \"RetroMAE adopts\", \"aware of the above defect\", \"continuous effort has been made\", \"one popular strategy is to\", \"Another popular strategy leverages auto-encoding (AE)\", \"in this paper, we propose\", \"The proposed pre-training framework not only simplifies the existing AE based methods\", \"the auto-encoding task becomes much more challenging compared with the existing methods\", \"secondly, it ensures training signals to be fully generated from each pre-training sentence\", \"knowing that the decoder only contains one-single layer, we propose the enhanced decoding with two-stream attention [b35] and position-specific attention mask [b5]\", \"we summarize the pre-training workflow of the encoding and enhanced decoding as Algorithm 1\". \n\nHere are the source papers with their confidence score:\n\n1. \"b35: xlnet generalized autoregressive pretraining for language understanding\": 0.9\n2. \"b5: unified language model pre training for natural language understanding and generation\": 0.9\n3. \"b21: less is more pre training a strong siamese encoder using a weak decoder\": 0.8\n4. \"b6: condenser a pre training architecture for dense retrieval\": 0.8\n5. \"b4: bert pre training of deep bidirectional transformers for language understanding\": 0.8\n6. \"b20: roberta a robustly optimized bert pretraining approach\": 0.8\n7. \"b2: diffcse difference based contrastive learning for sentence embeddings\": 0.7\n8. \"b0\": 0.7 (The title of this paper is not available)\n\nThe confidence score is high when the paper is mentioned along with the development of a key feature of the RetroMAE model. Lower confidence scores indicate that the paper was mentioned in context but may not have been a direct source of inspiration for the core contributions of the RetroMAE model."], "634d80a390e50fcafd4e7c23": ["Based on the text and title of the paper, the paper presents a new method for Event Extraction (EE), specifically for handling overlapped and nested EE scenarios. The paper proposes a new framework named OneEE, which transforms overlapping and nested EE into word-word relation recognition. The work heavily relies on the BERT model [b2] as the encoder to get contextualized word representations.\n\nHere are the most relevant source papers:\n\n1. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b2]: This is the base model used by the author for the creation of the OneEE framework. The confidence score would be 1 since it is directly mentioned and is central to their methodology.\n\n2. \"Event Extraction as Multi-Turn Question Answering\" [b16]: This paper seems to be cited multiple times, and it seems to be related to the challenges that this paper is addressing. The confidence score would be 0.9.\n\n3. \"CASEE: A Joint Learning Framework with Cascade Decoding for Overlapping Event Extraction\" [b29]: This paper is compared with the proposed model in the text suggesting that it is a source of inspiration for the current work. The confidence score would be 0.8.\n\n4. \"Jointly Multiple Events Extraction via Attention-Based Graph Information Aggregation\" [b21]: This paper seems to be related to the challenges that this paper is addressing in the context of joint extraction of multiple event triggers and arguments. The confidence score would be 0.7.\n\n5. \"Joint Event Extraction via Recurrent Neural Networks\" [b25]: This paper is mentioned in the context of traditional EE methods that the current paper seeks to improve upon. The confidence score would be 0.6.\n\n6. \"Exploring Pre-Trained Language Models for Event Extraction and Generation\" [b38]: This paper is mentioned as a prior study for overlapped and nested EE. The confidence score would be 0.6.\n\nThe confidence scores are based on the level of influence and direct relevance to the paper's novel contributions.\n\nThe JSON format of the result:\n```json\n{\n  \"b2\": 1,\n  \"b16\": 0.9,\n  \"b29\": 0.8,\n  \"b21\": 0.7,\n  \"b25\": 0.6,\n  \"b38\": 0.6\n}\n```", "Based on the provided information, here are the most pertinent source papers for the given paper:\n\n1. [b2]: 'bert pre training of deep bidirectional transformers for language understanding' - The paper mentions that it adopts BERT as the encoder to get contextualized word representations. This indicates that BERT has had a significant influence on the paper's approach.\n   \n2. [b16]: 'event extraction as multi turn question answering' - This paper is mentioned multiple times in the context of previous research that the current paper builds upon. It seems to be particularly relevant to the paper's discussion of complicated irregular EE scenarios and the problem with pipeline-based methods.\n\n3. [b20]: 'joint event extraction via structured prediction with global features' - This paper is mentioned as a representation of traditional methods for EE. While the current paper aims to address the limitations of such traditional methods, they still form an important foundation for the new research.\n   \n4. [b21]: 'jointly multiple events extraction via attention based graph information aggregation' - This paper is referred to in the context of the extraction of multiple event triggers and arguments. It seems to have made a significant contribution to the development of the current paper's method.\n   \n5. [b29]: 'casee a joint learning framework with cascade decoding for overlapping event extraction' - This is the state-of-the-art model that the current paper aims to improve upon. It is therefore a key source paper for the current research. \n\n6. [b38]: 'exploring pre trained language models for event extraction and generation' - This paper is mentioned in the context of previous studies for overlapped and nested EE, indicating that it is a key source paper for the current research.\n\nThe confidence scores for these papers are assumed to be 1 due to their significant influences on the current paper, forming its methodology and main contributions. \n\nThe json format of the result:\n\n{\"b2\":1, \"b16\":1, \"b20\":1, \"b21\":1, \"b29\":1, \"b38\":1}", "Based on the text of the paper and the titles of the references, the below references seem to have most inspired the author:\n\n1. [b16] \"event extraction as multi turn question answering\"\n2. [b2] \"bert pre training of deep bidirectional transformers for language understanding\"\n3. [b20] \"joint event extraction via structured prediction with global features\"\n4. [b21] \"jointly multiple events extraction via attention based graph information aggregation\"\n5. [b38] \"exploring pre trained language models for event extraction and generation\"\n6. [b40] \"what the role is vs what plays the role semi supervised event argument extraction via dual question answering\"\n7. [b29] \"casee a joint learning framework with cascade decoding for overlapping event extraction\"\n\nThe JSON format of the result with a confidence score is:\n\n{\n  \"b16\": 0.9,\n  \"b2\": 0.8,\n  \"b20\": 0.85,\n  \"b21\": 0.8,\n  \"b38\": 0.85,\n  \"b40\": 0.9,\n  \"b29\": 0.8\n}\n\nThe confidence score is set based on the number of times the source papers are referred to in the text, and the relevance of the source paper's title to the content of the current paper.", "Based on the description of the paper and the content of the referenced papers, the most influential source papers for this text include:\n\n1. 'b16': 'Event extraction as multi-turn question answering'\n2. 'b29': 'CASEE: A joint learning framework with cascade decoding for overlapping event extraction'\n3. 'b2': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'\n4. 'b38': 'Exploring pre-trained language models for event extraction and generation'\n5. 'b21': 'Jointly multiple events extraction via attention-based graph information aggregation'\n6. 'b25': 'Joint event extraction via recurrent neural networks'\n\nHere is a JSON representation of the high confidence source papers:\n\n```json\n{\n  \"b16\": 0.9,\n  \"b29\": 0.85,\n  \"b2\": 0.8,\n  \"b38\": 0.75,\n  \"b21\": 0.7,\n  \"b25\": 0.7\n}\n```\n\nThis is based on the fact that the mentioned papers represent the main challenges outlined in the paper, the proposed algorithm by the author, and the novel methods introduced to tackle these challenges.", "Based on the text and the author's notes, the source papers that most inspired the current work are:\n\n1. Reference 'b38': \"Exploring pre trained language models for event extraction and generation\" - This paper is referenced when discussing the challenges of irregular EE (overlapped and nested EE) and the pipeline-based methods used to address these. The current paper also uses a similar approach, suggesting inspiration from this source.\n\n2. Reference 'b16': \"Event extraction as multi turn question answering\" - This paper is mentioned multiple times throughout the text, indicating its significance. It introduces the concept of overlapping and nested EE and the problems associated with it, which is a core part of the current paper.\n\n3. Reference 'b29': \"CASEE a joint learning framework with cascade decoding for overlapping event extraction\" - This paper is referenced when discussing the state-of-the-art model for overlapping and nested EE. The current paper builds upon this work by introducing a novel tagging scheme and a new model, OneEE.\n\n4. Reference 'b2': \"BERT: Pre training of deep bidirectional transformers for language understanding\" - This paper is mentioned as the source of the encoder used in the OneEE model proposed in the current paper.\n\n5. Reference 'b21': \"Jointly multiple events extraction via attention-based graph information aggregation\" - This paper is referenced when discussing traditional EE methods and their limitations, which the current paper aims to address.\n\n6. Reference 'b25': \"Joint event extraction via recurrent neural networks\" - This paper is referenced when discussing traditional EE methods. The current paper uses a different approach, suggesting it might have been inspired by the limitations of this previous work.\n\n7. Reference 'b40': \"What the role is vs what plays the role semi supervised event argument extraction via dual question answering\" - This paper is mentioned in relation to the datasets used for evaluating the proposed OneEE model, suggesting its relevance and significance.\n\n{\"b38\": 0.90, \"b16\": 0.85, \"b29\": 0.80, \"b2\": 0.75, \"b21\": 0.70, \"b25\": 0.65, \"b40\": 0.60}"], "62b3da1e5aee126c0fb1b3bc": ["Based on the text of the academic paper, the source papers that might have inspired the author the most could be the following:\n\n1. \"Efficient and effective training of language and graph neural network models\" - The paper's main contribution is a framework (LM-GNN) for the joint training of large-scale language models and graph neural networks, which is the main topic of this cited paper. Confidence score: 0.9\n\n2. \"Graph Attention Networks\" - The paper introduces a model that uses graph attention networks, so this work likely served as an inspiration. Confidence score: 0.8\n\n3. \"Graph Convolutional Networks\" - The paper uses graph convolutional operations, which is a primary focus of this cited paper. Confidence score: 0.8\n\n4. \"Pre-training of deep bidirectional transformers for language understanding\" - The paper uses a transformer model (BERT) to encode text data, which is a prominent concept in this cited paper. Confidence score: 0.8\n\n5. \"Geometric deep learning: going beyond Euclidean data\" - This paper likely inspired the author as it discusses using GNN models for non-Euclidean data, a concept which is used in this paper. Confidence score: 0.7\n\n6. \"Distributed hybrid CPU and GPU training for graph neural networks on billion scale graphs\" - This paper discusses the challenges related to the large-scale training of GNN models, a topic which is also discussed in this paper. Confidence score: 0.7\n\n7. \"Regularization and semi-supervised learning on large graphs\" - The paper discusses semi-supervised learning, a learning task that is also addressed in this paper. Confidence score: 0.6\n\nBased on the context and the use of key phrases in the paper, these source papers are identified as the most relevant ones. Please note that the confidence scores are subjective and can vary based on further analysis. \n\nThe results are summarized below in the requested JSON format:\n\n{\n\"b5\": 0.9,\n\"b22\": 0.8,\n\"b16\": 0.8,\n\"b8\": 0.8,\n\"b21\": 0.7,\n\"b29\": 0.7,\n\"b3\": 0.6\n}\n", "Based on the text of the academic paper and the provided titles of the references, the source papers that inspired this paper the most appear to be:\n\n1. 'efficient and effective training of language and graph neural network models': This seems to be the main source of inspiration as the paper is based on the idea of developing a high-efficiency and effective framework for jointly training large-scale language models and graph neural network models. \n2. 'pre-training of deep bidirectional transformers for language understanding [b8]': The authors mention using the BERT model as the transformer in their LM-GNN framework, indicating this paper as a significant source of inspiration. \n3. 'modeling relational data with graph convolutional networks [b20]': The authors use a modified RGCN encoder in their LM-GNN framework which originates from this paper. \n4. 'node feature extraction by self supervised multi scale neighborhood prediction [b7]': The authors reference this paper when discussing a previous work that identifies the benefits of pre-training BERT models with graph data. \n5. 'distributed hybrid cpu and gpu training for graph neural networks on billion scale graphs [b29]': The authors extend the distributed GNN training framework from this paper to accommodate their end-to-end fine-tuning setting.\n\nTherefore, the JSON result would be:\n\n{\n    \"b0\": 0.9,\n    \"b8\": 0.85,\n    \"b20\": 0.8,\n    \"b7\": 0.75,\n    \"b29\": 0.7\n} \n\nThe confidence score is based on the inferred degree of influence on the main contributions of the paper.", "Based on the provided text, the following references seem to be the most influential in the development of this paper's contributions:\n\n1. 'b8': 'pre training of deep bidirectional transformers for language understanding'\n2. 'b7': 'node feature extraction by self supervised multi scale neighborhood prediction'\n3. 'b20': 'modeling relational data with graph convolutional networks'\n4. 'b12': 'representation learning on graphs'\n5. 'b26': 'a comprehensive survey on graph neural networks'\n6. 'b17': 'adsgnn behavior graph augmented relevance modeling in sponsored search'\n7. 'b31': 'textgnn improving text encoder via graph neural network in sponsored search'\n\nThese papers can be identified as influential based on the text mentioning specific methods, concepts, and algorithms that these papers have introduced or studied in depth. For example, 'pre training of deep bidirectional transformers for language understanding' (b8) is likely influential as the authors mention the use of BERT, a transformer model, in their proposed framework. Similarly, 'node feature extraction by self supervised multi scale neighborhood prediction' (b7) is mentioned in the context of pre-training BERT models with graph data, indicating it may have influenced this process. \n\nHowever, the confidence score for each paper cannot be determined without more information about how these papers directly contributed to the development of the methods and algorithms presented in the paper. \n\nThe result in json format would be: {\"b8\":0, \"b7\":0, \"b20\":0, \"b12\":0, \"b26\":0, \"b17\":0, \"b31\":0} with the confidence score that needs to be filled in.", "Based on the text of the paper, the primary challenges outlined are training large scale language models and graph neural networks efficiently and effectively. The author proposes a stage-wise fine-tuning framework, LM-GNN, for encoding text data with transformers and GNN models. By analyzing the text for key phrases indicating reliance on previous research and identifying the novel methods and approaches, the most pertinent source papers can be identified as follows:\n\n{\n\"b5\": 0.6, \n\"b12\": 0.8, \n\"b7\": 0.95, \n\"b20\": 0.65, \n\"b8\": 0.9, \n\"b26\": 0.7, \n\"b29\": 0.75\n}\n\nThe confidence scores are assigned based on the relevance of each reference paper to the main contributions of the paper. For example, reference [b7] is given a high confidence score of 0.95 as it is directly mentioned in the context of pre-training BERT models with graph data. Similarly, [b8] is given a high confidence score of 0.9 as it is used as the transformer in the LM-GNN framework. Other references such as [b5], [b12], [b20], [b26], and [b29] are also relevant but to a lesser degree, hence the lower confidence scores.", "The main challenges outlined in the paper are: \n1. Addressing the question of how to train better GNN models with rich text features. \n2. The integration of language models (LM) and graph neural networks (GNN) for efficient and effective training. \n3. Overcoming efficiency issues related to the large number of neighbors required by message passing in GNNs. \n4. Effectiveness challenges that arise from the fact that the pre-trained language model is well optimized in language tasks but has not been trained before in graph tasks. \n\nBased on the key phrases and the novel methods and approaches introduced to tackle these challenges, the following source papers seem to be most relevant:\n\n1. \"Pre-training of deep bidirectional transformers for language understanding\" [b8]: This paper is mentioned in the context of using the BERT model as the transformer in the LM-GNN framework, indicating its methodological influence on the current paper.\n2. \"Modeling relational data with graph convolutional networks\" [b20]: The current paper uses a modified RGCN encoder, which indicates that the methods from this paper have been adapted and used.\n3. \"Graph attention networks\" [b22]: This reference is likely relevant as it is mentioned in the context of achieving state-of-the-art performance in SSL.\n4. \"Node feature extraction by self supervised multi scale neighborhood prediction\" [b7]: This paper is mentioned when discussing previous work that also identifies the benefits of pre-training BERT models in graph data, but did not explore to fine-tune the BERT and GNN model together.\n5. \"Inductive representation learning on large graphs\" [b11]: It is used to explain the application of random sampling to reduce the number of neighbors in GNNs.\n6. \"Distributed hybrid CPU and GPU training for graph neural networks on billion scale graphs\" [b29]: This paper is used to explain the distributed GNN training.\n\nThe confidence scores for these references are not available in the provided context, hence I cannot provide them. However, based on the text it can be inferred that these are the most pertinent source papers. \n\nThe json format would look like this:\n{\n    \"b8\": \"unknown confidence score\",\n    \"b20\": \"unknown confidence score\",\n    \"b22\": \"unknown confidence score\",\n    \"b7\": \"unknown confidence score\",\n    \"b11\": \"unknown confidence score\",\n    \"b29\": \"unknown confidence score\"\n}"], "62e0acfd5aee126c0f20a05e": ["Based on the text of the paper, the most relevant source papers that inspire the author are:\n\n1. \"TVM: An automated end-to-end optimizing compiler for deep learning\" [b10]\n2. \"Open Neural Network Exchange\" [b1]\n3. \"Fuzzing: A survey\" [b27]\n4. \"Finding compiler bugs via live code mutation\" [b53]\n5. \"Automated conformance testing for JavaScript engines via deep compiler fuzzing\" [b65]\n6. \"Finding and understanding bugs in C compilers\" [b64]\n7. \"Fuzzing tensor level intermediate representation in TVM\" [b47]\n8. \"Exposing numerical bugs in deep learning via gradient back propagation\" [b62]\n\nBased on the text, these papers are the most directly relevant to the author's novel contributions, including the creation of a fuzz testing method for identifying bugs in deep learning compilers, the use of a gradient-based model input search method, and the use of differential testing to identify bugs.\n\nHere is the JSON format:\n\n{\"b10\": 0.9, \"b1\": 0.8, \"b27\": 0.7, \"b53\": 0.85, \"b65\": 0.75, \"b64\": 0.8, \"b47\": 0.9, \"b62\": 0.85}", "The paper proposes a fuzzy testing method to generate more effective test cases for deep learning compilers. The method includes three aspects: 1. generating sub-models of various patterns; 2. for the problem that nearly 41% of bugs are ignored due to floating-point exceptions in existing test cases, a gradient-based model input search method is proposed; 3. use the differential testing method to compare the output of the compiled model with the output of the common deep learning framework to determine the bug. \n\nBased on the given text and the titles of the references, the references [b10], [b28], [b32], [b56], [b62] and [b39] seem to be the most relevant sources that inspired the author. \n\nThis is because [b10] is about TVM, a deep learning compiler which is mentioned multiple times in the text. References [b28], [b32] and [b56] are about previous work on fuzzing and differential testing, which is the approach the paper adopts. [b62] is about exposing numerical bugs in deep learning via gradient back propagation, which relates to the gradient-based model input search method. Finally, [b39] is about the Z3 SMT solver which the paper uses for constraint solving.\n\n```json\n{\n  \"b10\": 0.9,\n  \"b28\": 0.8,\n  \"b32\": 0.8,\n  \"b56\": 0.8,\n  \"b62\": 0.7,\n  \"b39\": 0.7\n}\n```", "Based on the provided text and the author notes, the main challenges outlined in the paper are: 1) Generating structurally diverse and valid models for testing the deep learning compilers and 2) Finding suitable weights/inputs for the compiled model for differential testing. The authors propose a tester for deep learning compilers, called NNSmith, which addresses these challenges.\n\nFollowing the steps, here are the potential source papers:\n\n1. \"TVM: An Automated End-to-End Optimizing Compiler for Deep Learning\" [b10]: The TVM compiler is mentioned in the text, and it seems to be the main inspiration for developing the NNSmith testing tool.\n\n2. \"Onnx Runtime: Cross-Platform High-Performance ML Inferencing and Training Accelerator\" [b37]: The authors mention ONNXRuntime as one of the compilers they test using NNSmith.\n\n3. \"Coverage-Guided Tensor Compiler Fuzzing with Joint IR Pass Mutation\" [b28]: This paper has been mentioned in the context of previous work that the authors build upon and improve.\n\n4. \"Graph-based fuzz testing for deep learning inference engines\" [b32]: This paper is mentioned as previous work, which the authors of the current paper appear to build upon.\n\n5. \"Deep learning library testing via effective model generation\" [b56]: The authors compare their work to the approach described in this paper and appear to build on this work.\n\n6. \"Exposing numerical bugs in deep learning via gradient back propagation\" [b62]: The authors seem to have taken inspiration from this work for addressing the challenge of finding suitable weights/inputs for the compiled model for differential testing.\n\nBased on the above, the final json format would look like:\n\n{\n\"b10\" : 0.8,\n\"b37\" : 0.7,\n\"b28\" : 0.6,\n\"b32\" : 0.6,\n\"b56\" : 0.7,\n\"b62\" : 0.9\n}\n\nThe confidence score is an arbitrary value between 0 and 1, with 1 being the most confident. The scores here reflect the perceived impact each paper had on the current paper.", "Based on the text of the paper, the source papers that seem to have inspired the authors most directly are:\n\n1. \"finding deep learning compilation bugs with nnsmith\" - The paper is based on this work and extends it. Confidence Score: 1.0\n\n2. \"tvm an automated end to end optimizing compiler for deep learning\" - The paper discusses the use of TVM for optimizing deep learning models, indicating that it was a key inspiration. Confidence Score: 0.9\n\n3. \"coverageguided tensor compiler fuzzing with joint ir pass mutation\" - The authors adopt a fuzzing and differential testing approach similar to this work. Confidence Score: 0.8\n\n4. \"graph based fuzz testing for deep learning inference engines\" - The authors mention the challenges faced by the basic approach of fuzzing and differential testing, which this work addresses. Confidence Score: 0.6\n\n5. \"deep learning library testing via effective model generation\" - The authors discuss the use of model generation for testing, indicating that this work inspired them. Confidence Score: 0.6\n\n6. \"exposing numerical bugs in deep learning via gradient back propagation\" - The authors propose a gradient-guided search to avoid floating point exceptional values, which is similar to the approach in this work. Confidence Score: 0.7\n\n7. \"adam a method for stochastic optimization\" - The authors use Adam, an adaptive learning rate scheduling algorithm, for setting the learning rate, indicating that this work was a key inspiration. Confidence Score: 0.6\n\n8. \"z an efficient smt solver\" - The authors use an SMT solver for checking the satisfiability of constraints, which is similar to the approach in this work. Confidence Score: 0.5\n\nThe returned result in json format is:\n\n{\"finding deep learning compilation bugs with nnsmith\": 1.0, \"tvm an automated end to end optimizing compiler for deep learning\": 0.9, \"coverageguided tensor compiler fuzzing with joint ir pass mutation\": 0.8, \"graph based fuzz testing for deep learning inference engines\": 0.6, \"deep learning library testing via effective model generation\": 0.6, \"exposing numerical bugs in deep learning via gradient back propagation\": 0.7, \"adam a method for stochastic optimization\": 0.6, \"z an efficient smt solver\": 0.5}", "The source papers for the given paper are:\n\n1. \"Finding deep learning compilation bugs with NNSmith\" [b0]: This paper is the main inspiration for the given paper, as it proposes the system NNSmith for automated bug detection in deep learning compilers. The given paper heavily builds upon the techniques and methodology proposed in this work, including the use of fuzzing and differential testing, and the generation of diverse and valid computation graphs. Confidence score: 0.9\n\n2. \"Fuzzing: A survey\" [b27]: This paper provides a comprehensive overview of the concept of fuzzing, which is a key technique used in the given paper for generating test cases for bug detection. Confidence score: 0.75\n\n3. \"Graph-based fuzz testing for deep learning inference engines\" [b32]: This paper introduces the use of graph-based fuzz testing for deep learning, which is a method that the given paper adopts and extends. Confidence score: 0.7\n\n4. \"TVM: An automated end-to-end optimizing compiler for deep learning\" [b10]: This paper presents the TVM compiler, which is one of the deep learning compilers that the given paper tests for bugs. Confidence score: 0.65\n\n5. \"Exposing numerical bugs in deep learning via gradient back propagation\" [b62]: This paper discusses the problem of numerical bugs in deep learning, which is a key challenge that the given paper aims to address. In particular, the given paper uses a similar gradient-guided search method for generating valid model inputs and weights. Confidence score: 0.6\n\n6. \"Differential testing for software\" [b36]: This paper discusses the concept of differential testing, which is another key technique used in the given paper for comparing the results of different compilers. Confidence score: 0.6\n\n7. \"Deep learning library testing via effective model generation\" [b56]: This paper discusses the challenge of testing deep learning libraries, which is a related problem to the one addressed in the given paper. Confidence score: 0.55\n\nOverall, the given paper draws heavily from these source papers in its methodology, problem formulation, and proposed solutions.\n\nThe result is:\n\n{\n  \"b0\": 0.9, \n  \"b27\": 0.75, \n  \"b32\": 0.7, \n  \"b10\": 0.65, \n  \"b62\": 0.6, \n  \"b36\": 0.6, \n  \"b56\": 0.55\n}"], "62c28ae45aee126c0f8a1660": ["Based on the text and the author note, the source papers for this paper seem to be:\n- \"Graph neural architecture search\" [b9] for introducing the concept of automatic GNN architecture design\n- \"Semi-supervised classification with graph convolutional networks\" [b15], \"Graph attention networks\" [b33], \"How powerful are graph neural networks\" [b45], \"Inductive representation learning on large graphs\" [b11] for introducing the concept of GNN models\n- \"Darts differentiable architecture search\" [b22], \"Efficient neural architecture search via parameter sharing\" [b27] for introducing the concept of differential NAS and weight sharing\n\nThe reference papers which inspired the paper's novel methods most are:\n- \"Graph neural architecture search under distribution shifts\" [None]\n- \"Out of distribution (OOD) dataset curator and benchmark for AI-aided drug discovery: a focus on affinity prediction problems with noise annotations\" [b13] for introducing the problem of distribution shifts in real-world graph applications\n\nThe paper's novel contributions seem to be:\n- Proposing a new method, GRACES, to customize GNN architectures for each graph instance\n- Designing a self-supervised disentangled graph encoder, architecture self-customization with prototype strategy, and customized super-network\n- The first to study graph neural architecture search for graph classification under distribution shifts\n\nFrom these contributions, the source papers that directly contributed to these specific aspects are:\n- \"Disentangled contrastive learning on graphs\" [b20] and \"Graph self-supervised learning: a survey\" [b23] for introducing the concept of self-supervised learning and disentangled graph encoder\n- \"Principal neighbourhood aggregation for graph nets\" [b6] for introducing the concept of architecture self-customization\n- \"Heterogeneous graph transformer\" [b32] for introducing the concept of a super-network\n\nHence, the proposed source papers are:\n\n{\"b9\": 1, \"b15\": 0.8, \"b33\": 0.8, \"b45\": 0.8, \"b11\": 0.8, \"b22\": 0.9, \"b27\": 0.9, \"None\": 1, \"b13\": 1, \"b20\": 0.9, \"b23\": 0.9, \"b6\": 0.9, \"b32\": 0.9} \n\nThe confidence scores are assigned based on how closely these papers are likely to have influenced the main contributions of the paper.", "Based on the text of the academic paper and the titles of the references, the source papers that most inspired the author are likely:\n1. \"graph neural architecture search under distribution shifts\" [None]\n2. \"Graph neural architecture search\" [b9]\n3. \"sgas sequential greedy architecture search\" [b19]\n4. \"darts differentiable architecture search\" [b22]\n5. \"graph neural network architecture search for molecular property prediction\" [b14]\n6. \"neural architecture search with reinforcement learning\" [b56]\n7. \"efficient neural architecture search via parameter sharing\" [b27]\n\nThese papers are directly related to the main topic of the paper, which is graph neural architecture search under distribution shifts. The author even used key phrases like \"inspired by\" and \"motivated by\", and also mentioned the concepts and methods used in these papers. Therefore, it can be inferred that these source papers have greatly influenced the development of the author's research.\n\nThe result in json format:\n\n{\n  \"None\": 0.9,\n  \"b9\": 0.8,\n  \"b19\": 0.7,\n  \"b22\": 0.8,\n  \"b14\": 0.7,\n  \"b56\": 0.7,\n  \"b27\": 0.7\n}", "Based on the text provided and the titles of the references, the source papers that most likely inspired the given paper are:\n\n- \"Graph neural architecture search\" [b9]: This is directly mentioned in the text as a technique the authors are building upon. \n- \"Semi-supervised classification with graph convolutional networks\" [b15]: The authors mentioned GCN, a model type that was proposed in this paper.\n- \"Graph attention networks\" [b33]: GAT is another model type that the authors mentioned in their work.\n- \"How powerful are graph neural networks\" [b45]: GIN is another directly mentioned model type in the paper.\n- \"Darts: differentiable architecture search\" [b22]: This paper is relevant as the authors of the given paper also used a super-network and gradient-based methods for optimization in their approach.\n- \"NAS ood: neural architecture search for out-of-distribution generalization\" [b0]: The authors' work also focuses on out-of-distribution generalization, making this paper a likely source of inspiration.\n\nHere is the result in JSON format:\n\n{\n\"b9\": 0.9,\n\"b15\": 0.7,\n\"b33\": 0.7,\n\"b45\": 0.7,\n\"b22\": 0.8,\n\"b0\": 1.0\n}", "Based on the given text and the titles of the references, the following source papers appear to be the most directly influential to the author's research on Graph neuRal Architecture Customization with disEntangled Self-supervised learning (GRACES):\n\n1. \"graph neural architecture search under distribution shifts\" - The title indicates a direct relevance to the topic of GRACES and the exploration of architecture search under distribution shifts. (Confidence Score: 0.9)\n\n2. \"Graph neural architecture search\" [b9] - The author's method is an extension of graph neural architecture search, indicating that this is a primary source. (Confidence Score: 0.8)\n\n3. \"How powerful are graph neural networks\" [b45] - This paper seems to provide foundational knowledge on GNNs, which the author's method is built upon. (Confidence Score: 0.7)\n\n4. \"Semi-supervised classification with graph convolutional networks\" [b15] - The author's method uses a self-supervised approach, which is a form of semi-supervised learning. This indicates that this paper could have provided foundational knowledge. (Confidence Score: 0.8)\n\n5. \"Graph attention networks\" [b33] - This paper could have provided a basis for the author's exploration of GNNs, as attention mechanism is an important part of many GNN models. (Confidence Score: 0.7)\n\n6. \"DARTS: Differentiable architecture search\" [b22] - The author's method involves a differentiable architecture search, suggesting this paper might have provided a methodological basis. (Confidence Score: 0.8)\n\n7. \"Understanding and exploring the network with stochastic architectures\" [b7] - This paper might have provided inspiration for the author's exploration of varied GNN architectures. (Confidence Score: 0.7)\n\n8. \"Graph self-supervised learning: a survey\" [b23] - This paper could have provided a comprehensive overview of self-supervised learning techniques used in the author's method. (Confidence Score: 0.8)\n\nThe JSON format of the result is:\n{\n    \"None\": 0.9,\n    \"b9\": 0.8,\n    \"b45\": 0.7,\n    \"b15\": 0.8,\n    \"b33\": 0.7,\n    \"b22\": 0.8,\n    \"b7\": 0.7,\n    \"b23\": 0.8\n}", "Based on the text of the paper, it appears that the source papers that inspired the authors most are:\n\n- \"graph neural architecture search\" [b9]: This paper is directly referred to in the text as a method that has been utilized to search for an optimal GNN architecture. This suggests that the authors have used this paper to guide their work. \n\n- \"sgas sequential greedy architecture search\" [b19]: The authors of the paper in question seem to have been inspired by this paper in their efforts to develop their own Graph neuRal Architecture Customization with disEntangled Self-supervised learning (GRACES).\n\n- \"semi-supervised classification with graph convolutional networks\" [b15], \"graph attention networks\" [b33], and \"how powerful are graph neural networks\" [b45]: These papers are mentioned as models that have been proposed and achieved great successes in many graph tasks, which might have served as a source of inspiration for the authors.\n\n- \"darts differentiable architecture search\" [b22]: The authors have adopted the weight sharing method proposed in this paper to make their model's training more efficient.\n\n- \"graph self supervised learning a survey\" [b23]: This paper might have inspired the authors to use self-supervised learning tasks in their model.\n\n- \"towards out of distribution generalization a survey\" [b31]: This paper might have provided the authors with insights on how to handle test graphs under distribution shift.\n\nThe confidence scores would need to be calculated based on further analysis of the paper's text and how directly each source paper contributed to the novel contributions of the paper.\n\nReturn results in json format:\n\n{\n\"b9\": 0.9,\n\"b19\": 0.8,\n\"b15\": 0.7,\n\"b33\": 0.7,\n\"b45\": 0.7,\n\"b22\": 0.8,\n\"b23\": 0.8,\n\"b31\": 0.8\n}"], "63f5888490e50fcafd27c90c": ["Based on the text, the author primarily discusses the challenges of memory capacity and bandwidth in embedding tables in deep learning recommendation models. The main solution proposed is a hybrid embedding representation that uses a mix of embedding tables and deep hashing embedding (DHE), as well as a dynamic representation-hardware co-design technique called Multi-Path Recommendation (MP-Rec) to optimize performance. \n\nThe key phrases that suggest inspiration from previous work are often followed by references. In this case, the references that seem to be most directly related to the paper's contributions are:\n\n- [b35]: \"In order to be processed by recommendation models, sparse feature IDs must first be transformed into dense embedding vectors.\" \u2014 This reference is directly related to the concept of embedding tables, which is a key topic of the paper.\n- [b28]: \"Examples include Tensor Train Compression (TT-Rec) [b53] and Deep Hash Embedding (DHE) [b28].\" \u2014 DHE is a key method used in the paper's proposed solution.\n- [b27]: \"Generated embeddings from DHE can achieve higher model quality for some CTR predictions tasks [b27] [b41] [b45].\" \u2014 This reference is directly related to the use of DHE in the paper's proposed solution.\n- [b3]: \"Note that for many recommendation use cases, accuracy improvements > 0.1% are considered significant. [b3] [b47].\" \u2014 This reference seems to provide the benchmark for significant improvements in the field, which the paper aims to achieve.\n- [b9] and [b21]: \"Graphcore IPUs and Google TPUs [b9] [b21] [b24] [b25] [b26].\" \u2014 These references are directly related to the hardware accelerators discussed in the paper.\n\nBased on this, the proposed source papers are:\n\n{ \"b35\": 0.9, \"b28\": 0.9, \"b27\": 0.8, \"b3\": 0.7, \"b9\": 0.7, \"b21\": 0.7 }", "Based on the provided text and the titles of the references, the source papers that the author indicates that they inspire them most are:\n- \"Understanding Training Efficiency of Deep Learning Recommendation Models at Scale\" [b0]\n- \"On the Factory Floor: ML Engineering for Industrial-Scale Ads Recommendation Models\" [b3]\n- \"Maxim Naumov, Sam Naghshineh, and Mikhail Smelyanskiy. Low precision hardware architectures meet recommendation model inference at scale.\" [b6]\n- \"The Graphcore Second Generation IPU\" [b9]\n- \"DeepRecSys: A System for Optimizing End-to-End At-Scale Neural Recommendation Inference\" [b12]\n- \"The Architectural Implications of Facebook's DNN-based Personalized Recommendation\" [b14]\n- \"Learning to Embed Categorical Features without Embedding Tables for Recommendation\" [b28]\n- \"Ten Lessons from Three Generations Shaped Google's TPUv I Industrial Product\" [b24]\n\nThis is the JSON representation of the results:\n\n{\n  \"b0\": 0.8,\n  \"b3\": 0.9,\n  \"b6\": 0.8,\n  \"b9\": 0.9,\n  \"b12\": 0.9,\n  \"b14\": 0.9,\n  \"b28\": 0.8,\n  \"b24\": 0.8\n} \n\nThe confidence score represents how strongly the paper seems to be influenced by the source paper, with 1 being the strongest influence and 0 being no influence.", "Based on the text provided, here are the source papers that most likely inspired the present paper:\n\n1. \"Understanding capacity driven scale out neural recommendation inference\" (b35): The paper discusses the challenges of memory capacity and bandwidth in large-scale embedding tables, which is a primary challenge addressed in the present paper.\n2. \"Learning to embed categorical features without embedding tables for recommendation\" (b28): The paper introduces alternative embedding representations that reduce memory capacity requirements, a concept also proposed in the present paper.\n3. \"TT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\" (b53): The paper proposes different embedding representations for reducing memory requirements, which is a similar approach adopted in the present paper.\n4. \"The Graphcore Second Generation IPU\" (b9): It's mentioned in the context of using custom accelerators for compatible representations, which suggests that it inspired the hardware components of the present paper.\n5. \"In Datacenter Performance Analysis of a Tensor Processing Unit\" (b26): It's mentioned in the context of using custom accelerators for compatible representations, which suggests that it inspired the hardware components of the present paper.\n6. \"Deep Learning Recommendation Model for Personalization and Recommendation Systems\" (b37): It's mentioned in terms of embedding tables which is a critical component of state-of-the-art recommendation models.\n7. \"Deep Interest Network for Click Through Rate Prediction\" (b61): It's mentioned in the context of e-commerce applications of recommendation systems, which might have inspired the application scenarios of the present paper.\n\nBased on these findings, the results in the JSON format would be:\n\n{\n\"b35\": 0.9,\n\"b28\": 0.85,\n\"b53\": 0.8,\n\"b9\": 0.75,\n\"b26\": 0.75,\n\"b37\": 0.7,\n\"b61\": 0.65\n}\n\nThese confidence scores are assigned based on how closely related each source paper's title and the context in which it's mentioned in the present paper's text seem to be to the present paper's topic and challenges. It's assumed that a higher score corresponds to a higher degree of relevance and influence.", "Based on the given text and provided titles of references, the source papers that appear to have heavily influenced the main contributions of the paper are:\n\n{ \n\"b28\": 1,\n\"b53\": 0.9,\n\"b3\": 0.8,\n\"b14\": 0.7,\n\"b35\": 0.6,\n\"b32\": 0.5\n}\n\nExplanation:\nThe paper primarily focuses on the challenges with embedding tables in deep learning recommendation models and proposes a hybrid representation of embedding tables and deep hashing embedding (DHE) to address these challenges. The source paper \"b28: Learning to embed categorical features without embedding tables for recommendation\" seems to be the most relevant, given the direct reference to DHE, a key concept in the paper. The paper also discusses Tensor Train Compression (TT-Rec), which is the primary focus of \"b53: TT-Rec tensor train compression for deep learning recommendation models\", hence the high confidence score. The paper also cites \"b3: On the factory floor ML engineering for industrial scale ads recommendation models\", \"b14: The architectural implications of Facebook\u2019s DNN-based personalized recommendation\", \"b35: Understanding capacity driven scale out neural recommendation inference\" and \"b32: FBGEMM: Enabling high performance, low precision deep learning inference\" throughout the text, indicating their relevance to the topic and contributions of the paper.", "Based on the challenges outlined in the paper, the novel contributions, and the mentions of previous work, the source papers that inspired this work are:\n\n1. \"Deep learning recommendation model for personalization and recommendation systems\" [b37]: This paper is referenced in the context of discussing the importance of the embedding table in recommendation models. The current paper's hybrid embedding representation seems to be a direct extension of the concepts presented in this work.\n   \n2. \"Learning to embed categorical features without embedding tables for recommendation\" [b28]: This paper is mentioned when discussing the alternative embedding representations that reduce memory capacity requirements. The Deep Hash Embedding (DHE) method seems to be a key inspiration for the proposed hybrid approach in this paper.\n\n3. \"TT-Rec: Tensor Train Compression for deep learning recommendation models\" [b53]: This paper is also referenced in the context of alternative embedding representations. \n\n4. \"The Graphcore second generation IPU\" [b9]: The new hybrid representation proposed in this work is designed to be compatible with a range of hardware platforms, including the Graphcore Intelligence Processing Units (IPUs), indicating this paper provided direct inspiration.\n\n5. \"In-datacenter performance analysis of a Tensor Processing Unit\" [b26]: This paper is cited when discussing the evaluation of the hybrid embedding representation on Google Tensor Processing Units (TPUs), indicating the TPU-based methods in this work provided inspiration.\n\n6. \"RecPipe: Co-designing models and hardware to jointly optimize recommendation quality and performance\" [b13]: This paper is cited when discussing the need for hardware-software co-design in recommendation systems, and likely provided inspiration for the proposed Multi-Path Recommendation (MP-Rec) technique.\n\nThe JSON format of the result is:\n{\n    \"b37\": 0.9,\n    \"b28\": 0.85,\n    \"b53\": 0.8,\n    \"b9\": 0.75,\n    \"b26\": 0.75,\n    \"b13\": 0.7\n}"], "626603225aee126c0f2338e4": ["Based on the provided text, the following source papers seem to be the most relevant. Please note, the confidence score is a subjective evaluation and could be adjusted with additional context:\n\n{\n    \"b17\": 0.8, \n    \"b43\": 0.7, \n    \"b65\": 0.7, \n    \"b25\": 0.6,\n    \"b35\": 0.6,\n    \"b45\": 0.5,\n    \"b1\": 0.4,\n    \"b31\": 0.4\n}\n\nExplanation: \n- References 'b17', 'b43', and 'b65' are mentioned in the context of the paper's novel approach to dealing with memory inefficiencies in deep point cloud analytics, indicating that they may have provided significant inspiration or foundational understanding. \n- 'b25' is mentioned in the context of providing a comprehensive survey on deep learning for point clouds, which may have provided necessary background and understanding for the authors.\n- 'b35', 'b45', 'b1', and 'b31' are referenced in relation to hardware optimizations and architecture considerations, suggesting they may have provided knowledge or inspiration for the hardware co-design aspect of the paper's contributions.\n  \nPlease note that this analysis is based on the information given and a full review of the source papers would provide a more accurate evaluation.", "Based on the text and author notes, the paper discusses about overcoming challenges in point cloud algorithms due to irregular memory access patterns, by proposing an optimized hierarchical point cloud storage structure that limits memory access within a subtree. The author also mentions an additional mechanism that compensates for the loss of precision in neighbor search due to this optimization.\n\nThe following source papers seem to be most relevant based on the text:\n\n1. \"mesorasi architecture support for point cloud analytics via delayed aggregation\" [b17]: This paper is mentioned in the context of the primary challenges outlined in the paper, particularly when discussing the inefficiencies in memory access in point cloud algorithms. \n\n2. \"quicknn memory and performance optimization of kd tree based nearest neighbor search for d point clouds\" [b43]: This is referenced when discussing existing algorithms that enable streaming accesses, but at a cost of increased search work and redundant DRAM accesses. The new method proposed in the paper seems to be an improvement on this.\n\n3. \"tigris architecture and algorithms for d perception in point clouds\" [b65]: This is also mentioned in the context of existing methods that enable streaming accesses, and the proposed method seems to be a direct improvement on this. \n\n4. \"pointnet deep learning on point sets for d classification and segmentation\" [b47]: The context of this reference suggests it may be related to the techniques used in the paper, although it does not appear to be a direct inspiration for the main contributions.\n\n5. \"pointnet deep hierarchical feature learning on point sets in a metric space\" [b48]: Similarly to [b47], this paper is related to the methods used in the paper but does not appear to be a direct inspiration for the main contributions.\n\nBased on the provided information, the confidence scores for these references are estimated as follows:\n\n- [b17]: 0.8\n- [b43]: 0.75\n- [b65]: 0.75\n- [b47]: 0.6\n- [b48]: 0.6\n\nThis is an initial estimation and may need to be refined based on a more detailed analysis of the paper and the specific context of the references.\n\nThe final result is:\n\n{\n\"b17\": 0.8,\n\"b43\": 0.75,\n\"b65\": 0.75,\n\"b47\": 0.6,\n\"b48\": 0.6\n}", "Based on the given text, the following references seem to be the source papers that inspired this work:\n\n{\n\"b17\": 0.9,\n\"b65\": 0.8,\n\"b43\": 0.7,\n\"b68\": 0.6,\n\"b35\": 0.5,\n\"b25\": 0.4\n}\n\nThis is an approximate inference based on the frequency and context of their citation in the text. \n\nThe paper \"b17: mesorasi architecture support for point cloud analytics via delayed aggregation\" seems to be a key source, as it is mentioned in the context of practical challenges with point cloud algorithms. References \"b65: tigris architecture and algorithms for d perception in point clouds\" and \"b43: quicknn memory and performance optimization of kd tree based nearest neighbor search for d point clouds\" are mentioned as prior work that also enables streaming accesses. \"b68: point x a spatial locality aware architecture for energy efficient graph based point cloud deep learning\" and \"b35: pointacc efficient point cloud accelerator\" could have inspired the paper's hardware design. \"b25: deep learning for d point clouds a survey\" provides a comprehensive background in the field. \n\nPlease note that this is a rough inference and the confidence score is arbitrary. It's based on the frequency and context of citations but doesn't guarantee the actual influence level of these papers on the given work.", "Based on the provided text, the paper discusses an optimized hierarchical structure for point cloud storage, which reduces memory access during neighborhood search operations, whilst also introducing a mechanism for training with imprecise neighborhood searching to mitigate accuracy loss. This suggests that the paper's main contributions are in the realm of optimizing memory usage and preserving accuracy in point cloud operations.\n\nThe source papers aren't explicitly specified through phrases such as \"inspired by,\" or \"motivated by,\" but based on the context and the challenges addressed by the author, the following papers seem most relevant:\n\n- \"b17: Mesorasi: Architecture support for point cloud analytics via delayed aggregation\"\n- \"b43: QuickNN: Memory and performance optimization of KD-tree based nearest neighbor search for 3D point clouds\"\n- \"b65: Tigris: Architecture and algorithms for 3D perception in point clouds\"\n- \"b68: Point X: A spatial locality aware architecture for energy efficient graph based point cloud deep learning\"\n\nThese papers all deal with memory optimization, point cloud analytics, and 3D perception in point clouds, which aligns them closely with the contributions of the paper in question.\n\nSo the result in JSON format would be:\n\n{\n\"b17\": 0.8,\n\"b43\": 0.7,\n\"b65\": 0.7,\n\"b68\": 0.6\n}", "The source papers for the given academic paper seem to be:\n1. \"mesorasi architecture support for point cloud analytics via delayed aggregation\" [b17]: The paper heavily relies on this work to highlight the inefficiencies in memory accesses in point cloud algorithms and highlights how point cloud networks are inefficient on today's hardware architectures. Confidence score: 0.9\n2. \"tigris architecture and algorithms for d perception in point clouds\" [b65]: The paper references Tigris as a previous work that enables streaming accesses but at a cost of more search work and DRAM traffic. Confidence score: 0.8\n3. \"quicknn memory and performance optimization of kd tree based nearest neighbor search for d point clouds\" [b43]: The paper compares its contributions with QuickNN, stating that its approach reduces the total search load and DRAM traffic compared to QuickNN. Confidence score: 0.8\n4. \"darkroom compiling high level image processing code into hardware pipelines\" [b26]: The paper uses this reference to compare how regular 2D perception domains like video and image processing are optimized in hardware architectures. Confidence score: 0.7\n5. \"pointnet deep learning on point sets for d classification and segmentation\" [b47]: This paper is pointed out as a core method in the field of deep point cloud analytics. Confidence score: 0.8\n6. \"pointacc efficient point cloud accelerator\" [b35]: The paper might have used this reference to compare the efficiency of its own optimization techniques. Confidence score: 0.7\n\nInferred in JSON format:\n{\n  \"b17\": 0.9,\n  \"b65\": 0.8,\n  \"b43\": 0.8,\n  \"b26\": 0.7,\n  \"b47\": 0.8,\n  \"b35\": 0.7\n}"], "63fec3cd90e50fcafdd70322": ["Based on the text and the titles of the references, the source papers that inspired the given paper can be identified as follows:\n\n- \"glm dialog noise tolerant pre training for knowledge grounded dialogue generation\" [b6]: The authors discuss the use of GLM for their knowledge-grounded dialogue model, showing a clear influence from this paper. Confidence score: 0.9\n\n- \"plato k internal and external knowledge enhanced dialogue generation\" [b2]: The authors discuss the exploitation of external knowledge in dialogues, which is a major challenge tackled in the given paper. Confidence score: 0.8\n\n- \"knowledge grounded dialogue generation with pre trained language models\" [b34]: This paper is about grounding large-scale language models with external knowledge, which is the core contribution of the given paper. Confidence score: 0.9\n\n- \"controlling conditional language models without catastrophic forgetting\" [b13]: The authors mention the challenge of catastrophic forgetting when training their model, which is a concept discussed in this paper. Confidence score: 0.7\n\n- \"xdai a tuning free framework for exploiting pre trained language models in knowledge grounded dialogue generation\" [b29]: The given paper uses a similar approach for exploiting pre-trained language models in knowledge-grounded dialogues. Confidence score: 0.7\n\n- \"plato xl exploring the large scale pre training of dialogue generation\" [b1]: This paper is about large-scale pre-training for dialogue generation, which is a methodology followed in the given paper. Confidence score: 0.7\n\nReturn the result in the JSON format:\n{\n    \"b6\": 0.9,\n    \"b2\": 0.8,\n    \"b34\": 0.9,\n    \"b13\": 0.7,\n    \"b29\": 0.7,\n    \"b1\": 0.7\n}", "Based on the text and the author's notes, the paper seems to be primarily focused on developing a knowledge-grounded dialogue model (GLM-Dialog) that can handle Chinese language conversation and manage the challenges of limited high-quality datasets and diverse exploitation of external knowledge. The paper also proposes a new evaluation method and data augmentation and model training strategies. \n\nThe following references seem to be the most pertinent source papers that inspired this work:\n\n1. \"GLM: General Language Model Pretraining with Autoregressive Blank Infilling [b6]\": This paper is directly related to the paper's topic as GLM-Dialog is based on the GLM model.\n\n2. \"PLATO-K: Internal and External Knowledge Enhanced Dialogue Generation [b2]\": This paper seems to be a source of inspiration for the method of exploiting diverse external knowledge.\n\n3. \"A Knowledge-Grounded Neural Conversation Model [b7]\": This paper formalized the task of knowledge-grounded dialogue and might have been the inspiration for this approach.\n\n4. \"XDAI: A Tuning-Free Framework for Exploiting Pre-Trained Language Models in Knowledge-Grounded Dialogue Generation [b29]\": The strategies proposed in the paper to leverage the pre-trained model seem to draw inspiration from this reference.\n\n5. \"Link the World: Improving Open Domain Conversation with Dynamic Spatiotemporal Aware Knowledge [b37]\": This paper might have influenced the construction of the external knowledge pool.\n\n6. \"PLATO-XL: Exploring the Large-Scale Pre-Training of Dialogue Generation [b1]\": This paper's insights into language style and social media data could have inspired the handling of language style discrepancies in GLM-Dialog.\n\nBased on the analysis, the final result may look like:\n\n{\n\"b6\": 1,\n\"b2\": 0.9,\n\"b7\": 0.85,\n\"b29\": 0.8,\n\"b37\": 0.75,\n\"b1\": 0.7\n}", "Based on the text of the academic paper, the source papers that inspired the authors most are:\n\n1. \"GLM: General Language Model Pretraining with Autoregressive Blank Infilling\" [b6]: The authors directly mentioned that they used GLM10B, which is a model introduced in this paper, for their GLM-Dialog model. Therefore, this paper is a crucial source paper. Confidence Score: 0.9\n\n2. \"PLATO-K: Internal and External Knowledge Enhanced Dialogue Generation\" [b2]: This paper is mentioned several times as a key inspiration for the development of the GLM-Dialog model, especially in the context of dealing with the challenge of exploiting external knowledge. Confidence Score: 0.85\n\n3. \"XDAI: A Tuning-free Framework for Exploiting Pre-trained Language Models in Knowledge-grounded Dialogue Generation\" [b29]: This paper is quoted when the authors talk about the data collection process for training GLM-Dialog, suggesting that it influenced the authors' approach to data collection. Confidence Score: 0.8\n\n4. \"A Knowledge-grounded Neural Conversation Model\" [b7]: Mentioned as the paper that formalized the task of knowledge-grounded dialogue, which is the main focus of the current paper. Confidence Score: 0.75\n\n5. \"Dialogue in the Wild: Learning from a Deployed Role Playing Game with Humans and Bots\" [b20]: This paper is mentioned in the context of applications of dialogue systems, indicating its relevance as a source paper. Confidence Score: 0.7\n\n6. \"Controlling Conditional Language Models without Catastrophic Forgetting\" [b13]: This paper is referenced when the authors talk about avoiding catastrophic forgetting, a key challenge in their work. Confidence Score: 0.65\n\n7. \"Revisiting Pre-trained Models for Chinese Natural Language Processing\" [b3]: This paper is referenced in the context of comparing language models, indicating it influenced their methodological approach. Confidence Score: 0.6\n\n8. \"PLATO-XL: Exploring the Large Scale Pre-training of Dialogue Generation\" [b1]: This paper is mentioned as an inspiration for their dialogue language models. Confidence Score: 0.55\n\nThe returned result in the json format would look like this:\n\n{ \"b6\": 0.9, \"b2\": 0.85, \"b29\": 0.8, \"b7\": 0.75, \"b20\": 0.7, \"b13\": 0.65, \"b3\": 0.6, \"b1\": 0.55 }", "Based on the text and the challenges outlined in the paper, the following references seem to have been the most influential:\n\n1. 'glm dialog noise tolerant pre training for knowledge grounded dialogue generation' [b6] - This paper is referenced multiple times and appears to be the basis for the GLM-Dialog model proposed in the current paper. The authors fine-tuned the GLM10B model, which is the core contribution of this paper. Confidence score: 0.9\n\n2. 'plato k internal and external knowledge enhanced dialogue generation' [b2] - This paper is referenced in relation to the exploitation of external knowledge in real-world conversations, a concept that is central to the GLM-Dialog model. Confidence score: 0.8\n\n3. 'xdai a tuning free framework for exploiting pre trained language models in knowledge grounded dialogue generation' [b29] - This paper is mentioned in the context of the GLM-Dialog's online deployment. Confidence score: 0.7\n\n4. 'controlling conditional language models without catastrophic forgetting' [b13] - This paper is cited as a solution to the catastrophic forgetting problem mentioned in the current paper. Confidence score: 0.7\n\n5. 'revisiting pre trained models for chinese natural language processing' [b3] - This paper appears to be a source of the backbone language model used in GLM-Dialog, as it is mentioned in the backbone preparation section. Confidence score: 0.6\n\n6. 'knowledge grounded dialogue generation with pre trained language models' [b34] - Appears to be a key reference for the concept of knowledge-grounded dialogue. Confidence score: 0.6\n\n7. 'godel large scale pretraining for goal directed dialog' [b16] - Mentioned as one of the recent English dialogue systems that have shown impressive performance. Confidence score: 0.5\n\n8. 'lamda language models for dialog applications' [b22] - Similar to [b16], this paper is cited as an example of a successful English dialogue system. Confidence score: 0.5\n\nThe JSON format of the result is:\n\n{\n\"b6\": 0.9,\n\"b2\": 0.8,\n\"b29\": 0.7,\n\"b13\": 0.7,\n\"b3\": 0.6,\n\"b34\": 0.6,\n\"b16\": 0.5,\n\"b22\": 0.5\n}", "Based on the text and the context provided, here are the source papers that have been most influential in the development of this paper:\n\n1. \"b6\": 'glm general language model pretraining with autoregressive blank infilling'\n2. \"b1\": 'plato xl exploring the large scale pre training of dialogue generation'\n3. \"b2\": 'plato k internal and external knowledge enhanced dialogue generation'\n4. \"b29\": 'xdai a tuning free framework for exploiting pre trained language models in knowledge grounded dialogue generation'\n5. \"b16\": 'godel large scale pretraining for goal directed dialog'\n6. \"b21\": 'blenderbot a deployed conversational agent that continually learns to responsibly engage'\n7. \"b22\": 'lamda language models for dialog applications'\n\nIt is important to note that the text directly cites these works and expresses clear reliance on their methodologies, concepts or findings. Additionally, they are mentioned in the context of critical discussion points or challenges, indicating a strong connection to the paper's main contributions. \n\nHere is the result in the required json format:\n{\n\"b6\": 1,\n\"b1\": 0.9,\n\"b2\": 0.9,\n\"b29\": 0.8,\n\"b16\": 0.8,\n\"b21\": 0.7,\n\"b22\": 0.7\n}\n\nThe confidence scores are assigned based on the frequency of their mention in the text and the extent to which their methodologies, findings or concepts are applied in this paper."], "6466faedd68f896efaeb70be": ["Based on the text of the paper and the associated notes, the source papers that most inspire this work are:\n\n1. \"Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking\" [b6]\n2. \"Neural Message Passing for Quantum Chemistry\" [b14]\n3. \"Inductive Representation Learning on Large Graphs\" [b16]\n4. \"Semi-Supervised Classification with Graph Convolutional Networks\" [b21]\n5. \"Fast Graph Representation Learning with PyTorch Geometric\" [b13]\n6. \"Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs\" [b52]\n7. \"Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods\" [b25]\n8. \"A Critical Look at the Evaluation of GNNs under Heterophily: Are We Really Making Progress?\" [b35]\n\nThe confidence scores for these sources are difficult to estimate without specific metrics or insights into the author's thought process. However, given the explicit references and context, a reasonable guess might be:\n\n{\n\"b6\": 0.9,\n\"b14\": 0.8,\n\"b16\": 0.85,\n\"b21\": 0.8,\n\"b13\": 0.75,\n\"b52\": 0.9,\n\"b25\": 0.85,\n\"b35\": 0.8\n}", "Based on the text and the titles of the references, the source papers that most likely inspired the given paper are:\n\n- \"Graph Neural Networks: A Review of Methods and Applications\" [b51]: The given paper is about Graph Neural Networks (GNNs) and this reference paper provides a review of methods and applications of GNNs.\n- \"Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs\" [b52]: The given paper discusses the challenges of homophily in GNNs and this reference paper goes beyond homophily to discuss its limitations and effective designs in GNNs.\n- \"Semi-Supervised Classification with Graph Convolutional Networks\" [b21]: The given paper uses GCN for node classification and this reference paper provides a method for semi-supervised classification with GCN.\n- \"Neural Message Passing for Quantum Chemistry\" [b14]: The given paper mentions the Message-Passing Framework (MPNNs) and this reference paper deals with neural message passing, specifically for quantum chemistry.\n- \"Directed Graph Convolutional Network\" [b43]: The given paper introduces a new framework for deep learning on directed graphs, similar to this reference paper which deals with directed graph convolutional networks.\n\nThe JSON format of the result would be:\n{\n    \"b51\": 0.9,\n    \"b52\": 0.85,\n    \"b21\": 0.8,\n    \"b14\": 0.75,\n    \"b43\": 0.8\n} \n\nThe confidence scores are given based on the relevance of the reference paper to the key points in the given academic paper.", "Based on the analysis of the text, the source papers that inspired the given paper the most are:\n\n1. \"edge directionality improves learning on heterophilic graphs\"\n2. \"spectral networks and locally connected networks on graphs\"\n3. \"deep gaussian embedding of graphs unsupervised inductive learning via ranking\"\n4. \"neural message passing for quantum chemistry\"\n5. \"semi-supervised classification with graph convolutional networks\"\n6. \"graph convolutional networks\"\n7. \"graph attention networks\"\n8. \"how powerful are graph neural networks\"\n\nThe JSON format of the answer is:\n\n{\n\"b0\": 1,\n\"b7\": 0.9,\n\"b6\": 0.8,\n\"b14\": 0.7,\n\"b21\": 0.7,\n\"b46\": 0.6,\n\"b49\": 0.6\n}", "Based on the text, the main contributions of the paper are the introduction of the concept of effective homophily and the proposal of the Directed Graph Neural Network framework (Dir-GNN). These contributions are directly based on the analysis of the limitations of previous Graph Neural Networks (GNNs) which did not handle directed graphs well.\n\nBelow are the source papers that appear to have influenced the main contributions of the paper:\n\n1. \"b21: Semi-supervised classification with graph convolutional networks\" - Confidence Score: 0.9. The paper proposes the Dir-GNN framework as an extension of the Graph Convolutional Network (GCN) model.\n\n2. \"b14: Neural message passing for quantum chemistry\" - Confidence Score: 0.8. This paper introduced the Message Passing Neural Network framework, which the authors note can be easily applied to directed graphs.\n\n3. \"b49: How powerful are graph neural networks\" - Confidence Score: 0.8. The authors argue that Dir-GNN is more expressive than standard MPNN, a concept explored in this paper.\n\n4. \"b52: Beyond homophily in graph neural networks: current limitations and effective designs\" - Confidence Score: 0.8. The concept of effective homophily appears to be an extension of the ideas in this paper on the limitations of GNNs when handling heterophily.\n\n5. \"b16: Inductive representation learning on large graphs\" - Confidence Score: 0.8. The authors compare their Dir-GNN model with GraphSage, a model proposed in this paper, indicating that it may have influenced their work.\n\n6. \"b46: Graph attention networks\" - Confidence Score: 0.7. The authors compare their Dir-GNN model with Graph Attention Networks, a model proposed in this paper, indicating that it may have influenced their work.\n\n7. \"b19: Open graph benchmark datasets for machine learning on graphs\" - Confidence Score: 0.7. The authors validate their model on the OGBN-Arxiv dataset from the Open Graph Benchmark, indicating that this paper may have influenced their work.\n\n8. \"b25: Large scale learning on non homophilous graphs new benchmarks and strong simple methods\" - Confidence Score: 0.7. The authors validate their model on the Arxiv-Year and Snap-Patents datasets introduced in this paper, indicating that it may have influenced their work. \n\n```json\n{\n    \"b21\": 0.9,\n    \"b14\": 0.8,\n    \"b49\": 0.8,\n    \"b52\": 0.8,\n    \"b16\": 0.8,\n    \"b46\": 0.7,\n    \"b19\": 0.7,\n    \"b25\": 0.7\n}\n```", "Based on the text and the titles of the references, the source papers that inspired the author the most are likely:\n\n1. \"Beyond homophily in graph neural networks current limitations and effective designs\" [b52]\n2. \"Semi supervised classification with graph convolutional networks\" [b21]\n3. \"Neural message passing for quantum chemistry\" [b14]\n4. \"Inductive representation learning on large graphs\" [b16]\n5. \"Graph neural networks: a review of methods and applications\" [b51]\n6. \"Deep gaussian embedding of graphs unsupervised inductive learning via ranking\" [b6]\n7. \"Graph convolutional networks\" [b38]\n\nThese references are selected considering the key phrases mentioned in the text, the novel methods and approaches introduced, and their direct relevance to the paper's key contributions. \n\nThe confidence scores for these might be:\n\n{\"b52\": 0.9,\n \"b21\": 0.8,\n \"b14\": 0.7,\n \"b16\": 0.7,\n \"b51\": 0.8,\n \"b6\": 0.8,\n \"b38\": 0.7} \n\nPlease note that the confidence scores are subjective and might vary based on the detailed content of each source paper."], "6459ac6bd68f896efa659285": ["Based on the text, the key challenges are combining Large Language Models with semantic technologies to enable reasoning and inference, and extracting information from raw texts to construct Knowledge Graphs. The authors propose using NLP models, specifically REBEL [b10] and Chat-GPT [b11], for this task, and they evaluate their approach by analyzing the use case of sustainability.\n\nThe following references are directly relevant to the paper's novel contributions:\n\n- \"REBEL: Relation Extraction By End-to-end Language generation\" [b10]: The authors state that they use this model to extract information from text data on the Web. This is one of the paper's primary methods, and it directly contributes to their methodology. \n\n- \"Chat-GPT Technical Report\" [b11]: The authors also use this model to extract information from text data on the Web. Similar to the REBEL model, this also directly contributes to their methodology.\n\n- \"A Review of Knowledge Reasoning over Knowledge Graphs\" [b1]: This paper is the basis for the authors' motivation to connect the Deep Learning space with the semantic space. This paper likely provided foundational knowledge and methods for their work.\n\n- \"Named Entity Recognition without Gazetteers\" [b2], \"Exploring Various Knowledge in Relation Extraction\" [b3], and \"A Survey on Semantic Parsing\" [b4]: These papers are listed in the context of the advanced NLP techniques required for creating Knowledge Graphs. Hence, they likely influenced the authors' understanding of these techniques.\n\nHere is the JSON output:\n\n{\"b10\": 1.0, \"b11\": 1.0, \"b1\": 0.9, \"b2\": 0.85, \"b3\": 0.85, \"b4\": 0.85}", "Based on the provided text and the titles of the references, the source papers that inspired the authors of the given paper are:\n\n- \"REBEL: Relation Extraction By End-to-end Language generation\" [b10]\n- \"GPT: Technical Report\" [b11]\n- \"Training language models to follow instructions with human feedback\" [b14]\n- \"Proximal policy optimization algorithms\" [b15]\n- \"Entity linking with a knowledge base: issues, techniques and solutions\" [b21]\n- \"OWL: Web ontology language overview\" [b22]\n\nThe source papers were identified based on the direct mentions and the usage of their methodologies in the implementation of the author's research. It can be inferred that the authors were inspired by these papers as they built upon the techniques and models proposed in these papers to develop their own research.\n\nReturning the result in JSON format:\n\n{\n\"b10\": 0.9,\n\"b11\": 0.9,\n\"b14\": 0.8,\n\"b15\": 0.8,\n\"b21\": 0.7,\n\"b22\": 0.7\n}", "Based on the given text, the following list of source papers with their respective confidence scores is derived:\n\n{\n\"b1\": 0.7,\n\"b5\": 0.9,\n\"b6\": 0.9,\n\"b7\": 0.9,\n\"b10\": 1.0,\n\"b11\": 1.0,\n\"b13\": 0.9,\n\"b14\": 0.8,\n\"b15\": 0.6,\n\"b22\": 0.7,\n\"b23\": 0.7\n} \n\nThis list includes the papers that the authors appear to have been most directly inspired by and that are most relevant to the paper's novel contributions, as indicated by phrases such as \"we chose\", \"we use\", \"we show\" and \"one of the models utilized in this study is\". The confidence scores reflect the degree to which the authors seem to rely on each paper, with a score of 1 representing a very strong reliance.", "Based on the text, the most relevant source papers are:\n\n1. \"'REBEL: Relation Extraction By End-to-end Language generation' [b10]\" - This paper is directly referenced and explained as one of the main methods used in this paper for relation extraction. The authors even detailed how they adapted the method to suit their needs. Confidence score: 0.9\n\n2. \"'GPT Technical Report' [b11]\" - This is the source paper of ChatGPT, which is the other main method used in this paper. The authors not only use ChatGPT but also discuss its mechanics and functionalities in detail. Confidence score: 0.9\n\n3. \"'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension' [b13]\" - This paper is mentioned as the base of REBEL, thus, it can be considered as a source paper. Confidence score: 0.7\n\n4. \"'Training language models to follow instructions with human feedback' [b14]\" - This paper is referred to as the training method for ChatGPT, suggesting a methodological influence on the current paper. Confidence score: 0.7\n\n5. \"'A Review: Knowledge Reasoning Over Knowledge Graph' [b1]\" - This paper is mentioned in the introduction as a counterpart of the current paper's approach. It could have guided the authors in developing their methodology. Confidence score: 0.6\n\n6. \"'Named Entity Recognition Without Gazetteers' [b2]\" - This paper is mentioned as a part of the advanced NLP techniques the authors use in their work. Confidence score: 0.6\n\nThe confidence score is based on the significance of the paper to the contribution of the current paper's novelty.\n\nThe returned JSON format is:\n{\"b10\": 0.9, \"b11\": 0.9, \"b13\": 0.7, \"b14\": 0.7, \"b1\": 0.6, \"b2\": 0.6}", "Based on the text and the titles of the references, the following references appear to have most directly influenced the paper's main contributions:\n\n{\n\"b5\": 0.9,\n\"b10\": 0.9,\n\"b11\": 0.9,\n\"b1\": 0.8,\n\"b3\": 0.7,\n\"b4\": 0.7,\n\"b7\": 0.7,\n\"b9\": 0.6\n}\n\nExplanation:\n- \"b5\": Language Models Are Few-Shot Learners: The paper mentions the usage of GPT-3, which is the main subject of this reference.\n- \"b10\": REBEL: Relation Extraction By End-to-End Language Generation: The paper uses the REBEL model for relation extraction, which is the main subject of this reference.\n- \"b11\": GPT Technical Report: The paper uses OpenAI's ChatGPT for information extraction, which is the main subject of this reference.\n- \"b1\": A Review Knowledge Reasoning Over Knowledge Graph: The paper discusses the creation of Knowledge Graphs, which is the main subject of this reference.\n- \"b3\": Exploring Various Knowledge in Relation Extraction: The paper discusses Relation Extraction, which is the main subject of this reference.\n- \"b4\": A Survey on Semantic Parsing: The paper discusses the use of NLP techniques like Semantic Parsing, which is the main subject of this reference.\n- \"b7\": BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: The paper mentions the usage of BERT model, which is the main subject of this reference.\n- \"b9\": Strategic Perspectives of Corporate Sustainability Management to Develop a Sustainable Organization: The paper discusses sustainability and its integration into corporate activities, which is the subject of this reference."], "635753cc90e50fcafdddd9f5": ["Based on the text and the author notes, the paper introduces a framework for optimizing the layout of tensors and operator-level optimizations in deep learning compilation. It emphasizes achieving a high-performance tensor program for different hardware platforms. Here are the pertinent source papers inferred from the text:\n\n1. \"TVM: An Automated End-to-End Optimizing Compiler for Deep Learning\" [b9]: This paper seems to have influenced the work significantly, as the authors have built upon the loop transformation primitives of TVM and TVM's idea of an automated end-to-end optimizing compiler. \n\n2. \"Learning to Optimize Tensor Programs\" [b10]: This paper might be a source as it introduces the concept of learning to optimize tensor programs, which seems to align with the presented method.\n\n3. \"Ansor: Generating High-Performance Tensor Programs for Deep Learning\" [b82]: The authors of this paper appear to have been influenced by the Ansor framework for generating high-performance tensor programs.\n\n4. \"Efficient Primitives for Deep Learning\" [b11]: This paper might be a source as it introduces efficient primitives for deep learning, which is a key concept in the current paper.\n\n5. \"AutoTVM: Learning to Optimize Tensor Programs\" [b10]: This paper might be a source as it introduces the concept of learning to optimize tensor programs, which seems to align with the presented method.\n\n6. \"Optimizing CNN Model Inference on CPUs\" [b43]: The authors seem to have been inspired by this paper's approach to optimizing convolutional neural network (CNN) model inference on CPUs.\n\n7. \"Intel NGraph: An Intermediate Representation, Compiler, and Executor for Deep Learning\" [b17]: This paper might be a source as it introduces the concept of a deep compiler for deep learning models, which aligns with the presented method.\n\n8. \"TensorFlow: A System for Large-Scale Machine Learning\" [b0]: This paper might be a source as it introduces TensorFlow, a system for large-scale machine learning. The authors of the current paper used TensorFlow models as input for their compiler.\n\nBased on these inferences, the following source papers are most pertinent:\n\n{ \"b9\": 0.9, \"b10\": 0.8, \"b82\": 0.8, \"b11\": 0.75, \"b43\": 0.75, \"b17\": 0.7, \"b0\": 0.7 } \n\nThis output suggests that the paper \"TVM: An Automated End-to-End Optimizing Compiler for Deep Learning\" is the most influential source paper, followed by \"Learning to Optimize Tensor Programs\" and \"Ansor: Generating High-Performance Tensor Programs for Deep Learning\".", "Based on the text of the paper and its references, the source papers that inspired this work most likely include:\n\n1. \"TVM: An automated end-to-end optimizing compiler for deep learning\" [b9]\n2. \"Learning to optimize tensor programs\" [b10]\n3. \"Efficient primitives for deep learning\" [b11]\n4. \"Cache conscious structure layout\" [b12]\n5. \"Compiler transformations for high performance computing\" [b3]\n6. \"Tiramisu: A polyhedral compiler for expressing fast and portable code\" [b5]\n7. \"Ansor: Generating high-performance tensor programs for deep learning\" [b82]\n\nThese papers were selected based on the text's repeated emphasis on tensor layout optimization, loop optimization, and auto-tuning techniques for performance optimization. The paper frequently refers to methods and insights from these sources, indicating their significant influence on the paper's contributions.\n\nIn the JSON format, the results would look like this:\n\n{\n\"b9\": 0.9,\n\"b10\": 0.85,\n\"b11\": 0.8,\n\"b12\": 0.8,\n\"b3\": 0.75,\n\"b5\": 0.75,\n\"b82\": 0.7\n}\n\nThe confidence scores indicate the estimated relevance of each reference, with a higher score suggesting a more significant influence.", "Based on the text of the paper, the main challenges are:\n\n1. The inability to jointly perform graph-level and operator-level optimizations due to the coupling between data storage and operator implementation.\n2. The high cost of changing layouts, leading to the unidirectional and one-off optimization flow.\n3. The inefficiency of the search space reconstruction during joint tuning.\n4. The explosion of search space due to the combination of layout and loop tuning.\n\nThe paper proposes a deep compiler that jointly performs graph-level and operator-level optimizations. The authors introduce a layout propagation mechanism to eliminate layout transformation overhead and devise a joint layout and loop auto-tuning framework to improve efficiency. \n\nThe most relevant sources that inspired this work are:\n\n- [b9]: \"tvm an automated end to end optimizing compiler for deep learning\" is heavily referenced and seems to be the main source of inspiration for this work. The authors compare their work to TVM and also use it for loop optimization.\n- [b69]: \"tensor comprehensions framework agnostic high performance machine learning abstractions\". This paper is referenced in the discussion of existing deep compilers, indicating that it probably inspired the authors' approach.\n- [b5]: \"tiramisu a polyhedral compiler for expressing fast and portable code\". This paper is also mentioned in the context of existing deep compilers.\n- [b82]: \"ansor generating high performance tensor programs for deep learning\". This paper is compared with the current work in the evaluation section, indicating that it might have been a source of inspiration.\n- [b80]: \"akg automatic kernel generation for neural processing units using polyhedral transformations\". This paper is mentioned as an example of existing auto-tuning techniques.\n\nBased on these observations, the most pertinent source papers are:\n\n```json\n{\n  \"b9\": 1,\n  \"b69\": 0.9,\n  \"b5\": 0.9,\n  \"b82\": 0.8,\n  \"b80\": 0.8\n}\n```\n\nPlease note that these results are inferred based on the text of the paper and the titles of the references. The actual source papers might be different depending on the detailed content of the references.", "Based on the text analysis, we identify the following papers as the most pertinent source papers that inspired the author:\n\n1. \"TVM: An automated end-to-end optimizing compiler for deep learning\" [b9]: The paper discusses the TVM stack and its loop optimization techniques, which are directly relevant to the ALT framework's design for operator-level loop optimization. \n\n2. \"Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions\" [b69]: The paper discusses Tensor Comprehensions and how they optimize deep learning computations, which is relevant to the design of the ALT framework.\n\n3. \"Learning to optimize tensor programs\" [b10]: The paper discusses the use of machine learning to optimize tensor programs, which is also addressed in ALT.\n\n4. \"AKG: Automatic Kernel Generation for Neural Processing Units Using Polyhedral Transformations\" [b80]: The paper discusses the use of automatic kernel generation for neural processing units, which is directly relevant to the ALT's design for operator-level loop optimization. \n\n5. \"Ansor: Generating High-Performance Tensor Programs for Deep Learning\" [b82]: The paper discusses the Ansor framework and its auto-tuning techniques. The ALT framework also includes an auto-tuning module for joint data layout and loop optimization.\n\nThe reference numbers and confidence scores are as follows:\n\n{ \n\"b9\": 0.9,\n\"b69\": 0.8,\n\"b10\": 0.8,\n\"b80\": 0.9,\n\"b82\": 0.9\n}\n", "Based on the provided text and the author notes, the paper focuses on a new framework to support efficient tensor layout and operator optimization for deep learning compilation. The key challenges are the overhead of layout transformation and the inefficiency due to the search space reconstruction during joint tuning. It also addresses the issue of improving efficiency given the search space explosion with the combination of layout and loop tuning. \n\nThe paper mentions several previous works. However, the key inspirations seem to be:\n\n- TVM [b9]: This is referenced multiple times, and the paper's main focus, tensor layout and operator-level optimization, seems to be a direct extension of the work done in TVM. The authors also use the loop primitives of TVM for their loop transformation.\n- Ansor [b82]: This work is referenced in the context of auto-tuning techniques which the current paper also uses.\n- TensorFlow [b0]: The authors mention TensorFlow to explain how their system works with existing frameworks.\n- Tensor Comprehension [b69]: This work is mentioned as an example of existing deep compilers which the current paper aims to improve upon.\n\nBased on these observations, the source papers of the given paper should be:\n\n{ \"b9\": 0.9, \"b82\": 0.8, \"b0\": 0.7, \"b69\": 0.7 }\n"], "63a2794890e50fcafd29405f": ["Based on the text of the paper and the titles of the references, the most pertinent source papers that inspired the author could be:\n\n{ \n    \"b0\": 0.8, \n    \"b17\": 0.9, \n    \"b43\": 0.85, \n    \"b45\": 0.85,\n    \"b29\": 0.7, \n    \"b30\": 0.75, \n    \"b12\": 0.7, \n    \"b2\": 0.65\n}\n\nThese papers are selected based on the key phrases and the roles they played in shaping the author's novel contributions. It seems that the author was heavily inspired by the concepts of large language models as reasoning teachers (b0, b17), chain-of-thought prompting (b43, b45), pre-training of language models (b29, b30), knowledge distillation (b12), and scaling language modeling (b2). The confidence scores are subjective and represent a rough estimate of how strongly each source paper influenced the author's work.", "Based on the provided information, the most pertinent source papers that inspired the given academic paper seem to be:\n\n1. 'b17': 'Large Language Models Are Zero-Shot Reasoners' - The paper heavily refers to the concept of 'zero-shot reasoning' and credits this work multiple times. The concept also seems to be a fundamental part of the methodology proposed in the paper, which suggests a strong reliance on this previous work.\n   \n2. 'b45': 'Chain of Thought Prompting Elicits Reasoning in Large Language Models' - This paper's methodology of using 'chain of thought prompting' to improve the performance of language models is also mentioned multiple times and appears to be central to the methods proposed in the given paper.\n\n3. 'b49': 'Understanding Knowledge Distillation in Nonautoregressive Machine Translation' - This paper's concept of 'knowledge distillation' seems to be employed in the given paper's methodology. \n\n4. 'b26': 'Show Your Work: Scratchpads for Intermediate Computation with Language Models' - This paper's concept of 'intermediate computation' is mentioned and the given paper seems to adopt a similar approach to tackle the challenges.\n\n5. 'b30': 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer' - This paper's work on transfer learning is referenced and seems to have a methodological similarity with the approach of the given paper.\n\n6. 'b2': 'Palm: Scaling Language Modeling With Pathways' - The given paper also refers to 'scaling' language models, which suggests that the approach in this paper may have influenced the methods used in the given paper.\n\nReturn the result:\n\n{\n    \"b17\": 0.9,\n    \"b45\": 0.85,\n    \"b49\": 0.8,\n    \"b26\": 0.8,\n    \"b30\": 0.75,\n    \"b2\": 0.75\n}", "Based on the text of the paper, the author most directly refers to the following papers as sources of inspiration:\n\n1. \"Large language models are zero-shot reasoners\" [b17]: This paper is mentioned multiple times in the text, and is the method the authors choose to use for prompting their teacher model. The authors' method builds directly on the techniques described in this paper.\n\n2. \"Self-consistency improves chain of thought reasoning in language models\" [b43]: This paper is mentioned as demonstrating the possibility of eliciting complex reasoning abilities in large language models, which is a key part of the authors' method.\n\n3. \"Chain-of-thought prompting elicits reasoning in large language models\" [b45]: This paper is cited multiple times as the origin of the chain-of-thought prompting method, which the authors use to generate reasoning samples.\n\n4. \"Training verifiers to solve math word problems\" [b3] and \"Training compute optimal large language models\" [b13]: These papers are mentioned as attempts to fine-tune small models with explicit reasoning steps, which is a technique the authors build upon in their method.\n\n5. \"Large language models can self-improve\" [b15]: This paper is mentioned as a concurrent work that uses a similar approach to the authors'.\n\n6. \"Knowledge distillation: A survey\" [b10]: This paper is mentioned in the context of the authors' method being a variant of knowledge distillation, suggesting that it provided some inspiration for their approach.\n\nBased on the context in which these papers are mentioned in the text, the confidence scores for each are:\n\n{\n\"b17\": 1,\n\"b43\": 0.8,\n\"b45\": 1,\n\"b3\": 0.7,\n\"b13\": 0.7,\n\"b15\": 0.6,\n\"b10\": 0.6\n}", "Based on the text of the paper, the most pertinent source papers that inspired the authors could be:\n\n1. \"large language models are reasoning teachers\" [b17]: The authors reference this paper frequently. They use it as a basis for using language models as teachers and for their zero-shot chain-of-thought prompting method.\n\n2. \"palm scaling language modeling with pathways\" [b2]: This paper is referenced in relation to the discussion around the need for extremely large models for optimal performance.\n\n3. \"training verifiers to solve math word problems\" [b3]: This paper is mentioned in relation to the need for explicit reasoning steps and the arduous reasoning annotation necessary for such methods.\n\n4. \"large language models are zero shot reasoners\" [b43]: The authors reference this paper when discussing the use of few-shot exemplars containing chain-of-thought reasoning.\n\n5. \"large language models can self improve\" [b15]: This paper is mentioned in relation to using few-shot chain-of-thought to self-generate fine-tuning examples.\n\n6. \"knowledge distillation a survey\" [b10]: This paper is referenced in relation to knowledge distillation, which is a key part of the authors' proposed method.\n\n7. \"zero shot knowledge distillation in deep networks\" [b25]: This paper is used to discuss the authors' distillation method, and it seems to have inspired the use of zero-shot knowledge distillation in the authors' method.\n\nThe JSON format of the result would be:\n\n{\n\"b17\": 0.9,\n\"b2\": 0.8,\n\"b3\": 0.8,\n\"b43\": 0.85,\n\"b15\": 0.8,\n\"b10\": 0.75,\n\"b25\": 0.75\n} \n\nThe confidence scores are assigned based on the frequency and importance of the mention of each source paper in the text.", "Based on the information given, the source papers that inspired the current paper are the following:\n\n1. \"Large language models are zero shot reasoners\" [b17]\n2. \"Chain of thought prompting elicits reasoning in large language models\" [b45]\n3. \"Large language models are reasoning teachers\" [None]\n4. \"Training verifiers to solve math word problems\" [b3]\n5. \"GPT mix leveraging large scale language models for text augmentation\" [b46]\n6. \"Star bootstrapping reasoning with reasoning\" [b47]\n7. \"Knowledge distillation a survey\" [b10]\n8. \"Sequencelevel knowledge distillation\" [b16]\n\nThis is inferred from the text of the paper and the titles of the references. The text indicates that the paper's methodology was influenced by prompting methods, zero-shot reasoning, and knowledge distillation techniques, which are covered in these source papers.\n\nHere is the result in JSON format:\n{\"b17\": 0.9, \"b45\": 0.9, \"None\": 0.85, \"b3\": 0.8, \"b46\": 0.8, \"b47\": 0.8, \"b10\": 0.75, \"b16\": 0.75}.\n\nThe confidence scores are assigned based on the relevance of the reference titles and the key phrases in the paper text. A higher score indicates a stronger influence on the paper's methodology and contributions."], "63b3f1f890e50fcafdea0718": ["Based on the text and the titles of references, the most relevant source papers are:\n\n1. \"evaluation of the intel thread director technology on an alder lake processor\" - This paper is the core subject of the text and its findings and methodology are often referenced throughout the text.\n2. \"alder lake extends battery life\" - This paper is referenced in the context of discussing the benefits and challenges of AMPs, particularly in the mobile and embedded domains.\n3. \"benefits of the big little architecture\" - The text mentions the ARM big.LITTLE architecture as an example of the widespread utilization of AMP processors.\n4. \"operating system support for overlapping isa heterogeneous multi core architectures\" - The text discusses the challenges posed to the OS scheduler by AMPs, mentioning this paper specifically.\n5. \"heteromates providing high dynamic power range on client devices using heterogeneous core groups\" - This paper is mentioned in the context of discussing the performance benefits of AMPs.\n6. \"contention aware fair scheduling for asymmetric single isa multicore systems\" - The text discusses the ACFS algorithm and its performance, referencing this paper.\n7. \"towards completely fair scheduling on asymmetric single isa multicore processors\" - This paper is also discussed in the context of the ACFS algorithm.\n8. \"exploring machine learning for thread characterization on heterogeneous multiprocessors\" - The text discusses the use of machine learning for SF prediction, referencing this paper.\n\nThe confidences are hard to determine with the provided information, but here is a rough estimation in the JSON format:\n\n{\n\"1\": 0.9,\n\"b0\": 0.8,\n\"b1\": 0.7,\n\"b18\": 0.7,\n\"b10\": 0.6,\n\"b9\": 0.7,\n\"b25\": 0.7,\n\"b20\": 0.6\n}", "Based on the text of the academic paper, the following source papers seem to have inspired or motivated the author's work:\n\n- \"bias scheduling in heterogeneous multi core architectures\" [b15]\n- \"contention aware fair scheduling for asymmetric single isa multicore systems\" [b9]\n- \"single isa heterogeneous multi core architectures for multithreaded workload performance\" [b16]\n- \"towards completely fair scheduling on asymmetric single isa multicore processors\" [b25]\n- \"a machine learning approach for performance prediction and scheduling on heterogeneous cpus\" [b20]\n- \"exploring machine learning for thread characterization on heterogeneous multiprocessors\" [b17]\n- \"pmctrack delivering performance monitoring counter support to the os scheduler\" [b24]\n- \"intel and ia architectures software developer s manual volume\" [b13]\n\nThe above papers are directly relevant to the author's novel contributions and algorithm. They are all related to thread scheduling, machine learning for performance prediction, and working with asymmetric single ISA multicore processors, which are all directly relevant to the main contributions of the paper. Therefore, they are likely to be the source papers.\n\nHere is the result in the JSON format:\n\n{\n\"b15\": 0.9,\n\"b9\": 0.9,\n\"b16\": 0.88,\n\"b25\": 0.9,\n\"b20\": 0.85,\n\"b17\": 0.85,\n\"b24\": 0.87,\n\"b13\": 0.8\n} \n\nNote: Confidence score is assigned based on the relevance of the source paper to the author's work. A score closer to 1 indicates higher relevance.", "Based on the given text, the source papers that inspired the author of the paper the most are:\n\n1. \"evaluation of the intel thread director technology on an alder lake processor\" - This appears to be the primary source of inspiration for the paper, as the entire paper is about evaluating Intel's Thread Director technology on an Alder Lake processor.\n\n2. \"alder lake extends battery life\" - This paper likely provided context and background information for the author's work on Alder Lake processors.\n\n3. \"benefits of the big little architecture\" - This paper presumably influenced the author's understanding of the big.LITTLE architecture, which is an important aspect of AMPs.\n\n4. \"operating system support for overlapping isa heterogeneous multi core architectures\" - This paper likely played a role in shaping the author's thoughts on how operating systems can support AMPs.\n\n5. \"contention aware fair scheduling for asymmetric single isa multicore systems\" - This paper likely influenced the author's understanding of scheduling algorithms for AMPs.\n\n6. \"bias scheduling in heterogeneous multi core architectures\" - This paper may have inspired the author's thinking on scheduling on AMPs.\n\n7. \"towards completely fair scheduling on asymmetric single isa multicore processors\" - This paper likely influenced the author's implementation and discussion of the ACFS algorithm.\n\nGiven the context of the paper and the author notes, the source papers are directly relevant to the paper's novel contributions. \n\nHere is the result in JSON format:\n\n{\n\"b0\": 0.9,\n\"b1\": 0.7,\n\"b18\": 0.8,\n\"b9\": 0.8,\n\"b15\": 0.8,\n\"b25\": 0.8\n}", "Based on the text and author's notes provided, the following references seem to be the most relevant source papers that have inspired this work:\n\n1. \"evaluation of the intel thread director technology on an alder lake processor\": The author's study is directly evaluating Intel's Thread Director technology, making this reference key to the research.\n2. \"operating system support for overlapping isa heterogeneous multi core architectures\" and \"bias scheduling in heterogeneous multi core architectures\": These papers seem to be related to the author's exploration of scheduling algorithms in the context of asymmetrical cores, which is a major theme in the provided paper.\n3. \"exploring machine learning for thread characterization on heterogeneous multiprocessors\" and \"a machine learning approach for performance prediction and scheduling on heterogeneous cpus\": These papers are likely the source of the machine learning methodology used in the author's research for speedup factor prediction.\n4. \"single isa heterogeneous multi core architectures for multithreaded workload performance\": This paper likely provides fundamental insights into the author's research on asymmetric multicore architectures.\n5. \"contention aware fair scheduling for asymmetric single isa multicore systems\": The author's paper discusses fairness in scheduling, making this reference relevant.\n6. \"pmctrack delivering performance monitoring counter support to the os scheduler\": This paper is likely relevant to the author's implementation of performance monitoring counters (PMCs) in their research.\n\nAccordingly, the confidence scores for the source papers would be:\n\n{\n\"reference number\": \"confidence score\",\n\"b14\": 0.9,\n\"b18\": 0.85,\n\"b20\": 0.85,\n\"b16\": 0.8,\n\"b9\": 0.8,\n\"b24\": 0.8\n}", "Based on the text, the most pertinent source papers are:\n1. 'evaluation of the intel thread director technology on an alder lake processor'\n2. 'appropriate allocation of workloads on performance asymmetric multicore architectures via deep learning algorithms'\n3. 'rapid development of os support with pmcsched for scheduling on asymmetric multicore systems'\n4. 'contention aware fair scheduling for asymmetric single isa multicore systems'\n5. 'bias scheduling in heterogeneous multi core architectures'\n6. 'towards completely fair scheduling on asymmetric single isa multicore processors'\n7. 'a machine learning approach for performance prediction and scheduling on heterogeneous cpus'\n8. 'pmctrack delivering performance monitoring counter support to the os scheduler'\n\nThe returned results in json format are:\n{\"b0\": 0.8, \"b2\": 0.9, \"b3\": 0.7, \"b9\": 0.9, \"b15\": 0.8, \"b20\": 0.8, \"b24\": 0.7, \"b25\": 0.9} \n\nPlease note that the confidence scores are given based on the context relevance and repeated mentions in the text. The scores are between 0 and 1, where 1 indicates high confidence."], "6466fafbd68f896efaeb77ac": ["Based on the text, the following references seem to be the most significant sources for this paper:\n\n1. \"Climbing towards NLU: On meaning, form, and understanding in the age of data\" [b4]\n2. \"Program synthesis with large language models\" [b1]\n3. \"Evaluating large language models trained on code\" [b6]\n4. \"Can language models encode perceptual structure without grounding? A case study in color\" [b0]\n5. \"Probing classifiers: promises, shortcomings, and advances\" [b2]\n6. \"The formal semantics of programming languages: an introduction\" [b43]\n\nFrom the text, these references appear to be the most directly related to the discussions and methodologies introduced in the paper. \n\nThe resulting JSON could be:\n\n{\n  \"b4\": 0.9,\n  \"b1\": 0.85,\n  \"b6\": 0.85,\n  \"b0\": 0.8,\n  \"b2\": 0.8,\n  \"b43\": 0.8\n}", "Based on the text of the academic paper, the primary challenges outlined were that language models (LMs) trained purely on form-such as the conditional distribution of tokens in the training corpus-do not acquire meaning. Instead, they produce text only according to surface statistical correlations gleaned from the training data. The author's algorithm proposed to tackle this challenge is to train an LM on a corpus of programs and their specifications, then probe the LM's hidden states for a representation of the program semantics using a linear classifier.\n\nBased on the criteria outlined and the contents of the paper, the source papers that appear to have directly inspired the author's work most are:\n\n1. b33: \"The debate over understanding in AI's large language models\"\n2. b4: \"Climbing towards NLU: On meaning, form, and understanding in the age of data\"\n3. b42: \"Attention is all you need\"\n4. b1: \"Program synthesis with large language models\"\n5. b6: \"Evaluating large language models trained on code\"\n6. b10: \"Constructive design of a hierarchy of semantics of a transition system by abstract interpretation\"\n\nThe confidence scores could be evaluated based on the frequency and context of the papers' mention in the text.\n\nTo express this as a JSON object:\n\n```json\n{\n    \"b33\": 0.9,\n    \"b4\": 0.8,\n    \"b42\": 0.7,\n    \"b1\": 0.7,\n    \"b6\": 0.6,\n    \"b10\": 0.6\n}\n```\nThese scores are hypothetical and would need to be computed with a more rigorous method for a real task.", "Based on the text, the most pertinent source papers that inspired the author are:\n\n1. \"Climbing towards NLU on meaning form and understanding in the age of data\" [b4]: The paper refers to this work when presenting their hypotheses H1 and H2. It appears that this paper's ideas about language models only leveraging surface statistics and not learning meaning served as a significant motivation for the current paper's research.\n \n2. \"Abstract interpretation a unified lattice model for static analysis of programs by construction or approximation of fixpoints\" [b11]: This paper is referenced when they discuss abstract interpretation as a method they utilize, suggesting it played a role in their methodological approach.\n\n3. \"Karel the robot a gentle introduction to the art of programming\" [b36]: The paper refers to this work when describing their method of using the Karel programming language as a tool for their research, suggesting it heavily influenced their experimental setup.\n\n4. \"Attention is all you need\" [b42]: The paper refers to this work when describing their method of training a Transformer model for next token prediction, indicating they adopted this technique from this source paper.\n\n5. \"Probing classifiers promises shortcomings and advances\" [b2]: This paper is referenced when they discuss the challenges of controlling what is learned by the probe, suggesting it played a role in their probing experiments.\n\n6. \"Provable limitations of acquiring meaning from ungrounded form what will future language models understand\" [b30]: This paper is referenced in the related work section, indicating it served as a contrast to their findings and thus likely inspired the current paper's research to some degree.\n\nBased on the information provided, the confidence scores for each of these papers would be:\n\n{ \"b4\": 0.9, \"b11\": 0.8, \"b36\": 0.8, \"b42\": 0.8, \"b2\": 0.7, \"b30\": 0.7}\n", "Based on the given text, the paper's major challenges include determining whether language models (LMs) trained purely on form, like the conditional distribution of tokens in the training corpus, can acquire meaning or if they produce text only according to surface statistical correlations gleaned from the training data. The authors propose a new method that involves using a language modeling approach to the task of program synthesis and then probing the LM's hidden states for a representation of the program semantics using a linear classifier. \n\nAnalyzing the text and the referenced papers, the source papers that seem to have significantly influenced this work include:\n\n{\"reference number\": \"confidence score\"}\n1. \"b33\": 0.9 - The paper is directly related to the major challenge and hypothesis of the current work, questioning whether large language models can capture semantically meaningful information.\n2. \"b4\": 0.8 - This paper also tackles the question of whether LMs acquire meaning or just leverage surface statistics, which is a central challenge in the current paper.\n3. \"b42\": 0.8 - The current paper uses a Transformer, a concept introduced in this paper, as the LM for their experiment.\n4. \"b6\": 0.7 - This paper explores program synthesis with large language models, which is a key methodological approach in the current paper.\n5. \"b21\": 0.7 - This paper explores fine-tuning pretrained LMs on text that describes evolving situations, similar to the current paper's methodology.\n6. \"b44\": 0.7 - This paper provides an overview of transformers, which are used extensively in the current paper's methodology.\n7. \"b31\": 0.6 - This paper presents a survey that reveals the divide within the NLP community on whether an LM can understand natural language in a non-trivial sense, which is a key topic in the current paper's discussion.\n8. \"b10\": 0.6 - This paper introduces the concept of tracing a program's execution, which is used in the current paper's methodology for understanding semantics in programs. \n\nThe confidence scores associated with each source paper are an estimate of how directly each paper has contributed to the main contributions of the current paper, with 1 being the most direct and 0 being the least direct.", "Based on the given text and the provided instructions, the source papers that have inspired this paper can be inferred as:\n\n1. \"Climbing towards NLU on Meaning Form and Understanding in the Age of Data\" [b4]: This paper is referenced when discussing the possibility that Language Models (LMs) trained purely on form do not acquire meaning. The reference is directly linked to one of the main challenges addressed in the paper.\n\n2. \"Can language models encode perceptual structure without grounding a case study in color\" [b0]: This paper is indicated when discussing the possibility that pretrained LMs' representations of color terms are geometrically aligned with CIELAB space, which is related to the research question of the given paper.\n\n3. \"Probing classifiers promises shortcomings and advances\" [b2]: This paper is indicated when discussing the method of probing, which is a crucial part of the research methodology in the given paper.\n\n4. \"Provable limitations of acquiring meaning from ungrounded form what will future language models understand\" [b30]: This paper is referenced when the author argues that LMs cannot ground programs given only textual hints of semantics.\n\n5. \"Abstract interpretation a unified lattice model for static analysis of programs by construction or approximation of fixpoints\" [b11]: This paper is referenced when introducing the concept of abstract interpretation which is used to isolate a subset of the trace semantics in the given paper.\n\n6. \"Attention is all you need\" [b42]: This paper is referenced when discussing the training of a transformer model, which is a key part of the methodology in the given paper.\n\n7. \"Program synthesis with large language models\" [b1]: This paper is referenced when discussing the body of work on training large-scale, Transformer-based LMs for program synthesis.\n\nHere is the result in json format:\n\n{\n\"b4\": 0.9,\n\"b0\": 0.85,\n\"b2\": 0.8,\n\"b30\": 0.75,\n\"b11\": 0.7,\n\"b42\": 0.65,\n\"b1\": 0.65\n} \n\nThe confidence scores are assigned based on how directly relevant the source papers are to the paper's novel contributions and methods. A higher score indicates a stronger relevance."], "646c3addd68f896efa5d165d": ["Based on the text of the paper and the titles of the referenced papers, the most pertinent source papers that inspired this work are:\n\n1. \"Language models are few shot learners\" (b0): This paper is mentioned in the context of introducing the concept of in-context learning, which is a key feature of the algorithm proposed in the analyzed paper.\n2. \"Strategies for pre-training graph neural networks\" (b6): This paper is directly referenced in a discussion about existing graph pre-training methods, which suggests that it may have influenced the development of the proposed method.\n3. \"GPT GNN: Generative Pre-Training of Graph Neural Networks\" (b7): This paper is also mentioned in the context of existing graph pre-training methods, implying its significance to the proposed method.\n4. \"Task-adaptive few-shot node classification\" (b18): This paper is referenced during the discussion of existing meta-learning methods over graphs, indicating its relevance to the algorithm proposed in the analyzed paper.\n5. \"Relational multi-task learning: Modeling relations between data and tasks\" (b1): This paper is referenced as the inspiration for the proposed prompt graph representation, which is a critical aspect of the algorithm.\n6. \"Graph Contrastive Learning with Augmentations\" (b23): This paper is indicated in the comparison of the proposed method's performance, suggesting that it may have been a source of inspiration.\n\nI will now output the result in the requested format:\n\n{\n\"b0\": 0.9,\n\"b6\": 0.8,\n\"b7\": 0.8,\n\"b18\": 0.8,\n\"b1\": 1.0,\n\"b23\": 0.7\n}", "Based on the text of the academic paper and the given titles of the references, the following source papers seem most relevant to this work:\n\n1. \"Language models are few-shot learners\" (b0) - The paper mentions the \"capability of a pretrained model to perform novel and diverse tasks directly at the prediction time when prompted with just a few examples\". This is directly related to the concept of \"few-shot learners\" in the title of this reference.\n2. \"Strategies for pre-training graph neural networks\" (b6) - This reference is probably used as a basis for the pretraining methods and objectives used in the paper.\n3. \"GPT-GNN: generative pre-training of graph neural networks\" (b7) - The paper mentions the use of \"Pretraining Over Diverse In-Context Graph Systems (PRODIGY)\", which seems to be a method similar to the one in this reference.\n4. \"GCC: Graph contrastive coding for graph neural network pre-training\" (b12) - The paper discusses contrastive pretraining baselines, which this reference likely covers.\n5. \"Graph neural networks in recommender systems: a survey\" (b20) - This reference could be directly relevant to the paper's discussion of graph machine learning tasks, such as product suggestions across online e-commerce websites.\n6. \"Graph contrastive learning with augmentations\" (b23) - The paper refers to a novel self-supervised pretraining task, neighbor matching, which could be related to the concept of contrastive learning with augmentations.\n7. \"Few-shot knowledge graph completion\" (b24) - This reference could be relevant to the paper's discussion of knowledge graph completion tasks on novel graphs.\n\nThe confidence scores for these references can be estimated based on the relevance of the topics to the main contributions of the paper.\n\n{ \"b0\": 0.9, \"b6\": 0.8, \"b7\": 0.9, \"b12\": 0.8, \"b20\": 0.7, \"b23\": 0.8, \"b24\": 0.7 }", "Based on the text of the academic paper and the titles of the references, the source papers that most inspired the author are:\n\n1. \"Language models are few-shot learners\" [b0]: The concept of in-context learning is a central theme of the paper, which is a concept introduced by this reference. The author also specifically mentions this work.\n\n2. \"Relational multi-task learning modeling relations between data and tasks\" [b1]: This paper seems to have had a significant influence, as the author specifically mentions that they were inspired by this work when developing their own prompt graph representation.\n\n3. \"Strategies for pre-training graph neural networks\" [b6]: This paper seems to have influenced the author's methodology, as it is referenced in the context of discussing pre-training methods.\n\n4. \"GPT-GNN: Generative pre-training of graph neural networks\" [b7]: This paper seems to have inspired the author's use of pre-training in the context of graph neural networks.\n\n5. \"Graph meta-learning via local subgraphs\" [b8]: This paper seems to have inspired the author's meta-learning methods over graphs.\n\n6. \"Few-shot relational reasoning via connection subgraph pretraining\" [b9]: This paper seems to have influenced the author's approach to few-shot relational reasoning.\n\n7. \"Graph contrastive learning with augmentations\" [b23]: This paper seems to have inspired the author's use of contrastive learning with augmentations.\n\nThe JSON format of the top source papers with confidence scores is:\n\n{\n    \"b0\": 0.9,\n    \"b1\": 0.9,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b23\": 0.8\n}", "Based on the text and the titles of the references, the source papers that inspired the author most are likely the following:\n\n1. \"Language models are few shot learners\" [b0]: The current paper's main concept of in-context learning is greatly influenced by the idea of few-shot learning presented in this paper. The author frequently cites this paper, indicating its importance.\n\n2. \"Strategies for pre-training graph neural networks\" [b6]: This paper is referenced when discussing the limitations of existing graph pre-training methods, which the current paper aims to address. The lessons learned from these existing strategies likely influenced the development of the new approach.\n\n3. \"GPT-GNN: Generative Pre-Training of Graph Neural Networks\" [b7]: The new method developed in the current paper seems to be a significant improvement over the GPT-GNN method, indicating that this paper likely served as a key point of reference.\n\n4. \"Task-adaptive few-shot node classification\" [b18]: This paper's concept of task adaptation for few-shot learning over graphs seems to be a significant influence on the current paper's methods.\n\n5. \"Graph Contrastive Learning with Augmentations\" [b23]: The author frequently cites this paper when discussing the limitations of existing graph pre-training methods, which the current paper aims to address. The lessons learned from these existing strategies likely influenced the development of the new approach.\n\n6. \"Few-shot knowledge graph completion\" [b24]: This paper's concept of few-shot learning for knowledge graph completion seems to be a significant influence on the current paper's methods.\n\nThe confidence scores for each of these papers might look something like:\n\n{\"b0\": 0.9, \"b6\": 0.8, \"b7\": 0.8, \"b18\": 0.7, \"b23\": 0.8, \"b24\": 0.7}", "The paper discusses the development of a pre-training method for in-context graph learning. The method, called PRODIGY, aims to enable in-context learning for diverse graph machine learning tasks. The paper specifically focuses on in-context learning for node and edge classification tasks on graphs with few-shot prompting. The authors also present a general approach for solving these challenges for classification tasks on graphs, which includes a 'prompt graph,' an in-context graph task representation, and a framework for pretraining an in-context learner over prompt graphs.\n\nFrom the text, the following references can be inferred as the source papers:\n\n1. \"In-context learning is a novel and one of the most intriguing capabilities of language models [b0]\" - This indicates that the authors are inspired by the capabilities of language models in in-context learning as discussed in the paper 'Language models are few-shot learners' [b0]. Confidence score: 0.8\n\n2. \"Existing graph pretraining methods [b6] [b23] [b7] [b12] only aim to learn a good graph encoder and require fine-tuning to adapt to different tasks, while existing meta-learning methods over graphs [b18] [b8] [b2] [b16] [b24] only aim to generalize across different tasks within the same graph.\" - This indicates that the authors have taken inspiration from these papers to overcome the limitations of existing graph pretraining methods and meta-learning methods over graphs. Confidence scores: [b6]: 0.6, [b23]: 0.6, [b7]: 0.6, [b12]: 0.6, [b18]: 0.6, [b8]: 0.6, [b2]: 0.6, [b16]: 0.6, [b24]: 0.6\n\n3. \"Prompt Graph Representation Inspired by [b1]\" - This indicates a direct inspiration from the paper 'Relational multi-task learning modeling relations between data and tasks' [b1] in the development of their 'prompt graph' concept. Confidence score: 1.0\n\nThe final result in JSON format:\n\n{\n  \"b0\": 0.8,\n  \"b6\": 0.6,\n  \"b23\": 0.6,\n  \"b7\": 0.6,\n  \"b12\": 0.6,\n  \"b18\": 0.6,\n  \"b8\": 0.6,\n  \"b2\": 0.6,\n  \"b16\": 0.6,\n  \"b24\": 0.6,\n  \"b1\": 1.0\n}"], "64702deed68f896efa51ffa0": ["Based on the text of the paper and the titles of the references, the most pertinent source papers that inspired the author are:\n\n1. \"Text augmented open knowledge graph completion via pre-trained language models\" - The title of this paper directly aligns with the core contributions of the paper being analyzed, which focuses on open knowledge graph completion using pre-trained language models. The confidence score is 1.\n2. \"BERT pre-training of deep bidirectional transformers for language understanding\" [b11] - The author mentions the use of pre-trained language models (PLMs), specifically BERT, for knowledge graph completion. The confidence score is 0.9.\n3. \"Do pre-trained models benefit knowledge graph completion? A reliable evaluation and a reasonable approach\" [b27] - This paper seems to discuss the benefits of using pre-trained models for knowledge graph completion, a method that the author adopts and improves upon in their paper. The confidence score is 0.8.\n4. \"How can we know what language models know?\" [b19] - This paper discusses the challenges of understanding what knowledge is contained within language models, a challenge that the author addresses in their paper by proposing TAGREAL. The confidence score is 0.7.\n5. \"MetaPAD: Meta pattern discovery from massive text corpora\" [b18] - The author adopts the approach proposed in this paper for prompt selection during the prompt generation process. The confidence score is 0.7.\n6. \"TruePIE: Discovering reliable patterns in pattern-based information extraction\" [b21] - This paper's method is also used by the author in the prompt selection process. The confidence score is 0.7.\n7. \"KG-BERT: BERT for knowledge graph completion\" [b43] - This paper is mentioned as a pioneer work that the author aims to improve upon. The confidence score is 0.6.\n\nThe returned result is in the following json format:\n\n{\n\"Text augmented open knowledge graph completion via pre-trained language models\": 1,\n\"[b11]\": 0.9,\n\"[b27]\": 0.8,\n\"[b19]\": 0.7,\n\"[b18]\": 0.7,\n\"[b21]\": 0.7,\n\"[b43]\": 0.6\n}", "Based on the text and the titles of the references, the most pertinent source papers appear to be:\n\n1. \"Language models as knowledge bases\" [b29]: The paper frequently discusses the use of language models for knowledge graph completion, suggesting that this work was a significant influence. Confidence score: 0.9\n\n2. \"Translating embeddings for modeling multirelational data advances in neural information processing systems\" [b4]: This paper is mentioned in relation to the limitations in existing KG completion approaches. Confidence score: 0.8\n\n3. \"A robustly optimized BERT pretraining approach\" [b24]: The use of pre-trained language models for KG completion is a central theme in the paper, suggesting this work may have influenced the approach. Confidence score: 0.7\n\n4. \"Do pretrained models benefit knowledge graph completion? A reliable evaluation and a reasonable approach\" [b27]: This paper is referred to directly as a comparison point for the proposed TAGREAL method, indicating it likely had a significant influence. Confidence score: 0.85\n\n5. \"Mining frequent patterns without candidate generation\" [b14]: The use of the FP-Growth algorithm for prompt generation suggests this work was a direct inspiration. Confidence score: 0.75\n\n6. \"MetaPAD: Meta Pattern Discovery from Massive Text Corpora\" [b18]: This paper's methods are utilized for prompt selection, indicating it was a source of inspiration. Confidence score: 0.75\n\n7. \"TruePIE: Discovering Reliable Patterns in Pattern-Based Information Extraction\" [b21]: This paper's methods are also used for prompt selection, indicating it was an important source. Confidence score: 0.75\n\nThis analysis is based on the text and the titles of the references. Further confirmation would require a full-text analysis of the cited papers.", "Based on the text of the paper, the primary challenges are: the limitations of existing KG completion approaches, reliance on dense graphs, lack of consideration for open-world KGs, and the reliance on manually created prompts. The author proposes TAGREAL, a new framework for open KG completion that uses PLMs and automatically generates high-quality prompts. \n\nThe following references seem to be directly related to the foundation of the paper:\n\n1. b4 - \"Translating embeddings for modeling multirelational data advances in neural information processing systems\" - The author mentions this as an example of a method that assumes a closed-world KG and does not consider vast open knowledge in the external resources, which is a limitation that TAGREAL tries to overcome.\n\n2. b11 and b24 - Both are about BERT and its uses, which is a type of PLM that the authors have utilized in their TAGREAL framework. \n\n3. b27 - \"Do pretrained models benefit knowledge graph completion? A reliable evaluation and a reasonable approach\" - The author mentions this paper when discussing their method of not requiring manually predefined facts and prompts, which is a limitation of the approach described in b27 that TAGREAL overcomes.\n\n4. b34 - \"Autoprompt: Eliciting knowledge from language models with automatically generated prompts\" - The authors reference this when discussing the limitation of manual prompt creation, which TAGREAL alleviates with its automatic prompt creation feature.\n\n5. b18 and b21 - Both are about methods for mining patterns from large corpora, which the authors used for automatic prompt generation in TAGREAL.\n\n6. b32 - \"Automated phrase mining from massive text corpora\" - This is used in the paper for segmenting corpora to more natural and unambiguous semantic phrases.\n\n7. b14 - \"Mining frequent patterns without candidate generation\" - The authors use the FP-Growth algorithm mentioned in this paper to mine frequent appeared patterns to compose a candidate set.\n\n8. b31 - \"Okapi at TREC\" - The authors use the BM25 method mentioned in this paper to retrieve highly ranked support texts.\n\nThe json format of these references with an estimated confidence score would be:\n{\n\"b4\": 0.8,\n\"b11\": 0.9,\n\"b24\": 0.9,\n\"b27\": 0.8,\n\"b34\": 0.7,\n\"b18\": 0.7,\n\"b21\": 0.7,\n\"b32\": 0.8,\n\"b14\": 0.8,\n\"b31\": 0.8\n}", "Based on the provided information and analysis of the text, the source papers that most likely have inspired the paper are:\n\n1. \"Do pretrained models benefit knowledge graph completion? A reliable evaluation and a reasonable approach\" [b27]\n2. \"How can we know what language models know?\" [b19]\n3. \"MetaPAD: Meta Pattern Discovery from Massive Text Corpora\" [b18]\n4. \"TruePIE: Discovering Reliable Patterns in Pattern-Based Information Extraction\" [b21]\n5. \"AutoPhrase: Automated Phrase Mining from Massive Text Corpora\" [b32]\n6. \"KG-BERT: BERT for Knowledge Graph Completion\" [b43]\n7. \"Translating embeddings for modeling multirelational data\" [b4]\n8. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b11]\n\nHere is the confidence score for each reference:\n\n{ \"b27\": 0.9,\n  \"b19\": 0.85,\n  \"b18\": 0.8,\n  \"b21\": 0.8,\n  \"b32\": 0.75,\n  \"b43\": 0.75,\n  \"b4\": 0.7,\n  \"b11\": 0.7\n} \n\nThese papers are selected based on the heavy reliance of the paper on pre-existing research for its core contributions, direct relevance of the source papers to the paper's novel contributions, and the presence of key phrases that indicate direct inspiration.", "Based on the given text and the titles of the references, the source papers that inspire the author the most are likely the following:\n\n- \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b11]: The confidence score is 0.9 because the author uses pre-trained language models (PLMs) as a significant part of their work and BERT is a pioneering work in this area.\n\n- \"Language Models as Knowledge Bases\" [b29], [b30]: The confidence score is 0.85. The author's work is significantly influenced by the concept that language models can serve as knowledge bases. \n\n- \"Do Pretrained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach\" [b27]: The confidence score is 0.75. The author's work builds on this paper's method of using PLMs for knowledge graph completion and improves upon it by automatically generating prompts. \n\n- \"How can we know what language models know?\" [b19]: The confidence score is 0.7. This paper is likely a major inspiration for the author's work on probing the implicit knowledge in PLMs.\n\n- \"MetaPAD: Meta Pattern Discovery from Massive Text Corpora\" [b18]: The confidence score is 0.7. The author's methodology of using textual pattern mining methods for prompt mining was inspired by this paper.\n\n- \"TruePIE: Discovering Reliable Patterns in Pattern-Based Information Extraction\" [b21]: The confidence score is 0.65. This paper likely influenced the author's approach to prompt selection and optimization.\n\n- \"KG-BERT: BERT for Knowledge Graph Completion\" [b43]: The confidence score is 0.6. This paper is a pioneering work in fine-tuning PLMs for knowledge graph completion, which is a central aspect of the author's work.\n\n- \"RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space\" [b35]: The confidence score is 0.5. Although not directly related to the author's work, it is a fundamental work in the field of knowledge graph embedding, which could have provided some basis for the author's research.\n\nThe JSON format of the result is:\n\n{\n\"b11\": 0.9,\n\"b29\": 0.85,\n\"b30\": 0.85,\n\"b27\": 0.75,\n\"b19\": 0.7,\n\"b18\": 0.7,\n\"b21\": 0.65,\n\"b43\": 0.6,\n\"b35\": 0.5\n}"], "632630ff90e50fcafdf67436": ["Based on the provided text, we can identify the following source papers that most likely had a significant influence on the given paper's main contributions:\n\n1. Qureshi et al. [b28]: The paper mentions this work as the beginning of adaptive solutions for cache replacement, indicating it may have provided a significant motivation for the current paper's algorithm.\n2. SDBP [b18], SHiP [b39], and Hawkeye [b11]: These are all mentioned as examples of previous prediction-based policies that the current paper is building upon.\n3. Keramidas et al. [b17] and Petoumenos et al. [b27]: These works are mentioned as having proposed an idea that the current paper is directly improving upon, namely the prediction of each cache line's reuse distance.\n4. Belady's MIN policy [b4]: This policy is mentioned repeatedly throughout the text as a benchmark the current paper's algorithm is trying to emulate.\n5. Liu et al. [b22]: Mentioned as another machine learning model that attempted to emulate Belady's MIN policy, but was too expensive to implement in hardware.\n\nThe confidence scores will be assigned based on the frequency and context of the mentions in the paper's text. For example, works that are mentioned multiple times and whose ideas are directly built upon by the current paper will receive higher confidence scores.\n\nThe result in a json format: \n{\n    \"b28\": 0.9,\n    \"b18\": 0.8,\n    \"b39\": 0.8,\n    \"b11\": 0.9,\n    \"b17\": 0.85,\n    \"b27\": 0.85,\n    \"b4\": 0.9,\n    \"b22\": 0.7\n}", "Based on the text and the author's notes, the most relevant source papers that inspired this paper are:\n- \"back to the future leveraging belady s algorithm for improved cache replacement\" [b11]: This paper is mentioned multiple times and is directly compared to the presented work, indicating that it played a significant role in the development of the algorithm proposed in the paper.\n- \"cache replacement based on reuse distance prediction\" [b17]: The authors mention that their work improves upon this paper's idea, which suggests this paper's influence.  \n- \"instructionbased reuse distance prediction replacement policy\" [b27]: This paper is mentioned as being improved upon in the presented work, indicating that it was a source of inspiration.\n- \"adaptive insertion policies for high performance caching\" [b28]: It marks the beginning of adaptive solutions, which the paper's authors are building upon.\n- \"a study of replacement algorithms for a virtualstorage computer\" [b4]: The authors directly state that their work aims to mimic Belady's MIN policy, which is introduced in this paper.\n- \"ship signature based hit predictor for high performance caching\" [b39]: This paper is mentioned when the authors are discussing prediction-based policies, indicating that it was a source of inspiration. \n\nThe result in the json format is: \n{\n    \"[b11]\": 0.9,\n    \"[b17]\": 0.8,\n    \"[b27]\": 0.8,\n    \"[b28]\": 0.7,\n    \"[b4]\": 0.8,\n    \"[b39]\": 0.7\n}", "Based on the given text, the source papers that inspire the author most are:\n\n- b11: \"Back to the Future: Leveraging Belady's Algorithm for Improved Cache Replacement.\"\n- b17: \"Cache replacement based on reuse distance prediction\"\n- b27: \"Instruction-Based Reuse Distance Prediction Replacement Policy\"\n- b39: \"SHIP: Signature Based Hit Predictor for High Performance Caching\"\n- b4: \"A study of replacement algorithms for a virtual-storage computer\"\n\nThe paper makes use of ideas from these papers to develop a novel approach to cache replacement, using an ETA-based policy inspired by Belady's MIN policy (b4), and improving upon the binary classification approach of SHiP (b39) and the reuse distance prediction strategies of Keramidas et al. (b17) and Petoumenos et al. (b27). The author's work was also influenced by the prediction-based policies outlined in Hawkeye (b11).\n\nIn the JSON format, the result is:\n\n{ \n\"b11\": 0.9,\n\"b17\": 0.8,\n\"b27\": 0.7,\n\"b39\": 0.8,\n\"b4\": 0.9\n}", "Based on the given paper text and author notes, the source papers that inspire the author most are:\n\n1. \"Back to the future: Leveraging Belady's algorithm for improved cache replacement\" [b11]: This paper is mentioned several times throughout the text and is central to the development of the paper's core contributions. The author's new approach \"Mockingjay\" aims to improve upon the binary classification approach introduced in [b11].\n2. \"Cache replacement based on reuse distance prediction\" [b17]: The author's Mockingjay method also builds upon the ideas introduced in [b17] by attempting to mimic Belady's MIN policy more directly.\n3. \"Instruction-based reuse distance prediction replacement policy\" [b27]: The paper's proposed solution, Mockingjay, attempts to further improve upon the prediction-based solutions introduced in [b27].\n4. \"Back to the future: Leveraging Belady's algorithm for improved cache replacement\" [b11]: The author mentions the prediction-based solutions in [b11] as a comparison point to their novel ETA-based solution.\n\nThe following JSON object represents the source papers with a confidence score:\n\n{\n    \"b11\": 1.0,\n    \"b17\": 0.8,\n    \"b27\": 0.8\n}\nThe confidence scores are assigned based on the frequency and context of the mentions of these papers in the text. [b11] is given the highest score as it is mentioned multiple times and is a central point of comparison for the paper's main contributions. [b17] and [b27] are also given high scores as they are directly linked to the methods proposed in the paper.", "Based on the given text, the source papers for the given paper seem to be:\n\n{ \n\"b11\": 0.9, \n\"b17\": 0.8, \n\"b27\": 0.8, \n\"b4\": 0.7\n} \n\nExplanation:\n\nThe paper b11, \"Back to the future leveraging belady s algorithm for improved cache replacement\", appears prominently in the text, suggesting that it had a significant influence on the authors' work. \n\nThe papers b17, \"Cache replacement based on reuse distance prediction\", and b27, \"Instructionbased reuse distance prediction replacement policy\", are specifically mentioned as pioneering works in the field of cache replacement and appear to have directly influenced the development of the authors' Mockingjay algorithm.\n\nThe paper b4, \"A study of replacement algorithms for a virtual storage computer\", is the source of Belady's MIN policy, which the authors have tried to emulate.\n\nThe confidence scores are inferred from the degree of influence and reference frequency in the text. \n"], "6456389bd68f896efacf6b14": ["{\n    \"b13\": 0.9,\n    \"b7\": 0.9,\n    \"b14\": 0.8,\n    \"b0\": 0.75,\n    \"b4\": 0.7\n}", "{\n  \"b13\": 1.0,\n  \"b7\": 1.0,\n  \"b14\": 0.9,\n  \"b28\": 0.8\n}", "{\n  \"b13\": 0.9,\n  \"b7\": 0.9,\n  \"b14\": 0.8,\n  \"b1\": 0.8,\n  \"b17\": 0.7\n}", "{\n  \"b13\": 1.0,\n  \"b7\": 1.0,\n  \"b14\": 0.8,\n  \"b1\": 0.8,\n  \"b24\": 0.75,\n  \"b4\": 0.75\n}", "{\n    \"b13\": 0.9,\n    \"b7\": 0.9,\n    \"b14\": 0.8,\n    \"b1\": 0.8,\n    \"b17\": 0.75,\n    \"b24\": 0.7\n}"], "648697e6d68f896efaa87966": ["Based on the text and the titles of the references, the most pertinent source papers that inspired the given paper can be identified as follows:\n\n1. 'Weisfeiler and leman go neural higher order graph neural networks' (b43) - The paper discusses the limitations of standard message passing architectures and the Weisfeiler-Leman (WL) algorithm, which is also a major point in the given text. Confidence score: 0.9\n\n2. 'How powerful are graph neural networks' (b61) - The paper discusses the expressive power of GNNs, similar to the given paper. Confidence score: 0.9\n\n3. 'Shortest path networks for graph property prediction' (b1) - The given paper's proposed model, PathNN, is based on the concept of shortest paths, which this source paper also discusses. Confidence score: 0.85\n\n4. 'Provably powerful graph networks' (b39) - The given paper cites this source when discussing the power of their PathNN variants. Confidence score: 0.8\n\n5. 'PathNet model' (b54) - The paper mentions PathNet model, a model that also aggregates path information and is similar to the proposed PathNN model. Confidence score: 0.8\n\n6. 'Distance encoding design provably more powerful neural networks for graph representation learning' (b38) - The paper discusses models that use shortest path distances, similar to the PathNN model. Confidence score: 0.75\n\nHere is the result in the requested json format:\n\n```json\n{\n  \"b43\": 0.9,\n  \"b61\": 0.9,\n  \"b1\": 0.85,\n  \"b39\": 0.8,\n  \"b54\": 0.8,\n  \"b38\": 0.75\n}\n```", "Based on the provided information, the source papers that inspired this paper are:\n\n1. \"Weisfeiler and Leman go neural: Higher-order graph neural networks\" [b43]\n2. \"How powerful are graph neural networks\" [b61]\n3. \"Shortest path networks for graph property prediction\" [b1]\n4. \"Structure aware path aggregation graph neural network\" [b54]\n5. \"A note on two problems in connexion with graphs\" [b17]\n6. \"Distance encoding: Design provably more powerful neural networks for graph representation learning\" [b38]\n7. \"Building powerful and equivariant graph neural networks with structural message passing\" [b57]\n\nThis conclusion is derived from the text features that indicate a strong reliance on these research papers for developing the paper's core contributions. Also, these papers seem to share a strong methodological similarity with the paper being analyzed.\n\nThe returned result in JSON format would be:\n\n{\n\"b43\": 0.9,\n\"b61\": 0.9,\n\"b1\": 0.8,\n\"b54\": 0.8,\n\"b17\": 0.7,\n\"b38\": 0.7,\n\"b57\": 0.7\n}\n\nThis JSON object indicates the level of relevance of the source paper to the given paper, with a higher number indicating more relevance.", "Based on the analysis of the academic paper text and author notes, the most pertinent source papers that inspired the author are as follow:\n\n- 'b1': This paper, \"Shortest Path Networks for Graph Property Prediction\", is referenced in the main text when the author discusses that their PathNN model was inspired by the idea of shortest path distances, which is a concept that this paper heavily focuses on. Confidence Score: 0.9.\n  \n- 'b54': This paper, \"Structure Aware Path Aggregation Graph Neural Network\", was specifically called out as a similar work to the author's PathNN model. The author states that there are major differences between their approach and the PathNet model, but it would seem that they drew some inspiration from this source. Confidence Score: 0.7.\n  \n- 'b43': This paper, \"Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks\", is referenced multiple times when the author discusses the limitations of standard GNNs and how their PathNN model surpasses these limitations. It seems that this paper was a significant influence on the development of the PathNN model. Confidence Score: 0.8.\n  \n- 'b61': This paper, \"How Powerful Are Graph Neural Networks\", is also referenced multiple times and seems to be the basis for many of the problems the author is trying to solve with their PathNN model. Confidence Score: 0.8.\n\n- 'b39': The paper, \"Provably Powerful Graph Networks\", is mentioned when the author discusses the expressive power of their PathNN models. It seems that this paper inspired the author in their quest to create a model with greater expressive power than standard GNNs. Confidence Score: 0.7.\n\nThe JSON format of the result is: \n\n{\n\"b1\": 0.9,\n\"b54\": 0.7,\n\"b43\": 0.8,\n\"b61\": 0.8,\n\"b39\": 0.7\n}", "Based on the given information and the text of the academic paper, the identified source papers most likely to have inspired the authors are as follows:\n\n- b1: 'shortest path networks for graph property prediction' (Confidence Score: 0.9)\n- b43: 'weisfeiler and leman go neural higher order graph neural networks' (Confidence Score: 0.8)\n- b54: 'structure aware path aggregation graph neural network' (Confidence Score: 0.7)\n- b61: 'how powerful are graph neural networks' (Confidence Score: 0.7)\n- b26: 'expressiveness and approximation properties of graph neural networks' (Confidence Score: 0.6)\n- b17: 'a note on two problems in connexion with graphs' (Confidence Score: 0.6)\n\nThese papers were chosen based on the key challenges outlined in the paper, the algorithm proposed by the author and the direct references to these papers within the text. The confidence scores were determined based on the frequency and context of the references within the text, with a higher score indicating a higher likelihood of the paper having significantly influenced the author.", "Based on the text of the academic paper and the titles of the references, the following can be inferred as the most pertinent source papers:\n\n1. \"How powerful are graph neural networks\" [b61]: This paper is mentioned multiple times in the text, and it appears to be a key source for the understanding of the expressive power of GNNs and the Weisfeiler-Leman (WL) algorithm. Confidence Score: 0.9\n\n2. \"A comprehensive survey on graph neural networks\" [b60]: This paper is mentioned as a source that outlines the development and common use of GNNs. Confidence Score: 0.8\n\n3. \"Weisfeiler and Leman go neural higher order graph neural networks\" [b43]: This paper is mentioned multiple times and is particularly relevant as it indicates the connection between the neighborhood aggregation scheme and the WL algorithm, and also provides the idea of developing more complex models that focus on subgraphs. Confidence Score: 0.85\n\n4. \"Shortest path networks for graph property prediction\" [b1]: This paper is mentioned as a source that the proposed Path Neural Networks model is compared to. It indicates the use of shortest path distances for the expressiveness of the models. Confidence Score: 0.75\n\n5. \"The surprising power of graph neural networks with random node initialization\" [b0]: It is mentioned in the context of the expressive power of GNNs. Confidence Score: 0.7\n\n6. \"Provably powerful graph networks\" [b39]: This paper is mentioned in the context of the comparison of the expressive power of GNNs. Confidence Score: 0.75\n\n7. \"Structure aware path aggregation graph neural network\" [b54]: This paper is mentioned as the most related work to the proposed Path Neural Networks model. Confidence Score: 0.85\n\n8. \"Breaking the limits of message passing graph neural networks\" [b2]: Inferred from the title, it appears to contribute to the novel methods and approaches the paper introduces to tackle these challenges. Confidence Score: 0.7\n\nThe result in the json format would be:\n{\"b61\": 0.9, \"b60\": 0.8, \"b43\": 0.85, \"b1\": 0.75, \"b0\": 0.7, \"b39\": 0.75, \"b54\": 0.85, \"b2\": 0.7}\n"], "648697e6d68f896efaa8781e": ["Based on the text, the primary challenges outlined in the paper are the gap between topology-aware Graph Neural Networks (GNN) and inference-efficient Multi-Layer Perceptrons (MLP), the lack of exploration on the reliability of different knowledge points in GNNs, especially their roles in the distillation process. The paper proposes a method called Knowledge-inspired Reliable Distillation (KRD) to tackle these challenges. The KRD is modelled to quantify the knowledge in GNNs by measuring the invariance of their information entropy to noise perturbations, and it samples additional reliable knowledge points as supervision for training student MLPs.\n\nThe most pertinent source papers that inspired the author's work are:\n\n1. \"Graph less neural networks\" [b33]: The paper mentions that one solution to reduce the gap between GNNs and MLPs is to perform GNN-to-MLP knowledge distillation, this idea is motivated by [b33].\n\n2. \"Double wins boosting accuracy and efficiency of graph neural networks by reliable knowledge distillation\" [b0]: The paper mentions that RKD-MLP [b0] adopts a meta-policy to filter out unreliable soft labels, but this is essentially a downsampling-style strategy that will further reduce the already limited supervision. In contrast, this paper aims to provide more reliable supervision for training student MLPs, which can be considered as an up-sampling-style strategy.\n\n3. \"Distilling the knowledge in a neural network\" [b7]: The paper mentions that the knowledge distillation is first introduced in [b7] to mainly handle image data.\n\n4. \"On self distilling graph neural network\" [b2]: The paper mentioned that GNN-SD [b2] directly distills knowledge across different GNN layers, mainly aiming to solve the over-smoothing problem but with unobvious performance improvement at shallow layers.\n\n5. \"FreeKD freedirection knowledge distillation for graph neural networks\" [b3]: The paper mentioned that FreeKD [b3] studies a free-direction knowledge distillation architecture, with the purpose of dynamically exchanging knowledge between two shallower GNNs.\n\n6. \"Extract the knowledge of graph neural networks and go beyond it: an effective knowledge distillation framework\" [b30]: The paper mentioned that CPF [b30] directly improves the performance of student MLPs by adopting deeper/wider network architectures and incorporating label propagation in MLPs, both of which burden the inference latency.\n\n7. \"Extracting low high frequency knowledge from graph neural networks and injecting it into mlps: an effective gnn tomlp distillation framework\" [b26]: The paper mentioned that FF-G2M [b26] is one of the state-of-the-art graph distillation baselines.\n\nReturn results in json format:\n{\n\"b33\": 0.9,\n\"b0\": 0.8,\n\"b7\": 0.7,\n\"b2\": 0.7,\n\"b3\": 0.7,\n\"b30\": 0.7,\n\"b26\": 0.6\n}\n", "Based on the provided text and the author notes, the source papers that most inspired the given paper can be identified as follows:\n\n- \"Graph Less Neural Networks\" [b33]: This paper is mentioned multiple times in the text and is directly linked to the main challenge outlined in the paper - bridging the gap between Graph Neural Networks (GNNs) and Multi-Layer Perceptrons (MLPs). The authors of the given paper are motivated to improve upon the method proposed in [b33], which is the GNN-to-MLP knowledge distillation.\n\n- \"Double Wins: Boosting Accuracy and Efficiency of Graph Neural Networks by Reliable Knowledge Distillation\" [b0]: This paper is also directly related to the paper's main contribution, which is the proposal of a new method for knowledge distillation. In the text, it is mentioned as a method to improve upon.\n\n- \"On Self Distilling Graph Neural Network\" [b2]: Mentioned as a source that attempts to distill knowledge across different GNN layers, which is a concept utilized in the given paper.\n\n- \"Knowledge Distillation: A Survey\" [b5]: This paper might have provided the theoretical basis and understanding for the concept of knowledge distillation which is a major part of the given paper.\n\n- \"FreeKD: Free-Direction Knowledge Distillation for Graph Neural Networks\" [b3]: This work is mentioned as a study of a free-direction knowledge distillation architecture, which might have inspired the distillation approach used in the given paper.\n\nUsing these findings, the result can be returned in the requested JSON format as:\n\n{\n\"b33\": 1,\n\"b0\": 0.9,\n\"b2\": 0.8,\n\"b5\": 0.8,\n\"b3\": 0.7\n} \n\nIn this format, the keys are the reference numbers and the values are the confidence scores. The confidence scores are assigned based on how strongly the text suggests that each source paper inspired the given paper, with 1 being the highest possible confidence level.", "Based on the text of the academic paper, the most pertinent source papers that inspired the author the most are:\n\n1. \"quantifying the knowledge in gnns for reliable distillation into mlps\"\n2. \"double wins boosting accuracy and efficiency of graph neural networks by reliable knowledge distillation\"\n3. \"graph less neural networks\"\n4. \"extract the knowledge of graph neural networks and go beyond it an effective knowledge distillation framework\"\n5. \"learning efficient graph neural networks\"\n6. \"graph neural networks a review of methods and applications\"\n7. \"knowledge distillation a survey\"\n8. \"reliable data distillation on graph convolutional network\"\n\nThe JSON format for the answers is:\n\n{\n\"None\": 0.9,\n\"b0\": 0.8,\n\"b33\": 0.8,\n\"b30\": 0.7,\n\"b28\": 0.7,\n\"b27\": 0.6,\n\"b5\": 0.6,\n\"b34\": 0.6\n}\n", "Based on the provided text and the titles of the references, the following papers seem to have inspired the given paper most:\n\n1. \"Graph Less Neural Networks\" [b33]\n2. \"Double Wins: Boosting Accuracy and Efficiency of Graph Neural Networks by Reliable Knowledge Distillation\" [b0]\n3. \"Knowledge Distillation: A Survey\" [b5]\n4. \"Graph Neural Networks: A Review of Methods and Applications\" [b35]\n5. \"Learning Efficient Graph Neural Networks\" [b28]\n6. \"Inductive Representation Learning on Large Graphs\" [b6]\n7. \"Distilling the Knowledge in a Neural Network\" [b7]\n\nThese papers are identified based on the clues from the text where they are mentioned in the context of the paper's novel contributions or the methodology proposed by the authors. Key phrases such as \"inspired by\", \"motivated by\" are used as indicators.\n\nHere is the result in json format:\n\n{\n  \"b33\": 0.9,\n  \"b0\": 0.85,\n  \"b5\": 0.8,\n  \"b35\": 0.75,\n  \"b28\": 0.7,\n  \"b6\": 0.65,\n  \"b7\": 0.6\n} \n\nPlease note that the confidence scores are estimated based on the context in which these papers are cited in the text. The closer to 1, the more confident we are that the paper had a significant influence on the given paper.", "Based on the text of the paper, the following references appear to be the most influential sources:\n\n- \"b33\": \"graph less neural networks\". The paper is largely motivated by the work from reference b33, the GLNN paper, as it is mentioned multiple times and the current paper directly builds upon the GLNN's approach to bridge the gap between GNNs and MLPs.\n\n- \"b30\": \"extract the knowledge of graph neural networks and go beyond it an effective knowledge distillation framework\". Reference b30 is also mentioned in the context of previous attempts to reduce the gap between GNNs and MLPs, indicating that it was a significant influence.\n\n- \"b0\": \"double wins boosting accuracy and efficiency of graph neural networks by reliable knowledge distillation\". The authors draw upon the RKD-MLP approach from reference b0 to improve upon GLNN, which suggests that it was an important source.\n\n- \"b28\": \"learning efficient graph neural networks\". This paper is mentioned in the context of understanding the role of message passing in GNNs, which influenced the development of the authors' proposed approach.\n\n- \"b7\": \"distilling the knowledge in a neural network\". This paper is mentioned as the original proposition of knowledge distillation, which is a core concept of the current paper.\n\n- \"b6\": \"inductive representation learning on large graphs\". This paper is cited as a key understanding of the strengths of GNNs, which is fundamental to the work.\n\n- \"b11\": \"semi supervised classification with graph convolutional networks\". This paper is mentioned as the source of the teacher GNN framework, indicating that it was a significant influence.\n\nHere's the JSON format result: \n\n{\n\"b33\": 0.9,\n\"b30\": 0.85,\n\"b0\": 0.8,\n\"b28\": 0.75,\n\"b7\": 0.7,\n\"b6\": 0.65,\n\"b11\": 0.6\n}"], "6389d70490e50fcafdffd95e": ["Based on the text of the paper, the author indicates that their work is inspired by the following source papers:\n\n1. \"Compresso: Pragmatic Main Memory Compression\" [b5]\n2. \"Data Compression Accelerator on IBM Power and Z Processors\" [b10]\n3. \"Linearly Compressed Pages: A Low-Complexity, Low-Latency Main Memory Compression Framework\" [b4]\n4. \"Transparent Dual Memory Compression Architecture\" [b9]\n5. \"OS Memory Compression\" [b33, b34, b35, b36]\n6. \"Pinnacle: IBM MXT in a Memory Controller Chip\" [b3]\n7. \"A Robust Main Memory Compression Scheme\" [b6]\n\nThese source papers are inferred based on the references made to their content and methods throughout the paper, indicating a strong reliance on these works for developing the paper's core contributions. The confidence scores are not specifically provided here, but can be calculated based on the frequency and depth of references to each source paper.\n\nBased on the titles of the papers and the context in which they are referenced, it can be inferred that these papers have likely had a significant influence on the development of the author's research. The author's novel contributions, such as the proposed algorithm for hardware-based memory capacity compression and the method for addressing the challenges of high address translation overheads, seem to be directly inspired by these sources.\n\nHere is the result in json format:\n{\n\"b5\": \"0.9\",\n\"b10\": \"0.9\",\n\"b4\": \"0.8\",\n\"b9\": \"0.8\",\n\"b33\": \"0.7\",\n\"b34\": \"0.7\",\n\"b35\": \"0.7\",\n\"b36\": \"0.7\",\n\"b3\": \"0.7\",\n\"b6\": \"0.7\"\n}", "Based on the given text, the most relevant source papers are:\n\n1. \"Compresso: pragmatic main memory compression\": The paper heavily discusses Compresso in several parts, comparing its new method against it.\n2. \"A robust main memory compression scheme\": The author mentions this paper as a previous work exploring hardware memory compression.\n3. \"Pinnacle: IBM MXT in a memory controller chip\": The paper also mentions this work in the context of hardware memory compression.\n4. \"Data compression accelerator on IBM power and z processors\": The author's work is inspired by IBM's Deflate design and they specifically mention this paper.\n5. \"Translation-optimized Memory Compression for Capacity\": This is the main paper itself, but it seems to heavily reference the previously mentioned works.\n\nThe confidence score for each:\n\n1. \"Compresso: pragmatic main memory compression\": 0.9\n2. \"A robust main memory compression scheme\": 0.7\n3. \"Pinnacle: IBM MXT in a memory controller chip\": 0.7\n4. \"Data compression accelerator on IBM power and z processors\": 0.8\n5. \"Translation-optimized Memory Compression for Capacity\": 1.0\n\nThe JSON format of the result is:\n\n{\n  \"b5\": 0.9,\n  \"b6\": 0.7,\n  \"b3\": 0.7,\n  \"b10\": 0.8,\n  \"None\": 1.0\n}", "Based on the text of the paper and the instructions provided, the following references appear to be the most significant source papers:\n\n1. 'b5': 'compresso pragmatic main memory compression'\n2. 'b10': 'data compression accelerator on ibm power and z processors'\n3. 'b18': 'enabling transparent memory compression for commodity memory systems'\n4. 'b22': 'prefetched address translation'\n5. 'b42': 'high bandwidth decompression of variable length encoded data streams'\n\nThese references are selected based on the key phrases and themes in the paper such as \"memory compression\", \"data compression\", \"address translation\", and \"high bandwidth decompression\". \n\nThe JSON format would be:\n\n{\n\"b5\": 0.8,\n\"b10\": 0.7,\n\"b18\": 0.6,\n\"b22\": 0.7,\n\"b42\": 0.6\n}\n\nEach confidence score is an estimation of how directly each source paper contributed to the main contributions of the paper. A higher score indicates a higher level of direct contribution.", "Based on the text of the paper, the author's notes, and the titles of the references, the most relevant source papers for the given academic paper are:\n\n1. \"Pinnacle IBM MXT in a memory controller chip\" [b3]\n2. \"Linearly Compressed Pages: a low complexity, low latency main memory compression framework\" [b4]\n3. \"Compresso: Pragmatic Main Memory Compression\" [b5]\n4. \"Data compression accelerator on IBM power and z processors\" [b10]\n5. \"Enabling Transparent Memory Compression for Commodity Memory Systems\" [b18]\n6. \"Every Walk's a Hit: Making Page Walks Single Access Cache Hits\" [b23]\n7. \"What is memory compression in Windows\" [b33]\n\nThe confidence score for each source paper can be estimated as follows:\n\n```json\n{\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b5\": 0.8,\n    \"b10\": 0.8,\n    \"b18\": 0.7,\n    \"b23\": 0.7,\n    \"b33\": 0.6\n}\n```\n\nThese scores are estimated based on the frequency of the corresponding key phrases in the text of the paper, the relevance of the paper's topic to the source paper's topic, and the emphasis placed on the source paper in the text.", "Based on the provided paper text, the most important source papers appear to be the ones that have directly influenced the hardware memory compression solution proposed by the authors. \n\nHere are the papers that seem to be the most pertinent:\n\n1. \"pinnacle ibm mxt in a memory controller chip\" [b3]: This paper seems to be a pioneering work in hardware memory compression which the authors have built upon.\n\n2. \"compresso pragmatic main memory compression\" [b5]: This paper is compared with the proposed method and is likely to be a significant source of inspiration for the authors.\n\n3. \"data compression accelerator on ibm power and z processors\" [b10]: This paper is referred to multiple times, and the authors appear to have used the ASIC Deflate design mentioned in this paper as a base for their method.\n\n4. \"every walk s a hit making page walks single access cache hits\" [b23]: The authors refer to this paper when introducing an improvement in their method. It seems to be a significant reference for the development of their technique.\n\n5. \"elastic cuckoo page tables rethinking virtual memory translation for parallelism\" [b24]: This paper seems to have influenced the authors' approach to memory management.\n\n6. \"zram compressed ram based block devices\" [b35]: This paper is cited in the context of OS compression, which the authors have taken inspiration from to improve their hardware compression method.\n\n7. \"base delta immediate compression practical data compression for on chip caches\" [b51]: This paper is likely to have influenced the authors' approach to data compression in their method.\n\nBased on the information provided, the confidence scores for each source paper couldn't be calculated. However, the papers listed above appear to be the most relevant ones. \n\nThe final results would look like this in JSON format: \n{\n  \"b3\": \"pinnacle ibm mxt in a memory controller chip\",\n  \"b5\": \"compresso pragmatic main memory compression\",\n  \"b10\": \"data compression accelerator on ibm power and z processors\",\n  \"b23\": \"every walk s a hit making page walks single access cache hits\",\n  \"b24\": \"elastic cuckoo page tables rethinking virtual memory translation for parallelism\",\n  \"b35\": \"zram compressed ram based block devices\",\n  \"b51\": \"base delta immediate compression practical data compression for on chip caches\"\n} \n\nPlease note that the confidence scores could not be calculated due to lack of sufficient data."], "64389992d6db87a146dd25d0": ["{\n  \"b23\": 0.9,\n  \"b34\": 0.9,\n  \"b36\": 0.9,\n  \"b2\": 0.8,\n  \"b6\": 0.75,\n  \"b5\": 0.75,\n  \"b7\": 0.75,\n  \"b10\": 0.75\n}", "```json\n{\n  \"b23\": 0.9,\n  \"b35\": 0.85,\n  \"b36\": 0.85,\n  \"b34\": 0.8\n}\n```", "{\n  \"b23\": 0.9,\n  \"b34\": 0.8,\n  \"b2\": 0.7,\n  \"b36\": 0.9,\n  \"b6\": 0.7,\n  \"b7\": 0.7,\n  \"b14\": 0.7,\n  \"b15\": 0.7\n}", "```json\n{\n  \"b23\": 0.9,\n  \"b35\": 0.85,\n  \"b36\": 0.8,\n  \"b2\": 0.75,\n  \"b34\": 0.7\n}\n```", "```json\n{\n  \"b1\": 0.95,\n  \"b5\": 0.90,\n  \"b6\": 0.95,\n  \"b7\": 0.95,\n  \"b9\": 0.90,\n  \"b10\": 0.95,\n  \"b14\": 0.95,\n  \"b15\": 0.95\n}\n```"], "6482a38fd68f896efa8db695": ["Based on the text of the paper and the titles of the references, the most pertinent source papers appear to be the following:\n\n- b5: \"Editing factual knowledge in language models\"\n- b18: \"Transformer feed-forward layers are key-value memories\"\n- b26: \"Parameter-efficient transfer learning for NLP\"\n- b48: \"Locating and editing factual associations in GPT\"\n- b53: \"MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer\"\n- b78: \"Taming sparsely activated Transformer with stochastic experts\"\n\nThese papers are highlighted due to their relevance to the MixDA algorithm proposed by the author, based on the key phrases \"inspired by\" and \"we propose\", and their strong methodological similarity to the paper in question. The specific contributions from these papers to the method include the idea of decoupling the feed-forward networks (FFNs) into two parts, the concept of parameter-efficient fine-tuning, and the method of injecting domain knowledge into FFNs with minimal costs.\n\n{\n\"b5\": 0.9,\n\"b18\": 0.9,\n\"b26\": 0.85,\n\"b48\": 0.85,\n\"b53\": 0.8,\n\"b78\": 0.8\n}", "Based on the task details and the provided paper text, the source papers that inspired the given paper are:\n\n1. \"Transformer feed-forward layers are key-value memories\" [b18]\n2. \"Editing factual knowledge in language models\" [b5]\n3. \"Locating and editing factual associations in GPT\" [b48]\n4. \"Don't stop pretraining: Adapt language models to domains and tasks\" [b21]\n5. \"Parameter-efficient transfer learning for NLP\" [b26]\n6. \"AdapterFusion: Non-destructive task composition for transfer learning\" [b51]\n7. \"DEMIX: Layers: Disentangling Domains for Modular Language Modeling\" [b20]\n8. \"K-Adapter: Infusing knowledge into pre-trained models with adapters\" [b78]\n\nThese papers are inferred from the provided text based on the explicit mentions and the context in which the key phrases are used. The confidence score for each can be further determined by examining the degree of influence each paper has on the content of the given paper.\n\nBased on this criteria, we have:\n```json\n{\"b18\": 0.9, \"b5\": 0.8, \"b48\": 0.85, \"b21\": 0.7, \"b26\": 0.8, \"b51\": 0.7, \"b20\": 0.75, \"b78\": 0.8}\n```\n", "Based on the paper's text, the most probable source papers that the authors take inspiration from are:\n\n1. \"Transformer feed-forward layers are key-value memories\" [b18], this paper is mentioned multiple times in the text. The authors are inspired by the idea that knowledge is stored in feed-forward networks (FFNs), which they use in their method.\n2. \"Don't stop pretraining: Adapt language models to domains and tasks\" [b21], this paper is referred to when discussing the methodology for domain adaptation in pre-trained language models.\n3. \"Parameter-efficient transfer learning for NLP\" [b26], the authors mention this paper when discussing parameter-efficient fine-tuning (PEFT) methods, which are used in their proposed model.\n4. \"Mixture-of-Experts (MoE)\" [b57], this paper is referenced when discussing the mixture-of-adapters method used in their model.\n5. \"AdapterFusion: non-destructive task composition for transfer learning\" [b51], this is mentioned as an inspiration for the mixture of adapters in the authors' method.\n6. \"DEMIX: Disentangling Domains for Modular Language Modeling\" [b20], it is another paper related to the domain adaptation that the authors mention as a comparative work.\n7. \"K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters\" [b78], this is used to compare their method in terms of training on unstructured knowledge.\n\nThe JSON format of the source papers with confidence scores is:\n\n{\n\"b18\": 1,\n\"b21\": 0.9,\n\"b26\": 0.9,\n\"b57\": 0.8,\n\"b51\": 0.8,\n\"b20\": 0.7,\n\"b78\": 0.7\n}", "Based on the text and titles of the references in the paper, the most pertinent source papers are as follows:\n\n1. \"Transformer feed forward layers are key value memories\" [b18]: The authors mention being inspired by this study and it seems to be a key reference for the development of their domain-specific adapters. Confidence Score: 1.0\n\n2. \"Editing factual knowledge in language models\" [b5]: This paper also inspired the decoupling of the FFNs in the Transformer architecture. Confidence Score: 0.9\n\n3. \"Locating and editing factual associations in gpt\" [b48]: Another study that informed the authors' approach to decoupling FFNs and injecting domain-specific knowledge. Confidence Score: 0.9\n\n4. \"Don't stop pretraining: adapt language models to domains and tasks\" [b21]: The authors mention this paper's methods of continued pre-training and domain adaptation as influential. Confidence Score: 0.8\n\n5. \"Parameter efficient transfer learning for nlp\" [b26]: The authors refer to this paper's proposed parameter-efficient fine-tuning (PEFT) methods. Confidence Score: 0.8\n\n6. \"Benchmarking zero shot text classification models: datasets, evaluation, and entailment approach\" [b17]: This paper appears to have influenced the authors' consideration of prompt-based tuning. Confidence Score: 0.7\n\n7. \"A towards a unified view of parameter efficient transfer learning\" [b24]: This paper provides a unified view of parameter-efficient transfer learning, which is highly relevant to the paper's method. Confidence Score: 0.7\n\n8. \"AdaMix: Mixture of adapters for parameter efficient model tuning\" [b67]: This paper introduced the mixture of adapters, an approach that the authors also use. Confidence Score: 0.6\n\nThe JSON format of this result is:\n{\n  \"b18\": 1.0,\n  \"b5\": 0.9,\n  \"b48\": 0.9,\n  \"b21\": 0.8,\n  \"b26\": 0.8,\n  \"b17\": 0.7,\n  \"b24\": 0.7,\n  \"b67\": 0.6\n}", "Based on the given paper text and titles of the references, the source papers that most inspire the main contributions of this paper are likely the following:\n\n1. \"Transformer Feed-Forward Layers Are Key-Value Memories [b18]\" - The authors mention this paper while explaining their approach of decoupling the FFNs into two parts. They also used the findings of the paper to store the domain-specific knowledge in the feed-forward networks in PLMs. Confidence score: 0.9\n2. \"Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [b21]\" - This paper is mentioned as a method for domain adaptation, which is a key challenge addressed in the given paper. The authors have used this method to mitigate the costs of fine-tuning and continued pre-training methods. Confidence score: 0.8\n3. \"Parameter-Efficient Transfer Learning for NLP [b26]\" - This paper is referred to in the context of parameter-efficient fine-tuning (PEFT) methods, which is one of the approaches used in the given paper to adapt pre-trained language models for different domain-specific tasks. Confidence score: 0.7\n4. \"DEMIX Layers: Disentangling Domains for Modular Language Modeling [b20]\" - This paper is referred in the context of training adapters that specialize in domains and use mechanisms to route different adapters. It shares methodological similarity with the given paper. Confidence score: 0.7\n5. \"Mad-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer [b53]\" - This paper is also mentioned in the context of parameter-efficient fine-tuning (PEFT) methods and adapter-based tuning which is utilized in the given paper. Confidence score: 0.7\n\nOutput in JSON format:\n```\n{\n\"b18\": 0.9,\n\"b21\": 0.8,\n\"b26\": 0.7,\n\"b20\": 0.7,\n\"b53\": 0.7\n}\n```"], "640e949e90e50fcafd114d4b": ["{\n    \"b35\": 0.95,\n    \"b9\": 0.85,\n    \"b47\": 0.85,\n    \"b25\": 0.80\n}", "{\n  \"b35\": 0.9,\n  \"b9\": 0.8,\n  \"b47\": 0.8,\n  \"b25\": 0.7\n}", "{\n  \"b35\": 0.9,\n  \"b9\": 0.8,\n  \"b47\": 0.8,\n  \"b25\": 0.7\n}", "{\n    \"b35\": 0.95,\n    \"b9\": 0.85,\n    \"b47\": 0.85,\n    \"b25\": 0.75\n}", "{\n  \"b35\": 0.9,\n  \"b9\": 0.8,\n  \"b47\": 0.8\n}"], "63dcdb422c26941cf00b6136": ["Based on the given text and the titles of the references, the source papers that seem to have most influenced the given paper are:\n\n1. \"The Lottery Ticket Hypothesis: Finding Sparse Trainable Neural Networks\" [b16]: The paper frequently refers to this work and the algorithm it proposes. The phrases \"the lottery ticket hypothesis\" and \"LTH\" appear multiple times throughout the text. It appears that the author has used this algorithm as a basis for their own work and has made considerable comparisons and evaluations with it.\n\n2. \"The State of Sparsity in Deep Neural Networks\" [b20]: This paper is also referenced multiple times, and it seems to have been used as a basis for the author's study of the effectiveness of sparse algorithms.\n\n3. \"Rigging the Lottery: Making All Tickets Winners\" [b14]: The author mentions this paper when discussing different approaches to sparsity. \n\n4. \"SNIP: Single-shot Network Pruning Based on Connection Sensitivity\" [b37]: The author refers to this work when discussing different methods of sparsity and seems to be directly comparing their own results with this method.\n\n5. \"The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models\" [b31]: This paper is directly mentioned as a source of inspiration for the author's work and seems to have been used as a reference for developing the author's method.\n\n6. \"The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training\" [b44]: The author refers to this work when discussing the results and comparisons of their method.\n\nTherefore, the returned result is: {\"b16\": 1, \"b20\": 0.9, \"b14\": 0.8, \"b37\": 0.8, \"b31\": 1, \"b44\": 0.8}.", "Based on the text and the titles of the references, the most pertinent source papers that inspired the given paper are likely:\n\n1. \"The Lottery Ticket Hypothesis: Finding Sparse Trainable Neural Networks\" [b16]: The paper frequently references this work, especially when discussing pruning techniques and SNNs.\n\n2. \"The State of Sparsity in Deep Neural Networks\" [b20]: This paper is also frequently cited, particularly when discussing the evaluation of SNNs and pruning methods.\n\n3. \"Rigging the Lottery: Making All Tickets Winners\" [b14]: This paper is mentioned when the authors discuss the utilization of dynamic sparsity during training, suggesting it has influenced their approach.\n\n4. \"SNIP: Single-shot Network Pruning Based on Connection Sensitivity\" [b37]: The paper references this work when discussing prior-training pruning techniques, indicating it has influenced their work.\n\n5. \"The Optimal BERT Surgeon: Scalable and Accurate Second-order Pruning for Large Language Models\" [b31]: This paper is directly mentioned as a source of inspiration for second-order pruning approaches used in the paper.\n\n6. \"The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training\" [b44]: This paper is discussed when evaluating the performance of various pruning methods, indicating its influence on the authors' work.\n\nThese papers seem to have the strongest link to the novel contributions of the given paper. Here is the result in the JSON format:\n\n{ \"b16\": 0.9, \"b20\": 0.85, \"b14\": 0.8, \"b37\": 0.75, \"b31\": 0.85, \"b44\": 0.7 }", "Based on the text and titles of the references, the most relevant source papers appear to be:\n\n1. \"The Lottery Ticket Hypothesis for Pre-trained BERT Networks\" [b4]: The text mentions the Lottery Ticket Hypothesis (LTH) multiple times, indicating that it was a significant influence on the paper. The authors also discuss their attempts to apply LTH to their work and the results of these attempts.\n\n2. \"The State of Sparsity in Deep Neural Networks\" [b20]: This paper is cited multiple times, and its topic is directly related to the main contribution of the analyzed paper, which is about sparse neural networks.\n\n3. \"On Lazy Training in Differentiable Programming\" [b7]: The authors mention that they were inspired by this finding while conducting their research, suggesting that this paper was a significant source of inspiration.\n\n4. \"Rigging the Lottery: Making All Tickets Winners\" [b14]: This paper is mentioned in the context of its use in the main experiment of the paper, indicating that its methodology was influential.\n\n5. \"SNIP: Single-Shot Network Pruning Based on Connection Sensitivity\" [b37]: This paper is mentioned in the context of its use in the main experiment of the paper, indicating that its methodology was influential.\n\n6. \"The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models\" [b31]: The authors mention that this pruning approach is more reliable for their benchmark, indicating that this paper was a significant source of inspiration.\n\nThe confidence scores for these papers are subjective and may vary depending on the specific context and criteria used for evaluation. But as a rough estimate, we can assign the following scores:\n\n{\"b4\": 0.9, \"b20\": 0.8, \"b7\": 0.8, \"b14\": 0.8, \"b37\": 0.7, \"b31\": 0.7} \n\nThis means that we are relatively confident (70-90% on a scale of 0 to 1) that these papers were significant sources of inspiration for the analyzed paper.", "Based on the text analysis, the source papers that inspired the given academic paper the most can be identified as:\n\n1. The Lottery Ticket Hypothesis (LTH) [b16]\n2. Rigging the Lottery (RigL) [b14]\n3. SNIP [b37]\n4. Gradual Magnitude Pruning (GMP) [b93]\n\nThese papers are referenced multiple times throughout the text, and are directly tied to the paper's novel contributions and the challenges it addresses. The paper suggests that its main inspiration comes from previous works of the authors on Sparse Neural Networks (SNNs), especially those involving pruning and sparse training approaches. These references are also directly associated with the key phrases and methodological similarities identified in the task.\n\nThe JSON format of the result with confidence scores (assuming the confidence score is evenly distributed among these papers) would be:\n\n{\n    \"b16\": 0.25,\n    \"b14\": 0.25,\n    \"b37\": 0.25,\n    \"b93\": 0.25\n}", "Based on the given text and the author's note, the source papers that inspired the given paper are:\n\n1. \"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\" [b16] - the authors repeatedly reference this paper when discussing the main challenges and the algorithm proposed for the current paper. Confidence score: 0.8\n\n2. \"Rigging the Lottery: Making All Tickets Winners\" [b14] - the authors mention this paper when discussing the novel methods and approaches in the paper. Confidence score: 0.7\n\n3. \"The State of Sparsity in Deep Neural Networks\" [b20] - the authors mention this paper when discussing the novel methods and approaches in the paper. Confidence score: 0.7\n\n4. \"SNIP: Single-shot Network Pruning based on Connection Sensitivity\" [b37] - the authors mention this paper when discussing the novel methods and approaches in the paper. Confidence score: 0.6\n\n5. \"The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training\" [b44] - the authors mention this paper when discussing the novel methods and approaches in the paper. Confidence score: 0.6\n\n6. \"The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models\" [b31] - the authors mention this paper when discussing the findings and hypotheses of their study. Confidence score: 0.5\n\n7. \"Pruning Neural Networks at Initialization: Why are we Missing the Mark\" [b18] - the authors mention this paper when discussing the main challenges and the algorithm proposed for the current paper. Confidence score: 0.5\n\nThe JSON format of the answer is:\n\n{\n    \"b16\": 0.8,\n    \"b14\": 0.7,\n    \"b20\": 0.7,\n    \"b37\": 0.6,\n    \"b44\": 0.6,\n    \"b31\": 0.5,\n    \"b18\": 0.5\n}"], "6464b048d68f896efa35a285": ["Based on the text of the paper and the titles of the references, the most pertinent source papers are likely:\n\n1. \"Masked Autoencoders are Scalable Vision Learners\" [b26]\n2. \"Attention is All You Need\" [b60]\n3. \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" [b19]\n4. \"Generative Adversarial Networks\" [b24]\n5. \"BEGAN: Boundary Equilibrium Generative Adversarial Networks\" [b7]\n6. \"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\" [b54]\n\nThe confidence score for each paper is estimated as follows:\n- [b26]: 0.9\n- [b60]: 0.8\n- [b19]: 0.8\n- [b24]: 0.7\n- [b7]: 0.7\n- [b54]: 0.6\n\nThis is considering the frequency of their mention in the text and their relevance to the paper's topic, based on the titles and the context in which they are mentioned. The paper [b26] is most likely the primary source paper, as it is mentioned multiple times and is directly linked to the main topic and the method proposed in the paper. The others are also highly relevant to the paper's theme and proposed method, but their influence seems to be less direct or significant compared to [b26].\n\nThis is the result in json format:\n\n{\n    \"b26\": 0.9,\n    \"b60\": 0.8,\n    \"b19\": 0.8,\n    \"b24\": 0.7,\n    \"b7\": 0.7,\n    \"b54\": 0.6\n}", "Based on the provided text and the titles of the references, the source papers that have mostly influenced the main contributions of the paper could be:\n\n1. \"masked auto encoders meet generative adversarial networks and beyond\" [None], confidence score: 1.0. This paper is mentioned in the author's notes as the main inspiration for the new GAN-MAE framework. \n2. \"Masked Autoencoder (MAE) [b26]\" titled as 'masked autoencoders are scalable vision learners', confidence score: 0.9. The paper heavily references this work in its methodology and builds upon its concepts.\n3. \"GANs [b24]\" titled as 'generative adversarial networks', confidence score: 0.8. The GAN concept is fundamental to the proposed GAN-MAE framework.\n4. \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale [b19]\", confidence score: 0.7. The paper mentions the influence of Vision Transformer on the architecture design.\n5. \"began boundary equilibrium generative adversarial networks [b7]\", confidence score: 0.6. The paper refers to this work in its training scheme.\n6. \"b5 learning deep architectures for ai\" and \"b6 greedy layer wise training of deep networks\", confidence score: 0.5. These papers are mentioned under the topic of autoencoding, which is a basic concept in the paper.\n\nThe JSON format of the result would be:\n{\"None\": 1.0, \"b26\": 0.9, \"b24\": 0.8, \"b19\": 0.7, \"b7\": 0.6, \"b5\": 0.5, \"b6\": 0.5}", "Based on the text and the titles of the references given, the source papers that have most inspired this paper likely include:\n\n1. \"An Image is Worth x Words: Transformers for Image Recognition at Scale[b19]\" - The paper discusses a Vision Transformer architecture which is also used in the given paper.\n\n2. \"Masked Autoencoders Are Scalable Vision Learners[b26]\" - The given paper proposes a new method that integrates GAN idea into Masked Autoencoder (MAE), hence this MAE paper should be a strong source paper.\n\n3. \"Attention is All You Need[b60]\" - The given paper is based on the Transformer model, which was introduced in \"Attention is All You Need\".\n\n4. \"Generative Adversarial Networks[b24]\" - As the given paper proposes a Generative Adversarial Networks-based pre-training framework, this source paper is directly relevant.\n\n5. \"BEiT: BERT Pre-training of Image Transformers[b2]\" - The paper mentions \"BERT Pre-training of Image Transformers\" which seems aligned with the task of the given paper.\n\nThe confidence score for above references could be approximately estimated as 1 considering the high relevance to the research of the given paper.\n\nThe result in json format:\n\n{\n\"b19\": 1,\n\"b26\": 1,\n\"b60\": 1,\n\"b24\": 1,\n\"b2\": 1\n}", "Based on the given text and the analysis steps, the source papers for this academic paper seem to be as follows:\n\n1. Vision Transformer [b19]: The paper mentions that the Vision Transformer has become the standard architecture in computer vision and it has surpassed CNN feature extractors. This paper is very likely to be a source paper as it provides the base architecture used in the given paper.\n\n2. Self-supervised learning (SSL) algorithms [b11] [b13] [b25] [b27]: These papers are mentioned in the context of learning transferable representation from unlabeled data, which is a key aspect of the work presented in the paper.\n\n3. Masked image modeling (MIM) in SSL for vision transformers [b2] [b26]: These papers seem to be the primary references for the masked image modelling technique the authors utilize in their work.\n\n4. Masked Autoencoder (MAE) [b26]: This paper is a key source as the authors are proposing a new framework based on the Masked Autoencoder.\n\n5. Generative Adversarial Networks (GANs) [b24]: Since the authors propose a GAN-based framework, this paper, which introduces GANs, is a crucial source.\n\n6. GAN improvements [b7] [b50] [b72]: The given paper mentions several improvements on the original GAN which are likely to be influential in shaping the authors' own improvements on the GAN framework in their work.\n\nBased on the information provided, the confidence score for these papers would be high (around 0.9-1.0), as these papers are directly related to the main contributions of the given paper. However, it would be necessary to read the full text of the papers to confirm these inferences. \n\nThe JSON format of the result is: {\"b19\": 1.0, \"b11\": 0.9, \"b13\": 0.9, \"b25\": 0.9, \"b27\": 0.9, \"b2\": 0.9, \"b26\": 1.0, \"b24\": 1.0, \"b7\": 0.9, \"b50\": 0.9, \"b72\": 0.9}\n", "Based on the given text, the source papers which inspire the author most are:\n\n1. \"Masked Autoencoders Are Scalable Vision Learners\" [b26]: This paper is referred to multiple times throughout the text, and the novel algorithm proposed by the author is built upon the concepts and methods presented in this paper. The author specifically mentions building upon the masked autoencoder model introduced in this paper and makes comparisons to its performance.\n\n2. \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" [b19]: This paper is also mentioned several times, indicating that the author used the vision transformer model proposed in this paper as a key component of their algorithm.\n\n3. \"Generative Adversarial Networks\" [b24]: The author explicitly mentions using a Generative Adversarial Networks-based pre-training framework, which is the main contribution of this paper.\n\n4. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b35]: The author's paper is inspired by the masked language modeling technique used in this paper.\n\n5. \"Adam: A Method for Stochastic Optimization\" [b36]: The author used the AdamW optimizer for model training, which is a variant of the original Adam optimizer proposed in this paper.\n\n6. \"SGDR: Stochastic Gradient Descent with Warm Restarts\" [b49]: The author adopted the cosine schedule with warm-up for model training, which is proposed in this paper.\n\n7. \"Learning Deep Architectures for AI\" [b5]: This paper is mentioned in the \"Related Works\" section, and it provides the foundational concepts of autoencoding, which the author's algorithm heavily relies on.\n\nHere is the result in json format:\n\n```json\n{\n    \"b26\": 1,\n    \"b19\": 0.9,\n    \"b24\": 0.9,\n    \"b35\": 0.8,\n    \"b36\": 0.7,\n    \"b49\": 0.7,\n    \"b5\": 0.6\n}\n```\n\nThe confidence score is assigned based on the frequency of the paper's mention in the text and its relevance to the paper's novel contributions."], "63dcdb422c26941cf00b613b": ["{\n  \"b19\": 0.95,\n  \"b33\": 0.9,\n  \"b52\": 0.85\n}", "Based on the provided text from the academic paper and the reference titles, the key phrases and concepts within the paper were analyzed to identify the most relevant source papers that directly inspired or influenced its contributions. The novel contributions of the paper involve developing a new class of neural encoders, specifically the DIFFormer, which utilizes a diffusion process for data dependency uncovering and operates under energy constraints. \n\nThe primary references that seem to influence these contributions directly are:\n\n- [b19]: \"Learning Dynamical Systems from Data: A Simple Cross-Validation Perspective Part I: Parametric Kernel Flows\" - This reference is directly mentioned in relation to the diffusion-based learning approach the paper adopts.\n- [b33]: \"The Laplacian on a Riemannian Manifold: An Introduction to Analysis on Manifolds\" - This reference is cited in the context of the diffusion process inspired by heat diffusion on Riemannian manifolds.\n- [b24]: \"Semi-Supervised Classification with Graph Convolutional Networks\" - This paper has a methodological similarity in terms of employing graph-based techniques for semi-supervised learning, which aligns with the exploration of data dependencies in the discussed DIFFormer framework.\n- [b52]: \"Graph Neural Networks Inspired by Classical Iterative Algorithms\" - Cited in the context of using energy functions and diffusivity functions, which are core parts of the novel DIFFormer model proposed.\n- [b61]: \"Learning with Local and Global Consistency\" - This paper is mentioned as having inspired the robust version of the energy function used in the DIFFormer model.\n\nBased on the analysis and the context in which these references are mentioned, here is the JSON format result with confidence scores reflecting the direct relevance to the paper's contributions:\n\n```json\n{\n  \"b19\": 0.95,\n  \"b33\": 0.90,\n  \"b24\": 0.85,\n  \"b52\": 0.80,\n  \"b61\": 0.75\n}\n```\n\nThese references were prioritized because they are directly associated with the methodological advancements and theoretical bases mentioned in the paper for the development of DIFFormer.", "{\n    \"b19\": 0.95,\n    \"b33\": 0.9,\n    \"b52\": 0.85\n}", "{\n  \"b19\": 0.9,\n  \"b33\": 0.9,\n  \"b52\": 0.85\n}", "{\n    \"b33\": 0.9,\n    \"b19\": 0.85,\n    \"b52\": 0.8,\n    \"b24\": 0.75\n}"], "648000a9d68f896efaa12362": ["Based on the text and the author's notes, the source papers that most inspired the given paper are:\n\n1. \"Explaining and Adapting Graph Conditional Shift\" [None]: Confidence score = 0.95\n2. \"A Theory of Learning from Different Domains\" [b4]: Confidence score = 0.85\n3. \"Domain Adversarial Training of Neural Networks\" [b13]: Confidence score = 0.80\n4. \"Graph Convolution for Semisupervised Classification: Improved Linear Separability and Out of Distribution Generalization\" [b2]: Confidence score = 0.75\n5. \"Tree Mover's Distance: Bridging Graph Metrics and Stability of Graph Neural Networks\" [b10]: Confidence score = 0.70\n6. \"Semi-Supervised Classification with Graph Convolutional Networks\" [b18]: Confidence score = 0.65\n7. \"Shift Robust GNNs: Overcoming the Limitations of Localized Graph Training Data\" [b45]: Confidence score = 0.60\n8. \"Graph Domain Adaptation via Theory Grounded Spectral Regularization\" [b39]: Confidence score = 0.55\n\nThe confidence scores are assigned based on the frequency and the context of the references in the text, with higher scores indicating a stronger influence on the paper's main contributions.\n\nThe JSON format of the result is: \n{\n\"None\": 0.95,\n\"b4\": 0.85,\n\"b13\": 0.80,\n\"b2\": 0.75,\n\"b10\": 0.70,\n\"b18\": 0.65,\n\"b45\": 0.60,\n\"b39\": 0.55\n}", "{\"b4\": 0.8, \"b13\": 0.8, \"b18\": 0.9, \"b45\": 0.9, \"b33\": 0.9, \"b44\": 0.8, \"b10\": 0.7, \"b39\": 0.7}", "Based on the text and the given instructions, the source papers that most inspired the given paper can be identified as:\n\n1. b18 - Semi-supervised classification with graph convolutional networks  - The paper frequently refers to this work while explaining Graph Neural Networks (GNNs) and its role in node classification. The proposed algorithm in the paper is also inspired by this work.\n\n2. b45 - Shift robust gnns overcoming the limitations of localized graph training data - This paper is referred multiple times, especially when discussing the limitations of existing methods and the need for new approaches.\n\n3. b13 - Domain adversarial training of neural networks - This paper's methodology has been used as a comparison point and its limitations have motivated the development of the new method.\n\n4. b33 - Unsupervised domain adaptive graph convolutional networks - This is another paper that the authors frequently refer to, especially when discussing the limitations of existing GNNs under distribution shift. \n\n5. b10 - Tree mover\u2019s distance bridging graph metrics and stability of graph neural networks - This paper is mentioned when discussing theoretical analysis on the generalization bound of Graph Domain Adaptation (GDA) approaches. \n\n6. b44 - Beyond homophily in graph neural networks current limitations and effective designs - This paper is used to leverage their findings on graph heterophily and model architecture which the authors found as crucial for enhancing unsupervised domain adaptation on graphs.\n\nTherefore, the source papers are:\n{\n\"b18\": 0.9,\n\"b45\": 0.9,\n\"b13\": 0.8,\n\"b33\": 0.8,\n\"b10\": 0.7,\n\"b44\": 0.7\n}", "Based on the text, the paper is primarily about understanding the challenges of Graph Neural Networks (GNNs) in the face of data distribution shift, providing a theoretical analysis of this issue, and proposing a solution - a method for estimating and minimizing conditional shift on graphs for unsupervised domain adaptation. The proposed solution is called GCONDA, and it is based on understanding the effect of graph heterophily and model architecture on conditional shifts.\n\nThe source papers that have influenced this work are indicated by key phrases like \"inspired by\" or \"building upon our theoretical results\". Here are the identified source papers:\n\n1. \"Graph Neural Networks (GNNs)\" [b18] [b32] [b16] [b9]\n2. \"Unsupervised Domain Adaptation (UDA) and Domain Generalization (DG)\" [b4] [b13] [b0]\n3. \"node classification\" [b18]\n4. \"Domain Invariant Representation Learning (DIRL)\" [b13]\n5. \"graph conditional shift adaptation method, called GCONDA\" [b10] [b39] \n\nPlease note that the reference numbers [b18], [b32], [b16], [b9], [b4], [b13], [b0], [b10], [b39] are placeholders for the actual papers, which can be found in the list of references provided by the author. The confidence scores for these sources can't be provided as the text does not contain enough information to calculate them. The returned result would look like this in JSON:\n\n{\n  \"b18\": \"Graph Neural Networks (GNNs)\",\n  \"b32\": \"Graph Neural Networks (GNNs)\",\n  \"b16\": \"Graph Neural Networks (GNNs)\",\n  \"b9\": \"Graph Neural Networks (GNNs)\",\n  \"b4\": \"Unsupervised Domain Adaptation (UDA) and Domain Generalization (DG)\",\n  \"b13\": \"Unsupervised Domain Adaptation (UDA) and Domain Generalization (DG)\",\n  \"b0\": \"Unsupervised Domain Adaptation (UDA) and Domain Generalization (DG)\",\n  \"b10\": \"graph conditional shift adaptation method, called GCONDA\",\n  \"b39\": \"graph conditional shift adaptation method, called GCONDA\"\n}", "Based on the provided text and author notes, the source papers that inspired the given paper most are:\n\n1. \"Graph convolution for semisupervised classification improved linear separability and out of distribution generalization\" [b2]\n2. \"Graph domain adaptation a generative view\" [b8]\n3. \"A theory of learning from different domains\" [b4]\n4. \"Joint distribution optimal transportation for domain adaptation\" [b11]\n5. \"Conditional adversarial domain adaptation\" [b24]\n6. \"Is homophily a necessity for graph neural networks\" [b26]\n7. \"Shift robust gnns overcoming the limitations of localized graph training data\" [b45]\n8. \"Tree mover s distance bridging graph metrics and stability of graph neural networks\" [b10]\n\nThis is based on the text mentioning these papers in connection with key phrases indicating direct inspiration and their relevance to the development of the paper's core contributions. \n\nHere is the confidence score for each reference:\n{\"b2\": 0.85, \"b8\": 0.75, \"b4\": 0.8, \"b11\": 0.9, \"b24\": 0.7, \"b26\": 0.8, \"b45\": 0.9, \"b10\": 0.85} \n\nIt should be noted that these scores are subjective and based on my understanding of the text provided."], "6385788590e50fcafdf49aef": ["Based on the text and the titles of the references, the source papers that have inspired the author most are:\n\n- [b28]: \"Exploring the limits of transfer learning with a unified text to text transformer\" - The authors have built their model PUnifiedNER based on the T5 language model proposed in this paper. This reference appears multiple times indicating its significance in the development of the paper's core contributions. (Confidence score: 0.9)\n\n- [b41]: \"Chinese ner using lattice lstm\" - This paper has been mentioned multiple times and the authors have compared their work with the performance of this paper, showing its relevance. (Confidence score: 0.8)\n\n- [b13]: \"Unified named entity recognition as word-word relation classification\" - The authors have compared their work with this paper and explicitly discussed the differences between their approach and the approach used in this paper. (Confidence score: 0.8)\n\n- [b10]: \"The power of scale for parameter efficient prompt tuning\" - This paper is referred to in the context of discussing prompting methods, which is a key aspect of the authors' approach. (Confidence score: 0.7)\n\n- [b8]: \"UnifiedQA: crossing format boundaries with a single QA system\" - This paper is also mentioned in the context of discussing prompting methods, indicating its influence on the authors' approach. (Confidence score: 0.7)\n\n- [b12]: \"A span-based model for joint overlapped and discontinuous named entity recognition\" - This paper appears in the discussion of related work on deep learning approaches for NER, indicating that it may have also influenced the authors' work. (Confidence score: 0.6)\n\nPlease note that the confidence scores are indicative and not absolute measures of the relevance of the source papers.\n\nHere is the information in the requested json format:\n\n{\n  \"b28\": 0.9,\n  \"b41\": 0.8,\n  \"b13\": 0.8,\n  \"b10\": 0.7,\n  \"b8\": 0.7,\n  \"b12\": 0.6\n}", "Based on the text and the criteria provided, the following papers seem to be the main sources of inspiration for the given academic paper:\n\n1. \"Exploring the limits of transfer learning with a unified text to text transformer\" [b28]: The author mentions that their system, PUnifiedNER, is built upon the T5 language model proposed in this paper. They also refer to this paper when discussing the model architecture of their system. \n\n2. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b1]: The author mentions the rise of transformer-based, pre-trained language models like BERT leading to performance improvements in NER. \n\n3. \"Super Natural Instructions: Generalization via Declarative Instructions on Tasks\" [b34]: The author mentions that a large number of prompting methods have been proposed in the literature, inspired by this paper.\n\n4. \"Unified Named Entity Recognition as Word-Word Relation Classification\" [b13]: The author mentions this paper as a research thread attempting to unify different sub-tasks in NER into one framework.\n\n5. \"The Power of Scale for Parameter-Efficient Prompt Tuning\" [b10]: The author mentions this paper when discussing the effectiveness of applying language model adaptation to close the gap between T5 pre-training and downstream prompting-based seq2seq NER tasks.\n\nThis answer could be represented in the requested JSON format as:\n\n```json\n{\n    \"b28\": 0.9,\n    \"b1\": 0.8,\n    \"b34\": 0.7,\n    \"b13\": 0.7,\n    \"b10\": 0.7\n}\n```\n\nThe confidence score is assigned based on the level of direct reference and the significance of the contribution to the paper's main topic. For instance, \"b28\" is assigned the highest score due to its significant contribution to the core architecture of the proposed model in the paper.", "Based on the text provided, I identified the following source papers:\n\n1. \"Exploring the limits of transfer learning with a unified text-to-text transformer\" [b28]: The paper mentions that the Prompting-based Unified NER system (PUnifiedNER) is built upon the recently proposed T5 language model [b28].\n2. \"Language models are few shot learners\" [b0]: The paper discusses the use of pre-trained language models and GPT-3 [b0] in prompting methods.\n3. \"BERT: Pre-training of deep bidirectional transformers for language understanding\" [b1]: BERT [b1] is mentioned in relation to the use of pre-trained language models in prompting methods.\n4. \"Unified named entity recognition as word-word relation classification\" [b13]: The paper contrasts its approach with W2NER, which reframes NER as a word-word relation classification problem.\n5. \"The power of scale for parameter efficient prompt tuning\" [b10]: The paper mentions that a large number of prompting methods are proposed to reformulate downstream tasks into pre-training ones to further leverage pre-trained language models [b10].\n6. \"The essence of prompt learning is making better use of pre-trained language model by adding additional \"hints\"\" [b12]: This statement suggests that the paper [b12] was instrumental in the development of PUnifiedNER.\n7. \"A lexicon-based graph neural network for Chinese NER\" [b5]: The paper's main focus is on Chinese NER, and this reference might have provided valuable insights into this problem.\n\nThe source papers are returned in the JSON format as follows:\n\n{\n\"b28\": 1,\n\"b0\": 0.9,\n\"b1\": 0.9,\n\"b13\": 0.9,\n\"b10\": 0.8,\n\"b12\": 0.8,\n\"b5\": 0.7\n}", "Based on the provided text of the paper and the author's note, the most relevant source papers that inspired the given paper appear to be:\n\n1. \"exploring the limits of transfer learning with a unified text to text transformer\" [b28]: The paper heavily relies on the T5 model proposed in this paper and the authors explicitly mention using it as the base for their model. The authors also mention using the strategy of prefix language modelling as discussed in [b28].\n\n2. \"unified named entity recognition as wordword relation classification\" [b13]: The authors discuss the differences between their approach and the W2NER framework proposed in this paper. \n\n3. \"a span based model for joint overlapped and discontinuous named entity recognition\" [b12]: While discussing the related work in NER, the authors mention this paper and its approach to handle complex entity recognition tasks.\n\n4. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b1]: Authors mention this paper when discussing the rise of transformer-based models and their impact on NER.\n\n5. \"The Power of Scale for Parameter-Efficient Prompt Tuning\" [b10]: The authors talk about prompt learning, and how it is used in their work, which is inspired by this paper.\n\n6. \"a pre train prompt and predict a systematic survey of prompting methods in natural language processing\" [b17]: The authors mention this paper when discussing the essence of prompt learning.\n\n7. \"language models are few shot learners\" [b0]: The authors mention this paper in the context of the use of prompts in GPT-3.\n\n8. \"unifiedqa crossing format boundaries with a single qa system\" [b8]: This paper is mentioned while discussing the use of prompts in T5 model.\n\nBased on these observations, the source papers are:\n\n```json\n{\n\"b28\": 0.9,\n\"b13\": 0.8,\n\"b12\": 0.7,\n\"b1\": 0.75,\n\"b10\": 0.7,\n\"b17\": 0.7,\n\"b0\": 0.7,\n\"b8\": 0.7\n}\n```\nThe confidence scores are subjective and are based on the perceived influence of each paper on the given paper. A higher score indicates a stronger influence.", "Based on the given text, the paper presents a new model, PUnifiedNER, that handles multiple Named Entity Recognition (NER) tasks simultaneously. The new model uses the T5 language model and prompt learning to handle multiple datasets and entity types. The authors also use prompting-based seq2seq tasks and language model adaptation in their approach.\n\nThe key phrases and corresponding references that indicate the papers that inspired the authors are:\n\n1. \"Empowered by prompt learning, PUnifiedNER is built upon the recently proposed T5 language model [b28]\"\n2. \"Inspired by this, a large number of prompting methods are proposed in the literature to reformulate downstream tasks into pre-training ones to further leverage pretrained language models. [b12] [b34]\"\n3. \"Inspired by the success of using deep learning model in flat NER task, many work in the literature attempts to solve nested NER [b7] [b39] [b29] [b12]\"\n4. \"Therefore, inspired by the idea of continuously pre-training language models [b8] [b21] [b10] [b41], we adopt the prefix language modelling objective discussed by [b28]; [b10] to further adapt the pre-trained language model\"\n\nGiven the prominence of the T5 language model [b28] and the prompting methods [b12] [b34] in the paper's approach, these references stand out as the most influential. The other references also seem to have had a degree of influence in the development of the new model.\n\nThe resulting JSON format is:\n\n{\n    \"b28\": 0.9,\n    \"b12\": 0.8,\n    \"b34\": 0.8,\n    \"b7\": 0.75,\n    \"b39\": 0.75,\n    \"b29\": 0.75,\n    \"b8\": 0.7,\n    \"b21\": 0.7,\n    \"b10\": 0.7,\n    \"b41\": 0.7\n}"], "6391890790e50fcafd2b45f2": ["{\n  \"b2\": 0.9,\n  \"b29\": 0.8,\n  \"b44\": 0.85,\n  \"b76\": 0.9,\n  \"b78\": 0.9\n}", "{\n    \"b78\": 0.95,\n    \"b76\": 0.95,\n    \"b29\": 0.9,\n    \"b44\": 0.85,\n    \"b2\": 0.8\n}", "{\n  \"b78\": 0.95,\n  \"b76\": 0.9,\n  \"b29\": 0.85,\n  \"b44\": 0.8,\n  \"b75\": 0.75,\n  \"b57\": 0.7\n}", "{\n  \"b29\": 0.9,\n  \"b76\": 0.9,\n  \"b78\": 0.9,\n  \"b2\": 0.85\n}", "{\n    \"b78\": 0.95,\n    \"b76\": 0.9,\n    \"b29\": 0.85,\n    \"b44\": 0.8\n}"], "64893b17d68f896efa9826b7": ["{\"b23\": 1, \"b5\": 0.9, \"b9\": 0.8, \"b7\": 0.8, \"b15\": 0.8, \"b2\": 0.7, \"b13\": 0.6, \"b35\": 0.5}", "{\"b23\": 1, \"b5\": 1, \"b9\": 1, \"b13\": 0.8, \"b7\": 0.8, \"b15\": 0.8, \"b10\": 0.8, \"b11\": 0.8}", "{\"b23\": 0.9, \"b5\": 0.8, \"b9\": 0.8, \"b13\": 0.7, \"b15\": 0.7, \"b7\": 0.7}", "Based on the paper, the following are the source papers that inspired the author the most:\n\n1. \"WebGPT: Browser-Assisted Question Answering with Human Feedback\" [b23]\n2. \"GLM: General Language Model Pretraining with Autoregressive Blank Infilling\" [b5]\n3. \"Unsupervised Dense Information Retrieval with Contrastive Learning\" [b9]\n4. \"Reinforcement Learning from Human Feedback\" [b25]\n5. \"Contriever: Unsupervised Dense Information Retrieval with Contrastive Learning\" [b9]\n6. \"ELI: Long Form Question Answering\" [b6]\n\nThese papers are mentioned in the context of the development and improvement of WebGLM, the model proposed in the paper. The authors have used the concepts, methods or findings of these papers to overcome the limitations of existing models and improve the efficiency and cost-effectiveness of WebGLM.\n\nPlease note that the confidence score of each source paper is not provided in the paper, therefore it is not included in this response.\n\nHere is the data in JSON format:\n{\n  \"b23\": \"WebGPT: Browser-Assisted Question Answering with Human Feedback\",\n  \"b5\": \"GLM: General Language Model Pretraining with Autoregressive Blank Infilling\",\n  \"b9\": \"Unsupervised Dense Information Retrieval with Contrastive Learning\",\n  \"b25\": \"Reinforcement Learning from Human Feedback\",\n  \"b9_1\": \"Contriever: Unsupervised Dense Information Retrieval with Contrastive Learning\",\n  \"b6\": \"ELI: Long Form Question Answering\"\n}", "Based on the provided text, the paper's main challenges and contributions include the development of WebGLM, a web-enhanced QA system that is efficient, cost-effective, and human preference-aware. It outperforms similar systems such as WebGPT and Perplexity.ai. The system includes an LLM-augmented retriever, a bootstrapped generator, and a human preference-aware scorer. \n\nThe following source papers seem to have directly inspired or motivated this work:\n\n1. \"WebGPT: browser-assisted question answering with human feedback\" [b23] is mentioned multiple times, indicating its strong influence on this work. WebGLM is compared to WebGPT and claims to outperform it in terms of efficiency and cost-effectiveness.\n\n2. \"GLM: general language model pretraining with autoregressive blank infilling\" [b5] is mentioned as the base model for WebGLM, indicating its central role in the paper's contributions.\n\n3. \"Unsupervised dense information retrieval with contrastive learning\" [b9], known as Contriever, is used in WebGLM for efficient retrieval and has been improved upon in this work.\n\n4. \"Retrieval-augmented language model pre-training\" [b7] and \"Few-shot learning with retrieval-augmented language models\" [b11] are mentioned as representative works in the retrieval-augmentation field, which seems to be a key technique in WebGLM.\n\n5. \"Training language models to follow instructions with human feedback\" [b25] is mentioned in relation to the scorer component of WebGLM, indicating its influence on this aspect of the system.\n\n6. \"Language models are few-shot learners\" [b2] is the paper introducing GPT-3, which is a foundational work for large language models and is mentioned in the context of LLMs' potential to memorize knowledge.\n\nThe confidence scores are difficult to assign without more specific information regarding how heavily each paper influenced the work, but a rough estimate might be: \n\n{\"b23\": 0.9, \"b5\": 0.85, \"b9\": 0.8, \"b7\": 0.75, \"b11\": 0.75, \"b25\": 0.7, \"b2\": 0.65}\n"], "646aeca9d68f896efa05a572": ["{\n    \"b33\": 0.9,\n    \"b36\": 0.9,\n    \"b30\": 0.8,\n    \"b35\": 0.8,\n    \"b34\": 0.8\n}", "{\n  \"b33\": 0.9,\n  \"b36\": 0.8,\n  \"b30\": 0.75,\n  \"b35\": 0.75,\n  \"b34\": 0.75\n}", "{\n    \"b33\": 0.9,\n    \"b36\": 0.8,\n    \"b30\": 0.7,\n    \"b35\": 0.7,\n    \"b34\": 0.7\n}", "{\n  \"b33\": 0.9,\n  \"b36\": 0.8,\n  \"b30\": 0.75,\n  \"b35\": 0.75,\n  \"b34\": 0.75\n}", "{\n  \"b33\": 0.9,\n  \"b36\": 0.85,\n  \"b30\": 0.8,\n  \"b35\": 0.75,\n  \"b34\": 0.75\n}"], "64741c33d68f896efaa7b708": ["Based on the text and author notes, the source papers that seem to have inspired this work the most are:\n\n- \"Language models are few-shot learners\" [b8]: The paper discusses the proficiency of pre-trained language models in memorizing extensive corpora, which is a key aspect of the paper at hand.\n- \"Concrete problems in AI safety\" [b1]: This paper discusses the susceptibility of models to adversarial attacks, a challenge that the current paper aims to overcome.\n- \"Training a helpful and harmless assistant with reinforcement learning from human feedback\" [b5]: This work introduces reward modeling, a technique that this paper builds upon and improves.\n- \"The surprising creativity of digital evolution: a collection of anecdotes from the evolutionary computation and artificial life research communities\" [b26]: This work discusses reward gaming, a challenge this paper addresses.\n- \"Alignment of language agents\" [b21]: This paper discusses the challenge of ensuring AI systems align with human preferences and goals, which is a central theme in the paper at hand.\n- \"A Human blueprint for AI coexistence\" [b25]: The paper discusses the importance of AI demonstrating empathy and understanding, which the current paper aims to achieve.\n\nJSON format:\n\n{\n\"b8\": 0.9,\n\"b1\": 0.9,\n\"b5\": 1.0,\n\"b26\": 0.8,\n\"b21\": 1.0,\n\"b25\": 0.8\n}", "Based on the text and information provided, the source papers that have most likely inspired the given paper can be identified as follows:\n\n{ \"b12\": 0.8,\n  \"b21\": 0.75,\n  \"b24\": 0.85,\n  \"b27\": 0.9,\n  \"b33\": 0.7,\n  \"b46\": 0.85,\n  \"b52\": 0.8}\n\nThese papers seem to be most relevant as they deal with alignment of AI systems with human preferences and goals, social alignment, learning from human or AI system feedback, and the use of large language models for simulating human interactions. Also, these papers are cited in key sections of the text where the authors discuss their novel contributions and methods.", "Based on the text of the paper, the key challenges outlined are: ensuring the behavior of large language models aligns with human societal values, the models' poor generalization performance in unfamiliar environments, and their vulnerability to adversarial attacks. The proposed solution is to use a simulated human society (SANDBOX) where language models can interact and learn social norms. The authors introduced a novel alignment learning paradigm (Stable Alignment), which helps language models learn from simulated social interactions.\n\nFrom the text, the following references seem to have significantly inspired the work:\n\n- \"b12\": social alignment concept\n- \"b24\": language models being trained in social isolation\n- \"b27\" and \"b10\": reward modeling technique\n- \"b46\": Alpaca protocol for data construction.\n\nThese references are directly linked to the challenges and methods proposed in the paper. However, without the context of the full text and access to the references, it's not possible to accurately determine the source papers. \n\nHere is the inferred list of source papers in a JSON format:\n\n{\n\"b12\": 0.8,\n\"b24\": 0.8,\n\"b27\": 0.9,\n\"b10\": 0.9,\n\"b46\": 0.85\n}", "Based on the text of the paper and the titles of the reference papers, the source papers that most likely inspired this paper are:\n\n1. \"Alignment of language agents\" [b21]: This paper likely inspired the main topic of the current paper: aligning AI language models with human values and social norms. The current paper often discusses \"social alignment\", a term also used in this source paper. Confidence: 0.9\n\n2. \"Socially situated artificial intelligence enables learning from human interaction\" [b24]: This paper seems to have inspired the idea of training language models in a simulated social environment, which is a major part of the current paper's methodology. Confidence: 0.8\n\n3. \"Training a helpful and harmless assistant with reinforcement learning from human feedback\" [b5]: This paper probably provided inspiration for the use of reinforcement learning and human feedback in training language models, which is a key aspect of the current paper. Confidence: 0.8\n\n4. \"Deep reinforcement learning from human preferences\" [b10]: This paper likely influenced the use of reinforcement learning based on human preferences in the current paper. Confidence: 0.7\n\n5. \"Scalable agent alignment via reward modeling: a research direction\" [b27]: This paper seems to have inspired the use of reward modeling for aligning AI agents, which is discussed as a previous method in the current paper. Confidence: 0.7.\n\n6. \"LLaMA open and efficient foundation language models\" [b48]: The current paper uses an instruction-tuned Alpaca model for its experiments, which is a version of the LLaMA model. This suggests that this source paper might have provided the foundation for the current paper's experiments. Confidence: 0.8\n\n7. \"The effects of reward misspecification: mapping and mitigating misaligned models\" [b34]: The current paper discusses the issue of reward misspecification, which is the main topic of this source paper. This suggests that this source paper might have influenced the current paper's discussion of the limitations of previous methods. Confidence: 0.7\n\nReturning the result in the required format:\n\n{\n\"b21\": 0.9,\n\"b24\": 0.8,\n\"b5\": 0.8,\n\"b10\": 0.7,\n\"b27\": 0.7,\n\"b48\": 0.8,\n\"b34\": 0.7\n}", "Based on the text of the academic paper, the author's notes, and the titles of the references, the source papers that most likely inspired the current paper are:\n\n1. \"Alignment for advanced machine learning systems\" [b47]\n2. \"Training a helpful and harmless assistant with reinforcement learning from human feedback\" [b5]\n3. \"Deep reinforcement learning from human preferences\" [b10]\n4. \"Taxonomy of risks posed by language models\" [b51]\n5. \"Using large language models to simulate multiple humans and replicate human subject studies\" [b0]\n6. \"Socially situated artificial intelligence enables learning from human interaction\"[b24]\n7. \"Stable Alignment: Learning Alignment from Social Interactions Data Preparation\" [b14]\n8. \"Artificial intelligence values and alignment minds and machines\" [b12]\n\nAssuming a confidence score of 1 indicates the strongest influence on the paper, the confidence scores for each source paper could be:\n\n{\n\"b47\": 0.9,\n\"b5\": 0.9,\n\"b10\": 0.8,\n\"b51\": 0.8,\n\"b0\": 0.7,\n\"b24\": 0.7,\n\"b14\": 0.6,\n\"b12\": 0.6\n}\n\nThese papers are chosen based on their relevance to the challenges, methods, and novel contributions outlined in the current paper. The confidence scores are estimated based on the frequency and context in which these papers are referenced in the text."], "629b0af15aee126c0fbc9a00": ["{\"b1\": 0.8, \"b3\": 0.9, \"b4\": 0.9, \"b26\": 0.8, \"b27\": 0.75, \"b28\": 0.8, \"b29\": 0.7, \"b33\": 0.85, \"b37\": 0.95, \"b39\": 0.8, \"b41\": 0.7, \"b43\": 0.85, \"b44\": 0.85, \"b45\": 0.8, \"b47\": 0.9, \"b54\": 0.95, \"b59\": 0.8, \"b60\": 0.7, \"b61\": 0.7, \"b81\": 0.6, \"b82\": 0.8, \"b101\": 0.95, \"b113\": 0.65, \"b116\": 0.7}", "Based on the text and the titles of the references, the most relevant source papers that inspired the author are:\n\n{\"b37\": 1, \"b63\": 0.8, \"b64\": 0.8, \"b100\": 0.95, \"b33\": 0.7, \"b32\": 0.7, \"b39\": 0.6, \"b1\": 0.55, \"b2\": 0.5, \"b4\": 0.55} \n\nThe confidence score indicates how likely each paper influenced the main contributions of the given paper.", "Based on the information given in the paper, the source papers that most likely influenced the author can be identified as follows:\n\n- \"The MONDO Disease Ontology [b43]\" is repeatedly mentioned in the text and seems to be a core methodology for defining diseases in the paper's proposed algorithm, indicating a strong reliance on this source.\n- \"The Unified Medical Language System (UMLS) Knowledge Source [b45]\" is also frequently referenced, suggesting it as another key source paper.\n- \"The Human Phenotype Ontology [b44]\" is another work that might have had significant influence on the author, given its use in defining the 'effect/phenotype' node types in PrimeKG.\n- The paper \"The Comparative Toxicogenomics Database (CTD) [b80]\" is mentioned as the source of information about environmental exposures, indicating it as a key source.\n- The paper \"DrugBank [b79]\" is referenced multiple times as a source of pharmaceutical knowledge, indicating it as a significant source paper.\n\nTherefore, the results in the JSON format are:\n{\n  \"b43\": 1.0,\n  \"b45\": 1.0,\n  \"b44\": 0.8,\n  \"b80\": 0.8,\n  \"b79\": 0.8\n}\n\nNote: The confidence score is set to 0.8 for the last three papers, indicating a slightly lower confidence compared to the first two papers.", "Based on the mention of sources and the discussion around them, the following papers seem to have inspired the given paper:\n\n1. \"The Human Disease Network\" [b63]: Confidence Score - 0.8\n2. \"Human Symptoms-Disease Network\" [b64]: Confidence Score - 0.8\n3. \"Systematic Integration of Biomedical Knowledge Prioritizes Drugs for Repurposing\" [b37]: Confidence Score - 0.7\n4. \"An Integrative Knowledge Graph for Rare Diseases Derived from the Genetic and Rare Diseases Information Center (GARD)\" [b33]: Confidence Score - 0.7\n5. \"The MONDO Disease Ontology\" [b43]: Confidence Score - 0.7\n6. \"The Unified Medical Language System (UMLS) Integrating Biomedical Terminology\" [b45]: Confidence Score - 0.7\n7. \"The DisGeNET Knowledge Platform for Disease Genomics: Update\" [b77]: Confidence Score - 0.7\n8. \"The Reactome Pathway Knowledgebase\" [b94]: Confidence Score - 0.7\n\nJSON format:\n{\n    \"b63\": 0.8,\n    \"b64\": 0.8,\n    \"b37\": 0.7,\n    \"b33\": 0.7,\n    \"b43\": 0.7,\n    \"b45\": 0.7,\n    \"b77\": 0.7,\n    \"b94\": 0.7\n}", "Based on the text, the source papers that inspired the given paper the most are likely:\n- \"Systematic integration of biomedical knowledge prioritizes drugs for repurposing\" [b37]\n- \"Knowledge graph enabled cancer data analytics\" [b28]\n- \"Knowledge graph based recommendation framework identifies drivers of resistance in EGFR mutant non-small cell lung cancer\" [b2]\n- \"Building a PubMed knowledge graph\" [b30]\n- \"The Monarch Initiative in: an integrative data and analytic platform connecting phenotypes to genotypes across species\" [b43]\n- \"The human phenotype ontology in\" [b44]\n- \"The Comparative Toxicogenomics Database (CTD): update\" [b80]\n- \"The Reactome pathway knowledgebase\" [b94]\n\nIn json format:\n{\n\"b2\": 0.9,\n\"b28\": 0.8,\n\"b30\": 0.7,\n\"b37\": 1.0,\n\"b43\": 0.8,\n\"b44\": 0.8,\n\"b80\": 0.7,\n\"b94\": 0.7\n}\n\nThe confidence score is given on a scale of 0 to 1, with 1 being the highest confidence."], "63e1c14790e50fcafd2dd585": ["Based on the information and the text of the paper, the most pertinent source papers can be identified as:\n\n1. \"BabyAI a platform to study the sample efficiency of grounded language learning\" [b10] is mentioned multiple times and directly related to the research question and methodology of the paper. \n2. \"Language as a cognitive tool to imagine goals in curiosity-driven exploration\" [b12] is mentioned in the same context as the primary research question, and may have provided a key inspiration or methodological approach for the paper.\n3. \"Language models as zero shot planners extracting actionable knowledge for embodied agents\" [b23] is directly referenced in the discussion of the methodologies used in the paper.\n4. \"Training language models to follow instructions with human feedback\" [b36] is mentioned in the section about fine-tuning LLMs with RL, which is a key part of the paper's methodology.\n5. \"Proximal policy optimization algorithms\" [b43] is referenced in the description of the PPO fine-tuning method, which is central to the paper's methodology.\n6. \"Scienceworld is your agent smarter than a th grader\" [b49] is mentioned as part of the discussion on grounding LLMs with online RL and it seems to have inspired some aspects of the paper's methodology.\n7. \"A review of abstract concept learning in embodied agents and robots\" [b8] is referenced in the discussion of language grounding, which is a central theme of the paper.\n\nHere is the result in the requested json format:\n{\n    \"b10\": 1,\n    \"b12\": 0.9,\n    \"b23\": 0.9,\n    \"b36\": 0.9,\n    \"b43\": 0.8,\n    \"b49\": 0.8,\n    \"b8\": 0.7\n}", "Based on the provided text of the academic paper, the main challenges identified were the computational efficiency and generalization capabilities of the Large Language Models (LLMs) in interactive text environments. The authors propose an approach for functional grounding of LLMs using online reinforcement learning and Proximal Policy Optimization (PPO) to enhance the performance of the intelligent agent in reinforcement learning tasks.\n\nFollowing the guidelines provided, the following references appear to have directly influenced the main contributions of the paper:\n\n{ \n  \"b10\": 0.9, \n  \"b45\": 0.9, \n  \"b36\": 0.85, \n  \"b39\": 0.8, \n  \"b43\": 0.75,\n  \"b33\": 0.7,\n  \"b5\": 0.6,\n  \"b13\": 0.6\n}\n\nThe confidence scores are based on the direct relevance of the reference to the paper's novel contributions, methodological similarities, and the presence of comparative phrases indicating the reference's influence on the work.", "Based on the text provided, the following papers seem to have directly inspired the author's work:\n\n1. \"BabyAI: A platform to study the sample efficiency of grounded language learning\" [b10]\n2. \"Language models are few-shot learners\" [b7]\n3. \"BERT: Pre-training of deep bidirectional transformers for language understanding\" [b15]\n4. \"Flamingo: A visual language model for few-shot learning\" [b2]\n5. \"Training language models to follow instructions with human feedback\" [b36]\n6. \"Proximal policy optimization algorithms\" [b43]\n7. \"ScienceWorld: Is your agent smarter than a 5th grader?\" [b49]\n\nThe confidence scores for each of these papers could not be determined from the information provided. However, the explicit mentions of these papers in the context of the author's methodology and the similarity of their titles and content to the author's work suggest they were likely significant sources of inspiration.\n\nHere is the JSON format of the result:\n{ \"b10\" : \"0.8\", \"b7\" : \"0.7\", \"b15\" : \"0.6\", \"b2\" : \"0.8\", \"b36\" : \"0.8\", \"b43\" : \"0.7\", \"b49\" : \"0.6\"}", "Based on the text and the reference titles, it seems the paper primarily focuses on the use of Large Language Models (LLMs) for functional grounding in interactive textual environments, with a specific emphasis on using online Reinforcement Learning (RL) to improve task performance. The paper seems to be heavily influenced by previous research on LLMs, RL, and grounding methods. Here are the potential source papers:\n\n1. \"b15\": 'BERT: Pre-training of deep bidirectional transformers for language understanding'\n    - Confidence Score: 0.8\n2. \"b10\": 'BabyAI: A platform to study the sample efficiency of grounded language learning'\n    - Confidence Score: 0.7\n3. \"b43\": 'Proximal policy optimization algorithms'\n    - Confidence Score: 0.7\n4. \"b4\": 'Climbing towards NLU: On meaning, form, and understanding in the age of data'\n    - Confidence Score: 0.65\n5. \"b39\": 'Is reinforcement learning not for natural language processing? Benchmarks, baselines, and building blocks for natural language policy optimization'\n    - Confidence Score: 0.65\n6. \"b45\": 'Learning to summarize from human feedback'\n    - Confidence Score: 0.6\n7. \"b5\": 'Experience grounds language'\n    - Confidence Score: 0.6\n8. \"b33\": 'Dissociating language and thought in large language models: A cognitive perspective'\n    - Confidence Score: 0.55\n\nThese papers seem to be the most directly relevant to the novel contributions of the paper, including the use of LLMs for functional grounding via RL, the focus on sample efficiency, and the exploration of language grounding.", "Based on the provided text and the criteria you've outlined, the following references appear to be the most relevant source papers:\n\n1. \"grounding large language models in interactive environments with online reinforcement learning\" (No reference number given)\n\n2. \"can language models encode perceptual structure without grounding a case study in color\" [b0]\n\n3. \"flamingo a visual language model for few shot learning\" [b2]\n\n4. \"babyai a platform to study the sample efficiency of grounded language learning\" [b10]\n\n5. \"language models as zero shot planners extracting actionable knowledge for embodied agents\" [b23]\n\n6. \"training language models to follow instructions with human feedback\" [b36]\n\n7. \"proximal policy optimization algorithms\" [b43]\n\n8. \"scienceworld is your agent smarter than a th grader\" [b49]\n\nThe confidence score for each reference paper is difficult to determine without a specific method for calculating confidence. The presence of these papers in the text suggests they are relevant, but the exact degree of relevance (i.e., the confidence score) would require a more detailed analysis of the paper's content.\n\nPlease find the results in the json format:\n\n{\n    \"No reference number given\": 0.8,\n    \"b0\": 0.7,\n    \"b2\": 0.75,\n    \"b10\": 0.85,\n    \"b23\": 0.8,\n    \"b36\": 0.8,\n    \"b43\": 0.85,\n    \"b49\": 0.7\n}"], "64927546d68f896efa88a31b": ["{\n  \"b26\": 0.9,\n  \"b39\": 0.9,\n  \"b7\": 0.8,\n  \"b9\": 0.8,\n  \"b5\": 0.8,\n  \"b8\": 0.7\n}", "{\n    \"b26\": 0.8,\n    \"b39\": 0.8,\n    \"b7\": 0.9,\n    \"b9\": 0.9,\n    \"b8\": 0.85\n}", "{\n  \"b5\": 0.9,\n  \"b7\": 0.9,\n  \"b9\": 0.9,\n  \"b26\": 0.8,\n  \"b39\": 0.8,\n  \"b54\": 0.85\n}", "{\n    \"b5\": 0.9,\n    \"b7\": 0.9,\n    \"b9\": 0.9,\n    \"b26\": 0.8,\n    \"b39\": 0.8\n}", "{\n    \"b5\": 0.95,\n    \"b7\": 0.95,\n    \"b9\": 0.95,\n    \"b26\": 0.85,\n    \"b39\": 0.85\n}"], "6482a38ed68f896efa8db612": ["Based on the provided text, the most pertinent source papers that inspired the author of the given paper appear to be the following:\n\n1. \"High fidelity neural audio compression\" [b6]: This paper is referenced several times throughout the text, indicating that it has heavily influenced the development of the author's model, MUSICGEN. The author's use of an EnCodec audio tokenizer, which provides high fidelity reconstruction from a low frame rate discrete representation, is directly attributed to this source paper.\n\n2. \"Attention is all you need\" [b12]: The author's use of an autoregressive transformer-based decoder is directly attributed to this source paper, suggesting it has had a significant influence on the core algorithm of MUSICGEN.\n\n3. \"Generative music from text\" [b9]: This paper is referenced when discussing the representation of musical segments using multiple sequences of discrete tokens at different granularity and modeling them using a hierarchy of autoregressive models. It seems to have inspired the author's approach to modeling multiple parallel streams of acoustic tokens.\n\n4. \"Neural codec language models are zero shot text to speech synthesizers\" [b11]: This source paper is referenced in the context of tackling the problem of jointly modeling several parallel dependent streams in two stages and appears to have influenced the author's modeling framework for MUSICGEN.\n\n5. \"Large scale contrastive language audio pretraining with feature fusion and keyword to caption augmentation\" [b19]: This paper appears to have influenced the author's approach to text and melody-conditioned generation. The author experimented with joint text-audio representation, such as CLAP, which is discussed in this source paper.\n\n6. \"Open and efficient foundation language models\" [b4]: This paper is mentioned in the context of recent advancements in self-supervised audio representation learning and sequential modeling, indicating it may have influenced the author's approach to developing MUSICGEN.\n\nIn summary, the source papers that appear to have most significantly influenced the author's work are [b6], [b12], [b9], [b11], [b19], and [b4], in that order. \n\nPlease note that this analysis is based on the provided text and may not include all source papers that have influenced the author's work. \n\nFinal result in json format: {\"b6\": 0.9, \"b12\": 0.8, \"b9\": 0.7, \"b11\": 0.6, \"b19\": 0.5, \"b4\": 0.4}", "Based on the provided text, the source papers that inspired the given paper the most seem to be:\n\n- \"b6: High fidelity neural audio compression\"\n- \"b7: Text free prosody-aware generative spoken language modeling\"\n- \"b9: Generating music from text\"\n- \"b11: Neural codec language models are zero-shot text to speech synthesizers\"\n- \"b12: Attention is all you need\"\n- \"b14: Exploring the limits of transfer learning with a unified text to text transformer\"\n- \"b15: Scaling instruction fine-tuned language models\"\n\nThese papers are referenced multiple times in the text and when key methodologies are being described, indicating a strong reliance on their findings. The methods and techniques described in these papers are directly related to the novel contributions of the given paper. \n\nConverting this answer to JSON format would look like this:\n\n{\n  \"b6\": 0.9,\n  \"b7\": 0.8,\n  \"b9\": 0.9,\n  \"b11\": 0.8,\n  \"b12\": 0.8,\n  \"b14\": 0.7,\n  \"b15\": 0.7\n}\n\nThe confidence scores reflect the estimated influence of each source paper on the given paper, with a higher score indicating a stronger influence.", "Based on the text of the paper and the author's note, the key challenges outlined in the paper are generating high-quality music given a textual description and the new approaches are a general framework for modeling multiple parallel streams of acoustic tokens, unsupervised melody conditioning, and a novel modeling framework for codebook interleaving patterns. \n\nThe following references are identified as source papers that inspire the author most:\n\n1. \"b9\": 'generating music from text' - This paper is mentioned multiple times throughout the text, indicating that it has significantly influenced the author's work. \n\n2. \"b6\": 'high fidelity neural audio compression' - The author's model is over the quantized units from an EnCodec audio tokenizer, which this paper seems to introduce. \n\n3. \"b11\": 'neural codec language models are zero shot text to speech synthesizers' - The author's work is described as a generalization of previous studies, and this paper is mentioned as one of those previous studies. \n\n4. \"b12\": 'attention is all you need' - The author's model uses a transformer-based decoder, which likely refers to the transformer model introduced in this paper.\n\n5. \"b7\": 'text free prosody aware generative spoken language modeling' -  This paper is referenced as one of the previous studies that proposed modeling strategies.\n\nThe confidence score is assigned based on the frequency of these references in the paper and their relevance to the main contributions.\n\n{\n\"b9\": 0.9, \n\"b6\": 0.8, \n\"b11\": 0.7, \n\"b12\": 0.7, \n\"b7\": 0.6\n}", "Based on the text of the paper and the author's notes, the most pertinent source papers appear to be:\n\n{ \"b6\": 0.9, \"b12\": 0.9, \"b9\": 0.8, \"b11\": 0.8, \"b14\": 0.7, \"b13\": 0.7, \"b7\": 0.6, \"b8\": 0.6 }\n\nThe confidence scores are attributed based on the frequency and context of the references in the paper. References b6 and b12 are given a high confidence score as they are directly related to the core methods and approaches of the paper. References b9 and b11 are also given a high score due to their role in developing the paper's primary challenges and contributions. Other references such as b14, b13, b7, and b8 are also relevant to the paper's topic but are not as frequently mentioned or central to the paper's contributions.", "Based on the text, the paper introduces a new music generation model, MUSICGEN, which is conditioned on a text or melody representation and uses an autoregressive transformer-based decoder. The model relies heavily on the quantized units from an EnCodec audio tokenizer, which is linked to reference [b6] multiple times in the text. The paper also introduces a new modeling framework that generalizes to various codebook interleaving patterns, which seems to be a novel contribution of this paper. Several other works are cited in relation to this, including [b7], [b9], and [b11]. The text conditioning and melody conditioning methods are linked to [b14], [b15], and [b19]. Based on this, the source papers that most inspire this work could be:\n\n{\n\"b6\": 1,\n\"b7\": 0.9,\n\"b9\": 0.9,\n\"b11\": 0.9,\n\"b14\": 0.7,\n\"b15\": 0.7,\n\"b19\": 0.7\n}\n\nThe confidence scores are based on the frequency and context of the references in the text."], "6433f67f90e50fcafd6db326": ["{\n  \"b11\": 0.95\n}", "```json\n{\n  \"b11\": 0.95\n}\n```", "{\n  \"b11\": 1.0\n}", "{\n    \"b11\": 0.9\n}", "{\n  \"b11\": 1.0\n}"], "64a29612d68f896efa28bcf5": ["Based on the given text and author notes, the following are the source papers that inspired the paper most:\n\n{ \"b21\": 0.9, \"b57\": 0.9, \"b18\": 0.8, \"b3\": 0.8, \"b53\": 0.7, \"b32\": 0.6, \"b37\": 0.6, \"b31\": 0.5 }\n\nThe confidence scores are assigned based on the number of occurrences and emphasis given to each reference in the paper and author note. The papers with the higher confidence score have made the most significant contribution to the development of this paper. The papers b21 and b57 appear to be the most pertinent sources because of their contribution to the proposed algorithm and the implementation of TEA within the Berkeley Out-of-Order Machine (BOOM) core. References b18, b3, and b53 appear to be important sources as well, as they represent the state-of-the-art performance analysis approaches that this paper compares against. References b32, b37, and b31 seem to contribute to the experimental setup and performance monitoring, thereby influencing the methodology of the paper.", "Based on the provided information, it appears that the primary challenges outlined in the paper are related to improving hardware profiling mechanisms and breaking down the execution time impact of an instruction according to the architectural behavior that caused the instruction's latency. Several key phrases indicate inspiration from previous work and references that directly relate to the paper's novel contributions. For example, the paper discusses the state-of-the-art performance profiler TIP [b21] and other performance profilers [b2] [b3] [b17] [b18] [b20] [b29] [b37]. It also mentions Intel VTune [b30] and AMD ?Prof [b1] as existing performance analysis tools. Other referenced sources include Intel Alder Lake [b31], Berkeley Out-of-Order Machine (BOOM) core [b57], and the AMD IBS [b18] , Arm SPE [b3] , and IBM RIS [b28] approaches. However, the paper also makes clear that its methodology differs from these existing approaches. As such, the source papers that most inspired this paper are:\n\n{\n\"b21\": 0.9,\n\"b2\": 0.7,\n\"b3\": 0.7,\n\"b17\": 0.7,\n\"b18\": 0.7,\n\"b20\": 0.7,\n\"b29\": 0.7,\n\"b37\": 0.7,\n\"b30\": 0.9,\n\"b1\": 0.9,\n\"b31\": 0.6,\n\"b57\": 0.8,\n\"b18\": 0.8,\n\"b3\": 0.8,\n\"b28\": 0.8\n}\n\nThese confidence scores indicate a strong reliance on the research presented in these source papers for developing the paper's core contributions, with the highest scores given to references that are specifically highlighted as sources of direct inspiration or motivation.", "Based on the text of the paper and author's note, the source papers that most inspire the given paper are:\n\n1. \"TEA: Time-Proportional Event Analysis\" (None) Confidence Score: 0.9\n2. \"TIP: Time Proportional Instruction Profiling\" (b21) Confidence Score: 0.9\n3. \"Instruction Based Sampling: A new performance analysis technique for AMD family 17h processors\" (b18) Confidence Score: 0.7\n4. \"Performance Monitoring Event Reference\" (b31) Confidence Score: 0.6\n5. \"A performance counter architecture for computing accurate CPI components\" (b19) Confidence Score: 0.6\n6. \"FireSim: FPGA-Accelerated Cycle-Exact Scale-Out System Simulation in the Public Cloud\" (b32) Confidence Score: 0.6\n7. \"Performance Debugging in the Large via Mining Millions of Stack Traces\" (b24) Confidence Score: 0.5\n8. \"The RISC-V ISA Manual\" (b4) Confidence Score: 0.5\n\nNote: The confidence score is an estimation of how much the source paper influenced the given paper, with 1 being the highest influence and 0 being the least.", "Based on the text and the author's notes, the source papers that most likely inspired the given paper are:\n\n1. \"TIP: Time-Proportional Instruction Profiling\" (b21), Confidence score: 0.9 - The author directly mentions this paper as the prior work of the author and refers to it multiple times in the text, indicating it as a major inspiration, especially in the development of their algorithm.\n\n2. \"TEA: Time-Proportional Event Analysis\" (None), Confidence score: 0.8 - As it is the main paper being discussed and seems to be the novel method proposed by the author, this paper is probably most relevant to the paper's novel contributions. \n\n3. \"AMD IBS: Instruction-Based Sampling\" (b18), Confidence score: 0.7 - The author refers to this paper to highlight the differences in their approach, indicating that they have been inspired to improve upon the methods used in this paper.\n\n4. \"Arm SPE: Arm Architecture Reference Manual Supplement Statistical Profiling Extension\" (b3), Confidence score: 0.7 - Similar to AMD IBS, this paper is also referred to in comparison with the author's approach, suggesting that it was a source of inspiration.\n\n5. \"IBM RIS: Power Performance Monitor Unit User\u2019s Guide\" (b28), Confidence score: 0.7 - The author compares their approach with IBM RIS, indicating it as another source of inspiration.\n\n6. \"Performance Monitoring Event Reference\" (b31), Confidence score: 0.6 - The author mentions this paper in the context of discussing the technical details of their proposed algorithm, indicating that it has influenced their research.\n\n7. \"FireSim: FPGA-Accelerated Cycle-Exact Scale-Out System Simulation in the Public Cloud\" (b32), Confidence score: 0.6 - The author mentions using FireSim for their experimental setup, suggesting that it has influenced their methodology.\n\n8. \"SPEC CPU\" (b45), Confidence score: 0.6 - The author mentions using SPEC CPU benchmarks for their evaluation, indicating that it has influenced their experimental approach.\n\nPlease note that the confidence scores are estimated based on the frequency and context of mentions in the text. Further review of the source papers may be necessary to confirm their relevance. \n\nIn JSON format:\n\n{\n\"b21\": 0.9,\n\"None\": 0.8,\n\"b18\": 0.7,\n\"b3\": 0.7,\n\"b28\": 0.7,\n\"b31\": 0.6,\n\"b32\": 0.6,\n\"b45\": 0.6\n}", "Based on the text and challenges outlined in the paper, the following are the source papers that have inspired or influenced the author's work:\n\n1. \"TEA: Time-proportional Event Analysis\" [None] - Confidence Score: 0.9\n2. \"TIP: Time-proportional Instruction Profiling\" [b21] - Confidence Score: 0.8\n3. \"Performance Monitoring Event Reference\" [b31] - Confidence Score: 0.7\n4. \"A top-down method for performance analysis and counters architecture\" [b53] - Confidence Score: 0.65\n5. \"Instruction-based Sampling: A new performance analysis technique for AMD Family 17h processors\" [b18] - Confidence Score: 0.65\n6. \"FireSim: FPGA-accelerated cycle-exact scale-out system simulation in the public cloud\" [b32] - Confidence Score: 0.6\n7. \"Redefining the role of the CPU in the era of CPU-GPU integration\" [b5] - Confidence Score: 0.55\n8. \"Continuous profiling: Where have all the cycles gone?\" [b2] - Confidence Score: 0.5\n\nPlease note that the confidence scores are assigned based on the relevance of the source paper to the given paper's content and contributions, as well as the frequency and context of the references in the text.\n\nJSON format:\n{\n  \"None\": 0.9,\n  \"b21\": 0.8,\n  \"b31\": 0.7,\n  \"b53\": 0.65,\n  \"b18\": 0.65,\n  \"b32\": 0.6,\n  \"b5\": 0.55,\n  \"b2\": 0.5\n}"], "646c3addd68f896efa5d1805": ["{\n    \"b45\": 0.9,\n    \"b39\": 0.9,\n    \"b49\": 0.9,\n    \"b14\": 0.8,\n    \"b1\": 0.8,\n    \"b62\": 0.8,\n    \"b33\": 0.8\n}", "{\n    \"b14\": 0.9,\n    \"b1\": 0.85,\n    \"b33\": 0.85,\n    \"b62\": 0.85,\n    \"b45\": 0.8,\n    \"b39\": 0.8,\n    \"b49\": 0.8\n}", "{\n    \"b45\": 0.9,\n    \"b39\": 0.9,\n    \"b49\": 0.9,\n    \"b14\": 0.85,\n    \"b1\": 0.8,\n    \"b62\": 0.75,\n    \"b33\": 0.75\n}", "{\n  \"b14\": 0.95,\n  \"b1\": 0.85,\n  \"b33\": 0.8,\n  \"b62\": 0.75,\n  \"b45\": 0.9,\n  \"b39\": 0.9,\n  \"b49\": 0.9\n}", "{\n    \"b14\": 0.95,\n    \"b1\": 0.90,\n    \"b33\": 0.85,\n    \"b62\": 0.85,\n    \"b45\": 0.80,\n    \"b39\": 0.75,\n    \"b49\": 0.75\n}"], "635753d490e50fcafdddf4bd": ["Based on the text of the paper and the titles of the references, here are the most pertinent source papers that seem to have inspired the author:\n\n1. \"Soundstream an end to end neural audio codec\" [b74]: The paper explicitly mentions that their work is a continuation of the work done in this paper, focusing on audio signals. The paper also refers to the methods proposed by this paper multiple times throughout the text.\n\n2. \"Efficient neural audio synthesis\" [b33]: The paper refers to this paper when discussing the use of neural networks as trained transforms via an encoder-decoder mechanism. \n\n3. \"A spectral energy distance for parallel speech synthesis\" [b26]: This paper is referred to when the author discusses the reconstruction loss term in the frequency domain.\n\n4. \"Parallel wavegan a fast waveform generation model based on generative adversarial networks with multi resolution spectrogram\" [b72]: The paper refers to this paper when discussing the use of a multi-scale STFT-based discriminator.\n\n5. \"Neural discrete representation learning\" [b67]: This paper is referred to when the author discusses the use of Residual Vector Quantization to quantize the output of the encoder.\n\n6. \"Attention is all you need\" [b69]: This paper is referred to when discussing the use of a Transformer-based language model for entropy coding.\n\n7. \"Estimating or propagating gradients through stochastic neurons for conditional computation\" [b6]: This paper is referred to when discussing the use of a straight-through-estimator to compute the gradient of the encoder.\n\n8. \"End to end optimized image compression\" [b4]: This paper is referred to when discussing the challenges arising in lossy neural compression models.\n\nHere is the result in json format:\n\n```\n{\n    \"b74\": 1,\n    \"b33\": 0.9,\n    \"b26\": 0.8,\n    \"b72\": 0.7,\n    \"b67\": 0.9,\n    \"b69\": 0.8,\n    \"b6\": 0.7,\n    \"b4\": 0.8\n}\n``` \n\nPlease note that the confidence scores are subjective and are based on the number of times a paper is referenced and the context it is referenced in within the given text.", "Based on the text of the academic paper, the most pertinent source papers that inspired the authors are:\n\n1. \"Soundstream: an end to end neural audio codec\" [b74]: This paper is directly mentioned as the most relevant related work. The authors of the current paper have used the Residual Vector Quantization (RVQ) technique proposed in this paper.\n\n2. \"Neural discrete representation learning\" [b67]: This paper is referenced in the context of the Residual Vector Quantization (RVQ) technique, which is a key methodology in the current paper.\n\n3. \"Hifi-gan: generative adversarial networks for efficient and high fidelity speech synthesis\" [b37]: It's cited in the context of the multi-scale STFT discriminator, a technique used by the authors to improve the quality of generated samples.\n\n4. \"Common voice a massively multilingual speech corpus\" [b1]: The authors used this dataset for training the proposed model, which indicates that the methods and findings of this paper had a significant influence on the current work.\n\n5. \"Attention is all you need\" [b69]: The Transformer language model used in the current paper is a concept from this paper.\n\n6. \"Efficient neural audio synthesis\" [b33]: The authors of the current paper have used the same architecture for 24 kHz and 48 kHz audio, which indicates that they were influenced by the findings of this paper.\n\n7. \"Fast and accurate deep network learning by exponential linear units elus\" [b12]: The authors used ELU as a non-linear activation function, indicating that the findings of this paper influenced the proposed model's design.\n\nBased on the text and the titles of the source papers, these papers seem to be the most relevant references that directly influenced the main contributions of the current paper. Here are the references with confidence scores:\n\n{\n\"b74\": 0.9,\n\"b67\": 0.85,\n\"b37\": 0.85,\n\"b1\": 0.8,\n\"b69\": 0.8,\n\"b33\": 0.75,\n\"b12\": 0.75\n}", "Based on the text of the academic paper and the titles of the references, the most pertinent source papers that inspired this paper are:\n\n- [b74]: \"Soundstream an end to end neural audio codec\"\n- [b67]: \"Neural discrete representation learning\"\n- [b43]: \"Variable bitrate discrete neural representations via causal self attention\"\n- [b69]: \"Attention is all you need\"\n- [b58]: \"Learned video compression\"\n- [b49]: \"Speech coding based on a multi layer neural network\"\n- [b39]: \"Melgan generative adversarial networks for conditional waveform synthesis\"\n- [b11]: \"Global forecast highlights cisco\"\n\nThe json formatted output is:\n{\"b74\": 1, \"b67\": 0.9, \"b43\": 0.9, \"b69\": 0.8, \"b58\": 0.8, \"b49\": 0.7, \"b39\": 0.7, \"b11\": 0.6}", "Based on the information provided in the academic paper, the most pertinent source papers that inspired the author's work can be identified as:\n\n- 'b74': 'SoundStream: An End-to-End Neural Audio Codec': The paper frequently references this work, and it seems to be the most direct inspiration for the author's own codec model, EnCodec.\n\n- 'b67': 'Neural Discrete Representation Learning': This work is mentioned as the basis for the Residual Vector Quantization (RVQ) method that the author uses in their model.\n\n- 'b43': 'Variable bitrate discrete neural representations via causal self-attention': This paper is referenced in the context of neural audio codecs, suggesting it also provided inspiration for the author's work.\n\n- 'b39': 'MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis' and 'b37': 'HiFi-GAN: Generative Adversarial Networks for Efficient and High-Fidelity Speech Synthesis': Both these papers are cited in the context of the discriminator loss which is a key part of the author's model.\n\n- 'b16': 'Jukebox: A Generative Model for Music': The author refers to this work in the context of their training procedure for RVQ, indicating that they have taken inspiration from it.\n\n- 'b26': 'A spectral energy distance for parallel speech synthesis': This paper is cited in the context of the frequency domain loss in the author's model, suggesting it has also provided inspiration.\n\nIn JSON format, the source papers and their confidence scores (based on the frequency and context of their citation in the paper) would be:\n\n{\n\"b74\": 1,\n\"b67\": 0.9,\n\"b43\": 0.85,\n\"b39\": 0.8,\n\"b37\": 0.8,\n\"b16\": 0.75,\n\"b26\": 0.75\n}", "Based on the text, and the titles of the references, the source papers that inspired this work may include:\n\n1. \"SoundStream: An End-to-End Neural Audio Codec\" [b74]: The authors frequently refer to this work, and it seems to be the most relevant related work. They use the same Residual Vector Quantization (RVQ) as in this work. The methods proposed by [b74] inspired the authors in designing their model.\n2. \"Neural Discrete Representation Learning\" [b67]: The authors use the VQ-VAE model proposed by [b67] for their compression model.\n3. \"A Review of Vector Quantization Techniques\" [b68]: The authors may have used this work to develop their understanding of vector quantization, which is a key component in their model.\n4. \"Attention Is All You Need\" [b69]: The authors use a Transformer-based language model for entropy coding, which was originally proposed in [b69].\n5. \"Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks\" [b60]: The authors may have used weight normalization, as suggested by [b60], in their model.\n6. \"Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation\" [b6]: The authors use a straight-through-estimator to compute the gradient of the encoder, which is originally proposed by [b6].\n\nResult in json format:\n{\n    \"b74\": 1.0,\n    \"b67\": 0.8,\n    \"b68\": 0.8,\n    \"b69\": 0.8,\n    \"b60\": 0.7,\n    \"b6\": 0.7\n}"], "649a5e2ad68f896efad8460f": ["Based on the provided text and reference titles, the source papers that seem to have most significantly influenced the given paper are:\n\n{ \"b68\": 1,\n  \"b1\": 0.9,\n  \"b2\": 0.9,\n  \"b23\": 0.8,\n  \"b10\": 0.8,\n  \"b39\": 0.8,\n  \"b54\": 0.7,\n  \"b53\": 0.7 }\n\nExplanation:\n1. \"b68\" - 'Metadata-induced contrastive learning for zero-shot multi-label text classification in www': The paper directly refers to this work as a pioneering study and discusses its limitations, indicating that it was a significant influence.\n2. \"b1\" - 'SciBERT: A Pretrained Language Model for Scientific Text': SciBERT is used as the base model for the proposed method (FuTex), indicating a strong reliance on this work.\n3. \"b2\" - 'Longformer: The Long-Document Transformer': The paper directly discusses the results of adopting Longformer in their experiment, demonstrating its influence.\n4. \"b23\" - 'OAG-BERT: Towards a Unified Backbone Language Model for Academic Knowledge Services': OAG-BERT is compared with other methods, showing its relevance to the work.\n5. \"b10\" - 'FullMeSH: Improving Large-Scale MeSH Indexing with Full Text': This paper's method of exploiting the hierarchy structure of documents appears to be directly influenced by FullMeSH.\n6. \"b39\" - 'Parabel: Partitioned Label Trees for Extreme Classification with Application to Dynamic Search Advertising in www': Parabel is used in the self-training strategy of the proposed method.\n7. \"b54\" - 'LinkBERT: Pretraining Language Models with Document Links': The paper discusses the technique used in LinkBERT and how it has influenced the development of FuTex.\n8. \"b53\" - 'GraphFormers: GNN-Nested Transformers for Representation Learning on Textual Graph': GraphFormers is directly compared and contrasted with the proposed method, demonstrating its influence.", "Based on the text of the academic paper, the source papers that inspired the author most are likely to be:\n\n1. \"Weakly supervised multi-label classification of full-text scientific papers\" - The paper title itself indicates that it is closely related to the topic of the paper.\n   \n2. \"Meta metadata empowered weak supervision for text classification\" [b29] - The paper is referenced when discussing previous studies on weakly supervised text classification which the author is building upon.\n\n3. \"MICoL [b68] is a pioneering work that considers weakly supervised multi-label classification with more than 10,000 categories\" - Clearly, MICoL is a significant influence, based on the fact that it is described as a pioneering work.\n   \n4. \"Longformer: The Long-Document Transformer\" [b2] - The author adopted the Longformer model to classify papers, indicating a strong reliance on this paper.\n\n5. \"A simple framework for contrastive learning of visual representations\" [b6] - This paper is referenced in relation to the contrastive fine-tuning objective used in the paper.\n\n6. \"Parabel: Partitioned Label Trees for Extreme Classification with Application to Dynamic Search Advertising in WWW\" [b39] - This paper is used as a reference for the self-training strategy proposed by the author.\n\n7. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b11] - This paper is referenced when discussing the limitations of pretrained language models in handling long text, which the author is addressing in their paper.\n\nBased on the information provided, these source papers seem to be the most directly relevant to the novel contributions of the paper. However, without reading the full text of the paper, it is difficult to be certain. If any key phrases indicating direct inspiration or motivation exist in the text, the list may need to be adjusted.\n\nHere is the result in the required format:\n\n{\n    \"b29\": 0.9,\n    \"b68\": 1,\n    \"b2\": 0.9,\n    \"b6\": 0.8,\n    \"b39\": 0.8,\n    \"b11\": 0.7\n}", "Based on the provided text and titles of the references, the most pertinent source papers that inspired the given paper are likely the following:\n\n1. \"Scibert: A pretrained language model for scientific text\" [b1]: The paper cites this source multiple times, indicating it has been heavily used for language model pre-training in scientific documents.\n   \n2. \"Longformer: The long-document transformer\" [b2]: This source is directly referenced in the context of dealing with long text documents, which is a significant challenge outlined in the paper.\n   \n3. \"SPECTER: Document-level representation learning using citation-informed transformers\" [b8]: This source is also mentioned multiple times and is used as a base PLM for FuTex.\n\n4. \"Metadata-induced contrastive learning for zero-shot multi-label text classification in www\" [b68]: This source is directly referred to as a pioneering work in the field of weakly supervised multi-label classification.\n\n5. \"Weakly supervised multi-label classification of full-text scientific papers\" [None]: This source is the main topic of the paper.\n\n6. \"MICoL: Metadata Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification\" [b68]: It is mentioned multiple times and is directly related to the paper's novel contributions.\n\n7. \"Parabel: Partitioned Label Trees for Extreme Classification with Application to Dynamic Search Advertising in WWW\" [b39]: This paper is used in the self-training strategy of FuTex.\n\nIn JSON format, the result would be:\n\n{\n    \"b1\": 0.9,\n    \"b2\": 0.8,\n    \"b8\": 0.9,\n    \"b68\": 1.0,\n    \"None\": 1.0,\n    \"b39\": 0.8\n}\n\nThe confidence scores are assigned based on the frequency of the source being mentioned and its relevance to the paper's novel contributions.", "Based on the provided paper text, the following references seem to have directly influenced the paper's contributions:\n\n{\"b1\": 0.9, \"b2\": 0.8, \"b10\": 0.7, \"b11\": 0.7, \"b39\": 0.8, \"b54\": 0.7, \"b68\": 0.9}\n\nExplanation:\n\n- \"b1\" (SciBERT) is mentioned multiple times and is used as the base PLM for several methods, including the proposed FuTex framework.\n- \"b2\" (Longformer) is adopted to classify papers and the results are discussed in detail.\n- \"b10\" (FullMeSH) is used as inspiration for utilizing the in-paper hierarchy structure.\n- \"b11\" (BERT) is mentioned multiple times and is the basis for many other models discussed in the paper.\n- \"b39\" (Parabel) is used in the self-training strategy of FuTex.\n- \"b54\" (LinkBERT) is used as inspiration for the network-aware contrastive fine-tuning in FuTex.\n- \"b68\" (MICoL) is referred to as a pioneering work and is compared to FuTex in the experiments. \n\nThe confidence scores indicate how strongly these source papers seem to have influenced the given paper based on the frequency and context of their mentions in the text.", "Based on the text and the titles of the references, the source papers that most likely inspired the given paper are:\n\n1. \"scibert a pretrained language model for scientific text\" [b1]: The paper mentions the use of this model in the text and discusses its limitations, which leads to the development of the novel approach proposed in the paper.\n\n2. \"longformer the longdocument transformer\" [b2]: The paper adopts Longformer to classify papers and discusses its limitations in the context of their work, indicating it as a source of inspiration.\n\n3. \"weakly supervised multi label classification of full text scientific papers\" [b68]: This paper is directly mentioned as a pioneering work in the field and its limitations are discussed, making it a crucial source paper.\n\n4. \"specter document level representation learning using citationinformed transformers\" [b8]: It is mentioned in the comparison of methods section as a base PLM for several other methods including FuTex, indicating its influence.\n\n5. \"weaklysupervised text classification based on keyword graph\" [b62]: This paper is referred to multiple times in the context of existing methods for weakly supervised text classification, suggesting it as a source of inspiration.\n\n6. \"metadata induced contrastive learning for zero shot multi label text classification in www\" [b67]: This paper is referred to in the experiments section and seems to have influenced the given paper's experimental design.\n\n7. \"linkbert pretraining language models with document links\" [b54]: This model is used as a baseline for comparison and its method is discussed in the context of the paper's proposed approach, indicating its influence.\n\nThe JSON format of the results would be:\n\n{\n  \"b1\": 0.9,\n  \"b2\": 0.85,\n  \"b68\": 1.0,\n  \"b8\": 0.8,\n  \"b62\": 0.8,\n  \"b67\": 0.75,\n  \"b54\": 0.75\n}\n\nThe confidence scores are estimated based on the frequency and the context in which these references are mentioned in the paper. The scores are subjective and may vary with different interpretations of the text."], "6225978c5aee126c0f2d4a7c": ["Based on the text provided, the following references seem to be the most influential:\n\n1. Reference [b0]: The text makes various mentions of \"Loca\", a tool that is cited in this reference. The paper mentions the limitations of Loca, which seems to have motivated the creation of ReuseTracker. Confidence score: 0.9\n2. Reference [b41]: The paper discusses the flaws of the technique proposed in this reference, which seems to have driven the development of ReuseTracker. Confidence score: 0.8\n3. Reference [b64]: This reference is tied to \"Loca\" and is mentioned multiple times. It appears to have inspired the development of ReuseTracker. Confidence score: 0.8\n4. Reference [b60]: The paper discusses RDX, a tool cited in this reference. It seems to have influenced ReuseTracker as it aims to overcome its limitations. Confidence score: 0.8\n5. Reference [b7]: This reference is tied to prior PMU-based tools that profile reuse distance, providing a basis for creating a tool that can analyze multi-threaded codes. Confidence score: 0.7\n6. Reference [b37]: The paper refers to the concept of \"reuse distance\" defined in this reference, which is central to the paper's contributions. Confidence score: 0.7\n\nThe JSON format of the result:\n\n{\n  \"b0\": 0.9,\n  \"b41\": 0.8,\n  \"b64\": 0.8,\n  \"b60\": 0.8,\n  \"b7\": 0.7,\n  \"b37\": 0.7\n}", "Based on the text of the paper and the titles of references, the most pertinent source papers that have inspired this paper are:\n\n- [b41]: 'Scalable Analysis of Multicore Data Reuse and Sharing' by Dongyoon Lee and Benjamin Wester. The paper mentions the use of the techniques proposed in this reference and discusses the shortcomings of these techniques. \n\n- [b0] and [b64]: 'Dcompiler Loca Program Locality Analysis Tools' and 'Hotl A Higher Order Theory of Locality' because these papers discuss Loca, a reuse distance analysis tool which the paper mentions and critiques for its high runtime and memory overheads. \n\n- [b60]: 'Featherlight Reuse Distance Measurement' as it discusses RDX, another tool for measuring reuse distances of a single thread, which the paper mentions and critiques for its limitations. \n\n- [b7]: 'Fast Data Locality Profiling of Native Execution'. The paper mentions that prior PMU-based tools that profile reuse distance have focused only on reuses in a private cache of a core and single-threaded applications, indicating that this paper has inspired them to go beyond this limitation.\n\n- [b37]: 'Evaluation Techniques for Storage Hierarchies'. The paper uses the definition of reuse distance as defined in this paper indicating that this paper has inspired them in their work.\n\n- [b48]: 'Multicore Aware Reuse Distance Analysis'. The paper refers to this paper when discussing the overheads and accuracy of existing tools that perform online reuse detection.\n\nThe confidence score for each source paper is estimated as:\n\n- [b41]: 0.9\n- [b0]: 0.8\n- [b64]: 0.8\n- [b60]: 0.8\n- [b7]: 0.7\n- [b37]: 0.7\n- [b48]: 0.7\n\nPlease note that these confidence scores are estimated based on the frequency and context of their mentions in the text, and the actual influence of each source paper may vary.\n", "Based on the given text of the academic paper, these are the references that seem to have most inspired the paper:\n\n1. [\"Reuse distance as a metric for cache behavior\" (b8)](https://doi.org/10.1145/384197.384235): This paper is mentioned in the context of explaining reuse distance concept which is a key aspect of the given paper. The confidence score is 0.9.\n\n2. [\"Statcache a probabilistic approach to efficient and accurate data locality analysis\" (b6)](https://doi.org/10.1145/1403915.1403959): This paper appears in the context of discussing the limitations of previous approaches to measure data locality and the need for a more efficient tool. The confidence score is 0.8.\n\n3. [\"Fast data locality profiling of native execution\" (b7)](https://doi.org/10.1145/2464996.2467274): This paper is referenced while discussing previous PMU-based tools, indicating that it had an influence on the current work. The confidence score is 0.7.\n\n4. [\"Low-overhead reuse distance analysis algorithms that profile reuse distance in threads and shared caches for multi-threaded programs\" (b17)](https://dl.acm.org/doi/abs/10.1145/3178487.3178492): This paper is referenced in the context of explaining the novel algorithm the author introduced to tackle the challenges of reuse distance analysis for multi-threaded applications. The confidence score is 0.9.\n\n5. [\"A technique that avoids these flaws was proposed in Reference [b41]\" (b41)](https://doi.org/10.1145/3243734.3243838): This paper is mentioned as a direct inspiration for the current work, as it proposed a technique that the current paper builds upon to solve the outlined challenges. The confidence score is 1.\n\n6. [\"Loca, a reuse distance analysis tool\" (b0)](https://doi.org/10.1145/3178487.3178507): This paper is discussed in the context of existing tools for reuse distance analysis, which the current paper aims to improve upon. The confidence score is 0.9.\n\n7. [\"RDX\" (b60)](https://ieeexplore.ieee.org/abstract/document/7920916): This paper is also referenced while discussing existing tools for reuse distance analysis, which the current paper aims to improve upon. The confidence score is 0.9.\n\nSo the result in json format would be:\n```json\n{\n    \"b8\": 0.9,\n    \"b6\": 0.8,\n    \"b7\": 0.7,\n    \"b17\": 0.9,\n    \"b41\": 1,\n    \"b0\": 0.9,\n    \"b60\": 0.9\n}\n```", "Based on the text of the paper and the titles of the references, the most pertinent source papers that heavily influenced the main contributions of the paper appear to be:\n\n1. \"reuse distance as a metric for cache behavior\" [b8]: The paper heavily relies on the concept of reuse distance as a metric for data locality, which is a concept pioneered in this paper.\n\n2. \"accelerating multicore reuse distance analysis with sampling and parallelization\" [b47]: This paper likely influenced the development of the ReuseTracker tool, as it discusses similar themes of accelerating reuse distance analysis.\n\n3. \"hotl a higher order theory of locality\" [b64]: This paper likely influenced the theoretical basis of the paper's contributions, as it discusses a higher order theory of locality that could have been used to conceptualize the ReuseTracker tool.\n\n4. \"featherlight reuse distance measurement\" [b60]: This paper likely influenced the methods used for reuse distance measurement in the ReuseTracker tool.\n\n5. \"multicore aware reuse distance analysis\" [b48]: This paper likely influenced the multicore-aware aspects of the ReuseTracker tool.\n\n6. \"estimating cache misses and locality using stack distances\" [b10]: This paper likely provided important foundational concepts for the paper's contributions regarding cache misses and locality.\n\nIn JSON format, the answer could be represented as:\n```\n{\n    \"b8\": 0.9,\n    \"b47\": 0.9,\n    \"b64\": 0.8,\n    \"b60\": 0.8,\n    \"b48\": 0.8,\n    \"b10\": 0.7\n}\n```\nThis indicates that the papers [b8] and [b47] have the highest confidence scores, followed by [b64], [b60], [b48], and [b10]. The confidence scores range from 0.7 to 0.9, indicating a high degree of relevance to the main contributions of the paper.", "Based on the provided text and the instructions, the following references seem to be the most pertinent source papers:\n\n- \"Reuse distance as a metric for cache behavior\" [b8]: This work is often cited throughout the paper and seems to be the basis for the reuse distance concept employed by the authors.\n- \"Fast data locality profiling of native execution\" [b7]: This work is mentioned as a reference for PMU-based tools that profile reuse distance, which is a method also used by the authors.\n- \"A higher order theory of locality\" [b64]: This work is mentioned in the introduction and later as the implementation for the Loca tool, which is directly compared to the proposed ReuseTracker tool.\n- \"Scalable analysis of multicore data reuse and sharing\" [b41]: This work is indicated in the text as having inspired the authors to avoid the flaws of previous methods, thus it can be inferred as a source paper.\n- \"Fast and accurate exploration of multi-level caches using hierarchical reuse distance\" [b36]: This work is also mentioned in the text as having proposed a technique that the authors then built upon.\n\nHere is the JSON format for the reference papers with corresponding confidence scores:\n\n{ \n  \"b8\": 0.9,\n  \"b7\": 0.85,\n  \"b64\": 0.85,\n  \"b41\": 0.8,\n  \"b36\": 0.8\n} \n\nNote: The confidence score is a subjective measure of how directly and significantly the source paper is believed to have influenced the main contributions of the paper. A score closer to 1 indicates a higher level of influence."], "6427029c90e50fcafd5d6c03": ["Based on the academic paper text, the primary challenges outlined are addressing the memory wall problem and improving locality and concurrency. The authors propose a new cost metric for cache misses, Pure Miss Contribution (PMC), and a cache management framework that takes both data locality and concurrency into account, CARE.\n\nThe sources papers that inspired this work appear to be:\n\n1. \"Hitting the memory wall implications of the obvious\" [b49]: This paper is the basis for the challenge the authors are addressing, the memory wall problem.\n\n2. \"Concurrent average memory access time\" [b43]: The authors mention this work when discussing the concurrent memory access model and how it inspired their own PMC metric.\n\n3. \"A case for mlp aware cache replacement\"[b33]: This paper was used as the basis for discussing the limitations of MLP-based cache management and how their work aims to improve upon it.\n\n4. \"SHIP: Signature based hit predictor for high performance caching\" [b47]: This paper is referenced when discussing the limitations of locality-based cache management and how their work aims to address these limitations.\n\n5. \"Hawkeye: Back to the future leveraging belady's algorithm for improved cache replacement\" [b16]: This paper is mentioned when discussing re-reference prediction, an aspect that seems to be incorporated in their proposed CARE framework.\n\n6. \"A study of replacement algorithms for a virtual storage computer\" [b9]: This paper is referenced when discussing locality-based cache management and the authors appear to be building on this work in their proposed CARE framework.\n\nTherefore, based on the given criteria, the source papers are b49, b43, b33, b47, b16, and b9. \n\nHere is the result in json format: \n{\n  \"b49\": 0.9, \n  \"b43\": 0.8, \n  \"b33\": 0.8, \n  \"b47\": 0.7, \n  \"b16\": 0.7, \n  \"b9\": 0.7\n}", "Based on the text provided, the papers that seem to have had the most influence on the author's work are:\n\n1) \"Concurrent average memory access time\" [b43]\n2) \"A case for mlp aware cache replacement\" [b33]\n3) \"A study on modeling and optimization of memory systems\" [b27]\n4) \"High performance cache replacement using re reference interval prediction rrip\" [b18]\n5) \"Back to the future leveraging belady s algorithm for improved cache replacement\" [b16]\n6) \"Lpm a systematic methodology for concurrent data access pattern optimization from a matching perspective\" [b28]\n7) \"Ship signature based hit predictor for high performance caching\" [b47]\n8) \"Pacman prefetch aware cache management for high performance caching\" [b48]\n\nThe confidence scores for these papers are estimated as follows:\n\n1) [b43]: 0.9 - The author directly states that their work is inspired by the C-AMAT model proposed in this paper.\n2) [b33]: 0.85 - The author discusses the MLP-aware cache replacement policy proposed in this paper and suggests that their work can improve upon it.\n3) [b27]: 0.8 - This paper introduces the concept of \"pure miss cycles\", which is a key concept in the author's work.\n4) [b18]: 0.75 - The author discusses the RRIP policy introduced in this paper as part of their analysis of existing cache management schemes.\n5) [b16]: 0.7 - The author discusses the Belady's optimal solution simulated in this paper as part of their analysis of existing cache management schemes.\n6) [b28]: 0.65 - The author cites this paper in their discussion of concurrent cache/memory activities.\n7) [b47]: 0.6 - The author discusses the SHiP predictor introduced in this paper as part of their analysis of existing cache management schemes.\n8) [b48]: 0.55 - The author cites this paper in their discussion of the limitations of locality-based cache management.\n\nThese confidence scores are subjective and are based on the frequency and context of the citations in the paper's text.\n\nThe result in the JSON format:\n\n{\n  \"b43\": 0.9,\n  \"b33\": 0.85,\n  \"b27\": 0.8,\n  \"b18\": 0.75,\n  \"b16\": 0.7,\n  \"b28\": 0.65,\n  \"b47\": 0.6,\n  \"b48\": 0.55\n}", "Based on the provided academic paper text and the titles of the references, the most pertinent source papers that inspired the author can be inferred as follows:\n\n1) \"Concurrent average memory access time\" [b43]: The paper directly states that the new metric Pure Miss Contribution (PMC) is inspired by the C-AMAT model in this paper.\n\n2) \"A case for mlp aware cache replacement\" [b33]: This paper is mentioned several times, especially in the context of limitations of MLP-based cache management and the need for a more holistic metric, leading to the development of PMC.\n\n3) \"SHIP: Signature based hit predictor for high performance caching\" [b47]: This paper is referenced in the context of discussing locality-based cache management, which the authors aim to improve upon with their proposed metric and framework.\n\n4) \"Back to the future: leveraging belady's algorithm for improved cache replacement\" [b16]: This paper is referred to in the context of discussing re-reference prediction and the authors' aim to improve upon existing cache management schemes.\n\n5) \"A locality aware cost sensitive cache replacement algorithm\" [b21]: This paper is referred to in the context of cost-based cache management and its limitations, which the authors aim to address.\n\n6) \"High performance cache replacement using re-reference interval prediction (RRIP)\" [b18]: This paper is referred to in the context of discussing re-reference prediction and the authors' aim to improve upon existing cache management schemes.\n\n7) \"Hitting the memory wall: Implications of the obvious\" [b49]: This paper is referenced in the introduction, and it seems to have provided the fundamental problem which the authors are attempting to address.\n\nThe result in the json format:\n{\n\"b43\": 1.0,\n\"b33\": 0.9,\n\"b47\": 0.9,\n\"b16\": 0.8,\n\"b21\": 0.7,\n\"b18\": 0.7,\n\"b49\": 0.6\n}", "Based on the text and the titles of the references, the source papers which inspired the author most are:\n\n1. \"Concurrent average memory access time\" [b43]: The paper directly mentions that their concept of Pure Miss Contribution (PMC) is inspired by the C-AMAT model from this paper.\n\n2. \"A case for MLP aware cache replacement\" [b33]: The paper discusses the MLP-based cost and its limitations, suggesting that this paper played a significant role in shaping their own approach.\n\n3. \"SHIP: Signature based hit predictor for high performance caching\" [b47]: This paper is mentioned several times when discussing previous work on re-reference prediction, an important aspect of the proposed method.\n\n4. \"Back to the future: Leveraging Belady's algorithm for improved cache replacement\" [b16]: This paper is mentioned as a previous work on predicting re-reference characteristics, indicating that it influenced the development of their cache management framework.\n\n5. \"Designing a cost-effective cache replacement policy using machine learning\" [b39]: Mentioned in the discussion about previous work using machine learning for re-reference prediction, implying that it may have influenced the author's approach to cache management.\n\n6. \"A study on modeling and optimization of memory systems\" [b27]: This paper is cited when discussing the definitions of pure miss cycles and memory active cycles, suggesting it played a role in the development of the PMC concept.\n\nIn JSON format:\n{\n\"b43\": 0.9,\n\"b33\": 0.9,\n\"b47\": 0.85,\n\"b16\": 0.85,\n\"b39\": 0.8,\n\"b27\": 0.8\n}", "Based on the analysis of the given academic paper, the key challenges outlined in the paper are improving locality and concurrency in cache management. The paper proposes a new metric, Pure Miss Contribution (PMC), and a new cache management framework, CARE, to address these challenges.\n\nFrom the text, it is clear that the paper heavily relies on several previous studies. Here are the source papers identified:\n\n1. \"Concurrent average memory access time\" [b43]: The authors introduce and formally define the concept of Pure Miss Contribution (PMC) which is inspired by the C-AMAT model proposed in this paper. Confidence score: 0.95\n\n2. \"A case for MLP aware cache replacement\" [b33]: The authors discuss the limitations of MLP-based cache management and propose to improve upon it. Confidence score: 0.85\n\n3. \"SHIP: Signature based hit predictor for high performance caching\" [b47]: The authors discuss the limitations of locality-based cache management schemes, and this paper is specifically mentioned as one of the studies focusing on locality-based schemes. Confidence score: 0.75\n\n4. \"Back to the future: leveraging Belady's algorithm for improved cache replacement\" [b16]: The authors mention this paper when discussing re-reference prediction, a concept they incorporate into their cache management framework. Confidence score: 0.8\n\n5. \"Improving cache management policies using dynamic reuse distances\" [b12]: This paper is referenced when the authors discuss memory level parallelism, a concept that is key to their proposed solutions. Confidence score: 0.75\n\n6. \"Hitting the memory wall: implications of the obvious\" [b49]: This paper is mentioned in the introduction as addressing the memory wall problem, which is a key challenge that the authors are seeking to address. Confidence score: 0.7\n\nThe results in JSON format are:\n\n{\"b43\": 0.95, \"b33\": 0.85, \"b47\": 0.75, \"b16\": 0.8, \"b12\": 0.75, \"b49\": 0.7}"], "629435a25aee126c0f2fecb3": ["Based on the given text of the academic paper and the author's note, the most pertinent source papers that inspire this paper seem to be:\n\n1. \"Attention is All You Need\" [b78]: This is the original paper introducing the Transformer model, which is the fundamental basis of the paper in question. The authors of the paper in question further develop this model to tackle long sequence lengths, which is a known challenge with Transformer models.\n2. \"FlashAttention: Fast and Memory Efficient Exact Attention with IO Awareness\" [None]: This is presumably the paper that the authors are building on, as they directly name their new attention algorithm \"FlashAttention\". They describe their method as IO-Aware, which is a key feature of the method described in this paper. \n3. \"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\" [b47]: This paper might be a source of inspiration as it discusses reducing the complexity of attention-based models, which is a key goal of the paper in question.\n4. \"Reformer: The Efficient Transformer\" [b48]: This paper introduces the Reformer model, a variant of the Transformer model that aims to increase efficiency. It is mentioned in the context of methods that aim to reduce the complexity of attention.\n5. \"The Input-Output Complexity of Sorting and Related Problems\" [b0]: This paper is referenced in the context of IO-aware algorithms, which is a major focus of the paper in question.\n6. \"Data Movement Is All You Need: A Case Study on Optimizing Transformers\" [b40]: This paper is mentioned in the context of discussing the bottlenecks in Transformer operations. It's likely that the observations and insights from this paper informed the development of the FlashAttention method.\n\nThe JSON format would be as follows:\n\n{\"b78\": 1, \"None\": 0.9, \"b47\": 0.8, \"b48\": 0.8, \"b0\": 0.8, \"b40\": 0.7} \n\nPlease note that the confidence scores are subjective and set to maximum for the most influential paper, with relative scores for the others.", "Based on the provided text and the paper titles, here are the most likely source papers that inspired the authors of the paper under analysis:\n\n1. \"Attention is All You Need\" [b78]: This is likely a source paper as it introduced Transformers, which are the basis for the algorithm proposed in the current paper.\n2. \"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\" [b47]: Given the focus on attention mechanisms and efficiency, this paper might have provided key insights for the development of FlashAttention.\n3. \"Reformer: The Efficient Transformer\" [b48]: This paper also proposes an efficient Transformer architecture, which is the main topic of the current paper. The authors might have drawn inspiration from its methods.\n4. \"Linformer: Self-Attention with Linear Complexity\" [b80]: The linear complexity aspect might have been a key motivation for the authors, as it aligns with their goal of reducing computational complexity.\n5. \"The Input/Output Complexity of Sorting and Related Problems\" [b0]: This paper, which discusses the principle of IO-awareness, is directly mentioned in the text as a key concept for the current paper.\n6. \"Big Bird: Transformers for Longer Sequences\" [b88]: The current paper mentions the challenge with long sequences, which is the main focus of this paper, making it a likely source of inspiration.\n7. \"Data Movement is All You Need: A Case Study on Optimizing Transformers\" [b40]: This paper seems to be a key source, as it discusses the importance of memory accesses, which is a key challenge tackled by the authors of the current paper.\n\nIn JSON format:\n\n{\"b78\": 1,\n \"b47\": 0.9,\n \"b48\": 0.9,\n \"b80\": 0.8,\n \"b0\": 1,\n \"b88\": 0.7,\n \"b40\": 1}", "Based on the text of the paper, the titles of the references, and the author's notes, the following references appear to be the most significant source papers that influenced this research:\n\n1. \"Attention is All You Need\" [b78] - This paper introduces the Transformer model, which serves as the base of the algorithm proposed in the current paper. The authors identify the attention mechanism as a challenge that needs to be improved in this model.\n\n2. \"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\" [b47] - This paper introduces a low-rank approximation method that the current paper acknowledges but identifies as not solving the issue of memory access times.\n\n3. \"Reformer: The Efficient Transformer\" [b48] - Another paper that attempts to reduce the compute requirements of attention, which the current paper builds upon.\n\n4. \"The Input/Output Complexity of Sorting and Related Problems\" [b0] - This paper is crucial as it introduces the concept of IO-aware algorithms, which the current paper uses as a key principle for developing their algorithm.\n\n5. \"FlashAttention: Fast and Memory Efficient Exact Attention with IO Awareness\" [None] - This is not indicated in the reference list, but it appears to be the title of the current paper itself, indicating that the paper introduces a new method called FlashAttention.\n\n6. \"Linformer: Self-Attention with Linear Complexity\" [b80] - The paper refers to the Linformer as an example of an approximate attention method, which it aims to improve upon.\n\nIn the JSON format, the result would look like this:\n\n{ \"b78\": 0.9, \"b47\": 0.8, \"b48\": 0.8, \"b0\": 0.9, \"None\": 1.0, \"b80\": 0.7 }", "Based on the given information, the main challenges outlined in the paper were the time and memory complexity of self-attention models, particularly for long sequences. The author proposed FlashAttention, an attention algorithm that significantly reduces memory accesses through the use of tiling and recomputation, resulting in faster computation and less memory usage. The algorithm was implemented in CUDA for fine-grained control over memory access and to fuse all attention operations into one GPU kernel.\n\nHere are the source papers that most likely inspired the author's work:\n\n1. \"Attention is all you need\" [b78]: This is the seminal paper that introduced the Transformer model, which the author's work is directly improving upon. The author might have been inspired by the self-attention mechanism proposed in this paper and sought to optimize it.\n\n2. \"The input output complexity of sorting and related problems\" [b0]: The author mentioned that they analyzed the IO complexity of FlashAttention. This paper could have provided the theoretical foundation for their analysis.\n\n3. \"Reformer: The efficient transformer\" [b48]: The author's work seems to share similarities with the Reformer, which also aims to reduce the time and memory complexity of Transformer models. The author might have drawn inspiration from the techniques used in Reformer.\n\n4. \"Transformers are RNNs: Fast autoregressive transformers with linear attention\" [b47]: This paper proposes a method to reduce the compute requirements for attention, which aligns with the goal of the author's work. It could have served as a reference point for the author.\n\n5. \"Long Range Arena: A benchmark for Efficient Transformers\" [b76]: The author is trying to optimize Transformers for long sequences. This paper provides a benchmark for such models, which the author might have used to evaluate their work.\n\n6. \"Data movement is all you need: A case study on optimizing transformers\" [b40]: The author's work seems to address the same problem identified in this paper - the bottleneck caused by memory accesses in Transformers. The author could have been inspired by this paper to focus on optimizing memory accesses.\n\n7. \"Efficient transformers: A survey\" [b77]: This paper provides a comprehensive review of efficient Transformer models. It could have served as a valuable resource for the author to understand the existing methods and their limitations, and to position their work within this landscape.\n\nPlease note that the confidence scores are not provided as the output format was not specified in the task. Here is the result in json format:\n\n{\n    \"b78\": 1,\n    \"b0\": 0.9,\n    \"b48\": 0.8,\n    \"b47\": 0.8,\n    \"b76\": 0.7,\n    \"b40\": 0.7,\n    \"b77\": 0.7\n}", "Based on the provided text, the following source papers seem to be most pertinent:\n\n1. \"Attention is all you need\" [b78]\n2. \"Transformers are rnns fast autoregressive transformers with linear attention\" [b47]\n3. \"Reformer: the efficient transformer\" [b48]\n4. \"The input output complexity of sorting and related problems\" [b0]\n5. \"Scatterbrain: unifying sparse and low rank attention\" [b7]\n6. \"Transformers for longer sequences\" [b88]\n7. \"GPT-2: language models are unsupervised multitask learners\" [b63]\n\nThe paper spends a significant amount of time discussing the limitations of Transformers and how FlashAttention addresses these limitations. The papers listed above are frequently referred to throughout the text and appear to be the main inspirations for the current paper. \n\nHere is the JSON output: {\"b78\": 1, \"b47\": 0.9, \"b48\": 0.9, \"b0\": 0.8, \"b7\": 0.8, \"b88\": 0.7, \"b63\": 0.7}"], "64a78f10d68f896efa01dee3": ["Based on the provided text, the source papers that inspired the current paper could be:\n\n- \"chiplet cloud building ai supercomputers for serving large generative language models\", which is the title of the paper itself, therefore, it doesn't count as a source paper.\n- \"deepspeed inference enabling efficient inference of transformer models at unprecedented scale\" [b0] is mentioned in the context of state-of-the-art GPT-3 throughput on GPU and hardware utilization.\n- \"palm scaling language modeling with pathways\" [b4] is referred to when discussing the TCO/Token and latency Pareto frontier of Chiplet Cloud for GPT-3.\n- \"efficiently scaling transformer inference\" [b25] is mentioned in the context of discussing software-hardware co-design.\n- \"efficient large scale language model training on gpu clusters using megatron lm\" [b22] is referred to when discussing mapping strategies such as tensor and pipeline model parallelism.\n- \"moonwalk nre optimization in asic clouds\" [b13] is referenced when discussing the NRE model.\n- \"the datacenter as a computer designing warehouse scale machines\" [b2] could be a source paper as it might be related to designing and building hardware systems for LLMs.\n- \"attention is all you need\" [b37] might be a source paper as it introduces the transformer decode block, which is used in large generative language models.\n\nBased on the above analysis, the source papers that inspired the author most could be [b0], [b4], [b22], [b25], [b13], [b2] and [b37]. \n\nHowever, without the exact context where these papers are mentioned, it's hard to assign a confidence score to each of them. In addition, the author note and titles of each paper are in different languages, which adds another layer of complexity to this task. I would encourage you to provide more context and make sure all the information is in the same language for better results.", "Based on the given text and the challenges the paper aims to address, the following source papers seem to have inspired or contributed to the development of the work:\n\nReference 'b0': \"deepspeed inference enabling efficient inference of transformer models at unprecedented scale\" is mentioned multiple times, suggesting it has a significant influence on the work. The paper discusses the challenge of serving large generative language models on commodity hardware, like GPUs, which is hitting scalability wall, which is a key challenge discussed in the paper. Therefore, it seems likely that this work was a source of inspiration.\n\nReference 'b22': \"efficient large scale language model training on gpu clusters using megatron lm\" is also referenced multiple times and appears to detail methods of efficient large-scale language model training, which aligns with the paper's focus on improving the efficiency of large language model inference.\n\nReference 'b25': \"efficiently scaling transformer inference\" is also mentioned multiple times and appears to introduce methods for scaling transformer inference, a challenge directly addressed in the paper.\n\nReference 'b3': \"alec radford ilya sutskever and dario amodei\" is mentioned as the source of GPT-3, one of the models the paper's system is designed to handle. It is therefore plausible that this work was a source of inspiration, though not necessarily one that directly influenced the methods proposed in the paper.\n\nReference 'b4': \"palm scaling language modeling with pathways\" is mentioned in the context of the TCO/Token and latency Pareto frontier, suggesting it was also a source of inspiration.\n\nReference 'b13': \"moonwalk nre optimization in asic clouds\" paper is mentioned in the context of the non-recurring engineering (NRE) costs, which is one of the key challenges addressed in the paper. \n\n{\n  \"b0\": 0.9,\n  \"b22\": 0.8,\n  \"b25\": 0.8,\n  \"b3\": 0.6,\n  \"b4\": 0.6,\n  \"b13\": 0.6\n}", "Based on the text of the paper and the author notes, the paper is about designing a chiplet-based ASIC AI-supercomputer architecture for Large Language Models (LLMs) to reduce the Total Cost of Ownership (TCO) per generated token. The new approach proposed in the paper involves storing all model parameters and Key-Value (KV) values in on-chip SRAM memory. The paper also proposes a two-phase design-search methodology for hardware exploration and software evaluation. \n\nThe key references that seem to have inspired this work are:\n\n1. \"b0: deepspeed inference enabling efficient inference of transformer models at unprecedented scale\"\n2. \"b22: efficient large scale language model training on gpu clusters using megatron lm\"\n3. \"b25: efficiently scaling transformer inference\"\n4. \"b13: moonwalk nre optimization in asic clouds\"\n5. \"b31: megatron lm training multi billion parameter language models using model parallelism\"\n\nThe confidence score for each of the references are estimated as follows:\n\n{ \"b0\": 0.9, \"b22\": 0.8, \"b25\": 0.85, \"b13\": 0.75, \"b31\": 0.8 }\n\nThe confidence score is given based on the relevance of the reference to the work presented in the paper and the frequency of the reference's appearance in the text.", "Based on the text of the paper, the algorithm proposed by the author is Chiplet Cloud, a chiplet-based ASIC AI-supercomputer architecture for Large Language Models (LLMs) which aims to reduce the total cost of ownership (TCO) per generated token. This architecture is heavily inspired by the challenges and limitations of current hardware for serving generative LLMs which the paper has outlined and proposed solutions for, such as storing all model parameters and KV values in on-chip SRAM memory and performing a two-phase design-search methodology for hardware exploration and software evaluation.\n\nThe references that seem to be the most directly related to the paper's contributions are: \n\n- \"b0: deepspeed inference enabling efficient inference of transformer models at unprecedented scale\" \n- \"b10: ten lessons from three generations shaped google s tpuv i industrial product\"\n- \"b13: moonwalk nre optimization in asic clouds\"\n- \"b22: efficient large scale language model training on gpu clusters using megatron lm\"\n- \"b25: efficiently scaling transformer inference\" \n\nThese references were selected because they involve similar concepts and methods such as efficient inference of transformer models, lessons from designing accelerators, optimization in ASIC clouds, and scaling of language model training.\n\nTherefore, the returned result in json format would be:\n{\n    \"b0\": 0.8,\n    \"b10\": 0.75,\n    \"b13\": 0.85,\n    \"b22\": 0.7,\n    \"b25\": 0.8\n} \nThese confidence scores are assigned based on the relevance of the methodological similarities and the direct inspiration the current paper seems to have drawn from these source papers.", "Based on the provided text of the academic paper, the key challenges outlined by the paper are the high capital expenditure and energy cost of running Large Language Models (LLMs), the limitations of serving generative transformer-based large language models on commodity hardware, and the high costs of non-recurring engineering (NRE) for ASICs.\n\nThe novel methods proposed by the authors involve the introduction of a chiplet-based ASIC AI-supercomputer architecture for LLMs called \"Chiplet Cloud\" aiming to reduce the total cost of ownership (TCO) per generated token. The authors also propose a two-phase design-search methodology for hardware exploration and software evaluation.\n\nBased on the above information, the references that seem to have heavily influenced the main contributions of the paper are:\n\n- b22: \"Efficient large scale language model training on GPU clusters using Megatron LM\"\n- b0: \"DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale\"\n- b25: \"Efficiently Scaling Transformer Inference\"\n- b13: \"Moonwalk: NRE Optimization in ASIC Clouds\"\n\nThe paper uses these references to support their arguments and solutions. For example, reference b22 seems to be used in the context of outlining the challenge of training large language models, reference b0 and b25 seem to be used to highlight the limitations of current hardware for serving generative transformer-based large language models, and reference b13 appears to be used in the context of discussing the high NRE costs associated with ASICs.\n\nThese references seem to be directly relevant to the paper's novel contributions and are preceded by phrases indicating direct inspiration or motivation.\n\nThe confidence score for these references would be:\n\n\"b22\": 0.9\n\"b0\": 0.9\n\"b25\": 0.8\n\"b13\": 0.8"], "6346305e90e50fcafda07ab8": ["{\"b2\": 1, \"b33\": 0.9, \"b20\": 0.8, \"b17\": 0.7, \"b49\": 1, \"b44\": 0.9, \"b61\": 0.7}", "Based on the paper's text and the titles of the references, the source papers that most inspired the given paper are:\n\n1. \"Language models are few shot learners\" [b2]: This paper is mentioned explicitly in the introduction and is likely to have provided foundational knowledge on large language models and few-shot learning.\n2. \"Lifelong machine learning\" [b5]: The paper frequently mentions continual learning (CL), which is a key topic in this source. It is also directly cited in the related work section.\n3. \"Continual learning for sentence representations using conceptors\" [b33]: This paper is directly associated with the proposed system (CPT), as it is mentioned when describing the starting point of the system.\n4. \"Parameter efficient transfer learning for nlp\" [b21]: The paper's proposed system is said to adopt the parallel adapter idea from this source, indicating a direct influence on the methodology.\n5. \"Overcoming catastrophic forgetting with hard attention to the task\" [b49]: The paper takes inspiration from the hard attention idea in this source to train the task mask, which is a crucial part of the proposed methodology.\n6. \"ELLE efficient lifelong pre-training for emerging data\" [b44]: This source is directly compared to the proposed system, indicating that it influenced the development of the paper's methodology.\n\nHere is the result in the JSON format:\n{\n\"b2\": 0.9,\n\"b5\": 0.8,\n\"b33\": 0.75,\n\"b21\": 0.85,\n\"b49\": 0.85,\n\"b44\": 0.75\n}", "Based on the text and the titles of the references, the source papers of the given paper, which the author indicates that inspire them most are:\n\n{\"b2\": 0.9, \"b33\": 0.9, \"b5\": 0.8, \"b44\": 0.8, \"b20\": 0.8, \"b17\": 0.8, \"b49\": 0.8, \"b59\": 0.7}\n\nHere is the reasoning:\n\n1. \"b2\": This paper is mentioned in the context of discussing the ability of large LMs to perform few-shot learning well, which is a running theme in the current paper. The title is \"Language models are few shot learners\".\n\n2. \"b33\": This paper is mentioned as the basis for the starting point of the proposed system, indicating its importance. The title is \"Continual learning for sentence representations\".\n\n3. \"b5\": This paper is mentioned when discussing the challenges of catastrophic forgetting in continual learning, which is a major problem tackled in the current paper. The title is \"Lifelong Machine Learning\".\n\n4. \"b44\": This paper is mentioned when discussing the approach taken by the proposed system and how it differs from ELLE. The title is \"ELLE: Efficient Lifelong Pre-training for Emerging Data\".\n\n5. \"b20\": This paper is mentioned when discussing the inspiration for the CL-plugin used in the proposed system. The title is not provided in the references list.\n\n6. \"b17\": This paper is mentioned when discussing the structure of the CL-plugin and justifying why a parallel structure is used. The title is \"Towards a Unified View of Parameter-Efficient Transfer Learning\".\n\n7. \"b49\": This paper is mentioned when discussing the hard attention idea used in the proposed system's task masks. The title is \"Overcoming Catastrophic Forgetting with Hard Attention to the Task\".\n\n8. \"b59\": This paper is mentioned when discussing the benefits of post-training an LM with a large unlabeled domain corpus, which is a core part of the approach taken by the current paper. The title is \"BERT Post-training for Review Reading Comprehension and Aspect-Based Sentiment Analysis\". \n\nNote: The confidence scores are assigned based on the level of direct influence and relevance these papers have on the main contributions of the current paper. The scores are subjective and could vary based on different interpretations of the text.", "{\n\"b2\": 0.9,\n\"b33\": 0.85,\n\"b59\": 0.8,\n\"b5\": 0.75,\n\"b49\": 0.7,\n\"b20\": 0.65,\n\"b17\": 0.6,\n\"b44\": 0.55\n}", "Based on the text and given reference titles, the source papers that inspire the author the most can be identified as follows:\n\n1. b2: \"language models are few shot learners\" - The author mentions this paper while discussing the ability of large LMs to perform few-shot learning. \n2. b33: \"RoBERTa\" - This paper is mentioned multiple times as a base model that the author's proposed system (CPT) continually post-trains. \n3. b5: \"lifelong machine learning\" - The author cites this paper in relation to the concept of continual learning (CL) which is a major focus of the paper. \n4. b61: \"character level convolutional networks for text classification\" - This paper is referred to in relation to domain-adaptive pre-training which is a key part of the author's proposed system. \n5. b20: \"Parameter-Efficient Transfer Learning for NLP\" - This paper is cited in relation to the CL-plugin which is inspired by adapters, a key component of CPT.\n6. b49:  \"overcoming catastrophic forgetting with hard attention to the task\" - This paper is referred to when the author discusses the idea of hard attention for training task masks.\n\nThe json format of the answer is:\n\n{\n\"b2\": 0.9,\n\"b33\": 0.9,\n\"b5\": 0.8,\n\"b61\": 0.8,\n\"b20\": 0.8,\n\"b49\": 0.8\n} \n\nThe confidence scores are subjective and are given based on the frequency and context of the mentions in the paper."], "6327dda690e50fcafd67dea3": ["{\n    \"b15\": 0.95,\n    \"b41\": 0.9,\n    \"b4\": 0.85,\n    \"b18\": 0.85,\n    \"b16\": 0.8,\n    \"b17\": 0.8\n}", "{\n  \"b15\": 0.95,\n  \"b41\": 0.85,\n  \"b4\": 0.85,\n  \"b18\": 0.85,\n  \"b16\": 0.75,\n  \"b17\": 0.75,\n  \"b11\": 0.9,\n  \"b10\": 0.9\n}", "{\n    \"b15\": 0.95,\n    \"b4\": 0.9,\n    \"b18\": 0.9,\n    \"b11\": 0.85,\n    \"b10\": 0.85,\n    \"b1\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8\n}", "{\n  \"b15\": 1.0,\n  \"b11\": 0.9,\n  \"b10\": 0.8,\n  \"b4\": 0.7,\n  \"b18\": 0.6,\n  \"b1\": 0.5\n}", "{\n  \"b15\": 0.95,\n  \"b11\": 0.9,\n  \"b10\": 0.85,\n  \"b1\": 0.85,\n  \"b4\": 0.8,\n  \"b18\": 0.8,\n  \"b16\": 0.75,\n  \"b17\": 0.75\n}"], "6260bd7f5aee126c0fc6bba4": ["Based on the text and reference titles, the most relevant source papers that inspired this research could be:\n\n1. \"generating d molecules for target protein binding\" [None]\n2. \"the process of structure-based drug design\" [b0]\n3. \"symmetry adapted generation of d point sets for the targeted discovery of molecules\" [b6]\n4. \"a d generative model for drug design\" [b25]\n5. \"an autoregressive flow model for d molecular geometry generation from scratch\" [b27]\n6. \"generating d molecules conditional on receptor binding sites with deep generative models\" [b36]\n7. \"highly accurate protein structure prediction with alphafold\" [b14]\n8. \"moflow an invertible flow model for generating molecular graphs\" [b52]\n\nThe confidence score for each source paper could be:\n\n```\n{\n    \"None\": 0.9,\n    \"b0\": 0.8,\n    \"b6\": 0.7,\n    \"b25\": 0.9,\n    \"b27\": 0.9,\n    \"b36\": 0.9,\n    \"b14\": 0.7,\n    \"b52\": 0.8\n}\n```\nThe confidence scores are subjective and they represent the relevancy of the source paper to the given paper. The higher the score, the more relevant the paper is.", "Based on the text, the most relevant source papers that inspired or motivated the authors can be identified as:\n\n1. \"generating d molecules for target protein binding\" [b0]\n2. \"the process of structure based drug design\" [b0]\n3. \"an autoregressive flow model for d molecular geometry generation from scratch\" [b27]\n4. \"generating 3D molecules conditional on receptor binding sites with deep generative models\" [b36]\n5. \"a 3D generative model for drug design\" [b25]\n6. \"a generative model for molecular distance geometry\" [b44]\n7. \"highly accurate protein structure prediction with alphafold\" [b14]\n8. \"Glow: generative flow with invertible 1x1 convolutions\" [b15]\n\nThese papers are either directly mentioned as inspirations or have been referred to multiple times in the text for their relevant methodologies and concepts. \n\nHere is the output in the required json format:\n\n{\n\"b0\": 0.9,\n\"b27\": 0.85,\n\"b36\": 0.85,\n\"b25\": 0.8,\n\"b44\": 0.8,\n\"b14\": 0.75,\n\"b15\": 0.75\n} \n\nThe confidence score indicates how strongly each paper appears to have influenced the current paper, based on the number of mentions and the context in which they are discussed.", "Based on the text of the paper, the most pertinent source papers that inspired the authors are:\n\n1. \"generating d molecules for target protein binding\" (b0)\n2. \"the process of structure based drug design\" (b1)\n3. \"symmetry adapted generation of d point sets for the targeted discovery of molecules\" (b6)\n4. \"a d generative model for drug design\" (b25)\n5. \"a 3D generative model for drug design\" (b25)\n6. \"an autoregressive flow model for d molecular geometry generation from scratch\" (b27)\n7. \"generating 3D molecules conditional on receptor binding sites with deep generative models\" (b36)\n8. \"a generative model for molecular distance geometry\" (b44)\n\nHere are the confidence scores for each reference paper:\n\n{\n    \"b0\": 0.8,\n    \"b1\": 0.7,\n    \"b6\": 0.85,\n    \"b25\": 0.9,\n    \"b27\": 0.85,\n    \"b36\": 0.9,\n    \"b44\": 0.75\n} \n\nPlease note that these scores are based on the analysis of the text and the frequency of direct and indirect mentions of the concepts discussed in these papers. It's also worth noting that the generation of molecules, structure-based drug design, and the use of deep generative models are the main topics of the paper, which is why the papers on these subjects have higher scores.", "Based on the provided text and the author notes, the source papers that inspire the given paper the most are:\n\n1. \"generating 3d molecules for target protein binding\" [None]\n2. \"the process of structure-based drug design\" [b0]\n3. \"generating 3d molecules conditional on receptor binding sites with deep generative models\" [b36]\n4. \"a 3d generative model for drug design\" [b25]\n5. \"symmetry adapted generation of 3d point sets for the targeted discovery of molecules\" [b6]\n6. \"an autoregressive flow model for 3d molecular geometry generation from scratch\" [b27]\n7. \"3d convolutional neural networks for human action recognition\" [b12]\n8. \"highly accurate protein structure prediction with alphafold\" [b14]\n\nThe source papers and their confidence scores are as follows:\n\n```json\n{\n  \"None\": 0.9,\n  \"b0\": 0.8,\n  \"b36\": 0.85,\n  \"b25\": 0.8,\n  \"b6\": 0.75,\n  \"b27\": 0.75,\n  \"b12\": 0.7,\n  \"b14\": 0.7\n}\n```\n\nPlease note that these confidence scores are estimated based on the frequency and context in which these papers are referenced in the given text.", "Based on the text of the paper and the author's notes, the primary challenges outlined are: complicated conditional information, enormous chemical space and continuous 3D space, and the equivariance property. The paper introduces GraphBP, a generative framework that addresses these challenges by placing atoms one-by-one, extracting context with a 3D graph neural network, and preserving the desirable equivariance property through a local coordinate system. \n\nThe most pertinent source papers to the paper's novel contributions are likely to be those referenced in the sections discussing these key challenges and the proposed solution. Additionally, the papers referenced in the sections where comparisons are made with the proposed method are also likely to be significant.\n\nBased on this analysis, the source papers are likely to include:\n\n- LiGAN [b36]: As it is mentioned several times throughout the paper and compared directly with GraphBP.\n- G-SchNet [b6]: This paper is referenced multiple times, particularly in the section discussing the construction of a local spherical coordinate system, a key part of the GraphBP method.\n- G-SphereNet [b27]: This paper is referenced in several key parts of the text, including the discussion of the sequential generation process and the training scheme.\n- Autoregressive flow model [b34]: This paper is referenced in the explanation of the autoregressive model used in GraphBP method.\n- 3D molecule generation [b7] [b11] [b32] [b40]: These papers are referenced in the section discussing the challenges of 3D molecule generation, a key problem that GraphBP addresses.\n\nBased on this analysis, the source papers and their relevance scores are likely to be:\n\n{ \"b36\": 0.9, \"b6\": 0.8, \"b27\": 0.8, \"b34\": 0.7, \"b7\": 0.6, \"b11\": 0.6, \"b32\": 0.6, \"b40\": 0.6}\n\nPlease note that these results are based on the text analysis and may require further validation."], "62b2888c5aee126c0fbc731c": ["Based on the text of the paper, the following references likely played a significant role in shaping the author's research and the development of their novel algorithm, LIMO:\n\n- b29: \"Auto Encoding Variational Bayes\": The author uses a Variational Autoencoder (VAE), which is a key component of their LIMO algorithm. \n\n- b24: \"Junction Tree Variational Autoencoder for Molecular Graph Generation\": The author compares their method with JT-VAE and indicates that they follow a similar approach in using a VAE, but take a different approach in property prediction. \n\n- b60: \"Graph Convolutional Policy Network for Goal Directed Molecular Graph Generation\": This paper seems to be a key source of inspiration, as it's one of the baselines that the authors compare their method to. The author also mentions that reinforcement learning based methods are the most powerful for generating molecules, which is the approach taken by GCPN.\n\n- b30: \"Self Referencing Embedded Strings (SELFIES) a Robust Molecular String Representation\": The author uses SELFIES for ensuring chemical validity, which is a crucial part of their LIMO algorithm.\n\n- b61: \"MoFlow: An Invertible Flow Model for Generating Molecular Graphs\": MoFlow is mentioned as a method that predicts molecular properties from a latent space using a neural network, but has difficulty generating molecules with high property scores. This seems to be a challenge that the author's work aims to address.\n\n- b58: \"MARS: Markov Molecular Sampling for Multiobjective Drug Discovery\": This paper is also frequently compared with the author's method, indicating it has been a significant influence.\n\nBased on the information provided by the author, the confidence scores for these references could be as follows:\n\n{\"b29\": 0.9, \"b24\": 0.8, \"b60\": 0.8, \"b30\": 0.9, \"b61\": 0.8, \"b58\": 0.8}", "Based on the provided text, the following papers appear to be the most influential to the given paper:\n\n1. \"Junction tree variational autoencoder for molecular graph generation\" [b24]: This paper is the source of the Junction Tree VAEs that the authors of the given paper adopt and improve upon. The authors cited this paper several times and discussed its contributions in depth. Confidence Score: 0.9\n\n2. \"Graph convolutional policy network for goal directed molecular graph generation\" [b60]: This paper is mentioned as a state-of-the-art method that the authors compare their method with. The authors also discuss the limitations of the method proposed in this paper. Confidence Score: 0.8\n\n3. \"Generative models for molecule design\" [b32]: This paper is mentioned as a motivation for the authors' work. The authors also discuss the contributions of this paper and how they extend its method. Confidence Score: 0.8\n\n4. \"Auto encoding variational bayes\" [b29]: This paper is mentioned as the source of the Variational Autoencoder (VAE) framework the authors use. Confidence Score: 0.8\n\n5. \"Self referencing embedded strings (SELFIES) a robust molecular string representation\" [b30]: This paper is mentioned as the source of SELFIES, which the authors use to ensure chemical validity during optimization. Confidence Score: 0.8\n\n6. \"MARS: Markov molecular sampling for multi-objective drug discovery\" [b58]: This paper is mentioned when discussing the limitations of current methods and as a state-of-the-art method that the authors compare their method with. Confidence Score: 0.7\n\n7. \"OptiMol: Optimization of binding affinities in chemical space for drug discovery\" [b5]: This paper is mentioned as a source of inspiration for the authors' work. The authors also discuss the contributions of this paper and how they extend its method. Confidence Score: 0.7\n\n8. \"Inceptionism: Going deeper into neural networks\" [b40]: This paper is mentioned as the source of the inceptionism technique that the authors use. Confidence Score: 0.7\n\nNote that the confidence scores are subjective and are based on the frequency and context of citations in the provided text. The scores range from 0 (not confident) to 1 (very confident).\n\nThe JSON format of the result is:\n\n{ \n\"b24\": 0.9, \n\"b60\": 0.8, \n\"b32\": 0.8, \n\"b29\": 0.8, \n\"b30\": 0.8, \n\"b58\": 0.7, \n\"b5\": 0.7, \n\"b40\": 0.7 \n}", "Based on the given text and instructions, the following papers are likely to have inspired the authors:\n\n1. 'b24': 'junction tree variational autoencoder for molecular graph generation'\n2. 'b60': 'graph convolutional policy network for goal directed molecular graph generation'\n3. 'b29': 'auto encoding variational bayes'\n4. 'b30': 'self referencing embedded strings selfies a robust molecular string representation'\n5. 'b32': 'multi objective de novo drug design with conditional graph generative model'\n6. 'b61': 'moflow an invertible flow model for generating molecular graphs'\n7. 'b37': 'a discrete flow model for molecular graph generation'\n\nThe confidence scores are subjective and can be determined by the number of times each paper is cited or the importance of the contribution it made to the given paper. \n\nThe proposed JSON format answer would be:\n\n{\n   \"b24\": 0.9,\n   \"b60\": 0.8,\n   \"b29\": 0.95,\n   \"b30\": 0.85,\n   \"b32\": 0.75,\n   \"b61\": 0.8,\n   \"b37\": 0.7\n}\n\nThese scores are arbitrary and can be adjusted based on a more detailed analysis of the paper. \n\nA few considerations were made while determining these papers:\n\n- The use of a variational autoencoder (VAE) framework and a novel property predictor network architecture, which is a key concept in the paper, has been influenced by 'b29' and 'b24'. \n\n- The paper also takes inspiration from 'b30' in using self-referencing embedded strings (SELFIES) to ensure chemical validity during optimization.\n\n- The methods for generating molecules with desired properties are similar to those in 'b60', 'b32', 'b61', and 'b37'. \n\nThese papers are mentioned in the context of the novel contributions of the given paper, which indicates that they have directly influenced the research.", "Based on the given text and the author's notes, the key papers that inspired the paper's main contributions are:\n\n{ \"b24\": 0.8, \"b29\": 0.9, \"b30\": 0.9, \"b32\": 0.7, \"b60\": 0.7, \"b27\": 0.7, \"b63\": 0.6, \"b40\": 0.7 }\n\nHere's a brief explanation of these choices:\n\n- Reference \"b24\": The paper uses junction tree variational autoencoders, a technique introduced in this reference. Moreover, the reference is mentioned in the context of improvements and comparisons, indicating it had a direct influence on the paper's contributions.\n- Reference \"b29\": The authors used a Variational Autoencoder (VAE) as a main part of their methodology, indicating this paper as a key source.\n- Reference \"b30\": The paper uses self-referencing embedded strings (SELFIES) to ensure chemical validity during optimization, a method introduced in this reference.\n- Reference \"b32\": It's mentioned in the context of applying a variational autoencoder (VAE) to map SMILES strings to a continuous latent space, a key part of the paper's methodology.\n- Reference \"b60\": This paper is mentioned in the context of logP targeting, a task to which the authors apply their proposed method.\n- Reference \"b27\": This paper is mentioned in the context of reinforcement learning-based molecule generation, a technique the authors compare their method with.\n- Reference \"b63\": This paper is mentioned in the context of reinforcement learning-based molecule generation, a technique the authors compare their method with.\n- Reference \"b40\": The method called Inceptionism, which is a key part of this paper's methodology, was introduced in this reference.\n\nConfidence scores are estimated based on the relevance and depth of the influence of each reference on the main contributions of the paper.", "Based on the text analysis, the following papers have been identified as source papers due to their direct mention and relevance to the author's algorithm and challenges:\n\n1. \"Junction tree variational autoencoder for molecular graph generation\" (b24)\n2. \"Auto encoding variational bayes\" (b29)\n3. \"Self referencing embedded strings selfies a robust molecular string representation\" (b30)\n4. \"Inceptionism going deeper into neural networks\" (b40)\n5. \"Sample efficient optimization in the latent space of deep generative models via weighted retraining\" (b55)\n6. \"Graph convolutional policy network for goal directed molecular graph generation\" (b60)\n\nIn terms of confidence score, considering the direct inspiration and use of methods from these papers in the given paper, the following can be inferred:\n\n{\"b24\": 0.9, \"b29\": 0.85, \"b30\": 0.85, \"b40\": 0.8, \"b55\": 0.8, \"b60\": 0.75}\n\nThese scores reflect the strength of influence of each source paper on the given paper, with a higher score indicating a stronger influence."], "6385788690e50fcafdf4a0f3": ["Based on the text of the academic paper, the primary challenges outlined are the discovery of untrained subnetworks in Graph Neural Networks (GNNs) and their impact on the over-smoothing problem in deep GNNs. The author proposes a new training technique, Untrained GNNs Tickets (UGTs), which discovers matching untrained subnetworks within densely initialized GNNs at high sparsities.\n\nThe key phrases indicating inspiration from previous work are:\n- \"In parallel, works on untrained networks [b10] [b11] surprisingly discover the presence of untrained subnetworks in CNNs that can already match the accuracy of their fully trained dense CNNs with their initial weights, without any weight update.\" \n- \"The existence of such performant subnetworks will extend the recently proposed untrained subnetwork techniques [b10] [b11] in GNNs.\"\n- \"We find that the gradual sparsification technique [b17] [b18] can be a stronger performance booster.\"\n\nBased on these phrases, the papers that significantly inspired the authors are:\n\n- [b10]: \"Deconstructing lottery tickets: Zeros, signs and the supermask\"\n- [b11]: \"What's hidden in a randomly weighted neural network?\"\n- [b17]: \"To prune, or not to prune: exploring the efficacy of pruning for model compression\"\n- [b18]: \"The state of sparsity in deep neural networks\"\n\nFollowing the author's notes, the source paper that inspires the author most could be inferred from the title \"Untrained GNNs Tickets (UGTs)\". The title indicates that the paper is a combination of ideas from [b10] and [b11] which discuss untrained subnetworks and [b17] and [b18] which discuss sparsification techniques. \n\nFinally, the JSON format of the result, with the key as the \u201creference number\u201d and the value as the \u201cconfidence score\u201d would be:\n\n{\n    \"b10\": 0.9,\n    \"b11\": 0.9,\n    \"b17\": 0.9,\n    \"b18\": 0.9\n} \n\nThis means that the four papers are equally influential in the author's work, with a high confidence score of 0.9.", "Based on the given text and the guidance provided, the following references seem to be the most influential sources for this paper:\n\n- \"What\u2019s hidden in a randomly weighted neural network\" [b11] is referred to several times throughout the paper and is directly linked to the main challenge and method of the paper: finding untrained subnetworks in GNNs. The authors of the current paper extend the techniques used in [b11] to GNNs, which is a novel contribution. Confidence score: 0.9\n\n- \"Deconstructing lottery tickets: zeros, signs, and the supermask\" [b10] is another paper that the authors directly build on. They extend the untrained subnetwork techniques proposed in [b10] to GNNs. Confidence score: 0.8\n\n- \"Sparse training via boosting pruning plasticity with neuroregeneration\" [b19]. The authors use the global sparse variant proposed in [b19] to develop their main method (UGTs). Confidence score: 0.8\n\n- \"Deeper insights into graph convolutional networks for semi-supervised learning\" [b13]. The authors use the analysis provided in [b13] to explore the connection between untrained sparse networks and over-smoothing, a main challenge they address in their paper. Confidence score: 0.7\n\n- \"Graph attention networks\" [b4]. The authors validate their method across various GNN architectures including GAT introduced in [b4]. Confidence score: 0.6\n\n- \"How powerful are graph neural networks\" [b3]. This paper is another GNN architecture that the authors use to validate their method. Confidence score: 0.6\n\n- \"Semi-supervised classification with graph convolutional networks\" [b2]. This is another GNN architecture that the authors use to validate their method. Confidence score: 0.6\n\nThe final returned JSON format would be:\n{\n\"b11\": 0.9,\n\"b10\": 0.8,\n\"b19\": 0.8,\n\"b13\": 0.7,\n\"b4\": 0.6,\n\"b3\": 0.6,\n\"b2\": 0.6\n}", "Based on the given text, the paper seems to be primarily motivated by the works of: [b10] \"Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask\", [b11] \"What\u2019s hidden in a randomly weighted neural network\", [b19] \"Sparse Training via Boosting Pruning Plasticity with Neuroregeneration\", and [b18] \"The State of Sparsity in Deep Neural Networks\". The authors make significant references to these works, indicating that they have heavily influenced the development of the paper's core contributions. Therefore the source papers are:\n\n{\n\"b10\": 1,\n\"b11\": 1,\n\"b19\": 0.75,\n\"b18\": 0.75\n}", "{ \"b10\": 0.9, \"b11\": 0.9, \"b19\": 0.85, \"b17\": 0.8, \"b18\": 0.8, \"b12\": 0.7, \"b13\": 0.7, \"b45\": 0.7, \"b16\": 0.6 }", "Based on the given task and paper text, the key contributions of the paper include: the discovery of untrained subnetworks in Graph Neural Networks (GNNs); the proposal of a new training technique named UGTs that surpasses previous methods like Edge-Popup; and the finding that searching for untrained subnetworks in GNNs can mitigate the over-smoothing problem in deep GNNs. \n\nThe following references are cited directly in relation to these contributions and methods:\n\n- 'b0': 'the graph neural network model'\n- 'b10': 'deconstructing lottery tickets zeros signs and the supermask'\n- 'b11': 'what s hidden in a randomly weighted neural network'\n- 'b13': 'deeper insights into graph convolutional networks for semi supervised learning'\n- 'b19': 'sparse training via boosting pruning plasticity with neuroregeneration'\n- 'b17': 'to prune or not to prune exploring the efficacy of pruning for model compression'\n- 'b18': 'the state of sparsity in deep neural networks'\n\nThe confidence score for each reference could be determined by the frequency and context of the citation in the paper. Here, due to the absence of specific context or frequency information, all references are given a uniform score of 1 for simplicity. \n\nThe results in JSON format would be:\n\n```\n{\n  \"b0\": 1,\n  \"b10\": 1,\n  \"b11\": 1,\n  \"b13\": 1,\n  \"b19\": 1,\n  \"b17\": 1,\n  \"b18\": 1\n}\n```"], "6466fafbd68f896efaeb7607": ["Based on the information provided, the source papers that most inspire this paper can be identified as follows:\n\n1. \"BLIP: Bootstrapping Language Image Pretraining with Frozen Image Encoders and Large Language Models\" [b5]. This is mentioned as the inspiration for developing the MedBLIP model and the MedQFormer module to align image and text features. Confidence Score: 0.9\n\n2. \"BioMedLM: A Domain Specific Large Language Model for Biomedical Text\" [b9]. The authors used this as the basic language model and fine-tuned it using the LoRA technique. Confidence Score: 0.8\n\n3. \"Low Rank Adaptation of Large Language Models\" [b2020]. This work is mentioned in the context of fine-tuning the BioMedLM model. Confidence Score: 0.7\n\n4. \"Contrastive Learning from Unpaired Medical Images and Text\" [b8]. This paper is mentioned in the context of previous work on learning representations from large-scale medical image-text pairs. Confidence Score: 0.6\n\n5. \"Vision Language Pre-Training\" [b3]. This paper is mentioned as part of the discussion on vision-language pre-training models and their success in various applications. Confidence Score: 0.6\n\n6. \"Learning Transferable Visual Models from Natural Language Supervision\" [b4]. This is mentioned as one of the representative methods that align image representations with corresponding text representations. Confidence Score: 0.5\n\n7. \"Open Access Series of Imaging Studies (OASIS): Cross-Sectional MRI Data in Young, Middle-Aged, Nondemented and Demented Older Adults\" [b2]. This is one of the datasets used in the paper. Although it's not directly related to the method, it influenced the research design. Confidence Score: 0.4\n\n8. \"The National Alzheimer's Coordinating Center (NACC) Database: The Uniform Data Set\" [b1]. This is also one of the datasets used in the paper. Confidence Score: 0.4\n\nThe confidence scores are estimated based on how directly the referenced work contributed to the core methodology and results of the paper in question.\n\nReturn this result in the json format,\n\n```json\n{\n    \"b5\": 0.9,\n    \"b9\": 0.8,\n    \"b2020\": 0.7,\n    \"b8\": 0.6,\n    \"b3\": 0.6,\n    \"b4\": 0.5,\n    \"b2\": 0.4,\n    \"b1\": 0.4\n}\n```", "Based on the text and the titles of the references, the following source papers seem to have influenced the paper's main contributions:\n\n1. \"BLIP: Bootstrapping language-image pretraining with frozen image encoders and large language models\" [b5]\n2. \"BioMedLM: A domain-specific large language model for biomedical text\" [b9]\n3. \"Low-rank adaptation of large language models\" [b10]\n4. \"VisualGPT: Data-efficient adaptation of pretrained language models for image captioning\" [b20]\n5. \"PaLM-E: An embodied multimodal language model\" [b27]\n6. \"ChatCAD: Interactive computer-aided diagnosis on medical image using large language models\" [b29]\n\nThe confidence scores for these papers are:\n\n```json\n{\n  \"b5\": 0.9,\n  \"b9\": 0.9,\n  \"b10\": 0.8,\n  \"b20\": 0.8,\n  \"b27\": 0.7,\n  \"b29\": 0.7\n}\n```\n\nThese papers are identified based on their direct mention in the text and the context of their citation. The paper takes inspiration from these works to develop their own model for computer-aided diagnosis based on electronic health records.", "Based on the provided text of the paper and authors' notes, the most pertinent source papers could be:\n\n{ \"b5\": 0.9, \"b9\": 0.8, \"b3\": 0.7, \"b35\": 0.7, \"b8\": 0.6, \"b2020\": 0.6 }\n\nExplanation:\n- The paper makes frequent reference to \"BLIP-2 [b5]\", indicating a strong reliance on the methodologies and findings of this paper. Hence, a high confidence score of 0.9 is assigned.\n- The authors use \"BioMedLM [b9]\" as their basic language model, which suggests that this paper is a key source for the current paper's work. Its confidence score is 0.8.\n- The paper references \"VLP models [b3]\" and \"[b35]\" when discussing the methodological foundation of the study, indicating their importance to the authors' research. Thus, they are assigned confidence scores of 0.7.\n- The authors mention the \"Medical Bootstrapping Language-Image Pre-training (MedCLIP) [b8]\" when discussing the challenges faced in their work, indicating that this paper may have influenced their methodology or findings. Its confidence score is 0.6.\n- The authors use the \"LoRA Hu et al. [b2020]\" technique to fine-tune their basic language model, indicating some reliance on this paper. Its confidence score is set to 0.6.", "Based on the text and the given titles of the references, the most pertinent source papers are likely to be:\n\n1. \"BLIP: Bootstrapping Language Image Pretraining with Frozen Image Encoders and Large Language Models\" (b5)\n2. \"BioMedLM: A Domain-Specific Large Language Model for Biomedical Text\" (b9)\n3. \"Low Rank Adaptation of Large Language Models\" (b10)\n4. \"Contrastive Learning from Unpaired Medical Images and Text\" (b8)\n5. \"VisualGPT: Data-Efficient Adaptation of Pretrained Language Models for Image Captioning\" (b20)\n6. \"Flamingo: A Visual Language Model for Few-Shot Learning\" (b22)\n7. \"Scaling Instruction Finetuned Language Models\" (b23)\n\nThe above papers are identified based on the key phrases and their direct impact on the proposed model in the given paper. The confidence score for each paper is difficult to determine without knowing the specific details of the paper and the depth of its reliance on these sources.\n\nThe final result in json format would be:\n\n{\n\"b5\": \"0.9\",\n\"b9\": \"0.9\",\n\"b10\": \"0.8\",\n\"b8\": \"0.7\",\n\"b20\": \"0.7\",\n\"b22\": \"0.7\",\n\"b23\": \"0.6\"\n}\n\nNote: The confidence score is a hypothetical value based on the impact of the references on the given paper, and it may vary in actual scenario.", "Based on the text of the paper, the key phrases, and the titles of the references, the source papers that most inspire the author are:\n\n1. \"BLIP-2: Bootstrapping language image pretraining with frozen image encoders and large language models\" [b5] - This paper is directly mentioned in the text as an inspiration for the proposed MedBLIP model. Confidence score: 1.0\n2. \"BioMedLM: A domain-specific large language model for biomedical text\" [b9] - This model is used as the basic language model in the paper. Confidence score: 1.0\n3. \"Low-Rank Adaptation of Large Language Models\" [b10] - The author uses the technique mentioned in this paper to fine-tune their model. Confidence score: 0.8\n4. \"The Australian Imaging Biomarkers and Lifestyle (AIBL) study of aging: Methodology and baseline characteristics of individuals recruited for a longitudinal study of Alzheimer's disease\" [b11] - This is one of the datasets used for training of the MedBLIP model. Confidence score: 0.6\n5. \"MIRIAD: Public release of a multiple time point Alzheimer's MR imaging dataset\" [b12] - This is another dataset used for training of the MedBLIP model. Confidence score: 0.6\n6. \"Align Before Fuse: Vision and Language Representation Learning with Momentum Distillation\" [b15] - The paper seems to use a similar approach to align different types of medical data. Confidence score: 0.7\n7. \"VisualGPT: Data-Efficient Adaptation of Pretrained Language Models for Image Captioning\" [b20] - The paper appears to use a similar approach of using large language models for vision-language tasks. Confidence score: 0.7\n8. \"ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image Using Large Language Models\" [b29] - This paper seems to be related to the topic of the paper and could have provided inspiration. Confidence score: 0.6\n\nThe result in the json format is:\n\n{\n\"b5\": 1.0,\n\"b9\": 1.0,\n\"b10\": 0.8,\n\"b11\": 0.6,\n\"b12\": 0.6,\n\"b15\": 0.7,\n\"b20\": 0.7,\n\"b29\": 0.6\n}"], "622577a75aee126c0f008d4a": ["{\n    \"b3\": 0.9,\n    \"b5\": 0.9,\n    \"b13\": 0.8,\n    \"b4\": 0.9,\n    \"b8\": 0.8\n}", "{\n    \"b3\": 0.9,\n    \"b5\": 0.9,\n    \"b13\": 0.9,\n    \"b29\": 0.9,\n    \"b4\": 0.95,\n    \"b8\": 0.85\n}", "{\n  \"b3\": 0.9,\n  \"b5\": 0.9,\n  \"b13\": 0.9,\n  \"b4\": 0.8,\n  \"b8\": 0.8\n}", "{\n  \"b3\": 0.9,\n  \"b5\": 0.9,\n  \"b8\": 0.8,\n  \"b13\": 0.8,\n  \"b4\": 0.9,\n  \"b31\": 0.8\n}", "{\n  \"b3\": 0.8,\n  \"b4\": 0.9,\n  \"b5\": 0.8,\n  \"b8\": 0.7,\n  \"b13\": 0.6\n}"], "64741c33d68f896efaa7b7a1": ["Based on the text of the paper and the referred sources, the most pertinent source papers that inspired the author can be identified as follows:\n\n- \"b30: An image is worth x words transformers for image recognition at scale\" [0.9]: The paper heavily referenced the Vision Transformer (ViT) model, indicating its role as a central methodological influence.\n- \"b126: OFA unifying architectures tasks and modalities through a simple sequence to sequence learning framework\" [0.8]: The paper explicitly mentioned being inspired by the OFA framework for its design of BiomedGPT, a unified and generalist model.\n- \"b68: Denoising sequence to sequence pre training for natural language generation translation and comprehension\" [0.7]: The paper used the BART model as the backbone of its proposed BiomedGPT, which is a key factor in its design.\n- \"b2: Flamingo a visual language model for few shot learning\" [0.7]: The paper mentions the development of a generalist AI model, which might have taken inspiration from Flamingo's approach to few-shot learning.\n- \"b56: VILT vision and language transformer without convolution or region supervision\" [0.7]: The paper referred to the VILT model for its approach to pre-training visual/textual embedders and cross-modal modules.\n- \"b11: On the opportunities and risks of foundation models\" [0.6]: This paper seems to have provided a conceptual framework for the development of the BiomedGPT model.\n- \"b147: Generalized radiograph representation learning via cross supervision between images and free text radiology reports\" [0.6]: This paper is referred multiple times, indicating its relevance to the paper's contributions.\n\n```\n{\n\"b30\": 0.9,\n\"b126\": 0.8,\n\"b68\": 0.7,\n\"b2\": 0.7,\n\"b56\": 0.7,\n\"b11\": 0.6,\n\"b147\": 0.6\n}\n```", "Based on the text of the paper and the titles of the references, the most pertinent source papers that inspired the authors of this paper are:\n\n1. \"OFa: Unifying architectures, tasks and modalities through a simple sequence-to-sequence learning framework\" (b126) - The authors explicitly mention being inspired by this paper in developing BiomedGPT, which suggests a high degree of influence. Confidence Score: 0.9\n\n2. \"Attention is All You Need\" (b123) - This paper is the seminal work on Transformers, which BiomedGPT is based on. Confidence Score: 0.8\n\n3. \"An Image is Worth X Words: Transformers for Image Recognition at Scale\" (b30) - This is the paper that introduced the Vision Transformer (ViT), which is a key component of the proposed model. Confidence Score: 0.8\n\n4. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" (b27) - This work is the basis for Transformer-based NLP models, including BiomedGPT. Confidence Score: 0.7\n\n5. \"A Unified Sequence Interface for Vision Tasks\" (b18) - This paper is about using a unified model for various types of data, which is a central concept in the paper. Confidence Score: 0.7\n\n6. \"Pre-training Text Encoders as Discriminators Rather Than Generators\" (b21) - This paper provides a novel method of pre-training, which may have been used in the development of BiomedGPT. Confidence Score: 0.6\n\n7. \"BERT Pre-training of Image Transformers\" (b10) - This paper discusses how to pre-train an image transformer, which is a key part of the BiomedGPT model. Confidence Score: 0.6\n\n8. \"VILT: Vision and Language Transformer Without Convolution or Region Supervision\" (b56) - This paper discusses a multimodal transformer model, which aligns with the BiomedGPT approach. Confidence Score: 0.6\n\nThe results in json format are:\n\n{\n\"b126\": 0.9,\n\"b123\": 0.8,\n\"b30\": 0.8,\n\"b27\": 0.7,\n\"b18\": 0.7,\n\"b21\": 0.6,\n\"b10\": 0.6,\n\"b56\": 0.6\n}", "Based on the given text, the following papers appear to have had a significant influence on the author's work:\n\n- \"On the opportunities and risks of foundation models\" [b11]: The author acknowledges the emergence of transformer-based foundation models as a powerful tool for addressing biomedical challenges. This paper is likely a source of inspiration for the development of the BiomedGPT model.\n\n- \"BERT-derived models\" [b27]: The author mentions the BERT-derived models as a part of the development of the BiomedGPT model. This indicates that the BERT-derived models have influenced the author's work.\n\n- \"The Vision Transformer (ViT)\" [b30]: The author mentions the Vision Transformer (ViT) as the pretraining backbone for various biomedical imaging analysis tasks. This suggests that the Vision Transformer has had a significant influence on the author's work.\n\n- \"The CLIP architecture and its underlying contrastive pretraining\" [b101]: The author mentions the CLIP architecture and its underlying contrastive pretraining. This indicates that the CLIP architecture has influenced the design and development of the BiomedGPT model.\n\n- \"Recent breakthroughs have produced a new class of unified and generalist AI models\" [b2]: This paper is likely to have inspired the development of the BiomedGPT model as a unified and generalist model designed for handling various types of data.\n\n- \"Inspired by OFA\" [b126]: The author directly states they were inspired by OFA in the development of BiomedGPT.\n\nThe confidence scores for these papers would need to be determined based on the amount of influence they appear to have had on the author's work, which is subjective and would require a deeper analysis of the text.\n\nBased on the provided information, the result could be formatted in JSON format as follows:\n\n```json\n{\n    \"b11\": 0.9,\n    \"b27\": 0.85,\n    \"b30\": 0.85,\n    \"b101\": 0.85,\n    \"b2\": 0.9,\n    \"b126\": 1.0\n}\n```\n\nNote: The confidence scores are just approximations and would need to be determined more accurately through a more in-depth analysis of the text.", "Based on the given text, the primary challenge outlined in the paper is to design a transformer-based model that can handle various types of data in the biomedical field, including pure text tasks, pure image tasks, and multimodal tasks. The proposed solution is a new application called BiomedGPT, which employs a unified sequence-to-sequence abstraction for both the pretraining and finetuning stages. \n\nAfter reviewing the text and the titles of the reference papers, the most relevant source papers are:\n\n- \"Attention is all you need\" [b123]: This is a pioneering work on Transformer models, which are the basis of the BiomedGPT model.\n- \"An image is worth 16x16 words: Transformers for image recognition at scale\" [b30]: This paper introduces the Vision Transformer (ViT), a key component of the proposed model.\n- \"OFA: Unifying architectures, tasks, and modalities through a simple sequence to sequence learning framework\" [b126]: The BiomedGPT model is directly inspired by this work.\n- \"BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension\" [b68]: This paper introduces BART, the backbone of the BiomedGPT model.\n- \"Taming Transformers for high-resolution image synthesis\" [b34]: This paper presents the VQ-GAN model used for image quantization in BiomedGPT.\n- \"A unified sequence interface for vision tasks\" [b18]: This paper discusses a unified approach for handling various types of data, which is a key feature of BiomedGPT.\n\nThe assumed confidence score for each reference is given in the following JSON format:\n\n{ \"b123\": 1, \"b30\": 1, \"b126\": 1, \"b68\": 1, \"b34\": 1, \"b18\": 1}\n", "Based on the provided text, the following source papers seem to have influenced the given paper. \n\n- [b1]: 'Multimodal Biomedical AI' (Confidence Score: 0.9)\n- [b2]: 'Flamingo: A Visual Language Model for Few-shot Learning' (Confidence Score: 0.7)\n- [b11]: 'On the Opportunities and Risks of Foundation Models' (Confidence Score: 0.8)\n- [b30]: 'An Image is Worth x Words: Transformers for Image Recognition at Scale' (Confidence Score: 0.9)\n- [b126]: 'OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence to-Sequence Learning Framework' (Confidence Score: 1.0)\n- [b68]: 'Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension' (Confidence Score: 0.7)\n- [b56]: 'VILT: Vision and Language Transformer Without Convolution or Region Supervision' (Confidence Score: 0.8)\n- [b18]: 'A Unified Sequence Interface for Vision Tasks' (Confidence Score: 0.8)\n\nPlease note that these are not the only papers that could have influenced the given text, and the confidence scores are estimates based on the provided information."], "63e312f590e50fcafdc1a1bb": ["Based on the text and the titles of the references provided, the answer could be inferred as:\n\n{\n\"b2\": 0.9,\n\"b6\": 0.8,\n\"b14\": 0.7,\n\"b25\": 0.6,\n\"b18\": 0.5\n}\nThe above five papers seem to have the most direct relevance and influence on the main contributions of the paper, as they are related to the topic of spectral partitioning and multiway partitioning, which are the main challenges and methods discussed in the paper. These papers are also referenced in key parts of the paper, indicating a strong reliance on their research.", "Based on the given text and the titles of the references, the most pertinent source papers are:\n\n1. \"multiway spectral graph partitioning cut functions cheeger inequalities and a simple algorithm\" - This paper is the primary source of the methodology and the cut functions used in the given paper. It's referenced multiple times throughout the text. Confidence score: 0.9\n\n2. \"simple direct and efficient multi way spectral clustering information and inference a\" - This paper is referred to as a source of the initial approximation used in the spectral partitioning algorithm. Confidence score: 0.8 \n\n3. \"multiway spectral partitioning and higher order cheeger inequalities\" - This paper is directly associated with the cut function and the Cheeger inequalities used in the paper. Confidence score: 0.7\n\n4. \"multiway cuts and spectral clustering\" - This paper is referred to as a source of early results on multiway spectral partitioning and cut functions. Confidence score: 0.7\n\n5. \"multiway pspectral graph cuts on grassmann manifolds\" - This paper seems to be related to the algorithms used in the given paper. Confidence score: 0.6\n\n6. \"partitioning well clustered graphs spectral clustering works\" - This paper seems to be related to the algorithms used in the given paper. Confidence score: 0.6\n\n7. \"partitioning sparse matrices with eigenvectors of graphs\" - This paper is referred to as a source of a starting approximation used in the paper. Confidence score: 0.6\n\n{\"b0\": 0.8, \"b2\": 0.8, \"b10\": 0.7, \"b12\": 0.7, \"b14\": 0.6, \"b15\": 0.6, \"b16\": 0.6, \"None\": 0.9}", "Based on the text, the following papers seem to be the most relevant sources:\n\n1. \"Multiway spectral partitioning and higher order Cheeger inequalities\" [b10] - The text refers to this paper when discussing the cut functions used for k-partitioning.\n2. \"Simple, direct, and efficient multi-way spectral clustering\" [b2] - This paper is mentioned as the source of a starting approximation algorithm used in the paper.\n3. \"Partitioning sparse matrices with eigenvectors of graphs\" [b16] - This paper is referred to when discussing the computation of a starting approximation.\n4. \"Many sparse cuts via higher eigenvalues\" [b11] - The authors refer to this paper when discussing Cheeger inequalities for k-partitioning.\n5. \"Multiway spectral partitioning and higher order Cheeger inequalities\" [b10] - This paper is also mentioned in the context of cut functions and Cheeger inequalities.\n\nThe confidence score for each of these papers would be:\n\n{\"b10\": 0.9, \"b2\": 0.8, \"b16\": 0.7, \"b11\": 0.7}", "Based on the text and the titles of the references, the source papers that inspire this academic paper the most could be:\n\n- \"multiway spectral graph partitioning cut functions cheeger inequalities and a simple algorithm\"\n- \"spectral graph theory cbms regional conference series in mathematics number\"\n- \"multiway spectral partitioning and higher order cheeger inequalities\"\n- \"partitioning sparse matrices with eigenvectors of graphs\"\n- \"multiway pspectral graph cuts on grassmann manifolds\"\n- \"partitioning well clustered graphs spectral clustering works\"\n- \"normalized cuts and image segmentation\"\n- \"a tutorial on spectral clustering\"\n\nThe JSON format of the result would be:\n\n```\n{\n    \"b0\": 0.8,\n    \"b1\": 0.7,\n    \"b10\": 0.6,\n    \"b16\": 0.6,\n    \"b14\": 0.75,\n    \"b15\": 0.7,\n    \"b20\": 0.7,\n    \"b22\": 0.7\n}\n```\n\nThe confidence score is estimated based on the relevance of the title to the main topic of the paper and how frequently these papers are mentioned or referred to in the text.", "Based on the paper's content and the provided reference titles, the most pertinent source papers are:\n\n1. \"multiway spectral graph partitioning cut functions cheeger inequalities and a simple algorithm\" - This is heavily referenced throughout the paper, especially in the sections discussing the partitioning algorithm, cut functions, and Cheeger inequalities.\n2. \"measuring the stability of spectral clustering\" - This paper is mentioned in relation to the problem of choosing k from the data, which is a significant aspect of the paper.\n3. \"simple direct and efficient multi way spectral clustering information and inference a\" - This is cited in relation to the novel spectral partitioning method introduced in the paper.\n4. \"multiway spectral partitioning and higher order cheeger inequalities\" - The paper cites this work when discussing the method for spectral partitioning for k > 2.\n5. \"multiway cuts and spectral clustering\" - This paper is referenced in the context of defining cut functions for k-partitioning, which is a key part of the paper's methodology.\n6. \"multiway pspectral graph cuts on grassmann manifolds\" - This is mentioned in connection with the algorithms related to the multiway partitioning algorithm described in the paper.\n7. \"first principles multiway spectral partitioning of graphs\" - This source is referenced when discussing the properties of Lk, which forms a key part of the paper's methodology.\n\nThis leads to the following result, where the confidence score is estimated based on the relevance of the paper to the text and the frequency of the mention in the text:\n\n```\n{\n\"b0\": 0.7,\n\"b2\": 0.9,\n\"b6\": 0.85,\n\"b10\": 0.8,\n\"b14\": 0.75,\n\"b18\": 0.7\n}\n```"], "6487e9fad68f896efa482c3a": ["{\n    \"b2\": 0.95,\n    \"b24\": 0.95,\n    \"b5\": 0.85,\n    \"b6\": 0.85,\n    \"b7\": 0.85\n}", "{\n    \"b2\": 0.95,\n    \"b24\": 0.95,\n    \"b15\": 0.80,\n    \"b26\": 0.80,\n    \"b5\": 0.75,\n    \"b6\": 0.75,\n    \"b7\": 0.75\n}", "{\n    \"b2\": 0.95,\n    \"b24\": 0.95,\n    \"b6\": 0.9,\n    \"b15\": 0.85,\n    \"b26\": 0.85\n}", "{\n    \"b2\": 0.95,\n    \"b24\": 0.95,\n    \"b5\": 0.85,\n    \"b6\": 0.85,\n    \"b7\": 0.85\n}", "{\n    \"b2\": 0.9,\n    \"b5\": 0.85,\n    \"b6\": 0.85,\n    \"b7\": 0.85,\n    \"b24\": 0.9\n}"], "63fec3ce90e50fcafdd70610": ["Based on the provided text and the author notes, the following source papers are identified along with the confidence score:\n\n{\n  \"b1\": 0.8,\n  \"b2\": 0.75,\n  \"b3\": 0.65,\n  \"b4\": 0.7,\n  \"b5\": 0.6,\n  \"b6\": 0.75,\n  \"b7\": 0.6,\n  \"b8\": 0.85,\n  \"b9\": 0.8,\n  \"b10\": 0.7,\n  \"b11\": 0.7,\n  \"b12\": 0.75,\n  \"b13\": 0.6\n}\n\nExplanation:\n\n- The papers b1, b2, b3, b4, and b5 are indicated by the key phrases such as \"adopted from\" and \"inspired by\", showing a strong reliance on previous research for developing the paper's core contributions.\n- The papers b6 and b7 are directly linked to the main challenges and methods outlined in the paper, i.e., \"developing decision support systems or development bots that can assist architects\".\n- The papers b8, b9, b10, and b11 are specifically highlighted, indicating that the author took direct inspiration from these papers to develop the ChatGPT as a DevBot for the architecting process.\n- The paper b12 is used for the scenario-based method, which is a key part of the paper's proposed solution.\n- The paper b13 is referred to for UML, which is a method used in the paper for architectural synthesis.\n\nThe confidence score is assigned based on how closely each reference is related to the paper's novel contributions. For example, b8 has a high score of 0.85 because it directly contributes to the development of ChatGPT, a key contribution of the paper. On the other hand, b13 has a lower score of 0.6 because it is used for UML, a method that is not the paper's main focus.", "Based on the text and the author's notes, the following references seem to be the most influential for the paper:\n\n1. 'b1': 'a general model of software architecture design derived from five industrial approaches' - The authors refer to this paper when describing their methodology, indicating they have taken inspiration from its process-centric approach.\n   \n2. 'b3': 'the lonesome architect' - This paper is mentioned in connection with the main challenge the authors are addressing, namely providing support for the 'lonesome architect'.\n\n3. 'b6': 'how to design a program repair bot insights from the repairnator project' - This paper is mentioned as a source of inspiration for developing decision support systems or development bots.\n\n4. 'b8': 'chatgpt as a support tool for online behavioral task programming' - This paper is referred to as an example of the technology the authors are exploiting, ChatGPT.\n\n5. 'b9': 'engineering education in the era of chatgpt promise and pitfalls of generative ai for education' - This reference is used when discussing the potential role of ChatGPT in software engineering education.\n\n6. 'b12': 'a survey on software architecture analysis methods' - This paper is referred to when discussing the method they have adopted for architectural analysis, synthesis, and evaluation.\n\n7. 'b14': 'replication package for the paper towards human bot collaborative software architecting with chatgpt' - This paper seems to be a direct source related to the case study used in the current paper.\n\nBased on these findings, the resulting JSON format is:\n\n{\n  \"b1\": 0.9,\n  \"b3\": 0.8,\n  \"b6\": 0.85,\n  \"b8\": 0.9,\n  \"b9\": 0.8,\n  \"b12\": 0.85,\n  \"b14\": 0.95\n}\n\nThe confidence score is higher for reference 'b14' as it seems to be a direct source for the case study used in the current paper, while other references have slightly lower scores as they are more indirect influences.", "Based on the text and titles of the references, the most pertinent source papers that inspired the author of the given paper are as follows:\n\n{\n\"b1\": 0.9,\n\"b2\": 0.7,\n\"b3\": 0.8,\n\"b4\": 0.8,\n\"b8\": 0.9,\n\"b9\": 0.7,\n\"b12\": 0.9,\n\"b13\": 0.7\n}\n\nExplanation:\n- b1: The author mentioned \"process-centric approach [b1]\" and \"scenario-based method [b12]\" for the ChatGPT-enabled architectural analysis, synthesis, and evaluation of a microservices-driven software.\n- b2: The author used the term \"process for architecting software [b1], [b2]\". It is hence inferred that b2 is one of the sources.\n- b3: The author refers to the 'lonesome architect' concept from [b3], indicating that it influenced their thinking.\n- b4: The paper mentions the role of AI in software engineering (SE) which is also a topic of research in b4.\n- b8, b9: These references are cited when discussing the role of ChatGPT, which is a core element of the paper's contributions.\n- b12: This reference is directly linked to the method used by the authors for architectural analysis, synthesis, and evaluation.\n- b13: The author used UML for architectural synthesis due to a number of factors, such as available documentation, ease of use, diversity of diagrams, tool support, and wide-scale adoption as a language to represent software systems [b13].", "Based on the text of the paper and the author's notes, the following references appear to be the most pertinent source papers:\n\n1. \"A general model of software architecture design derived from five industrial approaches\" (b1): This paper may have provided the foundational concepts and methods for the process-centric approach and scenario-based method adopted in this study.\n\n2. \"Towards process-centered architecting for quantum software systems\" (b2): The authors of this paper might have adopted some ideas from this paper to apply ACSE in software development endeavours.\n\n3. \"How to design a program repair bot: Insights from the Repairnator project\" (b6): This paper could have been a significant source of inspiration for designing the AI approach in the study, specifically in the development of decision support systems or development bots.\n\n4. \"Towards improving software architecture degradation mitigation by machine learning\" (b7): The authors may have drawn inspiration from this paper to incorporate AI in the architecting process.\n\n5. \"ChatGPT as a support tool for online behavioral task programming\" (b8): This paper might have been crucial for understanding the potential of ChatGPT in software development.\n\n6. \"An analysis of the automatic bug fixing performance of ChatGPT\" (b11): This paper might have provided insights into the capabilities of ChatGPT, which the authors used in this study.\n\n7. \"A survey on software architecture analysis methods\" (b12): This survey could have provided the authors with various methods for software architecture analysis, which they adopted in their study.\n\nBased on the above analysis, here is the result in the requested JSON format:\n\n{\n    \"b1\": 0.8,\n    \"b2\": 0.7,\n    \"b6\": 0.8,\n    \"b7\": 0.7,\n    \"b8\": 0.9,\n    \"b11\": 0.9,\n    \"b12\": 0.8\n}\n\nThe confidence score is based on the analysis of the text, where 1 indicates the highest level of confidence.", "Based on the text and the reference titles, the source papers that most directly influence the main contributions of this paper include:\n\n- \"A general model of software architecture design derived from five industrial approaches\" [b1]: The paper follows a process-centric approach which is directly inspired by this reference.\n\n- \"Towards process centered architecting for quantum software systems\" [b2]: The paper talks about the ACSE process which might be inspired by this reference.\n\n- \"The lonesome architect\" [b3]: The paper discusses the concept of 'lonesome architect' which comes from this reference.\n\n- \"ChatGPT as a support tool for online behavioral task programming\" [b8] and \"Engineering education in the era of ChatGPT: promise and pitfalls of generative AI for education\" [b9]: The paper uses ChatGPT technology which is discussed in these references.\n\n- \"A survey on software architecture analysis methods\" [b12]: The paper uses scenario-based method for ChatGPT-enabled architectural analysis, synthesis, and evaluation which is likely to be inspired by this reference.\n\n- \"Replication package for the paper towards human bot collaborative software architecting with ChatGPT\" [b14]: This reference seems to be closely related to the paper's topic and might contain important source material.\n\nThe confidence score can be inferred as:\n\n {\n  \"b1\": 0.9,\n  \"b2\": 0.8,\n  \"b3\": 0.7,\n  \"b8\": 0.9,\n  \"b9\": 0.9,\n  \"b12\": 0.8,\n  \"b14\": 0.9\n}"], "63aa623e90e50fcafd978bc2": ["Based on the text provided, the following papers seem to be the most influential:\n\n1. \"Scaling language modeling with pathways\" [b13]: This paper is mentioned in relation to the core method of the given paper, which is the Flan-PaLM model. The authors of the given paper have adapted this model to the medical domain, thus this paper is likely to be a significant source of inspiration.\n\n2. \"Instruction prompt tuning to align LLMs to the medical domain\" [b62]: This paper is directly mentioned in the methods section of the paper, indicating it was a substantial influence on the approach taken in the given paper.\n\n3. \"Can large language models reason about medical questions\" [b49]: This paper is mentioned as a close precedent to the work of the given paper, indicating it likely influenced the approach taken.\n\n4. \"MedMCQA -A benchmark for medical question answering\" [b63]: This paper is mentioned as the source of a dataset used in the given paper, indicating it was likely influential in shaping the given paper's research design.\n\n5. \"MedQA (USMLE)\" [b32]: This paper is mentioned as the source of a dataset used in the given paper, indicating it was likely influential in shaping the given paper's research design.\n\n6. \"MultiMedQA -A benchmark for medical question answering\" [b28]: This paper is mentioned as the source of a dataset used in the given paper, indicating it was likely influential in shaping the given paper's research design.\n\n7. \"Language models are few-shot learners\" [b11]: This paper is mentioned in relation to the capabilities of large language models, indicating it was likely influential in shaping the given paper's understanding of the topic.\n\n8. \"On the opportunities and risks of foundation models\" [b9]: This paper is mentioned in relation to the role of language models in medicine, indicating it was likely influential in shaping the given paper's understanding of the topic.\n\nHere is a JSON object for the result:\n\n{\n  \"b13\": 0.9,\n  \"b62\": 0.8,\n  \"b49\": 0.8,\n  \"b63\": 0.7,\n  \"b32\": 0.7,\n  \"b28\": 0.7,\n  \"b11\": 0.7,\n  \"b9\": 0.7\n}", "Based on the given text of the academic paper, the main challenges outlined were to adapt large language models (LLMs) to the medical domain, and to evaluate the performance of these models in medical question answering. The novel method proposed by the authors is the \"instruction prompt tuning\", which is used to align the LLM Flan-PaLM to the medical domain, resulting in the Med-PaLM model.\n\nFrom the text, the following source papers can be identified as the most pertinent:\n\n1. \"Scaling language modeling with pathways\" [b13] - The authors of this paper presumably inspired the use of Flan-PaLM in the medical domain.\n2. \"Training language models to follow instructions with human feedback\" [b62] - This paper seems to have inspired the technique of \"instruction prompt tuning\" that was used to adapt Flan-PaLM to the medical domain.\n3. \"Can large language models reason about medical questions\" [b49] - This paper was likely a source of inspiration for the authors' approach to evaluating the reasoning capability of LLMs in the medical question answering context.\n4. \"A large language model for science\" [b78] - This paper, which introduced Galactica, a LLM for science, was probably a motivation for the authors to develop a similar model for the medical domain.\n\nUpon evaluating the titles of all referenced papers, these four seem to be the most directly related to the paper's novel contributions.\n\nHere is the result in json format:\n{\"b13\": 0.9, \"b62\": 0.95, \"b49\": 0.85, \"b78\": 0.8}", "Based on the provided text, the following references seem to be the most relevant source papers for this work:\n\n- 'b13': \"Scaling language modeling with pathways\" \n- 'b49': \"Can large language models reason about medical questions?\"\n- 'b90': \"Chain of thought prompting elicits reasoning in large language models\"\n- 'b11': \"Language models are few-shot learners\"\n- 'b62': \"Training language models to follow instructions with human feedback\"\n\nAlthough the paper does not directly mention phrases such as \"inspired by\" or \"motivated by\", these papers are heavily referenced throughout the text, particularly in the sections discussing the methodologies and approaches of the author's research. \n\nAccording to the information provided, the authors have based their work on the Flan-PaLM model, and have used the method of Instruct Tuning. They have also used the approach of chain-of-thought to complete inference tasks. The above-mentioned papers seem to be directly related to these topics and methods. \n\nReturning the results in JSON format:\n\n```\n{\n  \"b13\": 0.9,\n  \"b49\": 0.9,\n  \"b90\": 0.9,\n  \"b11\": 0.85,\n  \"b62\": 0.85\n}\n```\nThe confidence scores are subjective and estimated based on the frequency and context of the citations within the paper's text.", "{\n\"b13\": 0.9,\n\"b14\": 0.8,\n\"b49\": 0.8,\n\"b62\": 0.7,\n\"b90\": 0.7\n}", "Based on the text of the academic paper and the titles of the references, the most pertinent source papers seem to be:\n\n1. \"Scaling language modeling with pathways\" (b13): The paper's authors directly mention the use of Flan-PaLM, which appears to be derived from or inspired by the work in this paper.\n\n2. \"Can large language models reason about medical questions?\" (b49): The medical question answering context and the use of Instruct GPT-3, an instruction-tuned LLM, referenced in this paper appear to have directly influenced the work of the authors.\n\n3. \"Pretrained language models for biomedical and clinical tasks understanding and extending the state of the\" (b45): This paper might have influenced the approach of using LLMs in the medical field.\n\n4. \"Training language models to follow instructions with human feedback\" (b62): The instruction prompt tuning method proposed in the paper appears to be inspired by the method from this paper.\n\n5. \"MedMCQA a large scale multi subject multi choice dataset for\" (b63): The authors have used this dataset for their research, which indicates a strong reliance on the work done in this paper.\n\n6. \"A large language model for science\" (b78): The authors have used the idea of a large language model for a specific field, in this case, science, to inspire their work in medicine.\n\n7. \"Chain of thought prompting elicits reasoning in large language models\" (b90): The authors have used the chain-of-thought prompting method proposed in this paper to tackle the reasoning task.\n\nThe confidence score for each of these papers depends on the direct relevance to the paper's novel contributions, but all of them seem to have a significant influence. The JSON format would be:\n\n{\n    \"b13\": 1,\n    \"b49\": 1,\n    \"b45\": 0.8,\n    \"b62\": 0.9,\n    \"b63\": 0.8,\n    \"b78\": 0.8,\n    \"b90\": 1\n}"], "63f2e4ae90e50fcafd283025": ["Based on the provided text, it appears that the following papers have directly inspired the author's work based on the discussion of their methods and subsequent application/modification in the current paper. \n\nThe following \"reference number\" and \"confidence score\" pairs are inferred based on the language used in the text and how directly the works appear to influence the main contributions of the paper:\n\n1. \"b15\": 0.9 - The text discusses the Equivariant Diffusion Model (EDM) proposed in this paper and its limitations, which the authors of the current paper aim to overcome with their MiDi model. The authors also compare their results with the EDM model, indicating a direct inspiration.\n   \n2. \"b46\": 0.8 - The text discusses the use of discrete diffusion for graph generation as proposed in this work, and this method is then used in the current paper's MiDi model.\n   \n3. \"b41\": 0.7 - The authors mention the use of diffusion models in their MiDi model, specifically referencing this paper. \n\n4. \"b11\": 0.7 - The authors reference the effectiveness of discrete diffusion for graph generation as proposed in this paper, and this method is then used in their MiDi model.\n\n5. \"b38\": 0.6 - The authors discuss the use of EGNN layers in their model, which is proposed in this paper.\n\n6. \"b51\": 0.6 - The authors reference this paper in their discussion of ensuring SE(3)-equivariance in their model.\n\n7. \"b35\": 0.5 - Mentioned as concurrent work with unique contributions, the current paper appears to draw inspiration from this work.\n\n8. \"b16\": 0.5 - This paper is also mentioned as concurrent work with unique contributions, indicating a possible source of inspiration for the current paper.\n\nThe confidence scores are assuming that a score of 1 indicates a very high level of confidence in the source paper's influence on the current work, while a score of 0 indicates no confidence.", "Based on the given text, the author has mentioned several papers that have directly influenced their work. These are:\n\n1. \"Equivariant diffusion for molecule generation in 3D\" [b15]: This paper is directly mentioned in the context of the drawbacks of existing models and how they have inspired the current work to address these issues.\n\n2. \"Denoising diffusion probabilistic models\" [b13]: This work is referred to when discussing the approach of the current paper, indicating that it has provided inspiration for the model.\n\n3. \"Discrete denoising diffusion for graph generation\" [b46]: This paper seems to have provided inspiration for the approach of the current paper, specifically in the use of discrete diffusion for graph generation.\n\n4. \"Structured denoising diffusion models in discrete state spaces\" [b0]: The paper is referred to when discussing the noise model, indicating that it has provided inspiration for the model.\n\n5. \"Mudiff: Unified diffusion for complete molecule generation\" [b16]: This paper is referred to in the related work section and seems to have provided some inspiration for the current work.\n\n6. \"A generalization of transformer networks to graphs\" [b8]: The current paper seems to have been inspired by the transformer architecture mentioned in this paper.\n\n7. \"Equivariant graph neural networks\" [b38]: The EGNN layers mentioned in this paper have been used in the current work, indicating that it has provided inspiration.\n\nBased on this analysis, the json format answer would be:\n\n{\n\"b15\": 0.9,\n\"b13\": 0.85,\n\"b46\": 0.85,\n\"b0\": 0.8,\n\"b16\": 0.75,\n\"b8\": 0.7,\n\"b38\": 0.7\n}", "Based on the text and author notes, the following references appear to be the most pertinent source papers that inspired the paper:\n\n1. \"Equivariant Diffusion for Molecule Generation in 3D\" [b15]: The text mentions this paper various times, indicating that it was a significant source of inspiration for the current paper. The authors of the current paper are evidently building on the 3D molecule generation approach proposed in [b15], but improving upon it by generating a molecular graph and 3D coordinates simultaneously.\n\n2. \"Diffusion Models for Graphs Benefit from Discrete State Spaces\" [b46]: This paper is mentioned as the basis for the noise model used in the current paper, suggesting that it played an important role in the development of the current paper's methodology.\n\n3. \"MUDIFF: Unified Diffusion for Complete Molecule Generation\" [b16]: The text mentions that [b16] proposes richer positional encodings for the transformer layer, indicating that it may have influenced the current paper's method for handling positional encodings.\n\n4. \"MolGenSurvey: A Systematic Survey in Machine Learning Models for Molecule Design\" [b7]: This paper appears to be a source of inspiration as it provides a comprehensive overview of machine learning models for molecule design, which is the field within which the current paper is situated.\n\n5. \"Equivariant Diffusion Model (EDM)\" [b15] and \"Mixed Graph+3D denoising diffusion (MiDi) model\": Both these models are mentioned multiple times and seem to have a direct influence on the current research.\n\nHere is the result in the json format:\n\n{\n  \"b15\": 0.9,\n  \"b46\": 0.8,\n  \"b16\": 0.7,\n  \"b7\": 0.7\n}", "Based on the text of the paper and the author notes, the following papers seem to be the most likely sources of inspiration for this work:\n\n1. \"Equivariant Diffusion for Molecule Generation in 3D\" (b15)\nThe paper heavily references and draws upon the methods of this paper, particularly in the generation of 3D molecules and use of diffusion models. This paper is directly mentioned in the context of the author's own approach and method, suggesting a high level of influence.\n\n2. \"Denoising Diffusion Probabilistic Models\" (b13)\nThe author's method is a denoising diffusion model, suggesting this paper provided significant inspiration in the development of their algorithm. \n\n3. \"Structured Denoising Diffusion Models in Discrete State Spaces\" (b0)\nThe author's model uses a noise model that depends on the data type, similar to the approach proposed in this paper. \n\n4. \"Diffusion Models for Graphs Benefit from Discrete State Spaces\" (b11)\nThe author's method uses a discrete diffusion for atom types, formal charges and bond types, similar to the approach proposed in this paper.\n\n5. \"Equivariant Graph Neural Networks\" (b38)\nThe author's model uses relaxed Equivariant Graph Neural Networks (rEGNNs) for processing the coordinates, which are similar to the EGNN layers proposed in this paper.\n\n6. \"Geom-Drug: Energy-Annotated Molecular Conformations for Property Prediction and Molecular Generation\" (b1)\nThe author's model is tested on the GEOM-DRUGS dataset, suggesting that this paper could have inspired the author's choice of dataset and therefore had significant influence on the results obtained.\n\n7. \"MolGenSurvey: A Systematic Survey in Machine Learning Models for Molecule Design\" (b7)\nThe paper gives a survey of machine learning models for molecule design, and the author's model is a machine learning model for molecule design, suggesting this paper provided significant inspiration in the development of their algorithm.\n\nThe confidence scores are estimated by the frequency of citation and the relevance of the topics in the source papers to the author's paper.\n\n{ \"b15\": 0.9, \"b13\": 0.8, \"b0\": 0.8, \"b11\": 0.8, \"b38\": 0.7, \"b1\": 0.6, \"b7\": 0.7 }", "Based on the given text and author notes, the following source papers seem to be the most relevant:\n\n1. \"Equivariant Diffusion Model (EDM) [b15]\" - The paper explicitly refers to this work as a baseline and significantly improves upon its results. It also mentions the EDM model was later extended in other works [b18] and [b33], suggesting that these may also have been influential.\n2. \"Mixed Graph+3D denoising diffusion (MiDi) [b0]\" - This is the model proposed by the authors and the paper is directly about the development and testing of this model.\n3. \"[b16]\" and \"[b35]\" - These works are mentioned as concurrent to the authors' work and propose similar 2D+3D diffusion models for molecule generation. They seem to have influenced the authors' thinking and approach.\n4. \"[b11]\" and \"[b46]\" - These works found that discrete diffusion is more effective for graph generation, which the authors' model also uses.\n5. \"[b38]\" - The EGNN layers from this work are used in the authors' model.\n6. \"[b41]\" - This work is cited in the context of modern diffusion models, which the authors' model is a part of.\n7. \"[b51]\" - This work is cited in the context of protein generation models, which could have been a source of inspiration for the authors' molecule generation model.\n\nIn JSON format, the confidence scores for these source papers could be represented as:\n\n{\n  \"b15\": 1.0,\n  \"b0\": 1.0,\n  \"b16\": 0.85,\n  \"b35\": 0.85,\n  \"b11\": 0.8,\n  \"b46\": 0.8,\n  \"b38\": 0.75,\n  \"b41\": 0.7,\n  \"b51\": 0.65\n}"], "64be53113fda6d7f06326238": ["Based on the provided text of the paper and the task at hand, the following references are identified as potential sources that the authors rely on or are inspired by:\n\n{ \"b5\": 0.9,\n \"b15\": 0.8,\n \"b2\": 0.7,\n \"b1\": 0.65,\n \"b25\": 0.6,\n \"b42\": 0.55,\n \"b7\": 0.5,\n \"b17\": 0.45 }\n\nExplanation:\n- [b5] \"QASPER\" is directly compared to the proposed QASA approach, indicating a strong influence.\n- [b15] \"Dense Passage Retrieval for Open Domain Question Answering\", is mentioned in the context of the retrieval-then-read approach, which could form the basis of the QASA method.\n- [b2] \"LongFormer\" is mentioned as an alternative to the proposed multi-step subtask approach, suggesting its methodological influence.\n- [b1] \"Query-focused Abstractive Summarization\" is compared to QASA in the discussion of multi-document summarization.\n- [b25] \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" is possibly associated with the use of large language models in the QASA method.\n- [b42] \"Discourse Structure of Long-form Answers\" is cited during the discussion of evidential rationale-generation, a key aspect of the QASA method.\n- [b7] \"Questions and Challenges for the New Psychology of Reasoning\" is cited in the context of the dual process model of cognitive reasoning, which forms the theoretical basis of the QASA method.\n- [b17] \"Natural Questions\" is compared to QASA in the context of reading comprehension tasks, indicating its probable influence.\n\nNote: The confidence scores are subjective and reflect the strength of the inferred relationship between the source and the paper based on the provided text. They do not reflect the actual importance of these sources to the authors.", "Based on the text of the academic paper and its references, the source papers that most inspired the author could be:\n\n1. \"Dense passage retrieval for open domain question answering\" [b15]: The paper cites this work in the context of discussing approaches to open-domain QA and how they differ from their own approach. The reference to this paper suggests it could have influenced the development of their proposed QASA method.\n   \n2. \"Towards aicomplete question answering a set of prerequisite toy tasks\" [b41]: This reference appears in the discussion of reasoning tasks and their limitations, which seems to have motivated the development of the QASA benchmark and approach.\n\n3. \"A dataset of information seeking questions and answers anchored in research papers\" [b5]: This paper is mentioned when the authors discuss QA on academic research papers and how their work differs from previous efforts. The comparison indicates that this paper was a significant reference point for their work.\n\n4. \"Question asking during tutoring\" [b9] and \"Mechanisms that generate questions questions and information systems\" [b10]: Both references are used in the discussion of the preliminary study and the development of their question schema, suggesting that these papers played a critical role in shaping their approach to question generation.\n\n5. \"Dense hierarchical retrieval for open domain question answering\" [b21]: This paper is mentioned in the context of the Retrieve-then-read approach, indicating its relevance to the proposed QASA method.\n\n6. \"Factoid questions meet long form answers\" [b32]: This paper is referred to during the discussion of ELI5 and ASQA datasets. It seems to have influenced the authors' approach to handling long-form question answering.\n\n7. \"How do we answer complex questions discourse structure of long form answers\" [b42]: This paper is cited in the context of the evidential rationale-generation subtask, suggesting its influence on the methodological aspect of the QASA approach.\n\n8. \"Large language models encode clinical knowledge\" [b30]: This paper is cited when discussing the use of large language models in their QASA approach.\n\nThus, the confidence score for each source paper would be:\n\n{\n  \"b15\": 0.8,\n  \"b41\": 0.7,\n  \"b5\": 0.7,\n  \"b9\": 0.7,\n  \"b10\": 0.7,\n  \"b21\": 0.7,\n  \"b32\": 0.8,\n  \"b42\": 0.9,\n  \"b30\": 0.8\n}", "Based on the given text, the following source papers can be identified: \n\n1. \"Dual process models theorize that cognitive reasoning is a two-stage process\" [b39], [b36], [b7]\n2. \"Reading Comprehension (RC) is one type of reasoning task that can formulate various questions and answers. SQuAD [b27], NewsQA [b35], DROP [b6], and Natural Questions [b17] have been proposed.\"\n3. \"Recent work on open-domain QA [b15][b11][b21][b13][b14] exploits the Retrieve-then-read approaches\"\n4. \"Some reasoning tasks like bAbI and its permuted version [b41][b26] require logically correct spatial reasoning.\"\n5. \"QA for Academic Research Papers Several datasets have been proposed for QA on academic research papers including emrQA [b23], BioRead [b24], and BioMRC [b31]. Closest to our work, QASPER [b5]\"\n6. \"Open-domain Long-form QA ELI5 [b8] collected open-domain questions with paragraph-level answers collected from Reddit forum and extracted the relevant sentences from web documents\"\n7. \"ASQA [b32] requires to answer all the sub-questions over multiple passages.\"\n8. \"Query-focused Multi-Document Summarization (qMDS) For qMDS, some datasets in various domains have been proposed, such as QMSum for meeting transcripts [b44], Squality for science fiction [b37], and AQuaMuSe for wikipedia [b16].\"\n\nThe JSON format of the result is:\n\n{\n\"b39\": 0.8,\n\"b36\": 0.8,\n\"b7\": 0.8,\n\"b27\": 0.8,\n\"b35\": 0.8,\n\"b6\": 0.8,\n\"b17\": 0.8,\n\"b15\": 0.8,\n\"b11\": 0.6,\n\"b21\": 0.6,\n\"b13\": 0.6,\n\"b14\": 0.6,\n\"b41\": 0.6,\n\"b26\": 0.6,\n\"b23\": 0.6,\n\"b24\": 0.6,\n\"b31\": 0.6,\n\"b5\": 0.9,\n\"b8\": 0.8,\n\"b32\": 0.7,\n\"b44\": 0.7,\n\"b37\": 0.7,\n\"b16\": 0.7\n} \n\nThe confidence scores are estimated based on the context in which the papers were mentioned and the relevance of the topics discussed in these papers to the main topic of the given paper.", "Based on the given text of the academic paper, the source papers which are indicated by key phrases that inspire the author most are as follows:\n\n{ \"b5\": 0.9, \"b42\": 0.85, \"b28\": 0.8, \"b22\": 0.75, \"b8\": 0.7, \"b16\": 0.65, \"b2\": 0.6, \"b1\": 0.55 } \n\nHere, \"b5\" represents the paper titled \"A dataset of information-seeking questions and answers anchored in research papers\", \"b42\" represents \"How do we answer complex questions? Discourse structure of long-form answers\", \"b28\" is \"Know what you don't know: Unanswerable questions for SQuAD\", \"b22\" is \"S2ORC: The Semantic Scholar Open Research Corpus\", \"b8\" is \"Long-form question answering\", \"b16\" is \"Automatically generating datasets for query-based multi-document summarization\", \"b2\" is \"The Long Document Transformer\", and \"b1\" is \"Query-focused abstractive summarization: Incorporating query relevance, multi-document coverage, and summary length constraints into seq2seq models\". \n\nThe confidence score indicates the relevance of each source paper to the given academic paper. The higher the score, the more relevant the source paper is to the given paper.", "Based on the given academic paper text, these are the source papers that appear to have most influenced the author's work:\n\n1. \"Reasoning in vector space: An exploratory study of question answering\" [b19]\n2. \"SQuAD: Questions for machine comprehension of text\" [b27]\n3. \"Question asking during tutoring\" [b9]\n4. \"How do we answer complex questions? Discourse structure of long-form answers\" [b42]\n5. \"Towards AI-complete question answering: A set of prerequisite toy tasks\" [b41]\n6. \"The Long Document Transformer\" [b2]\n7. \"Dense Passage Retrieval for Open-Domain Question Answering\" [b15]\n\nThe confidence score for each source paper is determined by key phrases indicating direct inspiration or motivation, the relevance to the paper's novel contributions, and the methodological similarity. For example, paper [b19] might have a confidence score of 0.9 as it directly deals with reasoning in question-answering, a central theme in the given paper. \n\nPlease note that this answer assumes the paper's author has correctly cited their sources and that the mentioned papers indeed have significant influence on the work. Also, the provided titles of the reference papers are used for inference, the actual content of these papers might lead to a different conclusion. \n\nThe JSON format of the result would be:\n{\n  \"b19\": 0.9,\n  \"b27\": 0.85,\n  \"b9\": 0.8,\n  \"b42\": 0.9,\n  \"b41\": 0.8,\n  \"b2\": 0.85,\n  \"b15\": 0.8\n}"], "64a63bddd68f896efaec64af": ["Based on the provided text of the academic paper, the source papers that most inspire the author could be identified as follows:\n\n1. \"Translating embeddings for modeling multirelational data\" [b1]: The paper mentions graph-based KGC models and refers to this source, indicating that it has influenced the author's understanding of entity and relation representation using trainable embeddings.\n\n2. \"Convolutional 2D knowledge graph embeddings\" [b7]: This source is mentioned in the context of explaining how previous methods have been effective in modeling KG structural information but unable to incorporate linguistic context, which the current paper aims to address.\n\n3. \"Prefix tuning: optimizing continuous prompts for generation\" [b18]: This paper is directly mentioned when discussing the concept of Soft Prompt, indicating that it has influenced the development of the novel CSProm-KG model.\n\n4. \"The power of scale for parameter-efficient prompt tuning\" [b16]: This paper is also mentioned in the context of discussing the Soft Prompt concept, suggesting it has provided important inspiration for the author's research.\n\n5. \"Explaining and harnessing adversarial examples\" [b11]: This source is referred to when discussing the proposed Local Adversarial Regularization method, indicating that it has influenced the author's approach to enhancing the model's ability to distinguish textually similar entities.\n\n6. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b8]: Although not directly cited in the text, the use of pretrained language models (PLMs) and the problem of text overfitting in the context of KGC might have been inspired by this paper.\n\nUsing the confidence score estimation method, the result can be returned in the JSON format as follows:\n\n{\"b1\": 0.9, \"b7\": 0.8, \"b18\": 0.95, \"b16\": 0.95, \"b11\": 0.85, \"b8\": 0.75}", "Based on the provided text and titles of references, the source papers that most likely inspired the given paper are as follows:\n\n1. \"Translating embeddings for modeling multirelational data\" [b1]: The given paper discusses the use of entity and relation embeddings, which is a concept introduced in this source paper.\n\n2. \"Convolutional 2D Knowledge Graph Embeddings\" [b7]: The given paper mentions ConvE as a graph-based KGC model, which originates from this source paper.\n\n3. \"The power of scale for parameter-efficient prompt tuning\" [b16]: The given paper uses the concept of Conditional Soft Prompts, which is a structure-aware version of Soft Prompts. The idea of Soft Prompts comes from this source paper.\n\n4. \"Prefix tuning: Optimizing continuous prompts for generation\" [b18]: Same as above, this source paper is about Soft Prompts which is a key concept in the given paper.\n\n5. \"A Structure-augmented Text Representation Learning for Efficient Knowledge Graph Completion in WWW The Web Conference Virtual Event Ljubljana\" [b33]: The given paper refers to a problem that PLM-based models focus too much on textual information and overlook the graph structure, which is a topic discussed in this source paper.\n\n6. \"KG-BERT: BERT for Knowledge Graph Completion\" [b42]: This source paper applies Pretrained Language Models (PLMs) to Knowledge Graph Completion (KGC), which is a method also used in the given paper.\n\nBased on the analysis, the returned result in JSON format is:\n```\n{\n    \"b1\": 0.9,\n    \"b7\": 0.8,\n    \"b16\": 1.0,\n    \"b18\": 1.0,\n    \"b33\": 0.8,\n    \"b42\": 0.9\n}\n```\nThe confidence scores are estimated based on how central the topics from the source papers are to the main contributions of the given paper.", "Based on the text of the academic paper, the most pertinent source papers that inspired the author are:\n\n1. \"b1: Translating embeddings for modeling multirelational data\" - Mentioned as a representation of entities and relations using trainable embeddings. Confidence Score: 0.85\n2. \"b18: Prefix tuning optimizing continuous prompts for generation\" - Mentioned as an inspiration for the creation of the Conditional Soft Prompt. Confidence Score: 0.95\n3. \"b33: A structure augmented text representation learning for efficient knowledge graph completion\" - Referenced as an example of using PLM outputs to evaluate the plausibility of a given fact. Confidence Score: 0.75\n4. \"b35: Promda prompt based data augmentation for lowresource nlu tasks\" - Referenced as an example of how Soft Prompts can effectively avoid the issue of over-fitting towards textual information. Confidence Score: 0.80\n5. \"b42: Kg bert bert for knowledge graph completion\" - Cited as an example of PLM-based methods applied to fill the gap in KG structural information. Confidence Score: 0.75\n\nThe JSON format of the result is:\n{\n\"b1\": 0.85,\n\"b18\": 0.95,\n\"b33\": 0.75,\n\"b35\": 0.80,\n\"b42\": 0.75\n}", "Based on the text and the titles of the references, the source papers that most inspire the given paper are:\n- \"Translating embeddings for modeling multirelational data\" [b1]: This paper is mentioned when discussing the graph-based KGC models, which seems to be a critical part of the proposed solution in the given paper.\n- \"Convolutional 2D knowledge graph embeddings\" [b7]: Also mentioned in the context of graph-based KGC models.\n- \"Prefix tuning: optimizing continuous prompts for generation\" [b18]: Mentioned when discussing the original idea of Soft Prompts which the paper extends to create Conditional Soft Prompts.\n- \"The power of scale for parameter-efficient prompt tuning\" [b16]: Same as above, indicating a strong reliance on the concept of Soft Prompts.\n- \"A structure-augmented text representation learning for efficient knowledge graph completion in WWW the web conference virtual event Ljubljana\" [b33]: Mentioned in the context of PLM-based methods.\n- \"KG BERT: BERT for knowledge graph completion\" [b42]: This paper is mentioned as a part of PLM-based methods, and it seems to be the inspiration for using PLM in KGC tasks.\n- \"Promda: prompt-based data augmentation for low-resource NLU tasks\" [b35]: This paper is mentioned when discussing the effectiveness of Soft Prompts in avoiding the over-fitting issue, which is a key motivation for the proposed model in the given paper.\n- \"Explaining and harnessing adversarial examples\" [b11]: This paper is mentioned when discussing Local Adversarial Regularization, a technique proposed to improve the robustness of the model.\n\nTherefore, the JSON format would look like this:\n{\"b1\": 0.9, \"b7\": 0.8, \"b18\": 1, \"b16\": 0.9, \"b33\": 0.7, \"b42\": 0.7, \"b35\": 0.8, \"b11\": 0.8}", "Based on the provided text and the title of the references, the most pertinent source papers that inspired the author appear to be:\n\n1. \"Prefix Tuning: Optimizing Continuous Prompts for Generation\" [b18]: The author makes specific reference to \"Soft Prompt\" which is a concept introduced and deeply explained in this paper. The author also takes inspiration from this paper to propose a novel Conditional Soft Prompt. Confidence score: 0.9\n\n2. \"The Power of Scale for Parameter-efficient Prompt Tuning\" [b16]: It is mentioned along with [b18], and seems to provide foundational knowledge for the author's work on Soft Prompts. Confidence score: 0.8\n\n3. \"Translating Embeddings for Modeling Multi-relational Data\" [b1]: The author refers to this paper while discussing Graph-based KGC models and their significance in representing entities and relations using trainable embeddings. Confidence score: 0.8\n\n4. \"Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b8]: This is a foundation paper for PLMs, and the author's work is based on PLMs. Confidence score: 0.7\n\n5. \"Explaining and Harnessing Adversarial Examples\" [b11]: The author refers to this paper while discussing the Local Adversarial Regularization used to enhance CSProm-KG's ability to distinguish textually similar entities. Confidence score: 0.8\n\n6. \"KG-BERT: BERT for Knowledge Graph Completion\" [b42]: The author refers to this paper while discussing PLM-based methods for KGC. Confidence score: 0.7\n\n7. \"A Structure-augmented Text Representation Learning for Efficient Knowledge Graph Completion in WWW\" [b33]: It is mentioned along with [b42] in the context of PLM-based methods for KGC. Confidence score: 0.7\n\nIn JSON format, the results would be:\n\n{\n\"b18\": 0.9,\n\"b16\": 0.8,\n\"b1\": 0.8,\n\"b8\": 0.7,\n\"b11\": 0.8,\n\"b42\": 0.7,\n\"b33\": 0.7\n}"], "64cc77b33fda6d7f06aebd0d": ["Based on the given text, the source papers that inspire the author most can be identified as:\n\n1. \"AudioLDM: Text-to-Audio Generation with Latent Diffusion Models\" (b23)\n2. \"Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation\" (b38)\n3. \"Mixup: Beyond Empirical Risk Minimization\" (b40)\n4. \"Beat Transformer: Demixed Beat and Downbeat Tracking with Dilated Self-Attention\" (b41)\n5. \"High-Resolution Image Synthesis with Latent Diffusion Models\" (b30)\n6. \"Stable Diffusion for Real-Time Music Generation\" (b7)\n\nThe JSON format of the result: \n{\n    \"b23\": 1,\n    \"b38\": 1,\n    \"b40\": 1,\n    \"b41\": 1,\n    \"b30\": 0.8,\n    \"b7\": 0.8\n} \n\nThe confidence score is set to 1 for the first four references as they are directly mentioned in the methodology section and are clearly essential to the novel contributions of the paper. The latter two references (b30 and b7) are also important, but their influence is slightly less direct, hence the lower confidence score.", "Based on the given text, the source papers that inspire the current paper the most are:\n\n1. \"AudioLDM [b23] : Text to Audio Generation with Latent Diffusion Models\" - The author has adapted the methodologies and architectures of this paper to the music domain and has also used it as the base for their text-to-music generation model, MusicLDM.\n2. \"Stable Diffusion [b30]: High-resolution image synthesis with latent diffusion models\" - The methodologies and architectures of this paper are also adapted to the music domain in the construction of the MusicLDM model.\n3. \"Beat Transformer [b41]\" - The beat tracking model from this paper is used for preprocessing in the authors' mixup strategies, BAM and BLM.\n4. \"Mixup [b40]: Beyond Empirical Risk Minimization\" - The authors adapt the idea of mixup from this paper to augment training data and improve model performance.\n5. \"CLAP [b38]: Large scale contrastive language audio pretraining with feature fusion and keyword-to-caption augmentation\" - This paper's methodologies are adapted for extracting semantic embeddings of text and audio, and for testing plagiarism and novelty in text-to-music generation.\n\nBased on the above analysis, the source papers are: \n{\"b23\": 1, \"b30\": 0.8, \"b41\": 0.8, \"b40\": 0.8, \"b38\": 0.8 }", "Based on the provided text and notes, the author's work is primarily influenced by the following papers:\n\n1. 'AudioLDM: Text-to-audio generation with latent diffusion models [b23]': The author has adopted the architecture of AudioLDM and adapted it to the music domain, creating their own model, MusicLDM. This indicates a strong reliance on the methods proposed in this paper. Confidence Score: 1.0\n\n2. 'Large-scale contrastive language-audio pretraining with feature fusion and keywordto caption augmentation [b38]': The author uses the CLAP model from this paper as an integral part of their technique for generating text-conditioned musical audio. Confidence Score: 1.0\n\n3. 'Beat Transformer: Demixed beat and downbeat tracking with dilated self attention [b41]': The author has used the Beat Transformer model from this paper for beat tracking in their beat-synchronous mixup strategies, indicating direct inspiration. Confidence Score: 1.0\n\n4. 'Mixup: Beyond empirical risk minimization proc iclr [b40]': The author has applied the mixup strategy from this paper to address data limitations and plagiarism in music generation, showing direct application of the strategy proposed in this paper. Confidence Score: 0.9\n\n5. 'Highresolution image synthesis with latent diffusion models [b30]': The author has adapted the Stable Diffusion architecture from this paper to the music domain, as part of their MusicLDM model. Confidence Score: 0.8\n\n6. 'Denoising diffusion probabilistic models [b12]': The author's work involves training objectives from this paper's method, showing methodological similarities. Confidence Score: 0.7 \n\nJSON format:\n{\n\"b23\": 1.0,\n\"b38\": 1.0,\n\"b41\": 1.0,\n\"b40\": 0.9,\n\"b30\": 0.8,\n\"b12\": 0.7\n}", "Based on the provided text, the main challenges outlined in the paper are the limited availability of text-music parallel training data and the risk of plagiarism or lack of novelty in generated outputs. The authors propose a new model and training strategy that involves the use of beat-synchronous mixup strategies to generate novel text-conditioned musical audio from limited parallel training data. \n\nThe following references seem to be the most relevant to the paper's novel contributions:\n\n1. 'b23': 'audioldm text to audio generation with latent diffusion models' - mentioned as the basis of the text-to-music generation model (MusicLDM) being constructed by the authors.\n2. 'b30': 'highresolution image synthesis with latent diffusion models' - mentioned as being adapted to the music domain in the construction of MusicLDM.\n3. 'b38': 'large scale contrastive language audio pretraining with feature fusion and keywordto caption augmentation' - indicated as the source of the contrastive language-audio pretraining (CLAP) model used in MusicLDM.\n4. 'b40': 'mixup beyond empirical risk minimization proc iclr' - referenced as the source of the mixup idea adapted for data augmentation in the music generation context.\n5. 'b41': 'beat transformer demixed beat and downbeat tracking with dilated self attention' - used for beat-tracking during the implementation of the proposed beat-synchronous mixup strategies.\n\nHere is the result in the requested JSON format:\n\n{\n    \"b23\": 0.9,\n    \"b30\": 0.8,\n    \"b38\": 0.9,\n    \"b40\": 0.8,\n    \"b41\": 0.7\n}\n\nThese confidence scores are subjective and are based on the frequency and context of the references in the text. A score closer to 1 indicates that the reference is likely to be a central source that heavily influenced the main contributions of the paper.", "Based on the provided paper text, the following references are inferred as the most pertinent sources that inspired the paper:\n\n1. \"AudioLDM [b23]\" - The paper mentions that their model, MusicLDM, is adapted from AudioLDM and Stable Diffusion to the music domain. This indicates a strong reliance on AudioLDM for developing the paper's core contributions.\n2. \"Stable Diffusion [b30]\" - As mentioned above, Stable Diffusion along with AudioLDM forms the basis for the MusicLDM model, thus making it a key reference.\n3. \"CLAP [b38]\" - The paper mentions using a pretrained text and audio encoder (CLAP) for testing plagiarism and novelty in text-to-music generation. This implies that CLAP is an important source reference.\n4. \"mixup [b40]\" - The paper mentions adapting mixup, a concept from past work, to augment the training data. This indicates that mixup played a significant role in the development of the paper's methodology.\n5. \"Beat Transformer [b41]\" - The Beat Transformer is used for beat-tracking in the proposed beat-synchronous mixup strategies, making it an important source reference.\n6. \"generating music from text [b0]\" - This paper is referenced in the context of the challenges faced by text-to-music generation, indicating that it likely provided valuable insights for the development of the paper's contributions.\n\nIn JSON format, the result is as follows:\n{\"b23\": 0.9, \"b30\": 0.9, \"b38\": 0.8, \"b40\": 0.8, \"b41\": 0.8, \"b0\": 0.7}"], "64a29621d68f896efa28fd65": ["Based on the provided paper text and the titles of the cited references, the source papers that inspire the author the most could be:\n\n1. \"Exploiting coroutines to attack the killer nanoseconds\" [b27]: This paper is directly mentioned in the context of using light-weight coroutines for fast context switching. The proposed method of the paper also relies heavily on the use of coroutines to hide event latency, which is a central theme of this paper. Confidence score: 0.9\n\n2. \"Profile-guided yield instrumentation\" [b9]: This paper is mentioned when discussing profile-guided optimizations (PGO), which forms a core part of the paper's proposed methodology for hiding short events in software. Confidence score: 0.8\n\n3. \"Interleaving with coroutines: a practical approach for robust index joins\" [b52]: This paper is referenced in the discussion on light-weight coroutines and the interleaving of coroutine executions. The paper's method also involves interleaving coroutine executions to hide memory accesses. Confidence score: 0.7\n\n4. \"Efficient coroutines for the Java platform\" [b62]: This paper is referenced in the discussion about the use of light-weight coroutines to reduce switching overhead. It's one of the papers mentioned that discusses the use of coroutines in detail. Confidence score: 0.6\n\n5. \"Quantitative evaluation of Intel PEBS overhead for online system noise analysis\" [b0]: This paper is mentioned in the context of profile-guided optimizations and sample-based profiling, which are key to the paper's proposed methodology. Confidence score: 0.6\n\n6. \"A case against most context switches\" [b24]: This paper is mentioned when discussing the limitations of hardware mechanisms like simultaneous multithreading (SMT), which is a relevant background to the paper's discussion on hiding events in software. Confidence score: 0.5\n\nBased on the above analysis, the result in json format would be:\n\n{\n\"b27\": 0.9,\n\"b9\": 0.8,\n\"b52\": 0.7,\n\"b62\": 0.6,\n\"b0\": 0.6,\n\"b24\": 0.5\n}", "Based on the given text and the author notes, the academic paper seems to primarily focus on using profiling information to decide where to insert coroutine switching and prefetch instructions to hide LLC miss level latency and improve CPU utilization. The paper's solution is designed for deployability, with an emphasis on transparency for the user and controllable latency. It also ponders hardware optimizations for coroutine use. \n\nTo identify source papers, we need to look for referenced papers that seem to closely relate to these challenges and the proposed solutions. \n\nBased on the text analysis, the papers that inspire the author the most appear to be:\n\n- \"b27\": 'exploiting coroutines to attack the killer nanoseconds' - This paper seems to be discussing a similar topic of using coroutines to hide memory latencies. \n- \"b16\": 'kotlin coroutines design and implementation' - This paper could have provided insights into coroutine design and implementation, which is a key part of the author's proposal.\n- \"b9\": 'autofdo automatic feedback directed optimization for warehouse scale applications' - This paper discusses profile-guided optimizations, which the author has adopted in their proposal.\n- \"b65\": 'collecting performance data with papi c' - This paper is about collecting performance data, which is a crucial aspect of the author's approach.\n- \"b52\": 'interleaving with coroutines a practical approach for robust index joins' - This paper discusses interleaving coroutine executions, which is a technique the author uses in their proposal.\n- \"b15\": 'compiler support for lightweight context switching' - The author's proposal relies on fast context switching, making this paper a likely source of inspiration. \n\nHere is the JSON you requested:\n\n```json\n{\n    \"b27\": 0.9,\n    \"b16\": 0.8,\n    \"b9\": 0.85,\n    \"b65\": 0.8,\n    \"b52\": 0.75,\n    \"b15\": 0.8\n}\n```\nThe confidence score is set between 0 and 1, with 1 being the most confident. These scores are estimated based on the relevance of each source paper to the key topics and methods discussed in the text.", "Based on the text of the academic paper and the titles of the references, the most pertinent source papers that inspired this work can be identified as follows:\n\n1. \"Efficient coroutines for the java platform\" [b62]: The paper uses lightweight coroutines as one of the two main techniques for their main task. The confidence score is high due to the direct relevance of the method to the paper's novel contributions.\n\n2. \"Interleaving with coroutines a practical approach for robust index joins\" [b52]: This paper also seems to have influenced the author's work as they also use coroutines in their method. The confidence score is high due to the direct methodological similarity.\n\n3. \"Profile-guided automatic inline expansion for c programs\" [b8]: This paper is relevant as the author uses profile-guided optimization in their work. The confidence score is high due to the direct relevance of the method to the paper's novel contributions.\n\n4. \"Autofdo automatic feedback directed optimization for warehouse scale applications\" [b9]: This paper is relevant as the author uses feedback-directed optimization in their work. The confidence score is high due to the direct relevance of the method to the paper's novel contributions.\n\n5. \"Profiling a warehouse scale computer\" [b30]: This paper seems to have influenced the author's work as they mention the importance of profiling in their work. The confidence score is high due to the direct relevance of the method to the paper's novel contributions.\n\n6. \"Quantitative evaluation of intel pebs overhead for online system noise analysis\" [b0]: This paper is relevant as the author uses Intel's Precise Event Based Sampling (PEBS) in their work. The confidence score is high due to the direct relevance of the method to the paper's novel contributions.\n\n7. \"Compiler support for lightweight context switching\" [b15]: This paper seems to have influenced the author's work as they mention the importance of context switching in their work. The confidence score is high due to the direct relevance of the method to the paper's novel contributions.\n\n8. \"Exploiting coroutines to attack the killer nanoseconds\" [b27]: This paper seems to have influenced the author's work as they also use coroutines in their method. The confidence score is high due to the direct methodological similarity.\n\nHere is the result in the json format:\n{\n\"b62\": 1,\n\"b52\": 1,\n\"b8\": 1,\n\"b9\": 1,\n\"b30\": 1,\n\"b0\": 1,\n\"b15\": 1,\n\"b27\": 1\n}", "Based on the text and the author's notes, the following papers seem to be the most directly influential to the main contributions of the paper:\n\n1. \"Exploiting coroutines to attack the killer nanoseconds\" [b27]: The paper's main contribution is heavily reliant on the use of coroutines, and the phrase \"a novel combination of two emerging techniques: light-weight coroutines and sample-based profiling\" suggests this paper could have been a significant source of inspiration.\n\n2. \"Coz: Finding code that counts with causal profiling\" [b12]: The paper uses profiling techniques, which is a key part of the proposed algorithm in the paper.\n\n3. \"Profile-guided automatic inline expansion for c programs\" [b8]: The phrase \"sample-based profiling leverages hardware performance counters available in modern CPUs to sample hardware events of interest in production with negligible overhead\" suggests that this paper may have inspired the use of sample-based profiling.\n\n4. \"Autofdo: Automatic feedback-directed optimization for warehouse scale applications\" [b9]: The reference to \"feedback-directed optimization\" in the paper suggests this paper may have been a key inspiration.\n\n5. \"Efficient coroutines for the java platform\" [b62]: The paper's main contribution heavily relies on the use of coroutines, and this paper could have been a significant source of inspiration.\n\n6. \"Collecting performance data with PAPI-C\" [b65]: The paper uses profiling techniques, which is a key part of the proposed algorithm in the paper.\n\n7. \"Interleaving with coroutines: a practical approach for robust index joins\" [b52]: The mention of \"interleave coroutine executions\" suggests that this paper may have been an inspiration.\n\n8. \"Profile-guided optimizations (PGO), also called feedback-driven optimizations (FDO)\" [b26]: The paper's core algorithm is heavily reliant on profile-guided optimizations, suggesting that this paper was a significant source of inspiration.\n\nThese papers seem to be the most central to the development of the paper's main contributions. However, further analysis of the paper and its references may yield additional insights.\n\nThe result in json format:\n```\n{\n  \"b27\": 1,\n  \"b12\": 0.9,\n  \"b8\": 0.9,\n  \"b9\": 1,\n  \"b62\": 0.9,\n  \"b65\": 0.9,\n  \"b52\": 0.9,\n  \"b26\": 1\n}\n```", "Based on the text of the paper, the key challenges outlined are latency due to cache misses and the inefficiency of software mechanisms in handling these events. The paper proposes a novel combination of two emerging techniques: light-weight coroutines and sample-based profiling to address these challenges.\n\nHere are the key source papers that seem to have influenced this paper:\n\n- \"Efficient coroutines for the Java platform\" [b62]: The concept of light-weight coroutines seems to be inspired from this paper. As per the text, the authors adopt cooperative multitasking, and light-weight coroutines support fast context switchings that take only several nanoseconds, allowing them to interleave coroutine executions with low overhead.\n- \"Collecting performance data with papi c\" [b65]: The authors use sample-based profiling to leverage hardware performance counters available in modern CPUs to sample hardware events of interest in production with negligible overhead. This paper seems to be a source for this technique.\n- \"Profiling a warehouse-scale computer\" [b30]: This paper is mentioned in the context of discussing the significant portion of CPU stalls due to memory-bound CPU stalls, which seems to be a primary challenge that the authors are addressing.\n- \"Interleaving with coroutines: a practical approach for robust index joins\" [b52]: This paper is mentioned while discussing the recent works that interleave coroutine executions to hide memory accesses.\n- \"Profile-guided automatic inline expansion for C programs\" [b8]: This paper seems to have influenced the authors' approach to profile-guided automatic inline expansion.\n\nThe JSON format for the source papers would be:\n\n{\n  \"b62\": 0.8,\n  \"b65\": 0.8,\n  \"b30\": 0.6,\n  \"b52\": 0.6,\n  \"b8\": 0.6\n} \n\nThe confidence scores are estimated based on the relevance and recurrence of the references in the text related to the main challenges and methods proposed by the authors."], "63969ba790e50fcafdcf1d53": ["{\n    \"b36\": 0.9,\n    \"b12\": 0.9,\n    \"b42\": 0.85,\n    \"b55\": 0.85,\n    \"b37\": 0.8,\n    \"b15\": 0.75,\n    \"b31\": 0.75\n}", "{\n  \"b48\": 0.9,\n  \"b15\": 0.8,\n  \"b55\": 0.8,\n  \"b36\": 0.7,\n  \"b12\": 0.7,\n  \"b42\": 0.6\n}", "{\n  \"b48\": 0.9,\n  \"b36\": 0.9,\n  \"b12\": 0.8,\n  \"b37\": 0.85,\n  \"b55\": 0.9\n}", "{\n    \"b48\": 0.9,\n    \"b12\": 0.8,\n    \"b36\": 0.8,\n    \"b37\": 0.7,\n    \"b15\": 0.7,\n    \"b42\": 0.6,\n    \"b55\": 0.6\n}", "```json\n{\n  \"b42\": 0.95,\n  \"b15\": 0.90,\n  \"b55\": 0.85,\n  \"b36\": 0.80,\n  \"b12\": 0.75,\n  \"b37\": 0.70\n}\n```"], "64893b17d68f896efa982789": ["Based on the text and titles of the papers, the following source papers seem to have most inspired this work:\n\n1. \"vicuna an opensource chatbot impressing gpt\" [b2]\nConfidence Score: 0.9\nThe paper relies heavily on Vicuna, a large language model (LLM), for its methodological foundation. The authors of this paper fine-tune Vicuna on medical data and use its architecture for their own LLM, indicating a strong inspiration from this work. \n\n2. \"medclip contrastive learning from unpaired medical images and text\" [b16]\nConfidence Score: 0.9\nThis work is directly referenced as the source of the medical visual encoder used in the paper. This indicates a strong reliance on the methodology developed in this source paper.\n\n3. \"minigpt enhancing vision language understanding with advanced large language models\" [b22]\nConfidence Score: 0.8\nThis work is referenced multiple times as a comparison and benchmark. It\u2019s clear that this paper has served as a significant motivator or source of inspiration for the current work. The authors also adopt a similar methodology to Mini-GPT.\n\n4. \"mimic cxr a de identified publicly available database of chest radiographs with free text reports\" [b6]\nConfidence Score: 0.7\nThis dataset is used extensively in the paper for training and testing purposes. While it's not a methodological inspiration, it is a key resource that enables the research.\n\n5. \"preparing a collection of radiology examinations for distribution and retrieval\" [b3]\nConfidence Score: 0.7\nSimilar to [b6], this paper provides a critical dataset (OpenI) that the authors use in their experiments. It is a key source that the authors rely on for their work.\n\nThe confidence scores are assigned based on the frequency and context in which each source paper is mentioned in the text.", "Based on the text analysis, the source papers that are most directly related to the main contributions of the paper are:\n\n1. \"vicuna an opensource chatbot impressing gpt\" [b2]\n2. \"preparing a collection of radiology examinations for distribution and retrieval\" [b3]\n3. \"mimic cxr a de identified publicly available database of chest radiographs with free text reports\" [b6]\n4. \"medclip contrastive learning from unpaired medical images and text\" [b16]\n5. \"minigpt enhancing vision language understanding with advanced large language models\" [b22]\n\nThe json format is:\n\n{\n  \"b2\": 1.0,\n  \"b3\": 0.9,\n  \"b6\": 0.9,\n  \"b16\": 0.8,\n  \"b22\": 1.0\n}\n\nThe confidence scores are an estimation of how directly each paper has contributed to the main contributions of the paper.", "Based on the text of the paper and the titles of the references, the most pertinent source papers that inspire the authors are as follows:\n\n1. \"Vicuna an opensource chatbot impressing gpt\" [b2]\n2. \"MedClip contrastive learning from unpaired medical images and text\" [b16]\n3. \"Minigpt enhancing vision language understanding with advanced large language models\" [b22]\n4. \"Mimic cxr a de identified publicly available database of chest radiographs with free text reports\" [b6]\n5. \"Preparing a collection of radiology examinations for distribution and retrieval\" [b3]\n6. \"Chatdoctor a medical chat model fine tuned on llama model using medical domain knowledge\" [b7]\n\nThe confidence scores for each of these source papers are as follows:\n\n{\n\"b2\": 1,\n\"b16\": 0.9,\n\"b22\": 1,\n\"b6\": 0.8,\n\"b3\": 0.8,\n\"b7\": 0.7\n}\n\nThese papers are selected as they are directly relevant to the paper's novel contributions and methods. They heavily influenced the main contributions of the paper.", "{\n\"b2\": 0.8,\n\"b22\": 0.9,\n\"b16\": 0.7,\n\"b6\": 0.6,\n\"b3\": 0.6\n}", "Based on the provided text of the academic paper and the titles of the references, the most pertinent source papers that inspired the authors could be identified as follows:\n\n1. \"vicuna an opensource chatbot impressing gpt\" [b2]: Mentioned multiple times in the text. The authors used the Vicuna model in their research, suggesting that this paper had a significant influence on their work.\n\n2. \"minigpt enhancing vision language understanding with advanced large language models\" [b22]: Mentioned multiple times as a comparison and as a part of their methodology. \n\n3. \"medclip contrastive learning from unpaired medical images and text\" [b16]: Used in their model architecture, indicating it had a significant influence on their research.\n\n4. \"mimic cxr a de identified publicly available database of chest radiographs with free text reports\" [b6]: This is the dataset used in their experiments and methods.\n\n5. \"preparing a collection of radiology examinations for distribution and retrieval\" [b3]: Another dataset used in their experiments and methods.\n\n6. \"chatdoctor a medical chat model fine tuned on llama model using medical domain knowledge\" [b7]: Mentioned in the related work section as a nextgeneration AI doctor model. It might have inspired the authors in the development of their own model.\n\n7. \"medalpaca an open source collection of medical conversational ai models and training data\" [b4]: Mentioned in the related work section as one of the recent medical chatbots. It could have inspired the authors in their research.\n\nConsidering these findings, the results can be formatted into a JSON object as follows:\n\n{\n  \"b2\": 0.9,\n  \"b22\": 0.9,\n  \"b16\": 0.8,\n  \"b6\": 0.8,\n  \"b3\": 0.8,\n  \"b7\": 0.7,\n  \"b4\": 0.7\n}\n\nThis JSON object provides a concise list of the most central references that heavily influenced the main contributions of the paper. The \"confidence score\" represents the level of inferred inspiration from each source paper, with 0.9 being the highest and 0.7 being the lowest in this context."], "64a29612d68f896efa28bca5": ["Based on the given text and the titles of the references, the following papers seem to be the most influential sources for this paper:\n\n1. \"A study of replacement algorithms for a virtual storage computer\" [b11] : The paper talks about B?l?dy's OPT algorithm which is indicated as an inspiration for the design of many practical cache replacement policies.\n   \n2. \"High performance cache replacement using re-reference interval prediction (RRIP)\" [b25] : This paper is mentioned in the context of cache replacement policies.\n\n3. \"Cost sensitive cache replacement algorithms\" [b26] and \"Cache replacement algorithms with nonuniform miss costs\" [b27] : These two papers are referred in the context of cost-aware cache replacement policies.\n\n4. \"A case for MLP-aware cache replacement\" [b46] : This paper is mentioned as an example of prior research in cost-aware data cache replacement policies.\n\n5. \"Rebasing instruction prefetching: an industry perspective\" [b21] and \"Reestablishing fetch directed instruction prefetching: an industry perspective\" [b22] : These papers are referred in the context of aggressive front-end model in modern processors that are designed to tolerate instruction cache misses.\n\n6. \"High performing cache hierarchies for server workloads: relaxing inclusion to capture the latency benefits of exclusive caches\" [b24]: This paper is mentioned in the context of preserving instruction lines in the unified L2 cache.\n\n7. \"Adaptive insertion policies for high performance caching\" [b45]: This paper is referred in the context of bimodal cache replacement policy.\n\n8. \"Back to the future: leveraging Belady\u2019s algorithm for improved cache replacement\" [b23]: This paper is mentioned in the context of theoretical cache replacement policies that informed the design of practical ones.\n\nThe JSON format would be:\n```json\n{\n    \"b11\": 0.9,\n    \"b25\": 0.8,\n    \"b26\": 0.8,\n    \"b27\": 0.8,\n    \"b46\": 0.85,\n    \"b21\": 0.85,\n    \"b22\": 0.85,\n    \"b24\": 0.8,\n    \"b45\": 0.9,\n    \"b23\": 0.9\n}\n```\nThe confidence scores are estimated on a scale of 0 to 1, where 1 represents the highest confidence level.", "Based on the text, the following references are likely to be the source papers for this academic paper:\n\n1. \"B?l?dy's OPT algorithm achieves the minimum number of misses through ideal cache replacement, but it is not realizable because it requires perfect knowledge of future references [b11]. While theoretical in nature, OPT informs the design of many practical cache replacement policies [b23] [b25] [b37] [b45].\"\n2. \"Cost-aware cache replacement policies recognize this and attempt to increase performance even at the cost of increased cache misses. For modern architectures, many misses do not impact performance at all [b31] [b40] [b46] [b51].\"\n3. \"This paper presents EMISSARY (Enhanced MISS-Awareness Replacement Policy) to address the lack of cost-aware cache replacement policies for instructions. Observing that modern architectures entirely tolerate many L1I misses, EMISSARY prioritizes instruction lines whose miss caused a decode starvation [b21] [b22].\"\n4. \"The EMISSARY configurations in Figure 1 support up to eight protected lines (? = 8) per set in the L2 cache, leaving eight ways in the 16-way cache available for low-priority instruction and data lines [b45].\"\n5. \"The EMISSARY treatment option is orthogonal to the specific LRU algorithm used. For lines 2 and 4 of Algorithm 1, finding the LRU among the low-priority or the high-priority lines can be calculated precisely from a true LRU algorithm. With a pseudo-LRU (PLRU) algorithm, however, keeping separate PLRU's for low-and high-priority lines limits the imprecision [b45].\"\n\nThe JSON result would be:\n\n{\n\"b11\": 0.9,\n\"b23\": 0.8,\n\"b25\": 0.8,\n\"b37\": 0.7,\n\"b45\": 1.0,\n\"b31\": 0.8,\n\"b40\": 0.8,\n\"b46\": 0.8,\n\"b51\": 0.8,\n\"b21\": 0.9,\n\"b22\": 0.9\n}", "Based on the text of the academic paper, the following source papers seem to have had the most influence on the author's work:\n\n1. \"A study of replacement algorithms for a virtual storage computer\" [b11]\n2. \"Back to the future: leveraging belady\u2019s algorithm for improved cache replacement\" [b23]\n3. \"High performance cache replacement using re-reference interval prediction (rrip)\" [b25]\n4. \"A case for MLP-aware cache replacement\" [b46]\n5. \"Adaptive insertion policies for high performance caching\" [b45]\n6. \"Cost-sensitive cache replacement algorithms\" [b26]\n7. \"Cache replacement algorithms with non-uniform miss costs\" [b27]\n8. \"An imitation learning approach for cache replacement\" [b37]\n\nThese source papers are inferred based on the context in which they are mentioned within the text and the relevance of their titles to the paper's topic.\n\nThese papers are mentioned in the context of the author's discussion about cache replacement policies, which is a major topic of the paper. The authors also seem to have drawn inspiration from these source papers to develop their novel strategy, EMISSARY, for cache replacement, which suggests that these papers had a significant impact on their work.\n\nThe confidence score for each of these source papers is 1 due to the clear indication of their relevance to the paper's main contributions.\n\nThe output in json format would be:\n{\n\"b11\": 1,\n\"b23\": 1,\n\"b25\": 1,\n\"b46\": 1,\n\"b45\": 1,\n\"b26\": 1,\n\"b27\": 1,\n\"b37\": 1\n}", "The following source papers appear to have heavily influenced the main contributions of the given paper:\n\n1. \"Adaptive insertion policies for high performance caching\" [b45]: This paper seems to have influenced the design of the EMISSARY replacement policy. \n\n2. \"A case for mlp aware cache replacement\" [b46]: The given paper discusses cost-aware cache replacement policies and references techniques that prioritize lines that miss with lower memory-level parallelism (MLP), which are likely inspired by this source.\n\n3. \"Cache replacement algorithms with nonuniform miss costs\" [b27]: This source is likely central to the development of cost-aware cache replacement policies discussed in the given paper.\n\n4. \"A study of replacement algorithms for a virtual storage computer\" [b11]: The given paper refers to B?l?dy's OPT algorithm for achieving minimum cache misses, which is introduced in this source.\n\n5. \"Back to the future: Leveraging Belady's algorithm for improved cache replacement\" [b23]: The given paper refers to the influence of the OPT algorithm on practical cache replacement policies, which this source paper also discusses.\n\n6. \"High performing cache hierarchies for server workloads: Relaxing inclusion to capture the latency benefits of exclusive caches\" [b24]: This paper could be a source paper, considering the given paper discusses modern architectures and their ability to tolerate many L1I misses.\n\n7. \"A large, fast instruction window for tolerating cache misses\" [b35]: This source paper is probably related to the discussion in the given paper about aggressive out-of-order processors that can tolerate many L1D misses.\n\nIn JSON format, the confidence scores for these papers could be represented as follows:\n\n{\n\"b45\": 0.9,\n\"b46\": 0.85,\n\"b27\": 0.9,\n\"b11\": 0.8,\n\"b23\": 0.75,\n\"b24\": 0.7,\n\"b35\": 0.7\n}", "Based on the given text, the main challenges outlined in the paper are improving processor performance by reducing cache misses and creating a cost-effective cache replacement policy for instructions. The novel contribution of the paper is the presentation of EMISSARY (Enhanced MISS-Awareness Replacement Policy), a method that addresses the lack of cost-aware cache replacement policies for instructions. EMISSARY prioritizes instruction lines whose miss caused a decode starvation, improving performance and saving energy. The author also introduces a bimodal technique for EMISSARY, which involves mode selection and mode treatment. \n\nAnalyzing the text and the titles of the references, the following papers seem to have directly influenced the development of EMISSARY:\n\n1. \"A study of replacement algorithms for a virtual storage computer\" (b11).\n2. \"High performance cache replacement using re-reference interval prediction (RRIP)\" (b25).\n3. \"Back to the future: Leveraging Belady's algorithm for improved cache replacement\" (b23).\n4. \"Adaptive insertion policies for high performance caching\" (b45).\n5. \"A case for MLP-aware cache replacement\" (b46).\n6. \"Cost-sensitive cache replacement algorithms\" (b26).\n\nThese papers appear to be related to the challenges and methods mentioned in the text and are referenced in sections discussing the development and specifics of EMISSARY. \n\nIn JSON format, this would be:\n\n{\n    \"b11\": 0.9,\n    \"b25\": 0.8,\n    \"b23\": 0.8,\n    \"b45\": 0.7,\n    \"b46\": 0.7,\n    \"b26\": 0.7\n}\n\nThe confidence scores are subjective and based on the importance and relevance of the papers to the development and specifics of EMISSARY as indicated in the text."], "621454435aee126c0f1d702f": ["Based on the provided text and the titles of the references, the source papers that seem to have inspired the authors the most are:\n\n1. 'Sparsely-gated MoE [b30]' - 'Outrageously large neural networks: the sparsely gated mixture-of-experts layer': The authors mention that this paper provides an effective way to scale model capacity given a fixed computational cost and has played a significant role in increasing the training efficiency of large-scale language models.\n\n2. 'Switch Transformer [b9]' - 'Switch transformers: scaling to trillion parameter models with simple and efficient sparsity': The authors compare their method with the top-1 and top-2 gating counterparts in Switch Transformer, indicating that this paper might have inspired them.\n\n3. 'GShard [b20]' - 'Gshard scaling giant models with conditional computation and automatic sharding': The authors compare their method with GShard and mention it multiple times, indicating its importance to their work.\n\n4. 'Hard mixtures of experts [b11]' - 'Hard mixtures of experts for large scale weakly supervised vision': This paper is referred to when the authors discuss different methods for routing.\n\n5. 'Base layers [b21]' - 'Base layers: simplifying training of large sparse models': This paper is also referenced when discussing routing strategies.\n\n6. 'Hash layers [b27]' - 'Hash layers for large sparse models': Another paper discussing routing strategies, indicating its relevance to the authors' work.\n\nBased on this analysis, the confidence score for each paper would be:\n\n- b30: 0.9\n- b9: 0.8\n- b20: 0.8\n- b11: 0.7\n- b21: 0.7\n- b27: 0.7\n\nPlease note that this is a subjective analysis and the confidence scores are a rough estimate based on the frequency and context of the references in the provided text.", "Based on the information provided in the text and the provided titles of the referenced papers, the following references appear to be the most influential in the development of the current paper:\n\n1. \"b30\": \"outrageously large neural networks the sparsely gated mixture ofexperts layer\"\n2. \"b9\": \"switch transformers scaling to trillion parameter models with simple and efficient sparsity\"\n3. \"b20\": \"gshard scaling giant models with conditional computation and automatic sharding\"\n4. \"b21\": \"base layers simplifying training of large sparse models\"\n5. \"b27\": \"hash layers for large sparse models\"\n6. \"b7\": \"tricks for training sparse translation models\"\n7. \"b11\": \"hard mixtures of experts for large scale weakly supervised vision\"\n\nThe score of each source paper based on the confidence level can be represented in the JSON format as follows:\n\n{\"b30\": 0.85, \"b9\": 0.8, \"b20\": 0.75, \"b21\": 0.7, \"b27\": 0.65, \"b7\": 0.6, \"b11\": 0.55} \n\nThese scores are based on the frequency and importance of the mentioned references in the text. The paper \"b30\" seems to have the highest influence due to its multiple mentions and importance to the core contributions of the paper. The other papers follow in terms of their relevance and contribution to the development of the methods and approaches in the paper.", "Based on the given text, the key source papers that inspired this work are as follows:\n\n{\n    \"b9\": 0.9,\n    \"b20\": 0.9,\n    \"b30\": 0.8,\n    \"b21\": 0.7,\n    \"b27\": 0.7,\n    \"b7\": 0.7\n}\nThese papers are repeatedly referred to throughout the text, and the authors discuss either building upon or differentiating their work from these sources. In particular, references b9 and b20 appear to be the most central to this work, as they are frequently mentioned in the context of specific methodological approaches that the authors have adopted or modified. The other references (b30, b21, b27, b7) are also referenced multiple times as inspirations for the authors' approach.", "Based on the given text and the titles of the references, the source papers that most likely inspired the author are:\n\n- \"outrageously large neural networks the sparsely gated mixture ofexperts layer\" [b30]\n- \"switch transformers scaling to trillion parameter models with simple and efficient sparsity\" [b9]\n- \"gshard scaling giant models with conditional computation and automatic sharding\" [b20]\n- \"base layers simplifying training of large sparse models\" [b21]\n- \"hash layers for large sparse models\" [b27]\n- \"transformer xl attentive language models beyond a fixed length context\" [b4]\n- \"exponentially increasing the capacity to computation ratio for conditional computation in deep learning\" [b2]\n\nThe confidence score for each paper is estimated as:\n\n- [b30]: 0.9\n- [b9]: 0.8\n- [b20]: 0.85\n- [b21]: 0.7\n- [b27]: 0.7\n- [b4]: 0.65\n- [b2]: 0.6\n\nThe JSON output:\n\n{\n  \"b30\": 0.9,\n  \"b9\": 0.8,\n  \"b20\": 0.85,\n  \"b21\": 0.7,\n  \"b27\": 0.7,\n  \"b4\": 0.65,\n  \"b2\": 0.6\n}", "Based on the text of the paper and its references, the source papers that most inspired this work are likely to be:\n\n1. \"Switch transformers: scaling to trillion parameter models with simple and efficient sparsity\" - [b9]: The authors use the Switch transformer's method of selecting only the top expert per token using a softmax over the hidden state. They also make frequent comparisons with this work.\n\n2. \"GShard: scaling giant models with conditional computation and automatic sharding\" - [b20]: The authors compare their method with GShard\u2019s approach and also adopt some of its concepts.\n\n3. \"Outrageously large neural networks: the sparsely gated mixture-of-experts layer\" - [b30]: The authors base their work on the concept of a mixture-of-experts model introduced in this paper and improve upon it.\n\n4. \"Base layers: simplifying training of large sparse models\" - [b21]: The authors discuss how this work formulates a linear assignment that maximizes token-expert affinities while ensuring each expert receives an equal number of tokens, which is a concept they build upon.\n\n5. \"Hash layers for large sparse models\" - [b27] and [b7]: The authors mention these works as exploring alternative strategies for routing, which seems to be a key part of their research.\n\n6. \"Scaling laws for neural language models\" - [b19]: The authors reference this paper when discussing the power-law relationship between final model quality and the amount of data, model size, and compute time.\n\n7. \"Model parallelism [b29]\" - The authors mention this work when discussing different forms of model parallelism, which seems to indicate that it had some influence on their approach.\n\nThe confidence scores for these references are estimated based on the frequency and context of their mentions throughout the paper.\n\n```json\n{\n    \"b9\": 0.9,\n    \"b20\": 0.85,\n    \"b30\": 0.8,\n    \"b21\": 0.75,\n    \"b27\": 0.7,\n    \"b7\": 0.7,\n    \"b19\": 0.65,\n    \"b29\": 0.6\n}\n```"], "64ae66f63fda6d7f0684abc0": ["Based on the paper text and the titles of the references, the source papers that most likely inspired this research are:\n- \"A review on language models as knowledge bases\" [b0]: This paper is directly cited in the introduction, which suggests that it provided the basis for the authors' exploration of the types of knowledge encoded in Pretrained Language Models (PLMs).\n- \"DBpedia a nucleus for a web of open data\" [b1]: The authors used DBpedia to obtain classes and their instances in their research, indicating that it was a key resource.\n- \"BERT: Pre-training of deep bidirectional transformers for language understanding\" [b6]: The authors used BERT, a PLM, in their research, suggesting that they were inspired by the methodologies in this paper.\n- \"OpenPrompt an open source framework for prompt learning\" [b7]: The authors used OpenPrompt in their experiments, indicating that they adopted its methodologies.\n- \"Resource Description Framework (RDF) model and syntax specification\" [b4]: The authors referenced RDF in their research, suggesting that they were inspired by its approach to structuring knowledge.\n\nBased on the above analysis, the following is the result in json format, with the \"reference number\" as the key and the \"confidence score\" as the value:\n\n{\n\"b0\": 0.9,\n\"b1\": 0.9,\n\"b6\": 0.8,\n\"b7\": 0.8,\n\"b4\": 0.7\n}", "Based on the text and the titles of the references, the source papers that inspire this given paper the most are:\n\n1. \"A review on language models as knowledge bases\" [b0]: The paper refers to this work when discussing the capabilities of Pretrained Language Models (PLMs) to encode knowledge.\n2. \"DBpedia a nucleus for a web of open data\" [b1]: This paper uses DBpedia to obtain classes and their instances for the ontology building.\n3. \"BERT: Pre-training of deep bidirectional transformers for language understanding\" [b6]: This paper uses BERT in the probing methods.\n4. \"Ontological constitutions for classes and properties\" [b28]: The paper refers to this work when explaining ontological knowledge.\n5. \"RoBERTa: A robustly optimized BERT pretraining approach\" [b26]: RoBERTa is also used in the probing methods.\n6. \"OpenPrompt: An open source framework for prompt learning\" [b7]: This paper uses OpenPrompt in its probing methods.\n7. \"Leap of thought: Teaching pre-trained models to systematically reason over implicit knowledge\" [b37]: This paper refers to the work when discussing the reasoning task.\n\nThe confidence scores for each of these papers might look like:\n```json\n{\n  \"b0\": 0.85,\n  \"b1\": 0.80,\n  \"b6\": 0.90,\n  \"b28\": 0.75,\n  \"b26\": 0.90,\n  \"b7\": 0.80,\n  \"b37\": 0.80\n}\n```\nThese scores are based on the frequency and the context in which the papers are cited.", "Based on the provided information, the source papers that are most influential towards the given paper are:\n\n1. \"A review on language models as knowledge bases\" [b0]: The paper mentions that PLMs are capable of encoding a significant amount of knowledge from the pretraining corpora, which is a central concept of the given paper. \n\n2. \"DBpedia a nucleus for a web of open data\" [b1]: The given paper uses DBpedia to obtain classes and their instances.\n\n3. \"Resource Description Framework RDF model and syntax specification\" [b4]: The given paper mentions the Resource Description Framework Schema (RDFS) which is specified in this source paper.\n\n4. \"BERT pre-training of deep bidirectional transformers for language understanding\" [b6]: The given paper investigates BERT and RoBERTa, as mentioned in this source paper.\n\n5. \"OpenPrompt an open source framework for promptlearning\" [b7]: The given paper uses OpenPrompt, an open-source framework for prompt learning, which is discussed in this source paper.\n\n6. \"Wikidata a free collaborative knowledgebase\" [b38]: The given paper collects properties based on DBpedia and Wikidata.\n\nAs per the provided information, the confidence score for each paper could not be calculated. Hence, the results in json format would be:\n\n{\n\"b0\": \"Cannot calculate confidence score\",\n\"b1\": \"Cannot calculate confidence score\",\n\"b4\": \"Cannot calculate confidence score\",\n\"b6\": \"Cannot calculate confidence score\",\n\"b7\": \"Cannot calculate confidence score\",\n\"b38\": \"Cannot calculate confidence score\"\n}", "Based on the provided text and titles of the references, the source papers that appear to have most inspired this paper are:\n\n1. 'b0': 'a review on language models as knowledge bases'\n2. 'b1': 'dbpedia a nucleus for a web of open data'\n3. 'b6': 'bert pre training of deep bidirectional transformers for language understanding'\n4. 'b26': 'roberta a robustly optimized bert pretraining approach'\n5. 'b37': 'leap of thought teaching pre trained models to systematically reason over implicit knowledge'\n6. 'b38': 'wikidata a free collaborative knowledgebase'\n\nThese papers are indicated as they have been referenced in key sections of the paper discussing the main challenges and the algorithm proposed by the author. The paper seems to be building upon the concept of pretrained language models and their capability to encode knowledge, which are central themes in the referenced papers.\n\nThe JSON format of the result is:\n{\n  \"b0\": 0.9,\n  \"b1\": 0.9,\n  \"b6\": 0.8,\n  \"b26\": 0.8,\n  \"b37\": 0.8,\n  \"b38\": 0.8\n}\n\nThe confidence scores are subjective and are ranked based on the perceived importance of the reference in the context of the paper, with 1 being the most confident.", "Based on the text of the paper, the author primarily discusses the storage and understanding of ontological knowledge in Pretrained Language Models (PLMs). The author focuses on the ability of PLMs to encode ontological knowledge and their capacity to perform logical reasoning based on this knowledge. \n\nTo identify the source papers that have inspired this work, I'll look for references that are directly relevant to these topics. The following references seem to have had a significant impact:\n\n1. \"a review on language models as knowledge bases\" [b0]: This is a general paper about the use of language models as knowledge bases, which is the foundation of the paper's topic.\n\n2. \"dbpedia a nucleus for a web of open data\" [b1]: The author uses DBpedia to obtain classes and their instances for their study, which indicates that this paper might have been a significant source of inspiration.\n\n3. \"bert pre training of deep bidirectional transformers for language understanding\" [b6]: The author investigates BERT (and RoBERTa), which is a model for language understanding pretraining. This indicates that this paper was likely a crucial source.\n\n4. \"openprompt an open source framework for promptlearning\" [b7]: This tool was used in the experiments, indicating that it was an important source.\n\n5. \"roberta a robustly optimized bert pretraining approach\" [b26]: As with BERT, the author investigates RoBERTa, which suggests that this paper was also a crucial source.\n\n6. \"how can we know what language models know\" [b17]: The main topic of this paper is similar to the one in question, which suggests that it was a significant source of inspiration.\n\n7. \"language models as knowledge bases\" [b31]: This paper's topic is directly relevant to the research question, indicating that it was a significant source.\n\nThe confidence scores for each of these source papers is subjective and would require more detailed analysis to assign accurately. However, based on the relevance of the topics and the use of the tools and models in the experiments, I would suggest that all the mentioned papers have a high confidence score.\n\nHere is the proposed result in JSON format:\n\n{\n    \"b0\": 0.9,\n    \"b1\": 0.9,\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b26\": 0.9,\n    \"b17\": 0.9,\n    \"b31\": 0.9\n}"], "62a7fc635aee126c0ff5e394": ["Based on the text of the academic paper and the titles of the references provided, the source papers that inspire the paper's core contributions are:\n\n{\n\"b11\": 0.9,\n\"b37\": 0.9,\n\"b42\": 0.8,\n\"b43\": 0.8,\n\"b44\": 0.8,\n\"b45\": 0.8,\n\"b46\": 0.8,\n\"b47\": 0.8\n}\n \nThe confidence scores are assigned based on the relevance of the source papers to the main contributions of the paper. The papers \"b11: Fpocket: an open source platform for ligand pocket detection\" and \"b37: Point Transformer\" appear to be the most influential, as they provide the key methodologies the authors employ. Other papers, such as b42, b43, b44, b45, b46 and b47, also appear to be important sources of inspiration for the authors, focusing on the geometric properties of molecular surfaces and machine learning approaches.", "Based on the text and the titles of the references, the source papers that most likely inspired the author could be:\n\n- \"b37: Point Transformer\" - The authors mention using a transformer-based neural network model, which is originally proposed in this paper. Confidence score: 0.9\n- \"b11: Fpocket an open source platform for ligand pocket detection\" - The authors mention adopting a figure of merit based on two scores that are inspired by state-of-the-art biophysical pocket detection methods including Fpocket. Confidence score: 0.8\n- \"b7: Computational methods and tools for binding site recognition between proteins and small molecules from classical geometrical approaches to modern machine learning strategies\" - This paper was referenced in connection to the protein-ligand binding site recognition challenge which is the main topic of the given paper. Confidence score: 0.7\n- \"b6: On the definition and the construction of pockets in macromolecules\" - This paper is referenced when discussing the detection of pockets and cavities which is a main problem tackled in the given paper. Confidence score: 0.6\n- \"b5: Geometric detection algorithms for cavities on protein surfaces in molecular graphics a survey detection algorithms for cavities\" - This is referenced in the provided text when discussing algorithms for detecting pockets and cavities, which is a key aspect of the given paper. Confidence score: 0.6\n\nThe returned result in the json format is:\n{\n\"b37\": 0.9,\n\"b11\": 0.8,\n\"b7\": 0.7,\n\"b6\": 0.6,\n\"b5\": 0.6\n}", "Based on the given text and the titles of the references, the source papers that inspired the given paper most are:\n\n1. \"Point Transformer\" [b37] - This is the key method used in the paper for learning per-vertex local shape geometric features. This paper is directly mentioned in the text as the basis for the method used.\n\n2. \"Analyzing the topology of active sites on the prediction of pockets and subpockets\" [b35] - The evaluation measures used in this paper are inspired by this source. \n\n3. \"Geometric detection algorithms for cavities on protein surfaces in molecular graphics: A survey\" [b5] - This source is referenced when the author discusses the different types of algorithms that have been proposed for detecting pockets and cavities on protein surfaces.\n\n4. \"Computational methods and tools for binding site recognition between proteins and small molecules: From classical geometrical approaches to modern machine learning strategies\" [b7] - This paper is mentioned multiple times in the text, indicating it has heavily influenced the overall approach and understanding of the topic.\n\n5. \"Attention is all you need\" [b36] - This paper is the basis for the Transformer model used in the paper. \n\n6. \"Analytical molecular surface calculation\" [b0] - This paper is referenced several times in the beginning of the text, indicating the foundational basis for the methods used in the paper.\n\nBased on these findings, the JSON result would be: \n```json\n{\n    \"b37\": 1,\n    \"b35\": 0.9,\n    \"b5\": 0.8,\n    \"b7\": 0.8,\n    \"b36\": 0.7,\n    \"b0\": 0.7\n}\n```\nThe confidence score is between 0 and 1, with 1 being the most confident.", "Based on the text, the most likely source papers for this paper are:\n\nReference b11: \"fpocket an open source platform for ligand pocket detection\", as this paper is directly mentioned in the text as a method for scoring binding pockets, suggesting that it contributed to the methodological approach of the given paper. Confidence score: 0.9\n\nReference b7: \"computational methods and tools for binding site recognition between proteins and small molecules from classical geometrical approaches to modern machine learning strategies\", as it is mentioned multiple times in the text as a comprehensive review and comparison of methods for pocket detection, suggesting that it influenced the overall approach and comparison of the given paper. Confidence score: 0.8\n\nReference b37: \"point transformer\", as it is mentioned in the text as the basis for the method proposed by Hao Huang, Boulbaba Ben Amor, and Yi Fang in the given paper. Confidence score: 0.8\n\nReference b5: \"geometric detection algorithms for cavities on protein surfaces in molecular graphics a survey detection algorithms for cavities\", as it is mentioned in the text in the context of categorizing different types of pocket detection algorithms, suggesting that it contributed to the theoretical background of the given paper. Confidence score: 0.7\n\nReference b18: \"p rank machine learning based tool for rapid and accurate prediction of ligand binding sites from protein structure\", as it is mentioned in the text as a data driven approach for pocket detection, which is a focus of the given paper. Confidence score: 0.7\n\nReference b15: \"understanding and predicting druggability a high throughput method for detection of drug binding sites\", as it is mentioned in the text in relation to machine learning methods for scoring binding sites, which is a focus of the given paper. Confidence score: 0.6\n\nReference b17: \"structure based maximal affinity model predicts small molecule druggability\", as it is mentioned as an exception in the text for its unique approach to scoring function for druggability, which can be inferred as an inspiration for the scoring method in the given paper. Confidence score: 0.6\n\nHere is the result in the JSON format:\n\n{\n\"b11\": 0.9,\n\"b7\": 0.8,\n\"b37\": 0.8,\n\"b5\": 0.7,\n\"b18\": 0.7,\n\"b15\": 0.6,\n\"b17\": 0.6\n}", "Based on the text of the paper, the primary challenges outlined are the challenges associated with identifying protein-ligand binding sites, with a focus on identifying pockets able to bind ligands based on their shape. The author's approach to tackle these challenges involves the use of computational methods such as geometric algorithms and machine learning.\n\nFrom the text, we can infer that the author was inspired by, or used methods and algorithms from the following papers:\n\n1. \"Analytical molecular surface calculation\" [b0]: Mentioned multiple times in the text, this paper provides the fundamental understanding of molecular surface calculation.\n\n2. \"Nanoshaper vmd interface computing and visualizing surfaces pockets and channels in molecular systems\" [b2]: The author uses the NanoShaper software to build the proteins' Solvent Excluded Surface.\n\n3. \"Geometric detection algorithms for cavities on protein surfaces in molecular graphics a survey detection algorithms for cavities\" [b5]: This paper is cited when the author discusses the different types of algorithms used to detect pockets and cavities.\n\n4. \"Attention is all you need\" [b36]: This paper is referred to when the author discusses the use of a Transformer-based neural network model.\n\n5. \"Point transformer\" [b37]: This paper is referred to when the author discusses the adaptation of a Transformer-based model to learn per-vertex local shape geometric features.\n\n6. \"A density based algorithm for discovering clusters in large spatial databases with noise\" [b41]: This paper is referred to when the author discusses the clustering of candidate binding region vertices.\n\n7. \"Protein docking model evaluation by graph neural networks\" [b46]: This paper is referred to when the author discusses the graph neural network model used for the evaluation.\n\n8. \"DeepSurf: a surface-based deep learning approach for the prediction of ligand binding sites on proteins\" [b50]: This paper is referred to when the author discusses the use of surface-based deep learning for the prediction of ligand binding sites.\n\nThe JSON format of the result would be:\n\n{\n   \"b0\": 0.9,\n   \"b2\": 0.8,\n   \"b5\": 0.7,\n   \"b36\": 0.8,\n   \"b37\": 0.8,\n   \"b41\": 0.7,\n   \"b46\": 0.8,\n   \"b50\": 0.8\n}"], "63d9d87390e50fcafd57d878": ["Based on the analysis of the paper's text and the author's notes, the primary challenges outlined in the paper seem to be the high computational cost involved in generating quality audio from text, and the limitations of existing methods in text-to-audio (TTA) generation. The author introduces a method for TTA generation based on StableDiffusion, which uses latent diffusion models in a continuous latent representation space, and explores various zero-shot text-guided audio manipulations. \n\nFrom the text, the papers that seem to have inspired or motivated the author the most include:\n\n[b47]: 'High resolution image synthesis with latent diffusion models' - The author specifically mentions being \"inspired by StableDiffusion [b47]\". This paper seems to have heavily influenced the core contributions of this paper, hence its confidence score would be high.\n\n[b60]: 'Discrete diffusion model for text-to-sound generation' - The author frequently compares their work with this paper, indicating that it has influenced the topic. Thus, it can be considered as one of the source papers.\n\n[b24]: 'Textually guided audio generation' - The author also compares their work with this paper and uses it as a reference for some methods, so it seems to be a source paper. \n\n[b58]: 'Large scale contrastive language audio pretraining with feature fusion and keyword to caption augmentation' - The author uses the Contrastive Language-Audio Pretraining (CLAP) method from this paper for their work, indicating its influence.\n\n[b54]: 'Score-based generative modeling through stochastic differential equations' - This paper introduced the concept of diffusion models, which are key to the author's algorithm, making it a source paper.\n\nOverall, the source papers seem to be [b47], [b60], [b24], [b58], and [b54], which have contributed to the development of the author's novel method in TTA generation.\n\nFinal result in JSON format:\n```json\n{\n    \"b47\": 1,\n    \"b60\": 0.9,\n    \"b24\": 0.9,\n    \"b58\": 0.8,\n    \"b54\": 0.8\n}\n```\nThis result indicates that the paper 'High resolution image synthesis with latent diffusion models' [b47] has the highest influence on the given paper, followed by 'Discrete diffusion model for text-to-sound generation' [b60], and 'Textually guided audio generation' [b24], then 'Large scale contrastive language audio pretraining with feature fusion and keyword to caption augmentation' [b58], and finally, 'Score-based generative modeling through stochastic differential equations' [b54]. The confidence score represents the level of influence, with 1 being the highest.", "Based on the text, the most pertinent source papers are:\n\n1. 'audioldm text to audio generation with latent diffusion models' - This is the main paper we are analyzing and it doesn't count as a source paper.\n2. 'AudioGen [b24] fuses pairs of audio samples and concatenates their respective processed text captions' - This paper is frequently compared with and its methodology is built upon in the paper. \n3. 'DiffSound [b60] uses textconditional discrete diffusion models to generate discrete tokens compressed from mel-spectrogram' - This paper is also compared with and its methodology is utilized and improved on in the paper. \n4. 'High resolution image synthesis with latent diffusion models [b47]' - This paper is directly mentioned as inspiring the use of latent diffusion models for TTA generation. \n5. 'Contrastive Language-Audio Pretraining (CLAP) [b58]' - This paper's method is used to facilitate the TTA generation in the paper. \n6. 'Denoising diffusion probabilistic models [b12] [b54]' - These two papers' methodology is utilized in the paper's diffusion models. \n\nThe confidence scores for each source paper are estimated as follows:\n\n- 'AudioGen [b24]' - 0.9: The paper frequently refers to and builds upon this paper.\n- 'DiffSound [b60]' - 0.8: This paper's methodology is improved upon and utilized in the paper.\n- 'High resolution image synthesis with latent diffusion models [b47]' - 0.7: The paper directly mentions this work as inspiration for its main methodology.\n- 'Contrastive Language-Audio Pretraining (CLAP) [b58]' - 0.6: The paper uses this method to facilitate TTA generation.\n- 'Denoising diffusion probabilistic models [b12] [b54]' - 0.8: These papers' methodology is utilized in the paper's diffusion models.\n\nReturned answer:\n\n{\n\"reference number\": \"confidence score\",\n\"b24\": 0.9,\n\"b60\": 0.8,\n\"b47\": 0.7,\n\"b58\": 0.6,\n\"b12\": 0.8,\n\"b54\": 0.8\n}", "Based on the text of the paper, the most germane source papers are:\n\n1. \"High resolution image synthesis with latent diffusion models[b47]\" - The authors mention that their work is directly \"inspired by StableDiffusion [b47]\". This indicates a strong reliance on the methods proposed in this paper.\n\n2. \"Textually guided audio generation[b24]\" - This paper is compared to and also used as a benchmark for the proposed method. \n\n3. \"Discrete diffusion model for text-to-sound generation[b60]\" - The authors' approach is framed as a direct improvement on the ideas presented in this paper.\n\n4. \"Large-scale contrastive language-audio pretraining with feature fusion and keyword to caption augmentation[b58]\" - The authors mention this paper when discussing the use of Contrastive Language-Audio Pretraining (CLAP), which is a significant part of their proposed method.\n\n5. \"Denoising diffusion probabilistic models[b12]\" - This paper is repeatedly cited when the authors are discussing diffusion models, which seem to be a fundamental part of their proposed method.\n\n6. \"Score-based generative modeling through stochastic differential equations[b54]\" - This paper is also cited when discussing diffusion models, suggesting it to be a significant reference.\n\nIn JSON format, the references and their confidence scores are:\n\n{\n\"b47\": 1,\n\"b24\": 0.9,\n\"b60\": 0.9,\n\"b58\": 0.8,\n\"b12\": 0.8,\n\"b54\": 0.8\n}\n\nThese scores reflect the level of direct correlation of these source papers to the proposed method in the paper, with a score of 1 indicating a very high level of dependency.", "{\"b24\": 0.9, \"b60\": 0.9, \"b47\": 0.8, \"b58\": 0.7, \"b12\": 0.6, \"b54\": 0.6}", "Based on the given information, the paper appears to be heavily inspired and influenced by the following papers:\n\n- \"StableDiffusion\" [b47]: This paper is directly mentioned as a source of inspiration for the method of using latent diffusion models (LDMs) for text-to-audio generation.\n- \"DiffSound\" [b60]: This paper is referenced multiple times, indicating it has significant influence on the methods. However, the author also mentions it has been \"superseded\" by their work, suggesting they have built upon and improved its ideas.\n- \"AudioGen\" [b24]: Similarly, this paper appears as an inspiration for the author's work. They also mention improving upon its methods.\n- \"Contrastive Language-Audio Pretraining\" [b58]: The author's method leverages this concept, suggesting this paper has a significant influence on their work.\n- \"Denoising Diffusion Probabilistic Models\" [b12]: This paper is mentioned in relation to the diffusion models used in the author's work.\n- \"Textually Guided Audio Generation\" [b24]: The concept of text-guided audio manipulation appears to be inspired by this paper.\n\nThe following is the result in json format:\n\n{\n  \"b47\": 1.0,\n  \"b60\": 0.9,\n  \"b24\": 0.9,\n  \"b58\": 0.8,\n  \"b12\": 0.7\n}"], "643e0ad10746dc40e34197a9": ["Based on the text and additional information provided, the following source papers seem to be the most influential for the given paper:\n\n1. 'Training language models to follow instructions with human feedback' - [b0]\n2. 'Leveraging gpt for post hoc transformation of free text radiology reports into structured reporting a multilingual feasibility study' - [b1]\n3. 'The utility of chatgpt as an example of large language models in healthcare education research and practice systematic review on the future perspectives and potential limitations' - [b2]\n4. 'Stanford alpaca an instruction following llama model' - [b10]\n5. 'LLaMA: Open and efficient foundation language models' - [b14]\n6. 'LoRA: Low Rank Adaptation of large language models' - [b11]\n7. '8-bit matrix multiplication for transformers at scale' - [b12]\n\nThese papers are directly related to the methods and approaches the author introduced to tackle the challenges in the paper, such as the implementation of large language models (LLMs) in the medical field, the fine-tuning of these models using reinforcement learning through human feedback (RLHF), and the use of open-source models to mitigate privacy concerns. \n\nHere is the confidence score for each paper:\n\n```json\n{\n    \"b0\": 0.85,\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b10\": 0.8,\n    \"b14\": 0.8,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n}\n```\nThe confidence scores are based on the degree of relevance and the frequency of mentions in the text.", "Based on the information given, here's an inference of the source papers that might have inspired the given paper. \n\n{\n\"b0\": 0.9, \n\"b10\": 0.9, \n\"b11\": 0.7, \n\"b12\": 0.7\n}\n\nExplanation:\n\nThe paper seems to be heavily influenced by \"b0: Training language models to follow instructions with human feedback\" and \"b10: Stanford Alpaca an instruction following llama model\", as it is explicitly mentioned that the authors followed the approach detailed in these papers to train the models. Therefore, they have a high-confidence score.\n\nThe papers \"b11: LoRA Low Rank Adaptation of Large Language Models\" and \"b12: Bit Matrix Multiplication for Transformers at Scale\" are mentioned in the context of the methodologies employed to train the models. Therefore, these papers have a moderately high confidence score.\n\nThe rest of the referenced papers are mostly related to the datasets used or the evaluation of the models, but they don't seem to have directly influenced the main contributions of the paper. Therefore, they are not included in the list.", "Based on the analysis of the provided text, the following papers appear to be the most pertinent source papers:\n\n1. \"Training language models to follow instructions with human feedback\" [b0]\n2. \"LLaMA: Open and Efficient Foundation Language Models\" [b14]\n3. \"LoRa: Low Rank Adaptation of Large Language Models\" [b11]\n\nHere is the reasoning for each:\n\n1. \"Training language models to follow instructions with human feedback\" [b0]: This paper is mentioned multiple times in the text and it seems to be the foundation for the algorithm proposed by the author for training the language model. The concept of RLHF from this paper also appears to be central to the training of the models presented in this study, making it a primary source of inspiration.\n\n2. \"LLaMA: Open and Efficient Foundation Language Models\" [b14]: This is the base model that the authors used for fine-tuning. As such, this paper is directly relevant to the main contributions of the study.\n\n3. \"LoRa: Low Rank Adaptation of Large Language Models\" [b11]: This paper introduced the LoRa adaptation method, which the authors adopted to efficiently fine-tune their models. This method was key to their proposed solution.\n\nPlease note that these are just educated guesses based on the text provided, and further clarification from the author might be needed to confirm these sources.\n\nThe JSON format of the result is:\n{\"b0\": 1, \"b14\": 0.8, \"b11\": 0.8}", "Based on the provided text and the author's notes, the primary challenge outlined in the paper is to develop an open-source model specifically fine-tuned for biomedical tasks that respects privacy concerns. The proposed solution is a blend of new and established open-source biomedical datasets, adapted into an instruction-following format. This approach is inspired by several previous works. The identified source papers are:\n\n1. Training language models to follow instructions with human feedback [b0]: This paper is cited in the context of the underlying methodology of the proposed solution. It introduces the concept of training language models to follow instructions, which is a key aspect of the paper's proposed solution.\n   \n2. LLaMA (Large Language Model Meta AI) foundation models [b14]: LLaMA models are the foundation for the proposed model in the paper. The authors fine-tuned these models for their task, which indicates strong reliance on this work.\n\n3. Low-Rank Adaptation (LoRA) for weight updates [b11]: This paper introduces the LoRA method, which the authors implement to make the training process more efficient.\n\n4. 8-bit matrix multiplication for transformers at scale [b12]: This paper is cited as the methodology for further reducing memory and compute demands, indicating a strong influence on the paper's contributions.\n\n5. The COVID-19 Open Research Dataset Challenge (CORD-19) [b4]: The authors include data from this dataset in their work, suggesting that it played a role in the development of their model.\n\n6. Measuring Massive Multitask Language Understanding [b6]: This paper is cited in the context of the benchmarks used to evaluate the performance of the proposed model, indicating its influence on the evaluation methodology used.\n\n7. Training data from the MedQA benchmark [b7]: This paper is cited as a source of training data for the proposed model, indicating its influence on the training process.\n\n8. Stanford alpaca an instruction following llama model [b10]: This paper details the approach to fine-tuning the LLaMA models, which the authors adhere to for their model.\n\nThe proposed JSON format for the source papers and confidence scores is:\n\n{\n    \"b0\": 1.0,\n    \"b14\": 1.0,\n    \"b11\": 1.0,\n    \"b12\": 1.0,\n    \"b4\": 0.9,\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b10\": 0.9\n}", "Based on the text and the titles of the references, the source papers for the given academic paper are likely to be as follows:\n\n- \"Training language models to follow instructions with human feedback\" ['b0']: This paper is referenced multiple times in the text and discusses the foundation of the model being used in the paper. It has a high probability of being a source paper. Confidence score: 0.9.\n\n- \"What disease does this patient have? A large-scale open domain question answering dataset from medical exams\" ['b7']: This paper is referenced in the context of the data being used for training and evaluation of the model in the paper. It has a high confidence score due to its relevance to the methodology of the paper. Confidence score: 0.85.\n\n- \"LLaMA: Open and Efficient Foundation Language Models\" ['b14']: This paper is about the underlying model (LLaMA) used in the paper. It is very relevant to the paper's methodology. Confidence score: 0.9.\n\n- \"LoRa: Low-Rank Adaptation of Large Language Models\" ['b11']: This paper discusses the LoRa method, which is used in the paper for fine-tuning the model. It directly contributes to the model's optimization and is therefore considered a source paper. Confidence score: 0.85.\n\n- \"Stanford Alpaca: An instruction-following LLAMA model\" ['b10']: This paper is about a similar model (Stanford Alpaca) and the fine-tuning approach used in it is also employed in the paper. The author indicates that they adopt the same approach. Confidence score: 0.8.\n\nSo, the json format of these source papers is:\n\n{\n  \"b0\": 0.9,\n  \"b7\": 0.85,\n  \"b14\": 0.9,\n  \"b11\": 0.85,\n  \"b10\": 0.8\n}"], "6334268390e50fcafd6a5a91": ["Based on the text and the list of references provided, the most pertinent source papers that inspire the authors of the given paper are:\n\n1. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" (b1): The authors frequently reference BERT, indicating their method was inspired by the BERT models. The confidence score is 0.9.\n2. \"BioBERT: A Pre-trained Biomedical Language Representation Model for Biomedical Text Mining\" (b9): This paper informs the authors' decision to create a model specifically pre-trained for the biomedical domain. The confidence score is 0.85.\n3. \"Domain-specific language model pretraining for biomedical natural language processing\" (b8): The authors cited this paper multiple times, indicating that they were inspired by the concept of domain-specific language model pre-training. The confidence score is 0.85.\n4. \"Improving Language Understanding by Generative Pre-training\" (b4) and \"Language Models are Unsupervised Multitask Learners\" (b5): These papers introduced the GPT models that the authors used as a foundation for their BioGPT model. The confidence score is 0.8.\n5. \"BioELECTRA: Pretrained Biomedical Text Encoder using Discriminators\" (b27): The authors may have taken inspiration from this paper to develop a pretrained language model for biomedical NLP. The confidence score is 0.7.\n6. \"Relation Extraction by End-to-End Language Generation\" (b23): This paper likely guided the authors in their approach to relation extraction tasks in biomedical text mining. The confidence score is 0.7.\n7. \"DARE: Data Augmented Relation Extraction with GPT\" (b20): This paper appears to have served as a reference for the authors in terms of applying GPT to biomedical literature. The confidence score is 0.65.\n\nThe JSON format result is:\n```json\n{\n    \"b1\": 0.9,\n    \"b9\": 0.85,\n    \"b8\": 0.85,\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b27\": 0.7,\n    \"b23\": 0.7,\n    \"b20\": 0.65\n}\n```", "{\"b8\": 0.9, \"b9\": 0.8, \"b4\": 0.8, \"b5\": 0.7, \"b23\": 0.6, \"b24\": 0.6, \"b13\": 0.6}", "Based on the text of the academic paper, the following references seem to be the most pertinent source papers:\n\n1. b1 - \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"\n2. b5 - \"Language Models are Unsupervised Multitask Learners\"\n3. b8 - \"Domain-specific Language Model Pretraining for Biomedical Natural Language Processing\"\n4. b9 - \"BioBERT: A Pre-trained Biomedical Language Representation Model for Biomedical Text Mining\"\n5. b17 - \"SciBERT: A Pretrained Language Model for Scientific Text\"\n6. b23 - \"REBEL: Relation Extraction by End-to-End Language Generation\"\n7. b46 - \"Attention is All You Need\"\n8. b47 - \"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\"\n\nThe confidence score for the source papers is estimated as follows:\n\n```\n{\n    \"b1\": 0.9,\n    \"b5\": 0.9,\n    \"b8\": 0.9,\n    \"b9\": 0.85,\n    \"b17\": 0.85,\n    \"b23\": 0.8,\n    \"b46\": 0.8,\n    \"b47\": 0.8\n}\n```", "Based on the text and author notes, the following references appear to be the most influential sources for this paper:\n\n1. \"BioBERT [b9] and PubMedBERT [b8]\": These papers are directly mentioned as inspirations for the creation of BioGPT. Both papers discuss the use of pre-training models on biomedical texts. \n2. \"BERT-like models [b1] [b2] [b3]\": These papers are mentioned as inspiration for the understanding tasks addressed by the paper.\n3. \"GPT-like models [b4] [b5] [b6]\": These papers are highlighted as inspiration for the generative tasks addressed by the paper.\n4. \"domain shift [b7] [b8]\": These papers are referenced as they highlight the challenge of applying pre-training models to the biomedical domain, which BioGPT aims to address. \n\nHence, the JSON format result would be: \n{\n\"b9\": 0.9,\n\"b8\": 0.9,\n\"b1\": 0.8,\n\"b2\": 0.8,\n\"b3\": 0.8,\n\"b4\": 0.7,\n\"b5\": 0.7,\n\"b6\": 0.7,\n\"b7\": 0.8\n}\nHere, \"b9\" and \"b8\" are given the highest confidence scores as they are directly mentioned as the inspiration for the creation of BioGPT. The rest are given slightly lower confidence scores as they were mentioned as general inspirations for the tasks addressed by the paper.\n", "Based on the text of the paper and the author's notes, the most relevant source papers that have inspired this work seem to be:\n\n1. \"Domain specific language model pretraining for biomedical natural language processing\" [b8]: This paper is frequently cited, and the current work follows its guidance on in-domain data and vocabulary for pre-training.\n2. \"Biobert a pre trained biomedical language representation model for biomedical text mining\" [b9]: BioBERT is mentioned as an earlier iteration of pre-training models on biomedical texts. The current work aims to advance this concept by focusing on generative tasks.\n3. \"Improving language understanding by generative pre-training\" [b4] and \"Language models are unsupervised multitask learners\" [b5]: These papers introduced the GPT models, which the current work builds upon and adapts for the biomedical domain.\n4. \"A sequence to sequence approach for document level relation extraction\" [b24]: This paper seems to have guided the author's approach to relation extraction as a sequence-to-sequence learning task.\n5. \"A relation specific attention network for joint entity and relation extraction\" [b35]: This paper appears to have influenced the author's approach to joint extraction methods.\n6. \"Pre train prompt and predict a systematic survey of prompting methods in natural language processing\" [b47]: This paper could have guided the author's approach to converting labels into sequences in natural language.\n\nThe confidence score is an estimated level of influence these papers had on the current work. Here are the estimated scores:\n\n{\"b8\": 0.9, \"b9\": 0.8, \"b4\": 0.8, \"b5\": 0.8, \"b24\": 0.7, \"b35\": 0.7, \"b47\": 0.7}"], "64a29621d68f896efa28fd4a": ["Based on the information given, the source papers that inspired the given paper can be identified as follows:\n\n- 'xucode an innovative technology for implementing complex instruction flows' (b0): The paper mentions Intel Xu-Code as a way for processor architects to write microcode almost as efficiently as native assembly, which is a concept that seems to have been adapted in the development of Metal.\n- 'ebpf introduction tutorials and community resources' (b3): The paper uses the example of how the Linux kernel offers eBPF to modify system behavior as an approach that the authors took inspiration from in developing Metal.\n- 'hardware is the new software' (b11): The paper cites this reference when discussing the growing popularity of microcode as a way to deploy processor features, which is a key concept in their proposal.\n- 'sanctum minimal hardware extensions for strong software isolation' (b13): This source is mentioned in the context of security concerns raised by the use of microcode, which seems to have been a significant motivation for the authors to develop an open architecture.\n- 'palcode for alpha microprocessors system design guide' (b17): The authors discuss this intermediate instruction encoding as an example of vertical microcode, which is a concept that underpins their proposal for Metal.\n- 'transactional locking ii' (b16): This reference is given when discussing the implementation of transactional memory, indicating that it likely inspired the authors' approach.\n- 'yosys open synthesis suite' (b6) and 'synopsys standard cell libraries' (b5): These sources are cited in the context of implementing Metal on a 5-stage pipelined RISC processor, and so they likely provided significant inspiration for the authors' methodological approach.\n\nHere is the result in json format:\n\n{\n    \"b0\": 0.9,\n    \"b3\": 0.8,\n    \"b11\": 0.8,\n    \"b13\": 0.9,\n    \"b17\": 0.9,\n    \"b16\": 0.9,\n    \"b6\": 0.9,\n    \"b5\": 0.9\n}", "Based on the text and the titles of the references, the source papers that inspire the main contributions of the given paper the most are:\n\n1. \"xucode an innovative technology for implementing complex instruction flows\" [b0]\n2. \"hardware is the new software\" [b11]\n3. \"transactional locking ii\" [b16]\n4. \"an exploratory analysis of microcode as a building block for system defenses\" [b30]\n5. \"reverse engineering x processor microcode\" [b31]\n6. \"practical transparent operating system support for superpages\" [b38]\n\nThese papers are directly related to the main contributions of the given paper which are about developing an open architecture for system developers to define new instructions and implementing high level architectural extensions such as user defined privilege levels and custom page tables.\n\nHere is the result in JSON format:\n\n{\n    \"b0\": 0.9,\n    \"b11\": 0.8,\n    \"b16\": 0.7,\n    \"b30\": 0.85,\n    \"b31\": 0.85,\n    \"b38\": 0.75\n}", "Based on the given text and titles of the references, the source papers that inspire the author most are:\n\n1. 'xucode an innovative technology for implementing complex instruction flows' [b0]\n2. 'hardware is the new software' [b11]\n3. 'an open architecture for developing processor features' [None]\n4. 'millicode in an ibm zseries processor' [b22]\n5. 'palcode for alpha microprocessors system design guide' [b17]\n6. 'transactional locking ii' [b16]\n7. 'reverse engineering x processor microcode' [b31].\n\nThe primary challenges outlined in the paper are to provide an open architecture to system developers to define new instructions and the complexity of implementing complex architectural features using horizontal microcode. The author proposes a processor hardware extension, Metal, which allows developers to rapidly evolve the processor's architecture through software instead of hardware.\n\nThe source papers are identified based on phrases indicating direct inspiration or motivation like \"inspired by\", \"motivated by\", and comparisons with previous work such as \"Unlike [reference], we...\", \"Improving upon [reference], our method...\".\n\nThe confidence score is calculated based on how closely the source papers relate to the challenges and methods outlined in the paper. The score is higher for references directly linked to these main challenges and methods.\n\nThe JSON format of the result:\n\n{\n  \"b0\": 0.9,\n  \"b11\": 0.8,\n  \"None\": 0.7,\n  \"b22\": 0.85,\n  \"b17\": 0.75,\n  \"b16\": 0.7,\n  \"b31\": 0.8\n}", "Based on the given text and the title of the references, the source papers for the given paper could be:\n\n1. \"xucode an innovative technology for implementing complex instruction flows\" [b0]\n2. \"ebpf introduction tutorials and community resources\" [b3]\n3. \"hardware is the new software\" [b11]\n4. \"transactional locking ii\" [b16]\n5. \"palcode for alpha microprocessors system design guide\" [b17]\n6. \"an exploratory analysis of microcode as a building block for system defenses\" [b30]\n7. \"reverse engineering x processor microcode\" [b31]\n\nThese papers are likely to be the main sources of inspiration for the given paper as they are directly referenced in the context of key challenges, methods, and novel contributions outlined in the paper.\n\nThe JSON format of the result would be:\n\n{\n  \"b0\": 0.9,\n  \"b3\": 0.8,\n  \"b11\": 0.85,\n  \"b16\": 0.8,\n  \"b17\": 0.85,\n  \"b30\": 0.9,\n  \"b31\": 0.9\n}", "Based on the given text and the titles of the references, the most pertinent source papers that inspired the author are:\n\n1. \"xucode an innovative technology for implementing complex instruction flows\" (b0). The author mentions the use of higher level intermediate instruction encodings, such as Intel Xu-Code. The Metal model proposed in the paper seems to take inspiration from this concept.\n\n2. \"hardware is the new software\" (b11). The author mentions the popularity of microcode as a way to deploy processor features, which is a topic central to this source paper.\n\n3. \"the implementation of prolog via vax microcode\" (b19). This seems to be one of the source papers as it discusses the use of microcode, a concept that is extensively explored in the author's paper.\n\n4. \"reverse engineering x processor microcode\" (b31). This reference discusses the reverse engineering of x86 microcode and injecting vulnerabilities, which seems to be a topic that the author has expanded on.\n\n5. \"an open architecture for developing processor features\" (None). The author's paper proposes an open architecture, and this paper could have inspired that approach.\n\n6. \"transactional locking ii\" (b16). As the author implements a proof-of-concept transactional memory extension based on software transactional memory techniques, this paper seems to be a relevant source.\n\nHere is the result in JSON format:\n\n```\n{\n\"b0\": 0.9,\n\"b11\": 0.9,\n\"b19\": 0.8,\n\"b31\": 0.9,\n\"None\": 0.9,\n\"b16\": 0.8\n}\n```\n\nThe confidence scores are subjective and are based on the relevance of the topics discussed in the source papers to the themes and challenges identified in the author's paper."], "640a9ffc90e50fcafd03ca47": ["Based on the text, the source papers which most inspire the given paper are:\n\n1. 'disambiguation of company names via deep recurrent networks' [Confidence Score: 0.95]\n    - This is the main paper that the given paper is built upon. It proposes the use of a Siamese Recurrent Neural Network for disambiguating company names, which is the same approach used in the given paper. \n\n2. 'named entity disambiguation at scale' [Confidence Score: 0.9]\n    - This paper is also highly relevant to the given paper. It proposes a Deep Learning approach to Named Entity Disambiguation (NED), which is the same problem the given paper is trying to solve. \n\n3. 'a comprehensive benchmark framework for active learning methods in entity matching' [Confidence Score: 0.85]\n    - This paper is mentioned in the context of active learning, which is one of the main methods used in the given paper. It proves that active learning is beneficial in the Entity Matching domain, which is directly relevant to the given paper.\n\n4. 'toponym matching through deep neural networks' [Confidence Score: 0.8]\n    - This paper is relevant as it deals with a problem similar to the one discussed in the given paper. It proposes an approach based on a Siamese Deep Neural Network architecture and benchmarks it against several distance-based methodologies.\n\n5. 'learning text similarity with siamese recurrent networks' [Confidence Score: 0.8]\n    - This paper is relevant as it proposes a Siamese Deep Learning model which is similar to the approach taken in the given paper. It also introduces some interesting data augmentation techniques, which could have inspired the given paper.\n\n6. 'active learning a survey' [Confidence Score: 0.75]\n    - This paper provides a comprehensive survey on active learning, which is a key method used in the given paper. \n\n7. 'a guided tour to approximate string matching' [Confidence Score: 0.75]\n    - This paper is relevant as it discusses approximate string matching algorithms, which is related to the problem the given paper is trying to solve.\n\nThe final result in json format is:\n{\n   \"disambiguation of company names via deep recurrent networks\": 0.95,\n   \"named entity disambiguation at scale\": 0.9,\n   \"a comprehensive benchmark framework for active learning methods in entity matching\": 0.85,\n   \"toponym matching through deep neural networks\": 0.8,\n   \"learning text similarity with siamese recurrent networks\": 0.8,\n   \"active learning a survey\": 0.75,\n   \"a guided tour to approximate string matching\": 0.75\n}", "Based on the paper text and the titles of the references, the most pertinent source papers that inspired the authors would be:\n\n{'b7': 0.9, 'b17': 0.8, 'b22': 0.7, 'b2': 0.7, 'b19': 0.6, 'b1': 0.6, 'b15': 0.5, 'b4': 0.5}\n\nExplanation:\n- 'b7' (Siamese Neural Networks an Overview Artificial Neural Networks): The paper's primary contribution is a Siamese Recurrent Neural Network, suggesting a high relevance of this source.\n- 'b17' (Learning Text Similarity with Siamese Recurrent Networks): The paper's methodology and focus on text similarity indicate a strong influence from this source.\n- 'b22' (Toponym Matching Through Deep Neural Networks): The paper discusses a similar problem of toponym matching with a Siamese Deep Neural Network architecture, indicating a strong influence.\n- 'b2' (Named Entity Disambiguation at Scale): The paper's focus on Named Entity Disambiguation (NED) suggests that this source is directly relevant.\n- 'b19' (TF-IDF Character N-Grams versus Word Embedding Based Models for Fine-Grained Event Classification): The paper uses a similar tf-idf vectors approach to the one described in this source.\n- 'b1' (Active Learning: A Survey): The paper's use of Active Learning suggests a strong influence from this source.\n- 'b15' (A comprehensive benchmark framework for active learning methods in entity matching): This source is relevant as the paper also discusses Active Learning in the Entity Matching domain.\n- 'b4' (Scaling to Very Very Large Corpora for Natural Language Disambiguation): This paper is related to the disambiguation task and the use of Active Learning, indicating some relevance.", "Based on the text of the paper and the titles of the references, here are the source papers that seem to have the most influence on the author's work:\n\n1. \"b2\": \"Named entity disambiguation at scale\"\n2. \"b7\": \"Siamese neural networks an overview artificial neural networks\"\n3. \"b15\": \"A comprehensive benchmark framework for active learning methods in entity matching\"\n4. \"b17\": \"Learning text similarity with siamese recurrent networks in proceedings of the st workshop on representation learning for nlp\"\n5. \"b22\": \"Toponym matching through deep neural networks\"\n\nHere is the JSON format of the result:\n\n{\n    \"b2\": 0.9,\n    \"b7\": 0.8,\n    \"b15\": 0.8,\n    \"b17\": 0.7,\n    \"b22\": 0.7\n} \n\nThe confidence scores are assigned based on how directly each source paper is relevant to the author's novel contributions, with a higher score indicating a stronger influence.", "Based on the text given, the source papers that inspired the most are:\n\n1. 'b7': 'siamese neural networks an overview artificial neural networks'\n2. 'b17': 'learning text similarity with siamese recurrent networks in proceedings of the st workshop on representation learning for nlp'\n3. 'b22': 'toponym matching through deep neural networks'\n4. 'b2': 'named entity disambiguation at scale'\n5. 'b15': 'a comprehensive benchmark framework for active learning methods in entity matching'\n6. 'b13': 'deepal deep active learning in python'\n\nIn the JSON format, it will be: \n{\"b7\": 1, \"b17\": 0.9, \"b22\": 0.8, \"b2\": 0.7, \"b15\": 0.6, \"b13\": 0.5}", "Based on the text and titles of the references, we can infer that the following papers might have significantly influenced the paper:\n\n{\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b2\": 0.9,\n    \"b17\": 0.9,\n    \"b22\": 0.8,\n    \"b15\": 0.8\n}"], "628304515aee126c0f6f0c83": ["Based on the given text, the paper's main contributions seem to center around the development of SystemMatch, an end-to-end machine learning pipeline that optimizes a preclinical model to best approximate the behavior of a target in sapiens population, with the use of single-cell genomic data. Here are some possible source papers that may have inspired the authors:\n\n1. \"Harnessing the predictive power of preclinical models for oncology drug development\" [b7]: This paper might be an inspiration as it discusses the importance of preclinical models in drug development, which is a central theme of the given paper.\n\n2. \"Machine learning for perturbational single cell omics\" [b9]: This paper could have influenced the development of SystemMatch as it introduces the concept of using machine learning for modeling perturbational single-cell data.\n\n3. \"Compositional Perturbational Autoencoder for single cell response modeling\" [b16]: The given paper uses Compositional Perturbational Autoencoder (CPA), a concept introduced in this source paper.\n\n4. \"Deep generative modeling for single cell transcriptomics\" [b14]: This paper could have inspired the use of deep neural networks in the SystemMatch model.\n\n5. \"Finding groups in data an introduction to cluster analysis\" [b11]: This paper might have inspired the k-medoids algorithm modification in the SystemMatch pipeline.\n\nHere is the JSON format:\n\n{\n    \"b7\": 0.9,\n    \"b9\": 0.85,\n    \"b16\": 1,\n    \"b14\": 0.8,\n    \"b11\": 0.8\n}\n\nEach score represents the confidence that the corresponding paper was a source of inspiration for the given paper, with 1 being the highest level of confidence.", "Based on the provided text, the following references seem to be the most influential sources for this paper:\n\n- b16: Compositional Perturbational Autoencoder for single cell response modeling: The authors mention they use CPA to predict behavior of single cells in untested experimental conditions, indicating this paper has a strong influence on their methodology. \n\n- b7: Harnessing the predictive power of preclinical models for oncology drug development: This paper is cited as a source describing the importance of improving preclinical models for better predictive results in clinical trials. It appears to be a main motivation for the current paper's topic.\n\n- b9: Machine learning for perturbational single cell omics: This is the primary paper which introduces the concept of predicting cellular responses to perturbations, a key concept in the current paper.\n\n- b34: Single cell analyses reveal key immune cell subsets associated with response to pd l blockade in triple negative breast cancer: The current paper uses CPA [b34] to generate in silico perturbed cells for additional conditions, indicating this paper's work directly influenced their methodology.\n\nThe confidence score for each reference is subjective and depends on the degree of influence it has had on the current paper. For example, if b16 is the main methodological inspiration, it may have a higher confidence score than b7, which provides general background information.\n\nHere is the returned result in the JSON format:\n\n{\n    \"b16\": 0.9,\n    \"b7\": 0.8,\n    \"b9\": 0.85,\n    \"b34\": 0.9\n}", "Based on the text provided, the source papers that appear to have most inspired this paper are:\n\n1. \"Compositional Perturbational Autoencoder for single cell response modeling\" [b16]. The paper uses a method from this source to predict the behavior of single cells in previously untested combinations of experimental conditions.\n2. \"Harnessing the predictive power of preclinical models for oncology drug development\" [b7]. This source is likely to have inspired the initial problem that this paper aims to address: the lack of adequate time and resources to improve preclinical model systems that might be more predictive of clinical results.\n3. \"Machine learning for perturbational single cell omics\" [b9]. This paper was likely drawn upon for the goal of predicting cellular responses to perturbations.\n4. \"An optimized protocol for human M macrophages using M-CSF and IL-4, IL-10, TGF-beta yields a dominant immunosuppressive phenotype\" [b19]. This paper seems to be a direct comparison to the current work, as it uses a different approach to generate in vitro suppressive macrophages.\n5. \"Single cell omics from assay design to biomedical application\" [b1]. As the paper uses single-cell genomics data, this source likely provided critical background information.\n\nThe JSON object representing this information is:\n\n{\n\"b16\": 1,\n\"b7\": 0.9,\n\"b9\": 0.9,\n\"b19\": 0.8,\n\"b1\": 0.8\n}", "Based on the text provided, the following papers appear to be key sources for the paper and have directly influenced its core contributions:\n\n{ \n    \"b16\": 0.9, \n    \"b7\": 0.8, \n    \"b9\": 0.7, \n    \"b15\": 0.6, \n    \"b4\": 0.5, \n    \"b19\": 0.4, \n    \"b34\": 0.3 \n}\n\nExplanation:\n\n- [b16] \"Compositional Perturbational Autoencoder for single cell response modeling\" is directly cited as the paper that inspired the use of Compositional Perturbational Autoencoder (CPA) in the SystemMatch model. \n\n- [b7] \"Harnessing the predictive power of preclinical models for oncology drug development\" is mentioned in the introduction as a key source of inspiration regarding the need to improve preclinical model systems.\n\n- [b9] \"Machine learning for perturbational single cell omics\" is cited as a key reference for the importance of predicting cellular responses to perturbations, an integral part of the paper's methodology.\n\n- [b15] \"scGen predicts single cell perturbation responses\" is mentioned in the context of predicting cellular perturbation response, which is a part of the paper's methodology.\n\n- [b4] \"Ovarian cancer cells direct monocyte differentiation through a non canonical pathway\" and [b19] \"An optimized protocol for human M macrophages using M-CSF + IL-4 + IL-10 + TGF? generates the most suppressive macrophages\" appear to be sources of inspiration for the paper's exploration of macrophage differentiation and polarization.\n\n- [b34] \"Single cell analyses reveal key immune cell subsets associated with response to PD-L blockade in triple negative breast cancer\" is the source of the method used to generate in silico perturbed cells for additional conditions in the paper. \n\nThe confidence score is based on how directly each source paper appears to contribute to the paper's novel contributions, with a higher score indicating a stronger influence.", "Based on the analysis of the text, the following are the source papers that most inspired the authors:\n\n1. \"Harnessing the predictive power of preclinical models for oncology drug development\" [b7]\nConfidence Score: 0.9\nThe paper refers to the lack of attention given to improving preclinical models which are predictive of clinical results. This challenge is directly addressed in the source paper [b7], suggesting a strong link.\n\n2. \"Machine learning for perturbational single-cell omics\" [b9]\nConfidence Score: 0.85\nThe paper introduces a new use case for perturbational modeling in predicting optimal model systems.\n\n3. \"Compositional Perturbational Autoencoder for single cell response modeling\" [b16]\nConfidence Score: 0.85\nThe paper introduces the use of Compositional Perturbational Autoencoder (CPA), which is a key methodology in the paper.\n\n4. \"Single cell omics from assay design to biomedical application\" [b1]\nConfidence Score: 0.8\nThe paper references the development of single-cell omics, which is a crucial component of the methodology in the paper.\n\n5. \"Animal models in translational medicine validation and prediction\" [b2]\nConfidence Score: 0.75\nThe paper discusses the use of preclinical models in drug testing, which is relevant to the topic of the paper.\n\n6. \"Finding groups in data an introduction to cluster analysis\" [b11]\nConfidence Score: 0.7\nThe paper discusses the use of the k-medoids algorithm, which is a key methodology in the paper.\n\n7. \"Single-cell analyses reveal key immune cell subsets associated with response to PD-L blockade in triple negative breast cancer\" [b34]\nConfidence Score: 0.65\nThe paper refers to using [b34] to generate in silico perturbed cells, indicating a methodological influence on the paper.\n\nThe result in Json format:\n{ \"b7\": 0.9, \"b9\": 0.85, \"b16\": 0.85, \"b1\": 0.8, \"b2\": 0.75, \"b11\": 0.7, \"b34\": 0.65 }"], "646c3addd68f896efa5d1972": ["Based on the given text and titles of the references, the most relevant source papers that heavily influenced the main contributions of the paper are:\n\n1. Reference [b16] (\"equivariant diffusion for molecule generation in d\") is directly mentioned as an inspiration for the diffusion model used in the paper for molecule generation. Confidence Score: 0.9\n2. Reference [b11] (\"symmetryadapted generation of d point sets for the targeted discovery of molecules\") and [b32] (\"a d generative model for structure-based drug design\") are highlighted as early studies on 3D molecule generation that the author's method improves upon. Confidence Score: 0.7\n3. Paper [b33] (\"an autoregressive flow model for d molecular geometry generation from scratch\") is mentioned as suffering from the scale and error accumulation problem, which is addressed by the authors' approach. Confidence Score: 0.7\n4. Reference [b41] (\"\") is mentioned in the context of employing non-autoregressive models for 3D molecule generation, which is also the approach taken by the authors. Confidence Score: 0.7\n5. Reference [b20] (\"hierarchical generation of molecular graphs using structural motifs\") is mentioned in relation to the hierarchical approach the authors are using. Confidence Score: 0.7\n6. Reference [b26] (\"efficient learning of non autoregressive graph variational autoencoders for molecular graph generation\") is mentioned in the context of discussing the advantages of non-autoregressive generative models for 3D molecule generation, which aligns with the authors' approach. Confidence Score: 0.65\n\nThe JSON format result is:\n{\n    \"b16\": 0.9,\n    \"b11\": 0.7,\n    \"b32\": 0.7,\n    \"b33\": 0.7,\n    \"b41\": 0.7,\n    \"b20\": 0.7,\n    \"b26\": 0.65\n}", "The source papers for this given paper seem to be:\n- [b16]: 'Equivariant diffusion for molecule generation in d' - The author mentions that [b16] \"proposed the first diffusion model for molecule generation and significantly improves the validity of generated molecules.\" This suggests a strong reliance on the methods proposed by [b16], indicating that it is a source paper. Confidence score: 0.9\n- [b11]: 'Symmetryadapted generation of d point sets for the targeted discovery of molecules' - The author refers to \"early studies on 3D molecule generation usually adopt an autoregressive approach [b11]\", suggesting that the author used this paper as a foundation for their study. Confidence score: 0.7\n- [b33]: 'An autoregressive flow model for 3D molecular geometry generation from scratch' - The author indicates that this paper's model suffers from the scale and error accumulation problem, which their paper aims to address. Confidence score: 0.7\n- [b34]: 'HDMapGen: A Hierarchical Graph Generative Model of High Definition Maps' - It's mentioned in the context of hierarchical graph generation, which seems to be directly related to the author's novel contributions. Confidence score: 0.7\n\nThe JSON format would be:\n```\n{\n\"b16\":0.9,\n\"b11\":0.7,\n\"b33\":0.7,\n\"b34\":0.7\n}\n```", "Based on the given text, the most relevant source papers that inspire the author are:\n\n1) \"Equivariant diffusion for molecule generation in d\" [b16]: The author states that their work is directly inspired by this paper's successful application of the diffusion model in molecule generation, which significantly improves the validity of generated molecules.\n\n2) \"An autoregressive flow model for 3D molecular geometry generation from scratch\" [b33]: This paper is mentioned as it outlines the issues faced in the generation of atoms one by one in a language generation way, which the author's paper directly addresses.\n\n3) \"Hierarchical generation of molecular graphs using structural motifs\" [b20]: This paper is relevant as it introduces hierarchical graph generation, which is a key aspect of the author's approach.\n\n4) \"Molgrow: a graph normalizing flow for hierarchical molecular generation\" [b25]: This paper is relevant as it presents a method for hierarchical molecule generation, which is used by the author.\n\n5) \"A 3D generative model for structure-based drug design\" [b32]: This paper is one of the early studies on 3D molecule generation, which is a primary challenge the author's paper seeks to address.\n\n6) \"Nonautoregressive generative models for 3D molecule generation due to their natural advantages of global modeling ability\" [b6] [b26] [b41]: These three papers are mentioned as they highlight the advantages of nonautoregressive generative models, which the author's paper directly builds upon.\n\nThe JSON format of the result is:\n\n{\n    \"b16\": 1,\n    \"b33\": 0.9,\n    \"b20\": 0.9,\n    \"b25\": 0.9,\n    \"b32\": 0.8,\n    \"b6\": 0.8,\n    \"b26\": 0.8,\n    \"b41\": 0.8\n} \n\nThe confidence scores are assigned based on the relevance and direct influence of the source papers on the author's novel contributions.", "Based on the given text and provided titles, the source papers that most strongly influenced the current paper are:\n\n1. \"equivariant diffusion for molecule generation in d\" [b16]: The text indicates that the current paper's authors were inspired by the diffusion model proposed in this paper. They used it as a base to develop their own model, HierDiff, for molecule generation. Hence they directly acknowledge the significant impact of this work.\n\n2. \"a d generative model for structure based drug design\" [b32]: This paper is mentioned in the context of the historical development of 3D molecule generation, and it seems to have influenced the approach taken by the authors of the current paper.\n\n3. \"an autoregressive flow model for d molecular geometry generation from scratch\" [b33]: This paper is indicated as having introduced problems that the authors of the current paper are trying to address with their model, which suggests it had a significant influence on their work.\n\n4. \"hierarchical graph to graph translation for molecules\" [b19]: This paper appears to have provided the basis for the hierarchical approach used by the authors of the current paper.\n\nBased on these observations, the source papers with their confidence scores are:\n\n{\"b16\": 0.9, \"b32\": 0.75, \"b33\": 0.75, \"b19\": 0.7}\n\nNote: The confidence scores are subjective and indicate the level of influence the source papers appear to have had on the current paper, based on the given text. A score of 1 indicates the highest level of influence.", "Based on the given text and the information provided, the following papers are likely to be the source papers for this paper:\n\n1. \"b16: Equivariant diffusion for molecule generation in d\"\n2. \"b11: Symmetry adapted generation of d point sets for the targeted discovery of molecules\"\n3. \"b33: An autoregressive flow model for d molecular geometry generation from scratch\"\n4. \"b21: Torsional diffusion for molecular conformer generation\"\n5. \"b20: Hierarchical generation of molecular graphs using structural motifs\"\n6. \"b26: Efficient learning of non autoregressive graph variational autoencoders for molecular graph generation\"\n7. \"b41: (title not provided)\"\n\nThe confidence score for each of these references could be approximated as follows (note that these are approximate and based on the presence of key phrases and the overall relevance of the reference to the challenges and methods outlined in the paper):\n\n1. \"b16: Equivariant diffusion for molecule generation in d\" - Confidence Score: 0.9\n2. \"b11: Symmetry adapted generation of d point sets for the targeted discovery of molecules\" - Confidence Score: 0.8\n3. \"b33: An autoregressive flow model for d molecular geometry generation from scratch\" - Confidence Score: 0.8\n4. \"b21: Torsional diffusion for molecular conformer generation\" - Confidence Score: 0.7\n5. \"b20: Hierarchical generation of molecular graphs using structural motifs\" - Confidence Score: 0.7\n6. \"b26: Efficient learning of non autoregressive graph variational autoencoders for molecular graph generation\" - Confidence Score: 0.7\n7. \"b41: (title not provided)\" - Confidence Score: 0.6\n\nThis can be represented as a json object:\n\n{\"b16\": 0.9, \"b11\": 0.8, \"b33\": 0.8, \"b21\": 0.7, \"b20\": 0.7, \"b26\": 0.7, \"b41\": 0.6}"], "6423ac7790e50fcafd55eacf": ["Based on the given information, the most pertinent source papers that inspired the author in the given paper might be:\n\n- b4: 'hitter hierarchical transformers for knowledge graph embeddings' - The author mentioned they used a Transformer-based model, which was inspired by the HITTer model.\n\n- b32: 'kepler a unified model for knowledge embedding and pre trained language representation' - The author mentioned the success of transfer learning in natural language processing (NLP) and the use of pre-training on a large-scale KG, which aligns with the focus of the KEPLER paper.\n\n- b9: 'bert pre training of deep bidirectional transformers for language understanding' - The author mentioned the use of BERT-like encoder for entity surface forms, indicating a reliance on the BERT model.\n\n- b2: 'translating embeddings for modeling multirelational data' - The author mentioned the TransE-like scoring function, which originates from the TransE model.\n\nThe JSON format of the result could be: \n{\"b4\": 0.9, \"b32\": 0.8, \"b9\": 0.7, \"b2\": 0.6} \n\nThe confidence scores are subjective and might be different based on the judgement of the person who evaluates the paper. The scores provided here are just a rough estimation based on the given information.", "Based on the text and the titles of the references, the most relevant source papers for the given paper seem to be:\n\n1) 'HITTer: Hierarchical Transformers for Knowledge Graph Embeddings' [b4]: The paper introduces iHT, a pre-training backbone, which is inspired by the success of transfer learning in NLP and is augmented with a Transformer-based relational scoring module, a technique proven to be more expressive and versatile than conventional approaches. The similarity in method and the explicit mention of inspiration make this a highly relevant source paper.\n\n2) 'KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation' [b32]: The paper introduces a Transformer-based inductive KG representation model for KG completion and pre-trains it on the Wikidata5M dataset. The paper's novel contributions, the model and the dataset, are directly relevant to the source paper. \n\n3) 'MLMLM: Link Prediction with Mean Likelihood Masked Language Model' [b6]: The paper introduces inductive entity representations from text via link prediction, which seems to be a method used in the given paper. \n\n4) 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding' [b9]: The paper uses pretrained BERT weights to initialize the entity Transformer, which improves the model's performance. \n\nBased on this analysis, the JSON format result is:\n\n{\n \"b4\": 1,\n \"b32\": 0.95,\n \"b6\": 0.9,\n \"b9\": 0.85\n}", "Based on the text of the academic paper, the source papers that likely inspired the authors most are:\n\n1. \"Knowledge Enhanced Contextual Word Representations\" [b19] - As the paper discusses the important role of Knowledge Graphs (KGs) in natural language understanding tasks such as language modeling and this source paper deals directly with enhancing word representations using KGs.\n   \n2. \"Translating Embeddings for Modeling Multi-relational Data\" [b2] - The authors of the research paper propose a Transformer-based inductive KG representation model for KG completion. This suggests that they may have drawn inspiration from this paper, which discusses translating embeddings for KGs.\n\n3. \"KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation\" [b32] - The authors mention transferring knowledge between structured KGs and unstructured texts, a concept that is central to the KEPLER paper.\n\n4. \"HiTTER: Hierarchical Transformers for Knowledge Graph Embeddings\" [b4] - The authors mention a Transformer-based relational scoring module, which is a concept discussed in the HiTTER paper.\n\n5. \"MLMLM: Link Prediction with Mean Likelihood Masked Language Model\" [b6] - The authors mention the success of transfer learning in natural language processing (NLP) and improving the performance of KG completion models, which is a concept discussed in the MLMLM paper.\n\n6. \"Transformers: State of the Art Natural Language Processing\" [b34] - Given the focus on Transformer models in the authors' proposed algorithm, it's likely that this foundational paper on Transformers in NLP provided key inspiration.\n\nBased on the analysis, here is the result in the required format:\n{\n  \"b19\": 0.9,\n  \"b2\": 0.85,\n  \"b32\": 0.95,\n  \"b4\": 0.9,\n  \"b6\": 0.85,\n  \"b34\": 0.9\n}\n \nThe confidence scores are provided based on the paper text, with higher scores indicating a stronger inferred influence on the authors' work.", "Based on the provided information, the following references are inferred as the source papers:\n\n1. \"HITer: Hierarchical Transformers for Knowledge Graph Embeddings\" [b4]: This paper is mentioned in the context of the proposed model iHT using a Transformer-based relational scoring module. The authors indicate this approach has been proven more expressive and versatile than conventional methods. Confidence Score: 0.9\n\n2. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b9]: iHT's entity Transformer is designed in a similar way to BERT. This paper is also mentioned in the context of initializing the entity Transformer with pre-trained BERT weights. Confidence Score: 0.9\n\n3. \"Translating Embeddings for Modeling Multi-relational Data\" [b2]: This paper is referenced in the context of discussing conventional knowledge graph embedding approaches. Confidence Score: 0.7\n\n4. \"KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation\" [b32]: This paper is mentioned as a baseline for comparison. It's significant because the proposed model iHT outperforms it. Confidence Score: 0.7\n\n5. \"MLMLM: Link Prediction with Mean Likelihood Masked Language Model\" [b6]: This paper is mentioned as a baseline for comparison. iHT outperforms this model as well. Confidence Score: 0.7\n\n6. \"Large-scale Pre-training for Neural Information Processing Systems\" [b37]: This paper is cited in the context of the successful application of transfer learning in natural language processing. Confidence Score: 0.7\n\nBased on the above analysis, the JSON format output would be:\n\n{\"b4\": 0.9, \"b9\": 0.9, \"b2\": 0.7, \"b32\": 0.7, \"b6\": 0.7, \"b37\": 0.7}", "Based on the text analysis, the most pertinent source papers that inspired the authors could be:\n\n1. \"HITter: Hierarchical Transformers for Knowledge Graph Embeddings [b4]\": The authors mention that they used a Transformer-based relational scoring module, which was proven to be more expressive and versatile than conventional approaches, to capture various KG contexts. This indicates a strong reliance on this paper for developing the paper's core contributions. Confidence Score: 0.9\n\n2. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [b9]\": The authors followed the same input format as specified in [b9]. This suggests that the authors used this paper as a reference for implementing their entity Transformer. Confidence Score: 0.85\n\n3. \"KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation [b32]\": The authors compared the performance of their iHT model with the KEPLAR model and mentioned that their model outperforms the KEPLAR model. Confidence Score: 0.8\n\n4. \"MLMLM: Link Prediction with Mean Likelihood Masked Language Model [b6]\": The authors compared the performance of their iHT model with the MLMLM model and mentioned that their model outperforms the MLMLM model. Confidence Score: 0.75\n\n5. \"TransE: Translating embeddings for modeling multi-relational data [b2]\": The authors mention that they used TransE-like scoring function, this suggests that they took inspiration from this paper for developing their scoring function. Confidence Score: 0.7\n\n6. \"Freebase: A collaboratively created graph database for structuring human knowledge [b1]\": The authors used Freebase as one of the examples of KGs. Confidence Score: 0.65\n\nThe JSON format of the results can be:\n\n{\n  \"b4\": 0.9,\n  \"b9\": 0.85,\n  \"b32\": 0.8,\n  \"b6\": 0.75,\n  \"b2\": 0.7,\n  \"b1\": 0.65\n}"], "63b3f1fc90e50fcafdea3697": ["Based on the text of the paper and the author's notes, the paper is primarily concerned with solving the problem of tensor program optimization for dynamic shapes. The main method used for this involves breaking down the original shape into smaller micro-kernels for scheduling optimization, with a cost model also based on micro-kernels. The paper also considers the case of kernels requiring padding. The BERT model is used for evaluation, and the paper claims performance improvement and increased compilation speed, with search complexity reducing to O(1) from the O(|S|) of the auto-TVM method.\n\nFrom the text of the paper, the following references seem to be the most relevant:\n\n1. \"TVM [b9]\" - mentioned multiple times as the foundational auto-scheduler method that the paper builds upon.\n2. \"Ansor [b45]\" - also an auto-scheduler framework that is mentioned multiple times, and compared against in the evaluation.\n3. \"BERT [b12]\" - used for evaluation in the paper.\n4. \"cuDNN [b11] and cuBLAS [b24]\" - vendor libraries mentioned as benchmarks for comparison. \n5. \"Auto-schedulers [b1], [b37]\" - other auto-scheduler frameworks that are mentioned for comparison.\n6. \"TensorFlow [b0], PyTorch [b30], and MXNet [b8]\" - mentioned in relation to their reliance on vendor libraries.\n7. \"Neural architecture search [b48]\" - mentioned as a scenario where dynamic shapes are common.\n\nBased on these, the source papers could be represented as follows:\n\n```json\n{\n    \"b9\": 0.9,\n    \"b45\": 0.9,\n    \"b12\": 0.8,\n    \"b11\": 0.7,\n    \"b24\": 0.7,\n    \"b1\": 0.6,\n    \"b37\": 0.6,\n    \"b0\": 0.5,\n    \"b30\": 0.5,\n    \"b8\": 0.5,\n    \"b48\": 0.5\n}\n```\nThe confidence score is high for \"b9\" and \"b45\" as these are the key methods that the paper builds upon and compares against. The score is slightly lower for \"b12\", since while it is used for evaluation, it is not a methodological inspiration. \"b11\" and \"b24\" are important vendor libraries and hence have a moderate score. The other references (\"b1\", \"b37\", \"b0\", \"b30\", \"b8\", \"b48\") are mentioned in the context of the paper but do not seem to be direct inspirations for the methodology, and hence have lower scores.", "Based on the text provided, the source papers that have inspired the author's work are:\n\n1. \"Tensorflow a system for large scale machine learning\" (b0)\n2. \"Learning to optimize Halide with tree search and random programs\" (b1)\n3. \"MXNet a flexible and efficient machine learning library for heterogeneous distributed systems\" (b8)\n4. \"TVM an automated end-toend optimizing compiler for deep learning\" (b9)\n5. \"Learning to optimize tensor programs\" (b10)\n6. \"Efficient primitives for deep learning\" (b11)\n7. \"Pre-training of deep bidirectional transformers for language understanding\" (b12)\n8. \"Deep residual learning for image recognition\" (b17)\n9. \"Attention is all you need\" (b38)\n10. \"Generative high performance tensor programs for deep learning\" (b45)\n\nThe confidence scores are estimated based on the text and the author's notes, with a higher score indicating a stronger influence on the paper's novel contributions. \n\nThe result in JSON format:\n\n{\n\"b0\": 0.8,\n\"b1\": 0.7,\n\"b8\": 0.75,\n\"b9\": 0.85,\n\"b10\": 0.65,\n\"b11\": 0.7,\n\"b12\": 0.8,\n\"b17\": 0.65,\n\"b38\": 0.7,\n\"b45\": 0.85\n} \n\nPlease note that the confidence scores are subjective and are estimated based on the information provided in the text and the author's notes.", "Based on the information given in the task, author notes and the text of the paper, the most pertinent source papers that inspired the author are:\n\n- TVM [b9]: The author indicates that the TVM auto-scheduler methods have inspired their work, but they found limitations in its application to dynamic shape tensor programs which they have sought to overcome in their paper.\n\n- Ansor [b45]: The author compares their work with Ansor, another auto-scheduler, multiple times throughout the paper and it seems to have influenced the approach the author has taken.\n\n- BERT [b12]: The author evaluates their method using the BERT model and seems to be inspired by language modelling application.\n\n- Vendor Libraries [b11] [b24]: The author mentions vendor libraries, specifically cuDNN [b11] and cuBLAS [b24], as an inspiration for their work, but also as a comparison for the limitations their paper seeks to overcome.\n\n- Tensor Comprehensions [b37]: The author references Tensor Comprehensions as a current auto-scheduler method, indicating that it has inspired their work but they have sought to improve upon it.\n\n- Neural architecture search [b48]: The author mentions scenarios where their work would be useful, including neural architecture search [b48]. This suggests that the challenges posed by neural architecture search have inspired the author's paper.\n\nIn JSON format, the references are:\n\n{\n \"b9\": 0.8,\n \"b45\": 0.8,\n \"b12\": 0.7,\n \"b11\": 0.7,\n \"b24\": 0.7,\n \"b37\": 0.7,\n \"b48\": 0.6\n}\n\nThe confidence scores are estimated based on the frequency and context of the references in the text.", "Based on the text of the paper and the instructions provided, the auto-scheduler frameworks such as Ansor [b45], TVM [b9], Halide auto-scheduler [b1], and Tensor Comprehensions [b37] are mentioned as the primary inspirations for the presented work. The paper frequently contrasts its approach to these existing auto-schedulers, indicating that they were a significant motivator for the presented work. The specific phrase \"Inspired by [reference]\" is not used, but the paper's language strongly suggests these works are central to its contributions. \n\nHowever, the paper also mentions vendor libraries (e.g., oneDNN (oneAPI, 2021) on Intel CPUs; cuDNN [b11] and cuBLAS [b24] on NVIDIA GPUs) as essential background but not direct inspirations for the work.\n\nBased on this analysis, the most pertinent source papers are:\n\n{\n\"b45\": 0.9,\n\"b9\": 0.8,\n\"b1\": 0.7,\n\"b37\": 0.7\n}\n\nThe confidence score is based on the frequency and context in which these papers are mentioned. The paper Ansor [b45] has the highest confidence score as it is mentioned most frequently and in a context that suggests it was a significant motivator for the work presented. \n\nPlease note that this interpretation is based on the text provided and may not capture the full context of the paper's contributions and motivations.", "Based on the text provided, the main challenge outlined in the paper is the optimization problem of tensor programs with dynamic shapes. The current auto-scheduler methods rely heavily on the input operator's shape and loop structure to construct the search space and cost model. However, these methods are not effective for dynamic shape tensor programs due to the large search space they need to construct and the difficulty in creating an effective cost model due to changing input shapes.\n\nThe authors of the paper propose a solution in the form of a shape-generic search space, which breaks down the original shape into smaller micro-kernels for scheduling optimization. The cost model is also based on these micro-kernels. This approach allows for the reuse of learned shape-generic micro-kernel cost models and search results, resulting in a significant increase in compilation speed.\n\nTo identify the source papers that have directly inspired or motivated the authors, I looked for key phrases such as \"inspired by\", \"motivated by\", \"following.. we adopt ...\", etc. in the text. However, these phrases are not present in the provided text. Therefore, I will look for references that are directly linked to these main challenges and methods that have been outlined by the authors.\n\nUnfortunately, due to the lack of specific phrases indicating direct inspiration or motivation and the need to evaluate the relevance of each reference in relation to the paper's novel contributions, I am unable to provide a concise list of source papers at this time. However, a few papers that may have influenced this work based on the context and the problems addressed could be:\n\n- Auto-scheduler frameworks: Ansor [b45], TVM [b9], Halide auto-scheduler [b1], and Tensor Comprehensions [b37] \n- Neural architecture search [b48]\n- BERT [b12], a state-of-the-art language modelling application\n- Vendor libraries (oneAPI, 2021; [b11] [b24]\n- Existing auto-schedulers and their limitations [b43] [b36] [b40]\n\nThe confidence score for these papers would need to be reviewed and evaluated based on further analysis of the paper's text and the specifics of each reference.\n\nHere is the preliminary result in json format:\n\n{\n\"b45\": 0.8,\n\"b9\": 0.8,\n\"b1\": 0.8,\n\"b37\": 0.8,\n\"b48\": 0.7,\n\"b12\": 0.7,\n\"b11\": 0.6,\n\"b24\": 0.6,\n\"b43\": 0.6,\n\"b36\": 0.6,\n\"b40\": 0.6\n}"], "648fd298d68f896efa163bfb": ["Based on the provided text, the following references are identified as source papers:\n\n- \"Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b0]: The paper states that BERT, proposed in this work, is the earliest approach that utilized optimized objectives like Masked Language Model (MLM) to pre-train on large text corpora.\n- \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" [b2]: This paper introduced T5, which the current work references as an inspiration for treating multiple NLP tasks as text-to-text transformation tasks.\n- \"Language Models are Few-Shot Learners\" [b3]: The author cites GPT-3, which uses large auto-regressive model for few-shot predictions, as a model that influenced their approach.\n- \"Scaling Language Modeling with Pathways\" [b4]: The paper mentions PALM, a Transformers-based and Pathways-enabled large-scale language model, as a model that influenced their approach.\n- \"Training language models to follow instructions with human feedback\" [b6]: This paper is mentioned in the context of aligning with user intent across various tasks through fine-tuning and reinforcement learning with human feedback.\n- \"Low Rank Adaptation of Large Language Models\" [b17]: The paper mentions that they adopted LoRA, a parameter efficient fine tuning method, to efficiently train the large language model.\n- \"ZERO: Memory Optimizations Toward Training Trillion Parameter Models\" [b18]: The paper mentions that they employed ZeRO-2 to decrease memory usage and improve training speed.\n\nTherefore, the answer is:\n\n```json\n{\n    \"b0\": 0.9,\n    \"b2\": 0.9,\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b6\": 0.8,\n    \"b17\": 0.7,\n    \"b18\": 0.7\n}\n```\nThe confidence score is subjective and might vary depending on the context. In this case, the scores are based on the weight of the contribution each source paper had on the development of the main paper. The higher the score, the more influential the source paper is considered.", "Based on the given text, the source papers that inspired this work are:\n\n1. \"Pre-training of deep bidirectional transformers for language understanding\" - [b0]\n2. \"Exploring the limits of transfer learning with a unified text-to-text transformer\" - [b2]\n3. \"Language models are few-shot learners\" - [b3]\n4. \"Training language models to follow instructions with human feedback\" - [b6]\n5. \"On the dangers of stochastic parrots: can language models be too big?\" - [b10]\n6. \"A B-parameter open access multilingual language model\" - [b16]\n7. \"Low rank adaptation of large language models\" - [b17]\n8. \"Zero: memory optimizations toward training trillion parameter models\" - [b18]\n\nThe confidence scores are inferred from the content of the paper and the level of reliance on each paper's methodologies or findings.\n\n{\n    \"b0\": 0.8,\n    \"b2\": 0.85,\n    \"b3\": 0.9,\n    \"b6\": 0.75,\n    \"b10\": 0.65,\n    \"b16\": 0.8,\n    \"b17\": 0.85,\n    \"b18\": 0.7\n}", "Based on the text of the paper and the titles of the references, the source papers that inspired the authors the most seem to be:\n\n{'b2': 0.9, 'b3': 0.8, 'b4': 0.7, 'b6': 0.9, 'b11': 0.6, 'b15': 0.7, 'b17': 0.9, 'b18': 0.7}\n\nExplanation:\n\n- 'b2': The authors mention the T5 model multiple times and adopt its approach of treating NLP tasks as text-to-text transformations.\n- 'b3': GPT-3 is also mentioned multiple times, and the authors seem to have learned from its approach of using large auto-regressive models for few-shot predictions.\n- 'b4': PALM is mentioned as a resource-efficient model that achieves state-of-the-art results, which the authors likely drew inspiration from.\n- 'b6': The authors adopt a similar approach to InstructGPT, using fine-tuning and reinforcement learning with human feedback to improve their model.\n- 'b11': The cMedQA2 dataset, introduced in this paper, is used for training and evaluation, indicating its importance to the authors' research.\n- 'b15': The authors adopt the method proposed by this paper for reinforcement learning.\n- 'b17': The authors adopted the LoRA (Low-Rank Approximated adapter) method for efficient fine-tuning.\n- 'b18': The authors made use of ZeRO-2, mentioned in this paper, to decrease memory usage and improve training speed.\n\nThe confidence scores indicate the level of direct influence these papers seem to have had on the authors' research, with a higher score indicating stronger influence.", "Based on the text of the paper and the titles of the references, the source papers that most inspired the authors are:\n\n1. \"Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b0]: This paper introduced BERT, and the authors mention using this type of model in their work. Confidence: 0.9\n\n2. \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" [b2]: This paper introduced T5, which the authors directly mention using in their work. Confidence: 0.9\n\n3. \"Language Models are Few-Shot Learners\" [b3]: The authors of the current paper mention using a method similar to GPT-3's few-shot learning, indicating that this paper likely had a significant influence. Confidence: 0.8\n\n4. \"Training Language Models to Follow Instructions with Human Feedback\" [b6]: The authors mention using similar methods of fine-tuning and reinforcement learning with human feedback, indicating that this paper likely had a significant influence. Confidence: 0.8\n\n5. \"Low-Rank Adaptation of Large Language Models\" [b17]: The authors mention using the LoRA method for efficient fine-tuning, indicating that this paper likely had a significant influence. Confidence: 0.8\n\n6. \"A B Parameter Open Access Multilingual Language Model\" [b16]: The authors mention using the BLOOM-7B model as their base model, indicating that this paper likely had a significant influence. Confidence: 0.8\n\nBased on this analysis, the JSON format would be:\n\n{\n    \"b0\": 0.9,\n    \"b2\": 0.9,\n    \"b3\": 0.8,\n    \"b6\": 0.8,\n    \"b17\": 0.8,\n    \"b16\": 0.8\n}", "Based on the text, the following references are likely to have been significant sources of inspiration for this paper:\n\n{ \n\"b0\": 0.9, \n\"b2\": 0.8, \n\"b3\": 0.9, \n\"b4\": 0.7, \n\"b6\": 0.8, \n\"b11\": 0.7, \n\"b12\": 0.6, \n\"b17\": 0.8\n}\n\nExplanation:\n\nb0: The BERT model has been mentioned as one of the earliest approaches that influenced the pre-training and fine-tuning process of large language models which forms the basis for the ClinicalGPT model.\n\nb2: The T5 model is mentioned multiple times as a significant reference point, specifically for its approach treating NLP tasks as text-to-text transformation tasks.\n\nb3: The GPT-3 model is another important reference, particularly for its few-shot prediction capabilities.\n\nb4: The PALM model has influenced the ClinicalGPT model due to its resource efficiency and ability to achieve state-of-the-art few-shot results.\n\nb6: The InstructGPT model is referenced as a significant source of inspiration for aligning user intent and improving output generation.\n\nb11: The cMedQA2 dataset is crucial for the training and evaluation of the ClinicalGPT model.\n\nb12: The MEDQA dataset is another important training and evaluation resource.\n\nb17: The paper has adopted the LoRA method for its efficient fine-tuning, indicating the relevance of this reference."], "6456385ad68f896efacf20ac": ["Based on the text of the paper and the author notes, the following references are the source papers that inspired the research paper:\n\n1. 'LLaMA [b16]': The paper mentioned several times that its work is based on the LLaMA model, indicating a strong reliance on this previous research.\n\n2. 'Alpaca [b15] and Vicuna [b1]': The paper referred to these works as examples of how open-source LLMs were used and adapted for specific applications or domains, suggesting that these papers might have inspired the authors in their approach.\n\n3. 'S2ORC [b6]': The authors utilized this dataset for their research, indicating that this might be one of their source papers.\n\n4. 'GPT2 [b13]': The authors used the autoregressive generation objective introduced in this paper for their fine-tuning process, indicating that this paper might have been a source of inspiration.\n\n5. 'AdamW optimizer [b7]': The authors used this optimizer for their training, indicating that this paper might have been a source of inspiration.\n\n6. 'PEFT Low-Rank Adaptation (LoRA) [b3]': The authors used this method for their parameter-efficient fine-tuning, indicating that this paper might have been a source of inspiration.\n\n7. 'LMFlow [b2]': The authors compared their work with this paper, indicating that this paper might have been a source of inspiration.\n\n8. 'MedMCQA [b12]': The authors used this dataset for their research, indicating that this paper might have been a source of inspiration.\n\nThe confidence scores for these references can be assigned based on their relevance to the paper's novel contributions and the level of reliance indicated in the text. For example, 'LLaMA [b16]' should have a high confidence score as it is the basis for the work presented in this paper.\n\nHere is the JSON format of the results:\n\n```json\n{\n  \"b16\": 1.0,\n  \"b15\": 0.8,\n  \"b1\": 0.8,\n  \"b6\": 0.9,\n  \"b13\": 0.85,\n  \"b7\": 0.85,\n  \"b3\": 0.9,\n  \"b2\": 0.8,\n  \"b12\": 0.9\n}\n```", "Based on the author's notes and the text of the paper, the papers that inspired this one the most appear to be:\n\n1. \"Open and efficient foundation language models\" [b16]: This paper introduces the LLaMA model which this research is directly building upon. The authors propose a medical-domain-specific version of this foundational model, which they name PMC-LLaMA. The citation is directly linked to the main contribution of the paper and thus has a high confidence score.\n\n2. \"S ORC: The Semantic Scholar Open Research Corpus\" [b6]: This paper introduces the S2ORC dataset, which the authors used as the basis for their dataset. The authors specifically mention that they started with this dataset, indicating that it played a significant role in their research.\n\n3. \"Language models are unsupervised multitask learners\" [b13]: This paper introduces the GPT2 model, which the authors reference when explaining their fine-tuning process. The citation is directly linked to the method they used to train their model.\n\n4. \"Decoupled weight decay regularization\" [b7]: This paper is related to the AdamW optimizer which was used in the fine-tuning process.\n\n5. \"A dataset for biomedical research question answering\" [b5], \"MedMCQA: A large-scale multi-subject multi-choice dataset for medical domain question answering\" [b12] and \"What Disease Does This Patient Have? A Large-scale Open-domain Question Answering Dataset from Medical Exams\" [USMLE]: These papers introduce the PubMedQA, MedMCQA, and USMLE datasets, respectively, which were used in the evaluation process.\n\n6. \"PEFT: State-of-the-art Parameter Efficient Fine-tuning Methods\" [b8] and \"Low-rank adaptation of large language models\" [b3]: These papers are related to the parameter-efficient fine-tuning method which was used in this research.\n\nBased on these findings, the source papers with their corresponding confidence scores are as follows:\n\n{\n\"b16\": 1,\n\"b6\": 0.9,\n\"b13\": 0.8,\n\"b7\": 0.8,\n\"b5\": 0.7,\n\"b12\": 0.7,\n\"USMLE\": 0.7,\n\"b8\": 0.6,\n\"b3\": 0.6\n}", "Based on the text provided, the source papers that seem to have heavily influenced the paper's main contributions are:\n\n1. \"LLaMA [b16]\" - As the paper is built on the LLaMA model and fine-tuned it towards a medical-specific corpus, the LLaMA paper is a crucial reference. The confidence score might be around 0.9 due to the frequent mentions and direct usage of LLaMA in the paper.\n\n2. \"S2ORC [b6]\" - This paper is crucial as it provides the dataset that enables the fine-tuning of the LLaMA model towards a medical-specific corpus. The confidence score could be around 0.8 because it provides the very data for the model training.\n\n3. \"GPT2 [b13]\" - This paper is also important as the fine-tuning objective is based on the methodology introduced in GPT2. The confidence score might be around 0.7 due to the direct usage of GPT2's methodology.\n\n4. \"AdamW optimizer [b7]\" - This paper is significant as it provides the optimization method used in training the model. The confidence score could be around 0.7 because it provides the optimization method for model training.\n\n5. \"PEFT Low-Rank Adaptation (LoRA) [b3]\" - This paper is important as it offers the method for parameter-efficient fine-tuning. The confidence score might be around 0.7 due to the direct usage of its method.\n\n6. \"PubMedQA [b5]\" and \"MedMCQA [b12]\" - These papers are important as they provide the evaluation benchmarks. The confidence score might be around 0.7 because they provide the benchmarks for model evaluation.\n\n7. \"LMFlow [b2]\" - The paper is significant as it is used for comparison and benchmarking. The confidence score could be around 0.6 due to the comparative usage in the paper.\n\nIn JSON format:\n\n{\n\"b16\": 0.9,\n\"b6\": 0.8,\n\"b13\": 0.7,\n\"b7\": 0.7,\n\"b3\": 0.7,\n\"b5\": 0.7,\n\"b12\": 0.7,\n\"b2\": 0.6\n}", "Based on the information provided, the source papers for this academic paper would be:\n\n1. \"LLaMA [b16]\" - The paper primarily focuses on refining the LLaMA model for medical applications. This is the foundational work that the paper builds upon.\n2. \"PMC-LLaMA [None]\" - This seems to be the paper's own novel method that is being introduced.\n3. \"S2ORC [b6]\" - This is the dataset that the authors used for fine-tuning their model.\n4. \"GPT2 [b13]\" - The authors used the autoregressive generation objective from GPT2 during their fine-tuning process.\n5. \"AdamW optimizer [b7]\" - The authors used the AdamW optimizer for training their model.\n6. \"PEFT Low-Rank Adaptation (LoRA) [b3]\" - This is the method the authors used for parameter-efficient fine-tuning.\n7. \"PubMedQA [b5]\" and \"MedMCQA [b12]\" - These are the QA datasets that the authors used for evaluation.\n\nThe confidence score for each is estimated to be very high (approximately 1) as they are directly cited and used in the paper's methodology. \n\nThe JSON format of the result is:\n{\n\"b16\": 1,\n\"None\": 1,\n\"b6\": 1,\n\"b13\": 1,\n\"b7\": 1,\n\"b3\": 1,\n\"b5\": 1,\n\"b12\": 1\n}", "Based on the text and the author notes, the source papers that most inspired this paper seem to be:\n\n1. \"LLaMA\" [b16]: This is the primary model that the authors of the current paper are building upon. They are refining and adapting this model for a specific domain, which is a clear indication of direct inspiration.\n\n2. \"S2ORC\" [b6]: This is the dataset that the authors used to fine-tune their model. The usage of this dataset implies that the work done in this paper provided a basis for their research.\n\n3. \"GPT2\" [b13]: The authors used the autoregressive generation objective introduced in this paper for fine-tuning their model, indicating its influence on their methodology.\n\n4. \"AdamW optimizer\" [b7]: The authors used the AdamW optimizer in their fine-tuning process, indicating its influence on their methodology.\n\n5. \"PEFT\" [b8]: The authors used the PEFT method for parameter-efficient fine-tuning, indicating its influence on their methodology.\n\n6. \"LMFlow\" [b2]: The authors compared their evaluation method with LMFlow, indicating its influence on their methodology.\n\nBased on the task description, the confidence scores would be high for these papers because they are directly relevant to the paper's novel contributions.\n\nThe resulting JSON format would be:\n\n{\n\"b16\": 1,\n\"b6\": 0.8,\n\"b13\": 0.8,\n\"b7\": 0.8,\n\"b8\": 0.8,\n\"b2\": 0.7\n}"], "64c78ba13fda6d7f06dba840": ["Based on the given text and the criteria you have provided, the following source papers are identified to have inspired the given paper:\n\n1. \"melf multivariant executables for a heterogeneous world\" - This is the main concept of the paper and the main method and approach discussed in the paper is multivariant executables for a heterogeneous world.\n\n2. \"from global to local quiescence wait free code patching of multi threaded processes\" [b13] - This paper is mentioned several times in the text and it seems that it has inspired the author to develop a solution for semi-dynamic variability.\n\n3. \"multiverse compiler assisted management of dynamic variability in low level system software\" [b14] - The author uses the method proposed in this paper to generate static variants at compile time.\n\n4. \"operating system support for overlapping isa heterogeneous multi core architectures\" [b22] - This paper is likely to have inspired the author to develop a solution for heterogeneous multi-core architectures.\n\n5. \"popcorn bridging the programmability gap in heterogeneous isa platforms\" [b24] - This paper seems to be a source of inspiration for the author to develop a solution for bridging the programmability gap in heterogeneous ISA platforms.\n\n6. \"immediate multithreaded dynamic software updates using stack reconstruction\" [b35] - This paper appears to have inspired the author to propose a method for dynamic software updates.\n\nThe confidence score for each reference is estimated to be 1 as they all seem to have directly influenced the main contributions of the paper.\n\nThe result in json format is:\n{\"melf multivariant executables for a heterogeneous world\": 1, \"from global to local quiescence wait free code patching of multi threaded processes [b13]\": 1, \"multiverse compiler assisted management of dynamic variability in low level system software [b14]\": 1, \"operating system support for overlapping isa heterogeneous multi core architectures [b22]\": 1, \"popcorn bridging the programmability gap in heterogeneous isa platforms [b24]\": 1, \"immediate multithreaded dynamic software updates using stack reconstruction [b35]\": 1}", "Based on the text of the academic paper and the titles of the references, the source papers that most likely inspired the content of this paper are:\n\n1. \"melf multivariant executables for a heterogeneous world\" - This appears to be the core contribution of the paper, introducing the concept of multivariant ELF (MELF) for semi-dynamic variability, which is mentioned multiple times throughout the text. \n\n2. \"from global to local quiescence wait free code patching of multi threaded processes\" - The paper refers to this work when discussing the MMView mode and its requirement for local quiescence per thread, suggesting that it heavily influenced the paper's methodology.\n\n3. \"multiverse compiler assisted management of dynamic variability in low level system software\" - The term \"static variant generation\" is used in the paper, which likely comes from this source. It is part of the paper's methodology, suggesting that it had a significant influence.\n\n4. \"executable assertions an aid to reliable software\" - The paper discusses the concept of executable asserts, which is likely inspired by this work.\n\n5. \"practical dynamic software updating for c\" - The paper's discussion on dynamic software updating might have been influenced by this work.\n\n6. \"big little processing with arm cortexa cortex a improving energy efficiency in high performance mobile platforms\" - The paper's mention of dynamic hardware and use-case heterogeneity could have been inspired by this work.\n\n7. \"operating system support for overlapping isa heterogeneous multi core architectures\" - The paper's emphasis on multivariant execution in a single process is likely influenced by the ideas presented in this work.\n\n8. \"stackguard automatic adaptive detection and prevention of bufferoverflow attacks\" - The paper's mention of safety and security measures for code could be inspired by this work.\n\nThe json format of the result:\n\n{\n  \"None\": 1,\n  \"b13\": 0.9,\n  \"b14\": 0.8,\n  \"b6\": 0.7,\n  \"b12\": 0.7,\n  \"b18\": 0.6,\n  \"b22\": 0.6,\n  \"b1\": 0.5\n}", "Based on the given text, the source papers that are most pertinent and directly relevant to the paper's novel contributions are:\n\n- [b13]: From global to local quiescence wait free code patching of multi threaded processes\n- [b14]: Multiverse compiler assisted management of dynamic variability in low level system software\n- [b2]: Pointguardtm protecting pointers from buffer overflow vulnerabilities\n- [b4]: Addresssanitizer a fast address sanity checker\n\nThese papers seem to have direct relevance to the paper's novel contributions and the implementation of their methods. They are also not merely historical findings but are used in the core development of the paper's contributions. \n\nThe JSON format of the result:\n\n{ \"b13\": 0.9, \"b14\": 0.8, \"b2\": 0.7, \"b4\": 0.7 } \n\nThe confidence score represents the degree of relevance and inspiration from the source papers. The score is subjective and is based on the analysis of the text.", "Based on the given text, here are the source papers that inspired the author most:\n\n1. \"From global to local quiescence wait free code patching of multi threaded processes\" [b13]: This paper is directly mentioned in the text when discussing the MMView mode, suggesting it was a primary influence in the development of the author's multivariant ELF concept.\n\n2. \"Multiverse compiler assisted management of dynamic variability in low level system software\" [b14]: This paper is mentioned in the context of variant generation at compile time, indicating that it was a key influence on the author's approach to generating multiple function variants.\n\n3. \"SMP alternatives\" [b11]: This paper is mentioned when discussing special-purpose solutions for semi-dynamic variability. The author's work appears to be a response to the limitations of these solutions.\n\n4. \"Executable assertions an aid to reliable software\" [b6]: This paper is mentioned in the context of executable asserts, which is a key focus of the SQLite case study.\n\n5. \"LLD\" [b17]: Likely a key influence on the author's MELF linker, as the author extended LLD's linker for their implementation.\n\nThe confidence scores are as follows:\n\n{ \"b13\": 0.9, \"b14\": 0.85, \"b11\": 0.8, \"b6\": 0.75, \"b17\": 0.7 }", "Based on the provided text and instructions, the source papers that seem to have most influenced this paper are:\n\n1. 'From Global to Local Quiescence: Wait-Free Code Patching of Multi-Threaded Processes' [b13]: This source is directly referenced when talking about the MMView Kernel extension. This shows that the paper's proposed approach is built upon the concepts from this source.\n   \n2. 'Multiverse: Compiler-Assisted Management of Dynamic Variability in Low-Level System Software' [b14]: Directly mentioned in the context of static variant generation, indicating that this work is foundational to the methods employed in the paper.\n\n3. 'CMake: Cross-Platform Make' [b15]: Directly referenced when discussing variant generation with modern build systems, suggesting that it has inspired the approaches used in the paper.\n\n4. 'Stackguard: Automatic Adaptive Detection and Prevention of Buffer-Overflow Attacks' [b1], 'PointGuardTM: Protecting Pointers From Buffer Overflow Vulnerabilities' [b2], 'AddressSanitizer: A Fast Address Sanity Checker' [b4], 'Executable Assertions: An Aid to Reliable Software' [b6], 'Applying Design by Contract' [b7]: These sources are mentioned when discussing how the compiler can instrument the code with extra sanity checks, indicating that they have inspired the safety and security measures in the paper.\n\nBased on the given instructions, the source papers are directly relevant to the paper's novel contributions. Further, they are preceded by phrases indicating direct inspiration or motivation, making them a priority in the list.\n\nIn JSON format, with a confidence score, the result would be:\n\n{\n\"b13\": 0.9,\n\"b14\": 0.8,\n\"b15\": 0.8,\n\"b1\": 0.7,\n\"b2\": 0.7,\n\"b4\": 0.7,\n\"b6\": 0.7,\n\"b7\": 0.7\n}"], "64c78b9f3fda6d7f06db9a87": ["Based on the text and the titles of the papers, the most pertinent sources that inspired this paper are:\n\n1. 'b46': 'spectre attacks exploiting speculative execution'\n2. 'b57': 'demystifying intel branch predictors'\n3. 'b60': 'swivel hardening webassembly against spectre'\n4. 'b78': 'a case for partially tagged geometric history length branch prediction'\n5. 'b27': 'branchscope a new side channel attack on directional branch predictor'\n6. 'b9': 'new branch prediction vulnerabilities in openssl and necessary software countermeasures'\n7. 'b24': 'covert channels through branch predictors a feasibility study'\n8. 'b105': 'a lightweight isolation mechanism for secure branch predictors'\n\nThe return result in the json format should be:\n```\n{\n    \"b46\": 1,\n    \"b57\": 0.9,\n    \"b60\": 0.8,\n    \"b78\": 0.7,\n    \"b27\": 0.6,\n    \"b9\": 0.5,\n    \"b24\": 0.4,\n    \"b105\": 0.3\n}\n```\nThe confidence score is estimated based on the frequency and context in which these papers are cited in the text.", "Based on the text of the paper, the source papers which inspire the author most appear to be:\n\n- [b46]: Spectre attacks exploiting speculative execution. The paper frequently references this work in discussing the problem of Spectre attacks and the challenges of providing robust control flow isolation at the hardware level. Confidence: 0.9\n- [b60]: Swivel hardening webassembly against spectre. The paper mentions this work when discussing their own implementation of a defense mechanism against CBP poisoning/aliasing attacks, suggesting it played a significant role in their research. Confidence: 0.8\n- [b78]: A case for partially tagged geometric history length branch prediction. This paper seems to be a key reference in the development of the paper's novel algorithm, as it provides a comprehensive analysis of the branch predictors in Intel processors. Confidence: 0.85\n- [b27]: BranchScope: a new side-channel attack on directional branch predictor. This paper inspired the authors to focus on the CBP structure and how it can be used in real attacks as well as practical defenses. Confidence: 0.8\n- [b81]: A study of branch prediction strategies. This paper provides the background needed to understand the branch prediction strategies used in modern processors. Confidence: 0.7\n- [b9], [b10], [b11]: These papers are about BTB attacks, which the author refers to multiple times when discussing the background of their research. Confidence: 0.75\n\nIn the json format:\n\n{\"b46\": 0.9, \"b60\": 0.8, \"b78\": 0.85, \"b27\": 0.8, \"b81\": 0.7, \"b9\": 0.75, \"b10\": 0.75, \"b11\": 0.75}", "Based on the given text, the references that have most likely inspired the author of the paper are:\n\n1. \"b46\": \"Spectre attacks exploiting speculative execution\" - This paper is mentioned multiple times and its subject matter is directly related to the main challenges and novel contributions of the paper. This paper also appears to inspire the author in developing the method to prevent side channel attacks on the CBP and transient execution attacks.\n2. \"b60\": \"Swivel hardening WebAssembly against Spectre\" - This paper is mentioned as a base for the author's implementation of the CBP partitioning defense mechanism and its subject matter is directly related to the paper's core contributions.\n3. \"b27\": \"Branchscope a new side channel attack on directional branch predictor\" - This paper is used to explain the current problem that the author's paper is trying to solve. The author is inspired by this paper to develop a new general-purpose, low-overhead method of partitioning the CBP in existing CPUs.\n4. \"b78\": \"A case for partially tagged geometric history length branch prediction\" - This paper is referenced multiple times in the text, and used to explain the subject matter of the paper's core contributions. It is also related to the author's comprehensive reverse engineering of the branch predictors on Intel processor families.\n5. \"b75\": \"TAGE SC L branch predictors\" - This paper is also used to explain the subject matter of the paper's core contributions and its subject matter appears to be directly related to the author's work.\n6. \"b20\": \"SGXpectre stealing Intel secrets from SGX enclaves via speculative execution\" - This paper is used to explain the Spectre attacks that the author's paper is trying to prevent and its subject matter is directly related to the paper's core contributions.\n\nBased on the above analysis, my result in the json format is:\n\n{\n\"b46\": 0.9,\n\"b60\": 0.8,\n\"b27\": 0.8,\n\"b78\": 0.8,\n\"b75\": 0.8,\n\"b20\": 0.7\n}", "Based on the text of the academic paper, the primary challenge outlined is the isolation of microarchitectural structures for secure multi-user systems. The main method proposed to solve this issue is a comprehensive reverse engineering of the branch predictors on three high-end Intel processor families, revealing for the first time the structures and index functions of all tables in the predictor. \n\nLooking at the text for key phrases and examining novel methods, the following source papers can be identified:\n\n1. [b46]: \"Spectre attacks exploiting speculative execution\"\n2. [b53]: \"Meltdown reading kernel memory from user space\"\n3. [b81]: \"A study of branch prediction strategies\"\n4. [b78]: \"A case for partially tagged geometric history length branch prediction\"\n5. [b99]: \"Two-level adaptive training branch prediction\"\n6. [b27]: \"Branchscope a new side channel attack on directional branch predictor\"\n\nRe-evaluating the answer based on the novel contributions and methodological similarity, the following source papers can be identified:\n\n1. [b46]: \"Spectre attacks exploiting speculative execution\"\n2. [b78]: \"A case for partially tagged geometric history length branch prediction\"\n3. [b81]: \"A study of branch prediction strategies\"\n4. [b57]: \"Demystifying Intel branch predictors\"\n\nSo, the answer in json format would be:\n\n{\n\"b46\": 0.9,\n\"b78\": 0.8,\n\"b81\": 0.7,\n\"b57\": 0.7\n}", "Based on the text, the papers that mostly inspired the author could be:\n\n1. \"BranchScope: A New Side Channel Attack on Directional Branch Predictor\" [b27]: This paper is mentioned in the context of previous work exploiting the Conditional Branch Predictor (CBP) and is directly relevant to the paper's novel contributions in CBP partitioning and mitigation of side channel attacks on CBP.\n\n2. \"TAGE-SC-L Branch Predictors\" [b75]: This paper is mentioned when discussing the TAGE predictor, which is a core part of the analysis and reverse engineering efforts in the paper's method.\n\n3. \"Spectre Attacks: Exploiting Speculative Execution\" [b46]: This paper is mentioned multiple times, indicating that it may have significantly inspired the authors. The paper's contribution towards CBP partitioning is largely motivated by preventing attacks like Spectre.\n\n4. \"Jump over ASLR: Attacking Branch Predictors to Bypass ASLR\" [b25]: This paper is referred to in the context of the BTB-based attacks, which is one of the key challenges the paper addresses.\n\n5. \"A Case for Partially Tagged Geometric History Length Branch Prediction\" [b78]: This paper is mentioned when discussing the TAGE predictor and its importance in commercial high-performance processors.\n\n6. \"Spectre Side Channels\" [b82]: This paper is mentioned as a source of defenses against Spectre-based attacks, which is the primary challenge the paper is addressing.\n\n7. \"A Lightweight Isolation Mechanism for Secure Branch Predictors\" [b105]: This paper is mentioned when discussing general defenses against BPU attacks. It appears to have influenced the authors in their approach to partitioning the CBP.\n\n8. \"Preventing Timing Leaks Through Transactional Branching Instructions\" [b14]: This paper is mentioned when discussing constant-time programming as a defense against branch-based attacks, which is a closely related domain to the paper's topic.\n\nThe result in json format would be:\n\n{\n\"b27\": 1,\n\"b75\": 0.9,\n\"b46\": 1,\n\"b25\": 0.8,\n\"b78\": 0.9,\n\"b82\": 0.9,\n\"b105\": 0.9,\n\"b14\": 0.8\n}"], "6389d6fb90e50fcafdffbdc3": ["Based on the text and the methodology proposed, it seems that the following papers are the most pertinent sources:\n\n1. \"Path confidence based lookahead prefetching\" [b13]\n2. \"Efficiently prefetching complex address patterns\" [b14]\n3. \"Perceptron based prefetch filtering\" [b15]\n4. \"Best offset hardware prefetching\" [b16]\n5. \"Spatial memory streaming\" [b27]\n6. \"Bump bulk memory access prediction and streaming\" [b28]\n7. \"Augury using data memory dependent prefetchers to leak data at rest\" [b29]\n8. \"Advanced concepts on address translation appendix l in computer architecture a quantitative approach by hennessy and patterson\" [b32]\n\nThe paper takes inspiration from these sources to develop its own \"Page-size Propagation Module (PPM)\" to improve the effectiveness of spatial cache prefetchers. The author also refers to the drawbacks of existing prefetchers, which are outlined in these source papers.\n\nHere is the JSON format:\n{\n    \"b13\": 0.9,\n    \"b14\": 0.8,\n    \"b15\": 0.85,\n    \"b16\": 0.8,\n    \"b27\": 0.75,\n    \"b28\": 0.75,\n    \"b29\": 0.8,\n    \"b32\": 0.85\n}", "Based on the text and the titles of the references, the source papers that most inspire the given paper are likely:\n\n1. \"path confidence based lookahead prefetching\" [b13]: The text mentions this work as a key foundation for the study, and it's used as a basis for their proposal. \n\n2. \"efficiently prefetching complex address patterns\" [b14]: This paper is also mentioned in the context of existing prefetching strategies, which the proposed method aims to improve.\n\n3. \"perceptron based prefetch filtering\" [b15]: Similar to [b13] and [b14], this paper is also presented as a foundation for the current work.\n\n4. \"best offset hardware prefetching\" [b16]: This paper is mentioned along with [b13], [b14], and [b15] as the basis for their proposal.\n\n5. \"spatio temporal memory streaming\" [b27]: This paper is cited when discussing the benefits of spatial prefetchers, which is a key focus of the current paper.\n\n6. \"augury using data memory dependent prefetchers to leak data at rest\" [b29]: The text mentions this work in the context of security issues with page-crossing prefetching, which is a key concern the current paper addresses.\n\n7. \"efficient virtual memory for big memory servers\" [b33]: This paper is cited when discussing the pressure on the TLB hierarchy, which is a challenge the current paper addresses.\n\nHere is the result in json format: \n{\n\"b13\":0.9,\n\"b14\":0.9,\n\"b15\":0.9,\n\"b16\":0.9,\n\"b27\":0.8,\n\"b29\":0.8,\n\"b33\":0.8\n}", "Based on the provided text, the following references seem to be the most pertinent source papers:\n\n1. \"Path confidence based lookahead prefetching\" [b13]: This paper is referred to multiple times in the text and it appears to be the basis for the novel method proposed by the authors, the Page-size Propagation Module (PPM). \n\n2. \"Efficiently prefetching complex address patterns\" [b14]: This is another paper that is mentioned a few times in the text and seems to be directly related to the novel contributions of the paper. \n\n3. \"Perceptron based prefetch filtering\" [b15]: This paper is also referred to multiple times in the text. It seems to be related to the novel methods proposed by the authors. \n\n4. \"Best offset hardware prefetching\" [b16]: This paper is mentioned in the text and seems to be closely related to the novel contributions of the paper.\n\n5. \"Efficient virtual memory for big memory servers\" [b33]: This paper seems to be related to the challenges and solutions proposed in the paper, as it is referred to in the context of the issues with modern systems' use of large pages.\n\n6. \"Translation ranger operating system support for contiguity aware tlbs\" [b41]: This paper is referred to in the context of the limitations of existing cache prefetchers, which is a central topic of the paper. \n\n7. \"The amd zen processor\" [b3]: Mentioned in the context of modern high-performance computing chips employing various cache prefetchers.\n\nThe confidence scores for each of these papers would require a more in-depth analysis of the text and the references, but based on the information provided, all of them seem to be directly relevant to the novel contributions of the paper. \n\nHere is the answer in json format:\n\n{\n   \"b13\": 0.9,\n   \"b14\": 0.8,\n   \"b15\": 0.7,\n   \"b16\": 0.7,\n   \"b33\": 0.6,\n   \"b41\": 0.6,\n   \"b3\": 0.5\n}", "Based on the provided text and other information, the following source papers can be inferred:\n\n1. \"Path confidence based lookahead prefetching\" [b13]: The paper introduces a new module called \"Page-size Propagation Module (PPM)\" which is based on the lookahead prefetching concept in this paper. Confidence score: 0.9\n2. \"Efficiently prefetching complex address patterns\" [b14]: This paper is mentioned while discussing the experiments to highlight the potential of leveraging the presence of large pages for improving spatial cache prefetching effectiveness. Confidence score: 0.8\n3. \"Perceptron based prefetch filtering\" [b15]: The paper refers to this source when discussing the design of a composite scheme that transparently integrates large pages into the prefetcher's design. Confidence score: 0.8\n4. \"Best offset hardware prefetching\" [b16]: This source paper is referred to while discussing the design of a composite scheme named \"Page Size Aware Prefetcher with Set Dueling (Pref-PSA-SD)\". Confidence score: 0.8\n5. \"Large reach memory management unit caches\" [b34]: The paper discusses the challenges in the TLB hierarchy and frequent page walks that deteriorate application performance, a concept that comes from this source paper. Confidence score: 0.75\n6. \"Transparent huge pages\" [b59]: The paper refers to this source when discussing the support for large pages in modern systems. Confidence score: 0.7\n7. \"Page-size Propagation Module (PPM)\" [b13]: The paper introduces a new module called \"Page-size Propagation Module (PPM)\" which is based on the lookahead prefetching concept in this paper. Confidence score: 0.9\n\nThe json format of the result is:\n\n{\n \"b13\": 0.9,\n \"b14\": 0.8,\n \"b15\": 0.8,\n \"b16\": 0.8,\n \"b34\": 0.75,\n \"b59\": 0.7,\n}", "Based on the given text, the following source papers seem to be integral to the given paper. \n\n1. \"Path confidence based lookahead prefetching\" - [b13] - Confidence Score: 0.85\n2. \"Efficiently prefetching complex address patterns\" - [b14] - Confidence Score: 0.85\n3. \"Perceptron based prefetch filtering\" - [b15] - Confidence Score: 0.85\n4. \"Best offset hardware prefetching\" - [b16] - Confidence Score: 0.85\n5. \"Bingo spatial data prefetcher\" - [b20] - Confidence Score: 0.78\n6. \"Efficient virtual memory for big memory servers\" - [b33] - Confidence Score: 0.75\n7. \"Advanced concepts on address translation appendix l in computer architecture a quantitative approach by hennessy and patterson\" - [b32] - Confidence Score: 0.75\n8. \"Transparent huge pages\" - [b59] - Confidence Score: 0.75\n\nThe confidence score is based on the number of times these references are mentioned, the context in which they are mentioned, and the relevance to the key challenges and methods proposed in the paper."], "6426ed4590e50fcafd444b1b": ["Based on the text of the paper, the paper seems to be about the development of a new spatial prefetcher, the Dual Stream Data Prefetcher (DSDP), that learns from both the PC localized stream and the spatial localized stream. It also discusses about PC localization, spatial localization, and the use of localization as a filtering method.\n\nLooking for key phrases indicating inspiration or motivation from previous research, several references were mentioned in the context of existing prefetch algorithms and localization methods.\n\nHere are the source papers that could have most inspired this paper:\n1. \"PC localized Prefetcher\" and \"Spatial localized Prefetcher\" - these algorithms are mentioned as existing solutions that the author has built upon. The mentioned references are [b10], [b4] and [b16]. \n2. \"Localization\" - the concept of localization as a filtering method was a significant part of the paper's discussion. The mentioned references are [b20], [b19] and [b27].\n3. The paper also mentions the work of [b6], [b7], and [b23] as spatial prefetchers that lose many prefetch opportunities due to PC localization.\n\nSince the reference titles are given, we can match them with the reference numbers. The source papers are:\n\n- b10: 'Stride directed prefetching in scalar processors'\n- b4: 'Bingo spatial data prefetcher'\n- b16: 'Path confidence based lookahead prefetching'\n- b20: 'Data cache prefetching using a global history buffer'\n- b19: 'AC DC an adaptive data cache prefetcher'\n- b27: 'Branch history guided instruction prefetching'\n- b6: 'DSPatch dual spatial pattern prefetcher'\n- b7: 'Perceptron based prefetch filtering'\n- b23: 'Sandbox prefetching safe run time evaluation of aggressive prefetchers'\n\nBased on the given information, the following is a JSON format result:\n\n{\n\"b10\": 0.9,\n\"b4\": 0.9,\n\"b16\": 0.9,\n\"b20\": 0.9,\n\"b19\": 0.9,\n\"b27\": 0.9,\n\"b6\": 0.8,\n\"b7\": 0.8,\n\"b23\": 0.8\n}\nThe confidence score is high (0.9) for the papers that were mentioned in the context of existing solutions that the author has built upon. The score is slightly lower (0.8) for the papers that were mentioned as previous work that has limitations, which the current paper seeks to address.", "Based on the provided text and the author's notes, the paper mainly introduces the Dual Stream Data Prefetcher (DSDP), a new spatial prefetcher that tracks high probability events on both the PC localized stream and the spatial localized stream. The following references appear to be the most influential:\n\n1. 'b4': 'bingo spatial data prefetcher' - The text mentions that Bingo can achieve high accuracy by learning long events, which include Program Counter (PC), offset, and address. This points out that the method used in this paper may have inspired the authors.\n   \n2. 'b14': 'linearizing irregular memory accesses for improved correlated prefetching' - The paper mentions that some prefetching algorithms like ISB use PC localization to improve coverage and accuracy. This suggests that the ISB method has influenced the formation of the paper's core contributions.\n\n3. 'b20': 'data cache prefetching using a global history buffer' - The text mentions that the literature [b20] proposes a taxonomy that classifies prefetching algorithms with a pair X/Y. This could indicate that this paper inspired the authors in their research.\n\n4. 'b16': 'path confidence based lookahead prefetching' - The paper mentions that some prefetch algorithms like SPP will give each prefetch candidate a confidence value. This suggests that the methods used in this paper may have informed the author's research.\n\n5. 'b6': 'dspatch dual spatial pattern prefetcher' - The text suggests that existing spatial prefetchers [b6] that take PC localization as a filter would lose many prefetch opportunities, which will result in low coverage. This indicates that the authors may have been motivated to address the limitations of this method.\n\nIn JSON format, the answer could be represented as follows:\n\n{\"reference number\": [\"b4\", \"b14\", \"b20\", \"b16\", \"b6\"], \"confidence score\": [0.8, 0.8, 0.7, 0.7, 0.7]}", "Based on the given text and the titles of the references, the source papers that inspired the author most might be:\n\n1. \"dsdp dual stream data prefetcher\" - The paper is named after this reference, indicating a strong influence.\n2. \"bingo spatial data prefetcher\" - The text mentions that Bingo can become more accurate by tracking both long and short events.\n3. \"linearizing irregular memory accesses for improved correlated prefetching\" - The text mentions that some prefetch algorithms like ISB use PC localization to improve coverage and accuracy.\n4. \"data cache prefetching using a global history buffer\" - This paper is mentioned in the context of explaining the taxonomy that classifies prefetching algorithms.\n5. \"spatial memory streaming\" - The paper discusses the limitations of PC localization and the need for spatial localization, which this paper addresses.\n6. \"efficient metadata management for irregular data prefetching\" - The paper discusses the loss of prefetch opportunities due to localization, indicating a possible influence from this paper on the development of their approach.\n\nHere the json format for the answer:\n\n{\n\"b0\": 0.95,\n\"b4\": 0.85,\n\"b14\": 0.8,\n\"b20\": 0.75,\n\"b23\": 0.7,\n\"b31\": 0.65\n}", "Based on the given information, the author is inspired by previous work in the field of prefetching and memory access in computer systems. The author describes their novel method - the Dual Stream Data Prefetcher (DSDP) and references several previous works during their explanation. \n\nHere are the most relevant source papers:\n\n- \"bingo spatial data prefetcher\" [b4]: The author mentions this work when describing methods to improve accuracy in prefetching algorithms.\n\n- \"linearizing irregular memory accesses for improved correlated prefetching\" [b14]: The author refers to this work when discussing PC localization to improve coverage and accuracy.\n\n- \"data cache prefetching using a global history buffer\" [b20]: The author cites this paper when explaining the concept of a localization method and its application in prefetching algorithms.\n\n- \"spatial memory streaming\" [b26]: The author refers to this work when discussing the limitations of PC localization and how it can result in low coverage.\n\n- \"temporal streaming of shared memory\" [b29]: The author cites this work when discussing the predictor in their paper.\n\nThe confidence scores for these papers would be:\n\n{\n\"b4\": 0.8,\n\"b14\": 0.9,\n\"b20\": 0.8,\n\"b26\": 0.8,\n\"b29\": 0.7\n} \n\nThese scores are subjective and based on the frequency and context of the citations in the paper. The papers are all very directly related to the topic at hand and seem to heavily influence the author's work.", "Based on the text from the paper, the following references can be identified as potential source papers:\n\n1. 'b4': 'bingo spatial data prefetcher'\n2. 'b14': 'linearizing irregular memory accesses for improved correlated prefetching'\n3. 'b16': 'path confidence based lookahead prefetching'\n4. 'b20': 'data cache prefetching using a global history buffer'\n\nThese papers are identified as they are directly mentioned in the text and are strongly tied to the methods and approaches the author uses in their paper. The confidence scores for these papers are inferred based on the frequency and context of their mentions in the text.\n\nHere is the result in the required JSON format:\n\n{\n  \"b4\": 0.9,\n  \"b14\": 0.8,\n  \"b16\": 0.85,\n  \"b20\": 0.8\n}"], "64e432bf3fda6d7f0600af81": ["Based on the given text, the following source papers seem to have inspired the author most:\n\n1. \"Semi-supervised classification with graph convolutional networks\" [b24] - This paper is mentioned multiple times in the text, indicating it was a significant source of inspiration. It is referred to in the context of discussing Graph Neural Networks (GNNs) and their application in various tasks, including node classification.\n\n2. \"The lottery ticket hypothesis finding sparse trainable neural networks\" [b15] - This paper is cited in relation to the discussion on iterative pruning, a method adopted by the author in their approach. The lottery ticket hypothesis is a well-known concept in the field of neural networks, suggesting its influence on the author's work.\n\n3. \"Deep residual learning for image recognition\" [b21] - The author refers to the contributions of this paper to the field of computer vision, suggesting that it has inspired the use of deeper networks in their work.\n\n4. \"Fast learning with graph convolutional networks via importance sampling\" [b4] - This paper is mentioned in the context of discussing graph sampling, aligning with the research trajectory of the author's work.\n\n5. \"Going deeper with lean point networks\" [b25] - This paper is cited multiple times in the text, indicating its influence on the author's work. It is referred to in the context of discussing the guiding principles to control the aggregation depth for each node.\n\n6. \"A comprehensive survey on graph neural networks\" [b47] - This paper is referenced multiple times throughout the text, suggesting it provided a foundation for the author's understanding of GNNs.\n\n7. \"The lottery ticket hypothesis for pre-trained bert networks\" [b6] - This paper is mentioned in relation to the discussion on graph pruning algorithms, indicating its influence on the author's work.\n\nThe confidence scores for each of these references would need to be determined based on a more detailed analysis of the text, but it can be inferred that they are all likely to be quite high given the frequency and context of their mentions.\n\nThe results can be represented in the JSON format as follows:\n```\n{\n\"b24\": 0.9,\n\"b15\": 0.85,\n\"b21\": 0.8,\n\"b4\": 0.8,\n\"b25\": 0.8,\n\"b47\": 0.9,\n\"b6\": 0.85\n}\n``` \n\nNote: The confidence scores are hypothetical and are meant to indicate the level of influence of each paper on the author's work, with 1 being the most influential and 0 being the least.", "Based on the given text, the source papers that influenced the authors most are:\n\n1. \"Semi-supervised classification with graph convolutional networks\" [b24]\n2. \"Graph attention networks\" [b42]\n3. \"Representation learning on graphs with jumping knowledge networks\" [b48]\n4. \"The lottery ticket hypothesis: Finding sparse, trainable neural networks\" [b15]\n5. \"Deep residual learning for image recognition\" [b21]\n6. \"Simplifying graph convolutional networks\" [b45]\n7. \"Towards deeper graph convolutional networks\" [b31]\n8. \"A unified lottery ticket hypothesis for graph neural networks\" [b7]\n\nThe following JSON object represents the source papers and their corresponding confidence scores:\n\n{\n    \"b24\": 0.9,\n    \"b42\": 0.8,\n    \"b48\": 0.85,\n    \"b15\": 0.8,\n    \"b21\": 0.75,\n    \"b45\": 0.8,\n    \"b31\": 0.75,\n    \"b7\": 0.85\n}\n\nIn the above JSON object, the key represents the reference number and the value represents the confidence score. The confidence score is a subjective assessment based on the degree of influence that each source paper appears to have on the given paper, with a higher score indicating a stronger influence.", "Based on the analysis of the provided academic paper text, the most important source papers that inspired the authors are likely to be:\n\n1. \"Semi-supervised classification with graph convolutional networks\" [b24]\n2. \"Inductive representation learning on large graphs\" [b20]\n3. \"Deep residual learning for image recognition\" [b21]\n4. \"DeepGCNs: Can GCNs Go as Deep as CNNs?\" [b27] [b41]\n5. \"Simple and deep graph convolutional networks\" [b5]\n6. \"Representation learning on graphs with jumping knowledge networks\" [b48]\n7. \"The lottery ticket hypothesis: Finding sparse, trainable neural networks\" [b15]\n8. \"Open graph benchmark: Datasets for machine learning on graphs\" [b22]\n\nThese papers are selected based on the analysis of the citations made in the text, especially where the authors mention being inspired by, following, or adopting the methods from these works. Please note that the confidence scores cannot be provided without a specific scoring mechanism.\n\nIn JSON format:\n{\n  \"b24\": 0.9,\n  \"b20\": 0.8,\n  \"b21\": 0.7,\n  \"b27\": 0.8,\n  \"b41\": 0.7,\n  \"b5\": 0.8,\n  \"b48\": 0.7,\n  \"b15\": 0.7,\n  \"b22\": 0.8\n}", "Based on the text, the source papers that likely inspired the given paper most are:\n\n1. \"Graph Neural Networks (GNNs)\" [b24] [b20]\n2. \"The prominent performance of GNNs mainly stems from their message passing mechanism\" [b47]\n3. \"over-fitting [b39] , over-smoothing [b30] [b3] and vanishing gradients [b27] [b56] are three long-standing problems in the GNN area\"\n4. \"The first category involves prudently inheriting innovations from CNNs, such as residual/dense connections [b27] [b41] [b48] [b29] [b48] [b5] [b50]\"\n5. \"Another category is to combine various deep aggregation strategies with shallow neural networks [b45] [b10] [b31] [b57] [b39] [b18]\"\n6. \"By utilizing the simplest gradient guidance which can indicate potentially promising elements [b25] [b2]\"\n7. \"The computation of a massive number of parameters makes it extremely challenging to incorporate this algorithm into the deeper GNN framework.\" [b22]\n8. \"We utilize six graph benchmarks to evaluate the performance of our hypothesis.\" [b24] [b22]\n9. \"We adopt GCN [b24] , GIN [b49] and GAT [b43] as shallow GNNs backbones.\"\n10. \"We propose a node \"early stopping\" technique based on edge pruning to help better combat the issue of over-smoothing and over-fitting.\" [b30] [b3]\n\nIn JSON format:\n\n{\n\"b24\": 1.0,\n\"b20\": 0.9,\n\"b47\": 0.9,\n\"b39\": 0.8,\n\"b30\": 0.8,\n\"b3\": 0.8,\n\"b27\": 0.7,\n\"b56\": 0.7,\n\"b41\": 0.6,\n\"b48\": 0.6,\n\"b29\": 0.6,\n\"b5\": 0.6,\n\"b50\": 0.6,\n\"b45\": 0.5,\n\"b10\": 0.5,\n\"b31\": 0.5,\n\"b57\": 0.5,\n\"b18\": 0.5,\n\"b25\": 0.4,\n\"b2\": 0.4,\n\"b22\": 0.3,\n\"b49\": 0.2,\n\"b43\": 0.2\n}", "Based on the task description, key phrases in the text, and the titles of each paper, the most pertinent source papers that inspire the given paper are:\n\n1. \"Graph Neural Networks (GNNs)\" [b24] [b20]: The paper talks about GNNs multiple times, including in the introduction, indicating that this work is central to the understanding of the research problem.\n2. \"Residual/dense connections\" [b27] [b41] [b48] [b29] [b48] [b5] [b50]: These are repeatedly referenced as significant influences on the work.\n3. \"JKNet\" [b48]: The authors note that they adopt this work to maintain the discrepancies among different nodes. \n4. \"GCNII\" [b5] and \"ResGCN\" [b27]: The authors use these methods to carry information from the previous layer to avoid the aforementioned issues.\n5. \"DropEdge\" [b39]: The authors resort to this strategy to implicitly increase graph diversity and reduce message passing.\n6. \"The lottery ticket hypothesis for pre trained bert networks\" [b6]: The authors compare their work with this paper.\n7. \"unified graph sparsification (UGS)\" [b6]: The authors use the iterative pruning (IP) strategy within the framework of unified graph sparsification (UGS), indicating that this paper has a significant influence on their work.\n\n```\n{\n\"b24\": 0.8,\n\"b20\": 0.7,\n\"b27\": 0.9,\n\"b41\": 0.9,\n\"b48\": 0.9,\n\"b5\": 0.9,\n\"b50\": 0.8,\n\"b39\": 0.7,\n\"b6\": 0.85\n}\n```"], "64c78b993fda6d7f06db5b90": ["Based on the given text and the criteria for identifying source papers, the following references appear to be the most influential to the paper's novel contributions:\n\n1. \"Translating embeddings for modeling multi-relational data\" (b1): The paper mentions it as a traditional method of Knowledge Graph Completion (KGC). The new method proposed in the paper directly addresses the scalability issues of such methods.\n\n2. \"Knowledge is flat: A seq2seq generative framework for various knowledge graph completion\" (b3): The paper mentions it as a recent study that uses a seq2seq generation methodology for knowledge graph completion. The proposed model builds upon this method by adding a retrieval module for enhancing the generation of the missing entity.\n\n3. \"The probabilistic relevance framework: BM25 and beyond\" (b16): The paper explicitly mentions using a free-text retrieval module such as BM25 to find the relevant triplets for each incomplete triplet.\n\n4. \"Exploring the limits of transfer learning with a unified text-to-text transformer\" (b15): The paper uses T5 as the generation module in the proposed model, which is based on this reference.\n\n5. \"Leveraging passage retrieval with generative models for open domain question answering\" (b8): The paper applies a Fushion-in-Decoder (FiD) approach based on T5 as the generation module, which is inspired by this reference.\n\n6. \"OGB-LSC: A large-scale challenge for machine learning on graphs\" (b7) and \"Kepler: A unified model for knowledge embedding and pre-trained language representation\" (b24): The paper compares its method with these references on large-scale datasets and shows that the proposed method can significantly enhance the state-of-the-art performance.\n\nThe json format of the result is:\n{\"b1\": 0.9, \"b3\": 0.9, \"b16\": 1.0, \"b15\": 0.9, \"b8\": 0.9, \"b7\": 0.8, \"b24\": 0.8}", "Based on the text of the paper and the authors' notes, the most pertinent source papers seem to be:\n\n- \"Translating embeddings for modeling multi relational data\" [b1]: Mentioned as a traditional KGC method, and the paper discusses overcoming the limitations of this method.\n- \"Knowledge is flat: A seq2seq generative framework for various knowledge graph completion\" [b3]: A recent study that uses a similar seq2seq generation methodology for knowledge graph completion, which the paper builds upon.\n- \"The probabilistic relevance framework: BM25 and beyond\" [b16]: Referred to as a method used for information retrieval in the paper's proposed model.\n- \"Exploring the limits of transfer learning with a unified text to text transformer\" [b15]: Used as the underlying language model for the paper's proposed seq2seq model.\n- \"Kepler: A unified model for knowledge embedding and pre-trained language representation\" [b24]: Mentioned as a method that tried to avoid scalability issues by using pre-trained language models, but still had limitations that the paper addresses.\n- \"OGB-LSC: A large-scale challenge for machine learning on graphs\" [b7]: A dataset used for evaluation and comparison in the paper.\n\nHere is the result in the json format:\n\n{\n    \"b1\": 0.8,\n    \"b3\": 0.9,\n    \"b15\": 0.9,\n    \"b16\": 0.9,\n    \"b24\": 0.8,\n    \"b7\": 0.8\n} \n\nThe confidence scores are subjective and may vary. They are intended to represent the degree to which each source paper seems to have influenced the given paper, based on the number of times they are cited and the context in which they are discussed.", "Based on the text of the paper, the following references appear to be the most relevant source papers:\n\n1. 'b1': 'translating embeddings for modeling multi relational data' - The paper talks about traditional KGC methods such as TransE, indicating that it might be a source of inspiration for the paper.\n2. 'b3': 'knowledge is flat a seq seq generative framework for various knowledge graph completion' - This paper is cited in the context of seq2seq KGC methodology, which is a major focus of the paper at hand.\n3. 'b16': 'the probabilistic relevance framework bm and beyond' - The authors use BM25 for retrieval, indicating the relevance of this paper.\n4. 'b15': 'exploring the limits of transfer learning with a unified text to text transformer' - The T5 model from this paper is used in the generation module.\n5. 'b24': 'kepler a unified model for knowledge embedding and pre trained language representation' - This paper is the source of the Wikidata5M dataset used in the experiments.\n6. 'b7': 'ogb lsc a large scale challenge for machine learning on graphs' - This paper is the source of the WikiKG90Mv2 dataset used in the experiments.\n\nTherefore, the JSON format of the answer would be:\n\n{\n\"b1\": 0.9,\n\"b3\": 0.9,\n\"b16\": 0.9,\n\"b15\": 0.9,\n\"b24\": 0.8,\n\"b7\": 0.8\n}", "Based on the text of the paper, the author introduces a new method called ReSKGC for knowledge graph completion (KGC) which overcomes the limitations of existing seq2seq KGC methods. The main inspiration for this method comes from the combination of retrieval approach and seq2seq KGC methods. The key references that inspire this research are as follows:\n\n1. \"TransE [b1]\" - The author mentions TransE as a traditional KGC method which suffers from scalability issues. Although the author is overcoming the limitations of this method, it seems to be a foundational work for this research.\n2. \"DistMult [b26]\" - Similar to TransE, DistMult is also mentioned as a traditional KGC method with scalability issues.\n3. \"BM25 [b16]\" - BM25 is used as the retrieval module in the proposed method, indicating its significant influence on the research.\n4. \"Recent studies [b3] [b17]\" - These two papers are mentioned together as the recent studies that utilize a seq2seq generation methodology for KGC, indicating their direct relevance to the ReSKGC.\n5. \"Transformer [b21]\" - Transformer-based models are used in the proposed method for predicting the missing entity.\n6. \"T5 [b15]\" - T5 is used as the generation module in the proposed method.\n\nSo, the references are: {\"b1\": 0.8, \"b26\": 0.8, \"b16\": 1, \"b3\": 0.9, \"b17\": 0.9, \"b21\": 0.9, \"b15\": 0.9}. The confidence scores are given based on the importance of the references in the method proposed by the author.", "Based on the given text, the source papers that most inspire the author are:\n\n1. \"Translating embeddings for modeling multi-relational data\" [b1]: The text mentions this paper while discussing traditional KGC methods, indicating that the author takes inspiration from the method but identifies its limitations.\n\n2. \"Sequence-to-sequence knowledge graph completion and question answering\" [b17]: This paper is referenced in the context of recent studies utilizing a seq2seq generation methodology for knowledge graph completion.\n\n3. \"The probabilistic relevance framework: BM25 and beyond\" [b16]: This paper is directly referenced by the author as the retrieval module used in their proposed model.\n\n4. \"Exploring the limits of transfer learning with a unified text-to-text transformer\" [b15]: This paper is referenced as the base of the generation module in the proposed model.\n\n5. \"Leveraging passage retrieval with generative models for open domain question answering\" [b8]: This paper is referenced by the author while explaining the generation module of their proposed model.\n\n6. \"Wikidata: a free collaborative knowledgebase\" [b22]: This paper is mentioned as an example of a large real-world knowledge graph, which presents scalability challenges that the author's proposed model aims to address.\n\n7. \"Kepler: a unified model for knowledge embedding and pre-trained language representation\" [b24]: This paper is referenced when the author discusses the performance of their model on the Wikidata5M dataset.\n\n8. \"OGB-LSC: A large-scale challenge for machine learning on graphs\" [b7]: This paper is referenced when the author discusses the performance of their model on the WikiKG90Mv2 dataset.\n\nThe confidence scores can be generated based on the frequency and the context of the citations in the text. However, without knowing the specific algorithm for generating confidence scores, I can only provide the list of source papers.\n\nThe result in JSON format is:\n\n{ \"b1\": 0.8, \"b17\": 0.8, \"b16\": 0.9, \"b15\": 0.9, \"b8\": 0.9, \"b22\": 0.7, \"b24\": 0.7, \"b7\": 0.7 }"], "6323e96890e50fcafd8a40e6": ["Based on the text and the titles of the references, the source papers that inspired the authors most seem to be:\n\n- 'molecular design in drug discovery a comprehensive review of deep generative models brief' [b5]: The paper heavily relies on the deep learning generative models for de novo drug design, which are primarily discussed in this paper.\n\n- 'l structure based de novo drug design using d deep generative models' [b6]: This paper seems to have introduced the authors to the concept of structure-based de novo methods and how to develop generative algorithms.\n\n- 'a deep generative model for structure based de novo drug design' [b7]: Similar to [b6], this paper seems to have influenced the authors in terms of structure-based de novo drug design.\n\n- 'dockstream a docking wrapper to enhance de novo molecular design' [b8]: This paper seems to have influenced the authors in terms of using molecular docking for de novo molecular design.\n\n- 'defining and exploring chemical spaces trends chem' [b2]: This paper seems to have inspired the authors in terms of exploring chemical spaces for creating molecules with specific properties.\n\n- 'generative models for de novo drug design' [b4]: This paper seems to have provided the authors with a foundation on generative models for de novo drug design.\n\nIn JSON format:\n\n{\n\"b2\": 0.7,\n\"b4\": 0.7,\n\"b5\": 0.8,\n\"b6\": 0.8,\n\"b7\": 0.8,\n\"b8\": 0.7\n}", "Based on the text of the paper, the following references are most likely to have inspired the authors:\n\n1. \"generative models for de novo drug design\" [b4]: The authors discuss using deep learning-based generative models for molecular design, which is a core concept in the given paper.\n2. \"molecular design in drug discovery a comprehensive review of deep generative models brief\" [b5]: This paper reviews deep learning generative models for de novo drug design, clearly relevant to the current paper.\n3. \"l structure based de novo drug design using d deep generative models\" [b6]: The authors talk about using structures of both targets and ligands for developing generative algorithms, something the current paper also does.\n4. \"relation a deep generative model for structure based de novo drug design\" [b7]: This paper seems to be discussing similar themes to the current paper, such as structure-based de novo drug design.\n5. \"dockstream a docking wrapper to enhance de novo molecular design\" [b8]: This paper discusses a docking method used in the current paper to generate molecules for maximizing a score.\n\nThe confidence scores for these references would be estimated as follows:\n\n{\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.85,\n    \"b7\": 0.85,\n    \"b8\": 0.8\n}\n\nThese scores are estimated based on the relevance of the topics discussed in each reference paper to the current paper, and the frequency of their mention in the paper.", "Based on the provided paper text and the titles of the references, the source papers that inspire the given paper most are likely to be:\n\n1. 'virtual screening strategies in drug discovery a critical review' [b1]: The author mentions the limitations of the virtual screening approach in the introduction. This reference might have guided the author's motivation to develop a new method.\n\n2. 'generative models for de novo drug design' [b4]: It is mentioned that deep learning-based de novo molecular design has seen rapid development in recent years. This paper might have inspired the use of deep learning in the author's method.\n\n3. 'l structure based de novo drug design using d deep generative models' [b6]: This paper might have inspired the author to consider both the structures of targets and ligands for developing generative algorithms.\n\n4. 'relation a deep generative model for structure based de novo drug design' [b7]: This paper also might have had a direct influence on the author's method.\n\n5. 'dockstream a docking wrapper to enhance de novo molecular design' [b8]: The author mentions the use of molecular docking in the score function, which suggests this paper might have been an inspiration.\n\nThe JSON format of the results:\n\n{\n\"b1\": 0.9,\n\"b4\": 0.8,\n\"b6\": 0.7,\n\"b7\": 0.6,\n\"b8\": 0.8\n}\n\nNote: The confidence score is assigned based on inferred relevance from the text, with 1 being the highest and 0 being the lowest.", "Based on the provided text and instructions, the following source papers appear to have most directly influenced this research paper:\n\n1. \"molecular design in drug discovery a comprehensive review of deep generative models brief [b5]\". This paper is referenced in relation to the initial development of deep-learning generative models for drug design, which appears to be a significant aspect of the research paper's approach. Confidence score: 0.75.\n\n2. \"structure based de novo drug design using d deep generative models [b6]\" and \"relation a deep generative model for structure based de novo drug design [b7]\". Both papers are referred to when discussing the development of generative algorithms considering both target and ligand structures, a method that seems to be adopted in the research paper. Confidence score: 0.8.\n\n3. \"dockstream a docking wrapper to enhance de novo molecular design [b8]\". This paper is mentioned when discussing the calculation of scores in reinforcement learning, a key part of the research paper's method. Confidence score: 0.7.\n\n4. \"generative models for de novo drug design [b4]\". This paper is mentioned in the context of the development of deep learning-based de novo molecular design, which seems to be a big inspiration for the current study. Confidence score: 0.65.\n\nPlease note that these are inferred from the provided text and may not fully reflect the most influential papers, as the full text of the research paper and more context could provide a more accurate picture. Also, the confidence scores are subjective and based on the mentioned criteria. \n\nFinal JSON format:\n{\n\"b5\": 0.75,\n\"b6\": 0.8,\n\"b7\": 0.8,\n\"b8\": 0.7,\n\"b4\": 0.65\n}\n", "Based on the text and the note, the source papers that inspired the given paper most are:\n\n1. 'virtual screening strategies in drug discovery a critical review' [b1]: The author mentions that this conventional computational approach has limitations which inspired the development of their new method.\n\n2. 'a estimation of the size of drug like chemical space based on gdb data' [b3]: This paper seems to be the motivation behind the author's objective of exploring a larger chemical space.\n\n3. 'generative models for de novo drug design' [b4]: The author mentions the use of Deep Learning-based generative models in their approach.\n\n4. 'molecular design in drug discovery a comprehensive review of deep generative models brief' [b5]: This paper is likely a source of inspiration for the author's use of deep generative model for de novo drug design.\n\n5. 'l structure based de novo drug design using d deep generative models' [b6]: This paper inspires the author's adoption of structure-based de novo methods.\n\n6. 'relation a deep generative model for structure based de novo drug design' [b7]: The author's approach seems to be directly motivated by this paper.\n\n7. 'dockstream a docking wrapper to enhance de novo molecular design' [b8]: This paper provided the docking solution used by the author, showing a direct inspiration.\n\nTherefore, the JSON output would be:\n\n{\n\"b1\": 0.8,\n\"b3\": 0.8,\n\"b4\": 0.9,\n\"b5\": 0.9,\n\"b6\": 0.9,\n\"b7\": 0.9,\n\"b8\": 0.8\n}"], "64a407dcd68f896efaf1ba6d": ["Based on the text of the paper, the most relevant source papers are:\n\n1. \"A survey of computer architecture simulation techniques and tools\" [b1]: This paper is cited multiple times and it seems to be the basis for the discussion on different types of processor simulation techniques that the author uses.\n\n2. \"The GAP benchmark suite\" [b6]: This benchmark suite is used for the experimental setup and thus directly impacts the paper's main contributions.\n\n3. \"Control flow optimization via dynamic reconvergence prediction\" [b14]: This paper seems to be linked to the novel method proposed by the author to tackle the main challenge.\n\n4. \"An analysis of the performance impact of wrong path memory references on out of order and runahead execution processors\" [b23]: This paper is directly related to the challenge outlined in the paper regarding the impact of wrong-path instructions on performance.\n\n5. \"Using simpoint for accurate and efficient simulation\" [b25]: This method is used in the experimental setup of the paper.\n\n6. \"Intel alder lake cpu architectures\" [b26]: This paper is used to configure the simulator in the experimental setup.\n\n\nThe confidence scores of these papers are assumed to be high (around 0.8-1.0) as they are directly related to the paper's main contributions and methods. \n\nSo, the result in json format would be:\n\n{\"b1\": 0.9, \"b6\": 0.9, \"b14\": 0.9, \"b23\": 0.9, \"b25\": 0.9, \"b26\": 0.9}", "Based on the text of the paper, the source papers that most inspired the author are:\n\n- \"A survey of computer architecture simulation techniques and tools\" [b1], as it is directly referenced in the discussion about integrated functional and performance simulation and the decoupled functional-first simulation. Confidence score: 0.9\n- \"An evaluation of high-level mechanistic core models\" [b11], as it is mentioned in the context of the author's Sniper setup, which is used to implement their techniques. Confidence score: 0.9\n- \"An analysis of the performance impact of wrong-path memory references on out of order and runahead execution processors\" [b23], which is cited in the discussion about the impact of wrong-path instructions on performance, particularly through caches. Confidence score: 0.85\n- \"The GAP benchmark suite\" [b6], as it is used as the benchmark for evaluating the proposed techniques. Confidence score: 0.85\n- \"Pin: building customized program analysis tools with dynamic instrumentation\" [b21], as it is used as the functional simulator in the author's setup. Confidence score: 0.85\n- \"Simulating wrong-path instructions in decoupled functional-first simulation\" [None], as it is the title of the paper and likely the central source of inspiration for the work undertaken. Confidence score: 1.0\n\nAs a JSON result:\n{\n  \"b1\": 0.9,\n  \"b11\": 0.9,\n  \"b23\": 0.85,\n  \"b6\": 0.85,\n  \"b21\": 0.85,\n  \"None\": 1.0\n}", "Based on the text and author's note, the following references seem to be the most relevant source papers:\n\n1. \"A survey of computer architecture simulation techniques and tools\" [b1]: This paper is cited multiple times in the text and seems to be the primary source of the author's knowledge about functional-first and integrated simulation techniques. The author also uses this paper to contrast and compare their proposed solutions.\n\n2. \"An analysis of the performance impact of wrong path memory references on out of order and runahead execution processors\" [b23]: This paper is directly referred to when discussing the impact of wrong-path instructions on performance, which is a key problem addressed by the author's proposed solutions.\n\n3. \"The GAP benchmark suite\" [b6]: This paper provides the benchmarks used by the author to evaluate their proposed solutions. The results of these evaluations are key to the author's claims about the effectiveness of their solutions.\n\n4. \"Control independence in trace processors\" [b27]: Although not directly mentioned in the text, this paper might be relevant as the author's solutions involve control flow convergence, which is a topic addressed in this paper.\n\n5. \"Intel alder lake CPU architectures\" [b26]: This paper provides the simulator configuration parameters used by the author, which have a direct impact on the effectiveness of the proposed solutions.\n\nThe confidence scores are estimated as follows:\n\n{\n\"b1\": 0.9,\n\"b23\": 0.8,\n\"b6\": 0.7,\n\"b27\": 0.5,\n\"b26\": 0.6\n}", "Based on the given text, the following references seem to have heavily influenced the main contributions of the paper:\n\n1. \"A survey of computer architecture simulation techniques and tools\" [b1]\n2. \"Transparent Control Independence (TCI)\" [b3]\n3. \"The GAP Benchmark Suite\" [b6]\n4. \"An analysis of the performance impact of wrong path memory references on out of order and runahead execution processors\" [b23]\n5. \"Using SimPoint for accurate and efficient simulation\" [b25]\n6. \"Intel Alder Lake CPU architectures\" [b26]\n7. \"Control independence in trace processors\" [b27]\n\nThese references seem to be directly related to the paper's novel contributions and are mentioned in contexts that imply they helped shape the author's approach. For example, [b1] is mentioned in the context of discussing two common ways to implement a processor simulator, which seems to be a fundamental concept in this paper. Similarly, [b3] is used in the context of discussing the characteristics of functional-first simulation. \n\nHere is the result in the required json format:\n{\"b1\": 0.9, \"b3\": 0.85, \"b6\": 0.8, \"b23\": 0.9, \"b25\": 0.8, \"b26\": 0.85, \"b27\": 0.8} \n\nThe confidence score is estimated based on the context in which each reference is cited. A higher score indicates that the reference seems to be more central to the paper's main contributions.", "Based on the text and notes provided, the source papers that inspire the author most are likely to be:\n\n{ \"b1\": 0.8, \"b2\": 0.6, \"b6\": 0.7, \"b10\": 0.9, \"b11\": 0.8, \"b23\": 0.9, \"b31\": 0.5, \"b32\": 0.5 }\n\nExplanation:\n\n- \"b1\": This paper is mentioned multiple times in the text and seems to be a significant source of inspiration for the work on processor simulation techniques.\n\n- \"b2\": This paper is referred to when discussing the characteristics of functional simulators, indicating it has played a part in shaping the author's understanding and approach.\n\n- \"b6\": This paper is used as the source of the GAP benchmark suite, which is essential for the evaluation of the techniques proposed in the paper.\n\n- \"b10\": This paper is referenced when discussing the impact of wrong-path execution, a key focus of the paper.\n\n- \"b11\": This paper is cited when detailing the implementation of the proposed wrong-path modelling techniques, suggesting it has been influential.\n\n- \"b23\": This paper discusses the influence of wrong-path instructions on performance, which is a central theme of the paper.\n\n- \"b31\" and \"b32\": These papers are mentioned as sources of emerging workloads that highlight the importance of the author's research, but they are not directly related to the novel methods and approaches proposed.\n\nThe confidence scores reflect the depth of influence each paper likely has on the author's work, with higher scores indicating a greater degree of influence."], "63ed9f3290e50fcafd0f10bd": ["{\n  \"b10\": 0.95,\n  \"b11\": 0.95,\n  \"b32\": 0.9,\n  \"b33\": 0.9,\n  \"b43\": 0.85\n}", "{\n    \"b10\": 0.9,\n    \"b11\": 0.9,\n    \"b12\": 0.85,\n    \"b32\": 0.95,\n    \"b33\": 0.85,\n    \"b43\": 0.9,\n    \"b44\": 0.85,\n    \"b45\": 0.85\n}", "{\n    \"b10\": 0.95,\n    \"b32\": 0.9,\n    \"b33\": 0.9,\n    \"b43\": 0.85,\n    \"b44\": 0.85,\n    \"b46\": 0.8\n}", "{\n    \"b10\": 0.95,\n    \"b32\": 0.95,\n    \"b33\": 0.95,\n    \"b43\": 0.90,\n    \"b44\": 0.90,\n    \"b45\": 0.90\n}", "{\n    \"b10\": 0.95,\n    \"b32\": 0.9,\n    \"b33\": 0.9,\n    \"b43\": 0.85,\n    \"b44\": 0.85,\n    \"b45\": 0.85\n}"], "642702aa90e50fcafd5de13d": ["Based on the given text and the titles of the references, the following references seem to be the source papers that inspired the author most:\n\n1. \"Fast: Quick Application Launch on Solid State Drives\" [b23]: This paper is directly mentioned in the text as a recent Linux-based prefetcher for SSDs. It describes a method that the author of the current paper has built upon to improve tracing accuracy.\n\n2. \"Improving Application Launch Performance on SSDs\" [b24]: This paper is mentioned in the text in the context of challenges related to app launches. It seems to be a key source that the author used to understand the challenges with previous methods and build upon them.\n\n3. \"Analyzing and Improving GNOME Startup Time\" [b8]: This paper is mentioned in relation to app prefetching, a process that the author of the current paper is aiming to improve.\n\n4. \"DiskSeen: Exploiting Disk Layout and Access History to Enhance I/O Prefetch\" [b10]: This paper is also mentioned in relation to app prefetching. It seems to be one of the source papers that inspired the author\u2019s work on improving prefetching.\n\n5. \"The BFQ I/O scheduler\" [b9]: This paper might have been a source of inspiration as the BFQ I/O scheduler is mentioned in the text. Its relevance to the research implies that the paper might have been a major influence.\n\n6. \"Exploiting SSD Parallelism to Accelerate Application Launch on SSDs\" [b38]: This paper is mentioned in relation to the benefits of command queuing, a technique that the author of the current paper has used.\n\n7. \"Improving Smartphone Responsiveness Through I/O Optimizations\" [b31]: This paper might have been inspirational as the author is working on a related topic, i.e., improving application launch times on personal computing/communication devices.\n\nThe JSON format result would be:\n\n{\"b23\": 0.95, \"b24\": 0.93, \"b8\": 0.90, \"b10\": 0.88, \"b9\": 0.85, \"b38\": 0.82, \"b31\": 0.80}", "Based on the provided text, the following references appear to have a significant influence on the paper's main contributions:\n\n1. Reference [b23]: \"fast quick application launch on solid state drives\" - This paper is referred to multiple times throughout the text in relation to the design of app prefetchers and the concept of launch latencies. The proposed algorithm in the paper seems to be built upon or improve this previous work. Therefore, it is likely one of the main source papers. Confidence score: 0.9\n\n2. Reference [b38]: \"exploiting ssd parallelism to accelerate application launch on ssds\" - This reference is mentioned in relation to the use of parallelism for app launches, which is a major consideration in the paper's proposed design. The paper seems to heavily rely on the concepts introduced in this reference. Confidence score: 0.8\n\n3. Reference [b26]: \"disk schedulers for solid state drivers\" - The paper refers to this reference when discussing the concept of range merging and how it enhances the throughput of an SSD. This concept appears to be a core part of the paper's proposed solution. Confidence score: 0.7\n\n4. Reference [b5]: \"a study of integrated prefetching and caching strategies\" - This paper is mentioned when discussing the role of an app prefetcher. As the paper's core contributions include the development of a novel app prefetcher, it is likely that this reference has heavily influenced the paper. Confidence score: 0.6\n\n5. Reference [b34]: \"hmb ssd framework for efficient exploiting of the host memory buffer in the nvme ssd\" - This reference is mentioned when discussing the performance of SSDs. As SSD performance is a key consideration in the paper's proposed solution, it is likely that this reference has contributed to the paper's main contributions. Confidence score: 0.6\n\n6. Reference [b48]: \"mm map few pages around fault address if they are in page cache\" - This paper is referred to when discussing the concept of page fault monitoring. As this concept is used in the paper's proposed solution, it is likely that this reference has heavily influenced the paper. Confidence score: 0.6\n\nThe final output in JSON format would be:\n{\"b23\": 0.9, \"b38\": 0.8, \"b26\": 0.7, \"b5\": 0.6, \"b34\": 0.6, \"b48\": 0.6}", "Based on the given paper text and the titles of the references, the following references seem to have inspired the paper's main contributions:\n\n1. \"Fast Quick Application Launch on Solid State Drives\" [b23]: The paper discusses the design of an app prefetcher and its application to speed up app launches. Given the similarity in the problems and solutions, this paper is highly likely to be a source paper.\n\n2. \"The Performance of PC Solid-State Disks (SSDs) as a Function of Bandwidth, Concurrency, Device Architecture, and System Organization\" [b11]: This paper could have informed the author's understanding of SSD performance and how it affects app launch times.\n\n3. \"Informed Prefetching and Caching\" [b35]: This paper discusses a method to improve disk performance, which is a central theme in the given paper. \n\n4. \"DiskSeen: Exploiting Disk Layout and Access History to Enhance I/O Prefetch\" [b10]: This paper is likely a source paper as it discusses I/O prefetching, a method also used in the given paper.\n\n5. \"Essential Roles of Exploiting Internal Parallelism of Flash Memory based Solid State Drives in High-Speed Data Processing\" [b7]: This paper discusses methods of exploiting parallelism in SSDs, which is a major consideration for the given paper's prefetcher design.\n\n6. \"Improving Application Launch Performance on SSDs\" [b24]: Given the similarities in the problem being tackled and the proposed solutions, this paper is likely a source paper.\n\nThe following JSON object lists these source papers with a confidence score:\n\n```\n{\"b23\": 0.9, \"b11\": 0.7, \"b35\": 0.7, \"b10\": 0.8, \"b7\": 0.8, \"b24\": 0.9}\n```", "Based on the text and reference titles provided, the following references seem to be the most influential:\n\n{\n\"b23\": 0.9,\n\"b35\": 0.8,\n\"b5\": 0.7,\n\"b6\": 0.7,\n\"b38\": 0.7,\n\"b8\": 0.6\n}\n\nThe confidence score for each reference is estimated based on the relevance of the reference to the research paper's main contributions and novel approaches. These references are highly related to the paper's key concepts such as app prefetching, disk caching in Linux, and launch-related block tracing. The phrases indicating direct inspiration or motivation are not given in the text, so the confidence score is inferred from the relevance of the reference to the paper's main contributions and methods. The references that are not directly related to the paper's topic or appear at the beginning of the text are excluded.", "Based on the information in the text and the titles of the references, the most relevant source papers appear to be:\n\n{ \"b3\": 0.9, \"b23\": 1.0, \"b4\": 0.8, \"b10\": 0.95, \"b14\": 0.85, \"b22\": 0.95, \"b27\": 0.8 }\n\nExplanation:\n\n- \"b3\" is cited when discussing the predicted increase in data stored on QLC flash by 2025, and its slower read/write times compared to TLC flash. This is directly relevant to the paper's discussion on app launch latency.\n\n- \"b23\" is cited when discussing the design of app prefetchers, a central topic of this paper.\n\n- \"b4\" and \"b10\" are cited when discussing different prefetching strategies for mechanical and flash disks, which is a key method discussed in the paper.\n\n- \"b14\" is cited when discussing the Linux I/O stack from disk caching perspectives, which is related to the paper's topic about improving app launches.\n\n- \"b22\" is cited when discussing the challenges of accurately tracking launch-related blocks, a significant issue that the paper addresses.\n\n- \"b27\" is cited when discussing integrated prefetching and caching strategies, which is a key method discussed in the paper."], "63f2e4aa90e50fcafd2820a9": ["Based on the text and additional notes, the source papers that most likely inspired the given academic paper are:\n\n- 'b4': \"Relational inductive biases, deep learning, and graph networks\". The author mentions using graph neural networks (GNNs) in their work, which is a central focus of this paper.\n\n- 'b16': \"Variational graph autoencoders\". The author specifically refers to using variational graph autoencoders (VGAE) in their new algorithm, which is the primary subject of this paper.\n\n- 'b17': \"Semi-supervised classification with graph convolutional networks\". The author mentions using graph convolutional neural networks (GCN), which is the main topic of this paper.\n\n- 'b31': \"Graph attention networks\". The author mentions using graph attention networks in their work, which is the main focus of this paper.\n\n- 'b41': \"Modeling polypharmacy side effects with graph convolutional networks\". The author specifically mentions this paper in the context of their own approach to predicting polypharmacy side effects.\n\nBased on the author's focus on graph neural networks and variational graph autoencoders, as well as the specific mention of the paper on polypharmacy side effects, these are the papers most likely to have inspired the author's work. \n\nPlease note that the confidence score is not provided because the analysis is based on text interpretation and it's not possible to provide a numerical score in this format.\n\nHere are the results in JSON format:\n\n{ \"b4\": \"Relational inductive biases, deep learning, and graph networks\", \n  \"b16\": \"Variational graph autoencoders\", \n  \"b17\": \"Semi-supervised classification with graph convolutional networks\",\n  \"b31\": \"Graph attention networks\",\n  \"b41\": \"Modeling polypharmacy side effects with graph convolutional networks\" }", "Based on the text of the paper, the most relevant source papers that inspired the author are:\n\n- [b41] \"Modeling polypharmacy side effects with graph convolutional networks\": The author directly mentioned that this paper proposed a graph autoencoder to predict polypharmacy side effects on a multimodal graph. The author then stated their work aims to improve upon this method using variational graph autoencoders (VGAE).\n- [b16] \"Variational graph auto encoders\": The author cited this paper when introducing the concept of variational graph autoencoders (VGAE), which is a key methodology used in the author's research.\n- [b34] \"DeepDDS: Deep Graph Neural Network with Attention Mechanism to Predict Synergistic Drug Combinations\": This paper is referred to when discussing previous approaches to link prediction on graphs, which is a key challenge addressed by the author.\n- [b17] \"Semi-supervised Classification with Graph Convolutional Networks\" and [b31] \"Graph Attention Networks\": Both papers are cited when discussing graph neural networks (GNNs), a methodology that the author uses in their research.\n- [b23] \"SSI-DDI: Substructure-Substructure Interactions for Drug-Drug Interaction Prediction\": The author refers to this paper when discussing previous work on drug-drug interaction modeling, which is a primary challenge outlined in the paper.\n\nThe potential confidence scores might be as follows:\n- [b41]: 0.9\n- [b16]: 0.8\n- [b34]: 0.7\n- [b17]: 0.7\n- [b31]: 0.7\n- [b23]: 0.7\n\nIn json format:\n\n{\"b41\": 0.9, \"b16\": 0.8, \"b34\": 0.7, \"b17\": 0.7, \"b31\": 0.7, \"b23\": 0.7}", "Based on the given information, the source papers that inspired the author can be identified by following the author's citations throughout the text. The author seemed to be influenced by the following papers:\n\n1. \"Modeling polypharmacy side effects with graph convolutional networks\" [b41]: The author used this paper as a primary reference on various occasions, specifically mentioning the graph autoencoder proposed in it.\n\n2. \"Drug-drug interaction through molecular structure similarity analysis\" [b32]: Mentioned in the context of previous work on drug-drug interactions.\n\n3. \"DeepDrug a general graph based deep learning framework for drug drug interactions and drug target interactions prediction\" [b39]: Cited when discussing recent approaches using graph convolutional neural networks (GCN) and convolutional neural networks (CNN) to learn structural and sequential representations of drugs and proteins.\n\n4. \"Graph attention networks\" [b31]: The author described how several graph attention (GAT) layers have been recommended for learning on hand-crafted features of drugs to predict drug-pair interactions.\n\n5. \"Variational graph auto encoders\" [b16]: The author specifically cited this paper while discussing the use of variational graph autoencoders (VGAE) to compute the latent variables.\n\n6. \"Neural message passing for quantum chemistry\" [b10]: Mentioned in the context of the message-passing scheme employed by the author's proposed model.\n\n7. \"Modeling polypharmacy and predicting drug drug interactions using deep generative models on multimodal graphs\" [None]: Although not cited in the text, this paper seems to share a similar topic with the author's paper, which might indicate some influence.\n\nThe confidence scores can be assigned based on the frequency and context of the citations, with more frequently cited and contextually important papers having higher scores.\n\nThe result in json format would be:\n\n{\n\"b41\": 0.9,\n\"b32\": 0.8,\n\"b39\": 0.8,\n\"b31\": 0.8,\n\"b16\": 0.8,\n\"b10\": 0.7,\n\"None\": 0.7\n}", "Based on the text provided, the following papers seems to be the most influential to the research presented:\n\n1. \"modeling polypharmacy side effects with graph convolutional networks\" [b41]: This paper is several times directly mentioned and seems to be the source of the main method used in the paper. Confidence score: 0.9\n\n2. \"DeepDrug a general graph based deep learning framework for drug drug interactions and drug target interactions prediction\" [b39]: This paper seems to provide a key comparison and motivation for the methods used in the paper. Confidence score: 0.8\n\n3. \"Graph attention networks\" [b31]: This paper is referenced in the explanation of the methods, and it seems to provide a key algorithm used in the paper. Confidence score: 0.7\n\n4. \"variational graph auto encoders\" [b16]: This paper is directly mentioned and seems to be the source of the main algorithm used in the paper. Confidence score: 0.9\n\n5. \"Deep gaussian embedding of graphs unsupervised inductive learning via ranking\" [b5]: This paper is mentioned in the introduction and seems to provide a key comparison and motivation for the methods used in the paper. Confidence score: 0.7\n\n6. \"drug-drug interaction through molecular structure similarity analysis\" [b32]: This paper is mentioned in the introduction and seems to provide a key comparison and motivation for the methods used in the paper. Confidence score: 0.7\n\n7. \"NetGAN: Generating Graphs via Random Walks\" [b6]: This paper seems to provide a key comparison and motivation for the methods used in the paper. Confidence score: 0.7\n\nJSON Format:\n{\n   \"b41\": 0.9,\n   \"b39\": 0.8,\n   \"b31\": 0.7,\n   \"b16\": 0.9,\n   \"b5\": 0.7,\n   \"b32\": 0.7,\n   \"b6\": 0.7\n}", "Based on the given text and the titles of the references, the most likely source papers for the given paper would be:\n\n1. \"modeling polypharmacy side effects with graph convolutional networks\" [b41]\n2. \"DeepDrug a general graph based deep learning framework for drug drug interactions and drug target interactions prediction\" [b39]\n3. \"variational graph auto encoders\" [b16]\n4. \"graph attention networks\" [b31]\n5. \"neural message passing for quantum chemistry\" [b10]\n6. \"ssi ddi substructure substructure interactions for drug drug interaction prediction\" [b23]\n\nThese papers are identified as source papers as they directly relate to the methods and challenges outlined in the text. The text mentions using VGAE to learn the representations of drugs and proteins on a multimodal graph, which relates to [b16]. The use of graph attention networks ([b31]) and graph convolutional networks ([b41]) are also mentioned. The paper also discusses drug-drug interactions, which ties to [b39] and [b23]. [b10] is included as the text mentions the use of a two-layer graph convolutional neural network, which employs a message passing scheme.\n\nThe confidence scores for each reference would ideally be computed by a text analysis tool but, assuming equal confidence for each identified source paper, a possible output in JSON format would be:\n\n{\n\"b41\": 0.1667,\n\"b39\": 0.1667,\n\"b16\": 0.1667,\n\"b31\": 0.1667,\n\"b10\": 0.1667,\n\"b23\": 0.1667\n} \n\nThis assumes equal confidence for each source paper as no specific weighting or prioritization criteria were provided."], "62466dd35aee126c0f8b7b62": ["{\n  \"b0\": 0.8,\n  \"b3\": 0.9,\n  \"b15\": 0.95,\n  \"b18\": 0.85,\n  \"b20\": 0.9,\n  \"b30\": 0.95,\n  \"b31\": 0.9,\n  \"b42\": 0.85\n}", "{\n    \"b3\": 0.95,\n    \"b15\": 0.90,\n    \"b18\": 0.90,\n    \"b20\": 0.85,\n    \"b30\": 0.85,\n    \"b31\": 0.85,\n    \"b42\": 0.85\n}", "{\n    \"b3\": 0.9,\n    \"b15\": 0.8,\n    \"b18\": 0.7,\n    \"b20\": 0.9,\n    \"b30\": 0.9,\n    \"b31\": 0.8,\n    \"b42\": 0.85\n}", "{\n  \"b15\": 0.95,\n  \"b18\": 0.95,\n  \"b20\": 0.9,\n  \"b30\": 0.9,\n  \"b31\": 0.9,\n  \"b42\": 0.9\n}", "{\n    \"b30\": 0.9,\n    \"b15\": 0.85,\n    \"b18\": 0.85,\n    \"b20\": 0.8,\n    \"b3\": 0.75,\n    \"b42\": 0.7,\n    \"b31\": 0.65\n}"], "6459ac57d68f896efa657eec": ["Based on the paper's content, the following source papers are identified:\n\n1. \"Language models are few-shot learners\" [b2] - The paper mentions this work when discussing the drawbacks of fine-tuning PLMs and the concept of prompting, indicating a strong influence. Confidence score: 0.9\n2. \"Attention is all you need\" [b47] - The transformer architecture, which is fundamental to this paper's proposed algorithm, originates from this paper. Confidence score: 0.8\n3. \"Pre-trained language models: past, present and future\" [b9] - This source is relevant because it provides a comprehensive overview of pre-trained language models, which are a core part of the paper's methodology. Confidence score: 0.7\n4. \"GPT-GNN: Generative pre-training of graph neural networks\" [b14] - The paper uses this work as the foundation for its graph-grounded contrastive pre-training strategy. Confidence score: 0.75\n5. \"Exploring the limits of transfer learning with a unified text-to-text transformer\" [b37] - This source paper is relevant as it discusses transfer learning, a key concept in the paper. Confidence score: 0.65\n6. \"Self-supervised meta-learning for few-shot natural language classification tasks\" [b0] - This paper directly relates to the paper's novel contributions in the area of low-resource text classification and is explicitly referenced. Confidence score: 0.8\n7. \"Automating the construction of internet portals with machine learning\" [b27] - This paper is mentioned as the source of the Cora dataset used in the experiments. However, it's relevance to the paper's contributions is not so direct, but still impactful. Confidence score: 0.6\n8. \"Learning transferable visual models from natural language supervision\" [b35] - The paper refers to this work when discussing the application of contrastive learning, indicating a clear influence. Confidence score: 0.7\n\nHere is the json format:\n{\"b2\": 0.9, \"b47\": 0.8, \"b9\": 0.7, \"b14\": 0.75, \"b37\": 0.65, \"b0\": 0.8, \"b27\": 0.6, \"b35\": 0.7}", "Based on the provided paper text and author notes, the most pertinent source papers that inspired the work are:\n\n{ \"b2\": 1,\n  \"b15\": 0.9,\n  \"b14\": 0.8,\n  \"b17\": 0.8,\n  \"b23\": 0.7,\n  \"b35\": 0.7,\n  \"b57\": 0.6,\n  \"b49\": 0.6}\n  \nExplanation:\n- \"b2\": The authors mention GPT-3 and its drawbacks, as well as the concept of prompting, which are both key concepts in this reference.\n- \"b15\": BERT is mentioned as a key method for text classification and as part of the motivation for a new approach.\n- \"b14\": The paper refers to self-supervised graph neural networks and presents GPT-GNN as a method.\n- \"b17\": The authors discuss the concept of prompt tuning and its advantages, which appears to be a significant influence on their approach.\n- \"b23\": The paper discusses the optimization of continuous prompts, a concept that is central to the authors' model.\n- \"b35\": The authors mention contrastive pre-training, which is a strategy used in this reference.\n- \"b57\": The authors use graph neural networks (GNNs), which are the main focus of this reference.\n- \"b49\": This reference discusses the self-supervised GNNs and the use of pretext tasks on a label-free graph, which are concepts mentioned in the paper.", "Based on the provided text, the source papers that inspired the given paper are as follows:\n\n{ \n\"b2\": 0.8,\n\"b13\": 0.7,\n\"b14\": 0.7,\n\"b15\": 0.7,\n\"b17\": 0.7,\n\"b21\": 0.7,\n\"b23\": 0.7,\n\"b35\": 0.7\n} \n\nThe confidence scores are based on the frequency and context of the references in the provided text. However, it is recommended to confirm these findings by reading the original papers and assessing the relevance and influence on the given paper's contributions.", "{\n\"b2\": 1,\n\"b15\": 0.9,\n\"b14\": 0.85,\n\"b49\": 0.8,\n\"b57\": 0.8,\n\"b35\": 0.75,\n\"b17\": 0.7,\n\"b23\": 0.65\n}", "Based on the information given, the source papers that inspired the given paper the most are:\n\n1. \"Language models are few-shot learners\" [b2]\n2. \"Attention is all you need\" [b47]\n3. \"The power of scale for parameter-efficient prompt tuning\" [b17]\n4. \"Prefix tuning: Optimizing continuous prompts for generation\" [b18]\n5. \"Graph convolutional networks for text classification\" [b63]\n6. \"Learning transferable visual models from natural language supervision\" [b35]\n7. \"Improved deep metric learning with multi-class N-pair loss objective\" [b42]\n\nThe JSON format of the results would be:\n\n{\n\"b2\": 0.9,\n\"b47\": 0.85,\n\"b17\": 0.8,\n\"b18\": 0.8,\n\"b63\": 0.75,\n\"b35\": 0.7,\n\"b42\": 0.65\n}\n\nThe confidence scores are assigned subjectively based on the importance of the source paper to the given paper's novel contributions, as inferred from the text."], "64a407dcd68f896efaf1ba4e": ["{\n  \"b12\": 0.95,\n  \"b17\": 0.9,\n  \"b7\": 0.85\n}", "{\n    \"b12\": 0.90,\n    \"b17\": 0.85,\n    \"b6\": 0.80,\n    \"b7\": 0.75\n}", "{\n  \"b6\": 0.9,\n  \"b7\": 0.8,\n  \"b12\": 0.9,\n  \"b17\": 0.8\n}", "```json\n{\n    \"b12\": 0.95,\n    \"b17\": 0.9,\n    \"b7\": 0.85,\n    \"b6\": 0.85\n}\n```", "```json\n{\n  \"b12\": 0.95,\n  \"b17\": 0.9\n}\n```"], "64be63403fda6d7f063e57a7": ["Based on the information provided, the source papers that most directly influenced the given paper are likely:\n\n- \"Robust Markov decision processes\" [b71]\n- \"Policy gradient in robust mdps with global convergence guarantee\" [b0]\n- \"On the theory of policy gradient methods: Optimality, approximation, and distribution shift\" [b7]\n- \"Robust dynamic programming\" [b23]\n- \"On gradient descent ascent for nonconvex concave minimax problems\" [b33]\n- \"Double-Loop Robust Policy Gradient\" [b24]\n- \"What is local optimality in nonconvex nonconcave minimax optimization\" [b35]\n\nThe reasoning behind these choices is as follows:\n\n- The paper discusses the challenges of robust Markov decision processes (RMDPs), so papers discussing this topic, such as [b71], are likely to be significant influences.\n\n- The paper also introduces a new method, the Double-Loop Robust Policy Gradient (DRPG), which is inspired by double-loop algorithms designed for solving saddle point problems, such as those discussed in [b24] and [b35].\n\n- The paper uses policy gradient methods, so papers discussing these methods, such as [b0] and [b7], are also likely to be influences.\n\n- The paper discusses dynamic programming in the context of RMDPs, so [b23] is also likely to be an influence.\n\n- The problem of nonconvex nonconcave minimax optimization, discussed in [b33], is also relevant to the challenges discussed in the paper.\n\nAs a result, the returned dictionary of source papers and their confidence scores is as follows:\n\n{\"b71\": 0.9, \"b0\": 0.8, \"b7\": 0.8, \"b23\": 0.7, \"b33\": 0.7, \"b24\": 0.8, \"b35\": 0.8}.\n", "Based on the text and author notes, the source papers for the given paper seem to be:\n\n1. \"Robust Markov decision processes\" [b71]\n2. \"Policy gradient in robust mdps with global convergence guarantee\" [None]\n3. \"On the theory of policy gradient methods: Optimality, approximation, and distribution shift\" [b0]\n4. \"Double-Loop Robust Policy Gradient (DRPG)\" [b24]\n5. \"Solving a class of non-convex min-max games using iterative first order methods\" [b41]\n6. \"Robust control of Markov decision processes with uncertain transition matrices\" [b40]\n\nThe JSON format would be:\n{\n\"b71\": 0.9,\n\"None\": 0.8,\n\"b0\": 0.8,\n\"b24\": 0.7,\n\"b41\": 0.7,\n\"b40\": 0.7\n} \n\nThe confidence scores are hypothetical and represent how directly each paper contributes to the main contributions of the paper in question. They are estimated based on the frequency and context in which each source paper is referenced.", "Based on the text and titles of the references, the source papers that most inspired the given paper could be:\n\n- \"Robust Markov decision processes\" [b71]: This paper is mentioned multiple times throughout the text and seems to be a seminal work on RMDPs, which is the main topic of the given paper.\n- \"Policy gradient methods for reinforcement learning with function approximation\" [b65]: The given paper presents a policy gradient method for solving RMDPs. It is likely that the authors were influenced by this work on policy gradient methods.\n- \"On the theory of policy gradient methods: optimality, approximation, and distribution shift\" [b0]: This paper is repeatedly mentioned, suggesting that it is a significant source of inspiration for the authors.\n- \"Policy Gradient in Robust MDPs with Global Convergence Guarantee\" [None]: This is the title of the given paper, but it suggests that the authors are building on work about global convergence in policy gradient methods.\n- \"Robust Markov Decision Processes Beyond Rectangularity\" [b17]: This paper is cited in the context of dealing with non-rectangular RMDPs, suggesting that it has informed the authors' approach.\n- \"Efficient Algorithms for Smooth Minimax Optimization\" [b66]: This work is cited in the context of the paper's discussion of minimax optimization, indicating that it has influenced the authors' methods.\n- \"On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems\" [b33]: This work is referenced multiple times in the context of the paper's discussion of non-convex and non-concave problems, suggesting that it is a key source of inspiration.\n\nHere is the result in json format:\n{\n\"b71\": 0.9, \n\"b65\": 0.7,\n\"b0\": 0.8, \n\"None\": 0.8,\n\"b17\": 0.7,\n\"b66\": 0.7,\n\"b33\": 0.7\n}", "Based on the text of the paper, the source papers are most likely to be:\n\n1. \"Policy Gradient in Robust MDPs with Global Convergence Guarantee.\" This paper is the main source as the author's work is based on the algorithm discussed in it. The title of the source paper indicates a strong relation with the topic.\n   \n2. \"On the Theory of Policy Gradient Methods: Optimality Approximation and Distribution Shift.\" This paper might be a source as it discusses policy gradient methods which are mentioned multiple times in the author's work.\n   \n3. \"Robust Markov Decision Processes.\" This paper likely contributed to the author's understanding of RMDPs, which are central to their research.\n   \n4. \"On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems.\" The author's work involves solving non-convex minimax problems, which suggests this paper was a source.\n   \n5. \"Robust Control of Markov Decision Processes with Uncertain Transition Matrices.\" This paper likely informed the author's understanding of the challenges associated with uncertain transition functions in RMDPs.\n   \n6. \"On the Global Convergence Rates of Softmax Policy Gradient Methods.\" This paper might have influenced the author's work on the convergence properties of their proposed algorithm.\n   \n7. \"Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods.\" This paper might be a source as the author's work involves solving non-convex min-max problems, which is the main topic of this paper.\n\nIn the JSON format, the result would appear as follows:\n{\n  \"b0\": 0.9,\n  \"b7\": 0.8,\n  \"b71\": 0.9,\n  \"b33\": 0.8,\n  \"b40\": 0.8,\n  \"b38\": 0.8,\n  \"b41\": 0.8\n}", "Based on the text, the source papers that most influenced the author's work are:\n\n- \"Robust Markov decision processes\" [b71]: The author frequently cites this paper and directly mentions it when discussing the challenges of applying policy gradient methods to robust Markov decision processes (RMDPs). Additionally, the proposed method in the paper, the Double-Loop Robust Policy Gradient (DRPG) method, is a general policy gradient method for RMDPs, which indicates it was likely heavily inspired by this source.\n\n- \"On the theory of policy gradient methods: optimality, approximation, and distribution shift\" [b0]: This paper is cited when discussing the policy gradient methods and their guarantees to find a globally-optimal policy in tabular MDPs. It seems to be a significant influence on the author's work on policy gradient methods for RMDPs.\n\n- \"Stochastic recursive gradient descent ascent for stochastic nonconvex-strongly concave minimax problems\" [b35]: This paper is cited when the author introduces the DRPG method. The DRPG method is inspired by double-loop algorithms designed for solving saddle point problems, which strongly suggests this paper inspired their work.\n\n- \"On gradient descent ascent for nonconvex concave minimax problems\" [b33]: This paper is referenced when discussing the challenges in the convergence analysis for solving RMDPs, indicating it had a significant influence on the author's work.\n\n- \"Policy gradient in Lipschitz Markov decision processes\" [b48]: This paper is cited when discussing the policy gradient techniques in reinforcement learning. It seems to have influenced the author's methodological approach.\n\nBased on the analysis, the references that are most central to the paper's contributions are [b71], [b0], [b35], [b33], and [b48]. The confidence score for each reference is subjective and depends on the specific context, but a possible assignment could be: \n\n{ \"b71\": 0.9, \"b0\": 0.8, \"b35\": 0.8, \"b33\": 0.7, \"b48\": 0.7 }. \n\nThis score suggests that [b71] has the highest influence on the paper, followed by [b0], [b35], [b33], and [b48]."], "6326f71890e50fcafdd04899": ["{\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b8\": 0.85,\n    \"b46\": 0.8\n}", "{\n  \"b4\": 0.9,\n  \"b5\": 0.9,\n  \"b8\": 0.85,\n  \"b9\": 0.8,\n  \"b30\": 0.8,\n  \"b36\": 0.85,\n  \"b41\": 0.75,\n  \"b46\": 0.9\n}", "{\n  \"b46\": 0.95,\n  \"b4\": 0.9,\n  \"b5\": 0.85,\n  \"b8\": 0.8,\n  \"b41\": 0.75,\n  \"b47\": 0.7,\n  \"b48\": 0.65,\n  \"b49\": 0.6\n}", "{\n    \"b4\": 0.95,\n    \"b5\": 0.95,\n    \"b8\": 0.9,\n    \"b46\": 0.9,\n    \"b30\": 0.85\n}", "{\n    \"b5\": 0.9,\n    \"b4\": 0.9,\n    \"b8\": 0.9,\n    \"b30\": 0.85,\n    \"b46\": 0.85,\n    \"b41\": 0.8\n}"], "64f59fc23fda6d7f0648f1fb": ["Based on the given information, the following are the source papers that have inspired the text of the given paper:\n\n1. \"Reinforcement Learning from Human Feedback\" (b26) - The paper discusses RLHF as a key driver of success in modern language models and cites it as an effective technique for aligning language models to human preferences. Confidence Score: 1.0\n\n2. \"Training language models to follow instructions with human feedback\" (b21) - The RLHF pipeline introduced in this paper is reviewed in the given text, suggesting it's a fundamental reference for the author's work. Confidence Score: 1.0 \n\n3. \"Learning to summarize with human feedback\" (b26) - The given paper directly compares RLAIF against RLHF on the task of summarization, which is a key topic in this reference. Confidence Score: 1.0\n\n4. \"ChatGPT outperforms crowd workers for text-annotation tasks\" (b7) - The paper mentions this work when discussing that large language models exhibit a high degree of alignment with human judgment, indicating it's an influential reference. Confidence Score: 0.9\n\n5. \"Policy gradient methods for reinforcement learning with function approximation\" (b27) - The paper mentions the use of the Advantage Actor Critic (A2C) algorithm, which is a topic in this reference, for reinforcement learning. Confidence Score: 0.8\n\n6. \"Chain of thought prompting elicits reasoning in large language models\" (b31) - The paper discusses the use of chain-of-thought (COT) reasoning from AI labelers to improve alignment with human preferences, which is a topic in this reference. Confidence Score: 0.8\n\n7. \"Asynchronous methods for deep reinforcement learning\" (b18) - The paper mentions the use of A2C algorithm, a topic covered in this work, for reinforcement learning. Confidence Score: 0.7\n\nThis information will be returned in the following JSON format:\n\n{\n\"b26\" : 1.0,\n\"b21\" : 1.0,\n\"b26\" : 1.0,\n\"b7\" : 0.9,\n\"b27\" : 0.8,\n\"b31\" : 0.8,\n\"b18\" : 0.7\n}", "Based on the text of the academic paper and the titles of the references, the most relevant source papers are:\n\n1. \"Reinforcement Learning from Human Feedback (RLHF)\" [b26] [b21]: These references are critical to the paper as they introduce RLHF, which is a key concept the paper is based on and compares its results with.\n\n2. \"ChatGPT outperforms crowd workers for text-annotation tasks\" [b7]: This paper supports the author's claim about the alignment of large language models with human judgment, which is a fundamental assumption for the research.\n\n3. \"Bai et al. (2022b)\" [b1]: This paper introduces the technique of \"Reinforcement Learning from AI Feedback\" (RLAIF), which is the paper's central methodology. \n\n4. \"Chain of thought prompting elicits reasoning in large language models\" [b31]: This paper is significant because it introduces the concept of chain-of-thought reasoning that was used to improve AI-generated preferences.\n\n5. \"Asynchronous methods for deep reinforcement learning\" [b18]: This paper introduces the Advantage Actor Critic (A2C) algorithm, which the paper uses for reinforcement learning.\n\nIn JSON format:\n{\n\"b26\": 1,\n\"b21\": 1,\n\"b7\": 0.9,\n\"b1\": 1,\n\"b31\": 0.8,\n\"b18\": 0.8\n}", "Based on the provided details of the paper and the list of references, the source papers that likely influenced its core contributions are:\n\n1. \"Constitutional AI\" from Bai et al. (2022b): The paper mentions this work directly and elaborates on its exploration of Reinforcement Learning from AI Feedback (RLAIF) which is a fundamental concept in this paper. The authors of the current paper expand on this concept and directly compare RLAIF and RLHF. [Confidence Score: 0.9]\n\n2. \"Training language models to follow instructions with human feedback\" [b21]: This paper is mentioned as the one introducing the RLHF pipeline that the current paper uses. It is a fundamental methodological reference. [Confidence Score: 0.9]\n\n3. \"Learning to Summarize with Human Feedback\" [b26]: This paper is also mentioned as one introducing the RLHF pipeline and it seems directly related to the task of summarization that the current paper is focusing on. [Confidence Score: 0.9]\n\n4. \"Self-Consistency Improves Chain of Thought Reasoning in Language Models\" [b29]: This paper's concept of self-consistency is explicitly used and discussed in the current paper. [Confidence Score: 0.8]\n\n5. \"Asynchronous Methods for Deep Reinforcement Learning\" [b18]: The current paper uses a modified version of the Advantage Actor Critic (A2C) algorithm that this paper introduces. [Confidence Score: 0.8]\n\n6. \"Chain of Thought Prompting Elicits Reasoning in Large Language Models\" [b31]: The current paper utilizes the concept of Chain-of-thought reasoning from this paper. [Confidence Score: 0.7]\n\nHere is the answer in JSON format:\n\n{\n    \"b2\" : 0.9,\n    \"b21\" : 0.9,\n    \"b26\" : 0.9,\n    \"b29\" : 0.8,\n    \"b18\" : 0.8,\n    \"b31\" : 0.7\n}", "Based on the given paper and the context, the source papers that influenced this paper the most seem to be:\n\n1. \"Training language models to follow instructions with human feedback\" [b21]\n2. \"Learning to summarize with human feedback\" [b26]\n3. \"Reinforcement Learning from AI Feedback\" [b1]\n4. \"ChatGPT outperforms crowd workers for text-annotation tasks\" [b7]\n5. \"Chain of thought prompting elicits reasoning in large language models\" [b31]\n6. \"Self consistency improves chain of thought reasoning in language models\" [b29]\n7. \"Scaling laws for neural language models\" [b11]\n\nThese papers are chosen as they are directly referenced in the context of methodological development and problem solving in the paper, indicating a strong influence on the paper's core contributions.\n\nIn JSON format:\n\n{\n\"b21\": 0.9,\n\"b26\": 0.8,\n\"b1\": 0.85,\n\"b7\": 0.75,\n\"b31\": 0.8,\n\"b29\": 0.7,\n\"b11\": 0.7\n} \n\nThe confidence scores are approximations based on the extent of the influence inferred from the text and are not absolute measures of influence.", "Based on the text of the paper and the titles of the references, the source papers that seem to have inspired the author most are:\n\n1. \"Reinforcement Learning from Human Feedback\" [b26] [b21] - The author directly states that this is a key driver of success in modern conversational language models and is the primary method the paper seeks to improve upon.\n2. \"Reinforcement Learning from AI Feedback\" (Bai et al. 2022b) - This previous work on using AI preferences to train a reward model for reinforcement learning is directly compared and contrasted with the author's own approach.\n3. \"Training language models to follow instructions with human feedback\" [b21] - This paper is mentioned in the context of the RLHF pipeline that the author's work is based on.\n4. \"Learning to summarize with human feedback\" [b26] - This paper is also mentioned in the context of the RLHF pipeline that the author's work is based on.\n5. \"ChatGPT outperforms crowd workers for text-annotation tasks\" [b7] - This work is referred to support the potential of AI feedback and is likely a reference point for the author's work.\n6. \"Self consistency improves chain of thought reasoning in language models\" [b29] - This work is mentioned in the context of techniques the author experimented with to improve the alignment of AI-generated preferences with human preferences.\n7. \"Asynchronous methods for deep reinforcement learning\" [b18] - This paper is mentioned in the context of the reinforcement learning techniques they used.\n\nThe confidence scores are subjective and could range from 0.6 to 0.9 depending on the specific influence of each paper.\n\nThe result in JSON format would be:\n{\n  \"b26\": 0.9,\n  \"b21\": 0.9,\n  \"Bai et al. 2022b\": 0.8,\n  \"b7\": 0.7,\n  \"b29\": 0.7,\n  \"b18\": 0.6\n}"], "6459ac63d68f896efa658a19": ["Based on the text provided, the source papers that inspired the authors the most are:\n\n1. 'A unified MRC framework for named entity recognition' [b20]: The authors mention this paper as a source of inspiration for their approach of harnessing MRC to address NER. This paper seems to be highly influential as it is mentioned in relation to the core idea of the paper.\n\n2. 'HRCA advanced multiple choice machine reading comprehension method' [b52]: This paper is referenced when the authors discuss the MRC reasoning strategies they integrated into the NER tasks.\n\n3. 'Zero-shot learners for natural language understanding via a unified multiple choice perspective' [b51]: This paper is mentioned in relation to the design principle of the author's method.\n\n4. 'LUKE deep contextualized entity representations with entity aware self attention' [b50]: This paper is referenced when discussing the state-of-the-art NER works that the authors aim to match in performance.\n\n5. 'NER-BERT a pre trained model for low-resource entity tagging' [b24]: This paper is mentioned when discussing the dominant trend of introducing external data to PLMs.\n\n6. 'A survey on deep learning for named entity recognition' [b19]: This paper is referenced when the authors discuss the fundamental topic of their study, NER.\n\n7. 'Named entity and relation extraction with multi-modal retrieval' [b42]: This paper is also mentioned as part of the current state-of-the-art NER works.\n\nBased on the given text, these papers seem to be the most influential in shaping the author's work.\n\nThe confidence scores for each paper might be as follows:\n\n```\n{\n    \"b20\": 0.9,\n    \"b52\": 0.85,\n    \"b51\": 0.8,\n    \"b50\": 0.7,\n    \"b24\": 0.65,\n    \"b19\": 0.6,\n    \"b42\": 0.55\n}\n```", "Based on the text of the paper, the most pertinent source papers that inspired the author are:\n\n1. \"A unified MRC framework for named entity recognition\" [b20]: The author mentions that the paper is motivated by the idea of harnessing Machine Reading Comprehension (MRC) to address Named Entity Recognition (NER), suggesting that this paper is a significant influence.\n\n2. \"HRCA: Advanced multiple choice machine reading comprehension method\" [b52]: The author uses this paper's method, HRCA, as a key part of their novel approach, indicating that it is a primary source of inspiration.\n\n3. \"Zero-shot learners for natural language understanding via a unified multiple choice perspective\" [b51]: This paper's design principle, which involves a less manual process, is used to inspire the author's own method.\n\n4. \"Named entity recognition using BERT: BiLSTM-CRF for Chinese electronic health records\" [b5]: The author uses a similar framework to solve NER tasks, indicating that this paper could be a significant source of inspiration.\n\n5. \"LUKE: Deep contextualized entity representations with entity-aware self-attention\" [b50]: This paper's method is mentioned as a state-of-the-art NER work that the author aims to achieve comparable performance with, indicating that it is a significant influence.\n\n6. \"Named entity and relation extraction with multimodal retrieval\" [b42]: This paper is mentioned multiple times and the author's method is compared to it, indicating that it is a significant influence.\n\nBased on the provided information, the source papers that inspire the author most are [b20], [b52], [b51], [b5], [b50] and [b42].\n\nThe JSON format of the result would be:\n```json\n{\n  \"b20\": 1,\n  \"b52\": 1,\n  \"b51\": 1,\n  \"b5\": 1,\n  \"b50\": 1,\n  \"b42\": 1\n}\n```\nThe confidence score for all source papers is 1 since the text of the paper directly indicates that these papers significantly influence the author's work.", "Based on the given academic text and the titles of the references, the source papers of the given paper that appear to have the greatest influence on the author's work are:\n\n1. \"a unified mrc framework for named entity recognition\" [b20]: The author clearly states that their work is inspired by the idea of harnessing machine reading comprehension (MRC) to address named entity recognition (NER), which is a central theme of this paper. Confidence score: 0.9.\n\n2. \"hrca advanced multiple choice machine reading comprehension method\" [b52]: The author introduces a powerful human reading strategy, HRCA, into their work, demonstrating a direct application of the method proposed by this paper. Confidence score: 0.8.\n\n3. \"named entity and relation extraction with multi modal retrieval\" [b42]: This paper is mentioned several times throughout the text, particularly in relation to the idea of effectively utilizing additional information to enhance the understanding of input sequences. Confidence score: 0.7.\n\n4. \"zero shot learners for natural language understanding via a unified multiple choice perspective\" [b51]: The author seems to have adopted the design principle of UniMC proposed by this paper to introduce label information as options and to generate appropriate questions for prompting language models. Confidence score: 0.7.\n\n5. \"neural architectures for named entity recognition\" [b14]: This paper appears to have influenced the author's understanding of the BiLSTM architecture for tackling NER tasks. Confidence score: 0.6.\n\n6. \"enhancing nlp tasks via mrc perspective\" (Title inferred from text): This paper seems to provide the author's understanding of treating NLP tasks other than MRC as MRC problems, which is a fundamental part of their approach. Confidence score: 0.6.\n\nThe returned result in the json format with the key as \u201creference number\u201d and the value as \u201cconfidence score\u201d is: {\"b20\": 0.9, \"b52\": 0.8, \"b42\": 0.7, \"b51\": 0.7, \"b14\": 0.6, \"enhancing nlp tasks via mrc perspective\": 0.6}.", "Based on the text and instructions, the following papers appear to be the main inspirations for the paper:\n\n1. \"A unified MRC framework for named entity recognition\" [b20]: The paper mentions using a similar approach to convert the NER problem into an MRC problem.\n\n2. \"HRCA: advanced multiple choice machine reading comprehension method\" [b52]: The paper explicitly states that they used this method as part of their approach.\n\n3. \"Zero-shot learners for natural language understanding via a unified multiple choice perspective\" [b51]: The paper uses a similar methodology of converting NER to a multiple-choice problem, inspired by this paper.\n\n4. \"Named entity and relation extraction with multi modal retrieval\" [b42]: The paper discusses how it avoids the high costs and extra training steps of this method by proposing a new approach.\n\n5. \"LUKE: deep contextualized entity representations with entity-aware self-attention\" [b50]: This paper seems to be one of the SoTA works the authors compare their approach to.\n\n6. \"Biomedical named entity recognition using BERT in the machine reading comprehension framework\" [b40]: The paper mentions adapting this framework to solve the NER problem.\n\n7. \"Event extraction by answering almost natural questions\" [b10]: The paper discusses the use of MRC for NER, which seems to be a core concept in this paper.\n\nThe confidence scores for these references are estimated as follows:\n\n{\"b20\": 0.9, \"b52\": 0.9, \"b51\": 0.85, \"b42\": 0.85, \"b50\": 0.8, \"b40\": 0.7, \"b10\": 0.7}", "Based on the guidelines and the text of the paper, the source papers that inspire the author most are:\n\n1. b20: 'A unified MRC framework for named entity recognition' - The paper directly mentions this work and is highly relevant to the paper's core contributions of integrating MRC strategies into NER tasks.\n2. b51: 'Zero-shot learners for natural language understanding via a unified multiple choice perspective' - This paper inspires the author's approach to data construction and the inclusion of label information as options.\n3. b52: 'HRCA advanced multiple choice machine reading comprehension method' - The author specifically adopts the HRCA strategy from this paper.\n4. b42: 'Named entity and relation extraction with multi-modal retrieval' - This paper is mentioned in the context of the need for external data and the author's aim to achieve comparable performance without relying on additional data.\n5. b50: 'LUKE deep contextualized entity representations with entity aware self attention' - This work is mentioned as a state-of-the-art NER method that relies on external data, which the author aims to avoid.\n6. b3: 'Biomedical named entity recognition via knowledge guidance and question answering' - This paper's method of using artificially designed NER-to-MRC queries is discussed in the context of the authors' own approach to data reconstruction.\n7. b8: 'BERT pre-training of deep bidirectional transformers for language understanding' - BERT-MRC is mentioned as a method that the author's approach uses for data reconstruction.\n\nThe JSON format of the source papers and their confidence scores is:\n```json\n{\n  \"b20\": 1.0,\n  \"b51\": 0.9,\n  \"b52\": 0.9,\n  \"b42\": 0.8,\n  \"b50\": 0.8,\n  \"b3\": 0.7,\n  \"b8\": 0.7\n}\n```"], "63d9d87390e50fcafd57d920": ["{\n  \"b28\": 0.9,\n  \"b34\": 0.85,\n  \"b19\": 0.8,\n  \"b10\": 0.75\n}", "{\n  \"b28\": 0.95,\n  \"b39\": 0.90,\n  \"b27\": 0.85,\n  \"b4\": 0.80\n}", "{\n  \"b28\": 0.85,\n  \"b34\": 0.75,\n  \"b19\": 0.70,\n  \"b10\": 0.65,\n  \"b39\": 0.60\n}", "{\n    \"b28\": 0.95,\n    \"b39\": 0.9\n}", "```json\n{\n    \"b28\": 0.95,\n    \"b39\": 0.90,\n    \"b27\": 0.85,\n    \"b4\": 0.85,\n    \"b34\": 0.75,\n    \"b19\": 0.70,\n    \"b10\": 0.70\n}\n```"], "64659ad1d68f896efa87539f": ["Based on the text of the paper, the author notes, and the titles of the references, the source papers that have inspired the paper most are:\n\n1. \"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents\" [b19]\n2. \"Flamingo: A Visual Language Model for Few-shot Learning\" [b1]\n3. \"PMCVQA: Visual Instruction Tuning for Medical Visual Question Answering\" [None]\n4. \"Large Language Models Encode Clinical Knowledge\" [b31]\n5. \"A Dataset of Clinically Generated Visual Questions and Answers about Radiology Images\" [b17]\n6. \"Towards Visual Dialog for Radiology\" [b15]\n7. \"What Disease Does This Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams\" [b12]\n8. \"Performance of ChatGPT on USMLE: Potential for AI Assisted Medical Education Using Large Language Models\" [b16]\n\nThis is inferred from the explicit mention of these works in the paper and how their methodologies have contributed to the development of the paper's main contributions, such as the MedVInT model, the PMC-VQA dataset, and the exploration of medical visual question answering.\n\nThe JSON format is:\n\n{\n  \"b19\": 0.9,\n  \"b1\": 0.85,\n  \"None\": 0.83,\n  \"b31\": 0.8,\n  \"b17\": 0.75,\n  \"b15\": 0.75,\n  \"b12\": 0.7,\n  \"b16\": 0.7\n}\n\nThe confidence scores are assigned based on the degree of direct relevance and inspiration to the paper's core contributions.", "Based on the text and the titles of the references, the source papers that most likely inspired the paper are:\n\n1. \"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering\" [b0] - This paper introduces the concept of PMC-VQA, which is a significant part of the paper's method. Confidence Score: 0.9\n2. \"Flamingo: A Visual Language Model for Few-shot Learning\" [b1] - This paper introduces a model for visual-language representation learning, something that is explored in the paper. Confidence Score: 0.8\n3. \"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents\" [b19] - This paper introduces PMC-CLIP, a pre-training method using biomedical documents, which is used in the paper's methodology. Confidence Score: 0.9\n4. \"LLaMA: Open and Efficient Foundation Language Models\" [b34] - This paper introduces LLaMA, a language model that is used in the paper's methodology. Confidence Score: 0.8\n5. \"BLIP: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\" [b18] - This paper introduces BLIP, a method that is used in the paper's methodology. Confidence Score: 0.8\n6. \"Medical Visual Question Answering: A Survey\" [b20] - This paper presents an overview of the field of Medical Visual Question Answering, which is the main topic of the paper. Confidence Score: 0.7\n\nReturn results:\n```json\n{\n\"b0\": 0.9,\n\"b1\": 0.8,\n\"b19\": 0.9,\n\"b34\": 0.8,\n\"b18\": 0.8,\n\"b20\": 0.7\n}\n```", "Based on the given text, the source papers that inspired the authors most are:\n\n1. \"flamingo a visual language model for few shot learning\" [b1]: The authors refer to this paper when discussing the limited application of visual-language representation learning in the medical domain, indicating that they took inspiration from its approach.\n\n2. \"pmc clip contrastive language image pre training using biomedical documents\" [b19]: This paper is mentioned multiple times, especially when the authors talk about their proposed models, MedVInT-TE and MedVInT-TD, and the vision encoder they use, which is based on a pre-trained ResNet-50 adopted from PMC-CLIP. This shows that this paper heavily influenced their approach.\n\n3. \"a dataset of clinically generated visual questions and answers about radiology images\" [b17]: This paper is referred to when the authors introduce the existing MedVQA datasets they use for evaluation, indicating that it played a significant role in their research.\n\n4. \"slake a semanticallylabeled knowledge enhanced dataset for medical visual question answering\" [b22]: Similar to [b17], this paper is also mentioned in the context of the datasets the authors use for evaluation, highlighting its influence on their work.\n\n5. \"blip bootstrapping languageimage pre training with frozen image encoders and large language models\" [b18]: This paper is mentioned when the authors discuss the architecture of MedVInT-TD and how they pre-train the whole network using the PMC-OA dataset in a caption-based manner, which is similar to BLIP-2. This indicates that it influenced their methodological approach.\n\n6. \"pmc vqa visual instruction tuning for medical visual question answering\" [None]: This is the paper itself which can't be considered as a source paper.\n\nIn the JSON format, the result is:\n\n{\n\"b1\": 0.85,\n\"b19\": 1,\n\"b17\": 0.85,\n\"b22\": 0.85,\n\"b18\": 0.9\n}", "Based on the text of the paper and the titles of the references, the most pertinent source papers that inspired this work appear to be:\n\n- \"pmc vqa visual instruction tuning for medical visual question answering\". The paper's main method, MedVInT, is directly based on PMC-VQA. This paper could be considered the most significant source of inspiration for the current paper.\n- \"pmc clip contrastive language image pre training using biomedical documents\"[b19]. The method proposed by the author uses the vision encoder from PMC-CLIP, which indicates strong reliance on this previous work. \n- \"flamingo a visual language model for few shot learning\"[b1] and \"blip bootstrapping languageimage pre training with frozen image encoders and large language models\"[b18]. These papers seem to be important as the authors mentioned that their work is inspired by the research in visual-language representation learning, including Flamingo and BLIP.\n- \"a dataset of clinically generated visual questions and answers about radiology images\"[b17] and \"slake a semanticallylabeled knowledge enhanced dataset for medical visual question answering\"[b22]. These papers were used as existing MedVQA datasets for pre-training MedVInT and fine-tuning it. \n\nThe confidence score for each reference is estimated based on how directly each source paper contributed to the main contributions of the current paper:\n\n{ \"b19\": 1, \"b0\": 0.9, \"b1\": 0.8, \"b18\": 0.8, \"b17\": 0.7, \"b22\": 0.7 }\n\nPlease note that these confidence scores are subjective and should be validated with further evidence.", "Based on the text and the titles of the references, the source papers that most likely inspired the authors are:\n- \"PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents\" [b19]: The paper directly mentions PMC-CLIP and its usage of a pre-trained ResNet-50. This reference is directly tied to the methods used in the paper. Confidence Score: 0.9\n- \"Flamingo: A Visual Language Model for Few-shot Learning\" [b1]: The paper mentions Flamingo while discussing visual-language representation learning, showing this work had a direct influence on the methods used. Confidence Score: 0.8\n- \"BLIP: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\" [b18]: The paper refers to BLIP while discussing visual-language representation learning, showing an influence on the paper's methods. Confidence Score: 0.8\n- \"Large Language Models Encode Clinical Knowledge\" [b31]: This paper is mentioned to highlight the success of large language models in medical diagnosis, which is a fundamental concept in the paper. Confidence Score: 0.8\n- \"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering\" [None]: The primary paper itself, the entire paper revolves around the introduced model and the novel methods it introduces. Confidence Score: 1\n- \"The Pile: An 825 GB Dataset of Diverse Text for Language Modeling\" [b9]: This paper was likely influential, as the authors discuss the limitations of existing datasets and the need for larger datasets. Confidence Score: 0.7\n- \"Towards Visual Question Answering on Pathology Images\" [b11]: This paper is mentioned in the context of existing datasets for MedVQA, indicating that it may have influenced the authors' approach. Confidence Score: 0.7\n- \"Scaling Language Modeling with Pathways\" [b7]: As the paper discusses the progress of large language models (LLMs) in natural language processing (NLP) tasks, this paper could have influenced the authors. Confidence Score: 0.7\n\nHere is the result in JSON format:\n{\n\"b19\": 0.9,\n\"b1\": 0.8,\n\"b18\": 0.8,\n\"b31\": 0.8,\n\"None\": 1,\n\"b9\": 0.7,\n\"b11\": 0.7,\n\"b7\": 0.7\n}"], "6427029c90e50fcafd5d6cf3": ["{\n  \"b39\": 0.9,\n  \"b7\": 0.85,\n  \"b35\": 0.85\n}", "{\n    \"b39\": 0.9,\n    \"b7\": 0.8,\n    \"b35\": 0.8\n}", "{\n  \"b39\": 1.0,\n  \"b7\": 0.9,\n  \"b35\": 0.9\n}", "{\n    \"b39\": 0.9,\n    \"b7\": 0.8,\n    \"b35\": 0.8\n}", "{\n  \"b39\": 0.9,\n  \"b7\": 0.85,\n  \"b35\": 0.85\n}"], "634d805590e50fcafd4e05f7": ["Based on the text and the titles of the references, the following papers appear to be the source papers:\n\n1. \"A high-level hardware design language for pipelined processors\" - The paper is introducing a new language (PDL) for designing processor pipelines, which is the main topic of the given paper.\n2. \"Hardware specification with temporal logic: An example\" - Temporal logic is often used in hardware specification, and the paper appears to be discussing specifications in the context of PDL.\n3. \"Kami: A platform for high-level parametric hardware specification and its modular verification\" - The paper is discussing a high-level language for hardware specification, similar to how PDL is a high-level language for designing processor pipelines.\n4. \"Automated pipeline design\" - The paper is discussing automatic pipeline design, which is a feature of PDL.\n5. \"Replacing testing with formal verification in Intel CoreTM i Processor Execution Engine validation\" - The paper is discussing formal verification, which is an important aspect of creating a new hardware design language like PDL.\n6. \"An efficient algorithm for exploiting multiple arithmetic units\" - This paper might have inspired the way PDL handles parallel execution.\n7. \"Bluespec System Verilog: Efficient, correct RTL from high-level specifications\" - This paper also discusses high-level specifications for RTL, which is a relevant topic for PDL.\n\nReturned result in JSON format:\n\n{\n    \"b0\": 0.8,\n    \"b2\": 0.7,\n    \"b4\": 0.9,\n    \"b13\": 0.8,\n    \"b12\": 0.7,\n    \"b26\": 0.7,\n    \"b16\": 0.8\n}", "Based on the provided text, the author's main contribution is the proposal of a new approach called a Pipeline Description Language (PDL) that specifically targets the construction of processor pipelines. The author also discusses the implementation of hazard locks, pipeline threads, refining the hazard lock abstraction, and speculation. \n\nThe following references were identified as potentially relevant source papers:\n\n- 'b4': 'kami a platform for high level parametric hardware specification and its modular verification': This paper could have provided inspiration for the creation of the PDL as it discusses a platform for high-level hardware specification.\n- 'b13': 'automated pipeline design': The automated pipeline design discussed in this paper could be a significant source of inspiration for the creation of PDL, which also targets pipeline structure.\n- 'b17': 'automatic pipelining from transactional datapath specifications': This paper could have influenced the PDL's approach to pipeline structure and automatic pipelining.\n- 'b26': 'an efficient algorithm for exploiting multiple arithmetic units': The efficient algorithm proposed in this paper might have influenced the PDL's approach to pipeline structure and data hazard resolution.\n\nHere is the JSON format of the source papers and their confidence scores:\n\n```\n{\n  \"b4\": 0.8,\n  \"b13\": 0.7,\n  \"b17\": 0.7,\n  \"b26\": 0.6\n}\n```\n\nNote: The confidence scores are subjective and based on the information available in the text. They represent the estimated relevance of the source papers to the main contributions of the paper in question.", "Based on the text and the titles of the references, the following source papers seem to have the most direct influence on the development of the paper's core contributions:\n\n1. \"b14: Rapid generation of high quality RISC-V processors from functional instruction set specifications\" - Confidence: 0.9\n2. \"b17: Automatic pipelining from transactional datapath specifications\" - Confidence: 0.8\n3. \"b1: Chisel constructing hardware in a scala embedded language\" - Confidence: 0.7\n4. \"b25: The verilog hardware description language\" - Confidence: 0.7\n5. \"b26: An efficient algorithm for exploiting multiple arithmetic units\" - Confidence: 0.7\n6. \"b4: Kami a platform for high level parametric hardware specification and its modular verification\" - Confidence: 0.6\n7. \"b12: Replacing testing with formal verification in Intel CoreTM i processor execution engine validation\" - Confidence: 0.6\n\nThe confidence scores are based on the relevance of the source paper titles to the main contributions and challenges outlined in the text of the given paper. The papers that seem to closely relate to the development or use of a Pipeline Description Language (PDL), processor pipelining, hardware specification, and formal verification are given higher confidence scores.", "Based on the content of the paper, the following references seem to be the main sources of inspiration for the author:\n\n{\"reference number\": \"b1\", \"confidence score\": 0.5}\nThis paper is about Chisel, a hardware construction language embedded in Scala. The paper I analyzed also introduces a new language, Pipeline Description Language (PDL), to simplify processor design. \n\n{\"reference number\": \"b4\", \"confidence score\": 0.7}\nThis paper discusses Kami, a platform for high-level hardware specification and its modular verification. The paper I analyzed also discusses the benefits of high-level specification through PDL and mentions its ease of verification.\n\n{\"reference number\": \"b14\", \"confidence score\": 0.6}\nThis paper discusses the generation of RISC-V processors from functional instruction set specifications. The paper I analyzed also discusses the creation of RISC-V cores using PDL. \n\n{\"reference number\": \"b17\", \"confidence score\": 0.8}\nThis paper is about automatic pipelining from transactional datapath specifications. The paper I analyzed discusses PDL's ability to automatically generate pipelines.\n\n{\"reference number\": \"b25\", \"confidence score\": 0.5}\nThis paper is about Verilog, a hardware description language. The paper I analyzed also discusses a new language for hardware description, PDL, and compares it with traditional hardware description languages like RTL.\n\n{\"reference number\": \"b27\", \"confidence score\": 0.7}\nThis paper is about the RISC-V instruction set manual. The paper I analyzed discusses the creation of RISC-V cores using PDL.\n\n{\"reference number\": \"b30\", \"confidence score\": 0.6}\nThis paper is about a hardware design language for timing sensitive information flow security. The paper I analyzed also discusses a new language for hardware description, PDL, that provides an easy-to-understand one-instruction-at-a-time semantics.\n\nThese papers seem to have influenced the author's work on PDL, but further analysis may reveal additional sources of inspiration.", "Based on the information provided in the text, the most relevant source papers are:\n\n1. 'b12': 'replacing testing with formal verification in intel coretm i processor execution engine validation' - This paper is mentioned in the context of discussing the limitations of traditional validation methods for processors, and the need for novel approaches such as PDL. Confidence Score: 0.9\n\n2. 'b17': 'automatic pipelining from transactional datapath specifications' - This paper seems to be mentioned in connection to the discussion of PDL's ability to handle pipeline speculation, suggesting that it may have contributed to the development of this feature. Confidence Score: 0.8\n\n3. 'b25': 'the verilog hardware description language' - This paper is mentioned in the context of discussing the syntax of PDL, suggesting that it may have influenced the design of PDL. Confidence Score: 0.8\n\n4. 'b26': 'an efficient algorithm for exploiting multiple arithmetic units' - This paper is mentioned in the context of discussing the implementation of hazard locks in PDL, suggesting that it may have influenced the design of this feature. Confidence Score: 0.8\n\n5. 'b27': 'the risc v instruction set manual' - This paper is mentioned in the context of discussing the prevention of data hazards in PDL, suggesting that it may have contributed to the development of this feature. Confidence Score: 0.8\n\nThe papers are mentioned in the context of discussing key features and challenges of designing processors and how PDL addresses these, suggesting that they have strongly influenced the development of PDL. The confidence score is high for all papers due to the direct references to them in the context of discussing the design and features of PDL. However, without more information, it's not possible to definitively say which paper was the most influential."], "64a63bbad68f896efaec478f": ["{\n  \"b19\": 0.95,\n  \"b6\": 0.9,\n  \"b26\": 0.85,\n  \"b2\": 0.8,\n  \"b12\": 0.75,\n  \"b10\": 0.7,\n  \"b34\": 0.65,\n  \"b35\": 0.6\n}", "{\n    \"b2\": 0.9,\n    \"b6\": 0.8,\n    \"b19\": 0.8,\n    \"b26\": 0.95\n}", "{\n    \"b6\": 0.95,\n    \"b19\": 0.9,\n    \"b26\": 0.98\n}", "{\n  \"b19\": 0.9,\n  \"b6\": 0.85,\n  \"b12\": 0.8,\n  \"b26\": 0.75,\n  \"b2\": 0.7\n}", "{\n  \"b2\": 0.95,\n  \"b6\": 0.95,\n  \"b19\": 0.90,\n  \"b26\": 0.90\n}"], "63a1751790e50fcafd1f49ce": ["Based on the text and titles of the papers, the source papers that most likely inspired the given paper could be:\n\n1. \"b45\": \"metaicl learning to learn in context\"\n2. \"b50\": \"large dual encoders are generalizable retrievers\"\n3. \"b67\": \"finetuned language models are zero shot learners\"\n4. \"b49\": \"sentence t scalable sentence encoders from pre trained text to text models\"\n5. \"b32\": \"skip thought vectors\"\n6. \"b17\": \"simcse simple contrastive learning of sentence embeddings\"\n\nThese papers appear to have influenced the given paper based on the key phrases, novel methods, and challenges outlined in the text. The selection is based on the given paper's core contributions which include creating a model for embedding text that can adapt to different tasks and domains without needing additional training and the use of instruction-based fine-tuning.\n\nThe corresponding JSON format would look like this:\n{\n\"b45\": 0.9,\n\"b50\": 0.8,\n\"b67\": 0.9,\n\"b49\": 0.8,\n\"b32\": 0.7,\n\"b17\": 0.8\n}", "Based on the given text, the paper presents a model named INSTRUCTOR, which designs a general Embedder to better transfer text embedding representation to new tasks and fields without additional training. The main idea is to adjust text embeddings according to different downstream applications using task and domain descriptions. It employs instruction-based finetuning, departing from prior methods that only consider text input. \n\nThe key references that inspired them most are:\n\n1. \"instruction-based finetuning (Zhong et al., 2021; [b45] [b58] [b67]\"  - these references are mentioned in the context of the main method used in the paper. Thus, these papers likely inspired the work.\n2. \"Following prior work [b50] [b49]\" - this phrase follows the discussion of the architecture of INSTRUCTOR, indicating that these papers had a significant influence on the development of the architecture.\n3. \"We build INSTRUCTOR, based on the single encoder architecture [b25] [b50] [b49]\" - this phrase indicates that these papers provided the basis for the architecture of the proposed model.\n4. \"Following [b50]\" - this phrase appears in the discussion of the training objective of INSTRUCTOR, suggesting that this paper had a major influence on the training process.\n\nSo, the source papers should be [b45], [b58], [b67], [b50], [b49], and [b25]. \n\nHere is the answer in JSON format:\n\n{\n  \"b45\": 0.9,\n  \"b58\": 0.9,\n  \"b67\": 0.9,\n  \"b50\": 1.0,\n  \"b49\": 0.9,\n  \"b25\": 0.9\n}", "Based on the paper text and titles of the references, the most pertinent source papers that inspired the author are:\n\n1. 'b45': \"metaicl learning to learn in context\" - This paper seems to be the basis for the author's idea of instruction-based finetuning, a key concept in the INSTRUCTOR model. The text mentions instruction-based finetuning directly after referencing this paper. Confidence score: 0.9\n\n2. 'b50': \"large dual encoders are generalizable retrievers\" - This paper is referred to in the section explaining the architecture of INSTRUCTOR, indicating it was influential in the model's design. Confidence score: 0.8\n\n3. 'b67': \"finetuned language models are zero shot learners\" - This paper is referenced in the section about the core approach of the paper, suggesting it played a role in developing the INSTRUCTOR model. Confidence score: 0.7\n\n4. 'b19': \"don t stop pretraining adapt language models to domains and tasks\" - This paper is referenced when the authors talk about the common method to address the issue of degraded performance of existing embeddings when applied to new tasks or domains. Confidence score: 0.7\n\n5. 'b17': \"simcse simple contrastive learning of sentence embeddings\" - This paper is referred to when the authors discuss the problems with existing embeddings, such as SimCSE. This suggests it was a key reference for understanding the challenges in the field. Confidence score: 0.6\n\n6. 'b63': \"beir a heterogeneous benchmark for zero shot evaluation of information retrieval models\" - This paper is referred to when the authors discuss the performance degradation of existing embeddings on new tasks or domains, indicating it helped identify the problem that INSTRUCTOR is attempting to solve. Confidence score: 0.6\n\nPlease note that these are inferences based on the information provided and may not fully capture the author's inspirations.\n\nThe result in json format is:\n\n{\n\"b45\": 0.9,\n\"b50\": 0.8,\n\"b67\": 0.7,\n\"b19\": 0.7,\n\"b17\": 0.6,\n\"b63\": 0.6\n}", "Based on the provided text and the titles of the references, the source papers that might have inspired the current paper are:\n\n1. \"metaicl learning to learn in context\" [b45]\n2. \"multitask prompted training enables zero shot task generalization\" [b58]\n3. \"finetuned language models are zero shot learners\" [b67]\n4. \"don t stop pretraining adapt language models to domains and tasks\" [b19]\n5. \"large dual encoders are generalizable retrievers\" [b50]\n6. \"sentence t scalable sentence encoders from pre trained text to text models\" [b49]\n7. \"leveraging passage retrieval with generative models for open domain question answering\" [b25]\n\nThe confidence scores for each source paper are estimated based on the relevance of their titles to the challenges and methods outlined in the given paper text.\n\nHere is the result in JSON format:\n\n```\n{\n  \"b45\": 0.9,\n  \"b58\": 0.85,\n  \"b67\": 0.95,\n  \"b19\": 0.8,\n  \"b50\": 0.85,\n  \"b49\": 0.8,\n  \"b25\": 0.75\n}\n```\n\nPlease note that these scores are subjective and might change with a different interpretation of the provided text.", "Based on the text analysis, the following references seem to be the most pertinent source papers:\n\n1. \"large dual encoders are generalizable retrievers\" [b50]\n2. \"sentence t scalable sentence encoders from pre trained text to text models\" [b49]\n3. \"finetuned language models are zero shot learners\" [b67]\n4. \"metaicl learning to learn in context\" [b45]\n5. \"instruction-based finetuning\" [Zhong et al., 2021]\n6. \"don't stop pretraining: adapt language models to domains and tasks\" [b19]\n\nThe confidence score for each paper may vary, but it's reasonable to assume they all have a high confidence score (>0.8) due to the direct relevance to the author's work. \n\nThe result in json format would be:\n{\"b50\": 0.9, \"b49\": 0.9, \"b67\": 0.9, \"b45\": 0.9, \"Zhong et al., 2021\": 0.9, \"b19\": 0.9}"], "64337e3190e50fcafd76ef32": ["Based on the academic paper text and the titles of the references, the most pertinent source papers that inspire the given paper are:\n\n1. \"Language Models are Few-Shot Learners\" (b16): This paper is directly mentioned in the context of the powerful prompting capabilities of large language models, which are essential for the architecture proposed in the given paper.\n\n2. \"Social Simulacra: Creating Populated Prototypes for Social Computing Systems\" (b77): This paper is referred to in the context of using a large language model to generate users that would populate new social computing systems, which directly aligns with the concept of generative agents in the given paper.\n\n3. \"Human-Level AI's Killer Application: Interactive Computer Games\" (b57): This paper is mentioned multiple times, discussing the role of AI in games and the vision of creating believable agents, which is a central theme of the given paper.\n\n4. \"The Role of Emotion in Believable Agents\" (b8): The given paper discusses the creation of believable agents, which aligns with the theme of this source paper.\n\n5. \"Interactive Narrative: A Novel Application of Artificial Intelligence for Computer Games\" (b82): This paper is linked to the given paper's focus on crafting interactive artificial societies and agents inspired by games.\n\n6. \"The SOAR Cognitive Architecture\" (b59): This paper is referenced when discussing cognitive architectures that supported some of the earliest examples of believable agents, which is relevant to the given paper's introduction of a new architecture for generative agents.\n\n7. \"Generative Agents: Interactive Simulacra of Human Behavior\" (None): This might be the underlying paper or a highly relevant one as it shares the main theme - generative agents simulating human behavior.\n\nBased on these analysis, the list of source papers in json format would be:\n\n{\"b16\": 0.9, \"b77\": 0.9, \"b57\": 0.8, \"b8\": 0.8, \"b82\": 0.8, \"b59\": 0.7, \"None\": 0.7}", "{\"b76\": 0.9, \"b16\": 0.8, \"b37\": 0.7, \"b44\": 0.8, \"b35\": 0.6, \"b46\": 0.5, \"b51\": 0.6, \"b103\": 0.5}", "Based on the text and the author's notes, the source papers that most inspired this work are:\n\n1. \"Language Models are Few-Shot Learners\" [b16]: This paper is referenced multiple times within the text and is a clear methodological inspiration for the paper's use of large language models to simulate human behavior. Confidence Score: 0.9\n2. \"The Role of Emotion in Believable Agents\" [b8]: It is referred to multiple times and seems to be a foundational reference in designing believable artificial agents, which is the core contribution of the paper. Confidence Score: 0.8\n3. \"Human-Level AI's Killer Application: Interactive Computer Games\" [b57]: This paper is mentioned several times in the text, and given the paper's focus on gaming environments, it likely provided significant inspiration. Confidence Score: 0.8\n4. \"Social Simulacra: Creating Populated Prototypes for Social Computing Systems\" [b77]: This paper is referenced as a recent work demonstrating the efficacy of large language models for generating believable behavior in simulated environments, a key aspect of the present paper's methodology. Confidence Score: 0.7\n5. \"Power to the People: The Role of Humans in Interactive Machine Learning\" [b2]: This work is cited as a key part of the literature on human-AI interaction, which is a fundamental concept in the present paper. Confidence Score: 0.6\n6. \"Believable Agents for First-Person Shooter Games\" [b23]: This paper likely inspired the use of believable agents in a game-like scenario. Confidence Score: 0.6\n7. \"Interactive Narrative: A Novel Application of Artificial Intelligence for Computer Games\" [b82]: The paper's focus on creating interactive narratives in gaming contexts aligns closely with the goals and methods of the present paper. Confidence Score: 0.6\n8. \"Training Language Models to Follow Instructions with Human Feedback\" [b76]: This paper likely influenced the use of human feedback in training the generative agents in the present work. Confidence Score: 0.5\n\nThe JSON formatted result would be:\n\n{\n    \"b16\": 0.9,\n    \"b8\": 0.8,\n    \"b57\": 0.8,\n    \"b77\": 0.7,\n    \"b2\": 0.6,\n    \"b23\": 0.6,\n    \"b82\": 0.6,\n    \"b76\": 0.5\n}", "Based on the given text and the instructions, the source papers that most inspired this paper appear to be:\n\n1. \"Language models are few-shot learners\" (b16): The authors mention that despite progress in large language models, there is a need for an architecture that can manage constantly growing memories. This indicates that the work in this paper was a significant influence.\n\n2. \"The role of emotion in believable agents\" (b8): The paper discusses the creation of computational agents that can act consistently with past experiences, which aligns closely with the concept of believable agents discussed in this paper.\n\n3. \"Human level AI's killer application: Interactive computer games\" (b57): The authors discuss the application of their generative agents in an interactive society inspired by games, indicating influence from this paper.\n\n4. \"Game AI revisited\" (b107): This paper could have influenced the presented work, as it indicates the complexities of human behavior in the context of game AI, which the authors also address.\n\n5. \"Social Simulacra: Creating populated prototypes for social computing systems\" (b77): It's mentioned that large language models can become a key ingredient for creating believable agents, and previous work has demonstrated this approach, such as in the Social Simulacra.\n\n6. \"Interactive narrative: A novel application of artificial intelligence for computer games\" (b82): It's possible that this paper influenced the presented work, as the authors discuss the creation of their generative agents in the context of interactive narratives.\n\nThe JSON output is:\n\n{\n    \"b16\": 0.9,\n    \"b8\": 0.8,\n    \"b57\": 0.8,\n    \"b107\": 0.7,\n    \"b77\": 0.7,\n    \"b82\": 0.7\n}", "Based on the provided text and author notes, the source papers that mainly inspire the given paper can be identified by locating the references that are directly linked to the main challenges and methods presented in the paper. From the text, it is clear that the paper is about creating interactive artificial societies that reflect believable human behavior using a novel agent architecture, which leverages the power of large language models. Therefore, the source papers are most likely to be those that have contributed to the development of large language models, agent architecture, and interactive artificial societies. \n\nThe following references appear to be the most relevant:\n\n1. \"Human level ai s killer application interactive computer games\" [b57]: This paper provides a strong foundation for the application of AI in interactive games, which is a direct inspiration for the use of AI in creating interactive artificial societies in the given paper.\n\n2. \"Language models are few shot learners\" [b16]: This paper discusses the development and capabilities of large language models, which are a key component in the architecture proposed in the given paper.\n\n3. \"The soar cognitive architecture\" [b59]: This paper introduces a cognitive architecture that seems to have inspired the agent architecture proposed in the given paper.\n\n4. \"Social simulacra creating populated prototypes for social computing systems\" [b77]: This paper discusses the creation of populated prototypes for social systems, which aligns with the creation of interactive artificial societies in the given paper.\n\n5. \"Training language models to follow instructions with human feedback\" [b76]: This paper discusses training methods for language models, which may have influenced the training approaches used in the given paper.\n\n6. \"Believable and effective ai agents in virtual worlds current state and future perspectives\" [b95]: This paper reviews the current state and future perspectives of AI agents in virtual worlds, which is highly relevant to the given paper's topic.\n\n7. \"Interactive narrative a novel application of artificial intelligence for computer games\" [b82]: This paper discusses the use of AI in interactive narratives for computer games, which is related to the use of AI in creating interactive artificial societies in the given paper.\n\nThe confidence scores for these references, indicating the level of influence they had on the given paper, would need to be determined through a more detailed analysis of the given paper and the source papers. However, based on the information provided, a possible distribution of confidence scores could be as follows:\n\n{\"b57\": 0.9, \"b16\": 0.85, \"b59\": 0.7, \"b77\": 0.8, \"b76\": 0.75, \"b95\": 0.8, \"b82\": 0.7}"], "63bfce0790e50fcafd0a10d0": ["{\n    \"b31\": 0.9,\n    \"b34\": 0.8,\n    \"b125\": 0.7\n}", "{\n  \"b31\": 0.95,\n  \"b34\": 0.90,\n  \"b27\": 0.85,\n  \"b125\": 0.80\n}", "{\n  \"b31\": 1.0,\n  \"b34\": 0.9,\n  \"b27\": 0.9,\n  \"b125\": 0.8,\n  \"b105\": 0.7,\n  \"b109\": 0.7\n}", "{\n  \"b31\": 0.9,\n  \"b125\": 0.8,\n  \"b27\": 0.8,\n  \"b34\": 0.8,\n  \"b74\": 0.8,\n  \"b105\": 0.7,\n  \"b109\": 0.7\n}", "{\n    \"b31\": 0.95,\n    \"b34\": 0.95,\n    \"b125\": 0.90\n}"], "637aec2590e50fcafd92962b": ["{\n\"b4\": 0.9,\n\"b5\": 0.8,\n\"b7\": 0.75,\n\"b10\": 0.85,\n\"b15\": 1,\n\"b17\": 0.9\n}", "Based on the text and information provided, the author's main challenge is improving the utilization of the AXI bus for irregular data access patterns in vector processors. The author's novel contribution is the AXI-PACK, an extension to Arm's AXI4 protocol, which enables packed strided and indirect memory streams, as well as the extension of an open-source RISC-V vector processor to efficiently handle these irregular access patterns.\n\nThe most pertinent source papers that directly influence the author's work, based on the use of key language and direct relevance to the paper's novel contributions, would be:\n\n1. \"prodigy improving the memory latency of data indirect irregular workloads using hardware software co design\" [b4]\n2. \"stream based memory access specialization for general purpose processors\" [b5]\n3. \"impulse building a smarter memory controller\" [b10]\n4. \"amba axi and ace protocol specification\" [b15]\n5. \"an open source platform for high performance noncoherent on chip communication\" [b16]\n6. \"a new ara for vector computing an open source highly efficient risc v v vector processor design\" [b17]\n\nThese papers are relevant as they discuss topics such as improving memory latency for irregular workloads, memory controller design, and the AXI protocol, which are central to the author's paper. The paper [b17] is particularly relevant as it deals with the extension of an open-source RISC-V vector processor, which is a primary contribution of the author's paper.\n\nThe returned result in the json format would be:\n\n{\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b10\": 0.8,\n    \"b15\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 1.0\n}\n\nHere, the confidence score indicates the level of direct relevance and influence that each source paper has on the author's work, with a score of 1 indicating the highest level of relevance.", "Based on the text of the academic paper, the following references are identified as the source papers that inspired the author, with a confidence score:\n\n{ \n\"b4\": 0.9,\n\"b5\": 0.9,\n\"b7\": 0.8,\n\"b8\": 0.8,\n\"b9\": 0.8,\n\"b10\": 0.85,\n\"b13\": 0.85,\n\"b14\": 0.85,\n\"b17\": 0.9\n}\n\nThe author refers to these papers directly when discussing the challenges of irregular data access patterns and the solutions. They mention the use of stream abstractions, accelerating strided and indirect streams, and memory-side extensions as some of the methods used in these papers to tackle the issue. The author also refers to these papers when discussing the novel contributions of their own work, indicating that these papers have a direct influence on the development of their research.", "Based on the given text, the paper proposes a novel method, AXI-PACK, to improve the utilization of the AXI bus for strided and indirect accesses in vector processors, which is a solution to the challenge of low utilization of AXI bus for strided and indirect accesses in existing vector processors. The source papers that the paper might be inspired by are:\n\n1. 'prodigy improving the memory latency of data indirect irregular workloads using hardware software co design' [b4]: This paper talks about improving irregular workload performance, which is a challenge that the given paper also addresses. The given paper also uses a hardware-software co-design approach, similar to this source paper.\n\n2. 'stream based memory access specialization for general purpose processors' [b5]: This paper discusses stream abstractions for describing sequences of irregular accesses. The given paper also addresses the issue of stream abstractions, especially for strided and indirect streams, which are most common in practice.\n\n3. 'impulse building a smarter memory controller' [b10]: This paper proposes solutions that reduce access times and prevent narrow bus accesses. The given paper's proposed solution, AXI-PACK, also aims to reduce access times and prevent narrow bus accesses.\n\n4. 'an open source platform for high performance noncoherent on chip communication' [b16]: This paper discusses on-chip interconnect protocols, which is also a topic that the given paper addresses.\n\n5. 'amba axi and ace protocol specification' [b15]: This paper provides the specification for the AXI protocol, which is extended by the given paper's proposed solution, AXI-PACK.\n\n6. 'a new ara for vector computing an open source highly efficient risc v v vector processor design' [b17]: This paper discusses the vector processor, Ara, which the given paper extends to leverage AXI-PACK for efficient irregular memory accesses.\n\nThe confidence score for each source paper can be estimated as:\n\n{\n\"b4\": 0.8,\n\"b5\": 0.8,\n\"b10\": 0.8,\n\"b16\": 0.8,\n\"b15\": 0.9,\n\"b17\": 0.9\n}", "Based on the provided text of the paper and its references, the source papers that have most inspired this paper are:\n\n\"b4\": \"prodigy improving the memory latency of data indirect irregular workloads using hardware software co design\" - This paper is mentioned several times within the text, suggesting that it has provided significant inspiration for the current research. The authors discuss the limitations of the Prodigy system and how their own research aims to address these.\n\n\"b5\": \"stream based memory access specialization for general purpose processors\" - This paper is also mentioned multiple times and seems to be a key source of inspiration for the authors' algorithm. \n\n\"b7\": \"stream semantic registers a lightweight risc v isa extension achieving full compute utilization in single issue cores\" and \"b8\": \"indirection stream semantic register architecture for efficient sparse dense linear algebra\" - These papers are mentioned in connection with improving functional unit utilization and enabling significant speedups, which are key goals of the current research.\n\n\"b10\": \"impulse building a smarter memory controller\" - This paper is mentioned as part of the discussion on memory-side extensions, which is a significant part of the authors' proposed solution to the challenge they have identified.\n\n\"b15\": \"amba axi and ace protocol specification\" - This paper is the source of the AXI4 protocol, which the authors have extended in their research.\n\n\"b17\": \"a new ara for vector computing an open source highly efficient risc v v vector processor design\" - This paper is the source of the Ara vector processor, which the authors have modified in their research.\n\nThe confidence scores can be inferred by the frequency of references and the relevance of these references to the main contributions of the paper. Therefore, the final result may look like this:\n\n{\"b4\": 0.9, \"b5\": 0.8, \"b7\": 0.7, \"b8\": 0.7, \"b10\": 0.8, \"b15\": 1.0, \"b17\": 1.0}"], "64b60eaf3fda6d7f06eaf557": ["{\n    \"b4\": 0.9,\n    \"b10\": 0.8,\n    \"b12\": 0.8\n}", "{\n  \"b4\": 0.9,\n  \"b10\": 0.8,\n  \"b12\": 0.8\n}", "```json\n{\n    \"b4\": 0.95,\n    \"b10\": 0.85,\n    \"b12\": 0.85\n}\n```", "{\n  \"b4\": 0.9,\n  \"b10\": 0.8,\n  \"b12\": 0.8\n}", "{\n  \"b4\": 0.95,\n  \"b10\": 0.85,\n  \"b12\": 0.85\n}"], "64a407dcd68f896efaf1ba64": ["Based on the given paper text, the source papers that have most inspired the authors are:\n\n1. \"The gem5 Simulator\" [b2]\n2. \"FireSim: FPGA-Accelerated Cycle-Exact Scale-Out System Simulation in the Public Cloud\" [b16]\n3. \"A Survey of Computer Architecture Simulation Techniques and Tools\" [b1]\n4. \"ZSim: Fast and Accurate Microarchitectural Simulation of Thousand-Core Systems\" [b5]\n5. \"A Top-Down Method for Performance Analysis and Counters Architecture\" [b29]\n6. \"Reading M Performance Counters\" [b31]\n7. \"Chipyard: Integrated Design Simulation and Implementation Framework for Custom SoCs\" [b37]\n\nThese papers are selected based on their direct relevance to the main challenges and methods outlined in the paper, indicated by the key phrases used in the text. Please note that these are inferred based on the provided information and actual influence might differ. \n\nIn JSON format:\n\n{\n    \"b2\": 0.9,\n    \"b16\": 0.8,\n    \"b1\": 0.7,\n    \"b5\": 0.7,\n    \"b29\": 0.7,\n    \"b31\": 0.7,\n    \"b37\": 0.7\n} \n\nThe confidence score is assigned based on the relevance and frequency of the source paper in the given text. The score is a subjective interpretation based on the provided text.", "Based on the text, the source papers that inspired the given paper are likely:\n\n1. \"The gem5 simulator\" [b2]: The paper relies heavily on the gem5 simulator for its research, indicating that it was a significant source of inspiration.\n2. \"Firesim: FPGA-accelerated cycle exact scale out system simulation in the public cloud\" [b16]: The paper mentions using Firesim for some of its analysis, suggesting that it played a role in shaping the methodology.\n3. \"A survey of computer architecture simulation techniques and tools\" [b1]: This paper might have provided foundational knowledge for the authors.\n\nThe exact confidence scores would require a more detailed analysis of the text, but based on the provided information, we can make an educated guess:\n\n```\n{\n    \"b2\": 0.9,\n    \"b16\": 0.7,\n    \"b1\": 0.6\n}\n```\n\nPlease note that this is a rough estimate and the actual confidence scores may vary based on a more thorough analysis of the paper.", "Based on the text provided, the source papers that the author indicates inspire them most can be inferred as follows:\n\n- \"The gem simulator\" [b2]: The paper heavily references the gem5 simulator and its use in their research. \n- \"The future of architectural simulation\" [b20]: The paper discusses the challenges and future directions in computer architecture simulation, which aligns with the topics of the paper.\n- \"Firesim fpgaaccelerated cycle exact scale out system simulation in the public cloud\" [b16]: The authors use FireSim for part of their research, indicating the influence of this paper.\n- \"Transparent hugepage support\" [b40]: The authors propose using huge pages to improve simulations and reference this paper, indicating its influence.\n- \"Intel optimizations for dynamic language runtimes\" [b41]: The paper mentions the use of an open-source library by Intel to improve the execution of their simulations, suggesting this paper's influence. \n\nBased on the above analysis, the source papers are identified as:\n\n{\n\"b2\": 0.9,\n\"b20\": 0.8,\n\"b16\": 0.7,\n\"b40\": 0.7,\n\"b41\": 0.6\n}\n", "Based on the text provided, the paper discusses the use of the gem5 architectural simulator for hardware design and its performance on different platforms, including the Intel Xeon and Apple M1 Chips. The paper also discusses profiling the execution of gem5 on these platforms, performing a detailed architectural analysis of the simulator's execution, and proposing ways to improve its simulation speed. Below are the identified sources that seem to be the most relevant to the paper's primary challenges and methods:\n\n1. \"the gem simulator\" [b2]: This is the main tool used in this paper to perform hardware design simulations.\n2. \"firesim fpgaaccelerated cycle exact scale out system simulation in the public cloud\" [b16]: This tool is used to investigate the sensitivity of gem5's speed to some architectural parameters.\n3. \"a top down method for performance analysis and counters architecture\" [b29]: This method is used to perform a top-down microarchitectural performance analysis of gem5.\n4. \"intel vtune profiler\" [b28]: This tool is used to access the processor performance counters and perform the top-down microarchitectural analysis.\n5. \"transparent hugepage support\" [b40]: This system-level optimization is used to improve the simulation execution.\n6. \"intel optimizations for dynamic language runtimes\" [b41]: This library is used to remap a subset of gem5's code to 2MB huge pages.\n\nThe JSON result should be: \n\n{\"b2\": 0.9, \"b16\": 0.8, \"b29\": 0.8, \"b28\": 0.8, \"b40\": 0.7, \"b41\": 0.7}", "Based on the provided academic text and the criteria outlined, the following references are identified as the source papers that most inspired the given paper:\n\n1. \"The gem simulator\" [b2]\n2. \"zsim fast and accurate microarchitectural simulation of thousand core systems\" [b5]\n3. \"firesim fpgaaccelerated cycle exact scale out system simulation in the public cloud\" [b16]\n4. \"the gem simulator version\" [b19]\n5. \"a top down method for performance analysis and counters architecture\" [b29]\n6. \"chipyard integrated design simulation and implementation framework for custom socs\" [b37]\n7. \"transparent hugepage support\" [b40]\n\nJSON Format:\n\n{\n\"b2\": 0.9,\n\"b5\": 0.8,\n\"b16\": 0.8,\n\"b19\": 0.95,\n\"b29\": 0.85,\n\"b37\": 0.7,\n\"b40\": 0.75\n}\n\nThese papers were selected because they are directly related to the main topic of the paper. The paper heavily discusses and uses the concepts and methods suggested in these papers, such as the gem5 simulator, zsim, firesim, top-down performance analysis, and hugepage support. Each confidence score is a subjective measure of how directly each reference contributed to the paper's content."], "6423ac7790e50fcafd55eaa0": ["{\n    \"b27\": 0.85,\n    \"b4\": 0.85,\n    \"b28\": 0.85,\n    \"b67\": 0.8,\n    \"b38\": 0.8,\n    \"b36\": 0.8,\n    \"b14\": 0.75\n}", "{\n  \"b27\": 0.9,\n  \"b67\": 0.9,\n  \"b28\": 0.8,\n  \"b46\": 0.8\n}", "{\n  \"b27\": 0.9,\n  \"b14\": 0.9,\n  \"b43\": 0.85,\n  \"b67\": 0.8,\n  \"b4\": 0.75,\n  \"b28\": 0.75,\n  \"b46\": 0.7\n}", "{\n    \"b27\": 0.9,\n    \"b4\": 0.9,\n    \"b28\": 0.9,\n    \"b38\": 0.9,\n    \"b36\": 0.9,\n    \"b67\": 0.8\n}", "{\n    \"b14\": 0.9,\n    \"b27\": 0.95,\n    \"b4\": 0.9,\n    \"b28\": 0.95,\n    \"b67\": 0.9,\n    \"b38\": 0.9,\n    \"b36\": 0.9,\n    \"b43\": 0.85\n}"], "6459ac63d68f896efa6588b4": ["{\n    \"b32\": 0.95,\n    \"b5\": 0.85,\n    \"b17\": 0.75,\n    \"b41\": 0.75,\n    \"b2\": 0.7,\n    \"b25\": 0.65,\n    \"b4\": 0.6\n}", "{\n  \"b32\": 0.9,\n  \"b5\": 0.9,\n  \"b17\": 0.8,\n  \"b2\": 0.75,\n  \"b41\": 0.7\n}", "{\n    \"b32\": 0.95,\n    \"b5\": 0.90,\n    \"b17\": 0.85,\n    \"b2\": 0.80,\n    \"b41\": 0.75,\n    \"b4\": 0.70,\n    \"b14\": 0.65\n}", "{\n  \"b32\": 0.9,\n  \"b5\": 0.8,\n  \"b17\": 0.7,\n  \"b2\": 0.7,\n  \"b41\": 0.6\n}", "{\n  \"b32\": 0.95,\n  \"b5\": 0.95,\n  \"b17\": 0.9,\n  \"b2\": 0.85,\n  \"b41\": 0.8,\n  \"b15\": 0.8\n}"], "64741a3ad68f896efaa62202": ["{\"b9\": 0.9, \"b3\": 0.7, \"b19\": 0.7, \"b49\": 0.7, \"b2\": 0.6, \"b30\": 0.6, \"b38\": 0.6, \"b46\": 0.5}", "Based on the text of the academic paper and author notes, the source papers that inspire this paper the most are:\n\n1. \"Dissecting Supervised Contrastive Learning\" [b9]\n2. \"Intriguing Properties of Contrastive Losses\" [b5]\n3. \"A Theoretical Analysis of Contrastive Unsupervised Representation Learning\" [b2]\n4. \"The Power of Contrast for Feature Learning: A Theoretical Analysis\" [b20]\n5. \"Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss\" [b15]\n6. \"A Theoretical Study of Inductive Biases in Contrastive Learning\" [b14]\n7. \"Perfectly Balanced: Improving Transfer and Robustness of Supervised Contrastive Learning\" [b3]\n8. \"Can Contrastive Learning Avoid Shortcut Solutions?\" [b38]\n\nThese papers are selected because they are heavily referred to when discussing the key topics of class collapse and feature suppression in contrastive learning, and the development of the author's unified framework for understanding these phenomena.\n\nHere is the result in json format:\n\n{\n\"b9\": 1,\n\"b5\": 0.9,\n\"b2\": 0.9,\n\"b20\": 0.8,\n\"b15\": 0.8,\n\"b14\": 0.7,\n\"b3\": 0.7,\n\"b38\": 0.7\n}\n\nThe confidence scores are assigned based on the frequency and context of the references in the text. The scores are relative and indicate the relative relevance and influence of each source paper to this paper. The higher the score, the more influential the source paper is to the given paper.", "Based on the text and titles of the references, the source papers that most likely inspired this paper are:\n\n1. \"A theoretical analysis of contrastive unsupervised representation learning\" (b2) - The paper frequently cites this reference when discussing the theoretical aspects of contrastive learning. \n\n2. \"Dissecting supervised contrastive learning\" (b9) - This reference is referred to multiple times, especially when discussing the concept of class collapse in supervised contrastive learning.\n\n3. \"Addressing feature suppression in unsupervised visual representations\" (b27) - This paper seems to be a key reference for understanding feature suppression in unsupervised contrastive learning, a crucial aspect of the paper's analysis.\n\n4. \"Understand contrastive representation learning through alignment and uniformity on the hypersphere\" (b48) - This paper is cited in relation to the theory of contrastive learning, suggesting it played a role in shaping the authors' understanding of the topic.\n\n5. \"Gradient descent on two layer nets margin maximization and simplicity bias\" (b30) - The paper's discussion of the simplicity bias of gradient descent suggests this paper was influential.\n\n6. \"Can contrastive learning avoid shortcut solutions?\" (b38) - This paper is cited when discussing the limitations of contrastive learning, suggesting it may have inspired the paper's investigation of failure modes in contrastive learning.\n\n7. \"Perfectly balanced improving transfer and robustness of supervised contrastive learning\" (b3) - This paper is cited when discussing the benefits of combining supervised and unsupervised contrastive loss, suggesting it contributed to the paper's overall approach.\n\nHere is the result in the JSON format:\n\n{\n\"b2\": 0.9,\n\"b9\": 0.85,\n\"b27\": 0.8,\n\"b48\": 0.7,\n\"b30\": 0.75,\n\"b38\": 0.7,\n\"b3\": 0.65\n}", "Based on the given text, the main challenges outlined in the paper are two failure modes in contrastive learning - class collapse (in supervised contrastive learning) and feature suppression (in unsupervised contrastive learning). The paper proposes a unified framework for both supervised and unsupervised contrastive learning to answer which semantically relevant features are learned and uses this framework to characterize class collapse and feature suppression.\n\nHere are the source papers that are either directly mentioned with key phrases indicating inspiration or are directly linked to these main challenges and methods:\n\n- \"b9\": 0.7 (This paper is repeatedly mentioned, especially in relation to the problem of class collapse.)\n- \"b15\": 0.8 (The paper refers to this work in the context of the spectral contrastive loss.)\n- \"b2\": 0.7 (This paper is mentioned in the context of the theoretical understanding of contrastive learning.)\n- \"b48\": 0.6 (This paper is mentioned in the context of the theoretical understanding of contrastive learning.)\n- \"b3\": 0.9 (This paper is mentioned in the context of combining supervised and unsupervised contrastive losses.)\n- \"b19\": 0.7 (This paper is also mentioned in the context of combining supervised and unsupervised contrastive losses.)\n- \"b30\": 0.6 (This paper is highlighted as it describes the bias of SGD towards finding simpler solutions, which is the main deriving factor in collapsing class representations.)\n\nPlease note that this is a preliminary analysis and the results might be slightly different with a more in-depth reading of the paper.", "Based on the text and the methods proposed by the author, the papers that influenced the given paper most are:\n\n1. \"Dissecting supervised contrastive learning\" [b9]: This paper was mentioned multiple times throughout the text, indicating that it has a significant influence on the paper. It's also mentioned in context of the author's novel contributions, specifically in regard to their exploration of class collapse in supervised contrastive learning.\n\n2. \"Intriguing properties of contrastive losses\" [b5]: This paper was referred to when discussing feature suppression, one of the main challenges addressed in the paper. It's likely that this paper provided foundational knowledge on the topic.\n\n3. \"A theoretical analysis of contrastive unsupervised representation learning\" [b2]: This paper is referenced in the context of the existing CL theory, which the author builds upon in their research.\n\n4. \"Implicit bias of gradient descent on linear convolutional networks\" [b12]: This paper seems to be a key source for understanding the role of simplicity bias in class collapse and feature suppression, which is a central theme of the paper.\n\n5. \"Understanding contrastive representation learning through alignment and uniformity on the hypersphere\" [b48]: This paper is part of the theoretical foundation for the author's work, as it is mentioned in the context of the existing CL theory.\n\n6. \"Perfectly balanced improving transfer and robustness of supervised contrastive learning\" [b3]: This paper was mentioned when discussing combining supervised and unsupervised CL, which is a significant part of the author's proposed solution.\n\n7. \"Can contrastive learning avoid shortcut solutions\" [b38]: This paper is referred to when discussing the characterization of feature suppression, indicating its relevance to the paper's methodological contributions.\n\nIn conclusion, the source papers are:\n\n{\n\"b9\": 1,\n\"b5\": 0.9,\n\"b2\": 0.85,\n\"b12\": 0.85,\n\"b48\": 0.8,\n\"b3\": 0.75,\n\"b38\": 0.75\n}\n\nThese confidence scores are estimated based on the frequency and context of the references in the text."], "64a29621d68f896efa28fd54": ["{\n  \"b6\": 0.8,\n  \"b33\": 0.8,\n  \"b38\": 0.8,\n  \"b42\": 0.8,\n  \"b55\": 0.8\n}", "{\n    \"b32\": 0.95,\n    \"b33\": 0.95,\n    \"b38\": 0.95,\n    \"b42\": 0.95,\n    \"b55\": 0.95,\n    \"b51\": 0.9,\n    \"b40\": 0.9,\n    \"b68\": 0.9\n}", "{\n  \"b6\": 0.9,\n  \"b33\": 0.9,\n  \"b38\": 0.9,\n  \"b42\": 0.9,\n  \"b55\": 0.9\n}", "{\n    \"b6\": 0.9,\n    \"b33\": 0.9,\n    \"b38\": 0.9,\n    \"b42\": 0.9,\n    \"b55\": 0.9\n}", "{\n    \"b32\": 0.95,\n    \"b33\": 0.95,\n    \"b38\": 0.95,\n    \"b42\": 0.95,\n    \"b55\": 0.95,\n    \"b51\": 0.9,\n    \"b40\": 0.9,\n    \"b68\": 0.9\n}"], "64a29621d68f896efa28fd67": ["Based on the information provided, the academic paper appears to be heavily influenced by the following source papers:\n\n1. b31: 'why there are complementary learning systems in the hippocampus and neocortex insights from the successes and failures of connectionist models of learning and memory' - The paper takes inspiration from the cognitive theory of Complementary Learning Systems (CLS), which models the human brain as an online learning system. \n   \n2. b9: 'pythia a customizable hardware prefetching framework using online reinforcement learning' - This paper presents issues with the current state of online learning for prefetching, which this paper seeks to address.\n\n3. b16: 'learning memory access patterns' - The given paper seems to be proposing a novel approach to learning memory access patterns, likely inspired by this paper.\n\n4. b39: 'a hierarchical neural model of data prefetching' - This paper is mentioned in relation to the use of deep learning for prefetching, suggesting it likely influenced the current paper's approach to the same problem.\n\n5. b26: 'mind in network memory management for disaggregated data centers' - This paper is mentioned in regards to the implementation challenges of their proposed solution, suggesting it was influential in shaping the paper's direction.\n\n6. b2: '' - This paper is also mentioned in the context of the implementation, suggesting it was influential to the paper's development.\n\nThe JSON format of the answer would be:\n\n{\n \"b31\": 0.9,\n \"b9\": 0.85,\n \"b16\": 0.8,\n \"b39\": 0.7,\n \"b26\": 0.75,\n \"b2\": 0.7\n}", "Based on the text, the source papers that have most inspired the author of this paper could be:\n\n1. 'b31': 'why there are complementary learning systems in the hippocampus and neocortex insights from the successes and failures of connectionist models of learning and memory' - This paper seems to be the main inspiration for the author's approach, as it is directly referenced in the context of the novel solution proposed by the author (CLS theory, interleaved replay, Hebbian networks). Confidence score: 1.0.\n\n2. 'b9': 'pythia a customizable hardware prefetching framework using online reinforcement learning' - The author refers to the problem of learning memory access patterns, which is addressed in this paper. Confidence score: 0.8.\n\n3. 'b16': 'learning memory access patterns' -  This paper is also referenced in the context of the problem that the author is addressing, suggesting that it has been a significant influence. Confidence score: 0.8.\n\n4. 'b26': 'mind in network memory management for disaggregated data centers' - This paper is mentioned in the context of the implementation challenges that the author anticipates facing, which suggests that it might be an important influence. Confidence score: 0.75.\n\n5. 'b39': 'a hierarchical neural model of data prefetching' - The author refers to this paper when discussing the limitations of current DL techniques for prefetching, indicating that it has been a significant influence on the development of their own approach. Confidence score: 0.75.\n\n6. 'b2': '' - This paper is mentioned in the context of the target systems for online prefetching, which suggests that it has been a significant influence. Confidence score: 0.7.\n\n7. 'b43': 'hw sw co design for reliable in memory brain inspired hyperdimensional computing' - This paper is suggested due to its relevance to the author's bio-inspired approach. Confidence score: 0.65.\n\nBased on the author's notes and titles of each paper, the source papers are directly related to the author's novel contributions. The confidence score is estimated based on the relevance of each source paper to the author's approach and the frequency of their citations in the text. \n\nThe result in json format would be:\n\n{\n    \"b31\": 1.0,\n    \"b9\": 0.8,\n    \"b16\": 0.8,\n    \"b26\": 0.75,\n    \"b39\": 0.75,\n    \"b2\": 0.7,\n    \"b43\": 0.65\n}", "Based on the provided text and title information, the source papers that most influenced the given paper are:\n\n{\n    \"b9\": 0.9,\n    \"b16\": 0.9,\n    \"b39\": 0.9,\n    \"b31\": 0.9,\n    \"b11\": 0.8,\n    \"b14\": 0.8,\n    \"b43\": 0.8,\n    \"b48\": 0.8\n}\n\nThese papers are selected based on the criteria mentioned in the task. The author has directly mentioned these papers in the context of their work, indicating a strong reliance on previous research for developing the paper's core contributions. The papers are also directly relevant to the novel contributions of the paper. For instance, the papers b9, b16, and b39 are related to the learning memory access patterns, which is a major contribution of the given paper. The papers b31, b11, b14, b43, and b48 are related to the cognitive theory of Complementary Learning Systems (CLS) and brain-inspired Hebbian networks, which the author took inspiration from.", "Based on the text of the paper and the titles of the references, the most pertinent source papers which inspired the author are:\n\n1. \"prefetching using principles of hippocampal neocortical interaction\" - The paper discusses the use of principles found in the human brain to develop a robust solution for learning memory access patterns online without interference. The author specifically refers to the cognitive theory of Complementary Learning Systems (CLS), which models the human brain as an online learning system.\n\n2. \"learning memory access patterns\" - This paper is also a crucial reference as it is directly related to the main challenge that the author is trying to address in the paper, which is to accurately learn memory access patterns online without interference.\n\n3. \"revisiting fundamentals of experience replay\" - The author mentions the use of interleaved replay, a process where the learning of new and old information is interleaved to avoid catastrophic interference. This concept is likely to be inspired by this source paper.\n\n4. \"why there are complementary learning systems in the hippocampus and neocortex insights from the successes and failures of connectionist models of learning and memory\" - The author takes inspiration from the CLS theory, which is explained in this paper. The theory posits that the brain avoids catastrophic interference using interleaved replay.\n\n5. \"overcoming catastrophic forgetting by incremental moment matching\" - This paper is relevant as it discusses methods to overcome the problem of catastrophic interference, which is one of the main challenges that the paper is trying to tackle.\n\n6. \"deep learning based data prefetching in cpu gpu unified virtual memory\" - This paper is directly relevant as it discusses the use of deep learning for prefetching, which is the main method proposed by the author in his paper.\n\n7. \"contrastive hebbian learning with random feedback weights\" - This paper is relevant as the author discusses the use of bio-inspired Hebbian networks, which use far less resources than DNNs, to build efficient, online prefetchers. This concept is likely inspired by this source paper.\n\nReference and confidence score in JSON format:\n\n{\n    \"prefetching using principles of hippocampal neocortical interaction\": 1,\n    \"learning memory access patterns\": 0.9,\n    \"revisiting fundamentals of experience replay\": 0.8,\n    \"why there are complementary learning systems in the hippocampus and neocortex insights from the successes and failures of connectionist models of learning and memory\": 0.9,\n    \"overcoming catastrophic forgetting by incremental moment matching\": 0.8,\n    \"deep learning based data prefetching in cpu gpu unified virtual memory\": 0.9,\n    \"contrastive hebbian learning with random feedback weights\": 0.8\n}", "Based on the analysis of the paper text, the primary challenges outlined in the paper are about the problems of online learning of memory access patterns with traditional deep neural networks (DNNs), such as high computational and storage resources, the inability to adapt to changing system dynamics, and the issue of catastrophic interference. The author proposed a solution inspired by the human brain's Complementary Learning Systems (CLS) theory, which uses interleaved replay and bio-inspired Hebbian networks to overcome the limitations of DNNs.\n\nFollowing the guidelines, the source papers that appear to be most directly involved in the development of the author's core contributions include:\n\n- \"b16: Learning memory access patterns\" - This paper is likely a key inspiration for the author's work as it discusses learning memory access patterns, which is a main challenge outlined in the author's paper. Confidence score: 0.9\n- \"b9: Pythia a customizable hardware prefetching framework using online reinforcement learning\" - This paper is likely another significant source, as it discusses online learning, a concept strongly related to the author's work. Confidence score: 0.8\n- \"b31: Why there are complementary learning systems in the hippocampus and neocortex insights from the successes and failures of connectionist models of learning and memory\" - This paper directly relates to the author's novel contributions about the CLS theory and its application to overcome the limitations of traditional DNNs. Confidence score: 1.0\n- \"b22: Measuring catastrophic forgetting in neural networks\" - This paper discusses catastrophic interference, a problem that the author's paper aims to solve. Confidence score: 0.8\n- \"b13: Contrastive hebbian learning with random feedback weights\" - This paper is about Hebbian learning, a method adopted by the author in their solution. Confidence score: 0.9\n\nTherefore, the final result in json format would be:\n\n{\"b16\": 0.9, \"b9\": 0.8, \"b31\": 1.0, \"b22\": 0.8, \"b13\": 0.9}"], "64be5e653fda6d7f063a95ce": ["Based on the text of the academic paper and the titles of the references, the source papers that most inspired the given paper could be identified as follows:\n\n- b0: 'deep learning with differential privacy'\n- b17: 'differential privacyenabled federated learning for sensitive health data'\n- b24: 'the algorithmic foundations of differential privacy'\n- b38: 'personalized federated learning with differential privacy'\n- b50: 'private federated learning without a trusted server optimal algorithms for convex losses'\n- b56: 'differentially private federated learning on heterogeneous data'\n- b76: 'byzantinerobust distributed learning towards optimal statistical rates'\n- b80: 'bridging differential privacy and byzantine robustness via model aggregation'\n\nIn json format, the result may look like this:\n\n{\n\"b0\":0.8,\n\"b17\":0.7,\n\"b24\":0.9,\n\"b38\":0.7,\n\"b50\":0.85,\n\"b56\":0.75,\n\"b76\":0.8,\n\"b80\":0.9\n}\n\nThis list is limited to 8 references that appear to have a strong influence on the paper. The confidence scores are hypothetical and indicate the degree of relevance of each source paper to the given paper, with 1 being the most relevant.", "Based on the given text and the titles of the references, the most relevant source papers that inspired the author are:\n\n1. \"Differential privacy and robust statistics\" [b23]\n2. \"Differential privacy and robust statistics in high dimensions\" [b49]\n3. \"Private federated learning without a trusted server optimal algorithms for convex losses\" [b50]\n4. \"Byzantine machine learning made easy by resilient averaging of momentums\" [b27]\n5. \"Machine learning with adversaries byzantine tolerant gradient descent\" [b11]\n6. \"Robustness implies privacy in statistical estimation\" [b37]\n7. \"Privacy induces robustness information computation gaps and sparse mean estimation\" [b31]\n\nThese papers seem to be the most directly related to the paper's topic based on the key challenges and novel methods mentioned in the paper. \n\nThe result in json format:\n{\n  \"b23\": 0.9,\n  \"b49\": 0.9,\n  \"b50\": 0.9,\n  \"b27\": 0.85,\n  \"b11\": 0.85,\n  \"b37\": 0.8,\n  \"b31\": 0.8\n}", "Based on the given academic paper text and the author notes, the source papers which have directly inspired the presented work are likely to be:\n\n1. \"Differential privacy and robust statistics\" [b23]: This paper is likely a key source as it directly relates to the challenges outlined by the author: the need for robustness and differential privacy in distributed machine learning. This paper discusses the intersection of these two concepts.\n\n2. \"Being robust in high dimensions can be practical\" [b20]: This paper could have inspired the author's approach to handling high-dimensionality in the context of robustness.\n\n3. \"Robust learning information theory and algorithms\" [b66]: This paper might have contributed to the development of the robust learning methods proposed in the given paper.\n\n4. \"Differential privacy and Byzantine resilience in SGD: do they add up?\" [b32]: This paper discusses the combination of differential privacy and Byzantine resilience in stochastic gradient descent, which is relevant to the given paper's focus on distributed machine learning.\n\n5. \"Private and polynomial time algorithms for learning gaussians and beyond\" [b7]: This paper might have provided inspiration for the author's novel methods for ensuring privacy in distributed machine learning.\n\n6. \"Machine learning with adversaries: Byzantine tolerant gradient descent\" [b11]: The author's work on robustness against adversarial attacks in distributed machine learning might have been inspired by this paper.\n\n7. \"Robust and differentially private mean estimation\" [b48]: This paper might have been a key source for the author's work on data privacy and robustness in distributed learning.\n\nThe JSON format of the source papers is: \n\n{\n\"b23\": 0.9,\n\"b20\": 0.85,\n\"b66\": 0.85,\n\"b32\": 0.8,\n\"b7\": 0.75,\n\"b11\": 0.75,\n\"b48\": 0.7\n}", "Based on the text provided, the source papers that inspired this work can be identified by looking at the references that are directly linked to the main challenges and methods outlined in the paper. The main challenges of this paper are ensuring robustness and privacy in distributed machine learning (ML) systems, and the main method proposed to tackle these challenges is a new distributed ML algorithm that ensures both privacy and robustness. \n\nThe paper is inspired by the following references:\n\n1. \"Deep learning with differential privacy\" (b0): This paper seems to be a key reference about privacy in ML, which is one of the main challenges addressed in the analyzed paper.\n2. \"Machine learning with adversaries: Byzantine tolerant gradient descent\" (b11): This paper appears to be a primary source of inspiration for the robustness part of the challenge, as it introduces a Byzantine tolerant gradient descent algorithm.\n3. \"Differential privacy-enabled federated learning for sensitive health data\" (b17): This paper is probably a source of inspiration as it addresses privacy issues in federated learning, which is a form of distributed ML.\n4. \"Byzantine resilient highdimensional SGD with local iterations on heterogeneous data\" (b18): This paper appears to be a direct inspiration for the development of the new distributed ML algorithm, as it discusses Byzantine resilience in distributed learning.\n5. \"Private federated learning without a trusted server: optimal algorithms for convex losses\" (b50): This paper seems to have inspired the method of ensuring privacy in the proposed distributed ML algorithm.\n\nBased on this analysis, the source papers that most inspired this paper are b0, b11, b17, b18, and b50.\n\nThe confidence scores for each reference are as follows (assuming higher scores for references that are directly linked to the paper's novel contributions):\n\n- b0: 0.8\n- b11: 0.9\n- b17: 0.8\n- b18: 1.0\n- b50: 1.0\n\nSo in JSON format, the result would be:\n\n{\n    \"b0\": 0.8,\n    \"b11\": 0.9,\n    \"b17\": 0.8,\n    \"b18\": 1.0,\n    \"b50\": 1.0\n}", "Based on the text, the following references appear to be the most relevant:\n\n1. \"on the privacy robustness utility trilemma in distributed learning\" (No author given): The paper's title suggests that it could be a source paper. The paper explores the interplay between privacy, robustness, and utility in distributed learning, which is the main topic of the given paper.\n\n2. \"deep learning with differential privacy\" (b0): The given paper discusses the importance of ensuring privacy in distributed machine learning. This paper could have provided insights into how to balance privacy and robustness.\n\n3. \"machine learning with adversaries byzantine tolerant gradient descent\" (b11): This paper likely provided important insights into how to make the distributed ML algorithm robust against adversarial workers.\n\n4. \"differential privacy and robust statistics\" (b23): This paper likely provided valuable insights into how to achieve differential privacy and robustness in distributed ML.\n\n5. \"byzantine machine learning made easy by resilient averaging of momentums\" (b27): The given paper mentions the use of momentum in the algorithm, which could have been influenced by this paper.\n\n6. \"differential privacy and byzantine resilience in sgd do they add up\" (b32): This paper discusses the integration of privacy and robustness, which is a primary challenge outlined in the paper.\n\n7. \"private federated learning without a trusted server optimal algorithms for convex losses\" (b50): This paper likely influenced the development of the SAFE-DSHB algorithm mentioned in the given paper.\n\n8. \"bridging differential privacy and byzantine robustness via model aggregation\" (b80): The given paper discusses model aggregation, which could have been influenced by this paper.\n\nThe confidence scores for these papers would need to be determined based on a closer inspection of the given paper and the potential source papers. For example, you might look for explicit mentions of these papers in the text, similarities between the methodologies used, or whether the given paper cites these potential source papers in key sections. \n\nThe result in JSON format could look like this:\n\n{\n\"None\": 0.9,\n\"b0\": 0.8,\n\"b11\": 0.8,\n\"b23\": 0.7,\n\"b27\": 0.7,\n\"b32\": 0.9,\n\"b50\": 0.9,\n\"b80\": 0.8\n}"], "64ba03413fda6d7f062732f5": ["Based on the given text and titles of references, the source papers that inspired the given paper are likely to be:\n\n1. \"Auto-Encoding Variational Bayes\" [b31] - The paper proposes a principled framework for continuous RL policy as a multimodal density function, which is inspired by the success of generative models (such as the one in this paper) in modeling complex distributions. Confidence Score: 0.8\n\n2. \"Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\" [b32] - The paper mentions hierarchical methods and the use of latent space for policy parameterization, which could be inspired by this paper. Confidence Score: 0.7\n\n3. \"Maximum Entropy Reinforcement Learning\" [b70] - The paper adopts the variational method for optimal trajectory modeling, which is a key concept in maximum entropy reinforcement learning. Confidence Score: 0.7\n\n4. \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\" [b18] - The paper uses a technique that is similar to the Soft Actor-Critic method, which suggests that it might have been inspired by this paper. Confidence Score: 0.7\n\n5. \"Variational Option Discovery Algorithms\" [b0] - The paper proposes a novel variational bound as the optimization objective, which is a concept related to variational methods used in skill discovery methods. Confidence Score: 0.6\n\n6. \"Decision Transformer: Reinforcement Learning via Sequence Modeling\" [b7] - The paper adopts a sequence modeling perspective for policy, which is a concept used in the Decision Transformer method. Confidence Score: 0.6\n\n7. \"Provably Efficient Maximum Entropy Exploration\" [b22] - The paper introduces a novel density estimator to facilitate exploration in the environment, which is a concept related to maximum entropy exploration. Confidence Score: 0.6\n\nNote: The confidence score is a subjective measure based on the information given in the text. It might not accurately represent the actual influence of each source paper on the given paper. \n\nBelow is the result in JSON format:\n\n{\n    \"b31\": 0.8, \n    \"b32\": 0.7, \n    \"b70\": 0.7, \n    \"b18\": 0.7, \n    \"b0\": 0.6, \n    \"b7\": 0.6, \n    \"b22\": 0.6\n}", "Based on the provided paper text and titles of the references, the source papers that most likely inspired the given paper are:\n\n1. \"Auto Encoding Variational Bayes\" [b31]\n   The paper directly mentions drawing inspiration from generative models such as the one proposed in this work for policy parameterization.\n\n2. \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\" [b18]\n   This work is referenced multiple times in the context of using variational methods and the maximum entropy framework to model the posterior of optimal trajectories.\n\n3. \"Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review\" [b34]\n   This paper is referenced in relation to the perspective of treating reinforcement learning as probabilistic inference, which is a key aspect of the given paper's approach.\n\n4. \"Self-consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings\" [b8]\n   This paper's method is compared and contrasted with the given paper's approach in the context of skill discovery and optimization.\n\n5. \"Variational Option Discovery Algorithms\" [b0]\n   This paper's method is also compared with the given paper's approach, especially in the context of skill discovery.\n\n6. \"Decision Transformer: Reinforcement Learning via Sequence Modeling\" [b7]\n   This work is mentioned in the context of adopting a sequence modeling perspective for policy formulation, which directly informs the given paper's approach.\n\n7. \"Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\" [b32]\n   This paper is referenced in the context of hierarchical methods, which seems to inform the given paper's approach to policy parameterization.\n\n8. \"Diverse Skill Learning\" [b33]\n   This paper is referenced when discussing diverse skill learning and their approach to optimization, which likely influenced the given paper's method.\n\nThe confidence score for each of these references would be 1, given their direct and significant relevance to the given paper's novel contributions. \n\nThe potential JSON output is:\n{\n\"b31\": 1,\n\"b18\": 1,\n\"b34\": 1,\n\"b8\": 1,\n\"b0\": 1,\n\"b7\": 1,\n\"b32\": 1,\n\"b33\": 1\n}", "Based on the text and titles of the references, the source papers for this academic paper are likely to be:\n\n1. \"Auto-encoding variational Bayes\" (b31) - The text mentions that the paper introduces a novel variational constraint as an optimization objective, and variational inference is a key method introduced in the paper by Kingma and Welling. Confidence Score: 0.9\n\n2. \"Reinforcement learning and control as probabilistic inference tutorial and review\" (b34) - The paper's method is closely related to the \"RL as inference,\" which is a major topic in this paper by Levine. Confidence Score: 0.85\n\n3. \"Soft actor-critic off policy maximum entropy deep reinforcement learning with a stochastic actor\" (b18) - The paper's method RPG incorporates maximum entropy reinforcement learning, which is the focus of this paper by Haarnoja et al. Confidence Score: 0.8\n\n4. \"Hierarchical deep reinforcement learning integrating temporal abstraction and intrinsic motivation\" (b32) - The paper introduces a method that combines a learned world model and a novel density estimator for exploration, which is a theme shared with the hierarchical RL method in this paper by Kulkarni et al. Confidence Score: 0.75\n\n5. \"Decision transformer reinforcement learning via sequence modeling\" (b7) - The paper takes a sequence modeling perspective to view the policy as a density function over the entire trajectory space, which is a key idea introduced in this paper by Chen et al. Confidence Score: 0.7\n\n6. \"Variational option discovery algorithms\" (b0) - The paper mentions the connection of its method with skill discovery methods and the use of latent variables, which are topics covered in this paper by Eysenbach et al. Confidence Score: 0.65\n\n7. \"Unpaired image to image translation using cycle consistent adversarial networks\" (b80) - The paper uses a generative model for parameterizing the multimodal policies, and this paper by Zhu et al. is a seminal work on generative models. Confidence Score: 0.6\n\nIn JSON format:\n\n{\n  \"b31\" : 0.9,\n  \"b34\" : 0.85,\n  \"b18\" : 0.8,\n  \"b32\" : 0.75,\n  \"b7\" : 0.7,\n  \"b0\" : 0.65,\n  \"b80\" : 0.6\n}", "Based on the text and the reference titles provided, the source papers that most inspire the given paper are likely:\n\n- \"Auto-encoding variational bayes\" [b31]\n- \"Variational option discovery algorithms\" [b0]\n- \"Hierarchical deep reinforcement learning integrating temporal abstraction and intrinsic motivation\" [b32]\n- \"The option critic architecture\" [b2]\n- \"Soft actor critic off policy maximum entropy deep reinforcement learning with a stochastic actor\" [b18]\n- \"Unpaired image to image translation using cycle consistent adversarial networks\" [b80]\n- \"Generative adversarial networks\" [b14]\n- \"Reinforcement learning and control as probabilistic inference tutorial and review\" [b34]\n\nThe confidence scores for these references are estimated as:\n- b31: 0.9\n- b0: 0.9\n- b32: 0.8\n- b2: 0.8\n- b18: 0.75\n- b80: 0.75\n- b14: 0.7\n- b34: 0.7\n\nThe json format is:\n{\"b31\": 0.9, \"b0\": 0.9, \"b32\": 0.8, \"b2\": 0.8, \"b18\": 0.75, \"b80\": 0.75, \"b14\": 0.7, \"b34\": 0.7} \n\nPlease note that this analysis is based on the provided text and reference titles. The actual confidence scores may vary depending on the full content of the source papers and the given paper.", "Based on the provided text and titles of the references, the source papers that inspired this work most likely include:\n\n1. \"Variational Option Discovery Algorithms\" [b0]: The text indicates a relationship with skill discovery methods, where the paper's method is an extension of skill discovery methods under additional assumptions of rewards.\n\n2. \"The Option Critic Architecture\" [b2]: This paper is cited in the context of policy parameterization and exploration problems.\n\n3. \"Auto-Encoding Variational Bayes\" [b31]: The paper cites this work as inspiration for their use of the variational method and its application to the modeling of posterior of optimal trajectories.\n\n4. \"Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review\" [b34]: This paper is referred to in the context of the \"RL as inference\" framework and the use of the variational method.\n\n5. \"Reparameterized Policy Learning for Multimodal Trajectory Optimization\" [None]: This is the title of the paper itself and the proposed method (RPG) is a major contribution of this paper.\n\n6. \"One Solution Is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL\" [b33]: This paper is mentioned in the context of skill discovery methods and the learning of a diverse set of solutions.\n\n7. \"Hindsight Experience Replay\" [b1]: This paper is mentioned in the context of hierarchical methods and the use of goals for policy parameterization.\n\nThe JSON format answer is:\n{\n\"b0\": 0.9,\n\"b2\": 0.7,\n\"b31\": 1,\n\"b34\": 0.8,\n\"None\": 1,\n\"b33\": 0.7,\n\"b1\": 0.6\n}"], "6426ed4490e50fcafd443eef": ["Based on the text and the titles of the references, the source papers that most likely inspired the given paper are:\n\n1. \"a dram maker s accelerator in memory aim architecture for machine learning\" [b25]: This paper is mentioned multiple times throughout the text, and is also mentioned in relation to the development of the algorithm proposed in the given paper. \n\n2. \"deep residual learning for image recognition\" [b24]: This paper is mentioned in relation to the convolutional layers used in the algorithm of the given paper.\n\n3. \"efficientnet rethinking model scaling for convolutional neural networks\" [b56]: This paper is mentioned in relation to the use of convolutional layers in the given paper.\n\n4. \"pimflow compiler and runtime support for cnn models on processing in memory dram\" [b1]: This paper is mentioned several times and seems to be directly related to the main contributions of the given paper.\n\n5. \"transparent offloading and mapping tom enabling programmer transparent near data processing in gpu systems\" [b26]: This paper is mentioned in the given paper and is related to the processing in memory (PIM) technology discussed in the paper.\n\n6. \"dram manufacturer's constraint-aware design for commercially viable DRAM-PIM\" [b37]: This paper is mentioned multiple times in the text as a key motivation for the development of the algorithm proposed in the given paper.\n\nSo, the answer in the json format is:\n{ \"b25\": 1, \"b24\": 0.9, \"b56\": 0.9, \"b1\": 1, \"b26\": 0.8, \"b37\": 1 }", "Based on the provided academic paper text and the titles of the references, the source papers which have most influenced this paper appear to be:\n\n{ \"b25\": 1.0, \"b36\": 0.8, \"b37\": 0.8, \"b26\": 0.6, \"b57\": 0.6, \"b47\": 0.6, \"b32\": 0.6, \"b2\": 0.5, \"b14\": 0.5, \"b21\": 0.5}\n\nHere, the confidence score represents an estimate of how directly the referenced paper contributed to the methods and results of the given paper, based on the frequency and context in which they are cited. The paper seems to be primarily based on the work described in b25, b36, and b37, with significant contributions from b26, b47, b57, and b32, and some influence from b2, b14, and b21.", "Based on the text and titles of the references, the source papers that most likely inspired the given paper are:\n\n1. 'a dram maker s accelerator in memory aim architecture for machine learning' [b25]\n2. 'transparent offloading and mapping tom enabling programmer transparent near data processing in gpu systems' [b26]\n3. 'toward standardized near data processing with unrestricted data placement for gpus' [b32]\n4. 'pimflow compiler and runtime support for cnn models on processing in memory dram' [b1]\n5. 'performance analysis of multi level parallelism inter node intra node and hardware accelerators' [b20]\n6. 'cairo a compiler assisted technique for enabling instruction level offloading of processing in memory' [b21]\n7. 'pimprof an automated program profiler for processing in memory offloading decisions' [b57]\n\nThe JSON output:\n\n{ \"b25\": 0.9, \"b26\": 0.85, \"b32\": 0.8, \"b1\": 0.95, \"b20\": 0.75, \"b21\": 0.75, \"b57\": 0.7 }", "Based on the text and the titles of the references, the source papers that seem to have inspired the author the most are:\n\n1. 'A DRAM maker's accelerator in memory (AIM) architecture for machine learning' - [b25]\n2. 'Hardware architecture and software stack for PIM based on commercial DRAM technology' - [b36]\n3. 'PIMFlow: Compiler and Runtime Support for CNN Models on Processing-In-Memory DRAM' - [b1]\n4. 'Transparent Offloading and Mapping (TOM): Enabling Programmer-Transparent Near-Data Processing in GPU Systems' - [b26]\n5. 'A 1Ynm 8Gb 16Gb/s/pin GDDR6 based Accelerator-In-Memory (AIM) supporting 1.2TFLOPS MAC operation and various activation functions for deep learning applications' - [b37]\n\nHere is the returned result in JSON format:\n\n{\n\"b25\": 0.9,\n\"b36\": 0.8,\n\"b1\": 0.85,\n\"b26\": 0.75,\n\"b37\": 0.7\n} \n\nThese papers are related to the Processing-In-Memory (PIM) and deep learning acceleration, which are the main topics of the text. The confidence scores are assigned based on the frequency and context of the citations in the text.", "Based on the analysis of the given academic paper, the following references seem to be the source papers that most inspired the authors:\n\n1. \"A DRAM Maker's Accelerator-In-Memory (AIM) Architecture for Machine Learning\" [b25]\n2. \"Hardware Architecture and Software Stack for PIM Based on Commercial DRAM Technology: Industrial Product\" [b36]\n3. \"A ynm V GB GB/s Pin GDDR-Based Accelerator In-Memory Supporting TFLOPS MAC Operation and Various Activation Functions for Deep Learning Applications\" [b37]\n4. \"Transparent Offloading and Mapping (TOM): Enabling Programmer-Transparent Near-Data Processing in GPU Systems\" [b26]\n5. \"Scheduling Techniques for GPU Architectures with Processing-In-Memory Capabilities\" [b47]\n6. \"PIMFlow: Compiler and Runtime Support for CNN Models on Processing-In-Memory DRAM\" [b1]\n7. \"Deep Residual Learning for Image Recognition\" [b24]\n\nThe confidence scores are as follows:\n\n{ \n    \"b25\": 0.9,\n    \"b36\": 0.9,\n    \"b37\": 0.9,\n    \"b26\": 0.8,\n    \"b47\": 0.8,\n    \"b1\": 0.9,\n    \"b24\": 0.7\n}"], "638eb2ef90e50fcafd58b2af": ["{\n    \"b17\": 0.9,\n    \"b10\": 0.8,\n    \"b11\": 0.85,\n    \"b15\": 0.75,\n    \"b31\": 0.9,\n    \"b37\": 0.9\n}", "{\n  \"b31\": 0.95,\n  \"b10\": 0.85,\n  \"b11\": 0.85,\n  \"b17\": 0.9,\n  \"b37\": 0.95\n}", "{\n    \"b10\": 0.95,\n    \"b11\": 0.95,\n    \"b17\": 0.95,\n    \"b31\": 0.95\n}", "{\n  \"b31\": 0.95,\n  \"b10\": 0.90,\n  \"b11\": 0.90,\n  \"b17\": 0.85\n}", "{\n    \"b31\": 0.95,\n    \"b17\": 0.9,\n    \"b11\": 0.85,\n    \"b10\": 0.8,\n    \"b15\": 0.75\n}"], "63c8b59590e50fcafd90b721": ["Based on the text of the paper and the titles of the references, the source papers that most inspire the authors of this paper are:\n\n- \"b18\": \"polarized 3D high quality depth sensing with polarization cues\"\n- \"b22\": \"shape from polarization for complex scenes in the wild\"\n- \"b51\": \"deep shape from polarization\"\n- \"b32\": \"learning monocular dense depth from events\"\n- \"b17\": \"learning monocular dense depth from events\"\n- \"b45\": \"height from polarisation with unknown lighting or albedo\"\n- \"b52\": \"unsupervised event based optical flow using motion compensation\"\n\nThese references are directly related to the paper's novel contributions and appear to be the primary sources of inspiration for the paper's methodology and approach.\n\nThe json format of the result is:\n{\n\"b18\": 0.9,\n\"b22\": 0.9,\n\"b51\": 0.95,\n\"b32\": 0.85,\n\"b17\": 0.85,\n\"b45\": 0.8,\n\"b52\": 0.85\n}", "Based on the text of the paper and the list of references, the source papers that most likely inspired the authors are:\n\n1. 'b1': 'high sensitivity analysis of polarization by surface reflection machine vision and applications'\n2. 'b2': 'multi view surface reconstruction using polarization'\n3. 'b12': 'a x back illuminated stacked temporal contrast event based vision sensor with m pixels geps readout programmable event-rate controller and compressive data formatting pipeline'\n4. 'b18': 'polarized 3d high quality depth sensing with polarization cues'\n5. 'b22': 'shape from polarization for complex scenes in the wild'\n6. 'b31': 'esl event based structure light'\n7. 'b51': 'deep shape from polarization'\n\nThe confidence scores for these references are:\n\n{\n\"b1\": 0.9,\n\"b2\": 0.8,\n\"b12\": 0.95,\n\"b18\": 0.9,\n\"b22\": 0.95,\n\"b31\": 0.85,\n\"b51\": 0.95\n}\n\nThese papers are selected as they are directly related to the main challenges and methods outlined in the paper: the use of event cameras for shape recovery, the use of polarization information for 3D reconstruction, and the use of learning-based approaches for improving accuracy. The high confidence scores reflect the significant reliance on these papers for the development of the paper's core contributions.", "Based on the given academic paper text and the titles of the references, the most pertinent source papers that inspire this paper are:\n\n1. b1: 'high sensitivity analysis of polarization by surface reflection machine vision and applications' - The authors have mentioned the use of polarization information in their algorithm, which might be inspired by this source paper.\n\n2. b18: 'polarized 3D high quality depth sensing with polarization cues' - This paper might have influenced the authors in using polarization for depth sensing.\n\n3. b22: 'shape from polarization for complex scenes in the wild' - This paper's approach to shape from polarization in complex scenes could have influenced the authors' methodology.\n\n4. b51: 'deep shape from polarization' - The authors' use of a learning-based framework to predict surface normals using events to solve the issue of low fill-rate might be inspired by this paper.\n\n5. b14: 'division of focal plane asynchronous polarization imager' - The authors have mentioned the use of a rotating polarizing filter in front of the event camera, which could be inspired by this paper.\n\nHence, the returned result in the JSON format is:\n{\n\"b1\": 0.8,\n\"b18\": 0.8,\n\"b22\": 0.9,\n\"b51\": 1.0,\n\"b14\": 0.7\n}", "Based on the text given, the source papers that inspired the author most directly are:\n\n1. \"High Sensitivity Analysis of Polarization by Surface Reflection Machine Vision and Applications\" ['b1']: The author refers to the use of changes in polarization information to infer geometric properties of an object, which is discussed in this paper. \n\n2. \"Polarization-Based Inverse Rendering from a Single View\" ['b2']: The author mentions the use of multiple view points for polarization information, which is a method explored in this paper.\n\n3. \"A DB S Latency Asynchronous Temporal Contrast Vision Sensor\" ['b24']: The author discusses event cameras and their efficiency, which is the topic of this paper.\n\n4. \"ESL: Event-Based Structure Light\" ['b31']: The author refers to the use of event cameras in conjunction with a polarizer to estimate surface normals. This approach is discussed in this paper.\n\n5. \"Deep Shape from Polarization\" ['b51']: This paper seems to be a significant influence, with the author referring to data-driven priors and their shortcomings. \n\n6. \"Unsupervised event-based optical flow using motion compensation\" ['b52']: This paper is referred to when discussing the voxel grid representation used in the learning-based approach.\n\nSo, the references are in this format: {'b1': 0.9, 'b2': 0.9, 'b24': 0.8, 'b31': 0.9, 'b51': 1, 'b52': 0.8}.", "Based on the text and the titles of the references, the main source papers of the given paper seem to be:\n\n1. 'b18': 'polarized 3d high quality depth sensing with polarization cues': This paper deals with similar topics, specifically depth sensing using polarization, which is a key concept in the given paper.\n2. 'b22': 'shape from polarization for complex scenes in the wild': This paper seems to be highly relevant given that it discusses the estimation of shape from polarization, a main contribution of the given paper.\n3. 'b51': 'deep shape from polarization': The given paper uses a learning-based approach for shape detection from polarization, which is also the focus of this source paper.\n4. 'b1': 'high sensitivity analysis of polarization by surface reflection machine vision and applications': The given paper cites this work when discussing the principles of polarization, which are central to the proposed algorithm.\n5. 'b32': 'diederik paul moeys and davide scaramuzza event guided depth sensing': This is likely a key source as it discusses the use of events for depth sensing, which is a core component of the given paper's methodology.\n6. 'b52': 'unsupervised event based optical flow using motion compensation': The given paper discusses event-based methods, and this paper could be a significant source for that.\n\nHere is the result in JSON format:\n\n{\n\"b18\": 0.9,\n\"b22\": 0.9,\n\"b51\": 0.9,\n\"b1\": 0.7,\n\"b32\": 0.8,\n\"b52\": 0.7\n}"], "64bb03bb3fda6d7f06002e9f": ["{\n  \"b4\": 0.8,\n  \"b5\": 0.8,\n  \"b6\": 0.8,\n  \"b7\": 0.8,\n  \"b8\": 0.9,\n  \"b9\": 0.9\n}", "```json\n{\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b9\": 0.9\n}\n```", "{\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b9\": 0.9\n}", "{\n    \"b4\": 0.9,\n    \"b5\": 0.85,\n    \"b6\": 0.85,\n    \"b7\": 0.85,\n    \"b8\": 0.9,\n    \"b9\": 0.9\n}", "{\n  \"b4\": 0.8,\n  \"b5\": 0.7,\n  \"b6\": 0.7,\n  \"b7\": 0.75,\n  \"b8\": 0.85,\n  \"b9\": 0.75\n}"], "64cc25d83fda6d7f063be43e": ["Based on the given academic paper text and title of the references, the source papers that inspired the author most could be:\n\n1. \"level paging and level ept\" [b0]: The paper discusses the extension of VA width from 48 to 57 bits, indicating that it could be a significant source for the author's exploration of the impacts of a 128-bit processor microarchitecture.\n\n2. \"volume i user level isa document version\" [b2]: The author refers to this paper to justify the strategy of doubling the VA width to 128 bits.\n\n3. \"a new golden age for computer architecture\" [b3]: This paper seems to have inspired the author's discussion about the inability to rely on Dennard scaling and Moore's Law to absorb hardware cost in the context of 128-bit microarchitecture.\n\n4. \"don t use the page number but a pointer to it\" [b6]: The author cites this paper as a source of the region-based compression technique to mitigate the increase in tag and/or data arrays of relevant structures.\n\n5. \"revisiting clustered microarchitecture for future superscalar cores a case for wide issue clusters\" [b7] and \"dynamic cluster assignment mechanisms\" [b8]: These papers seem to have influenced the author's proposition to use clustering to handle the microarchitecture complexity.\n\n6. \"the load slice core microarchitecture\" [b12]: This paper seems to be the source of the idea of using the backward address slices (BAS) of address generating instructions for steering instructions to the dedicated 128-bit address cluster.\n\nThe returned result in json format would be:\n\n{\"b0\": 0.9, \"b2\": 0.8, \"b3\": 0.7, \"b6\": 0.8, \"b7\": 0.7, \"b8\": 0.7, \"b12\": 0.9}\n\nNote: The confidence score is based on the number of times the reference is cited and the relevance of the reference paper's topic to the novel contributions of the paper. A higher score indicates stronger relevance and influence.", "Based on the text and titles of the references, the most pertinent source papers to the given paper that inspired the author are:\n1. \"Volume i user level ISA document version\" [b2]: This paper is linked to the proposal of 128-bit microarchitecture which is the main contribution of the given paper. The author refers to the RISC-V 128-bit general purpose extension highlighted in this paper.\n2. \"The load slice core microarchitecture\" [b12]: The author uses the concept of backward address slices (BAS) from this paper to propose an efficient 128-bit microarchitecture.\n3. \"Dynamic cluster assignment mechanisms\" [b8]: This paper is referred to in the context of the proposed dynamic steering mechanism in the given paper, which is used to distribute instructions to different clusters.\n4. \"Don't use the page number but a pointer to it\" [b6]: This paper is mentioned in relation to the use of region-based compression to mitigate the increase in tag and/or data arrays of relevant structures, a significant aspect of the proposed 128-bit microarchitecture.\n5. \"Revisiting clustered microarchitecture for future superscalar cores: a case for wide issue clusters\" [b7]: This paper is likely a source of inspiration for the given paper's proposal for a microarchitecture using clustering.\n6. \"A new golden age for computer architecture\" [b3]: This paper is referenced in the introduction of the given paper, and it seems to set the context for the challenges faced in developing a 128-bit microarchitecture.\n\nThe confidence score for each reference can not accurately be determined based on the given information.\n\nThe final result in json format would be:\n{\n\"b2\": \"0.X\",\n\"b12\": \"0.X\",\n\"b8\": \"0.X\",\n\"b6\": \"0.X\",\n\"b7\": \"0.X\",\n\"b3\": \"0.X\"\n} \n\nNote: Replace \"X\" with the exact confidence score once calculated.", "Based on the text and author's note, the paper primarily discusses the challenges and solutions to transitioning from 64-bit to 128-bit addresses in CPU microarchitecture. The proposed solution involves using a separate cluster for 128-bit instructions, region-based compression to reduce the area and power overhead, and leveraging existing microarchitecture clustering techniques.\n\nTherefore, the key source papers that inspired this work could be:\n\n1. \"volume i user level isa document version\" [b2]: The paper mentions this work in the context of discussing the 128-bit extension for RISC-V. The proposed solution builds upon the idea of using 128-bit instructions in a separate cluster.\n   \n2. \"don\u2019t use the page number but a pointer to it\" [b6]: This paper is mentioned when discussing the use of region-based compression to mitigate the increase in tag and/or data arrays of relevant structures. This technique is a key part of the proposed solution.\n   \n3. \"revisiting clustered microarchitecture for future superscalar cores a case for wide issue clusters\" [b7] and \"dynamic cluster assignment mechanisms\" [b8]: Both of these papers are referred to when discussing the concept of clustering in microarchitecture, which is a central part of the proposed solution. The author uses this concept to propose a separate cluster for 128-bit instructions.\n   \n4. \"the load slice core microarchitecture\" [b12]: This paper is referred to when discussing the backward address slices (BAS) of address generating instructions. This technique is used in the paper to determine which instructions should be steered to the address cluster.\n\nHere is the result in JSON format:\n\n{\n    \"b2\": 0.9,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b12\": 0.8\n}", "Based on the provided information, the following are the most influential source papers:\n\n1. \"Volume I: User Level ISA Document Version\" [b2]: This paper is referred to several times, specifically in the context of the virtual address (VA) space and when discussing the 128-bit extension. It's clear that this paper is a primary source of inspiration for the current paper's discussion on VA width extension.\n   \n2. \"Revisiting clustered microarchitecture for future superscalar cores: A case for wide issue clusters\" [b7]: This paper is directly referenced when discussing the concept of dynamic steering and clustering in the proposed 128-bit microarchitecture, indicating its significant influence on the author's work.\n\n3. \"Dynamic cluster assignment mechanisms\" [b8]: This paper is also referenced in the context of dynamic steering, suggesting that it significantly informed the author's approach to this aspect of their proposed microarchitecture.\n\n4. \"The Load Slice Core Microarchitecture\" [b12]: This paper is referenced in relation to the proposed microarchitecture's approach to instruction steering, indicating that it played a key role in shaping the author's methodology.\n\n5. \"Don't Use the Page Number but a Pointer to it\" [b6]: This paper is referenced when discussing region-based compression, a technique used in the proposed microarchitecture, suggesting that it was a primary source of inspiration for this aspect of the author's work.\n\n6. \"The AMD Zen Processor\" [b4] and \"Intel Alder Lake CPU Architectures\" [b5]: Both of these papers are referenced in the context of modern microarchitectures, suggesting that they provided important context and inspiration for the author's work.\n\nBased on this analysis, the confidence scores for each source paper would be:\n\n{\n\"b2\": 1,\n\"b7\": 0.9,\n\"b8\": 0.9,\n\"b12\": 0.9,\n\"b6\": 0.8,\n\"b4\": 0.7,\n\"b5\": 0.7\n}", "Based on the text of the paper and the titles of the references, the source papers that most inspired the author are:\n\n1. 'level paging and level ept' [b0]\n2. 'volume i user level isa document version' [b2]\n3. 'the amd zen processor' [b4]\n4. 'intel alder lake cpu architectures' [b5]\n5. 'don t use the page number but a pointer to it' [b6]\n6. 'dynamic cluster assignment mechanisms' [b8]\n7. 'the load slice core microarchitecture' [b12]\n\nIn the paper, the author discusses the challenges and potential solutions of switching from a 64-bit to a 128-bit processor microarchitecture. The paper makes use of clustering to partition critical parts of the microarchitecture, and region-based compression to reduce the footprint of common structures. The author also cites the work of others to support their arguments and to explain the basis for their proposed solutions.\n\nThe papers identified above are those that the author cites in relation to the specific challenges and solutions discussed in the paper.\n\nHere is the result in the JSON format:\n\n{\n  \"b0\": 0.85,\n  \"b2\": 0.85,\n  \"b4\": 0.8,\n  \"b5\": 0.8,\n  \"b6\": 0.75,\n  \"b8\": 0.75,\n  \"b12\": 0.9\n}\n\nThe confidence scores are based on the relevance of the reference to the novel contributions and methods of the paper, with a higher score indicating a stronger influence."], "6449e7fc582c1376bbfc600e": ["The given academic paper focuses on the challenges related to the use of event cameras and proposes a fast method to compare event representations using the Gromov-Wasserstein Discrepancy (GWD). The paper is centered around dense event representations and their comparisons. \n\nBased on the information and context provided in the paper, the source papers that might have inspired this work include:\n\n1. \"Event-based vision meets deep learning on steering prediction for self-driving cars\" [b30]\n2. \"Hats: Histograms of averaged time surfaces for robust event-based object classification\" [b47]\n3. \"Event-based high dynamic range image and very high frame rate video generation using conditional generative adversarial networks\" [b48]\n4. \"A large-scale event-based detection dataset for automotive\" [b9]\n5. \"End-to-end learning of representations for asynchronous event-based data\" [b13]\n6. \"Event-based vision: A survey\" [b12]\n7. \"Gromov-Wasserstein averaging of kernel and distance matrices\" [b39]\n8. \"Mixed-Density Event Stacks\" [b34]\n\nReturn in the JSON format: \n\n{\n\"b30\": 0.9,\n\"b47\": 0.8,\n\"b48\": 0.9,\n\"b9\": 0.7,\n\"b13\": 0.9,\n\"b12\": 0.8,\n\"b39\": 0.9,\n\"b34\": 0.7\n}\n\nThe confidence scores are assigned based on the relevance of the source paper to the main contributions of the given paper. Higher scores indicate a stronger influence.", "Based on the text, the papers that most directly influenced the main contributions of the given paper are:\n\n- b13: \"End-to-end learning of representations for asynchronous event-based data\"\n- b34: \"Stereo depth from events cameras concentrate and focus on the future\"\n- b48: \"Event-based high dynamic range image and very high frame rate video generation using conditional generative adversarial networks\"\n- b0: \"EV SegNet: Semantic segmentation for event-based cameras\" \n- b51: \"Unsupervised event-based learning of optical flow, depth, and egomotion\"\n- b30: \"Event-based vision meets deep learning on steering prediction for self-driving cars\"\n- b18: \"Gryffin: An algorithm for Bayesian optimization of categorical variables informed by expert knowledge\"\n\nThe confidence score is calculated based on the number of mentions and the context in which these papers are referenced. \n\nHere is the final result in the required format:\n\n{\"b13\": 0.9, \"b34\": 0.8, \"b48\": 0.7, \"b0\": 0.6, \"b51\": 0.6, \"b30\": 0.6, \"b18\": 0.9 }", "Based on the provided text and the mentioned references, the most relevant source papers for the given paper appear to be:\n\n1. \"Event-based vision meets deep learning on steering prediction for self-driving cars\" [b30]\n2. \"Event-based vision: A survey\" [b12]\n3. \"End-to-end learning of representations for asynchronous event-based data\" [b13]\n4. \"Mixed-Density Event Stacks\" [b34]\n5. \"Time Ordered Recent Event Volumes (TORE) for event cameras\" [b1]\n\nThe source papers can be identified as follows:\n\n1. The reference [b30] is mentioned in the context of dense event representations, which seems to be a key topic of the paper.\n2. Reference [b12] is mentioned as the source of the concept of event cameras, which is a central topic of the paper.\n3. Reference [b13] is repeatedly mentioned in the context of event representations and their comparison, which is a primary challenge outlined in the paper.\n4. Reference [b34] is mentioned in the context of dividing events into multiple overlapping windows, which seems to be a key method adopted in the paper.\n5. Reference [b1] is mentioned as a bio-inspired approach that led to the introduction of Time Ordered Recent Event Volumes (TORE), which seems to be an important method related to event representations.\n\nBased on these references, the confidence score is as follows:\n\n\"b30\": 0.9\n\"b12\": 0.8\n\"b13\": 0.9\n\"b34\": 0.9\n\"b1\": 0.8\n\nNote: The confidence score is a subjective metric and can vary based on the detailed context and understanding of the paper's main contributions.", "Based on the provided text, the following references seem to have significantly inspired the paper:\n\n1) 'b13': 'end to end learning of representations for asynchronous event based data'\n2) 'b34': 'stereo depth from events cameras concentrate and focus on the future'\n3) 'b48': 'event based high dynamic range image and very high frame rate video generation using conditional generative adversarial networks'\n4) 'b51': 'unsupervised event based learning of optical flow depth and egomotion'\n5) 'b0': 'ev segnet semantic segmentation for event based cameras'\n6) 'b39': 'gromov wasserstein averaging of kernel and distance matrices'\n7) 'b18': 'gryffin an algorithm for bayesian optimization of categorical variables informed by expert knowledge'\n\nThe paper frequently refers to these works and their methods, indicating a high degree of influence. Please note that this analysis is based on the text provided and the titles of the references, and a more comprehensive analysis would also include a review of the cited papers themselves.\n\nHere is the confidence score for each source paper:\n\n{\"b13\": 0.85, \"b34\": 0.8, \"b48\": 0.8, \"b51\": 0.75, \"b0\": 0.7, \"b39\": 0.65, \"b18\": 0.6}", "Based on the provided text, the following references seem to have heavily influenced the main contributions of the paper:\n\n- 'b13': \"End-to-end learning of representations for asynchronous event-based data\"\n- 'b48': \"Event-based high dynamic range image and very high frame rate video generation using conditional generative adversarial networks\"\n- 'b34': \"Stereo depth from events cameras concentrate and focus on the future\"\n- 'b0': \"Ev-SegNet: Semantic Segmentation for Event-Based Cameras\"\n- 'b51': \"Unsupervised event-based learning of optical flow, depth, and egomotion\"\n- 'b1': \"Time-Ordered Recent Event (TORE) volumes for Event Cameras\"\n\nReasoning:\n'b13' is mentioned as a pioneering work that introduced a learnable representation. 'b48' and 'b34' are indicated as later works that the author has taken inspiration from. 'b0' is also mentioned as a method that tries to capture more event information. 'b51' and 'b1' are discussed as the earlier works that the paper's proposed method has improved upon. \n\nJSON format:\n\n{\n    \"b13\": 0.9,\n    \"b48\": 0.8,\n    \"b34\": 0.8,\n    \"b0\": 0.7,\n    \"b51\": 0.6,\n    \"b1\": 0.6\n}"], "6523793e939a5f4082e182a2": ["{\n    \"b16\": 0.95,\n    \"b17\": 0.95,\n    \"b18\": 0.95,\n    \"b19\": 0.95,\n    \"b20\": 0.95,\n    \"b23\": 0.95,\n    \"b24\": 0.95\n}", "{\n    \"b26\": 0.9,\n    \"b27\": 0.9,\n    \"b28\": 0.9,\n    \"b29\": 0.9,\n    \"b30\": 0.9,\n    \"b31\": 0.9,\n    \"b32\": 0.9\n}", "{\n  \"b26\": 0.9,\n  \"b27\": 0.9,\n  \"b28\": 0.9,\n  \"b29\": 0.9,\n  \"b30\": 0.9,\n  \"b31\": 0.9,\n  \"b32\": 0.9\n}", "{\n    \"b16\": 0.9,\n    \"b17\": 0.9,\n    \"b18\": 0.85,\n    \"b19\": 0.85,\n    \"b20\": 0.8,\n    \"b21\": 0.75,\n    \"b26\": 0.7,\n    \"b27\": 0.7\n}", "{\n    \"b16\": 0.9,\n    \"b17\": 0.9,\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b20\": 0.8,\n    \"b23\": 0.8,\n    \"b24\": 0.8\n}"], "64c78ba33fda6d7f06dbcb16": ["Based on the text and the author notes, the paper focuses on proposing a pipeline acceleration method for graph neural network (GNN) acceleration, which incorporates multi-GPU communication as a part of the pipeline. The method includes multi-GPU pipeline mapping methods, GPU internal mapping, pipeline design, and lightweight analytic modeling. In this context, the source papers that likely inspired the authors the most are those related to graph neural networks, GPU architecture, and pipeline acceleration methods. \n\nBased on the text analysis and author notes, the most pertinent source papers are as follows:\n\n1. \"Graph Attention Networks\" (b42): This paper is likely a key source as it introduces Graph Attention Networks, a type of GNN, which is the main focus of the paper in question.\n    \n2. \"How powerful are graph neural networks\" (b48): This paper discusses the power and efficacy of GNNs, which is directly relevant to the paper's focus on GNN acceleration.\n\n3. \"Optimization principles and application performance evaluation of a multithreaded GPU using CUDA\" (b39): This paper discusses the principles of GPU optimization, which is directly related to the paper's discussion of GPU architecture and pipeline acceleration.\n\n4. \"Runtime dependency analysis for loop pipelining in high-level synthesis\" (b0): As this paper discusses loop pipelining, it is likely to have inspired the pipeline acceleration method proposed in the paper.\n\n5. \"Deep graph library towards efficient and scalable deep learning on graphs\" (b44): This paper discusses the efficient and scalable deep learning on graphs, which directly relates to the paper's topic of GNN acceleration.\n\n6. \"NeuGraph: Parallel deep neural network computation on large graphs\" (b25): This paper is likely a key source as it discusses parallel deep neural network computation on large graphs which is directly relevant to the paper's focus on GNN acceleration.\n\n7. \"NVIDIA GPU Micro Architecture\" (b47): This may be a source paper as it provides information on NVIDIA GPU architecture, which the paper's proposed method relies on.\n\n8. \"MGG: A Computation-Centric Distributed Graph Processing System\" (b56): Although this paper's title is inferred, it directly relates to the topic of the paper, potentially indicating a strong influence.\n\nBased on these analyses, the final result is:\n\n{\n    \"b42\": 0.9,\n    \"b48\": 0.9,\n    \"b39\": 0.85,\n    \"b0\": 0.8,\n    \"b44\": 0.85,\n    \"b25\": 0.85,\n    \"b47\": 0.8,\n    \"b56\": 0.9\n}", "Based on the text given, here are the most likely source papers cited, along with their confidence scores:\n\n{\n    \"b20\": 0.9,\n    \"b42\": 0.8,\n    \"b48\": 0.8,\n    \"b15\": 0.7,\n    \"b17\": 0.7,\n    \"b27\": 0.7,\n    \"b35\": 0.8\n}\n\nThese papers are inferred based on the frequency of their mentions and the context of their mentions in the paper text. It appears that these cited papers are foundational in the development of the GNN model proposed in the paper. For example, papers b20, b42, and b48 are mentioned in the introduction as key to the success of GNN. Paper b15 is mentioned as evidence of the superiority of GNN over conventional graph analytics. Papers b17 and b27 are compared and contrasted with the proposed model in the paper, indicating their significance in the development of the model. Lastly, paper b35 is mentioned as the main communication backend for the proposed model, indicating its importance. The confidence scores are based on the level of importance given to each paper in the text.", "Based on the text and the task, the following papers seem to have inspired the author the most:\n\n1. \"b0\": 'runtime dependency analysis for loop pipelining in highlevel synthesis'\n2. \"b2\": 'balanced graph partitioning'\n3. \"b4\": 'legion expressing locality and independence with logical regions'\n4. \"b5\": 'dgcl an efficient communication library for distributed gnn training'\n5. \"b17\": 'improving the accuracy scalability and performance of graph neural networks with roc'\n6. \"b24\": 'pagraph scaling gnn training on large graphs via computation aware caching'\n7. \"b25\": 'neugraph parallel deep neural network computation on large graphs'\n8. \"b26\": 'pytorch direct enabling gpu centric data access for very large graph neural network training with irregular accesses'\n\nThese papers are directly related to the challenges and proposed approaches of the paper, including graph partitioning, pipeline construction, GPU programming support, and efficient GNN training.\n\nSo, the result in the json format is:\n```json\n{\n    \"b0\": 0.9,\n    \"b2\": 0.8,\n    \"b4\": 0.8,\n    \"b5\": 0.9,\n    \"b17\": 0.85,\n    \"b24\": 0.9,\n    \"b25\": 0.85,\n    \"b26\": 0.9\n}\n```\nThe confidence score is set between 0.8 and 0.9 based on the relevance of the paper to the research's main contributions.", "Based on the text and the author notes, the following references seem to be the most relevant source papers that inspired the given paper:\n\n1. \"b17\": 'improving the accuracy scalability and performance of graph neural networks with roc'\n2. \"b27\": 'large graph convolutional network training with gpu oriented data communication architecture'\n3. \"b44\": 'deep graph library towards efficient and scalable deep learning on graphs'\n4. \"b45\": 'gnnadvisor an efficient runtime system for gnn acceleration on gpus'\n5. \"b49\": 'hygcn a gcn accelerator with hybrid architecture'\n6. \"b50\": 'gnnlab a factored system for sample based gnn training over gpus'\n\nThese sources are directly related to the algorithm proposed by the author for graph neural network acceleration and they tackle the same challenges. Moreover, these papers seem to have a methodological similarity with the given paper. \n\nResult in the JSON format:\n{\n\"b17\": 0.9,\n\"b27\": 0.85,\n\"b44\": 0.9,\n\"b45\": 0.8,\n\"b49\": 0.8,\n\"b50\": 0.85\n} \n\nThe confidence scores are estimated based on the relevance of the source papers to the given paper's novel contributions and challenges.", "Based on the given text, the following references seem to be the most pertinent source papers:\n\n1. \"Runtime dependency analysis for loop pipelining in highlevel synthesis\" [b0]: This paper is mentioned in the context of the GNN workload specialty, which features two major types of partial dependency that facilitate pipelining.\n\n2. \"Dgcl an efficient communication library for distributed GNN training\" [b5]: This work is discussed in the related work section, regarding solutions that treat communication as a standalone operator.\n\n3. \"Improving the accuracy scalability and performance of graph neural networks with roc\" [b17]: The authors refer to this paper when discussing schedule transformation for dense communication.\n\n4. \"Fastgcn fast learning with graph convolutional networks via importance sampling\" [b7]: This research is mentioned in the context of algorithm modification for no communication, which can compromise the accuracy of GNN models.\n\n5. \"Pagraph scaling gnn training on large graphs via computation aware caching\" [b24]: The authors reference this paper when discussing algorithm modification for no communication.\n\n6. \"Neugraph parallel deep neural network computation on large graphs\" [b25]: This paper is cited in relation to schedule transformation for dense communication.\n\n7. \"Nvshmem communication library\" [b35]: The NVSHMEM library is mentioned several times throughout the paper as a tool for fine-grained inter-GPU communication.\n\nThe confidence scores are hard to determine without a detailed analysis of each reference, but they could be assigned based on the frequency and context of each mention in the paper.\n\nHere is the result in JSON format:\n\n{\n    \"b0\": 0.85,\n    \"b5\": 0.8,\n    \"b17\": 0.8,\n    \"b7\": 0.75,\n    \"b24\": 0.75,\n    \"b25\": 0.75,\n    \"b35\": 0.9\n}"], "65260ee8cd549670787e1513": ["Based on the analysis of the academic paper's text and the titles of the references, the most influential source papers are:\n\n1. \"cogvlm visual expert for large language models\" [None]: The paper's main contribution seems to be the development of the CogVLM model which is a visual expert for large language models. The novel method and approach introduced in this paper is the addition of a trainable visual expert to the language model. This source paper is directly relevant to the paper's novel contributions.\n\n2. \"a jointly scaled multilingual language image model\" [b8]: The paper seems to be inspired by the image-text joint training adopted by PaLI and Qwen-VL. The direct relevance of this reference to the paper's core contributions suggests it is a key source paper.\n\n3. \"vicuna an open source chatbot impressing gpt with chatgpt quality\" [b10]: The paper mentions CogVLM-17B trained from Vicuna-7B, making this a source paper.\n\n4. \"low rank adaptation of large language models\" [b18]: The comparison between p-tuning and LoRA in efficient finetuning indicates that this paper has influenced the author's work.\n\n5. \"aligning large multi modal model with robust instruction tuning\" [b26]: The authors discuss the performance degradation of p-tuning and shallow alignment, suggesting that they have been influenced by this paper.\n\n6. \"scaling up vision language pre training for image captioning\" [b18]: The paper's focus on image captioning suggests that this paper is a significant source.\n\n7. \"eva clip improved training techniques for clip at scale\" [b44]: The paper mentions the use of pretrained EVA2-CLIP-E in CogVLM-17B, indicating that this is a source paper.\n\nThe confidence scores are subjective and based on the relevance of each source paper to the main contributions of the paper. For instance, the paper [None] is given a high confidence score as it relates directly to the paper's main contribution, whereas [b8] is given a lower confidence score as it is referenced less frequently.\n\nThe result in json format:\n\n{\n  \"None\": 0.9,\n  \"b8\": 0.7,\n  \"b10\": 0.8,\n  \"b18\": 0.7,\n  \"b26\": 0.8,\n  \"b44\": 0.8\n}", "Based on the text of the paper and the list of references provided, the source papers that seem to have inspired the given paper most directly are:\n\n1. \"CogVLM: Visual Expert for Large Language Models\" - This is the given paper's title and it indicates that the main contribution of the paper is the development of the CogVLM model. \n\n2. \"BLIP: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\" [b23] - It is mentioned directly as a method that the authors have built upon.\n\n3. \"PaLI: A Jointly Scaled Multilingual Language-Image Model\" [b8] - The authors compare their method to this one and argue for the superiority of their method.\n\n4. \"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities\" [b3] - This is also compared to the authors' method and it is argued that their method is superior.\n\n5. \"P-tuning: Learning Task Prefix Embedding\" [b26] and \"LoRA: Low-Rank Adaptation of Large Language Models\" [b18] - These are both referred to as methods that the authors' method is compared to and builds upon.\n\n6. \"Vicuna: An Open-Source Chatbot Impressing GPT with ChatGPT Quality\" [b10] - This is the starting point for the training of the CogVLM model.\n\n7. \"EVA-CLIP: Improved Training Techniques for CLIP at Scale\" [b44] - This is used in the architecture of the CogVLM model.\n\n8. \"LLaVA-Instruct\" [b27], \"LRV-Instruction\" [b26], \"LLaVAR\" [b58] - These are used as data for supervised finetuning of the CogVLM model.\n\nIn JSON format, the confidence scores will be:\n\n{\n\"b23\": 1,\n\"b8\": 1,\n\"b3\": 1,\n\"b26\": 1,\n\"b18\": 1,\n\"b10\": 1,\n\"b44\": 1,\n\"b27\": 1,\n\"b26\": 1,\n\"b58\": 1\n} \n\nPlease note that the confidence scores are all set to 1, as the paper directly refers to these works as having a significant influence on the development of the proposed model.", "Based on the provided text and reference paper titles, the source papers that inspired the given paper appear to be:\n\n1. \"CogVLM visual expert for large language models\" - Mentioned explicitly multiple times as the work the paper is building upon or comparing with. Confidence score: 1\n\n2. \"PaLI a jointly scaled multilingual language image model\" - Referenced in relation to the method that the paper's authors adapted, albeit with challenges. Confidence score: 0.8\n\n3. \"Low rank adaptation of large language models\" - Mentioned explicitly as a source of inspiration for the paper's method. Confidence score: 0.7\n\n4. \"Aligning large multi modal model with robust instruction tuning\" - Mentioned in the context of discussing the performance of other alignment methods, which could suggest that this paper provided some inspiration for the proposed solution. Confidence score: 0.6\n\n5. \"mPLUG-DocOwl modularized multimodal large language model for document understanding\" - Referenced in the context of comparing the performance of the proposed method with other generalist paradigms, which could imply that this paper influenced the design of the experimental setup. Confidence score: 0.5\n\n6. \"Scaling up vision language pre training for image captioning\" - Mentioned in relation to the challenges faced when scaling up VLMs, which could suggest that this paper influenced the paper's exploration of that issue. Confidence score: 0.4\n\n7. \"General language model pretraining with autoregressive blank infilling\" - Referenced in the context of a discussion on language model pretraining, which could suggest that this paper provided some inspiration for the paper's method. Confidence score: 0.3\n\nIn JSON format, the results are:\n\n{\n    \"1\": 1,\n    \"2\": 0.8,\n    \"3\": 0.7,\n    \"4\": 0.6,\n    \"5\": 0.5,\n    \"6\": 0.4,\n    \"7\": 0.3\n}", "Based on the text and the titles of the references, the source papers that inspire the given paper the most are:\n\n1. \"b2\": 'vqa visual question answering'\n2. \"b3\": 'qwen vl a frontier large vision language model with versatile abilities'\n3. \"b10\": 'vicuna an open source chatbot impressing gpt with chatgpt quality'\n4. \"b18\": 'scaling up vision language pre training for image captioning'\n5. \"b26\": 'aligning large multi modal model with robust instruction tuning'\n6. \"b44\": 'eva clip improved training techniques for clip at scale'\n7. \"b45\": 'llama open foundation and fine tuned chat models'\n8. \"b60\": 'visual w grounded question answering in images'\n\nThe returned result in JSON format is:\n{\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b10\": 0.8,\n    \"b18\": 0.9,\n    \"b26\": 0.85,\n    \"b44\": 0.75,\n    \"b45\": 0.85,\n    \"b60\": 0.8\n}", "Based on the text of the paper and the titles of the references, the most pertinent source papers that inspired the given paper are:\n\n1. \"cogvlm visual expert for large language models\" (b0): This is the title of the paper itself, and it is mentioned several times throughout the text, indicating that it is a significant source of inspiration. Confidence Score: 1\n\n2. \"qwen vl a frontier large vision language model with versatile abilities\" (b3): This paper is directly compared to in the text, indicating that it had a substantial influence on the current paper. Confidence Score: 0.9\n\n3. \"a jointly scaled multilingual language image model\" (b8): This paper's method is adopted by the given paper, indicating it as a source of inspiration. Confidence Score: 0.9\n\n4. \"vicuna an open source chatbot impressing gpt with chatgpt quality\" (b10): This model is used for further training in the given paper, indicating it as a source of inspiration. Confidence Score: 0.8\n\n5. \"low rank adaptation of large language models\" (b18): The comparison between p-tuning and LoRA in the given paper indicates that this paper had a significant influence. Confidence Score: 0.7\n\n6. \"aligning large multi modal model with robust instruction tuning\" (b26): This method is directly compared to in the text, indicating that it had a substantial influence on the current paper. Confidence Score: 0.7\n\n7. \"eva clip improved training techniques for clip at scale\" (b44): The ViT encoder used in the current paper is directly inspired by this paper. Confidence Score: 0.6\n\nThe final result in json format is:\n{\n  \"b0\": 1, \n  \"b3\": 0.9,\n  \"b8\": 0.9,\n  \"b10\": 0.8,\n  \"b18\": 0.7,\n  \"b26\": 0.7,\n  \"b44\": 0.6\n}"], "6503bec83fda6d7f067c7787": ["Based on the text and titles of the papers, the source papers that inspired the given paper are:\n\n1. \"Language models are few-shot learners\" [b4]\n2. \"Scaling instruction finetuned language models\" [b9]\n3. \"Case-based reasoning for natural language queries over knowledge bases\" [b10]\n4. \"GoEmotions: a dataset of fine-grained emotions\" [b11]\n5. \"Rethinking the role of demonstrations: What makes in-context learning work\" [b24]\n6. \"Stanford Sentiment Treebank\" [b28]\n7. \"Mis-classified Vector Guided Softmax Loss for Face Recognition\" [b31]\n\nThese papers are the most frequently and directly cited in relation to the challenges and novel methods proposed in the paper. They are also preceded by phrases that indicate direct inspiration or motivation. The confidence scores for each are as follows:\n\n{ \"b4\": 0.9, \"b9\": 0.8, \"b10\": 0.85, \"b11\": 0.7, \"b24\": 0.75, \"b28\": 0.7, \"b31\": 0.65 }\n", "Based on the analysis of the text, the source papers that seem to have inspired the given paper the most are:\n\n{ \n\"b4\": 0.9, \n\"b23\": 0.8, \n\"b21\": 0.8, \n\"b29\": 0.7, \n\"b31\": 0.7, \n\"b10\": 0.6, \n\"b18\": 0.7, \n\"b0\": 0.6\n}\n\nThese papers are directly referred to in the text when discussing the core methodologies and motivations of the given paper. The confidence scores are determined based on the frequency of mention and the context in which they are discussed.", "Based on the given paper text, the following references seem to be the most influential:\n\n- \"b4\": 'Language models are few-shot learners'\n- \"b23\": 'Active learning principles for in-context learning with large language models'\n- \"b21\": 'DR-ICL: Demonstration Retrieved In-Context Learning'\n- \"b29\": 'An effective refinement strategy for KNN text classifier'\n- \"b31\": 'Mis-classified vector guided softmax loss for face recognition'\n- \"b10\": 'Case-based reasoning for natural language queries over knowledge bases'\n- \"b18\": 'The 3rd workshop on knowledge extraction and integration for deep learning architectures'\n\nThe references were selected based on the cues presented in the text, such as the phrases indicating inspiration or motivation, or methodological similarity. Also, these papers appear to be directly related to the novel contributions of the paper, specifically in the area of improving in-context learning techniques.\n\nThe resulting output in JSON format would be:\n\n{\n  \"b4\": 0.9,\n  \"b23\": 0.8,\n  \"b21\": 0.7,\n  \"b29\": 0.7,\n  \"b31\": 0.7,\n  \"b10\": 0.6,\n  \"b18\": 0.6\n}\n\nNote that the reference number is the key, and the confidence score is the value. The confidence score is subjective and is given based on the perceived importance and relevance of each reference to the main paper.", "Based on the provided text and the guidance given:\n\n1. The paper discusses the challenges of fine-tuning large language models (LLMs) and the sensitivity of LLMs to the choice of prompts. \n2. It proposes a novel method for selecting in-context learning (ICL) demonstrations by leveraging the model's existing knowledge about the output label space. \n3. The method involves using a text retriever to select semantically similar examples from the training data, identifying ambiguous labels, and selecting demonstrations based on these ambiguous labels and whether they were misclassified by the model.\n\nThe paper frequently cites the following references in the context of the proposed method and its results:\n\n1. [b23]: \"Active learning principles for in-context learning with large language models\"\n2. [b21]: \"DR-ICL: Demonstration Retrieved In-Context Learning\"\n3. [b18]: \"The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures\"\n4. [b29]: \"An effective refinement strategy for KNN text classifier\"\n5. [b31]: \"Mis-classified Vector Guided Softmax Loss for Face Recognition\"\n6. [b10]: \"Case-based reasoning for natural language queries over knowledge bases\"\n\nThe JSON format of the result with a confidence score of 0.95 for each reference (assuming similar contribution from each) is:\n\n{\n\"b23\": 0.95,\n\"b21\": 0.95,\n\"b18\": 0.95,\n\"b29\": 0.95,\n\"b31\": 0.95,\n\"b10\": 0.95\n}", "Based on the provided text and references, the source papers that inspire the authors most appear to be:\n\n- b4: \"Language models are few-shot learners\" - The authors refer to this paper multiple times, suggesting that the theoretical framework and results from this paper are foundational to their own work. They specifically mention the challenge of fine-tuning large language models, which is a central theme in this paper.\n\n- b23: \"Active learning principles for in-context learning with large language models\" - The authors build their method on top of the semantic similarity approach proposed in this paper. The paper is directly relevant to their work on in-context learning.\n\n- b21: \"DR-ICL: Demonstration-Retrieved In-Context Learning\" - The authors often mention this paper when discussing their own approaches to in-context learning and demonstration-based methods.\n\n- b24: \"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work\" - The authors cite this work when discussing the impact of label in ICL demonstrations, which influences their own approach.\n\n- b18: \"The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures\" - The authors cite this work when discussing prompt engineering and its challenges, which is a key focus of their study.\n\n- b10: \"Case-Based Reasoning for Natural Language Queries Over Knowledge Bases\" - The authors cite this work when discussing the effectiveness of semantically similar demonstrations, which is a core part of their proposed method.\n\n- b29: \"An Effective Refinement Strategy for KNN Text Classifier\" - This paper is cited when the authors discuss the usefulness of misclassified examples in their method.\n\nResults in JSON format:\n\n{\"b4\": 1, \"b23\": 0.9, \"b21\": 0.9, \"b24\": 0.8, \"b18\": 0.8, \"b10\": 0.7, \"b29\": 0.7}"], "645dad16d68f896efad9df53": ["Based on the text of the academic paper and the titles of the references, the most pertinent source papers that inspired the author could be:\n\n- \"Swin transformer hierarchical vision transformer using shifted windows\" [b42]\n- \"Training data efficient image transformers distillation through attention\" [b67]\n- \"Rethinking the value of network pruning\" [b43]\n- \"Taylor structured pruning [b51] to automatically find the important components in Swin-T and DeiT-T\" [b51]\n- \"Batch normalization accelerating deep network training by reducing internal covariate shift\" [b28]\n- \"Data movement is all you need: A case study on optimizing transformers\" [b29]\n\nThe author has mentioned these references explicitly while discussing the challenges and the solutions proposed in the paper. These references directly relate to the novel contributions of the paper. The details of the source papers are:\n\n```json\n{\n\"b42\": 0.9,\n\"b67\": 0.85,\n\"b43\": 0.8,\n\"b51\": 0.8,\n\"b28\": 0.75,\n\"b29\": 0.75\n}\n```\n\nPlease note the confidence score assigned to each source paper is subjective and may vary based on the understanding of the paper's text.", "Based on the text of the paper, it appears that several source papers have been cited as influential or inspiring, and these papers are directly relevant to the challenges and methods presented in the paper. By examining the text and the references involved, the following list of source papers can be identified:\n\n1. \"Swin transformer hierarchical vision transformer using shifted windows\" [b42]: This paper is mentioned multiple times throughout the text as a source of inspiration and a benchmark for comparison. The author's work is based on the challenges and methods presented in the Swin paper. It has a high confidence score.\n\n2. \"Training data efficient image transformers distillation through attention\" [b67]: This paper is also frequently cited in the text. It appears that the author's work is heavily influenced by the DeiT paper. It has a high confidence score.\n\n3. \"Efficientformer vision transformers at mobilenet speed\" [b38]: This paper is mentioned in the context of the increasing model sizes and computation overhead of vision transformers. It seems to provide a motivation for the author's work. It has a moderate confidence score.\n\n4. \"MobileViT-XS\" [b48]: This paper is cited as an example of a method that has achieved good performance with fewer Flops or parameters, but does not show significant wall-clock speedup. This seems to provide a challenge that the author's work seeks to address. It has a moderate confidence score.\n\n5. \"Rethinking the value of network pruning\" [b43]: This paper appears to provide methodological inspiration for the author's work, specifically in relation to improving parameter efficiency. It has a moderate confidence score.\n\n6. \"Attention is all you need\" [b69]: This seminal work on transformers is mentioned multiple times in the paper. It seems to provide a fundamental basis for the author's work. It has a moderate confidence score.\n\n7. \"Data movement is all you need a case study on optimizing transformers\" [b29]: This paper seems to provide a challenge that the author's work seeks to address, namely the issue of memory-bound operations in multi-head self-attention (MHSA). It has a moderate confidence score.\n\n8. \"FlashAttention Fast and memoryefficient exact attention with IO awareness\" [b14]: This paper is mentioned in the context of the memory inefficiency of operations in multi-head self-attention (MHSA). It seems to provide a challenge that the author's work seeks to address. It has a moderate confidence score.\n\nReturning the results in the requested json format:\n\n{\n  \"b42\": 1.0,\n  \"b67\": 1.0,\n  \"b38\": 0.7,\n  \"b48\": 0.7,\n  \"b43\": 0.7,\n  \"b69\": 0.7,\n  \"b29\": 0.7,\n  \"b14\": 0.7\n}", "Based on the text and the titles of the references, the source papers that the author was inspired by are:\n\n1. \"Attention is all you need\" (b69) - The author discusses the multi-head self-attention (MHSA) mechanism and its issues, which is a core concept from this paper.\n2. \"Training data-efficient image transformers & distillation through attention\" (b67) - The author mentions DeiT multiple times indicating a significant reliance on its methodologies.\n3. \"Swin transformer: hierarchical vision transformer using shifted windows\" (b42) - The author frequently mentions Swin and Swin-T, which suggests that this paper's methodologies heavily influenced their work.\n4. \"EfficientNet: Rethinking model scaling for convolutional neural networks\" (b65) - The author discusses the problems of increasing model sizes and computation overhead, which is a primary concern in this paper.\n5. \"Rethinking the value of network pruning\" (b43) - The author uses structured pruning to identify the most important network components, a method proposed in this paper.\n6. \"Data movement is all you need: a case study on optimizing transformers\" (b29) - The author references this paper while discussing the memory-access issue in transformer models.\n7. \"Shufflenet an extremely efficient convolutional neural network for mobile devices\" (b85) - The author mentions the concept of group convolution, which is a significant contribution from this paper.\n\nThe returned result in the JSON format is:\n\n{\n\"b69\": 0.9,\n\"b67\": 0.85,\n\"b42\": 0.8,\n\"b65\": 0.75,\n\"b43\": 0.7,\n\"b29\": 0.65,\n\"b85\": 0.6\n}", "Based on the given text and its references, the possible source papers for the EfficientViT paper could be:\n\n1. \"Attention is all you need\" [b69]: The main inspiration for the paper seems to be the transformer model which was first proposed in this paper. It is mentioned multiple times in the text. The Confidence score: 0.9.\n\n2. \"Training data-efficient image transformers and distillation through attention\" [b67]: This paper is frequently mentioned in the text as a basis for comparison and as a source of motivation for the authors' work. The Confidence score: 0.85.\n\n3. \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\" [b42]: This paper is also referenced multiple times, and it appears to be one of the primary sources of comparison for the authors' work. The Confidence score: 0.85.\n\n4. \"Rethinking the value of network pruning\" [b43]: This paper provided the methodology for identifying important network components used in the paper. The Confidence score: 0.8.\n\n5. \"Shufflenet: An Extremely Efficient Convolutional Neural Network for Mobile Devices\" [b85]: The concept of group convolution mentioned in the text is inspired by this paper. The Confidence score: 0.75.\n\n6. \"Batch normalization: Accelerating deep network training by reducing internal covariate shift\" [b28]: The authors of the paper use BatchNorm (BN) throughout the model, which is inspired by this paper. The Confidence score: 0.75.\n\n7. \"Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned\" [b71]: This paper seems to be the source of the concept of redundancy in attention maps mentioned in the paper. The Confidence score: 0.7.\n\n8. \"Decoupled weight decay regularization\" [b44]: The authors of the paper use AdamW optimizer for training the models, which is inspired by this paper. The Confidence score: 0.7.\n\nThe answer in json format:\n\n{ \"b69\": 0.9, \"b67\": 0.85, \"b42\": 0.85, \"b43\": 0.8, \"b85\": 0.75, \"b28\": 0.75, \"b71\": 0.7, \"b44\": 0.7 }", "Based on the text of the paper and the titles of the references, the most pertinent source papers that inspired the authors could be:\n\n1. \"Attention is all you need\" [b69]: This is a fundamental paper in the field of transformers, and the authors mention several times how they built upon the standard transformer models for their work.\n\n2. \"Swin transformer: hierarchical vision transformer using shifted windows\" [b42]: The authors mention Swin transformer multiple times, indicating that their work was built upon this model. \n\n3. \"Training data-efficient image transformers & distillation through attention\" [b67]: DeiT is mentioned several times as a model that the authors built upon for their work. \n\n4. \"Rethinking the value of network pruning\" [b43]: The authors used structured pruning, which is a method proposed in this paper, to identify the most important network components. \n\n5. \"Efficientnet: rethinking model scaling for convolutional neural networks\" [b65]: Even though it's not mentioned in the text, the title of this paper suggests a focus on efficiency, which is a core theme of the given paper.\n\n6. \"MobileViT: lightweight general-purpose and mobile-friendly vision transformer\" [b48]: Mentioned in the text as an example of other efficient models, suggesting the authors referred to this work.\n\n7. \"Shufflenet: an extremely efficient convolutional neural network for mobile devices\" [b85]: The authors took inspiration from the idea of group convolution from this paper.\n\nThe result in JSON format would be:\n```json\n{\n  \"b69\": 0.9,\n  \"b42\": 0.85,\n  \"b67\": 0.85,\n  \"b43\": 0.8,\n  \"b65\": 0.75,\n  \"b48\": 0.7,\n  \"b85\": 0.7\n}\n```\nThe confidence scores are set based on the number of times each paper is referenced in the text and the context in which they are mentioned."], "64dbf5883fda6d7f060c36fd": ["Based on the paper and the author notes provided, the most relevant source papers are:\n\n1. [b4]: \"Sequence clock a dynamic resource orchestrator for serverless architectures\" - This paper is prominently mentioned as a method the authors are building upon, using CPU quotas to regulate end-to-end serverless chain execution time, which is a core aspect of the paper's novel contributions.\n\n2. [b5]: \"Aquatope qos anduncertainty aware resource management for multi stage serverless workflows\" - Similar to [b4], this paper is also about regulating the end-to-end serverless chain execution time, which is a primary challenge outlined in the paper.\n\n3. [b8]: \"Gemini learning to manage CPU power for latency critical search engines\" - This paper is compared to the author's work in the evaluation section, which suggests that it has a direct impact on the paper's core contributions.\n\n4. [b10]: \"Tackling performance variability due to ras mechanisms with pid controlled dvfs\" - This paper is mentioned as a prior work that the authors build upon, particularly with respect to the approach of using PID for DVFS.\n\n5. [b7]: \"Retail opting for learning simplicity to enable qos aware power management in the cloud\" - This paper is referenced in the context of various works that have opted for adopting system-level techniques such as Dynamic Voltage and Frequency Scaling (DVFS), which is one of the main methods employed in the paper.\n\nTherefore, the JSON format of the result would be:\n\n{\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b8\": 0.85,\n    \"b10\": 0.85,\n    \"b7\": 0.8\n}\n\nNote: the confidence score is a subjective value based on the analysis of the text. It represents the level of direct influence and relevance each source paper has on the main contributions of the given paper.", "Based on the analysis of the text and the provided notes, the source papers that most inspired this work are:\n\n1. \"Sequence Clock: A Dynamic Resource Orchestrator for Serverless Architectures\" [b4]\n2. \"Gemini: Learning to Manage CPU Power for Latency Critical Search Engines\" [b8]\n3. \"Tackling Performance Variability due to RAS Mechanisms with PID Controlled DVFS\" [b10]\n4. \"AWS Step Functions\" [b0]\n\nReasoning:\n- The paper directly mentions taking inspiration from [b4] and [b8] for the utilization of CPU quotas and DVFS techniques, respectively, to tackle the challenges in FaaS scenario.\n- [b10] is referenced in the context of existing DVFS solutions which the paper aims to improve upon.\n- The paper mentions [b0] in the context of the back-end services that orchestrate the functions, which forms a significant part of the paper's architecture.\n\n{\n \"b4\": 0.9,\n \"b8\": 0.9,\n \"b10\": 0.8,\n \"b0\": 0.7\n}", "Based on the provided text, the following references can be identified as sources that inspired the paper:\n\n1. \"Sequence clock a dynamic resource orchestrator for serverless architectures\" [b4]\n2. \"Aquatope qos anduncertainty aware resource management for multi stage serverless workflows\" [b5]\n3. \"Gemini learning to manage cpu power for latency critical search engines\" [b8]\n4. \"Tackling performance variability due to ras mechanisms with pid controlled dvfs\" [b10]\n5. \"Sebs a serverless benchmark suite for function as a service computing\" [b15]\n\nThe reference [b4], [b5] are mentioned as a motivation for the authors' work. The [b8], [b10] are mentioned as a technique the authors have adopted and improved upon. The [b15] is mentioned as a resource that inspired the authors' evaluation methodology. \n\nHere is the confidence score for each source:\n\n{\n\"b4\": 0.9,\n\"b5\": 0.9,\n\"b8\": 0.85,\n\"b10\": 0.85,\n\"b15\": 0.8\n}\n\nThe confidence score is given based on the context in which these references were mentioned in the text, with \"inspired by\" or \"based on\" phrases giving a higher score.", "Based on the provided text, the paper is about leveraging Dynamic Voltage and Frequency Scaling (DVFS) for Function-as-a-Service (FaaS) workflows to regulate the end-to-end latency of serverless workflows. The new method uses a PID controller to regulate the end-to-end execution time of serverless chains across nodes, considering the propagated latency (slack) in each step. The key source papers, as indicated by the text, are as follows:\n\n1. \"Sequence clock a dynamic resource orchestrator for serverless architectures\" [b4]: The paper uses this reference to compare its method with the CPU quota scaling method for regulating end-to-end serverless chain execution time, which is used in this source paper.\n\n2. \"Gemini learning to manage CPU power for latency critical search engines\" [b8]: This paper is compared to the current work for its use of DVFS for single-service latency regulation. The current paper outperforms Gemini in power efficiency.\n\n3. \"Tackling performance variability due to RAS mechanisms with PID controlled DVFS\" [b10]: This paper is mentioned as being inadequate by design for addressing the needs of the updated cloud computing architecture model, inspiring the current paper to develop a more suitable solution.\n\n4. \"Serverless in the wild characterizing and optimizing the serverless workload at a large cloud provider\" [b1]: This paper is cited for its findings on latency in FaaS deployments, which the current paper uses to evaluate the effectiveness of its proposed solution.\n\n5. \"SeBS a serverless benchmark suite for Function-as-a-Service computing\" [b15]: This paper provides the benchmark suite used in the current paper to evaluate the efficacy and efficiency of its proposed solution.\n\n6. \"Workload characterization and DAG transformation for serverless workflows\" [b16]: This paper is mentioned to resemble the typical workflows used in the current paper for large-scale deployment scenarios.\n\nThe JSON format of the result is as follows:\n\n{\n  \"b4\": 0.9,\n  \"b8\": 0.9,\n  \"b10\": 0.8,\n  \"b1\": 0.8,\n  \"b15\": 0.7,\n  \"b16\": 0.7\n} \n\nPlease note that the confidence scores are subjective and depend on the interpretation of how directly these papers have influenced the main contributions of the current work.", "Based on the given text and the author notes, the paper presents a method for managing power consumption in a Function-as-a-service (FaaS) scenario by using a PID (Proportional-Derivative-Integral) controller to adjust Dynamic Voltage and Frequency Scaling (DVFS). The paper seems to be directly influenced by several previous works, and the most relevant source papers can be identified as follows:\n\n1. [b7] - \"retail opting for learning simplicity to enable qos aware power management in the cloud\". This paper seems to have influenced the use of DVFS for power management in the current paper.\n2. [b4] - \"sequence clock a dynamic resource orchestrator for serverless architectures\" and [b5] - \"aquatope qos anduncertainty aware resource management for multi stage serverless workflows\". These papers are mentioned in the context of CPU quotas to regulate the end-to-end serverless chain execution time and seem to have inspired the authors.\n3. [b10] - \"tackling performance variability due to ras mechanisms with pid controlled dvfs\". This paper has been mentioned in the context of existing DVFS solutions that the authors deemed inadequate, suggesting that it might have been an inspiration for the presented approach.\n4. [b8] - \"gemini learning to manage cpu power for latency critical search engines\". This paper was used as a benchmark to demonstrate the performance improvement of the proposed method, indicating that it was a significant influence.\n\nThe confidence scores for these source papers cannot be determined exactly based on the given text and author notes. However, it can be inferred that they have a strong influence on the presented work.\n\nIn JSON format the results would look like this:\n\n```json\n{\n    \"b7\": 0.8,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b10\": 0.75,\n    \"b8\": 0.85\n}\n```\nPlease note that the confidence scores are estimated based on the text and author notes, and the actual influence of each source paper on the presented work may vary."], "62ea18d35aee126c0fca1369": ["Based on the text of the academic paper, it seems the author's main challenges are dealing with long texts and finding a way to apply existing short text models to these long texts. The author's solution appears to be the SLED (SLiding-Encoder and Decoder) method which involves splitting long documents into overlapping chunks and encoding each chunk independently. \n\nThe author mentions certain papers and models throughout the text that seem to have inspired their work. \n\nThese include:\n- Transformer-based pretrained language models [b42] [b9] [b23] [b34] [b3], which have been successful in natural language processing.\n- Efficient transformer variants [b19] [b2] [b51] [b12] which are used for long-range language tasks.\n- BART [b23] and T5 [b34], which are pre-trained encoder-decoder models used in the SLED's framework.\n- Fusion-in-decoder [b17], which appears to be the method that the author uses for their SLED model.\n- LED [b2], a local sparse attention model, which the author compares their SLED model to.\n\nI will now return the source papers in a JSON format:\n\n```\n{\n\"b42\": 0.9, \n\"b9\": 0.8, \n\"b23\": 1.0, \n\"b34\": 1.0, \n\"b3\": 0.8, \n\"b19\": 0.8, \n\"b2\": 0.9, \n\"b51\": 0.8, \n\"b12\": 0.8, \n\"b17\": 1.0\n}\n```\n\nI have assigned a confidence score of 1.0 to references that the author directly states inspired their work, and a score of 0.9 or 0.8 to those that are mentioned in a less direct manner.", "Based on the paper text and the context provided, the relevant source papers that directly inspired this work can be identified as follows:\n\n- \"b23\": BART denoising sequence to sequence pre training for natural language generation translation and comprehension (Confidence Score: 0.9)\n- \"b34\": Exploring the limits of transfer learning with a unified text to text transformer (Confidence Score: 0.9)\n- \"b17\": Leveraging passage retrieval with generative models for open domain question answering (Confidence Score: 0.8)\n- \"b2\": Longformer the long document transformer (Confidence Score: 0.7)\n- \"b12\": LongT5 efficient textto text transformer for long sequences (Confidence Score: 0.7)\n- \"b38\": A scaling laws vs model architectures how does inductive bias influence scaling (Confidence Score: 0.6)\n- \"b42\": Attention is all you need (Confidence Score: 0.6)\n\nThese papers are likely to have provided the key inspirations for the development of the SLED model, as well as the application of the model to long-range language understanding tasks. The confidence scores are estimated based on the frequency and context of the references made to these source papers in the text.", "Based on the text of the academic paper and the criteria provided, the source papers that most inspired the authors can be identified as follows:\n\n1. \"Attention is All You Need\" [b42]\n2. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b9]\n3. \"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\" [b23]\n4. \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" [b34]\n5. \"Longformer: The Long-Document Transformer\" [b2]\n6. \"Scrolls: Standardized Comparison over Long Language Sequences\" [b37]\n7. \"LongT5: Efficient Text-to-Text Transformer for Long Sequences\" [b12]\n\nAccording to the contextual relevance, the confidence scores for these papers are as follows:\n{\"b42\": 0.9, \"b9\": 0.8, \"b23\": 0.8, \"b34\": 0.7, \"b2\": 0.7, \"b37\": 0.7, \"b12\": 0.8}", "Based on the given text, the paper proposes a method for handling long sequence text with short text pre-trained models, named as SLED. The approach splits long text into overlapping short texts, encodes each short text individually, then concatenates the results for further processing. The paper leverages pre-trained models such as BARTbase, T5base, and BARTlarge for long sequence extension.\n\nFrom the text, the following references seem to be most influential in developing the paper's core contributions:\n\n1. [b23] - BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension: Mentioned as the base for their pre-trained model, suggesting a direct influence on their method's development.\n\n2. [b34] - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5): Mentioned alongside BART as a model used in their framework.\n\n3. [b12] - LongT5: Efficient Text-to-Text Transformer for Long Sequences: The paper compares its results with this model, suggesting a benchmark influence.\n\n4. [b19] - Reformer: The Efficient Transformer: Mentioned as a more efficient transformer variant, suggesting an inspiration for their method's efficiency.\n\n5. [b2] - Longformer: The Long Document Transformer: Influential as it's also an efficient transformer variant.\n\n6. [b17] - Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering: The paper's method is similar to the Fusion in Decoder (FiD) approach introduced by this work.\n\nBased on the mentioned references, the confidence score can be assigned as follows:\n\n{\"b23\": 0.9, \"b34\": 0.9, \"b12\": 0.8, \"b19\": 0.7, \"b2\": 0.7, \"b17\": 0.8}\n\nPlease note that the scores are estimated based on the frequency and context of the references in the text and can have some degree of subjectivity.\n", "Based on the text and provided information, the following are the source papers for the current paper:\n\n1. \"Attention is all you need\" [b42]: This paper introduces the transformer architecture which is the base model for the current paper. The current paper uses the transformer model as its backbone and modifies it to handle long sequences.\n\n2. \"Bart: Denoising sequence to sequence pre-training for natural language generation, translation and comprehension\" [b23]: The current paper specifically mentions that it uses the BART model as a pre-trained encoder-decoder model in its SLED framework. \n\n3. \"Exploring the limits of transfer learning with a unified text-to-text transformer\" [b34]: The T5 model is also mentioned as one of the pre-trained models used in the current paper's SLED framework.\n\n4. \"Longformer: The Long-Document Transformer\" [b2]: The current paper mentions efficient transformer variants and specifically cites Longformer. \n\n5. \"Reformer: The Efficient Transformer\" [b19]: The current paper discusses transformer variants that are more efficient, and cites Reformer as an example of such models.\n\n6. \"Efficient Content-Based Sparse Attention with Routing Transformers\" [b36]: This paper is referenced in the context of discussing sparse attention variants.\n\n7. \"Big Bird: Transformers for Longer Sequences\" [b51]: The current paper mentions Big Bird in the context of discussing models that handle longer sequences.\n\n8. \"Scrolls: Standardized Comparison Over Long Language Sequences\" [b37]: The current paper uses the SCROLLS benchmark for evaluating its proposed method.\n\nThe confidence score for each of these papers would be 1 as they are all directly mentioned and discussed in the context of the current paper's method and evaluation.\n\nIn JSON format, the result would be:\n\n{\n  \"b42\": 1,\n  \"b23\": 1,\n  \"b34\": 1,\n  \"b2\": 1,\n  \"b19\": 1,\n  \"b36\": 1,\n  \"b51\": 1,\n  \"b37\": 1\n}"], "64ec1b763fda6d7f0626f480": ["Based on the text of the paper and the author's notes, the most relevant source papers are:\n\n- [b21]: \"Efficient long text understanding with short text models\" - This paper is clearly referenced as the basis for the improvements made in the paper, specifically regarding text segmentation and token selection.\n- [b2]: \"Longformer: the long document transformer\" - This paper is referenced in the discussion about previous approaches to long-text processing, which suggests it might have been an influence.\n- [b40]: \"Don't give me the details, just the summary: Topic-aware convolutional neural networks for extreme summarization\" - The use of reinforcement learning in NLP, which is a method also used in this paper, is mentioned.\n- [b54]: \"Proximal policy optimization algorithms\" - This paper is directly cited for its method, which is adopted in the paper.\n- [b49]: \"Is reinforcement learning not for natural language processing? Benchmarks, baselines, and building blocks for natural language policy optimization\" - It is cited as the inspiration for the design of the token selector.\n- [b8]: \"BERT: Pre-training of deep bidirectional transformers for language understanding\" - BERT is used as the basis for pre-trained language models in the paper.\n\nBased on the above, the JSON return will be:\n{\n\"b21\": 1,\n\"b2\": 0.9,\n\"b40\": 0.8,\n\"b54\": 0.8,\n\"b49\": 0.8,\n\"b8\": 0.7\n}", "Based on the text and references provided, the following references are identified as the most pertinent source papers:\n\n1. \"Efficient long text understanding with short text models\" - 'b21': The paper mentions the SLED method which is the basis for the improvements made in the given paper. This indicates a direct influence and motivation from this work.\n\n2. \"Proximal policy optimization algorithms\" - 'b54': This paper's algorithm is used to train the selector in the given paper, indicating a significant methodological influence.\n\n3. \"Reinforcement learning not for natural language processing benchmarks baselines and building blocks for natural language policy optimization\" - 'b49': This paper's approach inspired the design of the token selector in the given paper.\n\n4. \"Longformer the long document transformer\" - 'b2': This is mentioned as part of the challenge the paper is addressing and may have influenced the methods used to handle long-sequence processing.\n\n5. \"Reformer the efficient transformer\" - 'b25': This paper's methods may have influenced the efficient transformer methods used in the given paper.\n\n6. \"BERT pre-training of deep bidirectional transformers for language understanding\" - 'b8': This paper's method of using [BOS] and [EOS] tokens to represent sentence semantics might have influenced the design of the aligning scheme in the given paper.\n\nHere is the JSON format of the result:\n\n```\n{\n\"b21\": 0.9,\n\"b54\": 0.9,\n\"b49\": 0.8,\n\"b2\": 0.8,\n\"b25\": 0.8,\n\"b8\": 0.8\n}\n``` \n\nThe confidence scores are subjective and may vary. They are based on the degree of relevance and the influence of the source papers on the methodology of the given paper.", "Based on the text of the paper and the authors' notes, the source papers that most inspired them seem to be:\n\n1. 'Efficient long text understanding with short text models' [b21]: The authors mentioned that their work is an improvement upon the method SLED, which is proposed in this paper. They adopted aligned tokens and reinforcement learning from this paper.\n\n2. 'Proximal policy optimization algorithms' [b54]: The authors mention using the Proximal Policy Optimization (PPO) algorithm to train the token selector, which is proposed in this paper.\n\n3. 'Don't give me the details, just the summary topic aware convolutional neural networks for extreme summarization' [b40]: The authors mentioned that they were inspired by the success of reinforcement learning in NLP, which is discussed in this paper.\n\n4. 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding' [b8]: The authors used BERT base and BERT large pre-trained models for their experiments, which are proposed in this paper.\n\n5. 'Longformer: the Long-Document Transformer' [b2]: The authors discuss the challenges of long text processing tasks, which are a primary focus of this paper.\n\n6. 'Efficient Attentions for Long Document Summarization' [b20]: The authors compare their work with this paper in long-text summarization tasks.\n\nSo, the JSON format of this result would be:\n\n{\n\"b21\": 1,\n\"b54\": 0.9,\n\"b40\": 0.8,\n\"b8\": 0.8,\n\"b2\": 0.7,\n\"b20\": 0.7\n}", "Based on the given text and instructions, the source papers that most influenced this paper seem to be:\n\n- \"Efficient long text understanding with short text models\" [b21]\n- \"Proximal policy optimization algorithms\" [b54]\n- \"A neural attention model for abstractive sentence summarization\" [b51]\n- \"Efficient attentions for long document summarization\" [b20]\n- \"Unlimiformer longrange transformers with unlimited length input\" [b3]\n- \"Longformer the long document transformer\" [b2]\n\nThe confidence score for each source paper can be calculated based on the number of times they are mentioned and the context in which they are discussed in the paper. However, without the full paper and further context, it is difficult to give an accurate confidence score. The JSON format result would look something like this:\n\n{\n    \"b21\": 0.9,\n    \"b54\": 0.85,\n    \"b51\": 0.8,\n    \"b20\": 0.8,\n    \"b3\": 0.8,\n    \"b2\": 0.8\n}", "Based on the given text, the primary challenges outlined in the paper are the computational consumption of self-attention operations with long text processing tasks, and the difficulty of capturing the global semantic information among different chunks when using sub-sequence processing methods. The author's novel contributions include the Chunk, Align, and Select (SimCAS) method, the sequential batch alignment (SBA) operation, and the reinforcement learning-based token selector. \n\nThe key phrases that indicate a strong reliance on previous research are \"inspired by the recent success of reinforcement learning in NLP [b40]\", \"we adopt the Proximal Policy Optimization (PPO) [b54] algorithm\", and \"Inspired by [b49]\". \n\nFrom this analysis, the source papers that have most inspired the author are:\n\n- [b40]: \"Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization\".\n- [b54]: \"Proximal policy optimization algorithms\".\n- [b49]: \"Is Reinforcement Learning Not for Natural Language Processing Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization\".\n\nIn addition, the author mentions a few other papers directly relevant to the paper's novel contributions, and they are:\n\n- [b21]: \"SLED - Efficient Long Text Understanding with Short Text Models\"\n- [b2]: \"Longformer - The Long Document Transformer\"\n- [b69]: \"Big Bird - Transformers for Longer Sequences\"\n \nSo the most pertinent source papers are:\n\n{\"b40\": 0.9, \"b54\": 0.8, \"b49\": 0.8, \"b21\": 0.7, \"b2\": 0.7, \"b69\": 0.7}"], "636482d790e50fcafdccab10": ["{\n  \"b24\": 0.9, \n  \"b41\": 0.8,\n  \"b45\": 0.85\n}", "{\n    \"b22\": 0.9,\n    \"b23\": 0.9,\n    \"b45\": 0.8\n}", "{\n    \"b24\": 1.0,\n    \"b25\": 0.9,\n    \"b41\": 0.9,\n    \"b22\": 0.8,\n    \"b23\": 0.8,\n    \"b45\": 0.9\n}", "{\n  \"b24\": 0.9,\n  \"b22\": 0.8,\n  \"b23\": 0.8,\n  \"b45\": 0.9\n}", "{\n  \"b22\": 0.8,\n  \"b23\": 0.8,\n  \"b24\": 0.9,\n  \"b41\": 0.9,\n  \"b45\": 0.9\n}"], "62c4fd9a5aee126c0fad6f58": ["Based on the text and the titles of the references, the most pertinent source papers that inspire the author are:\n\n1. \"Fast zero-shot image tagging\" [b46]: The author is motivated by this work to develop a ML-ZSL method.\n2. \"Semantic diversity learning for zero-shot multi-label classification\" [b0]: This paper is used as a reference for the ML-ZSL methods.\n3. \"Open vocabulary object detection via vision and language knowledge distillation\" [b10]: The author takes inspiration from this paper to implement open-vocabulary classification.\n4. \"Learning to prompt for open vocabulary object detection with vision language model\" [b8]: The author is inspired by this work to propose label embedding through prompt tuning.\n5. \"Learning transferable visual models from natural language supervision\" [b33]: This paper is used as a reference for implementing the vision and language pre-training model CLIP.\n6. \"Vision and language transformer without convolution or region supervision\" [b15]: This work inspires the author to utilize multi-modal knowledge from VLP models.\n7. \"Discriminative region-based multi-label zero-shot learning\" [b30]: The author uses this work as a reference for the ML-ZSL methods.\n\nThe output in the required format is:\n\n{\"b46\": 1, \"b0\": 0.9, \"b10\": 1, \"b8\": 0.9, \"b33\": 0.9, \"b15\": 0.8, \"b30\": 0.8}", "Based on the provided paper text and the titles of the references, the source papers that inspired the given paper most are:\n\n- \"Fast zero-shot image tagging\" [b46]: This paper is mentioned in the context of discussing the existing methods of multi-label zero-shot learning (ML-ZSL), which is a key challenge identified in the given paper. The authors of the given paper also mention their main contributions in improving upon the ML-ZSL methods.\n\n- \"Learning transferable visual models from natural language supervision\" [b33]: This reference is mentioned multiple times in the context of discussing the use of Vision and Language Pre-training (VLP) models, which form a key part of the proposed algorithm in the given paper. The proposed method in the given paper is directly motivated by the image-text matching ability of these VLP models.\n\n- \"Open-vocabulary object detection using captions\" [b45]: This paper is mentioned as a key work in the area of open-vocabulary classification, which is a main topic of the given paper.\n\n- \"Learning to prompt for vision language models\" [b48]: This paper is mentioned in the context of discussing the technique of prompt tuning, which is used in the given paper's proposed algorithm.\n\n- \"Semantic diversity learning for zero-shot multi-label classification\" [b0]: This paper is also mentioned when discussing existing methods of ML-ZSL, and the given paper's contributions are compared to it.\n\nThe JSON format of the results is: \n{ \"b46\": 0.9, \"b33\": 0.9, \"b45\": 0.8, \"b48\": 0.8, \"b0\": 0.8 }", "The source papers that inspired the given paper are:\n- [b46]: Fast zero-shot image tagging\n- [b12]: A shared multi-attention framework for multi-label zero-shot learning\n- [b30]: Discriminative region-based multi-label zero-shot learning\n- [b33]: Learning transferable visual models from natural language supervision\n- [b7]: An image is worth x words: Transformers for image recognition at scale in iclr the ninth international conference on learning representations\n- [b10]: Open-vocabulary object detection via vision and language knowledge distillation\n- [b2]: Uniter: Learning universal image-text representations\n\nThe confidence scores for the relevance of each of these source papers to the given paper could be as follows:\n\n- [b46]: 0.9\n- [b12]: 0.85\n- [b30]: 0.8\n- [b33]: 0.95\n- [b7]: 0.9\n- [b10]: 0.85\n- [b2]: 0.8\n\nThese scores are based on the frequency of references to these papers and the context in which they are mentioned. The confidence scores range from 0 (not at all relevant) to 1 (very relevant). \n\nThe result in JSON format:\n\n{\n  \"b46\": 0.9,\n  \"b12\": 0.85,\n  \"b30\": 0.8,\n  \"b33\": 0.95,\n  \"b7\": 0.9,\n  \"b10\": 0.85,\n  \"b2\": 0.8\n}", "Based on the provided text and the titles of the references, the main source papers can be identified as follows:\n1. \"semantic diversity learning for zero shot multi label classification\" (b0): The paper mentions this work in the context of aligning image embeddings with relevant label embeddings in zero shot learning.\n2. \"a shared multi attention framework for multi label zero shot learning\" (b12): The text discusses this work in relation to the use of an attention module to capture both local and global features for better object recognition.\n3. \"learning transferable visual models from natural language supervision\" (b33): This paper is referenced multiple times, particularly in relation to the use of VLP models in multi-label classification tasks. \n4. \"open vocabulary object detection via vision and language knowledge distillation\" (b10): This work is mentioned in context of exploring open-vocabulary classification in object detection.\n5. \"open vocabulary one stage detection with hierarchical visual language knowledge distillation\" (b28): This paper is mentioned in relation to the use of knowledge distillation to facilitate the alignment of image and label embeddings.\n6. \"prefix tuning optimizing continuous prompts for generation\" (b23): This work is referenced in relation to the method of updating the label embeddings iteratively.\n\nBased on these observations, the JSON format of the main source papers and their confidence scores is as follows:\n\n{\n\"b0\": 0.85,\n\"b12\": 0.8,\n\"b33\": 0.9,\n\"b10\": 0.8,\n\"b28\": 0.8,\n\"b23\": 0.8\n}", "Based on the text of the academic paper provided and the titles of the reference papers, the most pertinent source papers that inspire the author the most could be:\n\n- 'b33': 'Learning transferable visual models from natural language supervision' - This paper seems to be the main source of inspiration for the development of the multi-modal knowledge transfer (MKT) framework proposed in the paper. The author mentions using the CLIP model (which is the focus of this source paper) in multiple instances throughout the text, indicating a heavy reliance on it.\n\n- 'b46': 'Fast zero-shot image tagging' - This paper is mentioned as a previous ML-ZSL method. The author seems to build upon the ideas presented in this paper, particularly about transferring knowledge between seen and unseen labels.\n\n- 'b20': 'Unicoder VL: a universal encoder for vision and language by cross-modal pre-training' - The author notes the importance of vision and language pre-training (VLP) models throughout the text, which is the main focus of this source paper.\n\n- 'b12': 'A shared multi-attention framework for multi-label zero-shot learning' - This paper appears to be an important reference for the author, as it proposes an attention module to capture local and global features, which is a method also used in the paper's proposed framework.\n\n- 'b30': 'Discriminative region-based multi-label zero-shot learning' - This paper is mentioned multiple times and seems to be a significant source of inspiration for the author, particularly in the development of methods for identifying unseen labels in an image.\n\n- 'b10': 'Open-vocabulary object detection via vision and language knowledge distillation' - This paper is related to the open-vocabulary classification task and the use of knowledge distillation, both of which are key elements in the author's proposed framework.\n\nBased on this analysis, the source papers are:\n\n```\n{\n    \"b33\": 0.9,\n    \"b46\": 0.85,\n    \"b20\": 0.8,\n    \"b12\": 0.75,\n    \"b30\": 0.85,\n    \"b10\": 0.8\n}\n```\n\nThe confidence score represents the degree of influence these papers had on the author's work, with 1 being the most influential."], "65123f453fda6d7f06e54a4b": ["Based on the text, the source papers that inspired the given paper most are:\n\n1. \"Learning Transferable Visual Models from Natural Language Supervision\" (b48) - This is the original paper introducing the CLIP model, which is the main backbone of the model in the given paper. It is mentioned multiple times, indicating the strong reliance on this work.\nConfidence score: 1\n\n2. \"Graph learning regularization and transfer learning for few shot event detection\" (b30) - The authors mention graph learning and how it has shown potential in modeling data knowledge structure. They express their motivation to exploit graph learning to model the structure knowledge for downstream tasks when tuning VLMs.\nConfidence score: 0.9\n\n3. \"Knowledge graph transfer network for few shot recognition\" (b6) - This paper seems to be a key reference for the authors' approach to using graph learning in the context of few-shot learning.\nConfidence score: 0.85\n\n4. \"Clip-guided prototype modulating for few-shot action recognition\" (b63) - This paper seems to be related to their method of using CLIP as backbone and apply it to few-shot learning tasks.\nConfidence score: 0.8\n\n5. \"Task Residual for Tuning Vision Language Models\" (b70) - This paper is mentioned in relation to the limitations of previous adapter-style works on ETL, which the authors propose to overcome with their new strategy.\nConfidence score: 0.75\n\n6. \"Clip-Adapter: Better Vision-Language Models With Feature Adapters\" (b16) - This is another key paper that the authors reference when discussing the limitations of previous research, it appears to have played a significant role in shaping the authors' research direction and their development of the GraphAdapter.\nConfidence score: 0.7\n\n7. \"Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification\" (b74) - This paper seems to be directly related to the idea of few-shot classification and adapter-style tuning, which are central concepts in the given paper.\nConfidence score: 0.7\n\nBased on the content of the paper, these references seem to be the most relevant and influential to the work being presented. \n\nThe JSON format:\n{\n\"b48\": 1,\n\"b30\": 0.9,\n\"b6\": 0.85,\n\"b63\": 0.8,\n\"b70\": 0.75,\n\"b16\": 0.7,\n\"b74\": 0.7\n}", "Based on the paper's text and the titles of the references, the most pertinent source papers that inspired this work are:\n\n1. \"Learning Transferable Visual Models from Natural Language Supervision\" [b48]\n2. \"Clip-Adapter: Better Vision-Language Models with Feature Adapters\" [b16]\n3. \"Task Residual for Tuning Vision Language Models\" [b70]\n4. \"Graph Learning Regularization and Transfer Learning for Few Shot Event Detection\" [b30]\n5. \"Graph Signal Processing: Graph Neural Network and Graph Learning on Biological Data: A Systematic Review\" [b32]\n6. \"Semi-Supervised Classification with Graph Convolutional Networks\" [b26]\n7. \"Learning to Prompt for Vision Language Models\" [b77]\n8. \"Tip-Adapter: Training-Free Adaptation of Clip for Few-Shot Classification\" [b74]\n\nAs the paper mentions the explicit use of methods from these papers and these references are directly tied to the proposed GraphAdapter method, these are the source papers that most likely inspired the authors. \n\nThe json format result will be:\n\n{\n\"b48\": 0.9,\n\"b16\": 0.85,\n\"b70\": 0.85,\n\"b30\": 0.8,\n\"b32\": 0.8,\n\"b26\": 0.8,\n\"b77\": 0.85,\n\"b74\": 0.85\n} \n\nNote: The confidence score is a subjective measure and may vary based on further analysis of the paper. It is a measure of how strongly the paper appears to be influenced by the source paper, with 1 being the maximum possible influence.", "Based on the given text, the author has been largely inspired by the following papers:\n\n1. \"graphadapter tuning vision language models with dual knowledge graph\" [None]\n\n2. \"clip-adapter better vision language models with feature adapters\" [b16]\n   \n3. \"learning transferable visual models from natural language supervision\" [b48]\n\n4. \"pre train prompt and predict a systematic survey of prompting methods in natural language processing\" [b35]\n\n5. \"knowledge graph transfer network for few shot recognition\" [b6]\n\n6. \"a survey on visual transfer learning using knowledge graphs\" [b42]\n\n7. \"svl adapter self supervised adapter for vision language pretrained models\" [b45]\n\n8. \"tip adapter trainingfree adaption of clip for few shot classification\" [b74]\n\nThe confidence scores need to be determined based on the degree of direct influence these papers have on the research, which can only be assessed by the author or through a more detailed text analysis.\n\nHere is the JSON format:\n\n{\n    \"None\": 0.9,\n    \"b16\": 0.8,\n    \"b48\": 0.7,\n    \"b35\": 0.6,\n    \"b6\": 0.8,\n    \"b42\": 0.8,\n    \"b45\": 0.6,\n    \"b74\": 0.7\n}", "Based on the text and the criteria mentioned, the key papers that inspired this research could be:\n\n1. \"Learning Transferable Visual Models from Natural Language Supervision\" [b48]: The paper is explicitly mentioned as a source of inspiration when introducing the GraphAdapter methodology, and CLIP is referred to multiple times as a foundation for the described approach.\n\n2. \"Task Residual for Tuning Vision Language Models\" [b70]: This paper is directly compared with the proposed approach and is clearly considered when outlining the limitations of previous adapter-style tuning methods. \n\n3. \"Graph Convolutional Networks\" [b26]: This paper is not directly mentioned in the text, but the authors' use of Graph Convolutional Networks (GCN) in the GraphAdapter methodology suggests that it was a significant influence.\n\n4. \"Learning to Prompt for Vision Language Models\" [b77]: This paper's method, CoOp, is frequently used as a baseline for comparison, indicating its influence on the authors' work.\n\n5. \"CLIP-Adapter: Better Vision Language Models with Feature Adapters\" [b16]: The authors frequently mention this paper when discussing the limitations of previous work and the motivation for their approach, indicating its importance in shaping their research.\n\n6. \"Efficient Transfer Learning for NLP\" [b22]: While not directly mentioned, the broader topic of efficient transfer learning discussed in this paper is central to the authors' research.\n\nIn JSON format, the confidence scores for these papers could look as follows:\n\n{ \n\"b48\": 0.9, \n\"b70\": 0.85, \n\"b26\": 0.75, \n\"b77\": 0.8, \n\"b16\": 0.85, \n\"b22\": 0.7\n}", "Based on the text analysis and the author's note, the source papers that inspired the given paper are as follows:\n\n1. \"graphadapter tuning vision language models with dual knowledge graph\" (Reference: None, Confidence Score: 1.0)\n2. \"learning transferable visual models from natural language supervision\" (Reference: b48, Confidence Score: 0.9)\n3. \"clip adapter better vision language models with feature adapters\" (Reference: b16, Confidence Score: 0.8)\n4. \"the many faces of robustness a critical analysis of out of distribution generalization\" (Reference: b20, Confidence Score: 0.7)\n5. \"knowledge embedded routing network for scene graph generation\" (Reference: b8, Confidence Score: 0.7)\n6. \"graph learning regularization and transfer learning for few shot event detection\" (Reference: b30, Confidence Score: 0.7)\n7. \"a survey on visual transfer learning using knowledge graphs\" (Reference: b42, Confidence Score: 0.6)\n8. \"task residual for tuning vision language models\" (Reference: b70, Confidence Score: 0.8)\n\nPlease note that the confidence scores are given on a scale of 0 to 1 based on the influence of the source papers on the given paper's contributions, with 1 being the most influential."], "6531e2ca939a5f4082f5d5d2": ["Based on the text, the source papers that most inspired the author are:\n\n1. \"A top down method for performance analysis and counters architecture\" [b2]\n2. \"Symbiotic job scheduling on the IBM power\" [b3]\n3. \"Improving IBM power performance through symbiotic job scheduling\" [b4]\n4. \"ARMv8-A performance monitoring user guide\" [b19]\n\nThese papers are mentioned in the context of methodological inspiration and direct comparison. They suggest a strong reliance on the mentioned research for developing the paper's core contributions. \n\nIn the JSON format:\n\n{\n\"b2\": 0.9,\n\"b3\": 0.9,\n\"b4\": 0.8,\n\"b19\": 0.8\n}", "Based on the text of the academic paper, the most pertinent source papers that inspired the author's work are:\n\n- \"A top down method for performance analysis and counters architecture\" [b2]: The author mentions that some existing approaches have been proposed for Intel [b2] and IBM [b3] , [b4] processors. However, they cannot be applied to ARM processors because the performance counter architecture widely differs across them.\n\n- \"Symbiotic job scheduling on the IBM POWER\" [b3]: The author uses this paper as a comparison point, indicating that their approach is simpler than the previous approaches proposed for Intel [b2] and IBM [b3] , [b4].\n\n- \"Improving IBM POWER performance through symbiotic job scheduling\" [b4]: The author uses this paper as a comparison point, indicating that their approach is simpler than the previous approaches proposed for Intel [b2] and IBM [b3] , [b4].\n\n- \"ARMv8-M Performance Monitoring User Guide\" [b19]: This paper provides the technical details required for the author's work on ARM processors.\n\n- \"Maximum Matching and a Polyhedron with 0, 1-Vertices\" [b20]: The author uses the Blossom algorithm [b20] in their thread-to-core allocation strategy.\n\n- \"SMiTe: Precise QoS Prediction on Real System SMT Processors to Improve Utilization in Warehouse Scale Computers\" [b17]: The author discussed this work as a comparison point. While this work uses a regression model that combines sensitivity and contentiousness to estimate performance interference in an Intel Sandy Bridge processor, the author's work focuses on ARM processors and is simpler.\n\nHere is the JSON format of the list:\n\n{\n  \"b2\": 0.9,\n  \"b3\": 0.9,\n  \"b4\": 0.9,\n  \"b19\": 0.8,\n  \"b20\": 0.8,\n  \"b17\": 0.7\n}", "Based on the text, the main challenges outlined in the paper are: (1) characterizing the synergy of applications with the performance counters available in the target processor, and (2) estimating the most synergistic pairs of applications. The author proposes a new algorithm named SYNPA to tackle these challenges. The algorithm uses a simple three-variable model at the dispatch stage and a linear regression model. \n\nBased on the text and titles, the following references seem to have influenced the paper's main contributions:\n\n1. \"A top-down method for performance analysis and counters architecture\" [b2]: The paper uses a top-down approach to solve the problem.\n2. \"Symbiotic job scheduling on the IBM Power\" [b3]: The author mentions this paper when discussing existing approaches, indicating that it may have influenced their work.\n3. \"A performance counter architecture for computing accurate CPI components\" [b15]: This paper might have inspired the use of performance counters in the proposed algorithm.\n4. \"ARMv8-M performance monitoring user guide\" [b19]: This document likely provided crucial information about ARM processor's performance counters, which the author used in their algorithm.\n5. \"Maximum matching and a polyhedron with vertices\" [b20]: The author uses the Blossom algorithm proposed in this paper to find the optimal combination of applications.\n6. \"System Level Performance Metrics for Multiprogram Workloads\" [b23]: This paper might have influenced the author's approach to performance estimation.\n\nReturn the result in the json format:\n\n{\n  \"b2\": 0.9,\n  \"b3\": 0.8,\n  \"b15\": 0.7,\n  \"b19\": 0.9,\n  \"b20\": 0.8,\n  \"b23\": 0.7\n}", "Based on the given text, the source papers that inspired the most are:\n\n1. \"Simultaneous multithreading maximizing on chip parallelism\" - This paper is directly mentioned in the introduction, indicating its fundamental influence on the topic. [b0]\n2. \"Marvell thunderx nextgeneration arm based server processor\" - This paper is also mentioned in the introduction and its findings are used to justify the research. [b1]\n3. \"A top down method for performance analysis and counters architecture\" - This paper is mentioned as a source of inspiration in the related work section, suggesting that it had a strong influence on the methodology developed in the paper. [b2]\n4. \"Symbiotic job scheduling on the IBM power\" - This paper is mentioned as a source of inspiration in the related work section, suggesting that it had a strong influence on the methodology developed in the paper. [b3]\n5. \"ARMv8.1A performance monitoring user guide\" - This paper is mentioned in the methodology section, indicating that it played a significant role in shaping the experimental design. [b19]\n\nThe confidence scores for these papers are 1, 0.9, 0.8, 0.7, and 0.6 respectively. It can be inferred from the text that these papers directly contributed to the methodologies and concepts used in the development of the paper.\n\nHere is the result in JSON format:\n\n{\n\"b0\": 1,\n\"b1\": 0.9,\n\"b2\": 0.8,\n\"b3\": 0.7,\n\"b19\": 0.6\n}", "Based on the text of the academic paper, the main challenges that were tackled by the author include: characterizing the synergy of applications with the performance counters available in the target processor and estimating the most synergistic pairs of applications.\n\nThe author proposed SYNPA, a thread allocation policy that selects synergistic applications and allocates them to the SMT cores of an ARM processor.\n\nThe key phrases and their corresponding references that indicate the source papers are:\n\n1. \"Simultaneous multithreading (SMT) [b0]\" - This indicates that the author was inspired by the work on SMT, specifically the paper \"Simultaneous Multithreading: Maximizing On-Chip Parallelism [b0]\".\n\n2. \"A few existing approaches have been proposed for Intel [b2] and IBM [b3], [b4] processors.\" - These references indicate that the author was motivated by the previous work on Intel and IBM processors.\n\n3. \"Despite ARM processors gaining popularity in the data center segment thanks to their high energy efficiency [b5]\" - This reference indicates that the author was inspired by the emerging popularity and high energy efficiency of ARM processors, as highlighted in the paper \"Article Processors Global Market [b5]\".\n\n4. \"In SMiTE [b17], authors propose a regression model that combines sensitivity and contentiousness to estimate performance interference in an Intel Sandy Bridge processor.\" - This reference indicates that the author was motivated by the work done in \"SMiTE: Precise QoS Prediction on Real-System SMT Processors to Improve Utilization in Warehouse Scale Computers [b17]\".\n\n5. \"Other works establish linear regression models that use performance counters to estimate the performance of each combination of applications. Two early attempts [b13], [b14] focused on a simulated generic and simplified processor.\" - The papers \"Methods for Modeling Resource Contention on Simultaneous Multithreading Processors [b13]\" and \"Probabilistic Job Symbiosis Modeling for SMT Processor Scheduling [b14]\" inspired the author's work on establishing a regression model using performance counters.\n\nBased on these key phrases and the novel contributions of the paper, the most pertinent source papers would be [b0], [b2], [b3], [b4], [b5], [b13], [b14], and [b17].\n\nResult in json format:\n{\n    \"b0\": 0.9,\n    \"b2\": 0.75,\n    \"b3\": 0.75,\n    \"b4\": 0.75,\n    \"b5\": 0.85,\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b17\": 0.85\n}"], "6514e2043fda6d7f062dc9f8": ["{\n    \"b39\": 0.9,\n    \"b5\": 0.8,\n    \"b30\": 0.8,\n    \"b2\": 0.8,\n    \"b10\": 0.9,\n    \"b23\": 0.9\n}", "{\n  \"b39\": 0.9,\n  \"b5\": 0.85,\n  \"b30\": 0.85,\n  \"b2\": 0.85,\n  \"b18\": 0.75,\n  \"b10\": 0.75,\n  \"b23\": 0.75\n}", "{\n  \"b39\": 0.9,\n  \"b5\": 0.8,\n  \"b30\": 0.8,\n  \"b2\": 0.8,\n  \"b10\": 0.7,\n  \"b23\": 0.7\n}", "{\n  \"b39\": 0.9,\n  \"b5\": 0.8,\n  \"b30\": 0.8,\n  \"b2\": 0.8,\n  \"b10\": 0.7,\n  \"b23\": 0.7\n}", "{\n    \"b39\": 0.9,\n    \"b5\": 0.9,\n    \"b30\": 0.8,\n    \"b2\": 0.8,\n    \"b10\": 0.7,\n    \"b23\": 0.7\n}"], "6531e2ca939a5f4082f5d4fe": ["Based on the provided text, the key phrases indicating inspiration or reliance on previous research, and the novel methods proposed by the author, the most pertinent source papers are:\n\n1. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b5]. This paper is frequently mentioned throughout the text and clearly influenced the methods used in the paper. The author of the analyzed paper uses BERT for initialization and for handling texts in the pretraining process.\n\n2. \"R-HGNN: Heterogeneous Graph Representation Learning with Relation Awareness\" [b41]. This work is used to implement the auxiliary heterogeneous graph neural network in the proposed pretraining framework.\n\n3. \"Heterogeneous Graph Transformer\" [b14]. The author uses this paper's OAG-Venue dataset for experiments.\n\n4. \"Node Vec: Scalable Feature Learning for Networks\" [b8]. The author mentioned this paper when discussing the importance of multi-order connections between nodes in network analysis.\n\n5. \"LinkBERT: Pretraining Language Models with Document Links\" [b39]. This paper is mentioned as a first-order topology-aware PLM, which the author improves upon by proposing a topology-aware pretraining task that captures multi-order connections.\n\n6. \"GLM: General Language Model Pretraining with Autoregressive Blank Infilling\" [b6]. The author compares his proposed method with this work, which is a representative method for heterogeneous network embedding.\n\n7. \"GI-ANT: Node Feature Extraction by Self-supervised Multi-scale Neighborhood Prediction\" [b3]. This paper is mentioned as a first-order topology-aware PLM, which the author improves upon by proposing a topology-aware pretraining task that captures multi-order connections.\n\nHere is the result in the requested JSON format:\n\n{\n\"b5\": 1,\n\"b41\": 0.9,\n\"b14\": 0.8,\n\"b8\": 0.7,\n\"b39\": 0.7,\n\"b6\": 0.6,\n\"b3\": 0.6\n}", "Based on the provided text and instructions, the main challenges of the paper are: \n\n1. Current language model pretraining tasks mainly focus on separately learning the text information of each entity, and ignore the important aspect of capturing the topological relationships between entities in Text-Attributed Heterogeneous Graphs (TAHGs).\n2. Abundant topological information exists in TAHGs and can reflect rich relationships, but common pretraining tasks ignore these connections.\n3. There is an imbalance in the textual descriptions of nodes in TAHGs, with some nodes having rich text descriptions and others having very little.\n\nThe novel methods proposed in this paper to tackle these challenges are:\n\n1. A new pretraining framework for language models (LMs) that explicitly considers topological and heterogeneous information in TAHGs.\n2. The definition of a context graph for the neighborhood of a target node within a specific order, and a topology-aware pretraining task to predict nodes involved in the context graph.\n3. A text augmentation strategy to enrich the textual descriptions of nodes with few texts using the text of their neighbors, aiming to address the imbalance problem.\n\nBased on these challenges and methods, the papers that seem to be the primary sources of inspiration are:\n\n1. [b5] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. This paper is mentioned in the context of the current language model pretraining tasks that the authors aim to improve upon.\n2. [b32] Attention Is All You Need: The authors' approach is built upon the Transformer architecture, which was first introduced in this paper.\n3. [b41] Heterogeneous graph representation learning with relation awareness: The authors use this method as part of their new approach.\n4. [b39] LinkBERT: Pretraining Language Models with Document Links: This paper is mentioned in the context of topology-aware PLMs, which the authors aim to improve upon.\n5. [b3] Node Feature Extraction by Self-supervised Multi-scale Neighborhood Prediction: This paper also relates to topology-aware pretraining tasks.\n\nThe corresponding json format is as follows:\n\n{\n    \"b5\": 0.9,\n    \"b32\": 0.8,\n    \"b41\": 0.8,\n    \"b39\": 0.7,\n    \"b3\": 0.7\n}", "Based on the given information and instructions, the source papers of the given paper that most influenced it according to its text are:\n\nb5: \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"\nb3: \"Node Feature Extraction by Self-supervised Multi-scale Neighborhood Prediction\"\nb39: \"LinkBERT: Pretraining Language Models with Document Links\"\nb23: \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\"\nb41: \"Heterogeneous Graph Representation Learning with Relation Awareness\"\n\nThese papers are most likely to have influenced the given paper because they are frequently mentioned in the key sections, and the paper's methodology and experiments directly build upon or distinguish themselves from these works. \n\nThe confidence score for each source paper is as follows:\n\n{ \"b5\": 0.9, \"b3\": 0.8, \"b39\": 0.8, \"b23\": 0.7, \"b41\": 0.7 } \n\nThese confidence scores are assigned based on the frequency and context of the paper's citations, the significance of the methods that they introduce, and their relevance to the given paper's novel contributions.", "Based on the text of the paper, the source papers that inspire the author the most could be:\n\n{\n\"b5\": 0.9,\n\"b32\": 0.8,\n\"b1\": 0.7,\n\"b11\": 0.7,\n\"b28\": 0.7,\n\"b41\": 0.9,\n\"b23\": 0.7,\n\"b30\": 0.8,\n\"b39\": 0.9,\n\"b3\": 0.9\n}\n\nThe papers b5, b41, b39, and b3 are prioritized because they are mentioned in the context of the authors' novel contributions, suggesting direct inspiration. The papers b32, b1, b11, b28, b23, and b30 are included for their relevance to the methods and challenges outlined in the paper. The confidence scores are based on the frequency and context of the reference in the paper's text.", "Based on the text of the paper, the primary challenges outlined in the paper are:\n\n1. Abundant Topological Information (C1): The paper indicates that existing Pretrained Language Models (PLMs) struggle to handle the rich relationships reflected by first-and higher-order connections in Text-Attributed Heterogeneous Graphs (TAHGs).\n\n2. Imbalanced Textual Descriptions of Nodes (C2): The paper highlights that in TAHGs, nodes are heterogeneous and their carried texts are often in different magnitudes, leading to an imbalance that existing PLMs fail to handle effectively.\n\nThe author proposes a new pretraining framework for LMs called THLM that integrates both Topological and Heterogeneous information in TAHGs into LMs.\n\nThe source papers that seem to have heavily influenced the main contributions of the paper based on phrases indicating direct inspiration or motivation are:\n\n1. [b5]: BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding. The paper uses this method in its methodology, and it seems to be a significant influence on the novel contributions of the paper.\n\n2. [b3]: Node Feature Extraction by Self-Supervised Multi-Scale Neighborhood Prediction. The paper mentions this work in the context of existing works that attempt to make PLMs aware of graph topology.\n\n3. [b39]: LinkBERT: Pretraining Language Models with Document Links. The paper compares its method with LinkBERT, indicating that it is a relevant previous work.\n\n4. [b41]: Heterogeneous Graph Representation Learning with Relation Awareness. The paper uses this method to implement the auxiliary heterogeneous graph neural network in its methodology, indicating that it is a significant influence on the novel contributions of the paper.\n\n5. [b2]: Language Models Are Few-Shot Learners. The paper mentions this work as an example of the power of LMs in modeling texts, indicating that it is a relevant previous work.\n\n6. [b0]: Heterogeneous Graph Neural Networks Analysis: A Survey of Techniques, Evaluations, and Applications. This paper is referenced when the author discusses the graph learning ability of graph neural networks, indicating that it is a relevant previous work.\n\nPlease note that these are inferred based on the textual analysis and the final confirmation should be done by reading the full papers and understanding the context of these citations in the paper. \n\nThe JSON format of the result is:\n\n{\n    \"b5\": 0.9,\n    \"b3\": 0.7,\n    \"b39\": 0.7,\n    \"b41\": 0.9,\n    \"b2\": 0.7,\n    \"b0\": 0.7\n}\n\nThe confidence scores are set based on how directly the papers seem to contribute to the novel methods and approaches introduced in the paper, with a higher score indicating a stronger influence."], "65252d90939a5f40827eabe7": ["Based on the given paper, the author mainly implemented a method named NEFTune for overcoming overfitting in large language models (LLMs) by adding random noise to the embedding vectors during the fine-tuning process. This approach substantially improved the model's performance as evaluated by AlpacaEval. \n\nThe author mentioned several papers that influenced this work. However, the ones that seem to be most directly linked to the paper's primary contributions are:\n\n1. [b0]: \"Language models are few shot learners.\" This paper is likely a key inspiration as it discusses the capacities of large language models which is a central aspect of the given paper.\n   \n2. [b1]: \"Training a better alpaca with fewer data.\" It seems that the author used this work as a benchmark when testing the effectiveness of their new method.\n\n3. [b18]: \"Training language models to follow instructions with human feedback.\" This paper could have been used by the author to guide the development of their training method.\n\n4. [b25]: \"Self-instruct: Aligning language model with self-generated instructions.\" This paper is mentioned in the context of the Alpaca dataset, which is used by the author, suggesting that it has influenced the work.\n\n5. [b2]: \"Vicuna: An open source chatbot impressing GPT with chatGPT quality.\" This paper is mentioned in the context of the ShareGPT dataset, indicating that it played a role in the research.\n\nBased on the above analysis, the source papers identified are [b0], [b1], [b18], [b25], and [b2]. However, without a more explicit indication from the author about which papers were most influential, it's difficult to provide a definitive answer. \n\nIn json format, the result would be:\n{\n    \"b0\": 0.9,\n    \"b1\": 0.85,\n    \"b18\": 0.85,\n    \"b25\": 0.7,\n    \"b2\": 0.7\n}  \n\nThe confidence scores are subjective and based on the implied relevance of each paper to the author's work.", "Based on the text and information provided, the paper is primarily focused on proposing a new algorithm called \"Noisy Embedding Fine Tuning (NEFTune)\". This algorithm aims to address the issue of overfitting in large model training by adding noise to the embedding layer during the training phase. \n\nThe following references seem to be most relevant to the paper's contributions:\n\n1. \"neftu n e noisy embeddings improve instruc tion finetuning\": This appears to be the source paper directly outlining the NEFTune approach, which is the primary focus of the given paper.\n\n2. \"training a better alpaca with fewer data\": The given paper discusses the improvements to the AlpacaEval model after applying NEFTune, suggesting this source may have provided foundational knowledge or techniques.\n\n3. \"efficient finetuning of quantized llms\": This paper might have influenced the approach to fine-tuning used in the given paper.\n\n4. \"finetuned language models are zero shot learners\": The given paper seems to be concerned with fine-tuning language models, making this source likely relevant.\n\n5. \"empowering large language models to follow complex instructions\": This paper might have provided foundational knowledge or techniques related to the training of large language models, which is a central theme in the given paper.\n\n6. \"training language models to follow instructions with human feedback\": Given that the NEFTune approach involves adding noise to training data (which can be considered a form of feedback), this source may have been influential.\n\nIn JSON format, the response would look something like this:\n\n{\n  \"b0\": 1,\n  \"b1\": 0.8,\n  \"b5\": 0.8,\n  \"b26\": 0.7,\n  \"b27\": 0.9,\n  \"b18\": 0.7\n} \n\nThe confidence scores assigned are based on the perceived relevance of each reference to the given paper's main contributions. A score of 1 indicates high confidence that the reference was a significant source of inspiration for the paper.", "Based on the text of the paper, the primary challenge addressed is overfitting in large language models, and the authors propose an algorithm to add random noise to the embedding vectors (NEFTune) to improve the outcome of instruction fine-tuning. \n\nHere are the potential source papers:\n\n1. \"b0: language models are few shot learners\" - The paper talks about large language models, which is the domain of our main paper. Confidence: 0.8\n2. \"b1: training a better alpaca with fewer data\" - This paper could be the inspiration behind the idea of getting the most out of small instruction datasets. Confidence: 0.9\n3. \"b2: vicuna an open source chatbot impressing gpt with chatgpt quality\" - The idea of a chatbot could be related to conversational tasks mentioned in the main paper. Confidence: 0.6\n4. \"b5: efficient finetuning of quantized llms\" - It is related to the concept of fine-tuning language models, which is core to the main paper. Confidence: 0.9\n5. \"b16: text only training for image captioning using noise injected clip\" - This paper might have inspired the idea of adding noise for improving the model performance. Confidence: 0.7\n6. \"b18: training language models to follow instructions with human feedback\" - This could be related to the concept of instruction fine-tuning. Confidence: 0.9\n7. \"b27: empowering large language models to follow complex instructions\" - This paper is probably related to the main contributions of the main paper. Confidence: 0.9\n\nInferred in JSON format:\n{\n\"b0\": 0.8, \n\"b1\": 0.9, \n\"b2\": 0.6, \n\"b5\": 0.9, \n\"b16\": 0.7, \n\"b18\": 0.9, \n\"b27\": 0.9\n}", "Based on the provided text, the paper focuses on a technique for language learning model (LLM) fine-tuning, called NEFTune (Noisy Embedding Fine Tuning), which helps to avoid overfitting and thus improves model performance. The source papers that the author seems to draw inspiration from, as indicated by direct mentions or relevance to the method described, appear to be:\n\n1. [b0]: 'Language Models are Few-Shot Learners': This paper appears to be the foundational paper for language learning models, which are a key part of the paper's focus.\n2. [b1]: 'Training a Better Alpaca with Fewer Data': This paper discusses improving language models with limited data, a challenge that NEFTune aims to address.\n3. [b5]: 'Efficient Fine-tuning of Quantized LLMs': This paper discusses an efficient fine-tuning technique for LLMs, a theme directly related to the NEFTune approach.\n4. [b18]: 'Training Language Models to Follow Instructions with Human Feedback': This paper deals with fine-tuning LLMs using instruction data, which is the primary method used in the paper.\n5. [b24]: 'LLaMA: Open Foundation and Fine-tuned Chat Models': This paper presents another model that the authors are building upon with their NEFTune approach.\n6. [b27]: 'Empowering Large Language Models to Follow Complex Instructions': This paper focuses on enhancing LLMs to better follow instructions, a challenge directly addressed by the paper.\n\nIn JSON format:\n{\n\"b0\": 0.9,\n\"b1\": 0.8,\n\"b5\": 0.8,\n\"b18\": 0.95,\n\"b24\": 0.9,\n\"b27\": 0.85\n} \n\nPlease note that the confidence scores are subjective and represent my interpretation of the paper's reliance on each source.", "Based on the provided text, the author seems to be influenced by the following papers:\n\n1. \"Noisy Embedding Instruction Fine Tuning (NEFTune)\" - The author uses the NEFTune algorithm to improve the performance of language models. This is the main method proposed in the paper.\n2. \"Language Models Are Few-Shot Learners\" - The author mentions that language models are typically fine-tuned on a small but carefully curated set of instruction data, which is a concept explored in this source.\n3. \"AlpacaEval: A Simulation Framework for Methods that Learn from Human Feedback\" - The author refers to AlpacaEval as an evaluation method for the model's conversational abilities. \n\nIt is important to note that the author's paper does not explicitly state that it is inspired or motivated by these works, but they are mentioned in the context of methodology and evaluation. \n\nThe result in json format would look like this:\n{\n  \"reference number\": \"confidence score\",\n  \"b0\": 0.8,\n  \"b6\": 0.8,\n  \"NEFTune\": 1\n}"], "626b49625aee126c0fffd149": ["{\n  \"b2\": 0.9,\n  \"b5\": 0.85,\n  \"b7\": 0.8,\n  \"b18\": 0.75\n}", "{\n  \"b2\": 0.9,\n  \"b5\": 0.8,\n  \"b7\": 0.85\n}", "{\n  \"b2\": 0.95,\n  \"b7\": 0.9,\n  \"b5\": 0.85,\n  \"b6\": 0.8\n}", "{\n  \"b2\": 0.9,\n  \"b5\": 0.8,\n  \"b7\": 0.75,\n  \"b6\": 0.7\n}", "{\n  \"b2\": 0.9,\n  \"b7\": 0.85,\n  \"b5\": 0.8,\n  \"b6\": 0.75,\n  \"b15\": 0.7\n}"], "62a6aabf5aee126c0ff36991": ["Based on the text of the academic paper, the following source papers appear to have most directly inspired the authors' work:\n\n1. \"Attention is all you need\" [b31]\n2. \"Graph attention networks\" [b32]\n3. \"A generalization of transformer networks to graphs\" [b11]\n4. \"Rethinking graph transformers with spectral attention\" [b23]\n5. \"Do transformers really perform badly for graph representation\" [b36]\n6. \"Gophormer ego graph transformer for node classification\" [b41]\n7. \"Graph convolutional networks meet markov random fields semi supervised community detection in attribute networks\" [b19]\n8. \"Semi-supervised classification with graph convolutional networks\" [b21]\n\nThe JSON format of the answer would be: \n\n{\n    \"b31\": 0.9,\n    \"b32\": 0.8,\n    \"b11\": 0.7,\n    \"b23\": 0.8,\n    \"b36\": 0.7,\n    \"b41\": 0.9,\n    \"b19\": 0.7,\n    \"b21\": 0.7\n}", "Based on the text of the paper, the source papers that seem to have inspired the research most directly are:\n\n1. Gophormer [b41]: The paper discusses Gophormer's attempt to reduce the training cost by sampling the ego-graphs of nodes, and identifies its limitations, which NAGphormer, the model proposed in the paper, aims to overcome.\n\n2. Graph Transformers [b11] [b23] [b36]: The paper mentions the strategies developed by existing Graph Transformers to capture the structural feature of nodes, and how NAGphormer differs and improves upon these strategies. \n\n3. Graph Attention Networks [b32]: The paper mentions how GAT inspired the attention-based readout function used in NAGphormer.\n\n4. Graph Neural Networks [b21] [b32] [b22]: The paper discusses GNN-based studies that aggregate neighborhood (or local) information, and how these have shown superior performance on all kinds of node classification tasks. This seems to have directly influenced the development of NAGphormer.\n\nSo, the returned result should be:\n{\n\"b41\": 1,\n\"b11\": 0.8,\n\"b23\": 0.8,\n\"b36\": 0.8,\n\"b32\": 0.9,\n\"b21\": 0.7,\n\"b22\": 0.7\n}", "Based on the provided text, the source papers that significantly inspired the given paper are:\n\n1. \"Graph Transformers: A Generalization of Transformer Networks to Graphs\" (b11) - the paper mentions this work in the context of generalizing transformers to non-Euclidean data, such as graphs. \n\n2. \"Gophormer: Ego Graph Transformer for Node Classification\" (b41) - the paper discusses Gophormer as a recent attempt to reduce the training cost by sampling the ego-graphs of nodes.\n\n3. \"Graph Attention Networks\" (b32) - the paper mentions this work in the context of GNN-based studies, and later, when introducing the attention-based readout function.\n\n4. \"GraphSAINT: Graph Sampling based Inductive Learning Method\" (b38) - the paper mentions this work in the context of scalable GNNs.\n\n5. \"Graph Neural Networks for Social Recommendation\" (b12) - the paper mentions this work in the context of graph learning tasks.\n\n6. \"Semi-Supervised Classification with Graph Convolutional Networks\" (b21) - the paper mentions this work in the context of graph learning tasks.\n\n7. \"Cluster GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks\" (b7) - the paper mentions this work in the context of large-scale datasets.\n\n8. \"GRAND: Scalable Graph Random Neural Networks\" (b13) - the paper mentions this work in the context of scalable GNNs.\n\nBased on the given paper, the JSON format would be:\n\n```\n{\n\"b11\": 1,\n\"b41\": 0.9,\n\"b32\": 0.85,\n\"b38\": 0.8,\n\"b12\": 0.75,\n\"b21\": 0.75,\n\"b7\": 0.7,\n\"b13\": 0.7\n}\n```\nThe confidence scores are assigned based on the depth of discussion and the context in which these papers are mentioned in the given paper.", "Based on the given text and author notes, the source papers that inspired the core contributions of the paper appear to be:\n\n1. \"A generalization of transformer networks to graphs\" [b11]: The paper proposes a novel model, Neighborhood Aggregation Graph Transformer (NAGphormer), for node classification. \n\n2. \"Graph attention networks\" [b32]: NAGphormer provides an attention-based readout function so as to adaptively learn the importance of each hop. This seems to be inspired by the Graph Attention Networks.\n\n3. \"Do transformers really perform badly for graph representation?\" [b36]: The paper discusses the limitations of existing Graph Transformers for large-scale networks for node classification. \n\n4. \"Attention is all you need\" [b31]: NAGphormer learns the node representation using the standard Transformer backbone, which is inspired by the original Transformer model.\n\n5. \"Gophormer ego graph transformer for node classification\" [b41]: This paper is directly mentioned as a previous attempt that NAGphormer improves upon.\n\n6. \"Semi-supervised classification with graph convolutional networks\" [b21] and \"Predict then propagate: Graph neural networks meet personalized pagerank\" [b22]: These papers are mentioned in the context of GNN-based studies that have shown superior performance on all kinds of node classification tasks.\n\n7. \"Re-thinking Graph Transformers with Spectral Attention\" [b23]: The authors adopt the eigenvectors of the Laplacian matrix of the graph to capture the structural information of nodes, which is a strategy also used in this paper.\n\nIn JSON format:\n\n{\n    \"b11\": 0.9,\n    \"b32\": 0.8,\n    \"b36\": 0.8,\n    \"b31\": 0.9,\n    \"b41\": 0.9,\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.8\n} \n\nThis list is based on the direct mentions, the methodological similarity, and the comparative phrases used in the paper. The confidence scores are inferred from the text's emphasis on these papers and the degree of inspiration and motivation these papers provided for the main contributions of the paper.", "Based on the text and the instructions:\n\n1. The task of the paper is to propose a novel model, termed Neighborhood Aggregation Graph Transformer (NAGphormer), for node classification. The paper introduces a novel neighborhood aggregation module called Hop2Token, and an attention-based readout function to adaptively learn the importance of each hop.\n\n2. The paper explicitly mentions that it is \"Motivated by the above observation,\" and then outlines its novel contributions. This indicates that the paper is inspired by previous work and observations.\n\n3. The paper cites various references throughout the text. Important phrases indicating inspiration or motivation are not directly mentioned, but some references are directly linked to the challenges and methods proposed in the paper.\n\n4. The paper introduces a novel neighborhood aggregation module called Hop2Token. It is directly inspired by Graph Transformer [b41] which is a recent attempt that aims to reduce the training cost by sampling the ego-graphs of nodes.\n\n5. The proposed attention-based readout function is inspired by GAT [b32].\n\n6. The paper proposes a new Graph Transformer method, NAGphormer, for node classification which can handle large graphs. It is potentially inspired by [b11], [b23] and [b36] which are also Graph Transformer methods.\n\n7. The structural encoding and attention-based readout function used in NAGphormer could be inspired by [b33] which is widely used in spectral clustering for capturing the structural information of nodes.\n\nThe source papers are:\n- Graph Transformer [b41]\n- GAT [b32]\n- Transformer methods [b11], [b23] and [b36]\n- Spectral clustering [b33]\n\nThe JSON format result would look like:\n\n{\n    \"b41\": 0.9,\n    \"b32\": 0.85,\n    \"b11\": 0.8,\n    \"b23\": 0.8,\n    \"b36\": 0.8,\n    \"b33\": 0.75\n}\n\nThe confidence score is estimated based on the relevance of the source papers to the novel contributions of the paper. The higher the score, the more relevant the source paper."], "6535d747939a5f408295c3c4": ["Based on the text analysis and the author notes, the source papers that most directly influenced this paper seem to be:\n\n- 'Graph Transformer Networks' [b59]: The paper mentions using Graph Transformer Networks as a backbone GNN architecture for encoding graph structure information.\n- 'Attention is all you need' [b37]: The use of transformers, which was first introduced in this paper, is mentioned several times.\n- 'Self-Instruct: Aligning Language Models with Self-Generated Instructions' [b44]: The concept of instruction tuning, which is a central part of the proposed method in the paper, was first introduced here.\n- 'Graph Attention Networks' [b38]: This paper is mentioned in the context of previous GNN architectures.\n- 'Semi-Supervised Classification with Graph Convolutional Networks' [b16]: Mentioned as an example of previous GNN architectures.\n- 'Learning Transferable Visual Models from Natural Language Supervision' [b29]: The technique of incorporating textual information into graph structure encoding process in a contrastive manner, which is used in the paper, was inspired by this work.\n\nThus, the source papers are:\n```json\n{\n    \"b59\": 1,\n    \"b37\": 0.9,\n    \"b44\": 0.8,\n    \"b38\": 0.7,\n    \"b16\": 0.7,\n    \"b29\": 0.9\n}\n```\nThe confidence scores are assigned based on the degree of the paper's reliance on the concepts and methods introduced in these works.", "Based on the text, the primary challenges outlined in the paper are aligning the structural information of a graph with the language space, guiding large language models to comprehend the structural information of graphs, and endowing large language models with the ability to reason step-by-step. The author's proposed solution, GraphGPT, addresses these challenges by aligning Large Language Models with graphs using a graph instruction tuning paradigm.\n\nThe most relevant source papers that inspired this work are likely to be:\n\n1. \"GPT Graph: Can Large Language Models Understand Graph-Structured Data? An Empirical Evaluation and Benchmarking\" [b5]: This paper is indicated as a direct comparison to the author's GraphGPT framework, and it seems to be the main prior work that the authors are building upon.\n2. \"Self-Instruct: Aligning Language Models with Self-Generated Instructions\" [b44]: This paper introduced the concept of instruction tuning, which the authors have adapted for their work.\n3. \"Attention is all you need\" [b37]: This paper introduced the transformer model, which is a core component in large language models.\n4. \"Graph Attention Networks\" [b38]: The authors mention attention networks in their paper, which indicate that this work has influenced their research.\n5. \"Graph Transformer Networks\" [b59]: The authors mention graph transformer networks multiple times, indicating its importance in their work.\n6. \"Semi-Supervised Classification with Graph Convolutional Networks\" [b16]: The authors refer to this work when introducing their methodology.\n7. \"GraphMAE: Self-Supervised Masked Graph AutoEncoders\" [b11]: This paper is mentioned as a generative SSL method, which might have inspired the authors in their work.\n\nHere is the proposed list in the json format:\n{\n\"b5\": 0.9,\n\"b44\": 0.8,\n\"b37\": 0.8,\n\"b38\": 0.8,\n\"b59\": 0.8,\n\"b16\": 0.7,\n\"b11\": 0.7\n}\nThe confidence score is subjectively assigned based on the perceived relevance of each paper to the research in the text. All papers seem highly relevant, hence the high scores.", "Based on the text of the academic paper, the most pertinent source papers are:\n\n- 'b44': 'self instruct aligning language models with self generated instructions': The paper introduces the concept of instruction tuning, which is also a key part of the method proposed in this paper. The authors proposed a dual-stage graph instruction tuning paradigm inspired by this concept. Confidence Score: 0.9\n\n- 'b37': 'attention is all you need': In the methodology section, the authors mentioned they employed a vanilla transformer as the text encoder, this transformer is proposed in this paper. Confidence Score: 0.8\n\n- 'b16': 'semi supervised classification with graph convolutional networks': The authors mentioned they incorporate a message-passing neural network architecture, which can be a graph convolutional network, as the structure-level pre-trained graph model. This method was proposed by this paper. Confidence Score: 0.8\n\n- 'b59': 'graph transformer networks': In the methodology section, the authors mentioned they employed a graph transformer as the graph encoder. This transformer is proposed in this paper. Confidence Score: 0.8\n\n- 'b29': 'learning transferable visual models from natural language supervision': The authors mentioned this paper when they talk about encoding of graph structures that can collaborate well with Language Models (LLMs). Confidence Score: 0.7\n\n- 'b1': 'exploring the potential of large language models (LLMs) in learning on graphs': The authors mentioned they were inspired by the instruction designs in this paper when they designed the graph matching task. Confidence Score: 0.7\n\n- 'b46': 'chain of thought prompting elicits reasoning in large language models': The authors incorporated the Chain-of-Thought (COT) distillation into their framework, which enhances the model's step-by-step reasoning abilities. This method is proposed in this paper. Confidence Score: 0.7\n\nSo, the final answer in the JSON format is:\n\n{\n\"b44\": 0.9,\n\"b37\": 0.8,\n\"b16\": 0.8,\n\"b59\": 0.8,\n\"b29\": 0.7,\n\"b1\": 0.7,\n\"b46\": 0.7\n}", "Based on the text and author notes, I identified the following source papers that have significantly inspired the author of this paper:\n\n1. \"Inductive representation learning on large graphs\" [b7]: The author mentioned the use of Graph Convolutional Networks (GCNs) which was introduced in this paper. \n\n2. \"Attention is all you need\" [b37]: The author used transformer models for text encoding which was proposed in this paper.\n\n3. \"Graph attention networks\" [b38]: The concept of using attention mechanisms in graphs, that was presented in this paper, is used in the author's work.\n\n4. \"Self instruct aligning language models with self generated instructions\" [b44]: The author's work is heavily inspired by the concept of instruction tuning introduced in this paper.\n\n5. \"Heterogeneous graph transformer\" [b14]: The author mentioned using graph transformer networks in their work, which is proposed in this paper.\n\n6. \"Chain of thought prompting elicits reasoning in large language models\" [b46]: The author adopted the Chain-of-Thought (COT) distillation from this paper.\n\nBased on the titles of these papers, we can infer that they are very relevant to the paper's novel contributions. \n\nThe JSON format of the result is as follows:\n{\n    \"b7\": 0.9,\n    \"b37\": 0.9,\n    \"b38\": 0.9,\n    \"b44\": 1.0,\n    \"b14\": 0.9,\n    \"b46\": 0.9\n}", "Based on the information provided in the paper's text and the authors' notes, the main contributions of the paper are: \n\n1. To improve the generalization ability of graph learning, the authors align graph domain-specific structural knowledge with the reasoning ability of Large Language Models (LLMs).\n2. They propose a method that uses a graph instruction fine-tuning paradigm to align large language models with graph structured data. This paradigm combines the method of self-supervised instruction fine-tuning to enhance the large language model's understanding and reasoning ability for graph structural knowledge.\n3. They also introduce task-specific instruction fine-tuning to improve the model's adaptability in various graph learning tasks.\n4. They evaluate GraphGPT on supervised and zero-shot graph learning tasks and compare it with state-of-the-art baseline methods to demonstrate its superior generalization ability in various settings.\n\nBased on these contributions, the key source papers that inspire the authors are:\n\n1. \"b1: Exploring the potential of large language models llms in learning on graphs\" - This paper appears to be a strong influence as it explores the potential of LLMs in graph learning. It is directly relevant to the main contribution of aligning LLMs with graph learning.\n\n2. \"b5: gpt graph can large language models understand graph structured data an empirical evaluation and benchmarking\" - This paper seems to be crucial, as it investigates the comprehension capability of LLMs for graph structured data. This is directly linked to the paper's main challenge of aligning LLMs and graph learning.\n\n3. \"b11: Graphmae self supervised masked graph autoencoders\" - This paper introduces a generative SSL approach, which is similar to the self-supervised instruction tuning paradigm proposed by the authors.\n\n4. \"b44: self instruct aligning language models with self generated instructions\" - This paper aligns with the authors' novel method of instruction tuning.\n\n5. \"b59: graph transformer networks\" - This paper seems to have inspired the authors' GraphGPT framework, which incorporates self-attention and positional encoding to capture global dependencies and structural patterns in the graph.\n\n6. \"b14: heterogeneous graph transformer\" - This paper could be a potential source, as it discusses the use of transformers in graph networks, a methodology also used by the authors.\n\nBased on the analysis above, the most pertinent source papers are:\n\n{\n\"b1\": 0.85,\n\"b5\": 0.85,\n\"b11\": 0.75,\n\"b44\": 0.8,\n\"b59\": 0.8,\n\"b14\": 0.75\n}"], "6503bec83fda6d7f067c7717": ["Based on the text and the titles of the references, the source papers for \"snakegan a universal vocoder leveraging ddsp prior knowledge and periodic inductive bias\" appear to be:\n\n1. \"ddsp differentiable digital signal processing\" [b3]: The paper frequently mentions DDSP and how it is used to generate a coarse-grained signal in the time domain. It also discusses introducing DDSP prior into the black-box GAN.\n2. \"neural networks fail to learn periodic functions and how to fix it\" [b26]: This paper is mentioned in relation to the Snake activation function, which is a key part of SnakeGAN.\n3. \"bigvgan a universal neural vocoder with large scale training\" [b16]: This paper is cited when discussing the anti-aliased multi-periodicity composition (AMP) block with Snake activation function, which is part of SnakeGAN.\n4. \"alias free generative adversarial networks\" [b11]: This paper is mentioned in relation to StyleGAN3, which is similar to the generator in SnakeGAN.\n5. \"fre gan adversarial frequency consistent audio synthesis\" [b12]: This paper is cited when discussing the use of discriminators in SnakeGAN.\n6. \"universal melgan a robust neural vocoder for high fidelity waveform generation in multiple domains\" [b7] and \"univnet a neural vocoder with multi resolution spectrogram discriminators for high fidelity waveform generation\" [b8]: Both papers are mentioned as recent works on universal vocoders, which is the focus of the SnakeGAN paper.\n\nThe JSON format result:\n\n{\n    \"b3\": 0.9,\n    \"b26\": 0.9,\n    \"b16\": 0.8,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b7\": 0.6,\n    \"b8\": 0.6\n}", "Based on the text of the paper and the titles of the references, the following papers appear to have most directly inspired the main contributions of the paper:\n\n1. \"DDSP: Differentiable Digital Signal Processing\" [b3]: The paper mentions the use of a DDSP-based prior knowledge of waveform composition in the generator of SnakeGAN. This suggests that the DDSP paper is a significant source of inspiration.\n   \n2. \"Neural networks fail to learn periodic functions and how to fix it\" [b26]: This paper is credited for the Snake activation function, which the paper emphasizes as a key part of the generator's ability to improve waveform generation and robustness under unseen scenarios.\n\n3. \"BigVGAN: A universal neural vocoder with large-scale training\" [b16]: The paper mentions the BigVGAN's generator with AMP block and Snake activation function as a method to address the problem of generalization ability, suggesting this paper had a significant influence.\n\n4. \"Fre-GAN: Adversarial Frequency-consistent Audio Synthesis\" [b12]: The paper uses the discriminators setting of Fre-GAN, including MPD and MSD, both with DWT instead of average pooling, indicating that this work has provided direct methodological influence.\n\n5. \"UnivNet: A neural vocoder with multi-resolution spectrogram discriminators for high fidelity waveform generation\" [b8]: The paper mentions UnivNet as a reference for adopting MPD with DWT and MRD discriminators.\n\n6. \"Universal MelGAN: A robust neural vocoder for high fidelity waveform generation in multiple domains\" [b7]: The paper cites Universal MelGAN when discussing recent works on universal vocoders, suggesting that this paper had an influence on the development of SnakeGAN.\n\n7. \"Alias-Free Generative Adversarial Networks\" [b11]: The paper draws inspiration from StyleGAN3 for its generator structure and for addressing aliasing artifacts in image synthesis.\n\nTherefore, the JSON format of the source papers along with their confidence scores is:\n\n{\n    \"b3\": 1.0,\n    \"b26\": 1.0,\n    \"b16\": 0.9,\n    \"b12\": 0.9,\n    \"b8\": 0.8,\n    \"b7\": 0.8,\n    \"b11\": 0.7\n}", "Based on the given text and the instructions, the source papers that were most influential for this paper on SnakeGAN, a universal vocoder, could be:\n\n1. \"DDSP: Differentiable Digital Signal Processing\" [b3]: This paper is referenced multiple times in the text, and the authors explicitly mention using DDSP-based prior knowledge in their proposed method. It seems to be a core component of their algorithm.\n2. \"Neural networks fail to learn periodic functions and how to fix it\" [b26]: The \"Snake\" activation function introduced in this paper seems to be a crucial part of the proposed method, and is mentioned multiple times.\n3. \"HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis\" [b13]: This paper is referenced when the authors discuss previous works on improving the discriminator architecture.\n4. \"Universal MelGAN: A Robust Neural Vocoder for High Fidelity Waveform Generation in Multiple Domains\" [b7]: This paper is referenced when discussing the concept of universal vocoders, which is a key part of the paper's challenges.\n5. \"UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation\" [b8]: Also mentioned in relation to the concept of universal vocoders.\n6. \"BigVGAN: A universal neural vocoder with large scale training\" [b16]: This paper is referenced when discussing improvements to the generator architecture.\n7. \"Alias-Free Generative Adversarial Networks\" [b11]: This paper is mentioned when discussing the structure of their generator and the problem of aliasing artifacts.\n\nThe JSON format answer should be: {\"b3\": 1, \"b26\": 0.9, \"b13\": 0.8, \"b7\": 0.8, \"b8\": 0.8, \"b16\": 0.7, \"b11\": 0.7}. The confidence score is subjective, but the scores here reflect the perceived influence of each paper on the given paper based on the frequency and context of their mentions in the text.", "Based on the context and analysis of the paper, the most pertinent source papers are:\n\n1. \"DDSP: Differentiable Digital Signal Processing\" [b3] - The paper frequently mentions the incorporation of DDSP-based prior knowledge in the development of SnakeGAN, suggesting a significant influence.\n\n2. \"Neural networks fail to learn periodic functions and how to fix it\" [b26] - The paper introduces the Snake activation function, which is a crucial component of SnakeGAN.\n\n3. \"Hifi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis\" [b13] - The paper mentions the use of HiFi-GAN, suggesting that it might have influenced the development of SnakeGAN.\n\n4. \"Universal MelGAN: A Robust Neural Vocoder for High Fidelity Waveform Generation in Multiple Domains\" [b7] - The paper discusses universal vocoders, which is a key topic in the paper.\n\n5. \"UnivNet: A Neural Vocoder with Multi Resolution Spectrogram Discriminators for HighFidelity Waveform Generation\" [b8] - The paper introduces UnivNet and discusses its discriminators, which appear to have influenced the development of SnakeGAN.\n\n6. \"Alias-Free Generative Adversarial Networks\" [b11] - The paper discusses the issue of aliasing artifacts in image synthesis that the SnakeGAN paper addresses.\n\n7. \"BigVGAN: A universal neural vocoder with large scale training\" [b16] - The paper introduces the BigVGAN generator with AMP block, which is similar to the structure of SnakeGAN.\n\nConsidering these sources, the following JSON object might represent the source papers and their confidence scores:\n\n```json\n{\n  \"b3\": 0.9,\n  \"b26\": 0.9,\n  \"b13\": 0.8,\n  \"b7\": 0.8,\n  \"b8\": 0.8,\n  \"b11\": 0.7,\n  \"b16\": 0.7\n}\n```", "Based on the text of the paper, the most pertinent source papers that influenced the authors are:\n\n1. \"DDSP: Differentiable Digital Signal Processing\" [b3] - The authors explicitly mention using the DDSP model in their work and it plays a crucial role in their method. Confidence Score: 0.9\n2. \"HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis\" [b13] - They reference this paper when discussing their methodology and it seems to have influenced their approach. Confidence Score: 0.8\n3. \"Neural Networks Fail to Learn Periodic Functions and How to Fix It\" [b26] - The authors introduce the Snake activation function from this paper into their method, indicating it played a significant role in the development of their approach. Confidence Score: 0.85\n4. \"Fre-GAN: Adversarial Frequency Consistent Audio Synthesis\" [b12] - They reference this paper when discussing the discriminators used in their method, suggesting it was a significant influence. Confidence Score: 0.75\n5. \"Universal MelGAN: A Robust Neural Vocoder for High Fidelity Waveform Generation in Multiple Domains\" [b7] - The authors mention this work when discussing universal vocoders, indicating it may have played a role in their research. Confidence Score: 0.7\n6. \"BigVGAN: A Universal Neural Vocoder with Large Scale Training\" [b16] - The authors reference this paper when discussing the anti-aliased multi-periodicity composition (AMP) block with Snake activation function in their method, suggesting it was an influence. Confidence Score: 0.75\n\nThe result in the JSON format is:\n{\"b3\": 0.9, \"b13\": 0.8, \"b26\": 0.85, \"b12\": 0.75, \"b7\": 0.7, \"b16\": 0.75}"], "6303504190e50fcafd769fe6": ["{\n  \"b36\": 0.9,\n  \"b28\": 0.8,\n  \"b0\": 0.7,\n  \"b9\": 0.7,\n  \"b14\": 0.7,\n  \"b26\": 0.7\n}", "{\n  \"b36\": 0.95,\n  \"b28\": 0.90\n}", "{\n  \"b36\": 0.95,\n  \"b28\": 0.9\n}", "{\n  \"b36\": 0.95,\n  \"b28\": 0.9,\n  \"b20\": 0.8\n}", "{\n    \"b36\": 0.9,\n    \"b28\": 0.85\n}"], "6364c0ba90e50fcafdbb4aad": ["Based on the text of the paper, the author's notes, and the titles of the references, the most pertinent source papers that inspired this paper could be:\n\n{\n\"b39\": 0.9,\n\"b47\": 0.85,\n\"b37\": 0.75,\n\"b12\": 0.65,\n\"b16\": 0.6,\n\"b34\": 0.55\n} \n\nThese papers are chosen based on the key phrases indicating inspiration, motivation, or direct reference in the paper's text. Confidence scores are estimated based on the frequency and context of these references. The higher the score, the more influential the source paper is likely to have been.", "Based on the text and the titles of the references, the source papers that have inspired this work could be:\n\n{\n\"b37\": 0.9,\n\"b47\": 0.9,\n\"b39\": 0.9,\n\"b12\": 0.8,\n\"b16\": 0.8,\n\"b34\": 0.8\n}\n\nThe papers b37 (Best offset hardware prefetching) and b47 (Multi lookahead offset prefetching) are mentioned as direct inspirations for the Berti L1D prefetcher. The paper b39 (Bouquet of instruction pointers instruction pointer classifier based spatial hardware prefetching) is also mentioned as a source of inspiration, but to a lesser degree. Papers b12 (Bingo spatial data prefetcher), b16 (Perceptron based prefetch filtering), and b34 (Path confidence based lookahead prefetching) are referenced as key works in the field of data prefetching, which might have indirectly influenced the development of Berti. The confidence scores are set based on the frequency and context of their mentions in the text.", "Based on the provided academic paper text and the author notes, the following are the source papers that have most inspired the given paper:\n\n1. \"Best offset hardware prefetching\" [b37]: This paper is directly referenced as an inspiration for the \"Berti\" algorithm proposed in the given paper, with the authors noting that the 'best offset prefetcher (BOP) inspires us to ask this question'. Confidence score: 1.0\n\n2. \"Multi-lookahead offset prefetching\" [b47]: The paper is mentioned as one of the L1D prefetchers. Berti is compared to this approach and it's one of the paper's contributions to improve upon it. Confidence score: 0.9\n\n3. \"Instruction pointer classifier-based prefetching\" [b39]: This is another paper that the authors compare their approach with and appears to have informed the development of their work. Confidence score: 0.9\n\n4. \"Berti: a per page best request time delta prefetcher\" [b45]: This paper is cited as the source of inspiration for the name of the prefetcher proposed in the given paper. Confidence score: 1.0\n\n5. \"Path confidence based lookahead prefetching\" [b34]: The paper is mentioned as a state-of-the-art delta prefetcher. It also seems to have influenced the algorithm proposed in the given paper. Confidence score: 0.8\n\n6. \"Perceptron based prefetch filtering\" [b16]: This paper is also mentioned in the context of prefetching techniques that the authors contrast their approach with. Confidence score: 0.8\n\n7. \"Bingo spatial data prefetcher\" [b12]: This paper is referenced as a state-of-the-art prefetching technique that the authors compare their approach with. Confidence score: 0.8\n\nBased on the provided information, these papers seem to be the most pertinent source papers. However, the confidence scores are estimated based on the given text and may not perfectly reflect the level of influence each source paper had on the given paper. \n\nThe JSON format of the results would look like this:\n{\"b37\": 1.0, \"b47\": 0.9, \"b39\": 0.9, \"b45\": 1.0, \"b34\": 0.8, \"b16\": 0.8, \"b12\": 0.8}", "Based on the text and titles of the papers, the following papers seem to be the most relevant and have heavily influenced the main contributions of the paper:\n\n1. \"Best offset hardware prefetching\" [b37]: The paper specifically mentions that Berti was inspired by the BOP technique presented in this work.\n2. \"Multi-lookahead offset prefetching\" [b47]: The paper provides a comparison of Berti with MLOP and hence, this work seems to have influenced the development of Berti.\n3. \"Bouquet of instruction pointers instruction pointer classifier based spatial hardware prefetching\" [b39]: This paper proposes IPCP, an L1D data prefetcher, which is compared with Berti in the given text, indicating that it has influenced the development of Berti.\n4. \"Berti a per page best request time delta prefetcher\" [b45]: This paper is directly mentioned as an inspiration for the Berti prefetcher.\n\nThe \"confidence score\" is subjective and can vary based on the interpretation of the text. Here, I assign a confidence score of 1 to paper [b37] and [b45] as they are explicitly mentioned as inspirations. For papers [b47] and [b39], I assign a confidence score of 0.9 as they are compared with Berti, indicating significant influence, but are not explicitly mentioned as inspirations.\n\nThe result in the JSON format is:\n{\n    \"b37\": 1,\n    \"b45\": 1,\n    \"b47\": 0.9,\n    \"b39\": 0.9\n}", "Based on the text and the task, the most pertinent source papers for the given academic paper seem to be:\n\n- The Best Offset Prefetcher (BOP) [b37]: This is directly mentioned as the inspiration for the Berti prefetcher proposed in the paper.\n- The Multi-Lookahead Offset Prefetching (MLOP) [b47]: This paper is mentioned as an extension of BOP, which is the main inspiration for the Berti prefetcher.\n- The Instruction Pointer Classifier-based Prefetching (IPCP) [b39]: This paper is mentioned as a state-of-the-art L1D prefetcher, and it seems that Berti is being compared with IPCP in terms of performance.\n- The paper from DPC-3 [b45]: It's stated that the Berti prefetcher is inspired by Berti from DPC-3, which is likely the paper with the title \"berti a per page best request time delta prefetcher\".\n\nHere is the result in the required JSON format:\n\n{\n\"b37\" : 1,\n\"b47\" : 0.8,\n\"b39\" : 0.8,\n\"b45\" : 1\n}\n\nThe confidence scores are all high (0.8 and 1) because these papers are directly mentioned in the text as important for the development of the Berti prefetcher."], "63dcdb422c26941cf00b6339": ["{\n    \"b7\": 0.9,\n    \"b12\": 0.9,\n    \"b20\": 0.9,\n    \"b30\": 0.9,\n    \"b40\": 0.8\n}", "{\n  \"b7\": 0.9,\n  \"b20\": 0.9,\n  \"b30\": 0.9,\n  \"b12\": 0.9,\n  \"b34\": 0.8,\n  \"b26\": 0.8\n}", "{\n    \"b7\": 0.9,\n    \"b20\": 0.9,\n    \"b30\": 0.9,\n    \"b40\": 0.9\n}", "{\n  \"b7\": 0.95,\n  \"b20\": 0.95,\n  \"b30\": 0.95,\n  \"b40\": 0.95\n}", "{\n    \"b7\": 0.9,\n    \"b20\": 0.85,\n    \"b30\": 0.85,\n    \"b40\": 0.8,\n    \"b12\": 0.75,\n    \"b34\": 0.75\n}"], "65364bdf939a5f40822568b2": ["Based on the given text and references, the source papers that potentially inspired the given paper could be:\n\n- 'b4': \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" - The given paper mentions using a similar methodology to BERT, suggesting that the authors drew inspiration from this work.\n- 'b7': \"Pinner: Former Sequence Modeling for User Representation at Pinterest\" - The given paper mentions directly that it considers this work to be the most similar to theirs under the methodology of NBP (Next Behavior Prediction).\n- 'b3': \"Learning Transferable User Representations with Sequential Behaviors via Contrastive Pretraining\" - Mentioned as an alternative methodology for learning behavioral sequence representation, indicating potential influence.\n- 'b6': \"Exploiting Behavioral Consistence for Universal User Representation\" - Again, mentioned as a similar literature to the given paper, likely indicating some level of inspiration or influence.\n- 'b8': \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" - The given paper directly states that it was inspired by this work to adopt a prompt training approach.\n\nThe json format with the reference number and confidence score (based on the given text) would look like this:\n\n{\n\"b4\": 0.8,\n\"b7\": 0.9,\n\"b3\": 0.7,\n\"b6\": 0.7,\n\"b8\": 0.9\n}", "Based on the given text, the source papers that inspired the author the most are:\n\n1. \"Exploring the limits of transfer learning with a unified text to text transformer\" [b8]: The author specifically mentioned being inspired by this work and adapted the prompt training approach from this paper.\n\n2. \"Learning representations by maximizing mutual information across views\" [b1]: This paper is mentioned in the context of using Siamese networks for learning user-oriented representations, which is a key part of the proposed method.\n\n3. \"Exploring simple siamese representation learning\" [b2]: This paper was referenced when discussing the contrastive regularization part of the proposed method.\n\n4. \"Exploiting behavioral consistence for universal user representation\" [b6]: This paper seems to have a similar methodology to the proposed work in that it also doesn't follow the conventional MBP or NBP methodology and focuses on predicting behavior distribution.\n\n5. \"Pinner former: Sequence modeling for user representation at Pinterest\" [b7]: The author directly compares their work to this paper and they both focus on predicting long-term user behavior.\n\n6. \"Learning Transferable User Representations with Sequential Behaviors via Contrastive Pretraining\" [b3]: This paper is referenced in the context of contrastive learning for modeling sequence-level user representation, which is a key part of the proposed method.\n\nSo, the final result in the json format should be:\n{\n\"b8\": 1,\n\"b1\": 0.9,\n\"b2\": 0.9,\n\"b6\": 0.8,\n\"b7\": 0.8,\n\"b3\": 0.8\n}", "Based on the text of the paper, the key challenges, and the algorithm proposed by the author, the following references seem to be the most pertinent source papers:\n\n'b7': \"pinner former sequence modeling for user representation at pinterest\" appears to be a significant inspiration for the author's work on the Multi-scale Stochastic Distribution Prediction (MSDP) method. The author specifically mentions this work while discussing previous literature that attempted to predict long-term user behavior, which appears to be a key aspect of their own approach.\n\n'b3': \"learning transferable user representations with sequential behaviors via contrastive pretraining\" seems to be a source paper that inspires the author's method. The author refers to this work while discussing other methodologies for learning behavioral sequence representation and then goes on to introduce a Contrastive Regularization term into their algorithm.\n\n'b6': \"exploiting behavioral consistence for universal user representation\" is another source paper that appears to have inspired the author's work. The author refers to this paper while discussing other methodologies that deviate from conventional Masked Behavior Prediction (MBP) or Next Behavior Prediction (NBP). \n\n'b8': \"exploring the limits of transfer learning with a unified text to text transformer\" is mentioned as a source paper which inspired the author's adoption of a prompt training approach.\n\nBased on the provided information, these are the source papers that seem to have heavily influenced the main contributions of the paper:\n\n{'b7': 0.9, 'b3': 0.8, 'b6': 0.75, 'b8': 0.7}\n\nPlease note that these confidence scores are tentative and may vary based on further information from the paper or the author.", "Based on the provided text and the reference titles, the source papers that inspired the given paper are as follows:\n\n1. \"Learning representations by maximizing mutual information across views\" [b1] - The paper mentions the use of a contrastive regularization term in the proposed algorithm, which is a concept from contrastive learning, a methodology employed in [b1]. Confidence Score: 0.8\n\n2. \"Exploring simple siamese representation learning\" [b2] - The paper references the idea of SimSiam from contrastive learning, which is a key concept in [b2]. Confidence Score: 0.8\n\n3. \"Learning transferable user representations with sequential behaviors via contrastive pretraining\" [b3] - The paper's methodology of predicting user behaviors' distribution over time seems to be inspired by the sequential behaviors via contrastive pretraining concept in [b3]. Confidence Score: 0.7\n\n4. \"Attention is all you need\" [b11] - The paper mentions the use of attention mechanisms, a key concept introduced in [b11], in the context of NLP and user modeling. Confidence Score: 0.7\n\n5. \"Exploring the limits of transfer learning with a unified text to text transformer\" [b8] - The paper's use of a prompt training approach seems inspired by the Text-to-Text Transfer Transformer (T5) model introduced in [b8]. Confidence Score: 0.8\n\n6. \"Exploiting behavioral consistence for universal user representation\" [b6] - The paper seems to draw on the idea of behavioral consistency from [b6]. Confidence Score: 0.8\n\nThese confidence scores are estimated based on the frequency of references to these papers in the given text and the relevance of their titles and mentioned concepts to the main contributions of the given paper. \n\nThe result in json format is:\n\n{\n\"b1\": 0.8,\n\"b2\": 0.8,\n\"b3\": 0.7,\n\"b11\": 0.7,\n\"b8\": 0.8,\n\"b6\": 0.8\n}", "Based on the given paper's text, the primary challenge is to predict user Behavior Distribution over a period of time to generate robust representation from user behavior sequences. The author of the paper develops a new paradigm called Multi-scale Stochastic Distribution Prediction (MSDP) to address this challenge.\n\nThe paper mentions several references, but based on the given guidelines the most relevant references are as follows:\n\n1. \"NLP [b4] [b11]\"\n2. \"mask some of the behaviors in the sequence and predict the masked ones [b9] [b13]\"\n3. \"an auto-regressive manner [b14] [b17]\"\n4. \"MBP model [b9]\"\n5. \"NBP model [b14]\"\n6. \"[b7]\"\n7. \"[b3]\"\n8. \"[b6]\"\n9. \"[b8]\"\n10. \"contrastive learning [b2]\"\n\nHowever, not all references are directly relevant to the paper's main contributions. After careful analysis, the following references are most pertinent as they are directly linked to the paper's core contributions and are indicated by key phrases:\n\n1. \"[b4]\" and \"[b11]\" are mentioned in the context of the methodology in NLP, which is one of the main bases of the proposed MSDP.\n\n2. \"[b7]\" is specifically mentioned as a similar literature to the work that also aims to predict the long-term user behavior over a certain period of time.\n\n3. \"[b6]\" is highlighted as another similar literature to the work that learns the sequence representation by maximizing the Kullback-Leibler divergence between the item occurrence distribution of a previous range of time and that of future.\n\n4. \"[b8]\" is explicitly mentioned as inspiring the adoption of a prompt training approach in the paper.\n\n5. \"[b2]\" is referred to in the context of contrastive learning, which is introduced as a behavior reconstruction task in the proposed method.\n\nBased on the above analysis, the most pertinent source papers are [b4], [b11], [b7], [b6], [b8], and [b2].\n\nHere is the JSON format result:\n{\n    \"b4\": 0.9,\n    \"b11\": 0.9,\n    \"b7\": 1,\n    \"b6\": 1,\n    \"b8\": 1,\n    \"b2\": 0.8\n}"], "623184035aee126c0f4848ed": ["Based on the key phrases and the context of the paper, the source papers that inspire this paper most are:\n\n- \"Dissecting neural computations in the human auditory pathway using deep neural networks for speech\" [b0] - The paper's primary challenge is to understand the computations and representations in the human auditory system during speech perception. This paper directly relates to this challenge and is cited when speaking about the novel methods used to tackle these challenges. Confidence Score: 0.9\n\n- \"Deep Speech: End-to-end Speech Recognition in English and Mandarin\" [b12] - This paper directly influenced the use of deep neural networks in automatic speech recognition (ASR), a key focus of the given paper. Confidence Score: 0.8\n\n- \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" [b14] - This paper inspired the use of the HuBERT model, a transformer-based self-supervised model. Confidence Score: 0.9\n\n- \"A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy\" [b16] - This paper provided the inspiration for using task-oriented pretrained DNN models in sensory neuroscience, a method adopted in the given paper. Confidence Score: 0.8\n\n- \"Neural encoding of speech and music in the auditory cortices\" [b33] - This paper is referenced in relation to the specific challenges of understanding neural responses throughout the auditory system. Confidence Score: 0.7\n\n- \"Wav2Vec: A Framework for Self-Supervised Learning of Speech Representations\" [b13] - This paper has been referenced for the use of the Wav2Vec model, which has been used in the given paper. Confidence Score: 0.8\n\nBased on the above references, it seems that the most critical source papers are b0, b12, b14, b16, b13, and b33. These papers have a direct impact on the main contributions of the paper.\n\nIn JSON format:\n\n{\"b0\": 0.9, \"b12\": 0.8, \"b14\": 0.9, \"b16\": 0.8, \"b13\": 0.8, \"b33\": 0.7}", "Based on the given text, the source papers that are most likely to have influenced this paper are:\n\n- \"HuBERT self-supervised speech representation learning by masked prediction of hidden units\" [b14]: The paper is cited multiple times throughout the text, and the model is used for experiments in this paper, indicating that it likely had a significant influence. It also appears to have inspired the authors to explore the use of self-supervised models for speech representation learning.\n\n- \"WavVec: a framework for self-supervised learning of speech representations\" [b13]: This paper is also frequently referenced and its model is used in the experiments.\n\n- \"Deep Speech: end-to-end speech recognition in English and Mandarin\" [b12]: The paper is referenced multiple times and its model is also used in the experiments.\n\n- \"A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy\" [b16]: This paper appears to be a significant source of inspiration for the authors, as it is referenced in the context of previous work on using supervised convolutional neural networks to build encoding models for auditory responses.\n\n- \"Dissecting neural computations in the human auditory pathway using deep neural networks for speech\" [b0]: Given its title and the content of the given paper, it seems highly likely that this source would have been influential. \n\n- \"Speech corpus of Chinese discourse and the phonetic research\" [b67]: This paper seems to have influenced the authors' use of Mandarin speech in their experiments.\n\nIn JSON format:\n{\n  \"b14\": 0.9,\n  \"b13\": 0.8,\n  \"b12\": 0.7,\n  \"b16\": 0.7,\n  \"b0\": 0.6,\n  \"b67\": 0.6\n}", "Based on the given text and author notes, the following references seem to be the most pertinent source papers:\n\n1. \"HuBERT: Self-supervised speech representation learning by masked prediction of hidden units\" [b14]: This paper is mentioned multiple times and is clearly integral to the study as the AI speech model (HuBERT) is a major focus of the research.\n\n2. \"Wav2Vec: A Framework for Self-Supervised Learning of Speech Representations\" [b13]: Wav2Vec is another self-supervised model used in the study and is mentioned several times, indicating its importance.\n\n3. \"Deep Speech: End-to-end speech recognition in English and Mandarin\" [b12]: This paper is mentioned as a part of the DNN models used in the research, and it's also important as it deals with automatic speech recognition (ASR), a key topic in the given paper.\n\n4. \"A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy\" [b16]: This paper is cited as a previous work that has used supervised CNNs for building encoding models for auditory responses, which seems to be a method employed in the current research.\n\n5. \"Parallel and distributed encoding of speech across human auditory cortex\" [b33]: This paper is referenced multiple times and seems to provide crucial data used in the research, particularly about intracranial cortical recordings.\n\n6. \"A speech envelope landmark for syllable encoding in human superior temporal gyrus\" [b45]: This paper appears to be relevant as it deals with speech envelope, which seems to be an important aspect in the research.\n\n7. \"Neural coding in the ventral visual cortex is largely driven by spatial statistics in retinotopic space\" [b19]: This paper seems to contribute to the understanding of neural coding, which is a key topic in the given paper.\n\nBased on my analysis, the source papers are:\n{\n\"b14\": 1.0,\n\"b13\": 0.9,\n\"b12\": 0.9,\n\"b16\": 0.8,\n\"b33\": 1.0,\n\"b45\": 0.8,\n\"b19\": 0.7\n}", "Based on the provided text, the main challenges outlined in the paper are to understand the computations and representations that occur and emerge throughout the auditory system during speech perception. The paper's novel contributions are the findings that: the hierarchy in DNNs trained to learn speech representations correlates with that in the ascending auditory pathway; unsupervised models without explicit linguistic knowledge can learn similar feature representations as the human auditory pathway; deeper layers in speech DNNs correlate with speech-responsive populations in the nonprimary auditory cortex, driven by specific computations aligned with critical linguistically relevant temporal structures; and DNN-based models, unlike traditional linear encoding models, can reveal language-specific properties in cross-language speech perception.\n\nFrom the text, the following references are indicated as sources that have inspired or motivated the paper's contributions through key phrases:\n\n1. \"HuBERT model, a transformer-based self-supervised model trained to predict masked portions of speech [b14]\"\n2. \"the Wav2Vec 2 unsupervised model, a transformer-based self-supervised model trained for contrastive learning that distinguishes spans of a speech utterance from distractors [b13]\"\n3. \"the Wav2Vec 2 supervised model, a transformer-based supervised but do not explain neural coding or perform well in natural speech recognition. Conversely, classical neural encoding models [b9] [b10] [b11] explain neural coding during speech perception but cannot be directly adapted to a unified computational framework of speech perception. Modern artificial intelligence (AI) models using deep neural networks (DNNs) are approaching human-level performance in automatic speech recognition (ASR) [b12] [b13] [b14]\"\n4. \"In particular, Kell et al. used supervised convolutional neural networks (CNNs) to build encoding models for auditory responses in functional magnetic resonance imaging (fMRI) recordings and showed an aligned hierarchy between the CNNs and the auditory cortex [b16]\"\n5. \"Unlike core object recognition in vision modeling, which uses static images [b21] , speech involves dynamic sequences often modeled by sequence-to-sequence (seq2seq) learning in modern AI [b13] [b14] [b22]\"\n6. \"These models extract dynamic representations of speech, shaped by both the current input (a nonlinear transformation of the current input) and the long-term dependencies in the input sequences (for example, the history of an input sequence)\"\n\nTherefore, based on the above analysis, the source papers for this paper are:\n\n1. \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units [b14]\"\n2. \"Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech Representations [b13]\"\n3. \"Estimating Spatio-Temporal Receptive Fields of Auditory and Visual Neurons from Their Responses to Natural Stimuli [b9]\"\n4. \"Selective Cortical Representation of Attended Speaker in Multi-Talker Speech Perception [b10]\"\n5. \"Emergence of Neural Encoding of Auditory Objects While Listening to Competing Speakers [b11]\"\n6. \"Deep Speech: End-to-End Speech Recognition in English and Mandarin [b12]\"\n7. \"A Task-Optimized Neural Network Replicates Human Auditory Behavior, Predicts Brain Responses, and Reveals a Cortical Processing Hierarchy [b16]\"\n8. \"Attention Is All You Need [b22]\"\n\nThe confidence scores for each source paper can't be determined accurately without a specific method or tool to quantify the degree of inspiration or influence each source paper has on the paper in question. However, each of the eight source papers listed could feasibly have a high confidence score given their direct relevance to the main contributions of the paper. \n\nHere is the result in JSON format:\n\n{\n    \"b14\": \"0.8\",\n    \"b13\": \"0.8\",\n    \"b9\": \"0.8\",\n    \"b10\": \"0.7\",\n    \"b11\": \"0.7\",\n    \"b12\": \"0.7\",\n    \"b16\": \"0.8\",\n    \"b22\": \"0.7\"\n}\n\nPlease note that the confidence scores are just estimated and may not accurately reflect the actual degree of influence each source paper has on the paper in question.", "Based on the paper text and the titles of the references, the most relevant source papers are:\n\n1. \"Hubert self-supervised speech representation learning by masked prediction of hidden units\" [b14]: This paper seems to be the main inspiration for the model used in the analyzed text, as it is mentioned multiple times and its methods are directly applied in the research. Confidence score: 1.0\n\n2. \"Wav vec a framework for self supervised learning of speech representations\" [b13]: This paper is also mentioned multiple times and is likely another major source of inspiration, as the authors also apply its methods in their research. Confidence score: 0.9\n\n3. \"Deep speech end to end speech recognition in english and mandarin\" [b12]: This paper's influence can be inferred from the text, as it appears to be a key reference in the discussion of automatic speech recognition. Confidence score: 0.8\n\n4. \"A task optimized neural network replicates human auditory behavior predicts brain responses and reveals a cortical processing hierarchy\" [b16]: This paper is referenced in the text as an example of how neural networks have been used to predict auditory responses, which seems to be a significant aspect of the research. Confidence score: 0.8\n\n5. \"Parallel and distributed encoding of speech across human auditory cortex\" [b33]: This paper is referenced multiple times in the text and seems to provide important background information for the research. Confidence score: 0.7\n\n6. \"Estimating spatio temporal receptive fields of auditory and visual neurons from their responses to natural stimuli\" [b9]: This paper seems to provide a framework for the methods used in the research, as it is mentioned in the context of evaluating the similarity between the auditory pathway and DNN models. Confidence score: 0.7\n\nThe result in json format would be:\n```\n{\n    \"b14\": 1.0,\n    \"b13\": 0.9,\n    \"b12\": 0.8,\n    \"b16\": 0.8,\n    \"b33\": 0.7,\n    \"b9\": 0.7\n}\n```"], "656d3a42939a5f4082629226": ["Based on the text and author notes, it appears that the paper is primarily focused on addressing a key challenge in Multimodal Large Language Models (MLLMs): their inability to predict future events based on present observations. The authors propose two novel training methods, Foresight Pre-Training (FPT) and Foresight Instruction-Tuning (FIT), to help MLLMs acquire this \"foresight minds\". These methods are inspired by the modern learning paradigm of LLMs. \n\nThe proposed solution is tested and validated using a new MLLM, Merlin, which shows impressive performance in future inference and visual understanding tasks.\n\nTo identify the source papers, the following references seem to be highly relevant:\n\n1) \"GPT-4V [b48]\" and \"Bard [b1]\" are mentioned as recent advancements in MLLMs but struggle to foresee future events based on current image observations. These papers seem to have inspired the authors to address this challenge.\n\n2) \"b19\" is cited as the neuroscience theory that gives the term \"predictive processing\" to the ability human beings have to predict future events based on current observations. This theory seems to form the foundational understanding for the authors' approach.\n\n3) \"b30\" and \"b54\" seem important as they are cited while explaining the two-stage system of how humans foresee the future. This understanding seems to have guided the design of the authors' proposed solution.\n\n4) \"b50\" and \"b70\" are mentioned as demonstrating the powerful logical reasoning ability of LLMs, which seems to have influenced the authors' approach to the second stage of their solution.\n\n5) \"b10\" and \"b73\" are mentioned as possible, but not ideal, ways to extract dynamic clues. These papers seem to have guided the authors' choice of learning objective.\n\n6) \"b41\", \"b81\", and \"b84\" are mentioned as previous MLLMs that only supported interaction with a single image. These papers seem to have inspired the authors to design Merlin to support multi-image interaction.\n\nBased on these points, the source papers influencing the paper's contributions are likely [b48], [b1], [b19], [b30], [b54], [b50], [b70], [b10], [b73], [b41], [b81], [b84].\n\nThe confidence score for each reference would need further analysis of the full text and the content of each referenced paper, which is beyond the scope of the provided text. However, a rough estimate based on the number of mentions and their context in the text could be used. \n\nHere is the result in a JSON format:\n\n{ \"b48\": 0.8, \n\"b1\": 0.8, \n\"b19\": 0.9, \n\"b30\": 0.9, \n\"b54\": 0.9, \n\"b50\": 0.7, \n\"b70\": 0.7, \n\"b10\": 0.75, \n\"b73\": 0.75, \n\"b41\": 0.8, \n\"b81\": 0.8, \n\"b84\": 0.8 }", "Based on the provided paper text and the author notes, the following references seem to have had a significant impact on the paper's main contributions:\n\n1. \"b48\": This paper is not directly cited in the text, but it is inferred from the author's notes that it may be GPT-4V. The current study's method of Foresight Pre-Training (FPT) and Foresight Instruction-Tuning (FIT) is said to be inspired by the modern learning paradigm of LLMs, which is likely to include GPT-4V.\n2. \"b1\": The paper 'palm technical report' appears to provide a key comparison point for the current study's methodology. The authors specifically state that they \"start from dividing human's process of foreseeing the future into a two-stage system [b30] [b54] : [b0] observing the dynamic clues of the subject and then [b1] analyzing the behavior pattern and reasoning what might happen according to the observation\".\n3. \"b19\": Mentioned as a source for the neuroscience theory of predictive processing, which is a foundational concept for the proposed methods.\n4. \"b30\" and \"b54\": Both appear to be key sources for the theory behind a two-stage system of foreseeing the future, which is a central component of the paper's methodology.\n5. \"b10\" and \"b73\": Both are mentioned in the context of explicit modeling of next frames, which seems to be directly related to the proposed \"future modeling\" methods.\n6. \"b18\", \"b26\", \"b31\", \"b59\", \"b64\", \"b77\": All these references are cited as sources for tasks containing richly labeled data used in the Foresight Pre-Training (FPT).\n7. \"b41\", \"b81\", \"b84\": These references are used for comparison with the current study's model, Merlin. They are mentioned as previous MLLMs which only supported interaction with a single image, while Merlin provides a richer multi-image interaction and future reasoning.\n\nThe JSON format answer is:\n{\n\"b48\": 0.9,\n\"b1\": 0.85,\n\"b19\": 0.7,\n\"b30\": 0.8,\n\"b54\": 0.8,\n\"b10\": 0.75,\n\"b73\": 0.75,\n\"b18\": 0.7,\n\"b26\": 0.7,\n\"b31\": 0.7,\n\"b59\": 0.7,\n\"b64\": 0.7,\n\"b77\": 0.7,\n\"b41\": 0.8,\n\"b81\": 0.8,\n\"b84\": 0.8\n} \n\nNote that the confidence scores are assigned subjectively based on the analysis of the text, with a higher score indicating a stronger inferred influence on the study.", "Based on the text and the author's notes, the paper focuses on addressing limitations of existing Multimodal Large Language Models (MLLMs) and introducing a new approach for future modeling. This approach involves two methods, Foresight Pre-Training (FPT) and Foresight Instruction-Tuning (FIT), which are inspired by the modern learning paradigm of Large Language Models (LLMs). The paper also introduces a new MLLM named Merlin.\n\nFrom the text, the following references are mentioned in the context of the authors' work or the methods they propose:\n\n1. b19: This reference is mentioned in the context of the neuroscience theory predictive processing, which could have inspired the authors' concept of \"foresight minds\".\n2. b30 and b54: These references are mentioned in the context of human's process of foreseeing the future, which could have inspired the authors' two-stage system.\n3. b50 and b70: These references are mentioned in the context of the logical reasoning ability of LLMs, which could have inspired the authors' work on MLLMs.\n4. b10 and b73: These references are mentioned in the context of explicitly modeling next frames, which could have influenced the authors' approach to future modeling.\n5. b18, b26, b31, b59, b64, and b77: These references are mentioned in the context of Foresight Pre-Training (FPT), which suggests that they could be the sources of the tasks used for multi-task learning.\n6. b41, b81, and b84: These references are mentioned in the context of the previous MLLMs, which could have influenced the authors' work on Merlin.\n\nBased on this, the following references appear to be the most pertinent source papers:\n\n- b19: \"The free energy principle: a unified brain theory\"\n- b30: \"Identifying natural images from human brain activity\"\n- b54: \"A system in the human brain for predicting the actions of others\"\n- b50: \"Detect what you need via reasoning\"\n- b70: \"Chain of thought: Prompting elicits reasoning in large language models\"\n- b10: \"Transformers: Predicting the future - Applying attention in next frame and time series forecasting\"\n- b73: \"Videogpt: Video generation using vq-vae and transformers\"\n- b18: \"Lasot: A high-quality benchmark for large-scale single object tracking\"\n- b26: \"Got-10k: A large high-diversity benchmark for generic object tracking in the wild\"\n- b31: \"Referitgame: Referring to objects in photographs of natural scenes\"\n- b59: \"Objects: A large-scale high-quality dataset for object detection\"\n- b64: \"Dancetrack: Multi-object tracking in uniform appearance and diverse motion\"\n- b77: \"From recognition to cognition: Visual commonsense reasoning\"\n- b41: \"Visual instruction tuning\"\n- b81: \"Bootstrapping multimodal llms via precise referring instruction tuning\"\n- b84: \"Minigpt: Enhancing vision-language understanding with advanced large language models\"\n\nIn JSON format:\n\n{\n  \"b19\": 0.9,\n  \"b30\": 0.8,\n  \"b54\": 0.8,\n  \"b50\": 0.85,\n  \"b70\": 0.85,\n  \"b10\": 0.75,\n  \"b73\": 0.75,\n  \"b18\": 0.9,\n  \"b26\": 0.9,\n  \"b31\": 0.9,\n  \"b59\": 0.9,\n  \"b64\": 0.9,\n  \"b77\": 0.9,\n  \"b41\": 0.85,\n  \"b81\": 0.85,\n  \"b84\": 0.85\n}\n\nThe confidence scores are based on the specific context in which the references are mentioned, with higher scores indicating a closer link to the authors' work or the methods they propose.", "Based on the text of the academic paper, the following source papers are identified:\n\n1. \"The free energy principle a unified brain theory\" [b19]: This paper is referred to in the introduction when discussing the concept of predictive processing in neuroscience. This paper could be one of the inspirations for the concept of \"Foresight minds\". Confidence Score: 0.6\n\n2. \"Transformers predicting the future applying attention in next frame and time series forecasting\" [b10]: This reference is mentioned when discussing the existing deficiencies in MLLMs and the idea of explicitly modelling next frames. Confidence Score: 0.7\n\n3. \"Emergent abilities of large language models\" [b69]: This paper is referred to when discussing the powerful logical reasoning ability of LLMs. Confidence Score: 0.7\n\n4. \"Objects a large scale high quality dataset for object detection\" [b59]: This paper is mentioned when discussing the various tasks containing richly labeled data for performing multitask learning. Confidence Score: 0.6\n\n5. \"MPlug Owl: Modularization Empowers Large Language Models with Multimodality\" [b74]: This paper is mentioned when discussing the results that significantly surpass previous baselines. Confidence Score: 0.6\n\n6. \"Large Language Models Fail on Trivial Alterations to Theory of Mind Tasks\" [b68]: This paper is referred to when discussing the limitations of current MLLMs in predicting future events based on the observed current state. Confidence Score: 0.7\n\nBased on the titles of the references, the following can be inferred:\n\n1. \"Chain of Thought: Prompting Elicits Reasoning in Large Language Models\" [b70]: This title suggests that the paper could be about a method similar to FIT, which articulates reasoning in conjunction with the trajectory for each object referenced. Confidence Score: 0.6\n\n2. \"Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning\" [b81]: The title suggests that the paper could be related to FIT, which is a form of Trajectory Chain of Thought. Confidence Score: 0.6\n\nThe final result in the JSON format:\n\n{\n\"b19\": 0.6,\n\"b10\": 0.7,\n\"b69\": 0.7,\n\"b59\": 0.6,\n\"b74\": 0.6,\n\"b68\": 0.7,\n\"b70\": 0.6,\n\"b81\": 0.6\n}", "Based on the provided text and the author notes, the source papers that inspired the authors the most can be determined. The key phrases such as \"inspired by\", \"motivated by\", \"based on\" are used to identify these papers. The primary challenges outlined in the paper are related to Multimodal Large Language Models (MLLMs) not being able to predict future events or outcomes based on current observations. The authors propose to address this issue with two innovative methods: Foresight Pre-Training (FPT) and Foresight Instruction-Tuning (FIT), which are inspired by the modern learning paradigm of LLMs. \n\nBased on these details, the source papers are:\n\n1. \"flamingo a visual language model for few shot learning\" [b0]: The authors are inspired by the work on multi-modal models like Flamingo and use its ideas in their work.\n\n2. \"palm technical report\" [b1]: The authors reference this work as part of their two-stage system of foreseeing the future.\n\n3. \"humans predict liquid dynamics using probabilistic simulation\" [b4]: This paper is referenced as part of the human ability to predict future events, which the authors aim to replicate in their model.\n\n4. \"transformers predicting the future applying attention in next frame and time series forecasting\" [b10]: This paper is likely a source of inspiration for the authors' work on using transformers to predict future frames.\n\n5. \"gpt 3 language models are few shot learners\" [b7]: The paper mentions GPT-3, indicating it could have influenced their work on large language models.\n\n6. \"language models are unsupervised multitask learners\" [b51]: This paper is referenced in the context of the authors' discussion on LLMs, suggesting it may have influenced their work.\n\n7. \"learning transferable visual models from natural language supervision\" [b52]: This paper is referenced in the context of the authors' discussion on visual and language models, suggesting it may have influenced their work.\n\nThe source papers are directly relevant to the paper's novel contributions and approaches. The confidence scores of these papers are not provided, but based on the context they are mentioned in, it is likely they had a significant influence on the authors' work.\n\nThe JSON format of the result would be:\n\n{\n\"b0\": 0.85,\n\"b1\": 0.8,\n\"b4\": 0.9,\n\"b10\": 0.9,\n\"b7\": 0.8,\n\"b51\": 0.8,\n\"b52\": 0.8\n}"], "6427029c90e50fcafd5d6bdb": ["Based on the text provided, the source papers that are most likely to have inspired the given paper include:\n\n1. \"b0: flatflash exploiting the byte accessibility of ssds within a unified memory storage hierarchy\": This paper seems to have inspired the use of flash memory in the author's system architecture. Confidence score: 0.9\n2. \"b48: a case for hardware based demand paging\": The mention of accelerating paging in the given paper likely indicates that this paper was an inspiration. Confidence score: 0.8\n3. \"b34: unison cache a scalable and effective die stacked dram cache\": This paper appears to have influenced the author's implementation of a DRAM cache. Confidence score: 0.7\n4. \"b49: asynchronous i o stack a low latency kernel i o stack for ultra low latency ssds\": The given paper's focus on low latency and asynchronous flash access may have been inspired by this paper. Confidence score: 0.8\n5. \"b63: aifm high performance application integrated far memory\": This paper's emphasis on high performance memory may have influenced the author's approach. Confidence score: 0.7\n6. \"b43: reaping the performance of fast nvm storage with udepot\": The given paper's use of flash memory for high performance may have been inspired by this work. Confidence score: 0.7\n\nJSON Format:\n{\"b0\": 0.9, \"b48\": 0.8, \"b34\": 0.7, \"b49\": 0.8, \"b63\": 0.7, \"b43\": 0.7}", "Based on the academic text and the given reference titles, the source papers that most influenced the paper are:\n\n1. \"A case for hardware based demand paging\" [b48] - The author's AstriFlash design seems to be largely inspired by the work in this paper. The confidence score here is 0.9.\n2. \"Asynchronous i o stack a low latency kernel i o stack for ultra low latency ssds\" [b49] - This paper is mentioned in the context of the author's discussion of overcoming performance overheads. The confidence score is 0.8.\n3. \"Reaping the performance of fast nvm storage with udepot\" [b43] - This paper is referenced multiple times throughout the text, indicating it may have played a significant role in formulating the author's approach. Confidence score: 0.7.\n4. \"Efficiently enabling conventional block sizes for very large die stacked dram caches\" [b50] - This paper seems to have influenced the author's approach to memory management in AstriFlash. Confidence score: 0.7.\n5. \"Rebooting virtual memory with midgard\" [b26] - This paper is mentioned in the context of addressing the challenge of translating addresses for servers with large DRAM provisions. Confidence score: 0.6.\n6. \"Architecting phase change memory as a scalable dram alternative\" [b47] - This paper appears to have been influential in the author's discussion of memory scaling challenges. Confidence score: 0.5.\n\nThis results in the following JSON format:\n\n{\n    \"b48\": 0.9,\n    \"b49\": 0.8,\n    \"b43\": 0.7,\n    \"b50\": 0.7,\n    \"b26\": 0.6,\n    \"b47\": 0.5\n}", "Based on the text of the academic paper and the titles of the references, the source papers that inspired the authors most are:\n\n- b0: 'FlatFlash: Exploiting the Byte Accessibility of SSDs within a Unified Memory-Storage Hierarchy'\n- b8: 'BSSD: The Case for Dual Byte and Block Addressable Solid State Drives'\n- b43: 'Reaping the Performance of Fast NVM Storage with uDepot'\n- b48: 'A Case for Hardware-Based Demand Paging'\n- b49: 'Asynchronous I/O Stack: A Low-Latency Kernel I/O Stack for Ultra-Low Latency SSDs'\n- b66: 'Knights Landing: Second Generation Intel Xeon Phi Product'\n\nThese papers are selected based on their relevance to the novel contributions of the given paper, which includes the introduction of a flash-based system for online services, the use of lightweight thread switching to handle flash accesses, and the elimination of demand paging overheads using an accelerated DRAM miss handler. The selected source papers either directly inspired these novel methods or provided a basis for comparison and improvement. \n\nHere is the result in the json format:\n{\n\"b0\": 0.9,\n\"b8\": 0.85,\n\"b43\": 0.9,\n\"b48\": 0.85,\n\"b49\": 0.85,\n\"b66\": 0.8\n}", "Based on the text of the paper, the most pertinent source papers are:\n\n1. \"b0: flatflash exploiting the byte accessibility of ssds within a unified memory storage hierarchy\" - The paper refers to the work done in b0 for providing direct user access to flash using normal loads and stores (confidence score: 0.9)\n2. \"b48: a case for hardware based demand paging\" and \"b49: asynchronous i o stack a low latency kernel i o stack for ultra low latency ssds\" - The paper cites the proposals made in these papers to combat the performance overheads by accelerating paging (confidence score: 0.85)\n3. \"b43: reaping the performance of fast nvm storage with udepot\" - The paper discusses the optimization of one class of overheads as done in b43 (confidence score: 0.8)\n4. \"b66: Knights Landing second generation Intel Xeon Phi product\" - The paper mentions using DRAM as a hardware-managed cache as done in Intel Knights Landing to eliminate the OS memory-management overheads (confidence score: 0.85)\n5. \"b63: aifm high performance application integrated far memory\" - The paper mentions the use of lightweight user-level thread switches as done in b63 (confidence score: 0.8)\n6. \"b16: improved multithreading techniques for hiding communication latency in multiprocessors\" - The paper refers to the work done in b16 for switch-on-miss architectures (confidence score: 0.7)\n7. \"b75: mechanisms for store wait free multiprocessors\" - The paper extends existing speculation proposals as done in b75 to cover the SB (confidence score: 0.75)\n8. \"b34: unison cache a scalable and effective die stacked dram cache\" - The paper refers to the work done in b34 for implementing an in-DRAM miss status table to track concurrent misses at low cost (confidence score: 0.7)\n\nThe final JSON format would be: {\"b0\": 0.9, \"b48\": 0.85, \"b49\": 0.85, \"b43\": 0.8, \"b66\": 0.85, \"b63\": 0.8, \"b16\": 0.7, \"b75\": 0.75, \"b34\": 0.7}", "Based on the text and the provided instructions, the following list of source papers can be identified:\n\n1. \"b0: flatflash exploiting the byte accessibility of ssds within a unified memory storage hierarchy\" - The authors mention \"providing direct access to flash [b0]\" as a previous proposal to combat performance overheads, which directly relates to their proposed method of tightly integrating flash and DRAM.\n\n2. \"b43: reaping the performance of fast nvm storage with udepot\" - The authors mention this source in relation to the concept of bypassing paging and virtual memory altogether, which seems to have influenced their approach.\n\n3. \"b48: a case for hardware based demand paging\" - Mentioned as a previous attempt to accelerate paging, which is a core challenge in the paper's issue.\n\n4. \"b49: asynchronous i o stack a low latency kernel i o stack for ultra low latency ssds\" - This is mentioned as another attempt to accelerate paging, showing it as a significant influence on the authors' work.\n\n5. \"b66: knights landing second generation intel xeon phi product\" - This paper is mentioned in the context of using DRAM as a hardware-managed cache, which is a key part of the authors' proposed solution.\n\n6. \"b63: aifm high performance application integrated far memory\" - The authors mention this paper in the context of using user-level threads for low-cost contexts to efficiently overlap flash accesses, which is a significant part of the paper's proposed solution.\n\n7. \"b34: unison cache a scalable and effective die stacked dram cache\" - Mentioned in the context of traditional on-chip cache designs, which appears to have influenced the methods proposed in this paper.\n\nThe confidence score for each paper is estimated as follows:\n\n```\n{\n \"b0\": 0.9,\n \"b43\": 0.8,\n \"b48\": 0.9,\n \"b49\": 0.9,\n \"b66\": 0.9,\n \"b63\": 0.9,\n \"b34\": 0.7\n}\n```\n\nThe score is based on the frequency and context of the mention in the text, with higher scores indicating a higher likelihood of direct inspiration or influence on the paper's main contributions."], "64a29612d68f896efa28bc9c": ["Based on the text and author notes, the paper mainly focuses on a new method to optimize resource allocation and scheduling on spatial AI accelerators. The paper introduces a Resource Allocation Tree notation for defining the inter-layer scheduling space on tiled accelerators and analyzes its impact on hardware behaviors. It also develops an end-to-end scheduling framework, SET, to explore this space efficiently.\n\nGiven this information, the source papers that might have heavily influenced this work are ones related to spatial AI accelerators, resource allocation, scheduling, and tiled architectures. Spotting key phrases and references that are directly relevant to the paper\u2019s novel contributions, the following papers seem to be the main sources:\n\n1. \"Fused-layer cnn accelerators\" [b2]: The authors mention Fused-layer as a method that allocates different computing resources for different layers and orchestrates the layers in a layer-pipeline (LP) manner. This method is mentioned as a reference point for the novel method proposed in this paper.\n\n2. \"Tangram optimized coarse grained dataflow for scalable nn accelerators\" [b17]: The authors refer to Tangram as a method that achieves significant energy savings on a large-scale accelerator, indicating that this work might have been an inspiration.\n\n3. \"Mind mappings enabling efficient algorithmaccelerator mapping space search\" [b20]: This paper is mentioned in the context of various methods that have explored the intra-layer scheduling space.\n\n4. \"Shortcut mining exploiting cross layer shortcut reuse in dcnn accelerators\" [b6]: This paper is cited when discussing the optimization of existing heuristic patterns.\n\n5. \"Cosa scheduling by constrained optimization for spatial accelerators\" [b23]: This paper is referred to when discussing various notations used to specify the overall space of intra-layer scheduling.\n\n6. \"Confuciux autonomous hardware resource assignment for dnn accelerators using reinforcement learning\" [b29]: This paper is referred to when discussing various methods that have explored the intra-layer scheduling space.\n\n7. \"Simba scaling deep learning inference with multi chip module based architecture\" [b47]: This paper is mentioned, which might have influenced the authors' approach to defining a HW-tile.\n\n8. \"Tenet a framework for modeling tensor dataflow based on relation centric notation\" [b38]: This paper might have guided the authors' approach to defining the intra-layer scheduling spaces.\n\nIn conclusion, the most pertinent source papers appear to be [b2], [b17], [b20], [b6], [b23], [b29], [b47], and [b38]. \n\nThe confidence score for each reference can be calculated based on the frequency and context of their mentions in the text, which is not provided in this prompt.\n\nHere is the result in the requested JSON format: \n\n{\n  \"b2\": 0.8,\n  \"b17\": 0.8,\n  \"b20\": 0.7,\n  \"b6\": 0.7,\n  \"b23\": 0.7,\n  \"b29\": 0.7,\n  \"b47\": 0.7,\n  \"b38\": 0.7\n}", "Based on the information provided, the papers that seemed to have inspired the author the most include:\n\n- \"Fused-layer cnn accelerators\" [b2]: This paper is mentioned in the context of discussing the impact of inter-layer scheduling, a key focus of the author's paper. \n\n- \"Tangram optimized coarse grained dataflow for scalable nn accelerators\" [b17]: This paper is also mentioned in the context of discussing inter-layer scheduling, and the author specifically notes that Tangram achieves significant energy savings through optimized layer-pipeline and layer-sequential techniques. \n\n- \"Mind mappings enabling efficient algorithm-accelerator mapping space search\" [b20]: Mentioned when discussing the exploration of the intra-layer scheduling space. \n\n- \"Confuciux scheduling by constrained optimization for spatial accelerators\" [b23]: This paper seems to have been influential in the author's research on scheduling strategies for spatial accelerators.\n\n- \"Understanding reuse performance and hardware cost of dnn dataflow a data centric approach\" [b33]: This paper seems to have been influential in the author's research on intra-layer scheduling, as it is mentioned in the context of discussing the various methods used to explore this space.\n\n- \"Simba scaling deep learning inference with multi chip module based architecture\" [b47]: This paper is cited in the context of discussing the control granularity of tiled accelerators, which is a key focus of the author's paper. \n\nThis is an educated guess based on the information provided. The confidence scores are not provided due to the lack of specifics in how they should be calculated. Ideally, they should be calculated using a more systematic approach such as text mining or citation analysis.\n\nHere is the JSON output:\n\n```\n{\n  \"b2\": \"Fused-layer cnn accelerators\",\n  \"b17\": \"Tangram optimized coarse grained dataflow for scalable nn accelerators\",\n  \"b20\": \"Mind mappings enabling efficient algorithm-accelerator mapping space search\",\n  \"b23\": \"Confuciux scheduling by constrained optimization for spatial accelerators\",\n  \"b33\": \"Understanding reuse performance and hardware cost of dnn dataflow a data centric approach\",\n  \"b47\": \"Simba scaling deep learning inference with multi chip module based architecture\"\n}\n```", "Based on the provided text and the titles of the papers, the source papers of the given paper seem to be:\n\n- \"inter layer scheduling space definition and exploration for tiled accelerators\" [None]: This paper seems to be the main inspiration for the given paper as it introduces the resource allocation modeling and inter-layer scheduling methods for spatial AI accelerators. Confidence Score: 1.0\n\n- \"Fused-layer cnn accelerators\" [b2]: Mentioned in the text as a method for allocating different computing resources for different layers and orchestrating the layers in a layer-pipeline manner. Confidence Score: 0.85\n\n- \"Tangram optimized coarse grained dataflow for scalable nn accelerators\" [b17]: The given paper cites this work as impactful to the field of inter-layer scheduling, specifically mentioning its energy savings results. Confidence Score: 0.85\n\n- \"Mind mappings enabling efficient algorithm-accelerator mapping space search\" [b20]: This paper has proposed ways to explore the spatial and temporal allocation of resources, which is a key element in the given paper. Confidence Score: 0.75\n\n- \"CoSA scheduling by constrained optimization for spatial accelerators\" [b23]: This paper's work on scheduling for spatial accelerators could have been a source of inspiration for the given paper. Confidence Score: 0.75\n\n- \"Simba scaling deep learning inference with multi chip module based architecture\" [b47]: This paper's work on scaling deep learning inference could have provided a foundation for the current paper's research. Confidence Score: 0.65\n\n- \"Tenet a framework for modeling tensor dataflow based on relation-centric notation\" [b38]: This paper's work on modeling tensor dataflow is relevant to the given paper's focus on inter-layer scheduling and could have been a source of inspiration. Confidence Score: 0.6\n\nThe returned result in the json format is: {\"None\": 1.0, \"b2\": 0.85, \"b17\": 0.85, \"b20\": 0.75, \"b23\": 0.75, \"b47\": 0.65, \"b38\": 0.6}\n", "Based on the given text and the criteria provided, the source papers that appear to have influenced the paper's novel contributions most directly are:\n\n1. \"Fused-layer cnn accelerators\" [b2] - The authors mention Fused-layer as an existing method that allocates different computing resources for different layers and orchestrates the layers in a layer-pipeline manner. This indicates that this paper might have served as a source of inspiration for the authors in developing their own method.\n\n2. \"Tangram optimized coarse-grained dataflow for scalable nn accelerators\" [b17] - The authors refer to Tangram as an example of a work that achieves energy savings by using optimized LP and LS techniques on a large-scale accelerator, suggesting that this work might have influenced their own approach.\n\n3. \"Inter layer scheduling space definition and exploration for tiled accelerators\" [b0] - This paper is by the same authors and it seems to directly relate to the topic of the paper.\n\n4. \"Mind mappings enabling efficient algorithmaccelerator mapping space search\" [b20] - This paper is mentioned in the context of various methods used for intra-layer scheduling, which is one of the key challenges addressed in this paper.\n\n5. \"Shortcut mining exploiting cross layer shortcut reuse in dcnn accelerators\" [b6] - This paper is referred to in the context of optimizing existing heuristic patterns, LP and LS, which is a challenge the authors of this paper are trying to address.\n\n6. \"Cosa scheduling by constrained optimization for spatial accelerators\" [b23] - This paper is mentioned in the context of various methods used for intra-layer scheduling, which is one of the key challenges addressed in this paper.\n\nThe confidence score for each paper can be calculated based on the number of times it is mentioned and the context in which it is mentioned. Here is a possible list of source papers in json format:\n\n{\n    \"b2\": 0.9,\n    \"b17\": 0.8,\n    \"b0\": 0.8,\n    \"b20\": 0.7,\n    \"b6\": 0.7,\n    \"b23\": 0.7\n} \n\nPlease note that the confidence scores are subjective and might vary based on the interpretation of the text.", "Based on the text of the paper, the primary challenges involve efficient resource allocation and scheduling for large-scale accelerators. The paper introduces a novel approach to inter-layer scheduling using a Resource Allocation Tree notation, which is a first in this field. \n\nBy analyzing the main text and the author notes, the source papers that appear to have strongly influenced the current research are:\n\n1. \"Fused-layer cnn accelerators\" [b2] - The text mentions this paper while discussing the concept of inter-layer scheduling. This paper seems to have introduced a key method that the authors have built upon.\n\n2. \"Tangram optimized coarse grained dataflow for scalable nn accelerators\" [b17] - Mentioned in the context of the significant energy savings achieved using their techniques. The authors also seem to have built on these techniques for their research.\n\n3. \"Simba scaling deep learning inference with multi chip module based architecture\" [b47] - This paper is referred to while discussing the concept of large-scale accelerators, which is a key part of the current research.\n\n4. \"Mind mappings enabling efficient algorithm-accelerator mapping space search\" [b20] - This paper appears to have influenced the authors' approach to exploring the scheduling space efficiently.\n\n5. \"Cosa scheduling by constrained optimization for spatial accelerators\" [b23] - This paper seems to have provided a basis for the authors' exploration of the scheduling space. \n\n6. \"Confuciux autonomous hardware resource assignment for dnn accelerators using reinforcement learning\" [b29] - This paper is mentioned in the field of intra-layer scheduling research, which is an important context for the authors' work on inter-layer scheduling. \n\n7. \"Tenet a framework for modeling tensor dataflow based on relation centric notation\" [b38] - Cited as a reference for the definition of the intra-layer scheduling spaces. This work seems to have influenced the authors' approach to defining the inter-layer scheduling space.\n\n8. \"Understanding reuse performance and hardware cost of dnn dataflow a data centric approach\" [b33] - This paper is used to understand the concept of intra-layer scheduling, which is crucial to the authors' research on inter-layer scheduling.\n\nBased on the information provided, the confidence score for each paper is estimated as follows:\n{ \"b2\": 0.9, \"b17\": 0.8, \"b47\": 0.85, \"b20\": 0.75, \"b23\": 0.75, \"b29\": 0.75, \"b38\": 0.7, \"b33\": 0.75 }"], "654f510b939a5f408289af51": ["Based on the given text, the paper's primary challenges include comparing the performance between RISC-V and AArch64, and how these differences affect a program's execution time. The novel method introduced is the use of The Simulation Engine to analyze path lengths and critical paths for a series of benchmarks. \n\nFrom the text, it's hard to find the direct source papers which inspire the author most because there are no key phrases indicating this. However, the following papers seem to be directly related to the main contributions of the paper:\n\n1. \"an empirical comparison of the risc v and aarch instruction sets\" - This paper directly relates to the main topic of the paper, which is comparing RISC-V and AArch64.\n\n2. \"automatic throughput and critical path analysis of x and arm assembly kernels\" - This paper appears to be relevant as it discusses critical path analysis, a method used in the given paper.\n\n3. \"memory bandwidth and machine balance in current high performance computers\" - This paper could be related, as the given paper discusses analyzing path lengths and critical paths, which could relate to memory bandwidth and machine balance.\n\n4. \"high performance in silico virtual drug screening on many core processors\" - This paper could be related as it discusses high performance computations, which is a focus of the given paper.\n\n5. \"a performance analysis of modern parallel programming models using a compute bound application\" - This paper seems to be related as it discusses performance analysis, a main method used in the given paper.\n\nBased on the analysis, the answer in json format would be:\n\n{\n\"b0\": 0.8,\n\"b6\": 0.9,\n\"b8\": 0.8,\n\"b9\": 0.7,\n\"b12\": 0.8\n}\n\nPlease note that this analysis is based on the titles of the papers and the given text, thus the confidence scores are estimated and might not be entirely accurate.", "Based on the text of the paper and the titles of the references, the source papers that likely most inspired this paper are:\n\n- \"Automatic throughput and critical path analysis of x and arm assembly kernels\" [b6]: This paper likely provided the methodological inspiration for the performance analysis conducted in this study.\n- \"A performance analysis of modern parallel programming models using a compute bound application\" [b12]: This paper might have provided the conceptual framework for evaluating the performance of different instruction set architectures.\n- \"Memory bandwidth and machine balance in current high performance computers\" [b8]: This paper might have inspired the authors to use the STREAM benchmark in their study.\n- \"Miniapps derived from production HPC applications using multiple programing models\" [b10]: This paper likely inspired the authors to use mini-apps in their experiment. \n\nTherefore, the JSON format result could be:\n\n{\"b6\": 0.9, \"b12\": 0.8, \"b8\": 0.7, \"b10\": 0.7}\n\nThe confidence score is given between 0 and 1 based on the relevance of the source paper to the text of the paper. Note that the confidence scores are subjective and may vary based on the exact content of the paper and its context.", "Based on the given text, the most pertinent source papers that influenced the given academic paper are:\n\n1. \"an empirical comparison of the risc v and aarch instruction sets\" (None)\nThe title of this paper indicates that it directly compares the RISC V and AArch instruction sets, which is the major focus of the given academic paper. The given paper also frequently discusses the two instruction sets.\n\n2. \"automatic throughput and critical path analysis of x and arm assembly kernels\" (b6)\nThe given academic paper conducts a similar analysis on the RISC V and AArch instruction sets, indicating that this paper could have influenced the methodology or approach used.\n\n3. \"high performance in silico virtual drug screening on many core processors\" (b9)\nThis paper appears to be related to the molecular docking simulation used in the given academic paper, suggesting that it could have influenced the selection or implementation of this simulation.\n\n4. \"a performance analysis of modern parallel programming models using a compute bound application\" (b12)\nThis paper's title suggests that it analyses the performance of modern programming models, similar to how the given academic paper analyses the performance of the RISC V and AArch instruction sets. It could have influenced the methodology or approach used in the given paper.\n\nBased on the titles, no other papers seem to be directly relevant to the main contributions of the given academic paper. \n\nThe confidence scores for these sources, on a scale from 0 to 1, could be:\n\n- None: 0.9\n- b6: 0.8\n- b9: 0.7\n- b12: 0.7\n\nThe confidence scores are higher for the first two references because they appear to be more directly related to the main focus of the given academic paper. The scores for the last two references are a bit lower as they seem to be related to specific aspects or methods used in the given paper, rather than its overall focus.\n\n{\n\"None\": 0.9,\n\"b6\": 0.8,\n\"b9\": 0.7,\n\"b12\": 0.7\n}", "Based on the main challenges, proposed algorithm, novel methods, and approaches in the paper, the source papers which the author indicates that they inspire them most are:\n\n    1. \"The Simulation Engine 6 from the University of Bristol's High Performance Computing group\" - The Simulation Engine seems to be a significant tool that the authors used to execute their research. However, there is no reference number for this.\n    2. \"STREAM [b8]\" - The benchmark for measuring sustained memory bandwidth was used in the study.\n    3. \"CloverLeaf Serial [b7]\" - This high energy physics simulation was used in the paper.\n    4. \"MiniBUDE [b9] [b12]\" - The authors use this app to approximate the behaviour of a molecular docking simulation used for drug discovery.\n    5. \"Minisweep [b10]\" - This radiation transportation mini app was used for nuclear reactor neutronics modeling.\n    \nThe titles of the papers are:\n    1. \"The Simulation Engine 6 from the University of Bristol's High Performance Computing group\" - No reference paper\n    2. \"STREAM [b8]\" - \"Memory bandwidth and machine balance in current high performance computers\"\n    3. \"CloverLeaf Serial [b7]\" - \"Cloverleaf preparing hydrodynamics codes for exascale\"\n    4. \"MiniBUDE [b9] [b12]\" - \"High performance in silico virtual drug screening on many core processors\" and \"A performance analysis of modern parallel programming models using a compute-bound application\"\n    5. \"Minisweep [b10]\" - \"Miniapps derived from production HPC applications using multiple programing models\"\n\nIn json format:\n\n    {\n        \"b8\": 0.9,\n        \"b7\": 0.9,\n        \"b9\": 0.9,\n        \"b12\": 0.9,\n        \"b10\": 0.9\n    }", "Based on the provided text, the paper is focused on comparing the performance of RISC-V and ARMv8-a architectures using a series of benchmarks. The paper's contributions include adding new RISC-V support to the SimEng microarchitectural simulator, conducting a path length analysis for a series of benchmarks, providing a critical path analysis, and showing results from a scaled critical path and windowed critical path analysis. \n\nHere are the source papers inferred from the text:\n\n1. 'Automatic throughput and critical path analysis of x and arm assembly kernels' [b6]: The paper mentions the use of a critical path analysis to gain theoretical minimum run times for a set of benchmarks targeting the two ISAs, which is a methodology that seems to be inspired by this reference. \n\n2. 'A performance analysis of modern parallel programming models using a compute bound application' [b12]: This paper might have been used as a basis for the performance analysis conducted in the paper.\n\n3. 'Cloverleaf preparing hydrodynamics codes for exascale' [b7]: The CloverLeaf benchmark used in the performance analysis might have been inspired by this paper.\n\n4. 'High performance in silico virtual drug screening on many core processors' [b9]: The use of the MiniBUDE benchmark in the analysis suggests that this paper might have been a source of inspiration.\n\n5. 'Memory bandwidth and machine balance in current high performance computers' [b8]: The STREAM benchmark used in the analysis is likely inspired by this paper.\n\n6. 'Miniapps derived from production HPC applications using multiple programing models' [b10]: The MiniSweep benchmark used in the analysis might have been inspired by this paper.\n\nThe confidence scores are estimated and may vary based on a more detailed analysis of the text.\n\nThe result in JSON format:\n\n{\n  \"b6\": 0.9,\n  \"b12\": 0.8,\n  \"b7\": 0.7,\n  \"b9\": 0.7,\n  \"b8\": 0.7,\n  \"b10\": 0.7\n}"], "646aecaad68f896efa05a6c5": ["Given the text of the paper and the titles of the references, the source papers that inspired the paper most seem to be:\n\n1. \"Tool Learning with Foundation Models\" [b49] - The paper mentions the limitations of in-context learning, which is a key concept introduced in this reference.\n2. \"Language Models Can Teach Themselves to Use Tools\" [b52] - This paper seems to be the primary inspiration for the development of ToolkenGPT as the authors have used the idea of representing each tool as a new token to augment the vocabulary.\n3. \"Chain of Thought Prompting Elicits Reasoning in Large Language Models\" [b61] - This reference is directly mentioned when the author discusses the compatibility of ToolkenGPT with advanced prompting techniques like Chain-of-Thought (CoT).\n4. \"React: Synergizing Reasoning and Acting in Language Models\" [b64] - This paper is mentioned when the author discusses the limitations of existing approaches like ReAct.\n5. \"LLaMA-13B\" [b58] - This paper could have inspired the authors as it is used as the base LLM in the experiments.\n6. \"Training Verifiers to Solve Math Word Problems\" [b9] - This paper is mentioned in the context of the numerical reasoning experiments conducted in the paper.\n7. \"API Bank: A Benchmark for Tool Augmented LLMS\" [b32] - This paper is mentioned when discussing a concurrent work for tool retrieval based on text embedding.\n\nBased on the information provided and the paper's text, the confidence score for each reference can be estimated as follows:\n\n{ \"b49\": 0.9, \"b52\": 1.0, \"b61\": 0.8, \"b64\": 0.9, \"b58\": 0.8, \"b9\": 0.7, \"b32\": 0.7 }", "Based on the paper text and the titles of the cited papers, the most pertinent source papers that inspired the work on ToolkenGPT are:\n\n1. \"Language Models Can Teach Themselves to Use Tools\" [b52]\n2. \"Tool Learning with Foundation Models\" [b49]\n3. \"Chain of Thought Prompting Elicits Reasoning in Large Language Models\" [b61]\n4. \"ReAct Synergizing Reasoning and Acting in Language Models\" [b64]\n5. \"TALM Tool Augmented Language Models\" [b46]\n6. \"API Bank A Benchmark for Tool Augmented LLMs\" [b32]\n7. \"The Power of Scale for Parameter Efficient Prompt Tuning\" [b30]\n8. \"Parameter Efficient Transfer Learning for NLP\" [b19]\n\nThis list of papers is derived based on their direct mentions in the paper's text, the relevance of their titles to the work presented in the paper, and the fact that these papers present methods, approaches, or concepts that directly contribute to the development of ToolkenGPT.\n\nIn JSON format:\n\n{\n\"b52\": 0.9,\n\"b49\": 0.9,\n\"b61\": 0.8,\n\"b64\": 0.8,\n\"b46\": 0.75,\n\"b32\": 0.75,\n\"b30\": 0.7,\n\"b19\": 0.7\n}", "Based on the text of the paper, the following source papers appear to have greatly influenced the authors in the development of ToolkenGPT:\n\n1. \"Language Models are Few-Shot Learners\" [b4]: This paper seems to be a fundamental reference for the authors, as it establishes the concept of Large Language Models (LLMs) which the authors build upon in their work.\n\n2. \"Tool Learning with Foundation Models\" [b49]: This paper is mentioned multiple times throughout the text, indicating a significant influence. It seems to provide the concept of in-context learning, which the authors build upon and try to improve with their method.\n\n3. \"Fine-tuning LLMs to use tools\" [b46] and [b52]: These papers are repeatedly referenced when discussing the conventional methods of tool learning for LLMs. The authors seem to use these papers as a basis for comparison and improvement in their work.\n\n4. \"Chain of Thought Prompting Elicits Reasoning in Large Language Models\" [b61]: This paper is specifically highlighted as being compatible with ToolkenGPT, indicating that its method has influenced the development of the authors' method.\n\n5. \"ReAct: Synergizing Reasoning and Acting in Language Models\" [b64]: This paper is directly compared to the authors' method, indicating that it has been a significant influence on their work.\n\n6. \"Param-Efficient Transfer Learning for NLP\" [b19] and \"Low-Rank Adaptation of Large Language Models\" [b20]: These papers appear to have influenced the development of ToolkenGPT's efficient tuning method.\n\nHere is the result in JSON format:\n{\n    \"b4\": 0.9,\n    \"b49\": 0.9,\n    \"b46\": 0.8,\n    \"b52\": 0.8,\n    \"b61\": 0.8,\n    \"b64\": 0.8,\n    \"b19\": 0.7,\n    \"b20\": 0.7\n}\n\nThe confidence scores are based on the frequency of mention, the context in which they are mentioned, and the degree to which the authors appear to build upon the concepts introduced in these papers.", "Based on the text of the paper and the titles of the references, the source papers that inspired the most are:\n\n1. \"Tool learning with foundation models\" [b49]: The paper's main approach, ToolkenGPT, is a tool learning method, which shares similarities with the ideas presented in this paper. The phrase \"In-context learning [e.g., [b64] [b49] [b6]\" seems to indicate that this paper was a major source of inspiration.\n\n2. \"Language models can teach themselves to use tools\" [b52]: The paper repeatedly mentions fine-tuning and in-context learning, which are discussed in this paper. The phrase \"Fine-tuning [e.g., [b52] [b46]\" suggests that this paper was a significant influence.\n\n3. \"Automatic multi-step reasoning and tool use for large language models\" [b45]: This paper is mentioned in the context of in-context learning, which is a key concept in the paper.\n\n4. \"Chain of thought prompting elicits reasoning in large language models\" [b61]: This paper is mentioned several times, especially in the context of advanced prompting techniques, which are used in the paper's approach.\n\n5. \"React synergizing reasoning and acting in language models\" [b64]: This paper is mentioned in the context of in-context learning and reasoning, which are key concepts in the paper.\n\n6. \"Language models are few shot learners\" [b4]: This paper is one of the first references mentioned and is likely a significant source of inspiration due to its foundational work on language models.\n\n7. \"API bank a benchmark for tool augmented llms\" [b32]: This paper is mentioned in the context of the retrieval of tools, which seems to be a relevant concept for the paper.\n\nThe confidence score for each source paper is subjective and depends on the specific analysis of the paper's text, but based on the discussion and mentions in the text, a possible assignment of confidence scores could be:\n\n{ \"b49\": 0.9, \"b52\": 0.8, \"b45\": 0.7, \"b61\": 0.7, \"b64\": 0.7, \"b4\": 0.6, \"b32\": 0.6 }", "Based on the text of the paper, the main challenges are: 1) mastering and controlling a wide array of tools for Large Language Models (LLMs), 2) the rapid emergence of new tools that introduces additional richness and complexity to the task of tool learning for LLMs, and 3) the limitations of in-context learning. The authors propose a solution called ToolkenGPT, which represents each tool as a new token (\"toolken\") to augment the vocabulary, allowing for quick adaptation to new tools without the need for any LLM fine-tuning.\n\nThe most directly relevant source papers seem to be:\n\n- [b64]: \"react synergizing reasoning and acting in language models\" - this paper is referenced multiple times in relation to in-context learning, a method which the authors state has limitations that their method, ToolkenGPT, overcomes.\n\n- [b49]: \"tool learning with foundation models\" - this paper is also referenced multiple times, specifically in relation to the challenges and limitations of in-context learning, which ToolkenGPT aims to solve.\n\n- [b46]: \"talm tool augmented language models\" - this paper is mentioned in relation to fine-tuning LLMs, which the authors argue is costly and struggles to generalize to emergent or updated tools. ToolkenGPT is presented as an alternative solution.\n\n- [b52]: \"language models can teach themselves to use tools\" - this paper is also mentioned in relation to the limitations of fine-tuning LLMs, and how their method, ToolkenGPT, provides a solution. \n\n- [b61]: \"chain of thought prompting elicits reasoning in large language models\" - this paper is mentioned as one of the latest popular approaches which ToolkenGPT outperforms.\n\nSo, the source papers are:\n\n{\"b64\": 0.9, \"b49\": 0.8, \"b46\": 0.8, \"b52\": 0.8, \"b61\": 0.7}"], "62c64f2e5aee126c0f6cf0f9": ["Based on the author's notes and the text of the paper, the most pertinent source papers that inspired the given paper are:\n- \"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\" [b5]\n- \"Dense Passage Retrieval for Open-Domain Question Answering\" [b16]\n- \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b9]\n- \"Condenser: A Pre-training Architecture for Dense Retrieval\" [b11]\n- \"Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder\" [b23]\n- \"coCondenser: Unsupervised Corpus-Aware Language Model Pre-training for Dense Passage Retrieval\" [b12]\n\nHere is the final result in the JSON format:\n```\n{\n    \"b5\": 1,\n    \"b16\": 0.9,\n    \"b9\": 0.85,\n    \"b11\": 0.8,\n    \"b23\": 0.75,\n    \"b12\": 0.7\n}\n```", "Based on the text and the titles of the references, the most relevant source papers for the given paper could be:\n\n1. \"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\" [b5]: The paper explicitly mentions that their method is inspired by ELECTRA.\n\n2. \"Condenser: A Pre-training Architecture for Dense Retrieval\" [b11]: The paper compares its method with Condenser, suggesting this paper has a significant influence.\n\n3. \"CoCondenser: Unsupervised Corpus-Aware Language Model Pre-training for Dense Passage Retrieval\" [b12]: CoCondenser is mentioned as a comparison in the paper, indicating it may have influenced the approach.\n\n4. \"Dense Passage Retrieval for Open-Domain Question Answering\" [b16]: This paper is mentioned in the introduction as a key component of the method's application.\n\n5. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [b9]: BERT is a fundamental model that this paper's method is based on.\n\n6. \"Less Is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder\" [b23]: This paper is mentioned as a similar observation, which suggests it could have inspired the work.\n\nIn JSON format, the results could be represented as:\n\n{\n\"b5\": 1,\n\"b11\": 0.8,\n\"b12\": 0.8,\n\"b16\": 0.7,\n\"b9\": 0.9,\n\"b23\": 0.7\n}\n\nIn the JSON object, the key is the reference number and the value is the confidence score.", "Based on the given text and author's notes, the most pertinent source papers that inspired the given paper could be:\n\n1. \"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\" - The author has explicitly mentioned that the method proposed in the paper is inspired by ELECTRA's alternative language modeling objective to improve sample efficiency and reduce mismatch between pre-training and fine-tuning. This is a strong indicator that the ELECTRA paper is a key source of inspiration. Confidence Score: 1\n\n2. \"Condenser: A pre-training architecture for dense retrieval\" - The author has compared their method with Condenser and pointed out the advantages of their method over Condenser. This indicates that the Condenser paper could be a source of inspiration, especially in terms of the challenges it presented that the author sought to overcome. Confidence Score: 0.8\n\n3. \"Dense Passage Retrieval for Open-Domain Question Answering\" - This paper was mentioned in the text and could be a source of inspiration given the direct relation to the topic of dense retrieval methods. Confidence Score: 0.7\n\n4. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" - The author has mentioned BERT in the context of their method and how it differs, indicating that the BERT paper could be a source of inspiration, especially in terms of the challenges it presented that the author sought to overcome. Confidence Score: 0.7\n\n5. \"coCondenser: Unsupervised Corpus-Aware Language Model Pre-Training for Dense Passage Retrieval\" - The coCondenser paper is mentioned in the comparison section, indicating that it could be a source of inspiration, especially in terms of the challenges it presented that the author sought to overcome. Confidence Score: 0.7\n\n6. \"RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering\" - This paper is mentioned in the results comparison, indicating that it could be a source of inspiration, especially in terms of the challenges it presented that the author sought to overcome. Confidence Score: 0.6\n\n7. \"Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder\" - This paper is mentioned in the context of the author's method, indicating that it could be a source of inspiration, especially in terms of the challenges it presented that the author sought to overcome. Confidence Score: 0.6\n\n8. \"Introduction to Information Retrieval\" - This paper was referenced in the context of the foundational information about information retrieval, indicating it could be a source of inspiration. Confidence Score: 0.5\n\nThe JSON format of the result is: \n{\"b5\": 1, \"b11\": 0.8, \"b16\": 0.7, \"b9\": 0.7, \"b12\": 0.7, \"b30\": 0.6, \"b23\": 0.6, \"b27\": 0.5}", "Based on the given text, the paper seems to be heavily influenced by the following source papers:\n\n1. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators [b5]\n2. Dense Passage Retrieval for Open-Domain Question Answering [b16]\n3. Condenser: A Pre-training Architecture for Dense Retrieval [b11]\n4. coCondenser: Unsupervised Corpus-aware Language Model Pre-training for Dense Passage Retrieval [b12]\n\nSo, the result will be: \n\n{\n    \"b5\": 0.9, \n    \"b16\": 0.8, \n    \"b11\": 0.7,\n    \"b12\": 0.7\n} \n\nPlease note that the confidence scores are subjective and may vary depending on the interpretation of the text.\n", "Based on the text and the titles of the references, the source papers for the given paper appear to be:\n\n1. \"ELECTRA: Pre-training text encoders as discriminators rather than generators\" (b5)\n2. \"Condenser: A pre-training architecture for dense retrieval\" (b11)\n3. \"Dense Passage Retrieval for Open-Domain Question Answering\" (b16)\n4. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" (b9)\n\nThe paper mentions that it uses an \"ELECTRA-inspired replaced language modeling goal\" to improve efficiency, which suggests that the ELECTRA paper (b5) was a significant inspiration. It also refers to the Condenser (b11) and BERT (b9) methods in the context of its own approach, suggesting that these papers were also influential. Lastly, it mentions the goal of \"dense retrieval\", which is the focus of the Dense Passage Retrieval for Open-Domain Question Answering paper (b16), suggesting that it may have been a source of inspiration.\n\nHere is the result in json format:\n{\n    \"b5\": 0.9,\n    \"b11\": 0.8,\n    \"b16\": 0.8,\n    \"b9\": 0.7\n}"], "641137fd90e50fcafd17b84e": ["Based on the text and the titles of the references, the most pertinent source papers are likely to be:\n\n- 'b2': 'language models are few shot learners': The paper frequently mentions the use of large language models (LLMs) for query expansion. The phrase \"Inspired by [b2]\" appears in the text, indicating that this paper had a significant influence on the work.\nConfidence score: 0.9\n\n- 'b11': 'dense passage retrieval for open domain question answering': The paper discusses the use of BM25 and dense retrieval, and this paper is likely to have inspired their work in that area.\nConfidence score: 0.8\n\n- 'b21': 'relevance feedback in information retrieval': This paper discusses query expansion, which is a key part of the paper's methodology, suggesting it may have been an influential source.\nConfidence score: 0.7\n\n- 'b27': 'a simlm pre training with representation bottleneck for dense passage retrieval': This paper is likely to have contributed to the methodology used in the paper, especially in the area of dense retrieval.\nConfidence score: 0.7\n\n- 'b9': 'precise zero shot dense retrieval without relevance labels': This paper is likely to have inspired the paper's work on zero-shot retrieval.\nConfidence score: 0.7\n\n- 'b29': 'approximate nearest neighbor negative contrastive learning for dense text retrieval': This paper is referred to in the context of hard negative mining, a factor that influences the performance of the dense retrievers.\nConfidence score: 0.6\n\nPlease note that these are educated guesses based on the text of the paper, and the confidence scores reflect the degree of certainty based on the information provided. \n\nJSON format:\n\n{\"b2\": 0.9, \"b11\": 0.8, \"b21\": 0.7, \"b27\": 0.7, \"b9\": 0.7, \"b29\": 0.6}", "Based on the text of the paper, the techniques and methods the authors used, and the context in which they cited certain papers, the source papers for this academic paper are:\n\n1. \"Language models are few-shot learners\" [b2]: The authors used LLMs for query expansion, which was a core part of their proposed method, query2doc. They also mentioned this paper when discussing LLMs' ability to memorize an enormous amount of knowledge and language patterns. Confidence score: 0.9.\n\n2. \"Dense passage retrieval for open domain question answering\" [b11]: This paper was cited when authors talked about dense retrievers and their performance when large amounts of labeled data are available. Confidence score: 0.7.\n\n3. \"Relevance-based language models\" [b12]: It's mentioned in the context of query expansion and pseudo-relevance feedback, which are key aspects of the authors' research. Confidence score: 0.7.\n\n4. \"Precise zero-shot dense retrieval without relevance labels\" [b9]: The authors compared their method with HyDE, a method proposed in this paper. Confidence score: 0.7.\n\n5. \"A SimLM pre-training with representation bottleneck for dense passage retrieval\" [b27]: This paper is referred to when authors discussed the use of strong dense retrievers and their use of query2doc. Confidence score: 0.7.\n\n6. \"Relevance feedback in information retrieval\" [b21]: It's mentioned when authors talked about the long-standing technique of query expansion. Confidence score: 0.6.\n\nBased on the titles of each reference, the authors' proposed method and the context of the citations, these papers have likely influenced the authors' research directly. \n\n{\"b2\": 0.9, \"b11\": 0.7, \"b12\": 0.7, \"b9\": 0.7, \"b27\": 0.7, \"b21\": 0.6}", "Based on the text of the paper, the primary challenges outlined seem to be improving sparse and dense retrieval systems, which the author proposes to address by introducing a method called query2doc. This method generates pseudo-documents using large language models (LLMs) and then expands the query with these pseudo-documents.\n\nFrom examining the text and the references, the following references appear to be the most relevant source papers:\n\n1. \"Language models are few-shot learners\" [b2]: The author mentions the effectiveness of LLMs as query expansion models, which indicates that the paper [b2] may have been a significant inspiration for the proposed method.\n   \n2. \"Dense passage retrieval for open domain question answering\" [b11]: This paper is referenced in the context of discussing dense retrievers, which is a key area that the author's work aims to improve.\n\n3. \"A SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval\" [b27]: The author mentions that strong dense retrievers, including SimLM, also benefit from the query2doc method, which suggests that this paper may have influenced the author's work.\n   \n4. \"Relevance-based language models\" [b12]: The author compares their proposed method with pseudo-relevance feedback, which is a technique discussed in this paper.\n   \n5. \"Precise Zero-shot Dense Retrieval without Relevance Labels\" [b9]: The author compares their method with the approach of HyDE, which is the subject of this paper.\n\n6. \"Generate Rather than Retrieve: Large Language Models are Strong Context Generators\" [b30]: This paper is mentioned in the related work section, suggesting it may have influenced the author's work.\n\nThe JSON format of the source papers and their confidence scores are:\n\n{\n   \"b2\": 0.9,\n   \"b11\": 0.8,\n   \"b27\": 0.8,\n   \"b12\": 0.7,\n   \"b9\": 0.7,\n   \"b30\": 0.7\n}\n\nThis denotes that the paper listed as \"b2\" has the highest confidence score of 0.9, indicating it is likely the most influential source for this paper. Other papers have confidence scores ranging from 0.7 to 0.8, suggesting they also significantly influenced this paper.", "Based on the given academic paper text, the primary challenges outlined are the limitations of lexical-based sparse retrieval and embedding-based dense retrieval, and the lack of a simple, effective query expansion method for both sparse and dense retrieval systems. The paper proposes a method called 'query2doc' which generates pseudo-documents by prompting Large Language Models (LLMs) and uses these pseudo-documents to expand the query, improving retrieval results.\n\nThe following references seem to have influenced the current paper's core contributions:\n\n- 'b2': Language models are few-shot learners. It is mentioned that LLMs are used for generating pseudo-documents, which indicates that this paper has been a source of inspiration.\n\n- 'b11': Dense passage retrieval for open domain question answering. This paper is mentioned in the context of dense retrieval, suggesting that it has influenced the authors of the current paper.\n\n- 'b12': Relevance-based language models. This paper is referred to when discussing query expansion, indicating that it has played a role in shaping the authors' perspective on this topic.\n\n- 'b18': From Doc2query to DocTTTTTquery. This paper's document expansion method seems to have inspired the creation of the 'query2doc' method in the current paper.\n\n- 'b27': A SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval. The paper mentions that SimLM also benefits from query2doc, indicating that this method's principles have informed the authors' approach.\n\n- 'b9': Precise zero-shot dense retrieval without relevance labels. This paper is compared with the current work, indicating that it has influenced the authors' perspective.\n\nThe confidence scores for each source paper are not provided in the text. However, they could be inferred by examining the extent to which each paper is discussed, the specificity of the discussion, and the directness of the connection between the source paper's concepts and the current paper's contributions.\n\nThe returned JSON would look like:\n\n{ \n  \"b2\": 0.9, \n  \"b11\": 0.7,\n  \"b12\": 0.7,\n  \"b18\": 0.8,\n  \"b27\": 0.7,\n  \"b9\": 0.7\n}", "Based on the given text, the following papers seem to have the most significant influence on the paper:\n\n1. \"Language models are few shot learners\" [b2] - The method proposed in the paper relies heavily on Large Language Models (LLMs) for query expansion, which is a core concept introduced in this reference. The paper also uses an improved version of the GPT-3 model, which is discussed in this source paper.\n\n2. \"Dense passage retrieval for open-domain question answering\" [b11] - The paper contrasts its performance with the Dense Passage Retrieval (DPR) method, indicating that it served as a benchmark in their research.\n\n3. \"SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval\" [b27] - This paper is referenced multiple times, indicating its importance. The authors also use SimLM in their experiments and compare their method's performance against it.\n\n4. \"PRECISE: Zero-shot Dense Retrieval Without Relevance Labels\" [b9] - This paper is directly compared to the proposed method, suggesting that it played a significant role in shaping the research.\n\n5. \"Relevance-based Language Models\" [b12] - The paper discusses the limitations of traditional query expansion methods like RM3 which are introduced in this source.\n\n6. \"A Comparative Study of Methods for Estimating Query Language Models with Pseudo Feedback\" [b15] - This is another paper related to the concept of query expansion and pseudo-relevance feedback which is a core part of the proposed method.\n\nHere is the list in a JSON format:\n\n```json\n{\n  \"b2\": 0.9,\n  \"b11\": 0.85,\n  \"b27\": 0.85,\n  \"b9\": 0.8,\n  \"b12\": 0.75,\n  \"b15\": 0.75\n}\n```\nThe confidence scores are estimated based on the frequency and context of the references in the given text, with 1 being the highest confidence."], "657c181a939a5f4082ab98e2": ["{\n    \"b20\": 0.9,\n    \"b22\": 0.9,\n    \"b34\": 0.9,\n    \"b42\": 0.9,\n    \"b43\": 0.9,\n    \"b54\": 0.9\n}", "{\n  \"b20\": 0.95,\n  \"b22\": 0.9,\n  \"b34\": 0.9,\n  \"b42\": 0.9,\n  \"b43\": 0.9,\n  \"b54\": 0.95\n}", "{\n    \"b20\": 0.9,\n    \"b22\": 0.9,\n    \"b34\": 0.9,\n    \"b42\": 0.9,\n    \"b43\": 0.9,\n    \"b54\": 0.9\n}", "{\n    \"b6\": 0.95,\n    \"b10\": 0.95,\n    \"b26\": 0.95,\n    \"b33\": 0.95,\n    \"b35\": 0.95,\n    \"b20\": 0.9,\n    \"b22\": 0.9,\n    \"b34\": 0.9,\n    \"b42\": 0.9,\n    \"b43\": 0.9,\n    \"b54\": 0.9\n}", "{\n  \"b20\": 0.95,\n  \"b22\": 0.95,\n  \"b34\": 0.95,\n  \"b42\": 0.95,\n  \"b43\": 0.95,\n  \"b54\": 0.95\n}"], "6326303790e50fcafdf36ca9": ["Based on the text provided, the author is proposing a novel approach to prefetching which involves a mathematical framework for understanding and predicting potential prefetcher performance, a methodology for evaluating prefetching capabilities, and a way to close the gap to optimal performance using memory-level parallelism. The paper also evaluates the accuracy of the framework using helper threads, software prefetching, and FPGA prefetching. \n\nThe most relevant source papers that appear to inspire this work are:\n\n- 'b4': 'classifying memory access patterns for prefetching'\n- 'b7': 'evaluation of hardware data prefetchers on server processors'\n- 'b20': 'a primer on hardware prefetching'\n- 'b31': 'apt get profile guided timely software prefetching'\n- 'b37': 'path confidence based lookahead prefetching'\n\nThese papers are directly related to prefetching, which is the main topic of the paper. The author's approach seems to be inspired or motivated by these papers, as the author is extending the techniques they discuss or addressing the limitations they identify. \n\nThe references that precede direct inspiration or motivation phrases are:\n\n- 'b4': 'classifying memory access patterns for prefetching' - appears after \"Memory access patterns are well known to be application dependent, which makes it hard to prefetch in an accurate and timely manner.\"\n- 'b31': 'apt get profile guided timely software prefetching' - appears after \"Therefore, we argue that prefetching needs to be driven by dynamic application behavior.\"\n\nThe json format for the source papers would be:\n\n{\"b4\": 0.9, \"b7\": 0.8, \"b20\": 0.85, \"b31\": 0.9, \"b37\": 0.8}", "Based on the text of the paper and the titles of the references, the source papers that most likely inspired this work are:\n\n{ \"b4\": 0.9, \"b31\": 0.8, \"b20\": 0.7, \"b7\": 0.65, \"b45\": 0.65, \"b2\": 0.6, \"b21\": 0.6, \"b37\": 0.55 }\n\nExplained as:\n\n- [b4] Classifying memory access patterns for prefetching: High confidence because the paper discusses memory access patterns and prefetching, two core concepts of the given paper.\n- [b31] Apt get profile guided timely software prefetching: High confidence because of the reference to \"timely software prefetching\", a concept which is also central to the given paper.\n- [b20] A primer on hardware prefetching: Moderate confidence as the paper discusses hardware prefetching, which is related to the given paper's topic.\n- [b7] Evaluation of hardware data prefetchers on server processors: Moderate confidence because of the relevance of hardware data prefetchers and server processors to the given paper.\n- [b45] Bouquet of instruction pointers instruction pointer classifier based spatial hardware prefetching: Moderate confidence because the paper appears to discuss a similar prefetching technique.\n- [b2] An event triggered programmable prefetcher for irregular workloads: Moderate confidence due to discussion of a prefetcher, though it may not be the same as the one discussed in the given paper.\n- [b21] Make the most out of last level cache in Intel processors: Moderate confidence because of relevance of cache optimization, though may not be directly related to the given paper's novel contributions.\n- [b37] Path confidence based lookahead prefetching: Moderate confidence because of relevance of lookahead prefetching, again may not be directly related to given paper's novel contributions.", "Based on the text and the titles of the references, the source papers that most likely inspired the given paper are:\n\n- \"an event triggered programmable prefetcher for irregular workloads\" [b2]: This paper seems to discuss dynamic application behavior, which the given paper also emphasizes when discussing prefetching needs.\n- \"classifying memory access patterns for prefetching\" [b4]: This paper possibly inspired the given paper's approach to understanding memory access patterns for prefetching.\n- \"a general framework for prefetch scheduling in linked data structures and its application to multi chain prefetching\" [b14]: This paper likely contributed to the given paper's proposed mathematical framework for understanding and predicting potential prefetcher performance.\n- \"a primer on hardware prefetching\" [b20]: This paper could have provided the basis for understanding prefetching techniques, which is central to the given paper.\n- \"apt get profile guided timely software prefetching\" [b31]: This paper might have inspired the use of prefetcher timeliness in developing an optimal software prefetching strategy.\n- \"bouquet of instruction pointers instruction pointer classifier based spatial hardware prefetching\" [b45]: This paper likely contributed to the understanding of prefetching capabilities, which is a key aspect in the given paper.\n\nTherefore, the JSON format of the answer would be: \n{\"b2\": 0.8, \"b4\": 0.8, \"b14\": 0.9, \"b20\": 0.7, \"b31\": 0.8, \"b45\": 0.75}", "Based on the text, the source papers that inspire the author most are:\n\n1. \"A State of the Art by Developing a Mathematical Framework That Computes an Optimized Schedule of Prefetches and Estimates Its Performance\" [b20] - This paper is indicated by the phrase \"we advance the state of the art by developing a mathematical framework that computes an optimized schedule of prefetches and estimates its performance\". It appears to have a strong influence on the author's methodology. Confidence score: 0.9\n\n2. \"Classifying Memory Access Patterns for Prefetching\" [b4] - This paper is mentioned twice and relates to the author's approach to prefetching, specifically in understanding and predicting prefetcher performance. Confidence score: 0.8\n\n3. \"Evaluation of Hardware Data Prefetchers on Server Processors\" [b7] - This paper is also mentioned twice and is relevant to the author's focus on prefetching techniques. Confidence score: 0.8\n\n4. \"An Event Triggered Programmable Prefetcher for Irregular Workloads\" [b2] - This paper is mentioned in the context of dynamic analysis to identify problematic loads, which is a key aspect of the author's proposed approach. Confidence score: 0.7\n\n5. \"Prefetching Using Markov Predictors\" [b32] - It is cited in the context of prefetching, a core concept in the paper. Confidence score: 0.6\n\n6. \"Bingo Spatial Data Prefetcher\" [b6] - Cited in the introduction as an example of a prefetching technique, which is the main topic of this paper. Confidence score: 0.6\n\n7. \"The Nas Parallel Benchmarks\" [b5] - This source might have been used as a benchmark in the author's experiments, thus influencing their results. Confidence score: 0.5\n\nThe confidence score is calculated based on the frequency and context of citation, as well as the relevance of the source paper's title to the author's novel contributions.\n\nThe result is returned in the JSON format:\n\n{\"b20\": 0.9, \"b4\": 0.8, \"b7\": 0.8, \"b2\": 0.7, \"b32\": 0.6, \"b6\": 0.6, \"b5\": 0.5}", "Based on the given text, the paper discusses a novel analytical framework for prefetching. The paper cites numerous references, but the references most strongly indicated as sources of inspiration can be inferred from the context in which they are mentioned, and the author notes. Here are the references that seem to be the most pertinent:\n\n{ \n\"b2\": 0.8,\n\"b4\": 0.8,\n\"b7\": 0.8,\n\"b31\": 0.8,\n\"b20\": 0.7,\n\"b21\": 0.7\n}\n\nThe scores are subjective and represent confidence levels based on the context in which these references are cited in the paper. \n\nReference [b2] and [b4] are mentioned in the context of the need for prefetching to be driven by dynamic application behavior, which aligns closely with the paper's approach to developing a prefetching strategy.\n\nReference [b7] is mentioned in the introduction of the text and later in the context of the variety of prefetching techniques available, indicating its relevance to the paper's topic.\n\nReference [b31] is mentioned multiple times in the text, first in the context of the impact of different prefetching distances on performance, and later in the context of prior work that used dynamic analysis to identify problematic loads.\n\nReference [b20] is mentioned in the context of software controlled prefetching techniques, which aligns with the paper's approach.\n\nReference [b21] is mentioned in the context of optimizing techniques like caching and prefetching to address the bottleneck of memory accessing."], "64b60eaa3fda6d7f06eaea30": ["```json\n{\n    \"b16\": 0.9,\n    \"b19\": 0.8,\n    \"b20\": 0.8\n}\n```", "```json\n{\n    \"b16\": 0.9,\n    \"b19\": 0.8,\n    \"b20\": 0.8,\n    \"b47\": 0.9,\n    \"b49\": 0.9\n}\n```", "```json\n{\n    \"b16\": 0.9,\n    \"b19\": 0.85,\n    \"b20\": 0.85,\n    \"b47\": 0.9\n}\n```", "```json\n{\n    \"b16\": 0.95,\n    \"b19\": 0.85,\n    \"b20\": 0.85,\n    \"b47\": 0.90\n}\n```", "{\n    \"b16\": 0.9,\n    \"b20\": 0.85,\n    \"b19\": 0.75,\n    \"b17\": 0.75,\n    \"b47\": 0.8,\n    \"b49\": 0.9\n}"], "6584f922939a5f408236fc36": ["Based on the given text, the following references seem to most directly inspire the author's work:\n\n1. \"Editing models with task arithmetic\" [b8]\n2. \"Low Rank Adaptation of large language models\" [b7]\n3. \"Time waits for no one analysis and challenges of temporal misalignment\" [b14]\n4. \"A pretrainer\u2019s guide to training data measuring the effects of data age domain coverage quality toxicity\" [b13]\n5. \"Exploring the limits of transfer learning with a unified text-to\" [b16]\n\nThese papers are mentioned in the context of the methods and approaches used in the paper's algorithm, and are associated with key phrases such as \"Recent work has shown that...\", \"we use pretrained T5 [b16] checkpoints for all our experiments\", and \"We measure temporal misalignment across a variety of domains...\".\n\nThe JSON format with the confidence score would look like this:\n\n{\n  \"b8\": 0.9,\n  \"b7\": 0.85,\n  \"b14\": 0.9,\n  \"b13\": 0.8,\n  \"b16\": 0.85\n} \n\nThe confidence scores are subjective and are based on the context in which these references are mentioned in the text. The score represents the degree of influence the source paper might have had on the given paper.", "Based on the given paper text and the titles of references, The source papers that inspire the author the most may include:\n\n1. \"Editing models with task arithmetic\" (b8)\n2. \"Task arithmetic in the tangent space improved editing of pre trained models\" (b15)\n3. \"A pretrainer\u2019s guide to training data measuring the effects of data age domain coverage quality toxicity\" (b13)\n4. \"Time waits for no one analysis and challenges of temporal misalignment\" (b14)\n5. \"LoRA: Low-Rank Adaptation of Large Language Models\" (b7)\n6. \"The power of scale for parameter efficient prompt tuning\" (b11)\n\nThe JSON format answer is:\n```json\n{\n    \"b8\": 0.9,\n    \"b15\": 0.8,\n    \"b13\": 0.7,\n    \"b14\": 0.9,\n    \"b7\": 0.8,\n    \"b11\": 0.7\n}\n```\nThis is inferred based on the methodological similarities and the key phrases in the paper text that indicate a strong reliance on these references. The confidence scores are subjectively assigned based on the relevancy of the references to the novel contributions of the paper.", "The source papers that inspired the given paper can be identified by looking for key phrases that indicate the use of previous research. Based on the text of the paper, the following papers seem to have influenced it the most:\n\n1. \"Editing models with task arithmetic\" [b8] - This paper is mentioned multiple times in the text and seems to have a strong influence on the methodology of the given paper, especially while introducing the concept of time vectors.\n2. \"Time waits for no one: analysis and challenges of temporal misalignment\" [b14] - This paper is referred to for explaining temporal misalignment and its effects on language model performance.\n3. \"Lora: low rank adaptation of large language models\" [b7] - This paper seems to be used as a reference for fine-tuning techniques used in the given paper.\n4. \"The power of scale for parameter efficient prompt tuning\" [b11] - This paper is mentioned as a method used for language model adaptation.\n\nIn JSON format, the result can be represented as:\n\n```json\n{\n  \"b8\": 0.9,\n  \"b14\": 0.8,\n  \"b7\": 0.7,\n  \"b11\": 0.7\n}\n```\n\nIn this, the key represents the reference number and the value represents the confidence score (between 0 and 1) indicating the level of influence of the source paper on the given paper.", "Based on the given text, the key phrases indicating inspiration from previous works, and the titles of the references, the source papers that inspired the given paper most likely include:\n\n- \"Editing models with task arithmetic\" [b8]: The concept of \"Time vectors\" seems to be inspired by the idea of task vectors from this paper.\n- \"Low-rank adaptation of large language models\" [b7]: The finetuning technique used in the given paper seems to be inspired by this work.\n- \"Time waits for no one: Analysis and challenges of temporal misalignment\" [b14]: This work is referenced in relation to the challenge of temporal misalignment, which is a primary focus of the given paper.\n\nThe confidence score is a subjective measure, but based on the available information, it can be estimated as follows:\n\n{\"b8\": 0.8, \"b7\": 0.7, \"b14\": 0.8}\n\nThis is subject to change based on additional context or information.", "Based on the provided information, the source papers that inspired this paper the most could be:\n\n1. \"Editing models with task arithmetic\" [b8]\n2. \"Time waits for no one: analysis and challenges of temporal misalignment\" [b14]\n3. \"Task arithmetic in the tangent space: improved editing of pre-trained models\" [b15]\n4. \"LoRa: Low-Rank Adaptation of Large Language Models\" [b7]\n5. \"A pretrainer's guide to training data: measuring the effects of data age, domain, coverage, quality, toxicity\" [b13]\n6. \"Dynamic word embeddings\" [b0]\n7. \"Temporal Adaptation of BERT and performance on downstream document classification: insights from social media\" [b18]\n\nThe confidence score for each of the source papers could be:\n\n{\"b8\": 0.9, \"b14\": 0.9, \"b15\": 0.9, \"b7\": 0.8, \"b13\": 0.8, \"b0\": 0.7, \"b18\": 0.7}\n\nThese papers are inferred based on the paper's text and they are also related to the main topic of the paper which is about the concept of time vectors and the temporal adaptation of language models."], "642e38e090e50fcafd6abcf8": ["Based on the given text and the information provided, the primary challenge of the paper appears to be creating a foundation model for image segmentation that can be prompted to solve a range of downstream segmentation problems. The solution involves three components: a promptable segmentation task, a segmentation model (SAM), and a data engine for collecting SA-1B, which is a dataset of over 1 billion masks. The paper also mentions the use of prompt engineering for zero-shot transfer to different tasks.\n\nConsidering these points, the most influential references in the paper seem to be:\n\n- [b9]: \"Language models are few-shot learners.\" This paper is mentioned multiple times and seems to be a major source of inspiration for the concept of few-shot learning and prompt engineering in the paper.\n\n- [b7]: \"On the opportunities and risks of foundation models.\" This might have served as a guide in the construction of the foundation model for segmentation.\n\n- [b81]: \"Learning transferable visual models from natural language supervision.\" This paper is likely to have influenced the idea of using text prompts in the model.\n\n- [b55]: \"Scaling laws for neural language models.\" This paper could have directed the authors towards strategies for scaling their model.\n\n- [b50]: \"Training compute optimal large language models.\" This paper may have influenced the strategies for training the model in this paper.\n\n- [b13]: \"End-to-end object detection with transformers.\" This paper might have inspired the use of transformers in the model.\n\n- [b46]: \"Masked autoencoders are scalable vision learners.\" This paper could have contributed to the idea of using masked autoencoders in the model.\n\n- [b32]: \"An image is worth 16x16 words: Transformers for image recognition at scale.\" This paper might have influenced the use of Transformers for image recognition in the model.\n\nThe confidence scores for each of these references will depend on the specific contextual information and need to be assigned accordingly.\n\nHere is the result in JSON format:\n{\n    \"b9\": 0.8,\n    \"b7\": 0.7,\n    \"b81\": 0.7,\n    \"b55\": 0.6,\n    \"b50\": 0.6,\n    \"b13\": 0.6,\n    \"b46\": 0.6,\n    \"b32\": 0.5\n}", "Based on the provided text, the following references seem to be the most influential sources for the paper:\n\n1. \"b9\": \"Language models are few-shot learners\"\n2. \"b7\": \"On the opportunities and risks of foundation models\"\n3. \"b81\": \"Learning transferable visual models from natural language supervision\"\n4. \"b82\": \"Zero-shot text-to-image generation\"\n5. \"b54\": \"Scaling up visual and vision-language representation learning with noisy text supervision\"\n6. \"b32\": \"An image is worth x words: Transformers for image recognition at scale\"\n7. \"b46\": \"Masked autoencoders are scalable vision learners\"\n8. \"b13\": \"End-to-end object detection with transformers\"\n\nThese papers are mentioned when discussing the key challenges and methods proposed in the paper. They are directly relevant to the paper's novel contributions. \n\nThe confidence score for each source paper:\n\n1. \"b9\": 0.9\n2. \"b7\": 0.8\n3. \"b81\": 0.85\n4. \"b82\": 0.7\n5. \"b54\": 0.8\n6. \"b32\": 0.75\n7. \"b46\": 0.7\n8. \"b13\": 0.8 \n\nThis is in the JSON format:\n\n{\n\"b9\": 0.9,\n\"b7\": 0.8,\n\"b81\": 0.85,\n\"b82\": 0.7,\n\"b54\": 0.8,\n\"b32\": 0.75,\n\"b46\": 0.7,\n\"b13\": 0.8\n}", "Based on the text of the academic paper, the most relevant source papers that inspired the authors are as follows:\n\n1. \"On the opportunities and risks of foundation models\" [b7]: The authors use the concept of foundation models, which can generalize to tasks and data distributions beyond those seen during training. This concept is key to their approach in building a foundation model for image segmentation.\n\n2. \"Language models are few-shot learners\" [b9]: The authors refer to this paper multiple times indicating the influence of the paper's concept of zero-shot and few-shot generalization, which they aim to implement in their segmentation model.\n\n3. \"Scaling language modeling with pathways\" [b20]: The authors mention empirical trends improving with model scale, dataset size, and total training compute, which is a key concept presented in this paper.\n\n4. \"CLIP: Learning Transferable Visual Models from Natural Language Supervision\" [b81]: This paper's concept of using engineered text prompts for zero-shot generalization to novel visual concepts and data distributions is referenced and used in the authors' approach.\n\n5. \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" [b32]: The authors base their image encoder on the Vision Transformer architecture proposed in this paper.\n\n6. \"Masked Autoencoders Are Scalable Vision Learners\" [b46]: The authors use the MAE pre-trained Vision Transformer presented in this paper for their image encoder.\n\n7. \"Interactive Image Segmentation with Latent Diversity\" [b63]: The authors' approach to ambiguity in segmentation, where the model is designed to predict multiple masks for a single prompt, seems to be influenced by this paper.\n\nHere is the JSON format:\n\n```\n{\n\"b7\": 0.9,\n\"b9\": 0.8,\n\"b20\": 0.8,\n\"b81\": 0.85,\n\"b32\": 0.85,\n\"b46\": 0.85,\n\"b63\": 0.8\n}\n```\nIn this JSON, the key is the reference number and the value is a confidence score between 0 and 1, with 1 being the highest confidence.", "Based on the text of the paper, the source papers that inspired the authors most are:\n\n1. \"On the opportunities and risks of foundation models\" [b7]\n2. \"Large language models pre-trained on web-scale datasets are revolutionizing NLP with strong zero-shot and few-shot generalization\" [b9]\n3. \"Foundation models have also been explored in computer vision, albeit to a lesser extent. Perhaps the most prominent illustration aligns paired text and images from the web. For example, CLIP [b81] and ALIGN [b54]\" \n4. \"A prompt simply specifies what to segment in an image, e.g., a prompt can include spatial or text information identifying an object.\" This suggests the use of prompting techniques, likely inspired by [b9].\n5. \"Surprisingly, we find that a simple design satisfies all three constraints: a powerful image encoder computes an image embedding, a prompt encoder embeds prompts, and then the two information sources are combined in a lightweight mask decoder that predicts segmentation masks.\" This model, known as SAM, seems to be a novel contribution of the authors but may draw inspiration from the encoder-decoder structure common in many deep learning models.\n6. \"To achieve strong generalization to new data distributions, we found it necessary to train SAM on a large and diverse set of masks, beyond any segmentation dataset that already exists.\" This approach is aligned with the common practice in machine learning and hence does not directly point to any specific source paper.\n\nIn JSON format:\n```\n{\n    \"b7\": 1,\n    \"b9\": 0.9,\n    \"b81\": 0.8,\n    \"b54\": 0.8\n}\n```", "Based on the provided text and author's notes, the following references appear to be the most pertinent sources that inspired the paper's core contributions:\n\n1. \"On the opportunities and risks of foundation models\" [b7]: The paper frequently mentions foundation models and the concept is central to the paper's contributions. The paper seems to extend the concept of foundation models to the field of image segmentation, which suggests that [b7] is a significant source of inspiration.\n\n2. \"Language models are few-shot learners\" [b9]: This paper is frequently cited when discussing zero-shot and few-shot generalization, which are key challenges outlined in the text. The author's paper seems to take inspiration from these concepts and applies them to image segmentation.\n\n3. \"End-to-end object detection with transformers\" [b13]: The paper's main contribution, the Segment Anything Model (SAM), uses a similar approach to object detection with transformers. The architectural design of SAM seems to be influenced by this paper.\n\n4. \"Masked Autoencoders are Scalable Vision Learners\" [b46]: This paper is mentioned when discussing the motivation behind using a Vision Transformer (ViT) as the image encoder in the SAM model. This indicates that the techniques proposed in [b46] directly influenced the paper's methodology.\n\n5. \"Learning Transferable Visual Models from Natural Language Supervision\" [b81]: This paper is referenced when discussing the use of text prompts in the SAM model, suggesting that it may have provided inspiration for this aspect of the paper's methodology.\n\n6. \"An Image is Worth x Words: Transformers for Image Recognition at Scale\" [b32]: Mentioned as a motivation for the choice of image encoder in SAM and seems to have influenced the design of the model.\n\n7. \"LVIS: A Dataset for Large Vocabulary Instance Segmentation\" [b43]: This paper is referenced when discussing the creation of the SA-1B dataset, suggesting that the authors may have taken inspiration from the methods used in this paper for data collection and annotation.\n\nBased on the above analysis, the resulting json is:\n\n{\n  \"b7\": 0.9,\n  \"b9\": 0.8,\n  \"b13\": 0.8,\n  \"b46\": 0.7,\n  \"b81\": 0.7,\n  \"b32\": 0.7,\n  \"b43\": 0.7\n}"], "64f7fc6a3fda6d7f06f43fbe": ["Based on the text and author's notes, the paper's main contributions are in proposing a self-supervised learning method for pitch estimation using music transposition, introducing a new equivariance loss, and using a lightweight model architecture with Toeplitz fully-connected layers. The method is evaluated against other self-supervised learning methods and shows comparable results to supervised training methods like CREPE. \n\nBased on this information, the most likely source papers, based on frequent mentions, direct comparison or adoption of methods, are:\n\n1. [b19] SPICE: Self-Supervised Pitch Estimation - The paper frequently references this work and it seems to have provided a foundation for the authors' work. The authors also make direct comparisons with its performance. Confidence Score: 0.9\n2. [b20] Equivariant self-supervision for Musical Tempo Estimation - The authors mentioned taking inspiration from this paper and adopted similar equivariant techniques for their work. Confidence Score: 0.9\n3. [b16] CREPE: A Convolutional Representation for Pitch Estimation - This paper appears to be the benchmark that the authors compare their results against. Confidence Score: 0.8\n4. [b22] A simple framework for contrastive learning of visual representations - The paper mentions using similar methods of data augmentation and Siamese networks as this work. Confidence Score: 0.7\n5. [b28] Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning - The paper refers to the methods employed by this work in the context of data augmentations and Siamese networks. Confidence Score: 0.7\n6. [b36] Equivariant contrastive learning - The paper refers to this work in the context of equivariant representation learning, which is a key aspect of the authors' proposed method. Confidence Score: 0.7\n\nThe selected papers are:\n```json\n{\n\"b19\": 0.9,\n\"b20\": 0.9,\n\"b16\": 0.8,\n\"b22\": 0.7,\n\"b28\": 0.7,\n\"b36\": 0.7\n}\n```", "Based on the text, the major challenges outlined by the paper are:\n\n1) Pitch estimation in music and audio analysis, specifically the need for a lightweight, generic model that doesn't require labeled data for training.\n\n2) The problem of \"collapse\" in self-supervised learning (SSL), where all inputs are mapped to the same embedding.\n\nThe following source papers appear to have inspired the author's work:\n\n1) b19: \"SPICE: Self-supervised Pitch Estimation\" is mentioned multiple times and seems to have inspired the SSL paradigm used in the paper, as well as the idea of formulating pitch estimation as a multi-class problem.\n\n2) b20: \"Equivariant Self-supervision for Musical Tempo Estimation\" seems to have inspired both the equivariance loss used in the paper and the idea of using a SSL paradigm based on Siamese networks and equivariance to pitch transpositions.\n\n3) b16: \"CREPE: A Convolutional Representation for Pitch Estimation\" is a deep learning model for pitch estimation that is compared to the author's model in terms of performance.\n\n4) b22: \"A Simple Framework for Contrastive Learning of Visual Representations\" inspired the use of data augmentations and Siamese networks in the SSL approach.\n\n5) b17: \"Deep Learning Architectures for Multi-pitch Estimation\" inspired the architecture of the model used in the paper.\n\nIn JSON format:\n\n{\"b19\": 0.9, \"b20\": 0.9, \"b16\": 0.8, \"b22\": 0.7, \"b17\": 0.7}", "Based on the given text of the academic paper, the author directly mentions being inspired by other works and uses the methods from these papers to address the primary challenges of pitch estimation. The author also introduces novel methods like a new equivariance loss and Toeplitz fully-connected layers.\n\nThe source papers are:\n\n1. \"Equivariant Self Supervision for Musical Tempo Estimation\": The author took inspiration from the equivariant pitch estimation and equivariant tempo estimation algorithms described in this paper. The author also uses a self-supervised learning paradigm based on Siamese networks and equivariance to pitch transpositions inspired by this paper. \n\n2. \"SPICE: Self-Supervised Pitch Estimation\": The author modeled pitch estimation as a multi-class problem, motivated by the regression problem model in this paper. This paper is also mentioned in the context of the novel class-based equivariance loss proposed by the author.\n\n3. \"A Simple Framework for Contrastive Learning of Visual Representations\": The author uses a SSL paradigm inspired by this paper. \n\n4. \"Barlow Twins: Self-Supervised Learning via Redundancy Reduction\": The author uses this paper to address the challenge of \"collapse\", when all inputs are mapped to the same embedding. \n\n5. \"VICReg: Variance Invariance Covariance Regularization for Self-Supervised Learning\": This paper is also used to address the \"collapse\" issue with a loss term on the statistics of a batch.\n\n6. \"Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning\": This paper is mentioned in the context of incorporating asymmetry between inputs to improve performance.\n\n7. \"Equivariant Contrastive Learning\": This paper is referenced in the discussion of learning a mapping that is equivariant to a set of transforms.\n\n8. \"CREPE: A Convolutional Representation for Pitch Estimation\": This paper is mentioned in the context of pitch estimation methods that have benefited from deep learning techniques. \n\nHere is the result in the json format:\n\n{\n\"b20\": 0.9,\n\"b19\": 0.8,\n\"b22\": 0.7,\n\"b23\": 0.7,\n\"b24\": 0.7,\n\"b28\": 0.7,\n\"b36\": 0.6,\n\"b16\": 0.6\n}", "Based on the given text and references, the source papers that inspired this paper the most are:\n\n{ \"b19\": 0.9, \"b20\": 0.9, \"b17\": 0.7, \"b16\": 0.7, \"b22\": 0.6, \"b28\": 0.5, \"b36\": 0.5}\n\nReasoning:\n- The phrase \"We take inspiration from the equivariant pitch estimation [b19] and the equivariant tempo estimation [b20]\" indicates that these papers had a significant influence on the development of the paper's core contributions.\n- The paper also mentions the usage of a SSL paradigm based on Siamese networks, which was inspired by [b19] and [b20].\n- The architecture of the paper's model is inspired by [b17].\n- CREPE [b16] is mentioned as a comparable method, suggesting it may have influenced the development of the paper's model.\n- The paper uses data augmentations and Siamese networks, which are also used in [b22], [b28].\n- The paper incorporates the concept of equivariant representation learning, which is discussed in [b36].", "Based on the given information, the relevant source papers that inspired the paper's contributions are:\n\n1. \"Equivariant self supervision for musical tempo estimation\" [b20]\n2. \"SPICE: Self-Supervised Pitch Estimation\" [b19]\n3. \"A simple framework for contrastive learning of visual representations\" [b22]\n4. \"Barlow twins: Self-supervised learning via redundancy reduction\" [b23]\n5. \"Deep learning architectures for multi-pitch estimation towards reliable evaluation\" [b17]\n6. \"Equivariant contrastive learning\" [b36]\n7. \"Self-supervised learning of split invariant equivariant representations\" [b38]\n\nThe JSON format of the result is:\n\n{\n\"b20\": 1,\n\"b19\": 0.9,\n\"b22\": 0.8,\n\"b23\": 0.8,\n\"b17\": 0.7,\n\"b36\": 0.7,\n\"b38\": 0.7\n}"], "6257c63c5aee126c0f47280f": ["Based on the analysis of the text, the most pertinent source papers are:\n\n1. \"Efficient methods for structured nonconvex nonconcave min max optimization\" (b9): The paper mentions this work multiple times, and it is clear that the authors' algorithm was inspired by it. They discuss how their algorithm extends the range of parameter in the weak MVI compared to EG+ of [b9], and they refer to [b9] when describing the (EG+) scheme. Confidence score: 0.9\n\n2. \"The limits of min-max optimization algorithms: Convergence to spurious non-critical sets\" (b17): This paper is also mentioned multiple times, and it seems the authors' work is building on the insights provided by this paper, specifically in relation to limit cycles. The authors even mention a \"difficult counter example of [b17]\" which their method can handle. Confidence score: 0.85\n\n3. \"A variational inequality perspective on generative adversarial networks\" (b12): This paper is referred to in the context of the variational inequalities (VIs) literature, which seems to be a key theoretical underpinning of the authors' work. Confidence score: 0.75\n\n4. \"Nonlinear forward backward splitting with projection correction\" (b13): This paper is mentioned in the context of interpreting each iteration of the algorithm as a projection onto a certain hyperplane. Confidence score: 0.7\n\n5. \"First order convergence theory for weakly convex-weakly concave min-max problems\" (b24): This paper is mentioned in the context of the weak Minty variational inequality (MVI), which is a key assumption in the authors' work. Confidence score: 0.65\n\n6. \"Optimistic mirror descent in saddle point problems: Going the extra (gradient) mile\" (b26): This paper is referred to in the context of the extragradient-type algorithms literature, which seems to be a key theoretical underpinning of the authors' work. Confidence score: 0.6\n\nHere is the result in the json format:\n\n{\n\"b9\": 0.9,\n\"b17\": 0.85,\n\"b12\": 0.75,\n\"b13\": 0.7,\n\"b24\": 0.65,\n\"b26\": 0.6\n}", "Based on the given text and author's notes, the source papers that most inspired the given paper could be identified as follows:\n\n1. \"efficient methods for structured nonconvex nonconcave min max optimization\" [b9]\n2. \"the limits of min max optimization algorithms convergence to spurious non critical sets\" [b17]\n3. \"a variational inequality perspective on generative adversarial networks\" [b12]\n4. \"optimistic mirror descent in saddle point problems going the extra gradient mile\" [b26]\n5. \"fast extra gradient methods for smooth structured nonconvexnonconcave minimax problems\" [b22]\n6. \"first order convergence theory for weakly convex weakly concave min max problems\" [b24]\n7. \"a modified forward backward splitting method for maximal monotone mappings\" [b41]\n\nThese papers are identified based on their direct relevance to the algorithm introduced and the challenges outlined in the given paper, as well as the explicit mentions in the text. The confidence score for each reference can be evaluated based on their frequency and context of mention in the paper, as well as the relevance of their contribution to the novel methods proposed in the paper. \n\nThe corresponding JSON format would be: \n{\n  \"b9\": 0.9,\n  \"b17\": 0.8,\n  \"b12\": 0.7,\n  \"b26\": 0.7,\n  \"b22\": 0.8,\n  \"b24\": 0.7,\n  \"b41\": 0.8\n}", "Based on the text provided and the titles of the referenced papers, the most pertinent source papers that inspired the given paper might be:\n\n1. 'Efficient methods for structured nonconvex nonconcave min max optimization' [b9]\n2. 'The limits of min max optimization algorithms convergence to spurious non critical sets' [b17]\n3. 'Fast extra gradient methods for smooth structured nonconvexnonconcave minimax problems' [b22]\n4. 'First order convergence theory for weakly convex weakly concave min max problems' [b24]\n5. 'Optimistic mirror descent in saddle point problems going the extra gradient mile' [b26]\n6. 'Non convex min max optimization provable algorithms and applications in machine learning' [b33]\n\nThese papers seem to be the most relevant to the given paper, as they address similar issues related to minimax optimization problems. However, further context from the given paper would be required to confirm this.\n\nIn the JSON format:\n\n{\n\"b9\": 0.9,\n\"b17\": 0.8,\n\"b22\": 0.8,\n\"b24\": 0.7,\n\"b26\": 0.7,\n\"b33\": 0.7\n} \n\nThe confidence score is given based on how similar the topics are between the source papers and the given paper. A score of 0.9 indicates a very high relevance, while a score of 0.7 still indicates a high relevance, but not as high as 0.9.", "Based on the text, the most relevant source papers seem to be:\n\n1. [b9]: 'Efficient methods for structured nonconvex nonconcave min max optimization'\n2. [b17]: 'The limits of min max optimization algorithms convergence to spurious non critical sets'\n3. [b26]: 'Optimistic mirror descent in saddle point problems going the extra gradient mile'\n4. [b41]: 'A modified forward backward splitting method for maximal monotone mappings'\n5. [b43]: 'Stochastic mirror descent in variationally coherent optimization problems'\n\nIn json format:\n{\n \"b9\": 1,\n \"b17\": 0.9,\n \"b26\": 0.8,\n \"b41\": 0.7,\n \"b43\": 0.6\n} \n\nThese papers are suggested based on the key phrases mentioned in the text and the challenges and methods proposed by the authors. Note that the confidence score is set by assessing the influence of each paper on the current paper's methodology.", "Based on the text of the paper, the source papers that most inspired the author are:\n\n1. \"Efficient methods for structured nonconvex nonconcave min max optimization\" [b9], which is mentioned multiple times and seems to be the primary inspiration for the developed algorithm. \n2. \"The limits of min max optimization algorithms convergence to spurious non critical sets\" [b17], which appears to have provided crucial insights on the limitations of existing methods that the new algorithm improves upon.\n3. \"A tight and unified analysis of gradient based methods for a whole spectrum of differentiable games\" [b0], which seems to be a comprehensive reference for the theoretical framework that the paper operates within.\n4. \"Optimistic mirror descent in saddle point problems going the extra gradient mile\" [b26], which is cited as a seminal work in the machine learning community that introduced the stabilizing training approach. \n5. \"Fast extra gradient methods for smooth structured nonconvexnonconcave minimax problems\" [b22], which is compared directly to the proposed approach and extended by the new algorithm.\n6. \"A variational inequality perspective on generative adversarial networks\" [b12], which also seems to be a significant influence on the paper's perspective.\n7. \"Stochastic subgradient method converges at the rate o k on weakly convex functions\" [b8], which is cited as a primary approach in global results derivation.\n\nThe JSON format of the result would be:\n\n{\n\"b9\" : 1,\n\"b17\" : 0.9,\n\"b0\" : 0.85,\n\"b26\" : 0.85,\n\"b22\" : 0.8,\n\"b12\" : 0.75,\n\"b8\" : 0.75\n}"], "63dcdb422c26941cf00b61c5": ["Based on the text and the titles of the references, the most pertinent source papers for the given paper are:\n\n{ \n\"b13\": 0.9, \n\"b29\": 0.8, \n\"b19\": 0.7, \n\"b33\": 0.7, \n\"b6\": 0.65, \n\"b20\": 0.6, \n\"b31\": 0.55, \n\"b32\": 0.5 \n}\n\nThe confidence scores are estimated based on the frequency and context of the citation in the text. \"b13\" and \"b29\" seem to have the highest influence on the paper's contributions, as they are frequently referenced in the context of the paper's main challenges and methodologies. Other references such as \"b19\", \"b33\", \"b6\", \"b20\", \"b31\", and \"b32\" are also relevant, but to a lesser extent.", "Based on the text of the academic paper and the provided reference titles, the most pertinent source papers that might have inspired the given paper are as follows:\n\n{ \"b13\": 0.9, \"b29\": 0.8, \"b33\": 0.7, \"b19\": 0.6, \"b20\": 0.5, \"b5\": 0.4, \"b6\": 0.3, \"b9\": 0.2 }\n\nThe confidence score has been assigned based on the relevance of the paper to the topic discussed in the given academic paper, and the frequency of direct or indirect mentions in the text. \n\nPlease note that the actual score may vary and the numbers given are used as an example to illustrate the output format.", "Based on the given text and the list of references, the following papers seem to be the most relevant sources:\n\n1. \"Efficient methods for structured nonconvex nonconcave min max optimization\" (b13): The paper under analysis is focused on a type of stochastic algorithm for nonconvex nonconcave problems which could be inspired by the methods discussed in this paper.\n\n2. \"A modified forward backward splitting method for maximal monotone mappings\" (b32): The paper mentions a generalization of the forward-backward-forward (FBF) algorithm, which could be inspired by the methods in this paper.\n\n3. \"Global convergence and variance reduced optimization for a class of nonconvex nonconcave minimax problems\" (b33): The paper discusses global convergence, which is also a topic in the paper under analysis.\n\n4. \"Escaping limit cycles global convergence for constrained nonconvex nonconcave minimax problems\" (b29): The paper under analysis discusses similar topics as this paper, such as global convergence and nonconvex nonconcave minimax problems.\n\n5. \"Solving nonconvex nonconcave min max problems exhibiting weak Minty solutions\" (b6): The paper under analysis is focused on weak Minty variational inequalities (MVI), which is also a topic in this paper.\n\nThe confidence score for each of these papers being a source is subjective and depends on further analysis of the paper. But based on the given text, a rough estimation could be:\n\n```\n{\n\"b13\": 0.8,\n\"b32\": 0.7,\n\"b33\": 0.75,\n\"b29\": 0.9,\n\"b6\": 0.85\n}\n```\n\nThis suggests that \"Escaping limit cycles global convergence for constrained nonconvex nonconcave minimax problems\" (b29) and \"Solving nonconvex nonconcave min max problems exhibiting weak Minty solutions\" (b6) are the most likely sources.", "Based on the given paper text and the titles of the references, the most pertinent source papers appear to be:\n\n1. \"Efficient methods for structured nonconvex nonconcave min max optimization\" [b13]\n2. \"Escaping limit cycles global convergence for constrained nonconvex nonconcave minimax problems\" [b29]\n3. \"Global convergence and variance reduced optimization for a class of nonconvex nonconcave minimax problems\" [b33]\n4. \"Stochastic first and zeroth order methods for nonconvex stochastic programming\" [b15]\n5. \"Solving nonconvex nonconcave min max problems exhibiting weak Minty solutions\" [b6]\n6. \"Stochastic extragradient general analysis and improved rates\" [b16]\n\nThese papers seem to be the most directly related to the given paper's topic and methodology, as they involve similar challenges and methods related to nonconvex nonconcave minimax problems and stochastic programming.\n\nThe result in the JSON format would look like:\n\n{\n  \"b13\": 0.9,\n  \"b29\": 0.9,\n  \"b33\": 0.8,\n  \"b15\": 0.7,\n  \"b6\": 0.7,\n  \"b16\": 0.7\n}\n\nThe confidence scores are given based on how closely each paper seems to relate to the given paper's topic and methodology, with a score of 0.9 indicating a very strong relation and a score of 0.7 indicating a strong relation. The scores may be subjective and should be confirmed by further analysis.", "Based on the provided text and the titles of the references, The most pertinent source papers seem to be:\n\n1. \"Efficient methods for structured nonconvex nonconcave min max optimization\" (b13): The paper's problem setting and approach seem to be directly inspired by this work, as evidenced by the repeated mention of nonconvex nonconcave min-max optimization and weak Minty variational inequalities (MVIs) - concepts central to this reference. Confidence score: 0.9\n\n2. \"Solving stochastic weak minty variational inequalities without increasing batch size\" (None): As this paper is focused specifically on solving weak MVI problems without increasing batch size, which matches the paper's main challenge and proposed solution. Confidence score: 0.85\n\n3. \"Escaping limit cycles global convergence for constrained nonconvex nonconcave minimax problems\" (b29): The paper references this work in relation to the challenge of guaranteeing convergence for a large class and the reliance on a large step size, which is a key aspect of the paper's algorithmic approach. Confidence score: 0.8\n\n4. \"Solving nonconvex nonconcave min max problems exhibiting weak minty solutions\" (b6): The paper's discussion of weak MVI settings and novel methods for solving them also suggests a strong influence from this reference. Confidence score: 0.75\n\n5. \"Revisiting stochastic extragradient\" (b27): The paper's mention of the extragradient method and its limitations in the monotone case indicates that this reference has also contributed to the development of the paper's algorithm. Confidence score: 0.7\n\nFinal answer in the json format:\n\n{\"b13\": 0.9, \"None\": 0.85, \"b29\": 0.8, \"b6\": 0.75, \"b27\": 0.7}"], "640fe64790e50fcafd9e276e": ["Based on the text and the notes provided by the author, the most pertinent source papers are:\n\n1. \"DeepSpeed Inference: Enabling efficient inference of transformer models at unprecedented scale\" [b0]\n2. \"Efficiently Scaling Transformer Inference\" [b29]\n3. \"Efficient and affordable post-training quantization for large scale transformers\" [b39]\n4. \"Group-wise quantization\" [b34]\n5. \"High throughput generative inference of large language models with a single GPU\" [b0]\n\nHere is the result in the requested JSON format:\n\n{\n  \"b0\": 0.9,\n  \"b29\": 0.8,\n  \"b39\": 0.75,\n  \"b34\": 0.7\n}", "Based on the text and its references, the most pertinent source papers for this academic paper seem to be:\n\n1. \"DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale\" [b0]\n2. \"Petals: Collaborative Inference and Fine Tuning of Large Models\" [b2]\n3. \"Group-wise Quantization\" [b34]\n4. \"Efficiently Scaling Transformer Inference\" [b29]\n5. \"Zero-Offload: Democratizing Billion-Scale Model Training\" [b31]\n6. \"SuperNeurons: Dynamic GPU Memory Management for Training Deep Neural Networks\" [b36]\n7. \"LightSeq: A High Performance Inference Library for Transformers\" [b37]\n\nThese papers are likely to be the most influential as they directly correspond to the methods and challenges described in the paper. It seems the author's main contributions were improving throughput for LLM inference on a single GPU, developing a search space of possible offloading strategies, and applying fine-grained group-wise quantization for reducing I/O costs and memory usage. These contributions directly build upon the methods and findings outlined in the aforementioned papers.\n\nThe JSON format is:\n\n{\n\"b0\": 0.9,\n\"b2\": 0.8,\n\"b34\": 0.8,\n\"b29\": 0.8,\n\"b31\": 0.8,\n\"b36\": 0.7,\n\"b37\": 0.7\n} \n\nThe confidence score is assigned based on how directly the papers influence the main contributions of the paper.", "Based on the text and the titles of the references, the following papers seem to have influenced the main contributions of the given paper:\n\n1. \"DeepSpeed Inference: Enabling efficient inference of transformer models at unprecedented scale\" [b0] - Confidence Score: 0.9\n2. \"TurboTransformers: An efficient GPU serving system for Transformer models\" [b9] - Confidence Score: 0.8\n3. \"Efficiently scaling Transformer inference\" [b29] - Confidence Score: 0.7\n4. \"Efficient Large-Scale Language Model Training on GPU Clusters using Megatron-LM\" [b25] - Confidence Score: 0.7\n5. \"GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers\" [b12] - Confidence Score: 0.8\n6. \"Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models\" [b27] - Confidence Score: 0.7\n7. \"A Distributed Serving System for Transformer-Based Generative Models\" [b40] - Confidence Score: 0.6\n8. \"Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT\" [b34] - Confidence Score: 0.7\n\nSome of the papers are selected based on the key phrases indicating direct inspiration or motivation in the text of the paper. Others are selected based on their titles suggesting a strong methodological similarity with the paper being analyzed.\n\nPlease note that the confidence scores are subjective and estimated based on the mentioned criteria.\nThe result in the required json format would be:\n\n{\n\"b0\": 0.9,\n\"b9\": 0.8,\n\"b29\": 0.7,\n\"b25\": 0.7,\n\"b12\": 0.8,\n\"b27\": 0.7,\n\"b40\": 0.6,\n\"b34\": 0.7\n}", "Based on the provided information and instructions, the source papers that most likely inspired the given paper are:\n\n- [b0]: \"DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale\"\n- [b2]: \"Petals: Collaborative Inference and Fine Tuning of Large Models\"\n- [b34]: \"Q-BERT: Hessian Based Ultra-Low Precision Quantization of BERT\"\n- [b39]: \"Efficient and Affordable Post-Training Quantization for Large Scale Transformers\"\n- [b25]: \"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM\"\n- [b40]: \"A Distributed Serving System for Transformer Based Generative Models\"\n\nThe json format result could be: \n{\n  \"b0\": 1,\n  \"b2\": 0.9,\n  \"b34\": 0.8,\n  \"b39\": 0.8,\n  \"b25\": 0.7,\n  \"b40\": 0.7\n}\n\nEach of these references is indicated by the phrases and context in the text to have contributed to the development of the paper's main contributions, such as the development of FlexGen, the implementation of offloading strategies for high-throughput generative inference, and the use of fine-grained group-wise quantization.", "Based on the provided text, the paper appears to be heavily influenced by the following references:\n\n{\n\"b0\": 0.8, \n\"b2\": 0.7, \n\"b34\": 0.9,\n\"b7\": 0.6,\n\"b12\": 0.6,\n\"b39\": 0.6,\n\"b38\": 0.6,\n\"b14\": 0.6\n}\n\nThese references are mentioned in places where the paper is discussing its main contributions and methods, indicating a strong reliance on these works. The confidence score has been assigned based on the degree of relevance and the frequency of the references in the text."], "6493c733d68f896efad19c1d": ["Based on the text and titles of the references, the most influential source papers for the given paper appear to be:\n\n1. \"'LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day' [b7]\": The paper's methodology seems to be heavily influenced by this reference, as it also discusses training a large language and vision assistant for biomedical applications. The confidence score for this reference is 0.9.\n\n2. \"'ChatDoctor: A Medical Chat Model Fine-Tuned on LLaMA Model Using Medical Domain Knowledge' [b23]\": The paper mentions using ChatGPT for building an instructions and dialogue fine-tuning dataset for ophthalmic diseases, which seems to be inspired by this reference. The confidence score for this reference is 0.8.\n\n3. \"'MedDialog: A Large-Scale Medical Dialogue Dataset' [b1]\": The authors use real conversations from the MedDialog dataset to improve the authenticity of their model, indicating that this reference was crucial for their data collection methodology. The confidence score for this reference is 0.8.\n\n4. \"'Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models' [b18]\": The authors discuss two methods for fine-tuning Large Language Models (LLMs) and reference this paper, suggesting that the methods proposed in this reference inspired their model architecture. The confidence score for this reference is 0.8.\n\n5. \"'Visual Instruction Tuning' [b10]\": The authors mention generative pretraining, a concept that seems to be inspired by this reference. The confidence score for this reference is 0.7.\n\n6. \"'ChatGLM-B: An Open Bilingual Pre-Trained Model' [b24]\": The authors used ChatGLM as their LLM for fine-tuning, which appears to be directly inspired by this reference. The confidence score for this reference is 0.7.\n\nThe result in json format:\n{\n  \"b7\": 0.9,\n  \"b23\": 0.8,\n  \"b1\": 0.8,\n  \"b18\": 0.8,\n  \"b10\": 0.7,\n  \"b24\": 0.7\n}", "Based on the text and the titles of the references, the source papers that inspire the given paper most are:\n\n1. 'vision language model for visual question answering in medical imagery' (b0): The paper uses a vision-language model for answering questions based on biomedical image inputs, which is a key concept in the given paper.\n\n2. 'pubmedclip how much does clip benefit visual question answering in the medical domain' (b2): The given paper uses a computer vision model for disease diagnosis, a concept that may have been influenced by this source paper.\n\n3. 'pathvqa questions for medical visual question answering' (b4): The given paper uses a visual question answering model for disease diagnosis, a concept that may have been influenced by this source paper.\n\n4. 'llava med training a large languageand vision assistant for biomedicine in one day' (b7): This paper introduces LLaVA-Med, a model that processes images directly without the need for separate medical visual models. This approach may have inspired the given paper's methodology.\n\n5. 'visual med alpaca a parameter efficient biomedical llm with visual capabilities' (b16): The given paper's model OphGLM is a large multimodal model in ophthalmology, which may be inspired by Visual Med-Alpaca's approach of incorporating images into LLMs.\n\n6. 'openended medical visual question answering through prefix tuning of language models' (b18): The paper discusses prefix tuning of language models for medical visual question answering, a method that may have been adopted in the given paper.\n\n7. 'glm b an open bilingual pre trained model' (b24): ChatGLM, a model used in the given paper, is an open-source dialogue language model that supports both English and Chinese conversations, which may have been inspired by this source paper.\n\nBased on the above analysis, the most pertinent source papers are:\n\n{\n\"b0\": 0.9,\n\"b2\": 0.8,\n\"b4\": 0.8,\n\"b7\": 0.9,\n\"b16\": 0.9,\n\"b18\": 0.9,\n\"b24\": 0.8\n}", "Based on the text, the most pertinent source papers for the academic paper are:\n\n1. \"LLaVA-Med: Training a Large Language-and Vision Assistant for Biomedicine in One Day\" [b7]: The paper is directly mentioned and seems to be serving as a base for the authors' methodology in constructing OphGLM. It also fits the criteria of being a methodological predecessor, as it describes the training of a large language and vision assistant for biomedicine, which aligns closely with the goal of OphGLM.\n\n2. \"ChatDoctor: A medical chat model fine-tuned on LLAMA model using medical domain knowledge\" [b23]: The authors mention this paper as a successful case in the field of biomedical large language models, which inspired their work. \n\n3. \"ChatGPT\" [b14]: The authors mention this paper as an instance of large language models that have achieved success in the general domain, and the authors of the current paper seem to have used it as the base model for fine-tuning.\n\n4. \"Visual Med-Alpaca: A parameter efficient biomedical LLM with visual capabilities\" [b16]: The authors mention this paper when discussing multimodal biomedical chatbots, which directly relates to their work on OphGLM.\n\n5. \"Open-ended medical visual question answering through prefix tuning of language models\" [b18]: The authors mention this paper when discussing the two methods of fine-tuning Large Language Models (LLMs) through biomedical visual question-answering dialogues, which is directly related to their work. \n\n6. \"MedDialog: A large-scale medical dialogue dataset\" [b1]: The authors mention this paper when discussing the process of building their fine-tuned fundus dialog dataset. \n\n7. \"ChatGLM: An open bilingual pre-trained model\" [b24]: The authors mention this paper when discussing the model they used for fine-tuning on their ophthalmology dialogue dataset. \n\nBased on these, the result in JSON format would be:\n\n{\"b7\": 0.9, \"b23\": 0.8, \"b14\": 0.8, \"b16\": 0.8, \"b18\": 0.8, \"b1\": 0.7, \"b24\": 0.7}\n\nThe confidence score is based on the extent of the direct relevance and the methodological similarity of the source paper to the academic paper, with 1 being the most confident.", "Based on the text, we can infer that the paper is about the construction of an ophthalmology large language-and-vision assistant (OphGLM) for biomedical application, especially for dealing with professional conversations in the field. The authors use knowledge graphs and real-world medical dialogues to improve the authenticity and usability of the model.\n\nHere are the source papers that likely inspired this work:\n\n1. \"LLaVA-Med: Training a large language and vision assistant for biomedicine in one day\" [b7]: The authors mention that LLaVA-Med is an end-to-end neural network model that processes images directly without the need for separate medical visual models.\n\n2. \"MedDialog: A Large-scale Medical Dialogue Dataset\" [b1]: The authors mentioned that they extracted real conversations about ophthalmic diseases from the MedDialog dataset, which contains genuine doctor-patient dialogues.\n\n3. \"Visual Med-Alpaca: A parameter-efficient biomedical LLM with visual capabilities\" [b16]: The authors mention Visual Med-Alpaca as a multimodal biomedical chatbot that can accept image inputs and generate text dialogue out-puts.\n\n4. \"Open-ended medical visual question answering through prefix tuning of language models\" [b18]: The authors mention that they used a three-layer MLP network to map visual features to the visual prefix, which is similar to the method used in this paper.\n\n5. \"ChatDoctor: A medical chat model fine-tuned on LLaMA model using medical domain knowledge\" [b23]: The authors mentioned that ChatDoctor is a biomedical large language model that has been successfully introduced.\n\n6. \"ChatGLM: An open bilingual pre-trained model\" [b24]: The authors used ChatGLM as the LLM and fine-tuned it on an ophthalmology dialogue dataset, indicating it as a direct source paper.\n\nSo, the result in json format would be:\n\n{\n\"b7\": 0.9,\n\"b1\": 0.85,\n\"b16\": 0.8,\n\"b18\": 0.7,\n\"b23\": 0.9,\n\"b24\": 1\n} \n\nThe confidence score is given on a scale of 0-1, with 1 being the highest level of confidence.", "Based on the text and the titles of the references, we can infer that the source papers that inspired this paper are:\n\n- \"LLaVA-Med: Training a Large Language-and Vision Assistant for Biomedicine in One Day\" [b7]: This paper appears to be a major inspiration, since it introduces a similar model for multimodal language-and-vision assistants in biomedicine. The reference to this paper is associated with the phrase \"Inspired by these successful cases in LLMs\" which shows a direct inspiration.\n\n- \"Visual Med-Alpaca: A Parameter Efficient Biomedical LLM with Visual Capabilities\" [b16]: This paper is likely another inspiration, as it introduces a multimodal biomedical chatbot capable of accepting image inputs.\n\n- \"ChatDoctor: A Medical Chat Model Fine-Tuned on LLaMA Model Using Medical Domain Knowledge\" [b23]: This paper is mentioned in the context of biomedical large language models, which is the key topic of this paper.\n\n- \"MedDialog: A Large-Scale Medical Dialogue Dataset\" [b1]: This paper is likely to be a source paper since it introduces a dataset of real doctor-patient dialogues, which was used in constructing the dialogue prompts.\n\n- \"ChatGLM: An Open Bilingual Pre-trained Model\" [b24]: This paper is likely to be a source paper because it introduces a pre-trained model used in the author's model.\n\n- \"OpenEnded Medical Visual Question Answering through Prefix Tuning of Language Models\" [b18]: This paper is likely to be a source paper since it introduces a method for fine-tuning Large Language Models (LLMs) through biomedical visual question-answering dialogues, which is a key part of the methodology in this paper.\n\n- \"BioGPT: Generative Pretrained Transformer for Biomedical Text Generation and Mining\" [b12]: This paper is likely to be a source paper since it introduces a pretrained LM used in the author's model.\n\nHere is the result in JSON format:\n\n{\n  \"b7\": 1,\n  \"b16\": 0.8,\n  \"b23\": 0.7,\n  \"b1\": 0.9,\n  \"b24\": 0.7,\n  \"b18\": 0.8,\n  \"b12\": 0.7\n}"], "62283c435aee126c0fd5de60": ["Based on the text and the author's notes, the paper introduces a novel method for generating a speaker's face, focusing on the accuracy and synchronization of mouth movements with audio/text inputs. The paper's novel contributions include a new Multiple Synergy Network (MSN) that uses multimodal data (text and sound) and facial data to improve landmark prediction accuracy, and a Video Consistency Network (VCN) that models temporal continuity between frames to ensure frame consistency. \n\nBy analyzing the text and the titles of the references, the most relevant source papers that seem to have heavily influenced the main contributions of the paper are:\n\n1. b14: \"synthesizing obama learning lip sync from audio\" - This paper is likely a primary source as it describes a similar process of mapping audio to realistic facial movements.\n2. b17: \"oba manet photo realistic lip sync from text\" - This paper also explores the use of audio and text inputs for realistic facial video generation, and it is mentioned multiple times in the text. \n3. b29: \"video to video synthesis\" - The use of optical flow in the proposed network is based on this paper. \n4. b27: \"talking face generation by adversarially disentangled audio visual representation\" - This paper's work on talking face generation likely influenced the current paper's methodology. \n5. b23: \"bltrcnn based rd articulatory movement prediction learning articulatory synchronicity from both text and audio inputs\" - This paper is directly referenced as an inspiration for the multimodal input approach. \n6. b19: \"end to end speech driven facial animation with temporal gans\" - The concept of a temporal generative adversarial network (GAN) is mentioned as an inspiration.\n7. b49: \"an empirical evaluation of generic convolutional and recurrent networks for sequence modeling\" - This paper likely inspired the use of recurrent neural networks (RNN) in the proposed method.\n\nHere is the result in the JSON format: {\"b14\": 0.9, \"b17\": 0.9, \"b29\": 0.8, \"b27\": 0.85, \"b23\": 0.9, \"b19\": 0.85, \"b49\": 0.85}. The confidence levels are high as these papers directly relate to the main concepts and methods used in the paper.", "Based on the text of the paper and the authors' notes, the following references seem to have the most relevance:\n\n1. \"Synthesizing obama learning lip sync from audio\" [b14]: The paper discusses the challenge of generating realistic facial movements in sync with audio input, which is a key problem tackled by the authors of the paper in question.\n   \n2. \"Oba manet photo realistic lip sync from text\" [b17]: The authors refer to the ObamaNet method, which focuses on predicting mouth landmarks using LSTM and generating each frame independently. The authors build upon this concept with the proposed MSN and VCN.\n\n3. \"Video to video synthesis\" [b29]: The authors cite this paper when discussing the difficulty of accurately estimating optical flow in regions with large motion, like the mouth area, which is a major challenge the authors aim to overcome with their proposed method.\n\n4. \"Talking face generation by adversarially disentangled audio visual representation\" [b27]: The authors refer to this work when discussing the need for better models to handle temporal dynamics in talking face generation.\n\n5. \"The sound of motions\" [b36]: This reference is cited when discussing the complementarity of multimodal inputs, a key theme in the authors' work.\n\n6. \"High resolution image synthesis and semantic manipulation with conditional gans\" [b53]: The authors cite this work when discussing the use of pix2pixHD for generating the initial frame in the VCN network.\n\nThe confidence score for each source paper is difficult to determine without access to the full text of both the source papers and the paper in question. However, based on the frequency and context of the citations, it seems that [b14] and [b17] would have high confidence scores, followed by [b29], [b27], [b36], and [b53].\n\nThe results can be represented in JSON format as follows:\n\n{\n  \"b14\": 0.9,\n  \"b17\": 0.9,\n  \"b29\": 0.8,\n  \"b27\": 0.8,\n  \"b36\": 0.8,\n  \"b53\": 0.7\n}", "Based on the given text and author notes, the author has proposed a method for generating a speaker's face, specifically focusing on the accuracy and synchronization of mouth movements. The primary challenges are derived from previous studies, and the author has implemented a Multiple Synergy Network (MSN) and a Video Consistency Network (VCN) to address these issues. \n\nThe source papers that have likely inspired this work can be identified by phrases indicating reliance on previous research, direct motivation, and specific methodologies. \n\nAfter analyzing the paper, the following source papers seem to have the most significant impact on the author's work:\n\n1. \"Synthesizing Obama: Learning lip sync from audio\" [b14] \n2. \"Obamanet: Photo-realistic lip-sync from text\" [b17]\n3. \"End to end speech driven facial animation with temporal GANs\" [b19]\n4. \"Talking face generation by conditional recurrent adversarial network\" [b18]\n5. \"Video to video synthesis\" [b29]\n6. \"An analysis of HMM based prediction of articulatory movements\" [b22]\n7. \"FlowNet: Evolution of optical flow estimation with deep networks\" [b28]\n8. \"Hierarchical cross modal talking face generation with dynamic pixel wise loss\" [b0]\n\nThe confidence scores for these sources are inferred based on their direct relevance to the paper's contributions and their appearance in the text.\n\nThe return result in JSON format would be:\n\n{\"b14\": 0.95, \"b17\": 0.95, \"b19\": 0.85, \"b18\": 0.85, \"b29\": 0.9, \"b22\": 0.9, \"b28\": 0.85, \"b0\": 0.95}", "Based on the text of the paper, the source papers that inspire the author the most are:\n\n1. \"Synthesizing Obama: Learning Lip Sync from Audio\" [b14]: The paper refers to this work while discussing the requirement of accurate mouth movements synced with audio/text input and while discussing the limitations of existing methods in considering articulator synergy.\n\n2. \"OBA-MANET: Photo-Realistic Lip-Sync from Text\" [b17]: This paper is referred to multiple times in the text. It is pointed out that this work also utilizes text input to generate realistic facial videos. It is also mentioned as a method that suffers from temporally incoherent videos due to lack of consideration for temporal coherence. \n\n3. \"Video to Video Synthesis\" [b29]: This work is referred to while discussing the difficulty of estimating optical flow in areas with large motion, like the mouth region. The author's method proposes to improve upon this by introducing a mouth generation branch to enhance mouth texture.\n\n4. \"Audio-Visual Scene Analysis with Self-Supervised Multisensory Features\" [b31]: This paper is referred to while discussing the complementarity of multimodal inputs in talking face generation.\n\n5. \"An Analysis of HMM-Based Prediction of Articulatory Movements\" [b22]: This paper is referred to in the context of how mouth movements are temporal synchrony between audio and text inputs in human speech production.\n\n6. \"Multiple Synergy Network\" [b26]: This paper is referred to when discussing the need for considering articulator synergy in talking face generation and how existing methods tend to ignore this aspect.\n\nThe confidence score for each reference is hard to quantify without a model, but based on the content and context of the references in the text, a rough estimate could be:\n\n{\"b14\": 0.9, \"b17\": 0.9, \"b29\": 0.8, \"b31\": 0.7, \"b22\": 0.7, \"b26\": 0.8}", "Based on the analysis of the text and the authors' notes, the main source papers inspiring this paper are as follows:\n\n{\"b14\": 0.9, \"b17\": 0.9, \"b27\": 0.8, \"b29\": 0.8, \"b23\": 0.8, \"b25\": 0.7, \"b41\": 0.7, \"b49\": 0.7}\n\nExplanation:\n1) \"b14\" and \"b17\" - These papers seem to be the most relevant as they are directly related to the main contributions of the paper, which are the development of the Multiple Synergy Network (MSN) for enhancing the prediction of key points and the Video Consistency Network (VCN) for generating consistent and accurate facial animations. These papers are also mentioned in the context of discussing existing methods and their limitations, which the author's method aims to overcome.\n\n2) \"b27\", \"b29\", \"b23\", \"b25\" - These papers are directly related to the main motivations of the paper, which are to ensure the temporal synchrony between audio and text inputs and to model the temporal dynamics of facial movements. \n\n3) \"b41\" and \"b49\" - These papers are related to the methodology employed in the paper, involving the use of generative adversarial networks and convolutional networks for sequence modeling.\n\n4) Other references are also mentioned in the text, but they appear to be more related to the background or auxiliary aspects of the study, rather than directly inspiring the main contributions of the paper."], "639a90a690e50fcafdf0a150": ["Based on the provided information, the source papers that inspired the given academic paper seem to be:\n\n1. \"Generative Adversarial Nets\" [b12] - The paper mentioned that they used GAN for image generation, which could be inspired by this source paper.\n2. \"Alias-Free Generative Adversarial Networks\" [b23] - The paper described using the alias-free technique from single image generation, which likely comes from this source paper.\n3. \"Temporal Shift Module for Efficient Video Understanding\" [b29] - The paper used a temporal shift module (TSM) in the discriminator, which could be inspired by this source paper.\n4. \"StyleGAN-V: A Continuous Video Generator with the Price, Image Quality, and Perks of StyleGAN\" [b44] - The primary methodology of this paper seems to be based on StyleGAN-V, as they made several improvements on it.\n5. \"Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks\" [b58] - This could be a possible source paper since the given paper's evaluation was performed on SkyTimelapse.\n6. \"First Order Motion Model for Image Animation\" [b43] - The given paper was evaluated on Taichi-HD, which could be inspired by this source paper.\n7. \"Learning to Drive by Watching YouTube Videos: Action-Conditioned Contrastive Policy Pretraining\" [b64] - The given paper was evaluated on the YouTube Driving dataset, indicating possible inspiration from this source paper.\n\nThe JSON format of the result:\n{\n    \"b12\": 0.9,\n    \"b23\": 0.9,\n    \"b29\": 0.9,\n    \"b44\": 1.0,\n    \"b58\": 0.8,\n    \"b43\": 0.8,\n    \"b64\": 0.8\n}", "Based on the text and titles of the references provided, the source papers that most likely inspired the author of this paper are:\n\n1. \"StyleGAN-V: A Continuous Video Generator with the Price Image Quality and Perks of StyleGAN\" [b44]: This paper is mentioned multiple times throughout the text, and it appears to be the main foundation upon which the current paper's methodology is built upon. The author uses the work as the baseline for comparison and identifies the limitations of the method proposed in the paper, which the current paper aims to address.\n\n2. \"Alias-Free Generative Adversarial Networks\" [b23]: This paper is referenced multiple times in the text as the source of the alias-free technique adopted to mitigate the texture sticking issue in the video generation process, which is a significant contribution of the author's paper.\n\n3. \"TSM: Temporal Shift Module for Efficient Video Understanding\" [b29]: The author uses the temporal shift module (TSM) proposed in this paper to model the temporal relations between frames in the discriminator, a key aspect of the author's approach.\n\n4. \"A Style-Based Generator Architecture for Generative Adversarial Networks\" [b20] and \"Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis\" [b61]: These papers are cited as a source of the understanding that GAN's deep layers mainly control details, which the author uses to justify the strategy of pre-training at the image level and then fine-tuning at the video level.\n\n5. \"Learning to Generate Time-lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks\" [b58]: This paper is the source of one of the datasets used for evaluation, and possibly inspired the methods for generating smooth transitions and temporal relations across frames.\n\nThe confidence scores of the source papers are as follows (confidence scores are subjective and may vary):\n\n{ \"b44\": 0.9,\n  \"b23\": 0.85,\n  \"b29\": 0.8,\n  \"b20\": 0.75,\n  \"b61\": 0.75,\n  \"b58\": 0.7 }", "The source papers for the given paper can be inferred from the text and author's note as follows:\n\n1. [b44] \"StyleGAN-V: A Continuous Video Generator with the Price, Image Quality, and Perks of StyleGAN\" - The paper directly states that it is making improvements on the StyleGAN-V method, thus indicating a strong reliance on this source. Confidence: 0.9\n\n2. [b23] \"Alias-Free Generative Adversarial Networks\" - The paper mentions that it adopts the alias-free technique from this source paper. Confidence: 0.85\n\n3. [b29] \"TSM: Temporal Shift Module for Efficient Video Understanding\" - The paper incorporates a temporal shift module (TSM) from this source. Confidence: 0.8\n\n4. [b20] \"A Style-Based Generator Architecture for Generative Adversarial Networks\" - The paper mentions the lessons learned from this source about the function of GAN's deep layers. Confidence: 0.75\n\n5. [b61] \"Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis\" - This paper also contributes to the understanding of GAN's deep layers and is referenced in the context of the pretraining stage. Confidence: 0.75\n\nBased on the given information, other papers are mentioned but not in a context that clearly indicates they were a significant source of inspiration for this work. The confidence score is based on the level of reliance and the direct contribution of these sources to the core contributions of this paper.\n\nReturn in the json format:\n{\n\"[b44]\": 0.9,\n\"[b23]\": 0.85,\n\"[b29]\": 0.8,\n\"[b20]\": 0.75,\n\"[b61]\": 0.75\n}", "Based on the text of the academic paper and the guidelines provided, the following references seem to have most directly inspired the work of this paper:\n\n1. \"StyleGAN-V [b44]: A continuous video generator with the price image quality and perks of stylegan\"\n2. \"Alias-free generative adversarial networks [b23]\"\n3. \"A good image generator is what you need for high resolution video synthesis [b45]\"\n4. \"TSM: Temporal Shift Module for Efficient Video Understanding [b29]\"\n5. \"Learning to generate time lapse videos using multi stage Dynamic Generative Adversarial Networks [b58]\"\n6. \"First order motion model for image animation [b43]\"\n\nThe JSON format of these source papers with a confidence score is:\n\n```\n{\n    \"b44\": 1,\n    \"b23\": 0.9,\n    \"b45\": 0.8,\n    \"b29\": 0.8,\n    \"b58\": 0.7,\n    \"b43\": 0.7\n}\n```", "Based on the text and the author's notes, the most relevant source papers for this academic paper appear to be:\n\n1. \"StyleGAN-V: A continuous video generator with the price, image quality, and perks of StyleGAN\" [b44]\n2. \"Alias-free generative adversarial networks\" [b23]\n3. \"Temporal shift module for efficient video understanding\" [b29]\n4. \"A style-based generator architecture for generative adversarial networks\" [b20]\n5. \"Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks\" [b58]\n6. \"Style-GAN3: Image and video editing with styleGAN\" [b0]\n7. \"Analyzing and improving the image quality of styleGAN\" [b21]\n\nThese papers are mentioned in the text and relate to the core topics and methods used in the paper. The paper [b44] is the most significant source, as it is referred to multiple times and the authors of the current paper make several modifications to it.\n\nThe confidence scores for each of these papers might look like this:\n\n- \"StyleGAN-V: A continuous video generator with the price, image quality, and perks of StyleGAN\" [b44]: 1.0\n- \"Alias-free generative adversarial networks\" [b23]: 0.9\n- \"Temporal shift module for efficient video understanding\" [b29]: 0.8\n- \"A style-based generator architecture for generative adversarial networks\" [b20]: 0.7\n- \"Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks\" [b58]: 0.7\n- \"Style-GAN3: Image and video editing with styleGAN\" [b0]: 0.6\n- \"Analyzing and improving the image quality of styleGAN\" [b21]: 0.6\n\nSo, the result in json format would be:\n```\n{\n    \"b44\": 1.0,\n    \"b23\": 0.9,\n    \"b29\": 0.8,\n    \"b20\": 0.7,\n    \"b58\": 0.7,\n    \"b0\": 0.6,\n    \"b21\": 0.6\n}\n```"], "6233f88c5aee126c0f94b3c4": ["{\n    \"b33\": 0.95,\n    \"b44\": 0.95,\n    \"b5\": 0.90,\n    \"b31\": 0.90,\n    \"b30\": 0.85,\n    \"b41\": 0.80,\n    \"b9\": 0.75,\n    \"b14\": 0.70\n}", "{\n  \"b33\": 0.9,\n  \"b5\": 0.9,\n  \"b31\": 0.9,\n  \"b44\": 0.9,\n  \"b37\": 0.9,\n  \"b30\": 0.85\n}", "```json\n{\n    \"b33\": 0.9,\n    \"b5\": 0.9,\n    \"b31\": 0.9,\n    \"b44\": 0.9,\n    \"b30\": 0.8\n}\n```", "{\n  \"b33\": 0.9,\n  \"b5\": 0.85,\n  \"b44\": 0.8,\n  \"b30\": 0.75,\n  \"b31\": 0.75,\n  \"b20\": 0.7\n}", "{\n  \"b33\": 0.9,\n  \"b44\": 0.85,\n  \"b5\": 0.8,\n  \"b30\": 0.75,\n  \"b2\": 0.7,\n  \"b31\": 0.65\n}"], "634967f890e50fcafdb5120a": ["Based on the given text and the author notes, the following source papers appear to have been the most influential in shaping the main contributions of the paper:\n\n1. \"ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks\" [b51]\n   - The authors directly state that they extend the ESRGAN model in their work. This suggests a strong reliance on the methodologies and findings of this paper.\n\n2. \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" [b42]\n   - The authors adopted the U-Net design for their discriminator, indicating that they were heavily influenced by the methodologies proposed in this paper.\n\n3. \"Spectral Normalization for Generative Adversarial Networks\" [b38]\n   - The authors employ spectral normalization to stabilize the training dynamics of their model, implicating this paper as a significant source of inspiration.\n\n4. \"Learning a Single Convolutional Super-Resolution Network for Multiple Degradations\" [b57]\n   - The paper cites this work in discussing blind super-resolution, which is a major theme of the paper.\n\n5. \"Blind Super-Resolution Kernel Estimation using an Internal GAN\" [b1]\n   - This paper is cited in the context of blind super-resolution, and it seems to influence the methodologies the authors use to tackle this issue.\n\n6. \"Designing a Practical Degradation Model for Deep Blind Image Super Resolution\" [b56]\n   - The authors refer to this paper when discussing their high-order degradation model, indicating that it played a significant role in their approach to solving the problem.\n\n7. \"Unsupervised Degradation Representation Learning for Blind Super-Resolution\" [b47]\n   - This paper is used to contrast the authors' approach to implicit modeling, suggesting that it offered a touchstone for developing their own methodologies.\n\nThe JSON format of the answer is:\n\n{\"b51\": 1, \"b42\": 0.9, \"b38\": 0.9, \"b57\": 0.8, \"b1\": 0.8, \"b56\": 0.8, \"b47\": 0.7} \n\nThe confidence scores are assigned based on the directness of the influence and the frequency of citation. The scores range between 0.7 and 1, with 1 representing the highest confidence.", "Based on the given text, following are the source papers which have inspired the author the most:\n\n1. ESRGAN: Enhanced Super Resolution Generative Adversarial Networks [b51]\n2. A U-Net based discriminator for generative adversarial networks [b42]\n3. Spectral Normalization for Generative Adversarial Networks [b38]\n4. Blind Super Resolution with Iterative Kernel Correction [b15]\n5. Unsupervised Image Superresolution using Cycle in Cycle Generative Adversarial Networks [b55]\n6. Designing a Practical Degradation Model for Deep Blind Image Super Resolution [b56]\n\nThese papers are indicated by key phrases such as \"inspired by\", \"motivated by\", \"we adopt\", \"we use\" etc. They are directly relevant to the paper's novel contributions and methods. Therefore, they are the most pertinent source papers for the given academic paper.\n\nIn the JSON format:\n{\n  \"b51\": 0.9,\n  \"b42\": 0.9,\n  \"b38\": 0.9,\n  \"b15\": 0.8,\n  \"b55\": 0.8,\n  \"b56\": 0.7\n}\n\nThe confidence scores are assigned based on the frequency and emphasis given to each reference in the text.", "Based on the text, the paper appears to be most inspired by the following source papers:\n\n1. \"ESRGAN: Enhanced Super Resolution Generative Adversarial Networks\" [b51]: The authors directly mention their work as an extension of the ESRGAN model. Thus, it can be inferred that [b51] is a significant source paper. Confidence score: 0.9 \n   \n2. \"A U-Net based discriminator for Generative Adversarial Networks\" [b42]: The authors mention applying the U-Net design to improve the discriminator in their model, indicating that [b42] has played a role in their work. Confidence score: 0.8 \n\n3. \"Spectral Normalization for Generative Adversarial Networks\" [b38]: The authors note that they have employed spectral normalization regularization, a method from [b38], to stabilize their training dynamics. Confidence score: 0.7\n\n4. \"Learning a Deep Convolutional Network for Image Super Resolution\" [b8]: As this paper is mentioned in the context of the pioneering work in this field, it can be inferred that it has been an inspiration, despite its appearance early in the text. Confidence score: 0.7 \n\n5. \"Blind Image Super Resolution: A Survey and Beyond\" [b29]: This survey paper is mentioned as a source for more comprehensive taxonomy, implying that it has provided some foundational information to the authors. Confidence score: 0.6\n\n6. \"Blind Super Resolution Kernel Estimation Using an Internal GAN\" [b1]: Mentioned in the context of blind super-resolution, it's probably one of the source papers, but its influence can't be confirmed. Confidence score: 0.5\n\n7. \"Designing a Practical Degradation Model for Deep Blind Image Super Resolution\" [b56]: This paper is mentioned in the context of the degradation process, suggesting that it may have influenced the authors' approach to this aspect of their work. Confidence score: 0.6\n\nThe confidence scores are assigned based on how directly the authors acknowledge the influence of each source paper in their text.\n\nThe final output in JSON format is:\n\n{\n    \"b51\": 0.9,\n    \"b42\": 0.8,\n    \"b38\": 0.7,\n    \"b8\": 0.7,\n    \"b29\": 0.6,\n    \"b1\": 0.5,\n    \"b56\": 0.6\n}", "Based on the provided text and titles of the references, the most pertinent source papers that inspired the authors' work on ESRGAN model and U-Net discriminator with spectral normalization could be:\n\n1. [b51] \"ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks\". The authors have expanded the ESRGAN model to build a super-resolution application, Real-ESRGAN. This indicates that [b51] is a key source of inspiration. Confidence Score: 1.0\n\n2. [b42] \"A U-Net based discriminator for Generative Adversarial Networks\". The authors have improved the VGG-style discriminator in ESRGAN to a U-Net design which is mentioned in [b42]. Thus, this paper is also a significant source of inspiration. Confidence Score: 1.0\n\n3. [b38] \"Spectral Normalization for Generative Adversarial Networks\". The authors used the spectral normalization regularization to stabilize the training dynamics, which is introduced in [b38]. Confidence Score: 0.9\n\n4. [b56] \"Designing a practical degradation model for deep blind image super resolution\". The authors adopted a second-order degradation process for a good balance between simplicity and effectiveness, which is proposed in [b56]. Confidence Score: 0.8\n\n5. [b57] \"Learning a single convolutional super-resolution network for multiple degradations\". The authors might have drawn inspiration from this paper since it deals with the topic of blind super-resolution. Confidence Score: 0.7\n\n6. [b11] \"Frequency separation for real world super resolution\". This paper might have inspired the authors since it approaches real-world super resolution, which is the main topic of the text. Confidence Score: 0.6\n\nIn JSON format, the result would be:\n{\n\"b51\": 1.0,\n\"b42\": 1.0,\n\"b38\": 0.9,\n\"b56\": 0.8,\n\"b57\": 0.7,\n\"b11\": 0.6\n}", "Based on the given paper text and the author\u2019s notes, the following references appear to be the most relevant source papers that inspired the authors:\n\n1. \"ESRGAN: Enhanced super-resolution generative adversarial networks\" [b51]: The paper has extended the ESRGAN model to deal with real-world low-resolution images. This indicates a direct inspiration from this paper.\n\n2. \"U-Net: Convolutional networks for biomedical image segmentation\" [b42]: The authors have used an U-Net design for the discriminator in their model, indicating an inspiration from this work.\n\n3. \"Spectral normalization for generative adversarial networks\" [b38]: The authors have applied spectral normalization regularization to stabilize the training dynamics. \n\n4. \"Blind super-resolution kernel estimation using an internal-GAN\" [b1]: The paper mentions blind super-resolution, a research area that this paper is contributing to.\n\n5. \"Learning a single convolutional super-resolution network for multiple degradations\" [b57]: The authors mention this work when discussing the limitations of previous approaches to blind super-resolution.\n\n6. \"Designing a practical degradation model for deep blind image super resolution\" [b56]: This work is mentioned in the context of the proposed second-order degradation process, which is a novel contribution of the paper.\n\nThe result in Json format: {\"b51\": 0.9, \"b42\": 0.8, \"b38\": 0.8, \"b1\": 0.7, \"b57\": 0.7, \"b56\": 0.7}.\n\nPlease note that the confidence scores are subjective and represent the estimated likelihood of the reference being a key source of inspiration for the paper based on the provided text. Higher scores indicate a higher likelihood of the reference being a key source."], "6287493f5aee126c0ffedf11": ["{\n  \"b40\": 0.9,\n  \"b19\": 0.8,\n  \"b34\": 0.8,\n  \"b11\": 0.8,\n  \"b2\": 0.9\n}", "{\n    \"b40\": 0.9,\n    \"b2\": 0.8,\n    \"b24\": 0.7\n}", "{\n    \"b40\": 0.9,\n    \"b2\": 0.85,\n    \"b34\": 0.8,\n    \"b11\": 0.8\n}", "{\n  \"b40\": 0.9,\n  \"b2\": 0.85,\n  \"b24\": 0.75,\n  \"b11\": 0.7\n}", "```json\n{\n    \"b40\": 0.9,\n    \"b2\": 0.8,\n    \"b24\": 0.7,\n    \"b34\": 0.7,\n    \"b11\": 0.6\n}\n```"], "62286c865aee126c0fa71627": ["Based on the text of the paper and the titles of the references, the source papers that inspired the most are:\n\n{ \"b4\": 0.9, \"b5\": 0.8, \"b15\": 0.7, \"b16\": 0.6, \"b18\": 0.6, \"b20\": 0.6 }\n\nExplanation:\n\nb4: \"legalization algorithm for multiple row height standard cell design\" and b5: \"routabilitydriven and fence aware legalization for mixed cell height circuits\" are directly mentioned as inspirations for the paper's novel contributions.\n\nb15: \"pathfinder a negotiation based performance driven router for fpgas\", b16: \"fastroute global router with efficient via minimization\", b18: \"nthu route a fast and stable global router\" are the papers that inspired the negotiation-based method used in the paper.\n\nb20: \"negotiation based layer assignment for via count and via overflow minimization\" is also a source of inspiration, as it uses the negotiation-based method in a different field, which indicates a strong reliance on this method.", "Based on the text of the academic paper and the titles of the references, the source papers that inspired this paper the most appear to be:\n\n- \"Legalization algorithm for multiple row height standard cell design\" [b4]\n- \"Routability-driven and fence aware legalization for mixed cell height circuits\" [b5]\n- \"Mixed cell height standard cell placement legalization\" [b9]\n- \"Negotiation based layer assignment for via count and via overflow minimization\" [b20]\n\nThe JSON format of the result with a score between 0 and 1 would be:\n\n```json\n{\n  \"b4\": 1,\n  \"b5\": 0.9,\n  \"b9\": 0.8,\n  \"b20\": 0.7\n}\n```\n\nThe scores assigned above are based on the assumption that the paper takes more inspiration from [b4], followed by [b5], [b9], and [b20]. The exact scores would need to be determined by further analysis of the text, such as how often and in what context these references are mentioned.", "Based on the given text and the author notes, the source papers that appear to have inspired the paper most directly are:\n\n- \"Legalization algorithm for multiple row height standard cell design\" [b4]: This paper is explicitly mentioned as an inspiration. \n- \"Routability-driven and fence aware legalization for mixed cell height circuits\" [b5]: This paper is also explicitly mentioned as an inspiration. \n- \"Pathfinder: a negotiation based performance driven router for fpgas\" [b15]: This paper seems to have inspired the negotiation-based method used in the paper. \n- \"NTHU-Route 2.0: A fast and stable global router\" [b18]: This paper is used to illustrate the negotiation-based method, which is a key part of the paper's approach.\n- \"Negotiation-based layer assignment for via count and via overflow minimization\" [b20]: This paper is used to illustrate the negotiation-based method, which is a key part of the paper's approach.\n\nBased on the titles of the papers, these papers appear to be the most relevant to the paper's novel contributions.\n\n{\n\"b4\": 0.9,\n\"b5\": 0.9,\n\"b15\": 0.8,\n\"b18\": 0.8,\n\"b20\": 0.8\n}", "Based on the given text, the source papers that most inspired the given paper can be identified. The key phrases indicating inspiration include \"inspired by [b4]\" and \"the negotiation-based method [b15]\". The paper also extensively references the works [b9], [b11], [b13], [b18], and [b20].\n\nTherefore, the source papers can be identified as:\n\n- \"legalization algorithm for multiple row height standard cell design\" [b4]\n- \"Pathfinder a negotiation based performance driven router for fpgas\" [b15]\n- \"mixed cell height standard cell placement legalization\" [b9]\n- \"mixed cell height legalization considering technology and region constraints\" [b11]\n- \"a robust modulus based matrix splitting iteration method for mixed cell height circuit legalization\" [b13]\n- \"nthu route a fast and stable global router\" [b18]\n- \"negotiation based layer assignment for via count and via overflow minimization\" [b20]\n\nThe JSON format would be:\n\n```json\n{\n    \"b4\": 0.9,\n    \"b15\": 0.9,\n    \"b9\": 0.7,\n    \"b11\": 0.7,\n    \"b13\": 0.7,\n    \"b18\": 0.7,\n    \"b20\": 0.7\n}\n```\nThe numbers represent the confidence scores, and they are approximated based on the extent of the paper's reliance on these works. The confidence scores for b4 and b15 are higher because these papers were explicitly mentioned as sources of inspiration.", "Based on the provided text and author notes, the identified source papers that have most influenced the given paper can be determined. The key phrases indicating inspiration or motivation, and the context of the paper suggest the following papers as sources:\n\n1. \"legalization algorithm for multiple row height standard cell design\" [b4]: The paper states \"Inspired by [b4]\", indicating this work was a significant source of inspiration for the current research.\n\n2. \"routabilitydriven and fence aware legalization for mixed cell height circuits\" [b5]: The paper mentions \"Different from [b4], the MGL method calculates the displacement curves of both the legalized cells and the target cell, which improved the insertion results of the cells, and two flow-based techniques were adopted to further reduce the displacement\", suggesting that this paper was another source of inspiration and methodology, especially in relation to the MGL method.\n\n3. \"a fast robust network flow based standard cell legalization method for minimizing maximum movement\" [b8]: The paper mentions \"Darav et al. [b8] set a precomputed maximum movement ? as a hard constraint to prune the candidate path with flow greater than ?.\", indicating this work was a significant source of inspiration for the current research.\n\n4. \"pathfinder a negotiation based performance driven router for fpgas\" [b15]: The paper states \"The negotiation-based method is first introduced in PathFinder [b15] to solve the FPGA routing problem\", indicating this work was a significant source of inspiration for the current research.\n\n5. \"nthu route a fast and stable global router\" [b18]: The paper states \"The main components of the global router NTHU-Route 2.0 [b18] and layer assignment method NVM [b20] are shown in Table I.\", indicating this work was a significant source of reference for the current research.\n\n6. \"negotiation based layer assignment for via count and via overflow minimization\" [b20]: The paper states \"The main components of the global router NTHU-Route 2.0 [b18] and layer assignment method NVM [b20] are shown in Table I.\", indicating this work was a significant source of reference for the current research.\n\nResult in the JSON format: {\"b4\": 1, \"b5\": 0.9, \"b8\": 0.8, \"b15\": 0.9, \"b18\": 0.8, \"b20\": 0.8}"]}