{"621454435aee126c0f1d7065": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\nThe key novel contributions of this paper seem to be:\n1) Evaluating whether automated disinformation generators can effectively contaminate the evidence sets of fact verification systems\n2) Demonstrating that synthetic disinformation drastically lowers the performance of these systems in two adversarial attack settings: adversarial addition of generated documents and adversarial modification of existing documents\n3) Showing that fact verification systems are sensitive to even small amounts of evidence contamination, and that synthetic disinformation is more influential than human-produced false content\n\nThe paper mentions being directly inspired by or building upon the following references:\n- [b46] (GROVER): A large-scale language model trained on news articles, which demonstrated the ability to generate synthetic news content that humans find as trustworthy as real news. This directly inspired the use of language models to generate disinformation in this paper.\n- [b39] (FEVER dataset and task formulation): Established the popular framework of claim verification against a large repository of evidence documents, which this paper adopts and extends to the adversarial disinformation setting.\n- [b42] (SCIFACT) and [b36] (COVIDFACT): Datasets that re-purposed the FEVER framework for scientific/medical claims, which are used as the testbeds in this paper's experiments.\n\nOther references that seem highly relevant to the paper's main contributions:\n- [b5], [b19]: Studies directly exploring whether language models could generate disinformation that influences human readers\n- [b4]: Mentions the possibility of malicious actors overloading content with adversarial information, which motivates this paper\n- [b21]: Shows that misinformation can make its way into trusted knowledge bases like Wikipedia, further motivating the threat model\n\nIn summary, here are the most pertinent source papers in my assessment, with confidence scores:\n\n{\n  \"[b46]\": 0.95,\n  \"[b39]\": 0.9,\n  \"[b42]\": 0.85, \n  \"[b36]\": 0.85,\n  \"[b5]\": 0.8,\n  \"[b19]\": 0.8,\n  \"[b4]\": 0.75,\n  \"[b21]\": 0.7\n}\n\nThe papers [b46], [b39], [b42] and [b36] are the most central references that directly inspired and enabled the main contributions. The others provide important motivation and additional context for the threat of adversarial disinformation in fact-checking.", "6221834e5aee126c0f23c25c": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b53]\": 0.9,\n  \"[b52]\": 0.8,\n  \"[b37]\": 0.7,\n  \"[b51]\": 0.6\n}\n\nConfidence scores:\n- [b53]: 0.9 - This paper is directly compared and contrasted multiple times as prior work on dataset condensation using gradient matching. The proposed method aims to overcome limitations of [b53] like bias towards hard examples. \n- [b52]: 0.8 - Also a very relevant prior work on dataset condensation using feature distribution matching. The proposed method is compared to [b52], with key differences highlighted.\n- [b37]: 0.7 - The pioneering work on dataset condensation using meta-learning is discussed as important context and motivation, though not as directly built upon.\n- [b51]: 0.6 - Another prior work on improving the data efficiency of [b53] using Siamese augmentation. Mentioned as relevant context.\n\nThe key phrases indicating inspiration and direct comparison are:\n- \"Unlike the approach of [b53], we account for the distribution consistency...\"\n- \"Our method is different from it [[b52]] in three main aspects:\"\n\nThe paper's main novel contributions appear to be 1) Layer-wise feature alignment to better capture data distribution compared to gradient matching, 2) Discrimination loss to explicitly make synthetic data discriminative, and 3) Dynamic bi-level optimization to adaptively update synthetic data and network.", "621ee1835aee126c0f26a902": "Based on analyzing the text, here are the key source papers that likely inspired or motivated this work, in JSON format with confidence scores:\n\n{\n  \"[b17]\": 0.9,\n  \"[b31]\": 0.8,\n  \"[b33]\": 0.8,\n  \"[b36]\": 0.7,\n  \"[b49]\": 0.7,\n  \"[b13]\": 0.6,\n  \"[b29]\": 0.6\n}\n\nThe key insights are:\n\n- The paper is heavily motivated by the limitations of current GNN benchmark datasets, as discussed in [b17], [b31], and [b33]. The authors argue that relying on only a few real-world datasets leads to inadequate generalization, incremental overfitting, and unscalable development.\n\n- [b36] and [b49] are cited when discussing the need for proper GNN experimental design and fair model comparisons, which this work aims to enable. \n\n- [b13] and [b29] are referenced regarding the need for reproducible settings to fairly compare GNN models.\n\n- The novel contribution is GraphWorld - a tunable, scalable and reproducible method for generating synthetic benchmark datasets to analyze GNN performance. This aims to address the limitations of relying only on real-world datasets.\n\nSo in summary, the key motivating works discuss the problems with current GNN benchmarking practices that this paper seeks to solve with the new GraphWorld methodology. The confidence scores are high for the papers that most directly inspired the core ideas.", "621635aa91e011b46d7ce15d": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b2]\": 0.9,\n  \"[b14]\": 0.8,\n  \"[b16]\": 0.7,\n  \"[b1]\": 0.6,\n  \"[b25]\": 0.5\n}\n\nConfidence scores:\n- [b2] (score 0.9): The paper takes direct inspiration from the MAML framework proposed in this reference for developing its meta adapter module to generate personalized models for different subgroups. The methodology has strong similarities.\n\n- [b14] (score 0.8): SIGN, proposed in this reference, is used as an example base GNN model to illustrate how Ada-GNN can enhance scalable GNNs. The decoupling approach of SIGN into graph pre-processing and post-classification aligns with Ada-GNN's methodology. \n\n- [b16] (score 0.7): SAGN, an extension of SIGN from [b14], is also used as an example base model. Its attention pooling of multi-hop neighborhood information is incorporated into Ada-GNN.\n\n- [b1] (score 0.6): ClusterGCN's approach of partitioning graphs into subgraphs is employed in Ada-GNN, but with the key difference of just tagging nodes with subgroup IDs rather than creating disjoint subgraphs. Still, the subgraph generation methodology is similar.\n\n- [b25] (score 0.5): GraphSAINT also restricts node receptive fields by generating subgraphs, which relates to Ada-GNN's subgroup-based personalization approach, though the subgraph sampling details differ.\n\nIn summary, the MAML framework [b2] and the scalable GNNs SIGN [b14], SAGN [b16] that decouple graph propagation and classification emerge as the most influential references that directly inspire the key novel contributions of Ada-GNN. The subgraph-based training approaches of [b1] and [b25] also share important similarities with Ada-GNN's subgroup-level personalization.", "620f0e735aee126c0fec4734": "Based on analyzing the paper, here are the key source papers that likely inspired or motivated this work, in order of confidence:\n\n{\n  \"[b9]\": 0.9,\n  \"[b35]\": 0.85,\n  \"[b8]\": 0.8,\n  \"[b33]\": 0.75,\n  \"[b11]\": 0.7\n}\n\nRationale:\n\nThe paper proposes a graph contrastive learning framework called S3-CL. A few key aspects point to the likely source papers:\n\n1. It uses a momentum encoder, citing this as being inspired by [b9] which introduced momentum contrast for visual representation learning. This is the clearest direct inspiration.\n\n2. For the structural contrastive learning, it uses graph diffusion to generate augmented graph views, citing [b35] which proposed feature propagation and simple graph convolution. \n\n3. The overall graph contrastive learning approach seems motivated by prior works like MVGRL [b8] and DGI [b33] which introduced contrastive learning on graphs.\n\n4. The semantic contrastive learning module has some similarities to MERIT [b11] in terms of inferring semantic clusters/prototypes and contrasting them.\n\nSo in summary, [b9] for the momentum encoder, [b35] for graph diffusion, and [b8], [b33], [b11] for the overall graph contrastive learning paradigm are the key likely sources of inspiration and motivation for the novel aspects of S3-CL, based on my analysis of the text. The confidence scores reflect the strength of influence I inferred from how these sources were described and used in the paper.", "622183525aee126c0f23c7c2": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b17]\": 0.9,\n  \"[b18]\": 0.9,\n  \"[b6]\": 0.8,\n  \"[b8]\": 0.7,\n  \"[b3]\": 0.7,\n  \"[b10]\": 0.6\n}\n\nConfidence scores:\n- [b17], [b18]: 0.9 - The paper states \"The interactive-based framework is usually performs better than the representation-based framework\", directly indicating that these two references, which likely propose interaction-based text matching methods, heavily inspired their work.\n\n- [b6]: 0.8 - Also falls under the category of \"interaction-based text matching\" that the authors build upon. \n\n- [b8], [b3]: 0.7 - The paper mentions \"Recent studies show that encoding multi-granularity information [b8] [b3] into sentences can mitigate this problem\", suggesting these references directly motivated their approach of incorporating contextual information.\n\n- [b10]: 0.6 - Mentioned as another recent study showing \"word sense information [b10]\" can help, which relates to their method of using external context, but is not as direct an influence as the interaction-based text matching references.\n\nThe other references like [b0], [b13], [b7], [b12] provide background on short text matching applications and representation-based methods, but do not seem to be as directly influential on the paper's novel contributions around context-enhanced interaction-based matching. The experimental comparison references are also not direct influences.\n\nIn summary, the interaction-based text matching references [b17], [b18], [b6] and the multi-granularity encoding references [b8], [b3] appear to be the most central prior works that inspired and motivated this paper's key contributions. The word sense reference [b10] is also relevant but to a lesser degree.", "62451c2b5aee126c0f47ac08": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b3]\": 0.9,\n  \"[b2]\": 0.8,\n  \"[b7]\": 0.7,\n  \"[b1]\": 0.6,\n  \"[Chen et al., 2020]\": 0.5\n}\n\nConfidence scores:\n- [b3] (0.9): This reference is directly cited as inspiration for the supervised contrastive loss used in the paper, which is a key novel contribution. The paper states \"Inspired by [b3], we propose a transductive fine-tuning based framework tailored for fewshot node classification.\"\n- [b2] (0.8): Referenced multiple times in relation to the graph contrastive learning methods that the paper builds upon and improves. Methodological similarities in using contrastive learning.\n- [b7] (0.7): Directly cited as motivation for the transductive fine-tuning paradigm the paper adopts, another key contribution. \"Inspired by [b7], we propose a transductive fine-tuning based framework tailored for fewshot node classification.\"\n- [b1] (0.6): Cited in the context of arguing that feature reuse rather than meta-learning enables fast adaptation, which motivates the paper's approach. \n- [Chen et al., 2020] (0.5): Referenced as a key work in contrastive learning that the paper compares its supervised contrastive loss against.\n\nIn summary, the references [b3], [b2], [b7] and [b1] emerge as the most influential, as they directly inspire and motivate the paper's main novel contributions of supervised contrastive pretraining and transductive fine-tuning for graph few-shot learning. The methodological similarities are also strongest with these works.", "6226c93d5aee126c0fd57ba8": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b21]\": 0.9,\n  \"[b38]\": 0.8,\n  \"[b7]\": 0.7\n}\n\nConfidence scores:\n[b21] (BootEA): 0.9\n- The paper mentions that \"BootEA is a powerful method that greatly improved the performance of the basic AlignE model. This bootstrapping method is closely related to RLEA as it also assumes that a candidate entity should not be matched more than once.\"\n- This indicates BootEA was a key inspiration and motivation for the sequential decision making approach proposed in this paper.\n\n[b38] (CEAFF): 0.8 \n- The paper states \"One most relevant work to this paper is CEAFF [b38], which also leverages RL algorithms and believes in 1-to-1 alignment.\"\n- Though it notes some differences, CEAFF seems to be the most closely related prior work that combines RL with entity alignment.\n\n[b7]: 0.7\n- In the Environment section, the paper describes using a \"curriculum learning strategy\" and cites [b7]. \n- This reference seems to directly inspire their approach of gradually increasing the difficulty of the entity alignment pairs shown to the RL agent during training.\n\nThe other references, while relevant for background and context, do not seem to be called out as directly inspiring the key novel contributions of this paper based on my analysis of the text. The core novelty appears to be formulating entity alignment as a sequential decision making problem solved with RL, while drawing on some key ideas from [b21], [b38] and [b7].", "6243ca915aee126c0fbd0aa0": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[14]\": 0.9,\n  \"[22]\": 0.8,\n  \"[0]\": 0.7,\n  \"[9]\": 0.6,\n  \"[16]\": 0.5\n}\n\nConfidence scores:\n- [14] (TGN): 0.9 \nThe paper takes heavy inspiration from the TGN architecture, adopting its key components like node memory, attention aggregator, and temporal sampler. It aims to provide a general framework unifying approaches like TGN.\n\n- [22] (APAN): 0.8\nThe mailbox module and pure memory-based architecture of APAN are incorporated as key features in the proposed TGL framework. \n\n- [0] (TGAT): 0.7  \nThe attention aggregator from TGAT is used as a core component in the TGL framework for message passing.\n\n- [9] (JODIE): 0.6\nJODIE's pure memory-based TGNN approach using RNNs to update node memory is supported by the general TGL framework.\n\n- [16] (DySAT): 0.5\nThe snapshot-based TGNN methodology of DySAT is compatible with and supported by the proposed TGL framework.\n\nIn summary, the paper aims to unify key innovations from recent TGNN architectures like TGN, APAN, TGAT, JODIE and DySAT into a general, scalable framework called TGL. It heavily builds upon their core ideas around temporal sampling, memory modules, attention and snapshot-based processing to enable efficient large-scale TGNN training.", "62296c7b5aee126c0f57d7aa": "Here is my analysis of the key source papers for the given paper, based on the criteria you provided:\n\n{\n  \"[b18]\": 0.9,\n  \"[b15]\": 0.8,\n  \"[b16]\": 0.7,\n  \"[b14]\": 0.6,\n  \"[b3]\": 0.6,\n  \"[b6]\": 0.5,\n  \"[b11]\": 0.5\n}\n\nConfidence scores:\n- [b18] (Graphormer): 0.9 \nThe paper directly builds upon and improves the Graphormer architecture. It is the key inspiration and foundation for this work.\n\n- [b15] (Transformer): 0.8\nThe Graphormer architecture itself is based on the standard Transformer. The placement of layer normalization in Transformer variants is a key aspect analyzed in this paper.\n\n- [b16]: 0.7\nThis reference provides insights on the impact of layer normalization placement in Transformers, which directly motivates the analysis of Pre-LN vs Post-LN in this paper.\n\n- [b14]: 0.6\nThe 3D adaptation of Graphormer uses Gaussian basis functions from this reference to encode spatial relations between atoms.\n\n- [b3] (Open Catalyst 2020): 0.6  \nThe OC20 dataset introduced in this reference is used to evaluate the 3D Graphormer model. Adapting Graphormer for this dataset is a major contribution.\n\n- [b6]: 0.5\nThe auxiliary objective of predicting atom displacements is inspired by this reference.\n\n- [b11]: 0.5\nThe theoretical analysis of Graphormer's expressiveness uses results from this reference that connects GNNs to distributed computing theory.\n\nThe other references like [b17], [b7], [b8] provide relevant background but do not seem to be as directly influential on the novel contributions as per my analysis. Let me know if you need any clarification or have additional questions!", "624fa8db5aee126c0f3a5be7": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b34]\": 0.9,\n  \"[b17]\": 0.8,\n  \"[b14]\": 0.7,\n  \"[b12]\": 0.7,\n  \"[b31]\": 0.6\n}\n\nRationale and key points:\n\n- The paper heavily relies on and extends the standard formulation of the Gaussian diffusion model introduced in [b31] and further developed in [b14], [b17], and [b34]. The authors state they achieve video generation with \"little modification other than straightforward architectural changes\" to the standard diffusion model.\n\n- The paper uses the discrete time ancestral sampler from [b14] and discusses the continuous time diffusion model formulation from [b38], [b34], [b5], [b17], indicating these are foundational works.\n\n- For the text-conditioned video generation experiments, the paper directly applies the classifier-free guidance method from [b12], citing it as a key technique for improving sample quality. \n\n- While other papers like [b23], [b32], [b40], [b33] are cited when discussing diffusion model details, the core novel contributions build most directly on [b31], [b14], [b17], [b34] and [b12].\n\n- The paper's main contributions are extending diffusion models to video with a factorized 3D U-Net architecture and introducing an improved gradient method for conditional sampling. The key prior works that enable these contributions are the foundational diffusion model papers cited above.\n\nSo in summary, while many diffusion model papers are discussed as background, the core ideas that this paper is \"inspired by\" and \"builds upon\" come from [b34], [b17], [b14], [b12] and [b31], in my assessment based on the criteria provided. The methodology shares strong similarities with these works.", "6243ca9b5aee126c0fbd1cfd": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b27]\": 0.9,\n  \"[b3]\": 0.8,\n  \"[b26]\": 0.7,\n  \"[b20]\": 0.6,\n  \"[b17]\": 0.5\n}\n\nRationale and key observations:\n\n- The paper makes a very direct connection to [b27], stating that it \"formalise[s] the notion of algorithmic alignment\" and \"predicts that graph neural networks (GNNs) algorithmically align with dynamic programming\". This reference seems to be the key inspiration and motivation for the paper's main ideas.\n\n- [b3] is cited when defining the GNN formulation the paper builds upon. It appears to be a foundational reference for the geometric deep learning concepts leveraged.\n\n- The paper mentions that [b26] provides a \"geometric view into how better generalisation can be achieved\" for GNNs, which relates to one of the paper's goals of identifying GNN architectures with good algorithmic alignment and generalization. \n\n- [b20] is an early work cited as demonstrating superior performance of max aggregation in GNNs for pathfinding algorithms. This directly supports one of the paper's corollaries about importance of aligned aggregators between GNNs and dynamic programming.\n\n- Similarly, [b17] is mentioned as observing the benefits of max aggregation for iterative algorithms, providing further evidence for the aligned aggregator concept the paper puts forth.\n\nThe paper's novel contributions appear to be 1) Formalizing the GNN-DP connection using category theory and integral transforms, and 2) Unifying prior results around aggregator alignment under this new theoretical framework. The selected references either directly inspired these contributions ([b27], [b3]) or provide empirical support for the framework's implications ([b20], [b17], [b26]).", "623d33155aee126c0f100f50": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b2]\": 0.9,\n  \"[b7]\": 0.8,\n  \"[b22]\": 0.8,\n  \"[b19]\": 0.7,\n  \"[b14]\": 0.6\n}\n\nConfidence scores:\n[b2] (SimCLR): 0.9 - The paper draws heavy inspiration from SimCLR for its contrastive learning framework. It points out gaps between the InfoMax principle used by SimCLR and downstream performance, which directly motivates the novel contributions of this paper.\n\n[b7] (MoCo): 0.8 - MoCo is referenced as prior work that GCC built upon by using a momentum encoder and maintaining a queue of samples. This paper aims to improve upon methods like MoCo.\n\n[b22] (GraphCL): 0.8 - GraphCL is a key prior work that applies contrastive learning to graphs following SimCLR. This paper builds upon GraphCL and aims to address its limitations.\n\n[b19] (DGI): 0.7 - DGI was the first prominent work to apply the InfoMax principle to graph representation learning. It forms the foundation that subsequent graph contrastive learning methods, including this paper, build upon.  \n\n[b14] (GCC): 0.6 - GCC is mentioned as a graph contrastive learning method that refers to MoCo. It represents the existing graph contrastive learning approaches that this paper seeks to improve.\n\nThe novel contribution of this paper is the Graph Complementary Contrastive Learning (GraphCoCo) approach that uses non-maximum erasing to learn complementary graph embeddings. It aims to address the issue of embeddings concentrating information in a few \"highlighted\" dimensions, which the paper argues limits the expressiveness of the learned representations.", "625e1a335aee126c0feca4ba": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b42]\": 0.9,\n  \"[b43]\": 0.9,\n  \"[b46]\": 0.9, \n  \"[b58]\": 0.8,\n  \"[b76]\": 0.8,\n  \"[b95]\": 0.8,\n  \"[b109]\": 0.7\n}\n\nThe key reasons for selecting these papers as the most influential sources are:\n\n1. The paper focuses heavily on message passing graph neural networks (MPNNs). References [b42], [b43], [b46], [b58], [b76], [b95] are directly cited when defining the MPNN framework and iterative message passing scheme, indicating they form the foundation for the MPNN methodology used in this paper.\n\n2. The paper analyzes the representational power of MPNNs in terms of graph isomorphism testing and ability to distinguish different graphs. Theorem 1 directly cites [b109] (the Graph Isomorphism Network paper) as a key prior result that connects the discriminative power of MPNNs to the Weisfeiler-Leman graph isomorphism test.\n\n3. These source papers are cited in the context of the core contributions of this paper - analyzing the theoretical properties and limitations of MPNNs. The frequent references and direct comparisons suggest the heavy influence of these prior works in motivating and shaping the novel analyses presented here.\n\n4. While other references like [b39] are also relevant in demonstrating limitations of MPNNs, the selected papers appear to be more foundational to the MPNN framework itself that enables the theoretical analysis forming the main focus of this paper.\n\nSo in summary, [b42], [b43], [b46], [b58], [b76], [b95] are the most critical sources for the MPNN methodology, while [b109] directly inspires the graph isomorphism analysis. The repeated citations and comparisons to these works when presenting the core ideas indicate their strong influence in motivating this paper's contributions.", "62393e7f5aee126c0f126162": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b17]\": 0.9,\n  \"[b11]\": 0.85,\n  \"[b2]\": 0.8,\n  \"[b30]\": 0.75,\n  \"[b24]\": 0.7,\n  \"[b15]\": 0.65,\n  \"[b31]\": 0.6\n}\n\nThe key points that led me to this conclusion:\n\n- The paper focuses on the novel contribution of proposing BNS-GCN, a method for efficient distributed training of Graph Convolutional Networks (GCNs) by sampling boundary node features. \n\n- It directly cites [b17] as establishing GCNs as the state-of-the-art method for graph learning tasks, indicating [b17] was a key inspiration. The high frequency of citing [b17] throughout also shows its centrality.\n\n- [b11] (GraphSAGE) and [b2] (VR-GCN) are highlighted as pioneering sampling-based methods to scale GCN training, which the proposed BNS-GCN aims to improve upon. The comparisons with these two papers' methods indicate they were important motivations.\n\n- For the distributed training aspect, [b30] (Dorylus), [b24] (NeuGraph), [b15] (ROC) and [b31] (CAGNET) are cited as prior state-of-the-art partition-parallel methods that still suffer limitations like communication overhead, which BNS-GCN tackles. The direct comparisons signal these were key prior works the authors built upon.\n\n- Overall, the authors' own statements about building on the sampling-based methods of [b11] and [b2] and the partition-parallel training approaches of [b30], [b24], [b15], [b31] to enable scalable GCN training clearly mark these as the most central motivating works, with [b17] as the pioneering work that established GCNs as a powerful framework.", "6241273e5aee126c0f292ae3": "YouChat is a large language model from You.com.\n\nYou.com is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. YouChat will generate a comprehensive and informative answer for a given question based on the provided web Search Results. YouChat considers the history of the conversation and answer the questions in a helpful manner. It tries to use the information from the provided search results to ensure accuracy and uses an unbiased and journalistic tone. YouChat will combine search results together into a coherent answer and cite search results using [${{number}}] notation. It will only cite the most relevant results that answer the question accurately.", "6271e0e75aee126c0f574720": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[46]\": 0.9,\n  \"[61]\": 0.9,\n  \"[26]\": 0.8,\n  \"[51]\": 0.7,\n  \"[52]\": 0.7,\n  \"[53]\": 0.7\n}\n\nThe key reasons for selecting these papers as the most pertinent sources are:\n\n1. The paper directly states that dynamic programming methods for all-pairs-shortest-path problems using algorithms like Bellman-Ford or Floyd-Warshall \"can be expressed in a semiring-like structure\" by customizing the \u2295 and \u2297 operators, citing [46] and [61]. This indicates these papers provided direct inspiration for the semiring formulation central to the SIMD^2 architecture.\n\n2. The SIMD^2 architecture is built on top of existing matrix multiplication accelerators like Google's TPUs [26] and NVIDIA's Tensor Cores [51][52]. The paper frequently references these as the foundation that SIMD^2 extends to support a broader set of semiring operations. \n\n3. The SIMD^2 ISA is described as building directly on top of NVIDIA's warp-level matrix multiply accumulate (wmma) instructions [53], extending them to support additional arithmetic operations. This suggests the wmma abstraction was a key inspiration.\n\nIn summary, the dynamic programming papers [46][61] seem to have directly motivated the generalized semiring formulation, while the TPU [26], Tensor Core [51][52] and wmma [53] papers provided the architectural foundation that SIMD^2 extends. These appear to be the most central prior works that the novel contributions of this paper build upon.", "6274c91a5aee126c0f71246a": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b35]\": 0.9,\n  \"[b1]\": 0.8,\n  \"[b40]\": 0.7,\n  \"[b32]\": 0.6,\n  \"[b13]\": 0.5\n}\n\nRationale:\n\nThe paper introduces an Uncertainty-aware Pseudo Label Refinery (UPLR) framework for unsupervised entity alignment. The key novel contributions are:\n\n1. Adapting the non-local model of [b40] to introduce a gate module to the Graph Attention Networks (GAT) [b35] framework. This Gate Graph Attention Network (GateGAT) allows efficiently modeling relationships between widely separated spatial regions.\n\n2. Designing a relational translation matrix inspired by TransE [b1] to further explore the translation information for relations based on triples. \n\n3. Using graph interaction divergence to measure the similarity of cross-lingual knowledge graph entity pairs and construct pseudo-labels in an unsupervised manner.\n\n4. Employing pseudo-labeling techniques similar to [b13] but combined with a novel non-sampling calibration strategy and gradual enhancement to reduce the impact of noisy labels.\n\nThe gate module added to GAT seems directly inspired by [b32]. The overall pseudo-labeling approach has strong similarities to [b13].\n\nTherefore, the papers [b35], [b1], [b40], [b32] and [b13] appear to be the most central references that the novel aspects of this paper build upon or are inspired by, based on the methodological similarities and comparative discussion in the text. The confidence scores reflect the degree of direct relevance inferred for each reference.", "626b868a6750f822a0a5e16d": "Here are the key steps I took to identify the most relevant source papers for this paper:\n\n1. The main novel contributions of the paper are:\n- Introducing the Flamingo family of Visual Language Models (VLMs) that can perform various multimodal tasks from only a few examples\n- Novel architecture for accepting interleaved visual and text data and generating open-ended text output\n- Leveraging large pretrained vision-only and language-only models while preserving their benefits and efficiently fusing modalities\n- Efficient adaptation to varying size visual inputs, making it applicable to both images and videos\n\n2. The paper mentions being directly inspired by or building upon:\n- Recent work in large-scale generative language models that are good few-shot learners, like GPT-3 [[b11]], Chinchilla [[b47]], PaLM [[b20]], Megatron-Turing NLG [[b94]] \n- The Chinchilla model [[b47]] which they use as the base for their largest model\n- Perceiver architecture [[b53]] for producing fixed number of visual tokens from varying visual input\n- Adapter techniques like bottleneck adapters [[b48]][[b115]] and vision-to-prefix transformers [[b85]]\n- Frozen pretrained language model approaches from VisualGPT [[b15]], MAGMA [[b28]], VC-GPT [[b73]] etc.\n\n3. The key architectural components that enable Flamingo's novel contributions are:\n- Interleaving cross-attention layers with frozen language-only layers \n- Gating mechanisms to minimize effect of new layers at initialization\n- Perceiver-based architecture to handle varying visual inputs efficiently\n\nBased on this analysis, here are the most relevant source papers in order of importance:\n\n{\n  \"b47\": 0.95,\n  \"b11\": 0.9, \n  \"b53\": 0.85,\n  \"b15\": 0.8,\n  \"b28\": 0.75,\n  \"b73\": 0.7,\n  \"b48\": 0.65,\n  \"b115\": 0.65\n}\n\nThe Chinchilla [[b47]] and GPT-3 [[b11]] papers are the most crucial influences, providing the foundation language models that Flamingo builds upon. The Perceiver [[b53]] architecture is key to Flamingo's ability to handle visual data efficiently. Papers like VisualGPT [[b15]], MAGMA [[b28]] and VC-GPT [[b73]] directly inspire Flamingo's approach of leveraging frozen pretrained language models for vision-language tasks. Finally, bottleneck adapter papers [[b48]][[b115]] influence some of Flamingo's architectural choices.", "626754bb5aee126c0fbccbaa": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b28]\": 0.9,\n  \"[b18]\": 0.8,\n  \"[b41]\": 0.8,\n  \"[b51]\": 0.7,\n  \"[b53]\": 0.7,\n  \"[b56]\": 0.7\n}\n\nRationale:\n- The paper takes heavy inspiration from the work of [b28] on de-biased learning to rank from biased feedback data. It employs a similar debiasing strategy using inverse propensity scoring to develop an unbiased utility-aware learning to rank approach. This is a core novel contribution.\n\n- The counterfactual learning framework proposed in the paper is motivated by the principles of counterfactual estimation/learning with logged bandit feedback developed in [b18], [b41], and [b51]. These papers provide the theoretical foundations that the authors build upon.\n\n- For the practical instantiation of their utility-aware query auto-completion system, the authors leverage techniques from extreme multi-label learning, specifically citing [b53] and [b56]. They adapt the PECOS approach from [b56] to make their query retrieval component utility-aware.\n\n- While other references like [b6], [b40] provide useful background on query auto-completion systems, the ones selected above appear to be the most central in directly inspiring and enabling the primary novel contributions of this paper - namely the utility-aware learning to rank framework and its practical realization.", "627332775aee126c0f18d585": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b7]\": 0.9,\n  \"[b24]\": 0.8,\n  \"[b22]\": 0.7,\n  \"[b33]\": 0.6,\n  \"[b38]\": 0.6\n}\n\nConfidence scores:\n[b7] (BERT paper): 0.9\n- The paper heavily relies on pre-trained language models (PLMs) like BERT to derive general text representations, especially for out-of-vocabulary seeds. BERT's ability to infer meanings of unseen words by segmenting them into subwords is key to the paper's approach.\n\n[b24] (Skip-Gram paper): 0.8 \n- The paper's local embedding objective is inspired by Skip-Gram's assumption that a term's semantics are similar to its local context. The proof that maximizing the objective implicitly factorizes PMI matrices also follows the Skip-Gram paper.\n\n[b22] (CatE paper): 0.7\n- CatE is a seed-guided embedding learning method that the proposed approach directly builds upon and compares to. The idea of learning term embeddings to capture category-discriminative information is motivated by CatE.\n\n[b33] and [b38]: 0.6 each\n- These papers inspire the modeling of term semantics based on the documents they appear in, which is incorporated into the local embedding objective.\n\nIn summary, the BERT paper [b7] and Skip-Gram paper [b24] provide the key motivations and technical foundations, while CatE [b22] is the most relevant prior work that the proposed SEETOPIC framework directly improves upon to tackle the new challenge of out-of-vocabulary seeds. [b33] and [b38] provide additional inspiration for capturing local corpus semantics.", "621ee1895aee126c0f26af67": "Here are the key steps I took to identify the most pertinent source papers for this paper:\n\n1. The main novel contributions of the paper are:\n- Proposing a new Scalable Graph Neural Architecture Paradigm (SGAP) to make GNN training more scalable \n- Defining a general design space for scalable GNNs based on the SGAP paradigm\n- Developing an auto-search system PaSca to automatically explore the scalable GNN design space\n\n2. Looking for key phrases indicating inspiration from previous work, I did not find any explicit phrases like \"inspired by\" or \"motivated by\". \n\n3. Identifying references directly linked to the main novel methods:\n- The SGAP paradigm is a new proposal, not directly attributed to previous work\n- The design space dimensions like different aggregators reference some prior papers like [[19]], [[20]], [[34]] but the overall design space is novel\n- The PaSca auto-search system with multi-objective search is a new contribution\n\n4. Evaluating the titles of referenced papers, some that seem most relevant to the core contributions are:\n- [[51]] Simplifying Graph Convolutional Networks (SGC)\n- [[11]] SIGN: Scalable Inception Graph Neural Networks \n- [[67]] Simple Spectral Graph Convolution (S2GC)\n- [[5]] Scalable Graph Neural Networks via Bidirectional Propagation (GBP)\nThese propose specific scalable GNN architectures that are considered instances in the new design space.\n\n5. The papers referenced for the aggregators like [[19]], [[20]], [[34]] provide key building blocks but are not the main inspiration.\n\nIn summary, while the paper builds on components from prior work, its main contributions of the SGAP paradigm, design space, and PaSca auto-search system appear to be novel proposals not directly attributed to previous papers. The papers most relevant to the scalable GNN topic are [[51]], [[11]], [[67]], [[5]] which propose specific architectures that are considered instances in the new design space.\n\nHere is the list of the most pertinent source papers, in JSON format with confidence scores:\n\n{\n  \"51\": 0.8,\n  \"11\": 0.7,\n  \"67\": 0.7, \n  \"5\": 0.8\n}\n\nThe scores reflect that these papers are quite relevant in proposing scalable GNN architectures that relate to the design space, but the paper's core contributions go beyond these specific instances. Let me know if you need any clarification or have additional questions!", "628749425aee126c0ffee615": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b12]\": 0.9,\n  \"[b26]\": 0.85,\n  \"[b39]\": 0.8,\n  \"[b29]\": 0.7\n}\n\nConfidence scores:\n[b12] - 0.9\nThe paper heavily relies on the loss correction method proposed in [b12], which estimates a confusion matrix using a small set of clean labels. This forms the core of their FCLC framework.\n\n[b26] - 0.85 \nThe forward loss correction idea that is central to their method is directly inspired by [b26]. They extend this idea to work in a cluster-wise manner.\n\n[b39] - 0.8\nFor fair comparison, they use the same backbone model structure as [b39] (NFETC). This indicates [b39] is a key reference point that their work builds upon.\n\n[b29] - 0.7\nThe paper leverages the concept of composite losses from [b29] in their loss correction approach. Understanding composite losses seems important to their methodology.\n\nThe paper introduces a novel FCLC framework to handle label noise in distantly supervised fine-grained entity typing. The key aspects - loss correction, confusion matrix estimation, feature clustering - draw direct inspiration and build closely upon the referenced papers, especially [b12] and [b26]. The backbone model follows [b39].\n\nSo in summary, [b12], [b26], [b39] and [b29] emerge as the most central and influential references that enable the main contributions of this paper, with [b12] and [b26] being the clearest examples of direct inspiration.", "62451c2b5aee126c0f47ab40": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b18]\": 0.95,\n  \"[b10]\": 0.8,\n  \"[b74]\": 0.7,\n  \"[b32]\": 0.7\n}\n\nConfidence scores:\n[b18] (0.95): The paper heavily relies on the BERT model proposed in this reference for its core language model pretraining approach. The authors extend BERT by incorporating document link information, but BERT remains the foundation.\n\n[b10] (0.8): GPT language models are mentioned alongside BERT in the introduction as the basis for modern NLP systems built on pretrained language models. While not as central as BERT, GPT still provides important context and motivation for the work.\n\n[b74] (0.7), [b32] (0.7): These two references are highlighted in the \"Graph machine learning perspective\" section as providing the theoretical motivation for the paper's two pretraining tasks - masked language modeling (MLM) and document relation prediction (DRP). MLM is linked to node feature prediction in graphs, while DRP is linked to link prediction. The paper aims to fuse language-based and graph-based self-supervised learning, directly inspired by these works.\n\nOther references like [b77], [b55], [b8], [b58] provide useful background information on language model pretraining, but do not seem to be as centrally tied to the novel contributions of this specific paper based on the provided criteria. The key aspects are extending BERT with document links, and combining language model pretraining objectives with graph representation learning objectives, which are best captured by the four references selected above, in my assessment.", "6233f88d5aee126c0f94b61a": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b9]\": 0.9,\n  \"[b45]\": 0.85,\n  \"[b52]\": 0.8,\n  \"[b14]\": 0.75,\n  \"[b36]\": 0.7\n}\n\nRationale:\n- The paper takes direct inspiration from the minimax approach in [b9] and [b45] for its sample selection mechanism, as evidenced by this quote: \"Inspired by the minimax approach [b9] [b45] , our selection mechanism is based on finding top-k 1 (out of K) worst-case augmented samples from the X set.\"\n- The paper builds upon and compares to the Unsupervised Data Augmentation (UDA) method from [b52] for the consistency training setup. \n- Knowledge Distillation [b14] is a key component that the paper adapts its method for in the teacher-student training setup.\n- The paper makes direct comparisons to CoDA [b36], using a similar evaluation setup: \"we also report results on the development set when models are trained only for 5 epochs (depicted by Dev), similar to CoDA [b36] , to make a comparison with baselines.\"\n\nIn summary, while the paper references many prior works, [b9], [b45], [b52], [b14] and [b36] emerge as the most central influences based on the direct inspiration drawn, methodological similarities, and comparative analysis performed relative to those works. The minimax approach from [b9] and [b45] forms the core of the paper's novel sample selection method.", "628c4ce15aee126c0ff596d4": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b11]\": 0.95,\n  \"[b34]\": 0.8,\n  \"[b18]\": 0.75,\n  \"[b35]\": 0.7,\n  \"[b51]\": 0.6,\n  \"[b52]\": 0.6\n}\n\nThe key points that led me to these source papers are:\n\n1. The paper introduces EGR, an E(3)-equivariant graph deep learning model for protein complex structure refinement and quality assessment. It directly states that it follows the work of [b11] to incorporate E(3)-equivariance. This indicates [b11] was a major inspiration.\n\n2. The paper combines insights from Equivariant Graph Neural Networks [b11], EquiDock [b34], and EquiBind [b18] in developing the EGR model architecture. The direct callouts to [b34] and [b18] suggest they were important previous works the authors built upon.  \n\n3. The paper compares EGR's performance to AlphaFold-Multimer [b35], using it to generate decoy structures, indicating [b35] influenced their approach and experimental setup.\n\n4. In the baselines, the paper compares to recent geometric deep learning methods SE(3)-Transformers [b51] and Steerable Equivariant Graph Neural Networks [b52], suggesting these works share relevant methodological similarities to EGR that make them important points of comparison.\n\n5. While other references like [b39], [b40] are mentioned in the related work on DL for protein structure refinement, the paper does not directly compare to or indicate building upon these works to the same degree as the papers listed above.\n\nSo in summary, based on the phrasing and comparisons, [b11], [b34], [b18], [b35], [b51] and [b52] appear to be the most central, directly influential previous works for the main contributions of this paper. The scores reflect my confidence in their relevance based on the textual evidence.", "62466dd35aee126c0f8b79e7": "Here is my analysis of the most pertinent source papers for the given paper, in JSON format with reference numbers as keys and confidence scores between 0 and 1 as values:\n\n{\n  \"[b18]\": 0.9,\n  \"[b4]\": 0.85,\n  \"[b17]\": 0.7,\n  \"[b3]\": 0.6,\n  \"[b16]\": 0.6,\n  \"[b14]\": 0.6\n}\n\nThe key points that led me to identify these as the most relevant source papers are:\n\n1. The paper directly states it is building upon and extending the LAMA benchmark from [b18] to probe factual knowledge in pre-trained language models. This is the clearest indication of a foundational source paper.\n\n2. The probing performance metrics of accuracy and consistency are directly adopted from [b4], showing heavy reliance on that paper's methodology. \n\n3. The causal intervention analysis using a Structured Causal Model and do-calculus directly cites [b17], indicating the causal analysis framework is inspired by that pioneering work.\n\n4. The specific pre-trained language models analyzed (BERT, RoBERTa, ALBERT) were introduced in [b3], [b16] and [b14] respectively. While not as central as the first 3 references, the paper's experiments depend on these models.\n\n5. Other references like [b1], [b2], [b6], [b10] etc. provide useful background context but do not seem to be as directly linked to the paper's core novel contributions around causal analysis of factual knowledge capture.\n\nSo in summary, [b18] and [b4] emerge as the clearest conceptual and methodological foundations, [b17] directly inspires the causal intervention approach, and [b3], [b16], [b14] provide the key models analyzed. The other references appear more peripheral based on the information provided in this excerpt of the paper.", "6296d90e5aee126c0f730b92": "Based on analyzing the paper, here are the most likely source papers that directly inspired or motivated this work, in JSON format with confidence scores:\n\n{\n  \"[b20]\": 0.95,\n  \"[b1]\": 0.85, \n  \"[b22]\": 0.8,\n  \"[b51]\": 0.7,\n  \"[b5]\": 0.7,\n  \"[b14]\": 0.6,\n  \"[b52]\": 0.6,\n  \"[b28]\": 0.6\n}\n\nThe key reasons for selecting these papers as the most likely sources of inspiration are:\n\n1. The paper directly builds upon Hammond's graph wavelet theory from [b20] to develop their Beta Wavelet Graph Neural Network (BWGNN) architecture. This is the core novel contribution.\n\n2. [b1] is cited when discussing that existing adaptive GNN filters are not guaranteed to be band-pass and spectral-localized, which motivates the need for BWGNN. \n\n3. [b22], [b51], [b5] are grouped together as examples of existing GNNs with adaptive filters that cover nearly all frequency profiles but are not tailored to anomaly detection like BWGNN is.\n\n4. [b14], [b52], [b28] are cited as examples of widely used Heat kernels, in contrast with the novel Beta kernel proposed in this paper to better address higher frequency anomalies.\n\nThe other referenced papers provide useful background and motivation but do not seem to be as directly influential on the main novel contributions as the ones selected above with high confidence scores. The paper's key insight about the \"right-shift\" phenomenon of anomalies in the graph spectrum seems to be original in this work.", "62997c0b5aee126c0f77cecc": "Here are the key steps I took to identify the most relevant source papers for this paper:\n\n1. Determined the main novel contributions:\n- Introducing Uncertainty and Representativeness Mixing (UnReMix) for contrastive training, which combines importance scores capturing model uncertainty, representativeness, and anchor similarity when selecting negative examples\n- Considering representativeness for hard negative sampling in contrastive learning in a computationally efficient way\n\n2. Searched for key phrases indicating strong reliance on previous work, but did not find any that directly point to the core contributions. \n\n3. Identified references linked to the main methods:\n- Gradient-based uncertainty metrics from semi-supervised and active learning [30, 1]\n- Feature similarity importance scores from HCL [42]\n- Representativeness based on average distance in embedding space, simplifying clustering approaches [33]\n\n4. Evaluated titles to infer relevance and removed background references.\n\n5. Re-evaluated considering methodological similarities:\n- Importance scores and sampling like HCL [42], but HCL only uses similarity \n- Uncertainty-based sampling like CM-ACC [36] but for unimodal data\n- Representativeness conceptually similar to clustering negatives [33]\n\nIn summary, based on the novel contributions and methodological similarities, I believe the most relevant source papers are:\n\n{\n  \"1\": 0.8,\n  \"30\": 0.7, \n  \"33\": 0.6,\n  \"36\": 0.7,\n  \"42\": 0.9\n}\n\nThe numbers represent the reference numbers and the scores represent my confidence level that each paper was a key influence, based on the reasoning above. Let me know if you need any clarification or have additional questions!", "622183525aee126c0f23c770": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria provided:\n\nThe key novel contributions of this paper are:\n1. Proposing a new contrastive regularization function that does not suffer from adverse effects when learning from noisy labels\n2. Theoretically analyzing the representations induced by the contrastive regularization, showing they keep information related to true labels and discard information related to corrupted labels\n3. Proposing a novel algorithm to learn contrastive representations from noisy labeled data\n\nThe key references that directly inspired or motivated these contributions, based on the language used, are:\n\n{\n  \"[b6]\": 0.9,\n  \"[b7]\": 0.9, \n  \"[b8]\": 0.9,\n  \"[b15]\": 0.8,\n  \"[b19]\": 0.8\n}\n\nThese references [b6], [b7], [b8], [b15], [b19] are cited when discussing the \"power of contrastive representation learning methods\" that have been \"extensively applied on various tasks\". This indicates the proposed contrastive regularization approach is heavily inspired by these previous works on contrastive learning.\n\nAdditionally, references [b9], [b13], [b23], [b24] are mentioned when discussing previous attempts to learn contrastive representations from noisy data by re-defining contrastive pairs. While the proposed approach aims to improve upon the limitations of these methods, they still provide important context and motivation for the paper's contributions.\n\nIn summary, while several references are cited throughout, the key source papers that most directly inspired the novel contributions of this paper are [b6], [b7], [b8], [b15] and [b19], based on the language used when citing them. The proposed contrastive regularization approach and theoretical analysis build upon the foundations of these previous works in contrastive representation learning.", "628d9e795aee126c0f9791fd": "Here is my analysis of the key source papers for this paper, based on the given criteria:\n\n{\n  \"[b16]\": 0.9,\n  \"[b45]\": 0.8,\n  \"[b46]\": 0.8,\n  \"[b47]\": 0.7,\n  \"[b52]\": 0.7\n}\n\nThe key novel contributions of this paper are:\n1) Developing an adversarial knowledge distillation framework (GraphAKD) for graph neural networks, using a trainable discriminator instead of fixed distance functions to measure discrepancy between teacher and student models. This allows adaptively detecting differences on various graph domains.\n2) The discriminator considers both node representations and logits to transfer inter-node and inter-class correlations from the teacher to the student. \n3) The discriminator is topology-aware by criticizing local affinity of connected nodes and global affinity of patch-summary pairs.\n\nThe papers that most directly inspired and motivated these novel aspects, based on the text, are:\n- [b16] on generative adversarial networks (GANs), which is directly referenced as modifying the teacher-student architecture into a GAN framework with the student as generator and a discriminator. This appears to be the core novel idea the GraphAKD framework builds upon.\n- [b45], [b46], [b47] which point out limitations of forcing the student to exactly mimic the teacher with fixed distance functions, and how the discriminator approach allows the student to capture correlations instead. These directly motivate the adversarial discriminator approach.\n- [b52] which uses a residual MLP as a visual representation discriminator, directly motivating the residual MLP logit identifier discriminator component.\n\nIn summary, while the paper references many prior works, the key source papers that most centrally inspired the main novel contributions appear to be [b16] for the core adversarial distillation idea, [b45-47] for the motivation to use a discriminator over fixed distances, and [b52] for the logit discriminator architecture. The text directly highlights these as inspiration.", "6287492a5aee126c0ffe8231": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"b23\": 0.9,\n  \"b31\": 0.8,\n  \"b34\": 0.7,\n  \"b14\": 0.6,\n  \"b2\": 0.4,\n  \"b1\": 0.4,\n  \"b30\": 0.3\n}\n\nRationale:\nThe paper proposes a pre-training and fine-tuning approach called PT-NTM for neural topic modeling. The key novel contributions are:\n\n1. Directly pre-training the neural topic model itself on a large corpus, rather than just using pre-trained word embeddings or language models. This allows the model to learn topic-related knowledge.\n\n2. Fine-tuning the pre-trained model on a target dataset with some re-initialization. This adapts the model to the new data while retaining the pre-trained knowledge.\n\n3. Showing that this pre-training and fine-tuning approach significantly improves topic coherence and diversity compared to training from scratch, while adding little computational overhead. \n\n4. Demonstrating the pre-trained models can achieve strong performance with much less training data.\n\nThe methodology has very close ties to W-LDA [b23], which the proposed model loss follows. ProdLDA [b31] is also highly relevant as a VAE-based neural topic model.\n\nThe paper frequently compares to models that incorporate pre-trained word embeddings like BAT [b34], G-BAT [b34] and external knowledge like ZeroShotTM [b2], CombinedTM [b1]. However, the key distinction is that PT-NTM pre-trains the topic model itself rather than external components.\n\nFinally, the idea of pre-training topic models on large corpora and transferring to new data is compared to TopicOcean [b30], though the methodologies differ.\n\nIn summary, [b23], [b31] and [b34] emerge as the most influential prior works, with [b14], [b2], [b1] and [b30] also providing important context and comparisons for the novel contributions of this paper. The confidence scores reflect the centrality of each reference to the core PT-NTM methodology introduced.", "62982a9a5aee126c0f6f5f99": "Here is my analysis of the key source papers for this paper, based on the criteria you provided:\n\n{\n  \"[b7]\": 0.9,\n  \"[b46]\": 0.8,\n  \"[b47]\": 0.8,\n  \"[b27]\": 0.7,\n  \"[b32]\": 0.6\n}\n\nConfidence scores:\n- [b7] (0.9): The paper takes direct inspiration from the vision transformer (ViT) architecture proposed in this paper. It uses ViT as a key comparison and builds upon the idea of treating the image as a sequence of patches.\n\n- [b46], [b47] (0.8): These papers on MLP-based vision models are highlighted as recent progress that the current work builds upon, alongside CNNs and vision transformers. The graph-based approach is positioned as an alternative to MLP and self-attention.\n\n- [b27] (0.7): The max-relative graph convolution proposed in this paper is directly adopted as a key component in the Grapher module at the core of the ViG architecture. \n\n- [b32] (0.6): Several architectural choices are inspired by the Swin Transformer, such as the pyramid structure and relative positional encoding in the pyramid version of ViG.\n\nThe key novel contributions of this paper are: 1) Representing the image as a graph of patches linked by content rather than just a grid or sequence, 2) Introducing the ViG block with Grapher and FFN modules to process the graph representation while maintaining feature diversity. The references most directly enabling these contributions are prioritized in the list above.", "626f3dd05aee126c0f8f76a7": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b2]\": 0.9,\n  \"[b11]\": 0.8,\n  \"[b29]\": 0.8,\n  \"[b5]\": 0.7,\n  \"[b25]\": 0.6\n}\n\nThe key reasons for selecting these papers as the most pertinent sources are:\n\n1. The paper makes several direct references to being inspired by or building upon the methods in CogView [[b2]], MAE [[b11]], and SimMIM [[b29]]. For example:\n- \"DALL-E [b23] , CogView [b2] and similar works [b28] [b7] leverage VQ-VAE [b25] to compress an image to a sequence of discrete tokens and pretrain large transformers for auto-regressive generation, greatly advancing the task in general domain.\"\n- \"The analysis from SimMIM [b29] exhibits the great importance of the mask percentage and patch distribution. We follow their results to sample 4 \u00d7 4 token patches at random until 75% of the tokens are in the mask regions.\"\n\n2. The novel Cross-modal general Language Model (CogLM) proposed in the paper is directly compared to and positioned as an improvement over GLM [[b5]] and MAE [[b11]]:\n- \"The main advantage over GLM is simplicity. To unify the generation and bidirectional understanding, GLM needs to define many new special tokens and a new type of position embedding...\"\n- \"MAE is designed for self-supervised learning on pure image data and not ready for generation. Even without text, CogLM is more parameter-efficient because MAE is an encoder-decoder structure.\"\n\n3. The image tokenizer used is based on the first-stage VQ-VAE approach from CogView [[b2]] which in turn leverages the VQ-VAE method [[b25]].\n\nSo in summary, the core novel contributions of CogLM and the hierarchical generation approach draw very heavily from and aim to improve upon the methods in CogView, MAE, SimMIM and GLM. The VQ-VAE tokenization is also a key enabler adopted from prior work.", "628c6264f66cd1000c54e4e9": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria provided:\n\n{\n  \"[b27]\": 0.9,\n  \"[b28]\": 0.9,\n  \"[b15]\": 0.8,\n  \"[b40]\": 0.8,\n  \"[b51]\": 0.7,\n  \"[b14]\": 0.7,\n  \"[b26]\": 0.6\n}\n\nThe key novel contributions of this paper seem to be:\n\n1. Using large pretrained language models like T5 and BERT to generate powerful text embeddings for text-to-image synthesis, rather than training text encoders from scratch or only on image-text data. This is directly inspired by works [b51] and [b14].\n\n2. Introducing classifier-free guidance with dynamic thresholding to enable high-quality image generation with large guidance weights. This builds upon the classifier-free guidance technique proposed in [b26]. \n\n3. Using a robust cascaded diffusion model architecture with noise conditioning augmentation for generating high-resolution images. This is heavily based on the diffusion model architectures and techniques introduced in [b27], [b28] and [b15].\n\n4. Comprehensive evaluation of text-to-image models using the newly proposed DrawBench dataset. Previous key works evaluated on simpler datasets like COCO ([b40]).\n\nSo in summary, the core novel elements - using frozen pretrained language models, improved classifier-free guidance, and cascaded diffusion models - are directly motivated by and build upon the key references [b27], [b28], [b15], [b40], [b51], [b14] and [b26]. The paper uses similar diffusion architectures and sampling techniques as these previous works, while introducing some important improvements. The confidence scores reflect the centrality of each reference to the main contributions.", "629c4e2a5aee126c0f6f8d0b": "Here is my analysis of the most pertinent source papers for the given paper, in JSON format with reference numbers as keys and confidence scores between 0 and 1 as values:\n\n{\n  \"[b0]\": 0.9,\n  \"[b7]\": 0.8,\n  \"[b8]\": 0.7,\n  \"[b10]\": 0.6,\n  \"[b11]\": 0.6,\n  \"[b14]\": 0.5,\n  \"[b15]\": 0.4\n}\n\nThe key points that led me to identify these as the most relevant source papers are:\n\n1. The paper directly states it began from a previously described message passing neural network (MPNN) architecture from reference [b0]. This is a clear indication that [b0] was a key inspiration and starting point.\n\n2. To enable broad applicability to various design problems, the paper replaced the fixed N to C terminal decoding order with an order agnostic autoregressive model, citing [b7] which describes this approach. \n\n3. The paper compares its designed sequences to those generated by AlphaFold [b8], indicating AlphaFold was an important benchmark.\n\n4. For the experimental validation, the paper tried to rescue previous failed designs made with Rosetta. References [b10] and [b11] describe key components of the Rosetta design protocol that this work aimed to improve upon.\n\n5. Reference [b14] is cited when discussing designing sequences for target protein nanoparticle assemblies that were originally designed using a compute-intensive Rosetta protocol, which this paper streamlines.\n\n6. Finally, the nanoparticle designs are noted to have applications in areas like structure-based vaccine design, citing [b15].\n\nSo in summary, [b0] provided the foundational architecture, [b7] and [b8] inspired key methodological advances, and [b10], [b11], [b14] and [b15] represent previous design approaches and applications that this paper aimed to improve upon with its new method. The scores reflect my confidence in the centrality of each reference to the main contributions of this paper based on the textual evidence.", "62aa9fb55aee126c0fa5cbb7": "Based on analyzing the paper, here are the key source papers that likely inspired or motivated this work, in order of relevance:\n\n{\n  \"[b18]\": 0.9,\n  \"[b6]\": 0.8,\n  \"[b2]\": 0.7,\n  \"[b19]\": 0.6,\n  \"[b25]\": 0.5\n}\n\nRationale:\n- [b18] on Graph Network Simulators (GNS) is cited multiple times and the paper states their work directly builds upon GNS. The hybrid graph network architecture and sector-based training to scale to larger graphs seem directly inspired by [b18].\n- [b6] introducing 3D U-Net is a key component of their hybrid architecture to model global pressure dynamics. \n- [b2] on interaction networks is cited as related work that a concurrent paper used to model subsurface dynamics, indicating its relevance.\n- [b19] using recurrent R-U-Net and [b25] using CNNs for subsurface modeling are discussed as prior insufficient approaches that motivated the need for their graph-based hybrid method.\n\nThe paper cites these works in the context of their core contributions - the hybrid graph network architecture, ability to scale to large grids, and improved accuracy over CNN and U-Net baselines. The ordering reflects the centrality of each reference to the main novel aspects introduced in this paper.", "6260bd7f5aee126c0fc6bbc3": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b46]\": 0.9,\n  \"[b22]\": 0.8,\n  \"[b33]\": 0.6,\n  \"[b7]\": 0.6,\n  \"[b25]\": 0.4,\n  \"[b40]\": 0.4,\n  \"[b13]\": 0.4\n}\n\nRationale:\n- The paper directly states that \"[b46]\" inspired the idea that individuals learn morals from stories and represent their moral experiences in narrative form. This is a key motivation for the paper's focus on bridging abstract morals and concrete story events. Confidence score: 0.9\n- The concept of \"value preferences\" and aligning the value preferences between stories and morals seems to draw inspiration from \"[b22]\". Confidence score: 0.8  \n- \"[b33]\" and \"[b7]\" are mentioned as examples of existing story understanding and generation tasks that the proposed tasks differ from. While not direct inspirations, they provide important context and motivation. Confidence score: 0.6\n- \"[b25]\", \"[b40]\" and \"[b13]\" are cited when discussing related work on story generation. They share some methodological similarities with this paper in terms of coherent story generation and imposing attributes like outlines. Confidence score: 0.4\n\nI limited the selection to the top 7 most relevant references based on the criteria of direct inspiration, methodological similarity, and providing important context for the novel contributions of associating stories with morals and capturing discourse relations. Let me know if you need any clarification or have additional questions!", "628304515aee126c0f6f0fed": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"b14\": 0.9,\n  \"b37\": 0.8,\n  \"b9\": 0.7,\n  \"b13\": 0.6,\n  \"b30\": 0.5\n}\n\nThe key points are:\n\n- The paper aims to address the multimodality problem in non-autoregressive machine translation (NAT) without relying on knowledge distillation. This is directly inspired by and builds upon the work in [b14] which first proposed NAT models but noted they suffer from the multimodality issue. \n\n- The directed acyclic decoder structure allowing the model to capture multiple translation modalities is conceptually similar to the glancing training approach in [b37] and uniform masking in [b9]. The paper directly compares to these.\n\n- The paper positions itself as a non-iterative NAT model, and compares performance to strong baselines like the CTC model [b30] and the iterative NAT in [b13].\n\n- Specific techniques like using an n-gram LM in beam search decoding also draw from [b13].\n\nSo in summary, [b14] is the most influential prior work this paper directly builds upon to tackle the core challenge, while [b37], [b9], [b13] and [b30] represent other key recent NAT approaches that this work compares to and incorporates some techniques from. The high confidence scores reflect that these are very directly relevant to the main contributions.", "62c28ae55aee126c0f8a1954": "Based on my analysis of the paper, the most pertinent source papers that inspired this work are:\n\n{\n  \"[b24]\": 0.9,\n  \"[b90]\": 0.8,\n  \"[b81]\": 0.7,\n  \"[b42]\": 0.6,\n  \"[b59]\": 0.5\n}\n\nThe key reasons are:\n\n1. The paper cites [b24] multiple times when discussing counterfactual inference and estimating treatment effects from observational data. This indicates [b24] provided important foundations for the counterfactual link prediction approach.\n\n2. [b90] and [b81] are cited as key prior works on GNN-based link prediction that the proposed CFLP method builds upon and compares against. The paper aims to improve link prediction over these baselines by leveraging causal inference.\n\n3. [b42] is cited when introducing key concepts of counterfactual causal inference, suggesting it provided important background for the causal modeling aspects of the paper. \n\n4. [b59] is referenced in the context of estimating individual treatment effects (ITE) and average treatment effects (ATE), metrics used extensively in the paper's methodology.\n\nThe paper directly states these works \"inspired\" or \"motivated\" the CFLP approach by providing foundations in GNN-based link prediction and causal inference that enabled combining the two. The frequent citations in key sections further confirm their centrality. While other references provided important context, these 5 appear most pivotal to the novel contributions based on the language used.", "620f0e735aee126c0fec46ff": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b7]\": 0.9,\n  \"[b22]\": 0.8,\n  \"[b24]\": 0.7,\n  \"[b6]\": 0.6,\n  \"[Dwivedi and Bresson, 2020]\": 0.6\n}\n\nConfidence scores:\n- [b7]: 0.9 - This reference is mentioned multiple times in the context of the limitations of current GNNs that the paper aims to address, such as the over-smoothing and over-squashing problems. The paper's novel contributions are directly motivated by overcoming these issues with GNNs.\n\n- [b22]: 0.8 - The Graphormer model proposed in this reference is discussed in detail in the \"Improved Attention Matrices from Graphs\" section as a key method that adds soft graph bias terms to the attention scores. One of the main novel aspects of the surveyed paper is categorizing and evaluating such graph-structure aware attention mechanisms.\n\n- [b24]: 0.7 - The proximity-enhanced multi-head attention (PE-MHA) mechanism from this Gophormer reference is another graph-informed attention method that is a core focus of the paper's contributions. \n\n- [b6]: 0.6 - This GraphTrans reference first proposes adding Transformer layers on top of GNN layers, which is identified as one of the 3 key architectures for combining GNNs and Transformers that the paper studies.\n\n- [Dwivedi and Bresson, 2020]: 0.6 - The attention masking and Laplacian eigenvector based positional embeddings proposed in this reference are discussed as key techniques in 2 of the 3 main categories the paper uses to group existing Graph Transformer models.\n\nIn summary, these 5 references appear to be the most influential in directly motivating the key novel contributions of the paper, which are 1) Categorizing architectures for combining GNNs and Transformers 2) Studying techniques for graph-aware positional embeddings and 3) Analyzing methods to inject graph structure into the attention computation. The paper performs an empirical study comparing these different approaches, building off the ideas and methods from these core references.", "628603bc970707000cbf14a8": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b30]\": 0.95,\n  \"[b17]\": 0.9,\n  \"[b14]\": 0.8,\n  \"[b67]\": 0.7,\n  \"[b2]\": 0.6\n}\n\nThe key points are:\n\n- The paper is heavily inspired by and builds upon the Masked Autoencoder (MAE) method from [b30]. It extends MAE from images to videos with minimal changes. The high confidence score reflects the paper directly stating \"Our method is simply MAE applied to the set of spacetime patches.\"\n\n- The paper uses vanilla Vision Transformer (ViT) architectures from [b17] for both the encoder and decoder. It highlights following the spirit of [b17] in learning strong representations with minimal inductive biases.\n\n- The masking approach is compared to BERT [b14] which pioneered masked language modeling. The optimal masking ratio is related to the information redundancy hypothesis proposed in [b30] and [b14].\n\n- The overall masked autoencoding methodology stems from classical work on denoising autoencoders [b67]. \n\n- While less central, [b2] is referenced a few times in regards to using a block-wise masking strategy in spacetime as an alternative sampling approach.\n\nThe scores prioritize the references that are most directly relied upon for the key novel contributions of extending MAE to videos with minimal domain-specific assumptions. Foundational work on denoising autoencoders and alternative masking strategies receive lower scores as they are less central to the main ideas.", "628ef0495aee126c0f82d966": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\nThe paper introduces a general, powerful, scalable (GPS) framework for building graph Transformers. The key novel contributions seem to be:\n\n1. Organizing positional encodings (PE) and structural encodings (SE) into local, global and relative categories to better integrate them into the GT architecture. \n\n2. Showing that GPS with linear global attention (e.g. using Performer or BigBird) can scale to much larger graphs than existing fully-connected GTs, even without explicit edge features in the attention.\n\n3. Extensive ablation studies evaluating the contributions of the PE, local MPNN, and global attention components.\n\n4. Implementing the GPS blueprint in a modular GRAPHGPS package.\n\nThe key references that seem to most directly inspire or motivate these contributions are:\n\n{\n  \"[b10]\": 0.9,\n  \"[b61]\": 0.9,\n  \"[b60]\": 0.8,\n  \"[b33]\": 0.7,\n  \"[b58]\": 0.7,\n  \"[b12]\": 0.6\n}\n\n[b10] and [b61] introduce the Performer and BigBird linear attention mechanisms that are adapted here to enable scaling GTs to much larger graphs.\n\n[b60] introduces the GraphGym framework that GRAPHGPS builds upon.\n\n[b33], [b58] and [b12] are important prior works on fully-connected GTs that use positional encodings and explicit edge features in the attention, which this work improves upon in terms of scalability by using linear attention and no explicit edge features.\n\nThe paper organizes and evaluates multiple previously proposed PE and SE schemes, but the core novel contributions are more focused on the overall GPS framework and architecture rather than the specific PE/SE schemes. The extensive ablation studies help quantify the contributions of the different components.\n\nIn summary, while the paper builds upon many prior works, the key source papers that seem to most directly inspire the main novel contributions are those introducing the linear attention mechanisms and modular GT implementation framework that enable the scalability improvements central to this work. The confidence scores reflect the centrality of each reference to the core contributions.", "628d9e795aee126c0f979247": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\nThe key novel contributions of this paper appear to be:\n\n1. Building multi-modal product embeddings that combine both image and text features, in contrast to prior work that typically only used a single modality. The transformer-based architecture for combining multi-modal features is a key aspect.\n\n2. Enabling the product embeddings to provide relevant recommendations across different surfaces (Home, Closeup, Search) in response to queries from different modalities (sequence of pins, single pin, text query). Making the embeddings compatible with learned representations for pins and search queries is important for this.\n\n3. Optimizing the embeddings for multiple engagement types (purchases, add-to-cart, saves, clicks) in a multi-task learning setup. This allows optimizing both the candidate generation and ranking phases for multiple objectives.\n\nBased on this, the most relevant references that directly inspired or enabled these contributions appear to be:\n\n{\n  \"[b32]\": 0.95,\n  \"[b3]\": 0.8,\n  \"[b20]\": 0.7,\n  \"[b0]\": 0.65,\n  \"[b33]\": 0.6\n}\n\n[b32] and [b3] are highlighted as key references for extending embeddings to combine multi-modal features, which is a core aspect of this work. [b20], [b0] and [b33] are relevant for the multi-task learning approach to optimize embeddings for multiple engagement types with a single representation.\n\nThe confidence scores reflect the centrality of each reference to the main contributions, with [b32] being the most crucial in laying the groundwork for the multi-modal architecture. The other references, while relevant, are not as strongly tied to the core novel aspects in my assessment.\n\nLet me know if you would like me to explain or modify the analysis in any way. I tried to focus on identifying the references most directly enabling the key technical innovations introduced in this paper.", "62bd48b80cd9e8000cfc9dc5": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b12]\": 0.9,\n  \"[b44]\": 0.85,\n  \"[b29]\": 0.8,\n  \"[b54]\": 0.75,\n  \"[b28]\": 0.7,\n  \"[b46]\": 0.65,\n  \"[b43]\": 0.6\n}\n\nConfidence scores:\n- [b12] (Vision Transformer, ViT): 0.9. The paper heavily relies on ViT as the foundation, with key phrases like \"Since the emergence of landmark works such as Vision Transformer (ViT) [b12]\" indicating direct inspiration.\n\n- [b44] (DeiT): 0.85. DeiT is referenced as building upon ViT and introducing teacher-student distillation, which the current paper likely utilizes. \n\n- [b29] (Swin Transformer): 0.8. Swin is highlighted for successfully applying transformers to detection and segmentation using a multi-stage strategy, which directly influences the macro search space design here.\n\n- [b54] (MetaFormer): 0.75. MetaFormer's use of average pooling in place of self-attention directly motivates a key question - \"how to design a search space to search high-performance Transformer-like architectures\".\n\n- [b28] (DARTS): 0.7. The micro search space and overall NAS approach draws heavily from DARTS.\n\n- [b46] (PVT): 0.65. Along with Swin, PVT's multi-stage design is referenced as inspiration for the macro search space. \n\n- [b43] (ResMLP): 0.6. The Spatial MLP operation in the micro search space comes from ResMLP.\n\nThe paper introduces a novel micro-meso-macro search space to search efficient transformer architectures. The micro and macro spaces borrow heavily from previous works like ViT, DARTS and Swin. The key novelty is in the meso space with the hamburger block structure. Overall, ViT, DeiT, Swin and MetaFormer emerge as the most influential source papers.", "62bd48b2cb97d2000c50c6a6": "Based on analyzing the text, here are the key source papers that likely inspired or motivated this work, in JSON format with confidence scores:\n\n{\n  \"[b29]\": 0.9,\n  \"[b39]\": 0.9,\n  \"[b47]\": 0.9,\n  \"[b28]\": 0.8,\n  \"[b37]\": 0.7,\n  \"[b43]\": 0.7\n}\n\nThe key pieces of evidence are:\n\n1. The paper heavily leverages the attention mechanism to guide the neural architecture search, taking inspiration from works like [b29], [b39], [b47] which showed that channels with smaller attention weights can be pruned with little impact on performance. This is the core novel idea of the paper.\n\n2. It aims to overcome limitations of DARTS [b28], namely the bi-level optimization and using architecture parameters to indicate operation importance. \n\n3. It cites [b37] and [b43] to point out that architecture parameters in DARTS may not accurately reflect operation strength, further motivating their attention-based approach.\n\n4. The overall methodology combines attention weights with both micro and macro search spaces that build upon prior NAS approaches, but the attention mechanism is the key novel aspect that enables their improvements.\n\nSo in summary, pruning techniques using attention weights were the biggest inspiration, while overcoming issues in DARTS was the key motivation. The attention-based methodology is the central contribution that ties together and improves upon prior micro and macro NAS approaches.", "6253cbd75aee126c0f191b95": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b30]\": 0.95,\n  \"[b28]\": 0.9,\n  \"[b24]\": 0.85,\n  \"[b67]\": 0.8,\n  \"[b29]\": 0.7\n}\n\nRationale:\n\nThe paper heavily relies on the AlphaFold2 model from [b30] to predict structures at scale for training their inverse folding models. This is a key novel aspect that allows them to overcome data limitations.\n\nThe paper builds upon the GVP-GNN architecture from [b28], which was the previous state-of-the-art for inverse folding. They scale up this architecture and also propose a new hybrid GVP-Transformer model inspired by it.\n\nThe sequence-to-sequence approach for inverse folding that treats it as a conditional sequence generation problem was pioneered by [b24]. This paper directly builds on that formulation.\n\nThe GVP-Transformer model incorporates a generic Transformer encoder-decoder architecture [b67] in combination with the geometric GVP-GNN layers. The Transformer is a core architectural component.\n\nFor training, the paper employs span masking, noting that it improves performance over independent random masking, citing [b29] which introduced span masking in NLP.\n\nThe confidence scores reflect the centrality of each reference to the main contributions and methods of the paper, with AlphaFold2 [b30] being the most crucial in enabling the key novel aspects, followed by the core architectural foundations drawn from [b28], [b24] and [b67]. The span masking training approach from [b29] is given a lower but still significant score as it directly informs their methodology.", "620c6b655aee126c0fe29013": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b44]\": 0.9,\n  \"[b32]\": 0.85,\n  \"[b26]\": 0.7,\n  \"[b42]\": 0.65,\n  \"[b35]\": 0.6\n}\n\nThe key points that led me to select these papers as the most relevant sources are:\n\n1. The paper directly states that it is inspired by Mixup [b44] and its variants like Manifold Mixup [b32] for data augmentation. It aims to extend the Mixup idea to graph data. This indicates [b44] and [b32] are the core inspirations.\n\n2. The paper discusses existing graph data augmentation strategies like DropEdge [b26] and Subgraph [b42][b35]. While it points out their limitations, these papers represent the key prior work the proposed method builds upon to enable graph data augmentation. \n\n3. The novel contributions of this paper are enabling Mixup for graph data by mixing up graphons to generate synthetic graphs. The key insight is using graphons as graph generators to make graphs regular, aligned and Euclidean to enable Mixup.\n\n4. Methodologically, the paper focuses on linearly interpolating graphons similar to how Mixup interpolates images. The theoretical analysis also has similarities to prior Mixup theory.\n\nSo in summary, [b44] and [b32] are the clearest sources of inspiration, while [b26], [b42], [b35] represent the key prior work on graph augmentation that this paper improves upon with its novel graphon mixup method. The confidence scores reflect the centrality of each paper to the core ideas.", "62d8c4565aee126c0f762dba": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b0]\": 0.9,\n  \"[b5]\": 0.8,\n  \"[b27]\": 0.7,\n  \"[b1]\": 0.6,\n  \"[b30]\": 0.6\n}\n\nRationale:\nThe paper introduces NUWA-Infinity, a model for infinite visual synthesis that can generate arbitrarily-sized high-quality images and videos. The key novel contributions are:\n\n1. An autoregressive over autoregressive generation mechanism to model dependencies between patches ([b0], [b5]) and within patches. This enables variable-size image/video generation.\n\n2. A Nearby Context Pool (NCP) to cache related patches as context for the current patch, saving computation without sacrificing patch-level dependency modeling. \n\n3. An Arbitrary Direction Controller (ADC) to decide suitable generation orders and learn order-aware position embeddings, useful for tasks like image outpainting.\n\nThe most relevant references based on the novel aspects are [b0] and [b5], which also use divide-and-conquer strategies and autoregressive models over patches. The paper improves on their limitations in modeling patch dependencies.\n\n[b27] (NUWA) is highly relevant as the direct predecessor that also supports both image and video synthesis. NUWA-Infinity significantly improves the generation quality over NUWA.\n\n[b1] and [b30] are relevant for the Mask-Predict approach that incorporates sliding windows and mask-and-predict for modeling patch dependencies, which NUWA-Infinity aims to improve upon.\n\nThe other references, while providing useful context, do not seem as centrally tied to the main novel contributions of NUWA-Infinity based on the details provided in the text. The confidence scores reflect the degree of relevance to the core aspects of the proposed NUWA-Infinity model.", "622eb2495aee126c0f62b12a": "Here are the key steps I took to identify the most pertinent source papers for this paper:\n\n1. The novel contributions of this paper are:\n- Introducing BERTopic, a topic model that leverages clustering techniques and a class-based variation of TF-IDF to generate coherent topic representations\n- Using UMAP to reduce dimensionality of document embeddings before clustering with HDBSCAN \n- Developing a class-based version of TF-IDF to extract topic representations from document clusters\n- Extending BERTopic for dynamic topic modeling by creating global and local topic representations over time\n\n2. Key methodological similarities and inspirations:\n- Uses document embeddings and clustering approach similar to [b37] and Top2Vec [b2]\n- Builds on the clustering embeddings approach from [b37] and extends it with class-based TF-IDF \n- Uses UMAP for dimensionality reduction, motivated by [b27] showing it preserves local and global structure\n- Uses HDBSCAN for clustering, an extension of DBSCAN from [b27]\n- Develops dynamic topic modeling as an extension, inspired by the original dynamic topic model by Blei and Lafferty (2006)\n\n3. The most pertinent source papers based on the above analysis:\n\n{\n  \"[b37]\": 0.95,\n  \"[b2]\": 0.9,\n  \"[b27]\": 0.85,\n  \"Blei and Lafferty (2006)\": 0.8\n}\n\nThe paper heavily relies on [b37] and [b2] (Top2Vec) for the core idea of clustering document embeddings to generate topics. [b27] provides the key algorithms used - UMAP for dimensionality reduction and HDBSCAN for clustering. The dynamic topic modeling extension is directly inspired by the seminal work of Blei and Lafferty (2006).", "628c4ce15aee126c0ff597ca": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\n{\n  \"[b25]\": 0.9,\n  \"[b6]\": 0.8,\n  \"[b27]\": 0.7,\n  \"[b23]\": 0.6\n}\n\nConfidence scores:\n[b25]: 0.9 - The paper heavily relies on the transformer architecture from this reference for its HLATR model, using it as the encoder. The position embeddings and self-attention mechanism are directly inspired by [b25].\n\n[b6]: 0.8 - [b6] is referenced multiple times in relation to using a contrastive loss for list-aware ranking in both the retrieval and reranking stages. The proposed methodology seems to build upon ideas from this prior work.\n\n[b27]: 0.7 - The joint optimization of retrieval and reranking stages using adversarial training in [b27] serves as important context and motivation for this paper's approach, though the methodology differs. \n\n[b23]: 0.6 - The knowledge distillation approach in [b23] for jointly optimizing retrieval and reranking is discussed as relevant prior work, indicating it influenced the authors, even if the technical details diverge.\n\nThe key novel contributions of this paper appear to be 1) Coupling retrieval rank order features with reranker output representations, and 2) Using a lightweight transformer model to learn from this fused representation for refined reranking. The most directly relevant references based on methodological similarity are [b25] for the transformer architecture and [b6] for contrastive learning ideas. [b27] and [b23] provide important context as prior work on joint retrieval/reranking optimization that inspired this paper's focus.", "6292aa4a8c0a46000c95808f": "Based on analyzing the paper, here are the most likely key source papers that inspired this work, along with confidence scores:\n\n{\n  \"[45]\": 0.9,\n  \"[52]\": 0.95,\n  \"[56]\": 0.8,\n  \"[57]\": 0.85,\n  \"[61]\": 0.7,\n  \"[62]\": 0.7,\n  \"[63]\": 0.7\n}\n\nThe key evidence for these selections:\n\n- The paper uses PWC-Net [52] as the foundation of their method, making heavy use of its pyramid, warping, cost volume architecture. This is the clearest and most direct inspiration.\n\n- To improve on PWC-Net for large displacements, they design a self-attention based multiscale feature learning module inspired by recent works [61-63] showing the effectiveness of attention mechanisms for capturing long-range dependencies. \n\n- For handling occlusions, they take inspiration from ContinualFlow [56] and IRR-PWC [57] which integrated occlusion estimation into the PWC-Net model. However, they propose an unsupervised occlusion prediction approach instead.\n\n- The LiteFlowNet architecture [45] is mentioned as an approach that aimed to balance accuracy and model size, which seems to be a general goal of this paper as well in building on top of PWC-Net.\n\nThe confidence scores are highest for [52], [57] and [45] since the reliance on those works is most directly stated. The attention mechanism papers [61-63] have slightly lower scores since the inspiration there is more indirect. Overall, this paper combines ideas from several key previous works in a novel way to improve optical flow estimation accuracy and robustness, especially for challenging cases with large displacements and occlusions.", "62a013785aee126c0ff695ef": "Here is my analysis of the most pertinent source papers for this paper, based on the criteria you provided:\n\n{\n  \"9\": 0.9,\n  \"21\": 0.8,\n  \"22\": 0.8,\n  \"24\": 0.95,\n  \"43\": 0.95,\n  \"61\": 0.85,\n  \"62\": 0.85\n}\n\nConfidence scores:\n- [9]: 0.9 - This paper takes direct inspiration from the masked language modeling approach used in BERT in the NLP field for its generative pre-training strategy.\n- [21, 22]: 0.8 - The contrastive pre-training methods in this paper follow the pioneering contrastive learning works in the computer vision field. \n- [24]: 0.95 - The knowledge-guided pre-training strategy aims to address the issues with the ill-defined masking based pre-training tasks proposed in this prior work.\n- [43]: 0.95 - Like this prior work, the proposed method also uses a high-capacity transformer model and incorporates additional molecular knowledge in pre-training to improve performance.\n- [61, 62]: 0.85 - The contrastive pre-training baselines that this work compares against are heavily based on the graph contrastive learning frameworks proposed in these papers.\n\nIn summary, this paper draws very direct inspiration and motivation from references [9], [24] and [43] in terms of using masking for generative pre-training, addressing issues with naive masking, incorporating molecular knowledge, and using a high-capacity transformer architecture. The contrastive learning baselines are also closely related to the methods in [61] and [62]. Overall, these references form the key foundations that the novel contributions of this paper aim to build upon and improve.", "62e744545aee126c0f33c1ec": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b5]\": 0.9,\n  \"[b25]\": 0.8,\n  \"[b37]\": 0.7,\n  \"[b29]\": 0.6,\n  \"[b30]\": 0.5\n}\n\nRationale:\nThe paper introduces a Self-Supervised Hypergraph Transformer (SHT) model to enhance graph-based collaborative filtering. The key novel contributions seem to be:\n\n1) Integrating a hypergraph neural network with a topology-aware Transformer to capture global cross-user collaborative relations \n\n2) A generative self-supervised learning framework that distills auxiliary supervision signals through a graph topological denoising scheme\n\n3) A graph-based meta transformation layer to project hypergraph-based global representations into local interaction modeling\n\nThe references that appear most directly related to these core aspects are:\n\n[b5] (LightGCN) and [b25] (NGCF) - These pioneered using graph convolutional networks for collaborative filtering, which the proposed SHT model builds upon. The paper states \"LightGCN [b5] omits the burdensome non-linear transformer during the embedding propagation and improve the recommendation performance.\"\n\n[b37] (PinSage) - Also mentioned as a foundational GCN-based model - \"PinSage [b37] and NGCF [b25] attempt to aggregate neighboring information by capturing the graph-based CF signals for recommendation.\"\n\n[b29] - Cited when discussing the development of graph neural networks for modeling graph-structural data, which is a key component of the proposed approach.\n\n[b30] (SGL) - Highlighted as a recent self-supervised graph learning approach for recommendation that inspired aspects of the self-supervised learning in SHT. The paper states \"Inspired by the graph self-supervised learning, SGL [b30] produces state-of-the-art performance by generating contrastive views with randomly node and edge dropout operations.\"\n\nSo in summary, [b5], [b25], [b37] and [b29] establish key graph neural network foundations that SHT directly builds upon, while [b30] inspired the self-supervised learning aspects. These appear to be the source papers that most directly motivated and enabled the main novel contributions of SHT.", "623be1965aee126c0f37abe2": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b18]\": 0.9,\n  \"[b41]\": 0.95\n}\n\nThe key reasons are:\n\n1. The paper directly states \"Compared with previous IE studies which treat labels as specific symbols, the text-to-structure generation paradigm treats labels as natural language tokens. By verbalizing and generating labels and structures, our method can effectively transfer knowledge from pre-trained language models such as BART [b18] , T5 [b41] , and related tasks can easily share knowledge because their labels have similar semantics (e.g., location and place) and share common label-text associations (e.g., victim for different event types).\" This clearly indicates the proposed method is directly inspired by and builds upon the BART and T5 models.\n\n2. In the Pre-training section, the paper states \"We initialize UIEbase and UIE-large with T5-v1.1-base and T5-v1.1large [b41]\". This shows the proposed UIE model directly uses T5 as its initialization, indicating a very strong methodological reliance on the T5 paper.\n\n3. The novel contributions of this paper seem to be 1) modeling IE tasks as text-to-structure generation using the proposed SEL language and 2) the structural schema instructor (SSI) mechanism. The key enabling factor for these contributions is transfer learning from pre-trained language models like BART and T5. Without this, the text-to-structure generation paradigm would likely not be feasible.\n\nTherefore, based on direct statements about inspiration, shared methodologies, and reliance on the pre-trained models for the main contributions, I believe papers [b18] (BART) and [b41] (T5) are the most central and influential prior works for this paper, with [b41] being slightly more important as the UIE model directly builds off of T5. The confidence scores reflect this assessment.", "6281b2a35aee126c0ffd514a": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria provided:\n\nThe key novel contributions of this paper seem to be:\n1) Using a Vector Quantized (VQ) codebook as a facial details dictionary for face restoration, which can provide comprehensive low-level features for all facial areas and is learned end-to-end. This improves upon previous component dictionaries.\n2) A parallel decoder structure with a texture decoder that generates HQ facial details from the VQ codebook, and a main decoder that warps the texture features to match the fidelity of the degraded input. This allows generating realistic details while preserving fidelity.\n\nBased on these main contributions, the most relevant references appear to be:\n\n{\"22\": 0.9, \"30\": 0.8, \"10\": 0.8, \"32\": 0.7, \"23\": 0.6}\n\n[22] DFDNet - This paper uses a facial component dictionary for face restoration. The proposed VQ codebook improves upon the limitations of DFDNet's dictionary by covering all facial areas and being learned end-to-end.\n\n[30] VQ-VAE - Introduces the vector-quantized codebook technique that is a key component of the proposed method. \n\n[10] VQGAN - Trains the VQ codebook with adversarial objectives to achieve high perceptual quality. The proposed method builds upon this.\n\n[32] GFP-GAN - A recent face restoration method that the proposed approach is compared against. It uses a pre-trained StyleGAN as a decoder.\n\n[23] Reference-based face restoration - The parallel decoder shares similar ideas as reference-based methods, using the texture decoder output as reference.\n\nIn summary, the most central references that directly motivated the main novel aspects appear to be [22], [30] and [10]. The other references provide important context but the core ideas build most heavily upon the facial dictionary concept from [22] and the VQ codebook from [30] and [10].", "624802f36750f84d00082030": "Here is my analysis of the most relevant source papers for the given research paper, with confidence scores:\n\n{\n  \"[b6]\": 0.95,\n  \"[b12]\": 0.9,\n  \"[b11]\": 0.85,\n  \"[b21]\": 0.8,\n  \"[b13]\": 0.75\n}\n\nThe paper makes several key novel contributions:\n1. Conducting the first randomized control trial to rigorously measure the impact of different types of notifications on SAV adoption rates \n2. Testing social and reciprocity nudges in the notification messages\n3. Partnering with a national CERT (NIC.br) to have them deliver notifications directly\n4. Using misconfigured open DNS resolvers as vantage points to identify networks not implementing SAV, enabling a much larger study population than prior work\n\nBased on this, the most relevant prior work appears to be:\n\n[b6] and [b13] from the Spoofer project, which the authors directly compare their work against. The Spoofer project previously studied the impact of notifications but without a randomized design. This paper aims to definitively test if the Spoofer findings hold up in an RCT.\n\n[b11] and [b12] which previously used the same open resolver method as this paper to identify networks without SAV. This technique enables the larger scale of this study compared to [b6].\n\n[b21] also previously sent notifications to network operators about open resolvers and measured remediation rates, making it relevant prior work even though it was not focused on SAV specifically.\n\nThe scores represent my confidence that each paper served as important inspiration and motivation for the key novel aspects of this paper's study design. The Spoofer papers rate highest since this work directly builds on and aims to validate their notification findings.", "63034ea190e50fcafd73e0b0": "Based on analyzing the text, here are the key source papers that likely inspired this work, along with confidence scores:\n\n{\n  \"[13]\": 0.9,\n  \"[16]\": 0.85,\n  \"[17]\": 0.8,\n  \"[33]\": 0.75,\n  \"[20]\": 0.7\n}\n\n[13] and [16] are cited multiple times and directly relate to the core cross-domain meta-learning methodology proposed in this paper. The phrases \"we propose the method of crossdomain meta-learning\" and \"our overall strategy is to imitate domain shifts by splitting source domains as virtual source and virtual target domains, as in previous works [16] [33]\" indicate these are key inspirations.\n\n[17] is also cited as related work on meta-learning strategies for domain generalization, which is highly relevant to the cross-domain few-shot learning problem tackled here. \n\n[33] is grouped with [16] as pioneering the idea of imitating domain shifts by splitting source domains, a key part of their approach.\n\nFinally, [20] introduces contrastive learning which is leveraged in this paper for cross-domain regularization to handle label scarcity. The phrase \"As meta-path portrays the relevance of node types in HG, the relationship it represents could reveal semantic patterns and thus preserve cross-domain invariance among related domains. Thereby we build the auxiliary self-supervised learning task by taking advantage of contrastive learning [20], as shown to be adept at generating discriminative representations in low-data regime.\" highlights the importance of this reference.\n\nIn summary, these 5 references, especially [13], [16] and [17], appear to be the most central prior works that directly motivated the novel cross-domain meta-learning approach proposed in this paper for tackling heterogeneous graph few-shot learning.", "62c2a9595aee126c0fcf0a32": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b54]\": 0.9,\n  \"[b39]\": 0.8,\n  \"[b44]\": 0.7,\n  \"[b43]\": 0.6,\n  \"[b23]\": 0.5\n}\n\nConfidence scores:\n[b54]: 0.9 - The paper makes multiple references to this work, specifically mentioning \"MetaAttack [b54]\" as a key attack method the proposed defense aims to address. The frequent and direct comparisons suggest this is a highly influential source.\n\n[b39]: 0.8 - The authors state \"According to [b39] [b54], most of the perturbations are edge insertions, which connect nodes with different labels.\" This indicates [b39] provides key insights about the nature of adversarial perturbations that motivate the proposed approach.\n\n[b44]: 0.7 - The paper compares results to \"GNN-Guard [b44]\" in Table 1 and discusses its limitations, suggesting it is an important prior work the proposed method aims to improve upon.\n\n[b43]: 0.6 - \"GRCN [b43]\" is compared in the results and its approach of using learned representations for edge weights is analyzed. While relevant, the connection is not as direct as the top 3 papers.\n\n[b23]: 0.5 - The \"vanilla GCN [b23]\" is referenced as a standard model the proposed \"advanced GCN\" aims to make more robust. However, the connection is more about general GNN architectures than the specific robustness contributions of this paper.\n\nIn summary, the papers [b54], [b39] and [b44] appear to be the most central and directly influential to the key ideas and methodology of this paper based on the provided criteria. The others provide relevant context but are not as critical to the core contributions.", "62f07ec290e50fcafde5ad10": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\n{\n  \"[b29]\": 0.9,\n  \"[b27]\": 0.85,\n  \"[b86]\": 0.8,\n  \"[b59]\": 0.75,\n  \"[b64]\": 0.7,\n  \"[b21]\": 0.6\n}\n\nConfidence scores:\n[b29] (0.9): This paper introduces the Contriever model which forms the basis of the retriever module in Atlas. The text states \"Our retriever module is based on the Contriever [b29], an information retrieval technique based on continuous dense embeddings.\"\n\n[b27] (0.85): The Fusion-in-Decoder architecture from this paper is directly used in the language model component of Atlas. \"For the language model, we rely on the T5 sequence-to-sequence architecture [b59]. We rely on the Fusion-in-Decoder modification of sequence-to-sequence models, and process each document independently in the encoder [b27].\"\n\n[b86] (0.8): The attention distillation training objective is heavily inspired by this paper. \"The first loss that we consider is based on the attention scores of the language model, and is heavily inspired by [b86].\"\n\n[b59] (0.75): The T5 model architecture is used for the language model component. Atlas also follows the text-to-text framework introduced in this paper. \n\n[b64] (0.7): The perplexity distillation training objective is loosely inspired by [b64], among others. \"Third, we discuss a simpler loss function which is loosely inspired by the objectives from the attention distillation and EMDR2 methods [b86] [b64].\"\n\n[b21] (0.6): The Contriever retriever model is pre-trained using the MoCo contrastive loss from this paper. \"The Contriever model is pre-trained using the MoCo contrastive loss [b21], and uses unsupervised data only.\"\n\nIn summary, the novel contributions of Atlas - the retriever-augmented architecture with a dense retriever and fusion-in-decoder language model, along with the joint pre-training schemes - draw direct inspiration from the referenced papers, especially [b29], [b27] and [b86]. The other references provide key components like the T5 architecture and contrastive pre-training.", "628464625aee126c0faca44e": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\nThe key novel contributions of this paper seem to be:\n\n1. Proposing CASCADER, a tiered ranking architecture that uses a sequence of language models with increasing complexity to adaptively reweight and rerank the outputs of more efficient base knowledge graph embeddings (KGEs). This allows balancing accuracy and efficiency.\n\n2. Introducing a novel subset selection scheme where at each tier, CASCADER predicts the minimal set of candidates to progress to be reweighted at the next tier. This minimizes invocation of more computationally complex LMs.\n\n3. Demonstrating that CASCADER achieves appreciable accuracy gains over baselines while also significantly improving efficiency compared to other cross-modal ensemble baselines.\n\nBased on this, the most relevant references seem to be:\n\n{\n  \"[b39]\": 0.8,\n  \"[b25]\": 0.9,\n  \"[b40]\": 0.7,\n  \"[b24]\": 0.7\n}\n\n[b39] and [b25] are highlighted as they directly inspire the idea of integrating KGEs and LMs with additive reweighting to improve link prediction. The paper builds upon this key idea.\n\n[b40] and [b24] are relevant as they inspire the candidate pruning approach used in CASCADER to control the accuracy-efficiency tradeoff by progressing only a subset of candidates from tier to tier.\n\nThe confidence scores reflect that [b25] seems to be the most direct inspiration, followed closely by [b39], while [b40] and [b24] provide important but less central ideas that are built upon in the CASCADER architecture.", "6303545e90e50fcafd7d3b71": "Based on the analysis of the paper, the most pertinent source papers that inspired this work are:\n\n{\n  \"[b5]\": 0.9,\n  \"[b16]\": 0.8,\n  \"[b32]\": 0.7,\n  \"[b28]\": 0.6\n}\n\nThe key evidence for selecting these papers is:\n\n1. [b5] (PNA) is directly mentioned as motivating the usage of a mixture of aggregators in a node-wise manner. The paper states \"PNA [b5] improves the expressiveness by considering a mixture of different aggregators, where the mixing coefficients are not flexibly determined based on the local patterns of each node similar to our aggregator controller.\"\n\n2. [b16] (Policy-GNN) is highlighted as also using a policy to determine each node's depth, similar to one aspect of the proposed node-wise architecture. \n\n3. [b32] (Ala-GCN) is described as terminating message passing based on an over-smoothing indicator, which is compared to the parametric depth controller proposed in this paper.\n\n4. [b28] (IterGNN) is mentioned as determining GNN depth on-the-fly in a similar manner as equation (3) in this paper, but not doing it in a node-wise way.\n\nThe paper directly states these works inspired key aspects of the node-wise architecture framework, especially the depth and aggregator controllers. The methodological similarities are clearly outlined. Other referenced papers like [b34], [b29], [b22] are noted as considering a single architectural aspect, but the inspiration is less direct compared to [b5], [b16], [b32] and [b28].", "62d16f8d5aee126c0fd82ddd": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b8]\": 0.9,\n  \"[b22]\": 0.85,\n  \"[b6]\": 0.8,\n  \"[b26]\": 0.75,\n  \"[b40]\": 0.7,\n  \"[b41]\": 0.7,\n  \"[b42]\": 0.7\n}\n\nRationale and confidence scores:\n\nThe paper introduces a novel technique of creating a \"universal\" kernel profile by combining profiles from multiple data center applications, in order to optimize the Linux kernel in a generalized way using profile-guided optimizations (PGO). \n\nThe most directly relevant references that likely inspired this work are:\n\n- [b8] and [b22] (confidence 0.9 and 0.85): These papers demonstrated the effectiveness of profile-guided optimizations in achieving significant speedups for data center applications. The current paper builds upon this by applying PGO to the Linux kernel specifically.\n\n- [b6] (confidence 0.8): This reference from Google likely highlighted the large instruction footprints of data center applications and the Linux kernel, motivating the need for the kernel optimizations introduced here.\n\n- [b26], [b40], [b41], [b42] (confidence 0.75, 0.7): These prior works focused on creating application-specific optimized kernels. While the current paper takes a different approach by creating a universal kernel profile, it was likely inspired by the idea of using PGO for the kernel from these previous works.\n\nThe key novel contributions of this paper are 1) Characterizing Linux kernel behavior and profiles across multiple data center applications and demonstrating similarity 2) Combining profiles to generate a universal kernel profile 3) Showing significant speedups from optimizing the kernel with this universal profile.\n\nSo in summary, while prior work demonstrated the benefits of PGO for applications and explored application-specific kernel customization, this paper makes the novel contribution of leveraging cross-application kernel profile similarity to create a universal optimized kernel. The references highlighted likely provided the closest inspiration and background for these ideas.", "62393e7f5aee126c0f12607f": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\n{\n  \"b23\": 0.95,\n  \"b30\": 0.85,\n  \"b22\": 0.8,\n  \"b27\": 0.7,\n  \"b20\": 0.6\n}\n\nReasoning:\nThe paper introduces a semi-supervised learning framework for document-level relation extraction. The key novel contributions appear to be:\n\n1. Using an axial attention module to capture interactions between entity pairs and encode two-hop reasoning paths. This is directly inspired by and builds upon the work in [b20].\n\n2. Proposing an Adaptive Focal Loss to address the class imbalance problem, extending the Adaptive Thresholding Loss from [b30]. \n\n3. Using knowledge distillation to adapt distantly supervised data, improving upon the naive pre-training approach in [b22].\n\n4. Achieving state-of-the-art results on the DocRED [b23] and HacRED datasets, outperforming the previous SOTA model SSAN-Adapt [b22].\n\nThe methodology frequently references [b23], [b30], [b22] and [b27], indicating these works provided key inspiration and baselines. In particular, [b23] introduced the DocRED dataset the model is evaluated on. The axial attention module is directly motivated by [b20].\n\nTherefore, I assign the highest confidence scores to [b23], [b30], [b22], [b27] and [b20] as being the most influential and pertinent previous works that this paper builds upon to make its novel contributions in document-level relation extraction. The scores reflect the strength of influence as indicated by the language used.", "6293c3025d72d8000db42919": "Here are the most relevant source papers for this paper, along with confidence scores:\n\n{\n  \"[b10]\": 0.95,\n  \"[b22]\": 0.9,\n  \"[b42]\": 0.85,\n  \"[b46]\": 0.8,\n  \"[b8]\": 0.75,\n  \"[b7]\": 0.7,\n  \"[b31]\": 0.65\n}\n\nThe key reasons are:\n\n1. The paper is heavily inspired by and builds upon the TVM compiler framework proposed in [b10] for block-level scheduling optimization. This is the most directly relevant prior work.\n\n2. MobileNet-v3 [b22] is used as the main baseline that CHaNAS is compared against, indicating its architecture search space design takes heavy inspiration from it. \n\n3. MnasNet [b42] and FBNet [b46] are two other hardware-aware NAS baselines compared against, whose search spaces and methods likely motivated the design of CHaNAS.\n\n4. ProxylessNAS [b8] is another NAS baseline compared that uses single-path weight sharing, which CHaNAS adopts for its supernet training.\n\n5. The elastic kernel size and progressive knowledge distillation techniques used are based on [b7].\n\n6. The weight sharing technique in the supernet training originates from the one-shot NAS method proposed in [b31].\n\nSo in summary, the TVM compiler work, recent hardware-aware NAS methods, and one-shot supernet training techniques are the key prior works that inspired the main contributions in CHaNAS. The confidence scores reflect the degree of direct relevance and inspiration each reference provided to the core ideas.", "62de84a55aee126c0f96fbb9": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b27]\": 0.9,\n  \"[b17]\": 0.8,\n  \"[b8]\": 0.7,\n  \"[b23]\": 0.6\n}\n\nConfidence scores:\n[b27]: 0.9 - This reference is mentioned multiple times as the key inspiration for applying contrastive learning to recommendation in a graph augmentation based approach. The proposed method is directly compared to and built upon the framework from this paper.\n\n[b17]: 0.8 - The InfoNCE contrastive loss from this paper is a core component used in the graph contrastive learning setup. Optimizing this loss is shown to be more important than the specific graph augmentations.\n\n[b8]: 0.7 - The LightGCN architecture from this paper is adopted as the backbone graph encoder. The proposed method aims to improve upon LightGCN by adding contrastive learning.\n\n[b23]: 0.6 - The analysis of alignment and uniformity properties of contrastive learning representations is heavily inspired by this computer vision paper. The uniformity metric is used to analyze the learned embeddings.\n\nIn summary, the key novel contributions - applying contrastive learning to recommendation without graph augmentations and instead using representation-level noise, are most directly inspired by [b27] and [b17]. The methodological setup builds upon LightGCN [b8] and analyzes uniformity based on ideas from [b23]. These appear to be the most central references that the main ideas of the paper aim to integrate and improve upon.", "63180bf590e50fcafded784e": "Here is my analysis of the key source papers for this paper, based on the criteria provided:\n\nThe paper introduces a novel contrastive learning method for recommendation called XSimGCL, which builds upon the authors' previous work SimGCL [23]. The key novel contributions are:\n\n1. Revealing that the InfoNCE contrastive loss is more important than graph augmentations for learning effective representations in contrastive recommendation. \n\n2. Proposing a simple noise-based augmentation approach to adjust the uniformity of the learned representation distribution.\n\n3. Developing XSimGCL, a more efficient and effective contrastive recommendation model compared to SimGCL, by sharing forward/backward propagation between the recommendation and contrastive tasks.\n\nBased on this, the most likely key source papers are:\n\n{\n  \"[23]\": 0.9,\n  \"[21]\": 0.8,\n  \"[27]\": 0.7,\n  \"[11]\": 0.6\n}\n\n[23] is the authors' preliminary work SimGCL, which XSimGCL directly builds upon and improves. The noise-based augmentation and focus on representation uniformity are motivated by SimGCL.\n\n[21] introduces the InfoNCE contrastive loss, which the paper identifies as the key driver of performance in contrastive recommendation, more so than graph augmentations. \n\n[27] proposes LightGCN, the graph encoder used in XSimGCL. XSimGCL aims to be as lightweight as LightGCN.\n\n[11] presents SGL, a representative graph augmentation based contrastive recommendation model that the paper analyzes in depth to motivate XSimGCL.\n\nThe paper also references some other contrastive learning and recommendation papers like [14], [12], [24], but they do not appear to be as central to the main novel contributions as the ones listed above. The key is the strong reliance on the authors' own prior work SimGCL, the InfoNCE loss, and the LightGCN architecture.", "628c4ce25aee126c0ff59b39": "Here are the most relevant source papers for this paper, based on the criteria provided:\n\n{\n  \"[b8]\": 0.9,\n  \"[b13]\": 0.8,\n  \"[b9]\": 0.7,\n  \"[b2]\": 0.6,\n  \"[b34]\": 0.5\n}\n\nConfidence scores:\n- [b8] (GPRGNN) has a high confidence score of 0.9 because the paper directly builds upon and compares to GPRGNN, which can express all polynomial filters and is described as \"one of the most expressive models\". The proposed JacobiConv aims to improve upon GPRGNN.\n\n- [b13] (Bern-Net) has a score of 0.8 as the paper analyzes it in detail, comparing the expressiveness of different polynomial bases used by spectral GNNs. Bern-Net is a key part of this analysis.\n\n- [b9] (ChebyNet) receives a 0.7 since it is described as able to express all polynomial filters like GPRGNN, making it very relevant to the theoretical analysis of spectral GNN expressiveness, a core contribution.\n\n- [b2] (ARMA) gets a 0.6 as it proposes using rational filters which are then approximated by polynomials, tying into the polynomial focus. However, it is slightly less relevant than the pure polynomial approaches.\n\n- [b34] earns a 0.5 for connecting the expressivity analyses of spectral GNNs done in this paper to that of spatial GNNs using Weisfeiler-Lehman tests, an important theoretical bridge, although it is not as directly related to the core spectral GNN contributions.\n\nThe scores prioritize the references that most directly inspired the novel contributions around analyzing and improving spectral GNN expressiveness using polynomial filter bases. Methodologically related works around polynomial filters receive the highest relevance.", "62c28ae45aee126c0f8a182f": "Here are the most relevant source papers for this paper, based on the criteria provided:\n\n{\n  \"[b15]\": 0.9,\n  \"[b10]\": 0.85,\n  \"[b46]\": 0.8,\n  \"[b23]\": 0.75,\n  \"[b24]\": 0.7,\n  \"[b16]\": 0.65\n}\n\nThe key reasons for selecting these papers as the most influential sources are:\n\n1. The paper takes direct inspiration from the spectral graph convolution defined in [b15] and [b10], using them as a foundation to generalize existing spectral graph convolution methods. This is evident from phrases like \"The definition of spectral graph convolution relies on Fourier transform on the graph domain\" and equations that build upon those papers.\n\n2. The analysis of correlation issues in existing GNN architectures is motivated by and compared against prior work on oversmoothing and low/high pass filters, as studied in [b46], [b23] and others. \n\n3. The proposed correlation-free architecture is positioned as an improvement over the polynomial filter designs explored in papers like [b10], [b16], [b24] etc. The authors aim to overcome limitations of those approaches.\n\n4. While many other papers are referenced, the ones selected above appear to be the conceptual and methodological cornerstones that directly inspired the novel contributions of this paper - namely the spectral analysis of correlation issues and the design of the correlation-free architecture with improved filter bases. The frequent comparisons to these specific prior works highlights their centrality.", "62d0db155aee126c0f9f111a": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\n{\n  \"[b19]\": 0.95,\n  \"[b40]\": 0.9,\n  \"[b23]\": 0.85, \n  \"[b27]\": 0.85,\n  \"[b15]\": 0.8\n}\n\nRationale:\nThe paper heavily relies on and compares to the dense retrieval work DPR from [b19] throughout, using it as the main baseline. This is the most central reference.\n\nThe paper aims to improve neural retrieval generalization over the cross-domain benchmark BEIR from [b40], which is mentioned multiple times as a key motivation and evaluation. \n\nFor the novel contribution of applying parameter-efficient learning to neural retrieval, the paper takes direct inspiration from the prompt tuning techniques in [b23] and the P-Tuning v2 method from [b27]. These two references are cited extensively.\n\nThe adapter technique from [b15] is another parameter-efficient baseline the paper experiments with, though it performs weaker than prompt tuning.\n\nI focused on these 5 references as they are the most directly linked to the paper's main goals - improving DPR's cross-domain generalization via parameter-efficient prompt tuning. The other citations provide relevant background but are less central to the key contributions in my assessment.", "628749345aee126c0ffeb827": "Here is my attempt at identifying the most pertinent source papers based on the given criteria, in JSON format with confidence scores:\n\n{\n  \"[b34]\": 0.9,\n  \"[b33]\": 0.8, \n  \"[b32]\": 0.8,\n  \"[b30]\": 0.7,\n  \"Bench-Capon, 2003\": 0.7\n}\n\nReasoning:\n\nThe paper introduces a consolidated multilevel taxonomy of 54 human values taken from four authoritative cross-cultural social science studies. The key phrases \"taken from\" and \"based on\" indicate a strong reliance on these previous works for developing the paper's core contribution of the value taxonomy.\n\nSpecifically, [b34], [b33] and [b32] are referenced multiple times and seem to provide the foundation for the value definitions and taxonomy structure. The high frequency of these citations and the comparative language used (e.g. \"Schwartz et al.'s refined theory, which, however, has been extended by us\") point to their central importance.\n\n[b30] is also referenced as a key work estimating the total number of human values and developing a practical survey, which likely helped inform the taxonomy.\n\nFinally, the paper brings values into computational argumentation, building upon the value-based argumentation framework of Bench-Capon, 2003. This pioneering work seems to have directly motivated applying values to argument mining.\n\nThe other referenced papers, while relevant, do not appear to be as directly influential to the main contributions based on the language used. The novel contributions revolve around consolidating a value taxonomy and dataset for argument mining, so the key references are those foundational social science and argumentation theory works.", "621635aa91e011b46d7ce129": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b41]\": 0.9,\n  \"[b32]\": 0.8,\n  \"[b44]\": 0.7,\n  \"[b4]\": 0.6\n}\n\nExplanation:\nThe paper introduces a novel Graph Convolutional Clustering (GCC) model that jointly performs graph embedding and clustering. The key source papers that inspire and motivate this work are:\n\n[b41] (confidence 0.9): The choice of aggregation function in GCC is directly inspired by the simple graph convolution proposed in this paper (SGC). The authors even name their approach \"Normalized Simple Graph Convolution\". So [b41] is the clearest and most direct inspiration.\n\n[b32] (confidence 0.8): The linear encoder and decoder approach in GCC is motivated by the Linear Graph Autoencoder from this paper (LGAE). The authors state \"Linear graph autoencoders (LGAE) [b32] have shown that a linear encoder with an inner product decoder can be powerful enough to reach competitive results\". \n\n[b44] (confidence 0.7): The overall formulation of jointly optimizing the embedding and clustering is connected to the Deep Clustering Network (DCN) approach from this paper. The authors draw parallels between the cost functions.\n\n[b4] (confidence 0.6): Some optimization details in GCC, specifically the cluster assignment update, are equivalent to the Deep Modularity Network from this paper, as stated by the authors.\n\nThe other references, while relevant for background and context, do not seem to be as directly inspirational for the novel aspects of GCC based on the language used. The key phrases and comparisons clearly point to [b41], [b32], [b44] and [b4] being the most influential prior works that motivated this paper's main contributions.", "62feff8d90e50fcafd599888": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b4]\": 0.9,\n  \"[b2]\": 0.8,\n  \"[b16]\": 0.7,\n  \"[b19]\": 0.6\n}\n\nConfidence scores:\n[b4]: 0.9 - The paper heavily relies on LightGCN [b4] as the backbone GNN model throughout the experiments. The novel Navip method is built directly on top of the LightGCN architecture.\n\n[b2]: 0.8 - UEBPR [b2] is used as a key comparison to validate whether the effect of the proposed Navip method overlaps with state-of-the-art IPS methods like UEBPR. This indicates Navip is motivated by and builds upon the ideas in [b2].\n\n[b16]: 0.7 - The pseudo-unbiased testing and validation data creation process follows the methodology from [b16]. This is a key part of the experimental setup to evaluate the debiasing effect.\n\n[b19]: 0.6 - SNIPS [b19] is mentioned as a method that addresses some drawbacks of vanilla IPS, which the proposed Navip method aims to improve upon. While not as central as the other references, [b19] provides important context and motivation for Navip.\n\nIn summary, [b4], [b2] and [b16] emerge as the most influential source papers, with [b4] being the foundation the novel method is built on, [b2] serving as a key point of comparison, and [b16] informing the experimental methodology. [b19] provides additional context but is less central to the main contributions.", "62cce6795aee126c0f2a7fcc": "Here is my analysis of the key source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b8]\": 0.9,\n  \"[b33]\": 0.8,\n  \"[b4]\": 0.7,\n  \"[b44]\": 0.6,\n  \"[b24]\": 0.5\n}\n\nRationale and confidence scores:\n\nThe paper introduces TensorIR, a new abstraction for automatic optimization of tensorized programs. The key novel contributions are:\n\n1. The block construct which isolates tensorized computation regions from outer loop nests. This enables independent transformation of the outer loops and inner tensorized computations.\n\n2. Automatic scheduling by searching over ways to divide the problem into sub-blocks guided by tensor primitives, and transforming the sub-problems. \n\n3. Expanding support beyond normal tensor expressions to irregular computations mixed with tensor computations.\n\nThe key references that directly inspired and motivated these contributions are:\n\n[b8] (confidence 0.9): TVM, which takes a bottom-up approach to model the search space with loop nest iterators around scalar operations. TensorIR improves upon this by introducing the block construct.\n\n[b33] (confidence 0.8): Halide, which also models the search space bottom-up like TVM. Again, the block construct is an improvement.\n\n[b4] (confidence 0.7): Tiramisu compiler, which has similarities to TVM and Halide that TensorIR builds upon. \n\n[b44] (confidence 0.6): The AKG compiler, which also models the search space bottom-up and TensorIR improves on.\n\n[b24] (confidence 0.5): MLIR/Affine, another bottom-up approach that TensorIR aims to improve upon with its block construct and automatic scheduling.\n\nThe paper directly compares TensorIR to these previous works, using phrases like \"TensorIR abstraction also allows us to represent and optimize programs that contain a mixture of irregular computations and tensor computations, expanding the possible support beyond a normal tensor expression [b8].\" This indicates they directly build upon and aim to improve these prior techniques.", "628749495aee126c0fff087a": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b0]\": 0.9,\n  \"[b31]\": 0.8,\n  \"[b44]\": 0.7,\n  \"[b15]\": 0.6,\n  \"[b40]\": 0.6\n}\n\nConfidence scores:\n[b0]: 0.9 - The paper directly states \"The hierarchical design of reasoning path is inspired by the human reasoning process for multihop QA task.\" This indicates [b0] was a key inspiration for their novel hierarchical reasoning path approach.\n\n[b31]: 0.8 - The T5 model from [b31] is used to initialize their generative FID and PATHFID models. This pre-trained model forms the foundation of their approach.\n\n[b44]: 0.7 - The paper mentions \"Thanks to large pre-trained transformers like T5 [b31], [b14] introduced fusion-in-decoder (FID) that leverages passage retrieval with generative models for open-domain QA\". This suggests the FID model from [b44], which they build upon, was an important prior work. \n\n[b15]: 0.6 - DPR from [b15] is mentioned as an example of a text retrieval system that can be used to retrieve the initial set of passages in the open-domain QA setting. While relevant, it seems more peripheral to the core contributions.\n\n[b40]: 0.6 - The paper states \"we observe that the success of the FID model does not extend to multi-hop QA, which is corroborated by the findings in [b40].\" This suggests [b40] provided important context and motivation, but was not as directly influential on the novel aspects of PATHFID itself.\n\nThe key novel contributions seem to be 1) Extending multi-hop QA beyond just answer generation to also generate the reasoning path, and 2) Enabling cross-passage interactions in the PATHFID+ model. The selected references, especially [b0], [b31] and [b44], appear to be the most influential prior works that inspired and enabled these main new aspects introduced in the paper. Let me know if you would like me to clarify or expand the analysis in any way.", "62bbc3865aee126c0fa68605": "Based on the analysis of the paper, here are the most likely source papers that heavily influenced the main contributions, along with confidence scores:\n\n{\n  \"[b13]\": 0.95,\n  \"[b16]\": 0.85,\n  \"[b8]\": 0.8,\n  \"[b12]\": 0.75,\n  \"[b2]\": 0.75,\n  \"[b20]\": 0.7,\n  \"[b30]\": 0.65\n}\n\nThe key points:\n\n- The paper takes direct inspiration from structural entropy [b13] to develop their novel hierarchical pooling approach SEP. Structural entropy is used to assess graph structural information and decode the essential hierarchical structure of graphs. This is the core idea the paper builds upon.\n\n- [b16] is also highly relevant as it applies structural entropy in bioinformatics, demonstrating its utility in measuring graph information, which sparks the authors to use it for pooling. \n\n- The paper aims to address issues in previous hierarchical pooling methods like TopKPool [b8], SAGPool [b12], ASAP [b20], DiffPool [b30] and MinCutPool [b2]. These methods either unnecessarily drop nodes leading to information loss, or damage graph local structure due to fixed compression ratios.\n\n- In particular, the proposed SEP method is compared extensively to TopKPool [b8] and MinCutPool [b2] baselines, indicating they are very relevant prior works.\n\nSo in summary, [b13] and [b16] directly motivate the core SEP methodology, while [b8], [b12], [b2], [b20] and [b30] are the most relevant prior pooling methods that SEP aims to improve upon. The confidence scores reflect the degree of relevance and influence each reference likely had on the key ideas.", "630359f590e50fcafd88dc06": "Here are the most relevant source papers for this paper, based on my analysis:\n\n{\n  \"[b23]\": 0.9,\n  \"[b40]\": 0.85,\n  \"[b20]\": 0.8,\n  \"[b46]\": 0.8,\n  \"[b11]\": 0.75,\n  \"[b29]\": 0.75,\n  \"[b32]\": 0.7,\n  \"[b37]\": 0.7\n}\n\nRationale:\n- The paper heavily relies on the causal assumptions and latent factor split into output-causative factors and others from [b23] and [b40], as stated directly: \"Following the discussion in Sec. 2.1.2, we next introduce the three main causal assumptions in DICM. \u2022 (\ud835\udc46, \ud835\udc49 )\u2192 \ud835\udc4b , \ud835\udc46 \u2192 \ud835\udc4c (latent generating mechanism) 1 : We introduce two latent factors \ud835\udc46, \ud835\udc49 as the abstractions that determine the observed variables (\ud835\udc4b , \ud835\udc4c ), which have been similarly assumed in existing works [b23] [b40].\"\n- [b20] and [b46] directly inspire a key component of their method: \"Motivated by the observation that loss gradients from robust models align better with salient data features and human perception, which well outlines the contour of an object in images [b20] [b46] (see visualization in Fig. 2 ), in the context of image classification, we specify to utilize magnitude of gradients as the importance of pixels and generate the mask.\"\n- The paper builds upon the Structural Causal Model framework from [b11] and [b29].\n- The Independent Causal Mechanisms principle cited from [b32] and [b37] is a key assumption underlying their causal model.", "628748bc5aee126c0ffc3ecd": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"b3\": 0.9,\n  \"b20\": 0.85,\n  \"b6\": 0.8,\n  \"b0\": 0.7\n}\n\nConfidence scores:\nb3 (0.9): This reference, likely referring to the multilingual BERT (mBERT) paper, is mentioned multiple times as a key enabler for cross-lingual transfer learning and a backbone model used in this work. The high frequency and methodological centrality suggest it was a major inspiration.\n\nb20 (0.85): Also mentioned multiple times, this reference seems to establish important baselines that directly inform the approach and experimental setup, such as using mBERT as the encoder and the BIO labeling scheme. The repeated comparisons indicate its strong influence.\n\nb6 (0.8): The siamese network reference is identified as the original inspiration for the paper's key novelty - the entity similarity evaluator. Introducing this technique from computer vision to NER seems to be a central contribution.\n\nb0 (0.7): As the earliest work on siamese networks, this reference is credited with introducing the core concept that enables the paper's entity similarity modeling. Though less directly related than [b6], it still provides critical conceptual inspiration.\n\nThe other references, while relevant for context and comparisons, do not appear to be as pivotal in directly motivating the main ideas and techniques of this paper based on the provided criteria. The repeated emphasis on mBERT, siamese networks, and specific experimental choices traced to prior work identifies [b3], [b20], [b6] and [b0] as the most salient inspirations.", "62bab8f95aee126c0f6afb82": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b26]\": 0.95,\n  \"[b18]\": 0.9,\n  \"[b4]\": 0.8,\n  \"[b6]\": 0.7,\n  \"[b1]\": 0.65\n}\n\nThe key points that led me to identify these as the most relevant source papers are:\n\n1. The paper directly states that its analysis of alignment and uniformity properties in collaborative filtering is \"inspired by recent progress in contrastive representation learning [b4] [b26]\". This indicates [b26] and [b4] were major sources of inspiration.\n\n2. The paper shows theoretically that \"perfectly aligned and uniform encoders form the exact minimizers of the BPR loss\". The BPR loss was originally proposed in [b18], so this is likely an important foundational paper. \n\n3. When empirically analyzing different CF methods, the paper compares to LGCN [b6] and ENMF [b1] and discusses their properties. This suggests [b6] and [b1] contain relevant prior methods that this paper builds upon.\n\n4. The novel contribution is directly optimizing alignment and uniformity in the proposed DirectAU objective. The key inspirations for this seem to be the alignment and uniformity metrics proposed in [b26] and the theoretical connection to the BPR loss from [b18].\n\nSo in summary, [b26], [b18] and [b4] appear to be the papers that most directly inspired the core ideas, with [b6] and [b1] also being quite relevant as methods that are analyzed and compared to. The confidence scores reflect the centrality of each paper to the main contributions here.", "628749125aee126c0ffe0f2d": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b7]\": 0.9,\n  \"[b22]\": 0.8,\n  \"[b23]\": 0.7,\n  \"[b10]\": 0.6,\n  \"[b19]\": 0.6\n}\n\nThe key novel contributions of this paper are:\n1) Enhancing a generative ODQA model (FiD) with a pointer network to enable directly copying words from retrieved passages while retaining the ability to generate new words. This aims to make the generated answers more faithful to the source.\n2) Reusing the encoder-decoder attention scores as the copy distribution to reduce computational cost.\n\nThe most relevant source papers, in order of importance, are:\n\n[b7] (confidence 0.9): The FiD model which this paper directly builds upon and enhances. The proposed FiD-PGN model is an extension of FiD.\n\n[b22] (confidence 0.8): Introduces the pointer-generator network that enables seq2seq models to copy words from the input. This paper is directly inspired by [b22] to add a pointer network to FiD.\n\n[b23] (confidence 0.7): The original paper proposing the copy mechanism that pointer-generator networks are based on. \n\n[b10] (confidence 0.6): The BART pre-trained language model that FiD uses. This paper also uses a BART or T5 encoder.\n\n[b19] (confidence 0.6): The T5 pre-trained language model that FiD and this paper use as the base model.\n\nThe pointer-generator network and copy mechanism are the core novel aspects that this paper incorporates into FiD to improve factual accuracy. The heavy reliance on FiD and usage of the same underlying language models also makes the FiD paper highly relevant.", "62cce67a5aee126c0f2a86f3": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b33]\": 0.9,\n  \"[b31]\": 0.85,\n  \"[b1]\": 0.8,\n  \"[b2]\": 0.8,\n  \"[b35]\": 0.7,\n  \"[b25]\": 0.65,\n  \"[b17]\": 0.6\n}\n\nConfidence scores:\n- [b33] and [b31]: These papers are given the highest confidence scores of 0.9 and 0.85 respectively. The text indicates they directly inspired the graph-based approach used in this paper, with phrases like \"Zhang et al. [b33] overcome the problem of author name ambiguity using relational information considering three graphs\" and \"Xu et al. [b31] build a network of papers with multiple relationships. A network-embedding method is proposed to learn paper representations\". The methodological similarity is high.\n\n- [b1] and [b2]: Confidence score of 0.8 each. The Char2Vec model from [b1] and BERT model from [b2] are directly used in this paper's architecture to generate vector representations, so they are key components of the novel method.\n\n- [b35], [b25], [b17]: Moderate confidence scores from 0.6-0.7. These unsupervised approaches are discussed as related work and likely provided some inspiration, but the text does not indicate as direct of an influence as the graph-based papers. For example, [b35] is noted for \"estimating the number of unique authors\" which is a sub-task but not the core focus here.\n\nThe other referenced papers provide relevant background but do not seem to be as directly influential on this paper's specific novel contributions of using semantic/symbolic representations in a graph-based neural network architecture for author disambiguation. Limiting to these top 7 most pertinent references captures the key inspirations and foundations.", "62f3220a90e50fcafd115bd6": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b3]\": 0.9,\n  \"[b26]\": 0.8,\n  \"[b4]\": 0.7,\n  \"[b12]\": 0.6,\n  \"[b21]\": 0.5,\n  \"[b11]\": 0.5,\n  \"[b28]\": 0.5,\n  \"[b27]\": 0.5\n}\n\nConfidence scores:\n- [b3] (0.9): The paper heavily relies on the Biaffine decoder from dependency parsing, originally proposed in this reference, to convert span classification into classifying start and end token pairs. This is a core part of their methodology.\n\n- [b26] (0.8): The proposed approach builds upon the span-based method for nested NER introduced in this reference. The paper follows the span classification formulation and uses the start/end tokens to represent spans, similar to [b26].\n\n- [b4] (0.7): Another key span-based approach that assigns labels to each span. The paper compares to and builds upon this general span classification framework.\n\n- [b12] (0.6): Transformed NER into a machine reading comprehension task to handle nested entities. Mentioned as prior work on span-based methods.\n\n- [b21], [b11], [b28], [b27] (0.5 each): A group of recent span-based approaches that the paper directly compares its results to, using the same datasets. Indicates these are highly relevant prior works.\n\nThe other references like [b7], [b15], [b24] that discuss sequence labeling methods for NER are more peripheral - they help set context but are not as directly related to the core novel contributions around using CNNs to model span interactions in a span-based architecture. The analysis focuses on references that directly inspired or are compared to the central aspects of the proposed approach.", "628748e05aee126c0ffd1130": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b20]\": 0.9,\n  \"[b13]\": 0.9,\n  \"[b39]\": 0.8,\n  \"[b17]\": 0.8,\n  \"[b0]\": 0.7\n}\n\nThe key factors leading to these papers being identified as the most pertinent sources are:\n\n1. The paper proposes using syntactic information to guide contrastive learning objectives during pre-training. This is directly inspired by [b20] and [b13] which showed that attention heads in language models exhibit syntactic structure akin to constituency grammar. The paper aims to further improve the learning of syntactic phrase structure in attention distributions using these insights.\n\n2. The tree-guided contrastive objective is motivated by previous works [b39], [b17] and [b0] which incorporated syntax-guided attention mechanisms to make the root node attend more to its syntactically related descendant nodes. The paper adapts this idea into a contrastive objective.\n\n3. The overall approach of injecting syntactic knowledge into pre-trained language models via auxiliary pre-training objectives is most similar to the methodology in [b39], although this paper uses contrastive learning instead of the predictive tasks in [b39].\n\n4. Specific comparisons are made to the syntax-aware local attention (SLA) mechanism from [b17], showing that the proposed contrastive pre-training can be combined with SLA for further gains.\n\nSo in summary, [b20] and [b13] provide the core inspiration for utilizing constituency structure to guide attention learning, while [b39], [b17] and [b0] motivate the approach of using dependency structure to guide representation learning, within the overall framework of syntactically-informed pre-training. The paper directly builds upon the key ideas and methodologies from these previous works.", "628afb4c5aee126c0f04e3aa": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b49]\": 0.9,\n  \"[b23]\": 0.9,\n  \"[b9]\": 0.8,\n  \"[b35]\": 0.7\n}\n\nThe key reasons for selecting these papers are:\n\n1. The paper heavily relies on diffusion probabilistic models, citing [b49] and [b23] multiple times as the foundation for their approach. Key phrases like \"We instantiate this idea as a trajectory-level diffusion probabilistic model [b49] [b23] called Diffuser\" and \"Diffusion probabilistic models [b49] [b23] pose the data-generating process as an iterative denoising procedure\" indicate the centrality of these two papers.\n\n2. The training procedure and reverse process covariance schedule directly follow the approach in [b9], as evidenced by \"Reverse process covariances \u03a3 i follow the cosine schedule of [b9]\".\n\n3. The reinforcement learning formulation as guided sampling appeals to the control-as-inference graphical model from [b35].\n\nWhile other papers like [b55], [b45], [b29] are mentioned in the introduction for context on trajectory optimization, the core novel contributions of this paper - using a diffusion model for planning and reinforcement learning - draw primarily from [b49], [b23], [b9] and [b35]. The methodology has strong similarities to these works in terms of the diffusion model architecture, training objective, sampling procedure, and reformulation of RL as conditional sampling.", "6344dede90e50fcafd24ceec": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"b32\": 0.95,\n  \"b7\": 0.9,\n  \"b6\": 0.85,\n  \"b37\": 0.8,\n  \"b8\": 0.75,\n  \"b40\": 0.7,\n  \"b10\": 0.65\n}\n\nThe key novel contributions of this paper seem to be:\n1) Proposing an adaptive node sampling strategy for Graph Transformers using multi-armed bandits to sample the most informative nodes. This is referred to as ANS-GT (Adaptive Node Sampling for Graph Transformer).\n2) Introducing a hierarchical attention scheme with coarse-grained global attention using graph coarsening, to help capture long-range dependencies efficiently.\n\nThe source papers that most directly inspired and motivated these contributions are:\n\n[b32] - The original Transformer architecture paper. This work heavily builds upon the Transformer.\n[b7] - The Vision Transformer (ViT) paper which applied Transformers to computer vision. Directly mentioned as a key Transformer variant that motivated this graph Transformer approach.\n[b6] - The BERT paper, another influential Transformer variant in NLP that is cited as motivation.\n[b37] - The Graphormer paper which attempted to generalize Transformers to graphs. This paper directly builds upon and improves Graphormer.\n[b8] - The GT-Sparse paper, an existing Graph Transformer approach. The proposed method aims to address limitations of GT-Sparse and other Graph Transformers.\n[b40] - The Graph-BERT paper, another Graph Transformer approach that restricted node neighborhoods. This paper improves upon its node sampling.\n[b10] - The GraphSAGE paper which used node sampling in GNNs. While not directly used, it inspired the adaptive node sampling idea in this paper.\n\nThe confidence scores reflect the degree to which each reference directly inspired the key novel contributions of adaptively sampling informative nodes and using hierarchical local/global attention. The Transformer foundations in [b32], [b7] and [b6] get the highest scores, followed by the Graph Transformer works in [b37], [b8] and [b40] that are directly built upon. GraphSAGE [b10] gets a lower score since the node sampling is more loosely related to the core ideas.", "628749415aee126c0ffee49c": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b32]\": 0.9,\n  \"[b37]\": 0.9,\n  \"[b20]\": 0.8,\n  \"[b27]\": 0.7\n}\n\nRationale:\nThe paper proposes a graph neural network model for fine-grained entity typing. The key novel contributions are:\n\n1. Modeling fine-grained entity typing as a heterogeneous graph learning problem, with mention nodes, type nodes, and edges representing sibling relationships between mentions, hierarchical relationships between types, and isLabel relationships. \n\n2. Proposing two similarity metrics (word distribution-based and typing distribution-based) for detecting sibling mentions, and an algorithm for sibling mention selection.\n\n3. Employing an attentive graph neural network to learn mention and type representations by aggregating information from sibling and type neighbors.\n\nThe methodology section directly states that it \"employs graph neural networks (GNNs) with L layers [b32] [b37] to aggregate the information of sibling mentions and types for learning mention representations.\" This indicates [b32] and [b37] likely introduced foundational GNN techniques that this paper builds upon for its core graph learning approach. So I assign them the highest confidence of 0.9.\n\nIt also uses a mention encoder that \"uses the backbone from [b20]\" and \"the unified feature vector for the mention is derived by concatenating both representations\", suggesting [b20] is an important reference for the mention encoding component. I give this a slightly lower but still high confidence of 0.8.\n\nFinally, the mention representations are initialized with \"ELMo [b27] word representations\", indicating the ELMo embeddings from [b27] are used as a building block. However, this is a more standard technique, so I assign a moderate confidence of 0.7.\n\nThe other cited papers seem to provide more general background and motivation rather than directly inspiring the novel contributions. Let me know if you need any clarification or have additional questions!", "623004305aee126c0f9b3299": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b11]\": 0.95,\n  \"[b4]\": 0.8,\n  \"[b7]\": 0.75,\n  \"[b28]\": 0.7,\n  \"[b19]\": 0.6\n}\n\nConfidence scores:\n- [b11] (0.95): The paper directly states that it builds upon the GRAND framework proposed in this reference. Key phrases like \"GRAND+ follows the general consistency regularization principle of GRAND\" and comparisons to GRAND throughout the paper indicate this is the most influential prior work.\n\n- [b4] (0.8), [b7] (0.75): These matrix approximation based scalable GNN papers, PPRGo and GBP, are highlighted as highly relevant to the efficient approximations proposed in GRAND+. The paper states \"The proposed GRAND+ framework is highly related to matrix approximation based methods such as PPRGo [b4] and GBP [b7].\"\n\n- [b28] (0.7): The simplified graph convolution (SGC) proposed in this paper is an important prior work for decoupling feature propagation and non-linear transformation, which the matrix approximation methods like GRAND+ build upon. \n\n- [b19] (0.6): This APPNP paper proposes using a propagation matrix based on personalized PageRank to address oversmoothing. It inspires GRAND+'s generalized mixed-order propagation matrix.\n\nThe other references, while providing important context, do not seem to be as directly influential on the novel contributions of GRAND+ based on the phrasing in the text. The key contributions appear to be the scalable approximations and confidence-aware loss, which build most directly on [b11], [b4], [b7] and [b28].", "629587485aee126c0fe15059": "Based on the analysis of the paper, here are the most relevant source papers that likely inspired the authors' work, along with confidence scores:\n\n{\n  \"[b44]\": 0.9,\n  \"[b53]\": 0.8,\n  \"[b43]\": 0.7,\n  \"[b55]\": 0.7,\n  \"[b10]\": 0.6,\n  \"[b63]\": 0.6\n}\n\nThe key reasons are:\n\n1. The Audio2Facial-Dynamics module models facial movements using unsupervised key-points and their first order dynamics, which is directly inspired by [b44] and [b53]. The authors even state \"inspired by [b44] [b53]\".\n\n2. The idea of using unsupervised zero-and first-order motion representations to model local flow fields on faces likely comes from [b43] and [b55], as indicated by \"The key is to transfer the local emotional deformations to an audio-driven talking face with selflearned key-points and local affine transformations.\"\n\n3. The self-supervised training strategy used in the Audio2Facial-Dynamics module adopts ideas from [b10] and [b63].\n\nThe other cited papers provide relevant background and comparisons, but these 6 references appear to be the most central in directly inspiring the novel contributions of this paper, especially the unsupervised motion modeling approach. The high confidence scores reflect the explicit attributions made by the authors.", "634d809490e50fcafd4e683f": "Based on analyzing the text, here are the key source papers that likely inspired or motivated this research, in JSON format with confidence scores:\n\n{\n  \"[b14]\": 0.9,\n  \"[b0]\": 0.7,\n  \"[b1]\": 0.7,\n  \"[b16]\": 0.6,\n  \"[b18]\": 0.6\n}\n\nThe key novel contributions of this paper are:\n1) Using leaked speech and cross-spectrum phase analysis (CSP) to estimate groupings of students in a classroom, rather than relying on linguistic approaches. This is directly inspired by [b14] which introduces CSP.\n2) Applying spectral clustering to the CSP-based adjacency matrix to automatically cluster students into discussion groups. The use of spectral clustering seems to be motivated by [b18].\n\nThe proposed method also relies on voice activity detection (VAD) to identify speech segments, likely drawing from power-based VAD methods in [b16]. \n\nFinally, the motivation for this work - analyzing group discussions by recording individual students - is set up by references [b0] and [b1] in the introduction.\n\nThe other references, while providing useful background, do not seem to be as directly linked to the core novel aspects of using CSP analysis of leaked speech to automatically group students based on discussion proximity. The methodology draws clear and direct inspiration from a smaller subset of the cited works.", "630359ec90e50fcafd88ceba": "Based on the analysis of the given paper, here are the most pertinent source papers that likely inspired or motivated this work, along with confidence scores:\n\n{\n  \"[b1]\": 0.9,\n  \"[b2]\": 0.8,\n  \"[b27]\": 0.7,\n  \"[b8]\": 0.6\n}\n\n[b1] and [b2] are referenced multiple times and directly relate to the core methodology of using jackknife resampling and influence functions to quantify uncertainty. The paper states \"Building upon the jackknife resampling [b33] and the general principle outlined in [b1], we seek to bridge the gap between frequentist based uncertainty quantification and graph neural networks.\" This indicates [b1] was a key inspiration.\n\n[b27] introduces influence functions which are a critical component leveraged in this paper to estimate leave-one-out parameters without retraining. \n\n[b8] provides a classical result used in Equation 5 to compute the influence functions.\n\nWhile other references like [b29], [b39], [b48] are mentioned in the context of existing deterministic uncertainty quantification approaches for GNNs, the proposed method is described as fundamentally different. So those papers likely provided context and motivation but were not as directly influential on the novel contributions.\n\nThe historical context references in the introduction like [b44], [b18], [b6] establishing the popularity of GCNs were excluded, as they do not directly relate to the paper's core techniques.", "629435a05aee126c0f2fe317": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"[b8]\": 0.9,\n  \"[b2]\": 0.8,\n  \"[b14]\": 0.7,\n  \"[b36]\": 0.6,\n  \"[b21]\": 0.5\n}\n\nExplanation:\n\nThe paper proposes EvenNet, a novel spectral GNN designed to generalize across graphs with different homophily levels. The key phrases indicating strong reliance on previous research are:\n\n1. \"Among them, spectral GNNs with learnable polynomial filters [b2] [b8] [b14] adaptively learn suitable graph filters from training graphs and achieve promising performance on both homophilic and heterophilic graphs.\" - This suggests the paper builds upon the methods from references [b2], [b8] and [b14].\n\n2. \"Zhu et al. [b35] first established the relationship between graph homophily and structural attacks. They claimed that existing attack mechanisms tend to introduce heterophily to homophilic graphs, which significantly degrade the performance of GNNs with low-pass filters.\" - The paper is motivated by the findings in [b35] to develop a solution.\n\n3. \"Geom-GCN [b21] uses a novel neighborhood aggregation scheme to capture long-distance information. Zhu et al. [b36] introduce several designs that are helpful for GNNs to learn representations beyond homophily.\" - The paper compares its approach to the methods in [b21] and [b36] for handling heterophilic graphs.\n\nAmong these, [b8], [b2] and [b14] are given the highest confidence scores as they directly relate to the novel spectral GNN approach proposed in the paper. [b36] and [b21] are also relevant as they deal with heterophilic GNNs which the paper aims to improve upon, but are slightly less central to the main contributions.", "626754c85aee126c0fbcdd75": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b30]\": 0.95,\n  \"[b41]\": 0.8,\n  \"[b27]\": 0.75,\n  \"[b26]\": 0.6,\n  \"[b36]\": 0.6\n}\n\nConfidence scores:\n- [b30] (SuperGlue): 0.95. The paper is heavily inspired by and builds upon SuperGlue, an attentional GNN for feature matching. It aims to provide a sparser, more efficient alternative to SuperGlue while preserving matching performance. SuperGlue is referenced multiple times as the foundation and comparison point.\n\n- [b41] (Transformer): 0.8. The attention mechanism popularized by the Transformer architecture is a key component used in the paper's ClusterGNN. Transformers and their quadratic complexity are discussed as motivation for the paper's approach.\n\n- [b27] (Routing Transformer): 0.75. The coarse-to-fine cluster-based GNN proposed in the paper is directly inspired by the Routing Transformer's content-based sparse attention for clustering tokens. \n\n- [b26] and [b36]: 0.6. The Dual-Softmax operator from these papers is adopted for computing the matching probability matrix.\n\nThe novel contributions of the paper are 1) A learnable, coarse-to-fine clustering method to establish sparse local graphs for feature matching, and 2) The ClusterGNN architecture that approximates attention on complete graphs using gradually forming clusters. These aspects aim to improve efficiency while maintaining the matching performance of attentional GNNs like SuperGlue.", "634f6ae390e50fcafdcb62af": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b1]\": 0.9,\n  \"[b17]\": 0.9,\n  \"[b8]\": 0.95,\n  \"[b7]\": 0.8,\n  \"[b45]\": 0.7,\n  \"[b44]\": 0.7,\n  \"[b46]\": 0.7\n}\n\nThe key points that led me to this assessment:\n\n- The paper is heavily inspired by and builds upon BERT [b1] and RoBERTa [b17] for learning language representations via self-supervised pretraining. It initializes the language model component with RoBERTa.\n\n- The cross-modal encoder architecture directly adopts GreaseLM [b8], indicating it is a core inspiration. GreaseLM in turn builds on ideas from QAGNN [b7] for jointly reasoning over text and knowledge graphs.\n\n- For the link prediction objective, the paper considers using scoring functions from key knowledge graph embedding models like DistMult [b45], TransE [b44] and RotatE [b46]. While not the main focus, incorporating these shows they provide important motivation.\n\n- The novel contributions seem to be: 1) Unifying masked language modeling and link prediction objectives for joint text-KG pretraining, and 2) Showing this pretraining approach leads to strong performance gains on downstream tasks.\n\n- Other referenced papers provide relevant background and motivation but the ones listed above appear to be the most central, direct influences based on the criteria around methodological similarities, comparative phrases, and connections to the primary novel contributions.\n\nLet me know if you would like me to explain or refine the analysis further. I aimed to focus on the most pertinent 5-8 references that directly inspired the key ideas, based on my understanding of the paper.", "6369c8cd90e50fcafde87ef2": "Based on analyzing the given text, here are the most likely source papers that inspired this work, in JSON format with confidence scores:\n\n{\n  \"[b6]\": 0.9,\n  \"[b17]\": 0.85,\n  \"[b60]\": 0.8,\n  \"[b87]\": 0.75,\n  \"[b36]\": 0.7,\n  \"[b41]\": 0.7,\n  \"[b84]\": 0.7,\n  \"[b86]\": 0.7\n}\n\nThe key reasons for selecting these papers are:\n\n1. The text heavily discusses prompt-based few-shot learning with pretrained language models (PLMs), citing papers like [b6], [b17], [b60] as pioneering works in this area that allow PLMs to adapt to new tasks with limited data.\n\n2. More recent prompt-based approaches that automate prompt design through gradient-based searching [b65] or continuous prompt embeddings [b36], [b41], [b84], [b86] are also highlighted as important related works. \n\n3. The core idea of the paper is generating training data for few-shot learning. [b87] is cited as a key prior work that uses PLMs to generate label-conditioned texts as additional training data.\n\n4. Methodologically, the proposed approach builds upon prefix-tuning the PLM [b41] to preserve its knowledge for generating novel samples.\n\nSo in summary, the prompt-based few-shot learning papers, especially those on automating prompt design and using PLMs for data generation, appear to be the primary inspiration for this work based on the background discussion and key ideas. The high density of citations to these papers further supports their centrality.", "6304456b90e50fcafd12fe4b": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b23]\": 0.9,\n  \"[b0]\": 0.8,\n  \"[b65]\": 0.8,\n  \"[b63]\": 0.7,\n  \"[b22]\": 0.6\n}\n\nConfidence scores:\n- [b23]: 0.9. The paper directly mentions using a \"curriculum learning paradigm [b23]\" to gradually generate a domain-adaptive curriculum to guide the pre-training process. This indicates [b23] was a key inspiration.\n\n- [b0]: 0.8. The paper discusses a \"domain-adaptive generalization bound\" proved by Ben-David et al. in [b0] and aims to derive a similar bound for graphs. [b0] seems to provide important theoretical foundations.\n\n- [b65]: 0.8. Theorem 1 which provides a population generalization result for graphs is directly cited from [b65]. This reference is central to the paper's theoretical analysis.\n\n- [b63]: 0.7. The differentiable pooling matrix used in the encoder is defined by following the approach in [b63]. This indicates methodological similarity.\n\n- [b22]: 0.6. The curriculum regularizer is derived from the robust non-convex penalties in [b22], suggesting some inspiration from this work on curriculum learning.\n\nIn summary, the curriculum learning paradigm from [b23], domain adaptation theory from [b0], and graph-based generalization bound from [b65] appear to be the most influential references that directly motivated key aspects of this paper's approach. The pooling method from [b63] and curriculum penalties from [b22] also share strong methodological similarities with components used in the proposed framework.", "637aec2590e50fcafd929667": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"[7]\": 0.9,\n  \"[35]\": 0.85,\n  \"[33]\": 0.8,\n  \"[13]\": 0.7,\n  \"[4]\": 0.6,\n  \"[38]\": 0.6,\n  \"[37]\": 0.5\n}\n\nConfidence scores:\n[7] - 0.9: This reference is mentioned multiple times as directly inspiring and motivating the SmoothQuant method. The paper states that [7] identified the key challenge of outlier activations in large language models that makes quantization difficult. SmoothQuant aims to address the limitations of the solution proposed in [7].\n\n[35] - 0.85: ZeroQuant [35] is discussed as a previous quantization method that SmoothQuant builds upon and aims to improve. It is used as a key baseline to compare against.\n\n[33] - 0.8: The outlier suppression method from [33] is another important baseline that SmoothQuant is compared to. Some techniques from [33] like clipping outlier tokens during calibration are adopted.\n\n[13] - 0.7: This reference provides background on the integer quantization fundamentals that SmoothQuant employs. \n\n[4], [38], [37] - 0.5-0.6: These references are for the large language models OPT-175B, BLOOM-176B and GLM-130B that SmoothQuant is evaluated on. They provide the model architectures that the novel quantization method is applied to.\n\nIn summary, references [7], [35] and [33] emerge as the most influential prior works that inspired the key ideas behind SmoothQuant - analyzing the outlier activation challenge, building on per-channel quantization, and comparing to state-of-the-art baselines. The other references provide important context on quantization primitives and benchmark models.", "6350bc6d90e50fcafdecf145": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b41]\": 0.9,\n  \"[b36]\": 0.9,\n  \"[b21]\": 0.8,\n  \"[b2]\": 0.7,\n  \"[b9]\": 0.6,\n  \"[b40]\": 0.6\n}\n\nConfidence scores:\n- [b41] and [b36]: 0.9. The paper directly states that the effectiveness of existing SSL methods on graphs \"hinge on the specific strategies of generating positive examples and negative examples\", and cites [b41] and [b36] as key examples showing how different example generation strategies benefit different types of graph datasets. This indicates the proposed SHGP method is heavily motivated by the limitations of [b41] and [b36].\n\n- [b21]: 0.8. The LPA (label propagation algorithm) proposed in [b21] is a core component of the novel Att-LPA module in SHGP. The attention coefficients from the Att-HGNN module are combined with LPA to produce pseudo-labels.\n\n- [b2]: 0.7. The paper compares SHGP with DeepCluster [b2], highlighting that SHGP performs structural clustering directly in the graph space, while DeepCluster uses K-means clustering in the embedding space. This suggests [b2] provided key inspiration, even if the approach differs.\n\n- [b9]: 0.6. HuBERT [b9] is mentioned as an SSL approach for speech representation learning, in contrast to the HIN focus of SHGP. While not as direct an influence, it still seems to provide relevant context.\n\n- [b40]: 0.6. JOAO [b40] is described as exploring automatic selection of example generation strategies, which the authors argue is still not fully automatic, unlike SHGP. So [b40] helps motivate the need for SHGP.\n\nIn summary, based on direct statements about building on and differentiating from prior work, along with methodological similarities, these 6 references appear to be the most central, direct influences on the novel contributions of SHGP. The confidence scores reflect the degree of direct inspiration indicated in the text.", "6346f67490e50fcafd950453": "Here is my analysis of the most pertinent source papers for this paper, based on the given criteria:\n\n{\n  \"[b47]\": 0.9,\n  \"[b6]\": 0.8,\n  \"[b9]\": 0.7,\n  \"[b21]\": 0.7,\n  \"[b23]\": 0.7,\n  \"[b27]\": 0.6,\n  \"[b25]\": 0.5\n}\n\nConfidence scores:\n[b47] (Nimble): 0.9 \n- The paper makes several direct comparisons to Nimble, positioning it as the most recent state-of-the-art hybrid tiered memory system. The analysis shows that Nimble's batch migration and use of huge pages are detrimental under disaggregated memory latencies, directly motivating HotBox's design.\n\n[b6] (Thermostat): 0.8\n- The paper cites Thermostat's analysis of access frequency correlation within huge pages. While drawing a different conclusion, this directly relates to the paper's discussion of hotness fragmentation.\n\n[b9], [b21], [b23]: 0.7 each\n- These papers propose swap-based approaches for disaggregated memory. The analysis shows that a swap-only approach is inefficient under higher latencies compared to a hybrid approach, a key motivation for HotBox.\n\n[b27], [b25]: 0.6, 0.5\n- Cited as prior work on hybrid memory access for tiered memory systems. While relevant, the paper argues these approaches target lower latencies and are ill-suited for disaggregated memory.\n\nIn summary, [b47] and [b6] provide the most direct inspiration by analyzing key factors (batch migration, huge pages, access frequency) and showing their limitations in disaggregated memory - driving HotBox's novel design. The swap-based approaches in [b9], [b21], [b23] also strongly motivate the hybrid approach. The other hybrid systems [b27], [b25] are relevant but less central.", "6344dedd90e50fcafd24cdcd": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\nThe key novel contributions of this paper are:\n1. Exploring how to design augmented views of hypergraphs for contrastive self-supervised learning (HyperGCL). They propose two approaches:\n   a) Fabricated augmentations guided by domain knowledge, including perturbing hyperedges directly or perturbing \"edges\" in the converted bipartite graph, and vertex augmentations like dropping, attribute masking, and subgraph sampling.\n   b) Learning augmentations in a data-driven manner using a novel variational hypergraph auto-encoder architecture. They propose an end-to-end differentiable pipeline using Gumbel-Softmax to jointly learn augmentations and model parameters.\n2. Showing that hyperedge augmentations are more beneficial for hypergraphs compared to vertex augmentations, and generative learned augmentations outperform fabricated ones.\n3. Introducing robustness and fairness evaluation for hypergraph learning, and showing HyperGCL boosts both.\n\nThe key phrases indicating inspiration from previous work are:\n- \"Inspired by the emerging self-supervised learning on images/graphs [b10] [b11] [b12] [b13] [b14] [b15] , especially the contrastive approaches [b11] [b13] [b16] [b17] [b18] [b19] [b20] [b21] [b22] [b23] [b24] , we set out to leverage contrastive self-supervision to address the problem.\"\n- \"Motivated by [b11] [b13] that appropriate data augmentations suffice for the effective contrastive views...\"\n\nBased on this, the most pertinent source papers are:\n{\n  \"b11\": 0.9,\n  \"b13\": 0.9,\n  \"b41\": 0.7,\n  \"b42\": 0.7,\n  \"b27\": 0.6\n}\n\nb11 and b13 are highlighted as providing direct inspiration for using contrastive self-supervised learning and data augmentations. b41, b42 inspire the variational auto-encoder approach. b27 provides the key Gumbel-Softmax technique used.\n\nThe other referenced papers provide useful background and motivation, but are not as directly influential on the core novel contributions. Limiting to these 5 key papers keeps the focus on the most central references.", "633e476490e50fcafde590a1": "Here are the most relevant source papers for this paper, along with confidence scores, based on my analysis:\n\n{\n  \"[b3]\": 0.9,\n  \"[b20]\": 0.85, \n  \"[b19]\": 0.8,\n  \"[b7]\": 0.75,\n  \"[b6]\": 0.7,\n  \"[b9]\": 0.65,\n  \"[b37]\": 0.6\n}\n\nThe key factors leading to these papers being identified as the most relevant sources are:\n\n1. The paper is heavily inspired by and builds upon pretrained transformer language models for summarization ([b3], [b19], [b20]). These are foundational to the paper's approach.\n\n2. The paper directly addresses the two notions of unfaithfulness highlighted by Durmus et al. ([b7]) and evaluates faithfulness using the QuestEval metric from Scialom et al. ([b6]). This shows the paper's core contributions are motivated by these works.\n\n3. The contrastive learning approach in Cao et al. ([b9]) to improve faithfulness is discussed and compared against. \n\n4. The max-margin loss to prevent language model overconfidence is directly inspired by and similar to the approach used for machine translation in [b37].\n\n5. The paper's novel contributions center around the multi-task learning framework with auxiliary QA, QA-enhanced attention in the decoder, and max-margin loss. The identified sources lay the groundwork that enables these contributions.\n\nIn summary, the selected papers represent the core foundations this work builds upon and the key prior approaches it aims to improve, showing clear relevance to the main goals and techniques. The ordering and scores reflect the centrality of each paper to the novel aspects introduced here.", "632812a590e50fcafd1a6a98": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b16]\": 0.9,\n  \"[b18]\": 0.85,\n  \"[b21]\": 0.8,\n  \"[b20]\": 0.7,\n  \"[b17]\": 0.6\n}\n\nRationale:\n\nThe paper introduces CoAtGIN, a novel graph-based architecture for molecular property prediction. The key novel contributions are:\n\n1. Using k-hop convolution in the graph convolution network for faster message aggregation \n2. Applying a linear transformer for global message passing through the graph\n3. Combining the virtual node approach from GIN-Virtual [b18] with the linear transformer\n\nBased on this, the most relevant references that directly inspired or motivated these contributions are:\n\n- [b16] (confidence 0.9): The Graph Isomorphism Network (GIN) architecture, which the paper builds upon and extends. GIN is a core foundation of CoAtGIN.\n\n- [b18] (confidence 0.85): GIN-Virtual, which introduced the virtual node approach that is adopted in CoAtGIN alongside the linear transformer. \n\n- [b21] (confidence 0.8): The CosFormer linear transformer architecture, which is used in CoAtGIN for efficient global attention.\n\n- [b20] (confidence 0.7): Provides the embedding block used to initialize node embeddings in CoAtGIN.\n\n- [b17] (confidence 0.6): Graph Attention Networks (GAT), another key GNN architecture that likely inspired the use of attention in CoAtGIN, though less directly than the linear transformer.\n\nThe paper compares CoAtGIN to these key prior architectures like GIN, GIN-Virtual and GAT, indicating they were important references. The novel integration of k-hop message passing, a virtual node, and linear attention to jointly capture local and global graph structure appears to be the main contribution building on this prior work.", "634e194790e50fcafd24f33e": "Here is my analysis of the key source papers for this paper, based on the criteria you provided:\n\n{\n  \"[b35]\": 0.9,\n  \"[b43]\": 0.9,\n  \"[b79]\": 0.8,\n  \"[b39]\": 0.8,\n  \"[b65]\": 0.7,\n  \"[b14]\": 0.7\n}\n\nThe key points are:\n\n- The paper is heavily inspired by the parameter-efficient fine-tuning strategies using prompts in NLP [[b35]], and specifically the Visual Prompt Tuning (VPT) method [[b43]] which inserts learnable prompts for vision tasks. This directly motivated their approach.\n\n- The intuition behind their method comes from feature modulation methods like [[b79]] and [[b39]] which scale and shift features to handle distribution mismatch between datasets. This is the core novel idea of their SSF method.\n\n- They also draw inspiration from conditional normalization methods like FiLM [[b65]] for modulating features, though they use an input-independent approach.\n\n- The model re-parameterization technique they employ to merge scale and shift parameters into the original weights without extra inference cost is based on [[b14]].\n\nSo in summary, the prompt tuning approaches [[b35]], [[b43]], especially VPT, along with the feature modulation methods [[b79]], [[b39]], [[b65]] form the key inspiration and basis for the novel SSF method proposed in this paper to enable parameter-efficient fine-tuning for vision models. The model re-parameterization from [[b14]] is also critical for their efficient inference.", "62982a9a5aee126c0f6f5ecb": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b22]\": 0.9,\n  \"[b69]\": 0.8,\n  \"[b60]\": 0.7,\n  \"[b24]\": 0.6,\n  \"[b49]\": 0.5\n}\n\nConfidence scores:\n[b22] (0.9): The paper takes direct inspiration from the \"Noisy Nodes\" approach proposed in this reference for using denoising as an auxiliary task to improve molecular property prediction. The authors build upon this idea to instead use denoising as a pre-training objective.\n\n[b69] (0.8), [b60] (0.7), [b24] (0.6): These references establish the connection between denoising and score matching that the authors leverage to show that their denoising objective corresponds to learning a particular force field. This forms the key theoretical justification for why denoising structures is an effective pre-training approach.\n\n[b49] (0.5): The Graph Network Simulator (GNS) architecture proposed in this reference is one of the two key model architectures the authors experiment with and enhance via pre-training. Improving GNS is one of the main novel contributions.\n\nThe paper introduces denoising 3D molecular structures as a new self-supervised pre-training approach to improve downstream property prediction, inspired by [b22] but applied in a novel way. The authors justify this approach by connecting denoising to force field learning using results from [b69], [b60], [b24]. Experimenting with the GNS architecture from [b49] and an improved variant is another key contribution. Overall, these 5 references appear to be the most influential in directly motivating and enabling the main ideas and results of this paper.", "63520de890e50fcafd60f43e": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b10]\": 0.9,\n  \"[b44]\": 0.8,\n  \"[b7]\": 0.7,\n  \"[b19]\": 0.6,\n  \"[b16]\": 0.5\n}\n\nConfidence scores:\n[b10] (0.9): The paper makes multiple references to \"prompt tuning\" and directly compares their method to it, indicating it was a key inspiration and baseline. Phrases like \"prompt tuning suffers from lower performance and convergence rate\" and \"To verify the impact of the propagation distance on performance, we conduct pilot experiments by shortening it in Section 4 and find that the performance first increases then decreases with the shortening of the length. This finding inspires us to present the late prompt\" show the heavy influence of [b10] in motivating their work.\n\n[b44] (0.8): This reference is mentioned several times in the context of inserting prompts into hidden layers and as a method the authors aim to improve upon. The mutual information estimation method is also credited to this paper. \n\n[b7] (0.7): Adapter modules are discussed as an alternative parameter-efficient tuning approach that the authors compare their method against. The bottleneck architecture used in their prompt generator is inspired by the adapter architecture.\n\n[b19] (0.6): This work on compressing adapters is referenced as an advanced adapter-based method that outperforms basic prompt tuning which the authors aim to improve.\n\n[b16] (0.5): Mentioned alongside [b10] as an early prompt tuning method that inserts soft prompts only into the word embedding layer, which has limitations the authors attempt to overcome.\n\nThe paper's key novel contributions appear to be 1) Analyzing the propagation distance trade-off in prompt tuning and introducing late prompts, and 2) Combining late prompts with instance-aware prompt generation. References [b10], [b44], [b7], [b19] and [b16] are the most relevant to these aspects based on the text. The other references provide useful background but are less central to the main innovations.", "6327dda690e50fcafd67dfe7": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b29]\": 0.9,\n  \"[b39]\": 0.8,\n  \"[b47]\": 0.7,\n  \"[b21]\": 0.7,\n  \"[b38]\": 0.6\n}\n\nConfidence scores:\n[b29]: 0.9 - This reference is directly cited multiple times when discussing the \"feature suppression issue\" which the proposed GraphCV method aims to address. The paper states that GraphCV can \"reduce redundant and biased information without sacrificing the sufficiency of the predictive graph features\" to \"reconcile the robustness and sufficiency of the learned representation\", directly inspired by [b29].\n\n[b39]: 0.8 - The key assumption that the \"most salient sub-structure is sufficient to make rational and correct label identification\" which recent works build upon is attributed to reference [b39]. This assumption motivates the GraphCV approach.\n\n[b47], [b21]: 0.7 - These two references propose using \"automated augmentations to extract the invariant rationale features\", which the authors build upon but identify limitations of. Addressing the limitations of these prior works is a key motivation for GraphCV.\n\n[b38]: 0.6 - This reference introduced the information bottleneck (IB) principle to graph learning, showing representations guided by IB are more robust and transferable. The IB principle is a key guiding framework that GraphCV aims to optimize for.\n\nThe paper directly states inspirations from and builds closely upon the ideas in [b29], [b39], [b47] and [b21] to motivate the GraphCV approach. [b38] provides the theoretical IB framework GraphCV optimizes for. Other references are either more peripheral or provide background context.", "63896cd690e50fcafde7a0f0": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b2]\": 0.9,\n  \"[b4]\": 0.8,\n  \"[b6]\": 0.7,\n  \"[b24]\": 0.6\n}\n\nConfidence scores:\n[b2] (0.9): This reference is preceded by the phrase \"To enlarge the maximum acceptable input length of pre-trained models, longformer encoder-decoder [b2] uses a local windowed attention mechanism with a task-guided global attention to make the consumption scales linearly as input length.\" This indicates the paper is directly inspired by and builds upon the longformer encoder-decoder architecture from reference [b2] to handle long input documents, which is a key challenge and novel contribution of the paper.\n\n[b4] (0.8): The paper introduces its new long-document summarization dataset Perseus, and directly compares it to the arXiv and PubMed datasets from reference [b4], stating \"Cohan et al. [b4] present two large-scale long-document summarization datasets, arXiv and PubMed, whose data are collected from the corresponding scientific paper websites, arXiv.org and PubMed.com.\" This shows [b4] is a key prior work for long-document summarization datasets that inspired Perseus.\n\n[b6] (0.7): Along with [b24], reference [b6] is mentioned as providing sparse attention mechanisms that can efficiently process long sequences, which relates to a key challenge of handling long input documents. The paper states \"Besides, there are many other sparse attention mechanisms [b6] [b24] that can efficiently process long sequences.\"\n\n[b24] (0.6): Grouped with [b6] as described above as prior work on efficient attention mechanisms for long sequences. While relevant, [b24] seems less central than [b2] and [b4] in directly inspiring the main contributions of the paper.\n\nIn summary, references [b2] and [b4] appear to be the most pertinent source papers, as they are directly called out as inspiration for the longformer architecture and long-document dataset used in this work. References [b6] and [b24] are also relevant in providing prior approaches for a key challenge of handling long input sequences, but are less critical to the paper's main novel contributions.", "62a165475aee126c0f509e38": "Here is my attempt at identifying the most pertinent source papers for this paper, in JSON format with confidence scores:\n\n{\n  \"[b45]\": 0.9,\n  \"[b52]\": 0.85, \n  \"[b14]\": 0.8,\n  \"[b40]\": 0.75,\n  \"[b23]\": 0.7,\n  \"[b33]\": 0.65,\n  \"[b39]\": 0.6,\n  \"[b12]\": 0.55\n}\n\nExplanation:\n\nThe key novel contributions of this paper seem to be:\n1) Proposing a self-supervised learning framework for non-homophilous graphs that leverages both local structure and global semantic information, without relying on graph augmentations \n2) Developing a variational inference based training algorithm to infer latent variables and learn encoder parameters\n3) Theoretically analyzing how the learned representations can achieve better downstream performance\n\nBased on this, the most relevant prior works appear to be:\n\n[b45] DGI - A key contrastive graph SSL baseline this paper compares against. The proposed method aims to improve upon contrastive methods like DGI for non-homophilous graphs.\n\n[b52] GraphCL, [b14] MVGRL, [b40] BGRL - Other recent contrastive graph SSL methods used as baselines. This paper highlights how these rely on graph augmentations which are difficult for non-homophilous graphs.\n\n[b23] GAE/VGAE - Variational autoencoder based graph embedding methods. The variational inference approach in this paper is inspired by these.\n\n[b33] DeepWalk, [b39] LINE, [b12] GraphSAGE - Influential unsupervised node embedding methods based on random walks and link reconstruction. This paper discusses their limitations in capturing structural info compared to contrastive SSL.\n\nAmong these, [b45] DGI seems to be the most relevant as a contrastive SSL baseline this paper directly aims to improve upon for non-homophilous graphs. The variational approach is also closely tied to [b23] GAE/VGAE.", "62393e7f5aee126c0f1260e9": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\nThe key novel contributions of this paper seem to be:\n1) Proposing QCPG, a quality controlled paraphrase generation model that can generate paraphrases conforming to specified semantic similarity, syntactic distance, and lexical distance constraints. The constraints are simpler than previous control mechanisms like parse trees or keyword lists.\n2) Introducing a method to predict the optimal control values to input to QCPG for a given sentence, to maximize the quality of the generated paraphrases.\n\nLooking at key phrases that indicate inspiration from or strong reliance on previous work:\n- \"inspired by [b15] we choose q syn (s, s \u2032 ) to be the normalized tree edit distance\" - This indicates the syntactic distance metric is inspired by reference [b15].\n- The paper mentions using Bleurt [b31] as the semantic similarity metric q sem , after finding it has the highest correlation with human judgments.\n\nExamining the methodology, the key components are:\n1) The QCPG encoder-decoder model for controlled paraphrase generation \n2) The QP model for predicting optimal control values\n3) The quantitative metrics used to measure semantic similarity, syntactic distance and lexical distance between paraphrases\n\nThe most relevant references that directly contribute to these novel aspects seem to be:\n- [b15] which inspires the syntactic tree edit distance metric\n- [b31] Bleurt, which is used as the semantic similarity metric\n\nBased on the criteria, here is my assessment of the most central source papers, with confidence scores:\n\n{\n  \"[b15]\": 0.8,\n  \"[b31]\": 0.7\n}\n\nThe other references like [b3], [b7], [b21] etc. provide useful background and motivation but do not seem to be as directly linked to the main novel contributions. The confidence scores reflect that [b15] and [b31] appear to most heavily influence key aspects of the methodology, while acknowledging some uncertainty without being able to fully examine the reference papers.", "630ed16690e50fcafd793a2d": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"b65\": 0.9,\n  \"b85\": 0.9,\n  \"b72\": 0.8,\n  \"b77\": 0.7,\n  \"b57\": 0.6,\n  \"b93\": 0.6\n}\n\nConfidence scores:\n- [b65] and [b85]: 0.9. The paper directly states \"Compared to previous works that only exploit the intra-tensor adaptivity [b65], [b72], [b85] or inter-tensor adaptivity [b72], [b77], the ANT framework can achieve both with high hardware efficiency.\" This indicates [b65] and [b85] were key inspirations for the intra-tensor adaptivity aspect of ANT.\n\n- [b72]: 0.8. Also mentioned in the quote above as inspiration for both intra-tensor and inter-tensor adaptivity. The paper later states \"Our evaluation results show that the ANT-based accelerator surpasses the existing mixed-precision accelerator BitFusion [b72] by 2.8\u00d7 performance improvement and 2.5\u00d7 energy reduction\", further confirming [b72] as a key reference point.\n\n- [b77]: 0.7. Mentioned alongside [b72] as prior work on inter-tensor adaptivity that ANT builds upon. The AdaptiveFloat type from [b77] is also discussed in the background.\n\n- [b57] and [b93]: 0.6. The paper states \"We build upon the previous works like PoT (i.e., power of two) type [b57], [b93], and show how to integrate them into our ANT framework\", indicating these PoT papers provided a foundation but were not the most central inspiration.\n\nIn summary, [b65], [b85], [b72] and [b77] appear to be the most influential prior works that directly motivated the novel ANT framework proposed in this paper, by providing key building blocks in intra-tensor adaptivity, inter-tensor adaptivity, and mixed precision that were expanded upon. The PoT works [b57] and [b93] were also integrated but seem less central to the main contributions.", "62a2b6955aee126c0f4d8e7b": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b19]\": 0.9,\n  \"[b17]\": 0.8,\n  \"[b18]\": 0.8,\n  \"[b4]\": 0.7\n}\n\nConfidence scores:\n[b19] (Visual Prompt Tuning, VPT): 0.9\n- The paper makes extensive comparisons to VPT and incorporates it as a key component in the proposed NOAH framework. VPT is directly used for developing the main contributions.\n\n[b17] (Adapter): 0.8\n[b18] (LoRA): 0.8  \n- Both Adapter and LoRA are incorporated into NOAH alongside VPT as the main prompt modules that NOAH searches over and combines. The paper's methodology is heavily based on integrating these two approaches.\n\n[b4] (AutoFormer): 0.7\n- The evolutionary neural architecture search algorithm from AutoFormer is adapted to search for the optimal combination of prompt modules in NOAH. This forms an important part of the paper's technical approach.\n\nThe paper aims to automatically search for the optimal prompt module design that combines Adapter, LoRA and VPT, motivated by the observation that none of the three dominates performance across diverse datasets. This search-based methodology, building directly upon those three approaches, is the key novel contribution. AutoFormer provides the search algorithm used to implement this idea. Therefore, these 4 references appear to be the most central influences based on the criteria of being directly linked to the main challenges, methods and contributions of the paper.", "6389d6b590e50fcafdfead12": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b6]\": 0.9,\n  \"[b16]\": 0.8,\n  \"[b28]\": 0.7,\n  \"[b0]\": 0.6,\n  \"[b2]\": 0.6,\n  \"[b3]\": 0.6,\n  \"[b11]\": 0.6\n}\n\nThe key novel contributions of this paper are:\n1) Providing an overview of different categories of instruction fusion and their limitations \n2) Characterizing opportunities for non-consecutive and non-contiguous memory fusion, showing that focusing on memory \u00b5-ops provides the best return on investment\n3) Proposing the Helios microarchitecture to enable non-consecutive memory fusion while handling challenges like dependencies between fused instructions\n\nThe paper seems to take the most direct inspiration from:\n\n- [b6] which introduces fusion idioms for RISC-V. This paper builds heavily on [b6]'s work by exploring additional memory fusion opportunities beyond what [b6] proposed.\n\n- [b16] and [b28] which considered fusing non-consecutive memory accesses, but only for contiguous accesses and with specific catalysts. This paper extends those ideas to non-contiguous accesses.\n\n- [b0], [b2], [b3], [b11] which focus on consecutive and contiguous fusion in commercial processors. The paper contrasts its techniques with the more limited fusion in those prior works.\n\nSo in summary, [b6], [b16] and [b28] seem to be the pioneering works that most directly motivated this paper's novel contributions in aggressive non-consecutive and non-contiguous memory operation fusion. The other references provide important context but the paper's techniques go significantly beyond them.", "63608e5090e50fcafdee1257": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b27]\": 0.9,\n  \"[b33]\": 0.8,\n  \"[b32]\": 0.8,\n  \"(Hu et al., 2021)\": 0.8,\n  \"[b55]\": 0.7,\n  \"[b64]\": 0.6,\n  \"[b61]\": 0.5\n}\n\nRationale:\n\nThe paper introduces Efficient Head Finetuning (EH-FT), a two-stage finetuning method that uses parameter-efficient tuning in the first stage to learn a good classification head, before fully finetuning the model in the second stage.\n\nThe key novel contributions are:\n1. Using parameter-efficient tuning methods like BitFit, LoRA and Prefix-Tuning to learn a good classification head in stage 1, instead of linear probing used in LP-FT [b27]. This allows the head to adapt to the downstream task even if it differs from the pretraining task.\n\n2. Showing that the parameter-efficient tuned head guides the backbone to a nearby optimal point with little change during the full finetuning in stage 2. This prevents catastrophic forgetting.\n\nThe most pertinent references based on the criteria are:\n\n[b27] - LP-FT, which the proposed EH-FT method builds upon and improves. The paper discusses the limitations of LP-FT in detail.\n\n[b33], [b32], (Hu et al., 2021) - Parameter-efficient tuning methods Prefix-Tuning and LoRA that are used for the stage 1 tuning in EH-FT and enable learning a good head.\n\n[b55] - A study showing parameter-efficient tuning restricts trainability and helps overcome catastrophic forgetting, supporting the key principle behind why EH-FT works.\n\n[b64], [b61] - Other finetuning strategies that are compared against but noted to be compatible with EH-FT.\n\nThe paper directly states being inspired by LP-FT [b27] and using parameter-efficient tuning methods [b33], [b32], (Hu et al., 2021). The other references provide important supporting evidence and comparisons. Overall this list focuses on the central novel aspects of the EH-FT method proposed in the paper.", "628ef0495aee126c0f82db2f": "Here is my analysis of the most pertinent source papers for the given text, based on the outlined criteria:\n\n{\n  \"[b32]\": 0.9,\n  \"[b28]\": 0.8,\n  \"[b17]\": 0.7,\n  \"[b24]\": 0.6,\n  \"[b30]\": 0.5\n}\n\nExplanation:\nThe paper focuses on addressing the false negative problem in the DocRED dataset for document-level relation extraction. The key novel contributions are:\n\n1. Identifying the false negative problem in DocRED due to incomplete annotations from the recommend-revise scheme and logical inconsistencies \n2. Proposing an iterative human-in-the-loop approach to revise DocRED and recover the incomplete annotations\n3. Providing a high-quality revised version of DocRED called Re-DocRED\n4. Proposing new evaluation metrics for more comprehensive assessment of document-level RE\n\nThe most relevant references based on the criteria are:\n\n- [b32] and [b28] are referenced multiple times when discussing the DocRED dataset and seem to be the foundation this work builds upon. The paper aims to address limitations in DocRED identified by analyzing top performing models like ATLOP [b32].\n\n- [b17] is a very recent work that also identifies the false negative issue in DocRED. This paper draws direct comparisons to [b17], highlighting how their approach improves upon it in dataset size, precision, addressing incompleteness, and scalability. \n\n- [b24] is relevant as it extended a previous TACRED re-annotation effort [Alt et al 2020] which is used as an example of prior work on revising sentence-level RE datasets.\n\n- [b30] introduces the TACRED dataset which is used in the preliminary analysis comparing sentence and document level RE.\n\nThe other references like [b5], [b15], [b11], [b19], [b8], [b6] are cited but do not seem to be as central to the main contributions. Overall this prioritizes the references that directly motivate and compare to the proposed DocRED revision approach.", "622819cdd18a2b26c7ab496a": "Here are the key source papers that likely inspired this work, based on my analysis:\n\n{\n  \"[b39]\": 0.9,\n  \"[b7]\": 0.8,\n  \"[b27]\": 0.7,\n  \"[b14]\": 0.6,\n  \"[b9]\": 0.5,\n  \"[b1]\": 0.5\n}\n\nThe key points:\n\n- The paper makes direct comparisons to XLNet [b39], BERT [b7], and T5 [b27], highlighting how GLM addresses limitations of those models. This suggests those works were major inspirations and baselines.\n\n- SpanBERT [b14] is referenced as using a different positional encoding scheme than GLM. This indicates SpanBERT's span-based pretraining was an important prior work. \n\n- UniLM [b9] and UniLMv2 [b1] are discussed as methods that combine autoencoding and autoregressive pretraining, which GLM aims to unify in a more effective way. This shows UniLM was an influential precursor.\n\n- The high frequency of comparisons, along with phrases like \"instead\" and \"compared with\", point to these papers as the primary inspirations that GLM builds upon and aims to improve.\n\nThe scores represent my confidence that each paper was a central inspiration, based on the directness of comparisons and similarities in methodology. Let me know if you need any clarification or have additional questions!", "623d90d46750f864fe4cafd6": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b13]\": 0.9,\n  \"[b14]\": 0.85,\n  \"[b17]\": 0.8,\n  \"[b25]\": 0.75,\n  \"[b30]\": 0.75,\n  \"[b38]\": 0.7,\n  \"[b42]\": 0.8\n}\n\nConfidence scores:\n- [b13], [b14], [b17]: These papers are frequently mentioned when discussing existing work on GNNs for inductive node classification, which is the key problem this paper aims to address. The authors note the limitations of these GNN approaches in ignoring joint label dependency.\n- [b25], [b30]: Directly mentioned as prior work that parameterizes CRF potentials using GNNs, which is similar to the proposed SPN approach. However, the optimization methods differ.\n- [b38], [b42]: Key references for the theoretical foundation of SPNs. [b38] discusses the challenges in CRF learning that SPN aims to overcome. [b42] provides the inspiration for the SPN proxy optimization based on pseudomarginals.\n\nI focused on references that were directly tied to the key novel aspects of SPNs:\n1) Combining GNNs and CRFs to enable structured prediction for inductive node classification \n2) Using a proxy optimization based on pseudomarginals to enable more efficient and stable CRF learning compared to prior approaches\n\nThe selected references provide important context for these contributions. I limited the list to the most salient references to keep the focus on the core influences.", "634d805690e50fcafd4e07bc": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\nThe paper introduces DIRECTCXL, a novel approach for memory disaggregation using the new CXL interconnect standard. The key novel contributions are:\n\n1. Demonstrating the first real system implementation of CXL 2.0 for memory disaggregation\n2. Designing a CXL switch and network topology to connect hosts to remote memory resources\n3. Implementing a software runtime to expose the remote CXL memory to applications\n\nBased on this, the most relevant references that likely inspired this work are:\n\n{\n  \"[b17]\": 0.9,\n  \"[b18]\": 0.8,\n  \"[b19]\": 0.7,\n  \"[b20]\": 0.7,\n  \"[b21]\": 0.7\n}\n\n[b17]-[b21] are the only references that directly mention CXL and anticipate its potential for memory disaggregation with low latency. The paper builds upon the concepts introduced in these pioneering CXL works.\n\nThe other references, while providing background on existing RDMA-based memory disaggregation approaches, do not seem to be as directly influential on the novel CXL-based methods introduced here. The paper's key contributions are centered around leveraging CXL in new ways, rather than incrementally improving RDMA techniques.\n\nBy focusing on the specific mentions of CXL and prioritizing references that share the paper's aim of using CXL for efficient memory disaggregation, this concise set of references likely captures the prior work that most directly inspired and enabled the novel contributions of this paper. The high confidence scores reflect the close alignment between these references and the paper's central themes.", "621ee1845aee126c0f26a9df": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b36]\": 0.95,\n  \"[b10]\": 0.8,\n  \"[b11]\": 0.8,\n  \"[b12]\": 0.8,\n  \"[b29]\": 0.7,\n  \"[b46]\": 0.65,\n  \"[b47]\": 0.65,\n  \"[b48]\": 0.65,\n  \"[b49]\": 0.65\n}\n\nThe key novel contributions of this paper are:\n1) Proposing the first full-stack CXL-based memory disaggregation design that is practical for broad deployment in public clouds\n2) Introducing techniques at the hardware, system software, and distributed system layers to enable disaggregation while preserving customer inertia and compatibility with virtualization accelerators\n3) Developing prediction models for VM memory latency sensitivity and frigid memory to intelligently schedule VMs on pooled vs local memory\n\nThe paper indicates a very strong reliance on the emerging CXL interconnect standard [[b36]] to enable fast, native load/store access to disaggregated memory in a deployable way. This is the key foundation that makes their disaggregation approach possible.\n\nThe paper also builds upon prior work on memory disaggregation in general [[b10]] [[b11]] [[b12]], but adapts the concepts to the specific constraints and requirements of the public cloud environment.\n\nIn discussing related work, the paper notes that page migration techniques used in prior two-tier memory systems [[b29]] [[b46]] [[b47]] [[b48]] [[b49]] are not applicable in their setting due to the need to preserve compatibility with virtualization accelerators. This indicates those works provided important context, but the paper had to develop new techniques to achieve its goals.\n\nSo in summary, [b36] describing CXL is the central enabling foundation, while [b10-b12] on disaggregation and [b29, b46-b49] on two-tier memory provide key background concepts and motivation, but the paper introduces novel techniques building on top of those to enable a practical public cloud solution. The confidence scores reflect the centrality of each reference to the main contributions.", "63438d2990e50fcafd4ebd66": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"[b43]\": 0.9,\n  \"[b18]\": 0.8,\n  \"[b27]\": 0.7,\n  \"[b33]\": 0.6,\n  \"[b34]\": 0.5\n}\n\nRationale and confidence scores:\n\n[b43] (confidence 0.9): This reference is mentioned multiple times in key phrases that indicate direct inspiration and reliance, such as \"We conduct a sensitivity analysis of different mapping axes to understand the contribution of each axis.\" and \"In this work, we use Timeloop [b43] as our cost model\". The paper seems to build heavily upon the map space representation and random search techniques from [b43].\n\n[b18] (confidence 0.8): The gradient-based search methods from this reference are analyzed in depth as one of the three main categories of mappers compared in the paper. Phrases like \"gradient-based [b18] (i.e., surrogate models)\" suggest the novel mapper comparison draws significant motivation from [b18].\n\n[b27] (confidence 0.7): Similar to [b18], the feedback-based search methods from this reference form another key category analyzed in the mapper comparison, which is a main contribution of the paper.\n\n[b33] [b34] (confidence 0.5-0.6): These references are mentioned a few times in the context of map space representations and cost models, which are important components leveraged in the paper's methodology. However, the reliance seems less direct compared to [b43], [b18] and [b27].\n\nThe paper introduces novel techniques like warm-start initialization and sparsity-aware search, but these build upon insights from the in-depth analysis of prior mapper categories. So the references corresponding to those key mapper categories analyzed appear to be the most central influences on the main contributions.", "63a413f790e50fcafd6d1f93": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b34]\": 0.9,\n  \"[b25]\": 0.8,\n  \"[b15]\": 0.7,\n  \"[b21]\": 0.6\n}\n\nConfidence scores:\n- [b34] (score 0.9): The paper makes direct reference to Transformers ([b34]) as inspiration, stating \"The model defined by Eqn. 1 follows the spirit of Transformers [b34] (where in particular \u00c3(l) can be seen as an attentive graph) that potentially enables message passing between any node pair in each layer\". This indicates the Transformer architecture was a key motivation.\n\n- [b25] (score 0.8): The random feature (RF) approximation used is directly based on [b25], as evident from \"The kernel function can be further approximated by random features (RF) [b25] which serves as an unbiased estimation\". This reference provides the foundation for the kernelized message passing.\n\n- [b15] (score 0.7): The reparameterization trick from [b15] is used to enable differentiable optimization of discrete graph structures, as stated \"Fortunately, we notice that the Eqn. 3 can be modified to incorporate the reparametrization trick [b15] to allow differentiable learning\".\n\n- [b21] (score 0.6): The Gumbel-Softmax technique, a key component of the proposed differentiable stochastic structure learning, references [b21] in \"\u03c4 controls the closeness to hard discrete samples [b21].\"\n\nThe titles of these referenced papers also align well with the key novel aspects of NODEFORMER. In contrast, many of the other references, especially those in the introduction, provide more general background and motivation rather than directly inspiring the core technical contributions. Let me know if you would like me to elaborate on the reasoning further.", "632bd2a990e50fcafdb7a24d": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b21]\": 0.9,\n  \"[b17]\": 0.85,\n  \"[b2]\": 0.7,\n  \"[b36]\": 0.65,\n  \"[b31]\": 0.6,\n  \"[b15]\": 0.55\n}\n\nConfidence scores:\n[b21] and [b17]: These papers are referenced multiple times when discussing the drawbacks of existing dense retrieval methods and the retrieve-then-read pipeline that the proposed method aims to improve upon. The authors directly compare their approach to these previous works.\n\n[b2]: This paper is mentioned as a related work that also generates identifier strings for documents, but has some downsides that the proposed method overcomes. \n\n[b36]: Referenced as a previous work showing knowledge can be retrieved from language models through direct generation, which relates to a key aspect of the proposed method.\n\n[b31]: Shares the idea of leveraging a large language model to generate relevant contexts, but the proposed method extends this in novel ways for knowledge-intensive tasks specifically.\n\n[b15]: Nucleus sampling from this paper is directly used as a component of the proposed method to increase diversity of generated documents.\n\nI focused on papers that were referenced in direct relation to the core novel contributions - using a large language model to generate diverse contextual documents for knowledge-intensive tasks in a generate-then-read pipeline. The papers given the highest confidence scores were those positioned as direct inspiration or key related works that the proposed method builds upon and compares to.", "62fa0d1390e50fcafd246228": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b15]\": 0.9,\n  \"[b8]\": 0.8,\n  \"[b0]\": 0.7,\n  \"[b19]\": 0.7,\n  \"[b36]\": 0.6\n}\n\nThe key novel contributions of this paper seem to be:\n1) Proposing a new multiplex heterogeneous graph neural network MHGCN that can automatically capture useful relation-aware topological signals for heterogeneous network embedding\n2) Integrating both network structures and node attributes in the representations \n3) Efficiently learning representations with a simplified convolution-based message passing mechanism\n\nThe most relevant references that directly inspire or motivate these contributions are:\n\n[b15] (score 0.9) - This work by Kipf et al. introduces the key idea of performing convolutional operations over graph neighborhoods for information aggregation, which is the foundation of the graph convolution module in MHGCN.\n\n[b8] (score 0.8) - LightGCN simplifies the design of graph convolutional networks by omitting embedding projection with non-linearity during message passing. MHGCN adopts a similar simplified convolution approach for efficiency.\n\n[b0] (score 0.7) - GATNE tackles multiplex network embedding by learning different embeddings for nodes and edges. The multiplex relation aggregation in MHGCN is inspired by differentiating edge types as in GATNE.\n\n[b19] (score 0.7) - FAME aggregates information from multiplex sub-networks while preserving relation-aware node dependencies. MHGCN's approach of generating sub-graphs for each edge type and aggregating them with learned importance weights is motivated by FAME.\n\n[b36] (score 0.6) - Dual-HGCN generates homogeneous hypergraphs from the multiplex bipartite graph and propagates information with hypergraph convolutions. While the overall approach is quite different, considering the multiplex bipartite structure provides relevant context.\n\nThe key phrases \"inspired by\" and \"motivated by\" directly link [b0] and [b19] as strong influences. The methodological similarities with [b15], [b8] and [b36] in terms of graph convolutions and handling of multiplex graph structures make them highly relevant to the core contributions, even without explicit attribution.", "634d805490e50fcafd4e02c8": "Here is my analysis of the most pertinent source papers for the given paper, in JSON format with confidence scores:\n\n{\n  \"[b5]\": 0.95,\n  \"[b0]\": 0.85, \n  \"[b12]\": 0.7,\n  \"[b13]\": 0.65,\n  \"[b17]\": 0.6,\n  \"[b18]\": 0.55,\n  \"[b19]\": 0.5\n}\n\nReasoning:\n\nThe paper introduces a novel contrastive learning based framework for learning representations of gate-level netlists that capture their boolean functionality. The key novel contributions are:\n\n1. Applying contrastive learning to netlists to extract functional semantics \n2. Designing a customized GNN architecture (FGNN) that learns independent aggregators for each gate type to capture logic functionality\n\n[b5] is likely the most pertinent reference, as the paper directly builds upon and compares to the DAG-customized GNN from [b5]. It is mentioned multiple times including \"Motivated by [b5], our proposed FGNN follows an asynchronous message passing scheme.\" This indicates [b5] provided key inspiration.\n\n[b0] is also highly relevant, as it proposes using GNNs for netlist representation learning, which this paper directly builds upon: \"In recent years, the fast-growing deep neural network techniques have shown great power in netlist representation learning...Ma et al. [b0] propose an iterative process to insert observation points into gate-level netlists based on the node representations learned by a graph neural network.\"\n\n[b12] and [b13] are pertinent as they propose customized GNN architectures for DAGs, which is similar to the FGNN methodology introduced here, although this work focuses more on capturing functionality.\n\n[b17], [b18] and [b19] are relevant references from the contrastive learning domain that likely inspired the contrastive learning framework proposed in this paper for netlists. However, they are less directly related to the core contributions compared to the GNN architecture references.\n\nIn summary, [b5] and [b0] are assessed as the most central references that the novel contributions directly build upon, while the other references provided important inspiration and background context. The confidence scores reflect the assessed degree of direct relevance and inspiration each reference provided for this paper's methods.", "63a413f790e50fcafd6d24b3": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b0]\": 0.9,\n  \"[b1]\": 0.9, \n  \"[b2]\": 0.8,\n  \"[b7]\": 0.95,\n  \"[b8]\": 0.95,\n  \"[b9]\": 0.85,\n  \"[b10]\": 0.9,\n  \"[b11]\": 0.8\n}\n\nThe key points that led me to select these references as the most pertinent source papers are:\n\n1. The paper focuses on the novel contribution of learning invariant graph representations under distribution shifts, when the graph data comes from a mixture of latent environments without explicit environment labels. References [b7], [b8], [b9] and [b10] directly inspire and motivate this work on invariant learning, even though they focus on images and text rather than graphs. The authors aim to extend the invariant learning methodology to graphs.\n\n2. The proposed Graph Invariant Learning (GIL) method identifies invariant subgraphs and uses them to infer environment labels and learn invariant representations. The use of GNNs in modules like invariant subgraph identification ([b0], [b1], [b2]) and the theoretical formulation of finding a maximal invariant subgraph generator ([b11]) are directly relied upon for the core technical contributions.\n\n3. Phrases like \"invariant learning, which aims to exploit the invariant relationships between features and labels across different distributions while disregarding the variant spurious correlations, can provably achieve satisfactory OOD generalization under distribution shifts [b7] [b8] [b9]\" and \"we make an assumption on \u03a6(G) as follows: The invariance assumption means that there exists a subgraph generator such that it can generate invariant subgraphs across different environments. [b11]\" clearly indicate the direct inspiration drawn from these works.\n\nIn summary, while the paper references many other works, the selected 8 references emerge as the most central, based on their strong conceptual and methodological links to the novel invariant graph learning approach proposed in this paper to tackle the challenge of OOD generalization. The authors directly build upon the key ideas and techniques from these works.", "62d16e8a5aee126c0fd6847a": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"[b77]\": 0.9,\n  \"[b27]\": 0.7,\n  \"[b63]\": 0.7,\n  \"[b15]\": 0.6\n}\n\nThe key reasons for selecting these papers as the most pertinent sources are:\n\n1. The paper heavily relies on and extends the authors' previous work on Elastic Cuckoo Page Tables (ECPTs) from reference [b77]. The nested translation design proposed in this paper is a direct extension of the native ECPT design from [b77]. This is evident from statements like \"We base our design on ECPTs, since past work has shown that they are a competitive design in a native environment.\" and frequent references to design elements from [b77] throughout the paper.\n\n2. The ECPT design used as the foundation resolves hash collisions using cuckoo hashing, which is based on the seminal works in references [b27] and [b63]. Without the cuckoo hashing techniques from these papers, the ECPT design would suffer from the traditional shortcomings of hashed page tables.\n\n3. The paper introduces a new Shortcut Translation Cache (STC) which \"caches the translations of gCWT data in a manner logically similar to how the Nested TLB [b15] caches the translations of radix page tables.\" This shows the STC is directly inspired by the functionality of the Nested TLB structure proposed in [b15].\n\nIn summary, [b77] provides the foundational ECPT design that is extended to the nested translation context, [b27] and [b63] enable the core cuckoo hashing approach used in ECPTs, and [b15] inspires the key STC structure introduced to optimize nested translations. These papers appear to be the most central influences on the novel contributions of this work.", "628704275aee126c0f5b583a": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b15]\": 0.9,\n  \"[b3]\": 0.8,\n  \"[b4]\": 0.7,\n  \"[b46]\": 0.6,\n  \"[b33]\": 0.5,\n  \"[b34]\": 0.4,\n  \"[b47]\": 0.3\n}\n\nConfidence scores:\n- [b15]: 0.9 - This reference is directly cited when discussing the problem of 2D page walks in virtualized environments causing significant page walk delays. It appears to be a key motivation for the paper's work on reducing page walk latency.\n\n- [b3]: 0.8 - Proposed using flat host page tables for virtual machine memory, which relates closely to this paper's approach of flattening page tables, especially for virtualized systems. \n\n- [b4]: 0.7 - Compendia is a parallel work on flattening page tables that this paper directly compares its approach and results to. It is highly relevant as prior art.\n\n- [b46], [b33], [b34], [b47]: 0.6-0.3 - These references propose techniques (POM_TLB, CSALT, ASAP, Elastic Cuckoo Hashing) to directly reduce the cost of page walks. The paper discusses them in detail, comparing and contrasting with its own approach of combining page table flattening and cache prioritization.\n\nThe paper does not appear to use phrases directly indicating inspiration like \"inspired by\". However, the related work section extensively discusses and compares with these prior techniques, suggesting they were influential in developing this paper's novel contributions of flattening page tables and prioritizing caching page table entries. The references selected relate most closely to the paper's core ideas.", "628d27f85aee126c0f4de037": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"b14\": 0.95,\n  \"b8\": 0.85,\n  \"b13\": 0.8,\n  \"b6\": 0.7,\n  \"b11\": 0.65\n}\n\nRationale and key observations:\n\n- The paper introduces Shotgun, a new BTB-directed front-end prefetcher. It is heavily inspired and motivated by the limitations of the state-of-the-art BTB-directed prefetcher Boomerang [b14]. Boomerang is directly compared against and Shotgun builds upon its key ideas. Therefore, [b14] is the most pertinent source.\n\n- Shotgun also draws significant inspiration from the original fetch-directed instruction prefetching (FDIP) work [b8]. The limitations of FDIP in handling BTB misses motivated Shotgun's new BTB organization. \n\n- Shotgun incorporates some features from the Confluence prefetcher [b13], specifically its proactive BTB fill mechanism. While not the primary inspiration, [b13] still notably influences Shotgun's design.\n\n- The paper discusses temporal streaming prefetching techniques [b6] [b11] as important prior work. While these lay some groundwork, Shotgun more directly aims to improve upon Boomerang rather than temporal streaming. So [b6] and [b11] are included but with lower confidence scores.\n\nIn summary, the Boomerang [b14] and FDIP [b8] papers emerge as the strongest influences based on how Shotgun is positioned as an advancement over their techniques. The Confluence work [b13] is also quite relevant. The temporal streaming papers [b6] [b11] provide context but are less central to Shotgun's key contributions.", "632630ff90e50fcafdf67484": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b0]\": 0.9,\n  \"[b1]\": 0.8,\n  \"[b18]\": 0.7\n}\n\nThe key reasons for selecting these papers as the most influential are:\n\n1. The paper heavily relies on the concepts of identifying regularity in applications to enable sampling, as originally proposed in [b0] and [b1]. This is evident from phrases like \"researchers have been able to identify regularity in single-threaded applications and exploit that to sample large applications into smaller application representatives\" and direct citations of [b0] and [b1] multiple times.\n\n2. The idea of using loop iterations as the unit of work for slicing the program's execution into regions is directly inspired by [b18], as stated: \"The idea of using loop iterations as slices for single-threaded programs was proposed in [b18].\"\n\n3. The overall methodology builds upon the foundations laid by [b0] in terms of using clustering to identify representative regions. Phrases like \"We use K-means clustering technique [23] along with a BIC goodness criteria [24] to select clustering in a method similar to previous work [b0]\" highlight the strong influence.\n\nWhile other references like [b9], [b10], [b11] are also relevant in terms of providing background on multi-threaded sampling techniques, the core novel contributions of this paper seem to be most directly motivated and enabled by the pioneering works [b0], [b1] and [b18]. The frequent direct citations and comparative phrases referring to these papers further support their central role as inspirations for this work.", "62d16e8a5aee126c0fd684cf": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"[b0]\": 0.9,\n  \"[b7]\": 0.7,\n  \"[b13]\": 0.7,\n  \"[b15]\": 0.6,\n  \"[b16]\": 0.8,\n  \"[b22]\": 0.8,\n  \"[b36]\": 0.8\n}\n\nConfidence scores:\n- [b0]: 0.9 - This paper seems to be the most direct inspiration, as the authors frequently refer to the bit-shuffle mechanism and invertible address mapping functions from [b0] as key components they build upon for their chunk-based address mapping approach.\n\n- [b16], [b22], [b36]: 0.8 - These papers are cited multiple times when discussing the importance of channel-level parallelism (CLP) in 3D-stacked memory like HBM and HMC. Maximizing CLP seems to be the primary motivation behind the novel address mapping techniques proposed.\n\n- [b7], [b13]: 0.7 - Cited as examples of prior software-only methods to control data placement based on access patterns, but at a coarse page granularity. The proposed chunk-based method aims to combine the benefits of hardware and software approaches.\n\n- [b15]: 0.6 - Mentioned when discussing their malloc implementation which maintains separate heaps per thread to reduce lock contention, similar to glibc arenas from [b15].\n\nThe other cited papers provide useful background information but do not seem to be as directly influential on the core novel contributions of chunk-based address mapping and the collaborative hardware/software approach to maximize CLP in 3D-stacked memory. The proposed techniques build most directly on the hardware mechanisms from [b0] while aiming to overcome limitations of prior software-only approaches like [b7] and [b13].", "628704555aee126c0f5c7953": "Here is my analysis to identify the most pertinent source papers for the given paper, in JSON format with reference numbers as keys and confidence scores between 0 and 1 as values:\n\n{\n  \"[b5]\": 0.95,\n  \"[b44]\": 0.8,\n  \"[b61]\": 0.7,\n  \"[b72]\": 0.6\n}\n\nExplanation:\nThe paper introduces APT-GET, a novel profile-guided mechanism to ensure the timeliness of software prefetch operations. The key novel contributions are:\n\n1. Using dynamic profiling information like execution time to determine optimal prefetch distance and injection site, unlike previous static approaches. This is heavily inspired by and builds upon the state-of-the-art static software prefetching technique in [b5].\n\n2. A new profiling methodology leveraging hardware support like Intel LBR to efficiently collect dynamic information. \n\n3. An LLVM compiler pass to automatically inject prefetches based on the profiling-determined optimal prefetch distance and injection site.\n\n[b5] is identified as the most pertinent source paper with 0.95 confidence, as the authors explicitly mention it provides the state-of-the-art static software prefetching that APT-GET improves upon. [b44] discusses prefetching fundamentals and is referenced when introducing key concepts, so it likely provided important background with 0.8 confidence. \n\n[b61] and [b72] are given lower confidence scores of 0.7 and 0.6, as they are referenced more in passing when discussing software prefetching challenges, but their specific techniques do not seem to be key inspirations for APT-GET's novel contributions.", "6287044a5aee126c0f5c33ff": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b26]\": 0.9,\n  \"[b19]\": 0.85,\n  \"[b25]\": 0.8,\n  \"[b33]\": 0.75,\n  \"[b24]\": 0.7\n}\n\nExplanation:\nThe paper focuses on investigating the reliability of uncertainty estimates for multimodal stress detection models under distribution shifts. The key novel contributions are:\n\n1. Systematically analyzing the impact of synthetic distribution shifts (Gaussian noise, missing data) on a multimodal stress detection dataset (WESAD)\n\n2. Comparing several approaches (deep ensembles, Mixup, Focal Loss) for improving the calibration and uncertainty quantification under these shifts\n\nThe most relevant references based on the criteria are:\n\n[b26] - This large-scale study compares approaches for calibration and shows calibration deteriorates under distribution shifts. The current paper builds upon this to specifically examine multimodal physiological data.\n\n[b19] - Proposes the deep ensembles method that is one of the key approaches investigated in the current paper for improving uncertainty estimates. \n\n[b25] - Shows Focal Loss can improve calibration, which motivated using it as one of the methods in the current paper.\n\n[b33] - Introduces Manifold Mixup, the regularization technique adapted in the current paper for time-series data to improve calibration.\n\n[b24] - Defines the rejection ratio metric used in the current paper to quantify the reliability of the uncertainty estimates.\n\nThe paper directly builds upon the insights and methods from these previous works to extend the investigation of calibration under distribution shifts to the domain of multimodal physiological data for stress detection. The phrases like \"Mukhoti and Kulharia et al. [b25] have pointed out that the Focal Loss can improve calibration\" and \"Mixup is originally created for computer vision... since we work with time-series data we apply Mixup on the embedding space\" indicate the paper adapts these previous techniques to its problem setting.", "62d7a7d15aee126c0f3fe03e": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"[b28]\": 0.9,\n  \"[b61]\": 0.8,\n  \"[b19]\": 0.7,\n  \"[b59]\": 0.7,\n  \"[b0]\": 0.6\n}\n\nRationale and confidence scores:\n\n[b28] (Belady's optimal replacement algorithm, confidence 0.9): The paper heavily relies on simulating Belady's optimal replacement algorithm to determine the \"branch temperature\" metric that is core to the proposed Thermometer technique. Phrases like \"We simulate the optimal BTB replacement policy (Belady's algorithm [b28] [b60])\" and \"Thermometer simulates the optimal BTB replacement policy offline on the branch execution profile\" indicate [b28] is a key inspiration.\n\n[b61] (SRRIP replacement policy, confidence 0.8): SRRIP is discussed extensively as the best performing prior BTB replacement policy. The paper compares directly against SRRIP and introduces concepts like reuse distance that are from [b61]. \n\n[b19] (GHRP replacement policy, confidence 0.7): GHRP is the only prior work specifically designed as a BTB replacement policy. The paper evaluates against GHRP as a key point of comparison.\n\n[b59] (Hawkeye replacement policy, confidence 0.7): Like SRRIP, Hawkeye is one of the main prior cache replacement policies that the paper adapts to the BTB and evaluates against. \n\n[b0] (Intel PT tracing, confidence 0.6): The profile collection step of Thermometer relies on Intel PT to collect traces, so [b0] is a key enabling mechanism for the technique, though more peripheral than the replacement policy papers.\n\nIn summary, the replacement policy papers [b28], [b61], [b19] and [b59] emerge as the most central prior works that the novel Thermometer technique builds upon and compares against. The optimal policy [b28] in particular serves as a key foundation for the new ideas.", "62d7a7d15aee126c0f3fe05a": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b22]\": 0.9,\n  \"[b94]\": 0.8,\n  \"[b91]\": 0.7,\n  \"[b90]\": 0.7,\n  \"[b82]\": 0.6,\n  \"[b152]\": 0.5,\n  \"[b153]\": 0.5\n}\n\nThe key novel contributions of this paper seem to be:\n\n1. Proposing a general-purpose polymorphic cache hierarchy architecture called t\u00e4k\u014d that allows software to observe and optimize data movement via cache-triggered callbacks. This is directly inspired by [b22] which introduces the concept of \"phantom\" cache-only address ranges.\n\n2. Implementing push-based semantics for commutative scatter-updates, motivated by the PHI architecture [b94]. \n\n3. Accelerating decoupled streaming graph traversals, taking inspiration from the HATS architecture [b91].\n\n4. Optimizing non-volatile memory transactions by eliminating journaling when possible, building on ideas from [b90].\n\n5. Exposing data movement to software to enable detecting cache side-channel attacks, extending concepts from [b80].\n\nThe paper repeatedly compares t\u00e4k\u014d to prior near-data computing architectures like [b82], highlighting how triggering computation based on data movement rather than core requests enables new optimizations.\n\nIt also references [b152] and [b153] when discussing how caching transformed data exploits locality to eliminate redundant work.\n\nSo in summary, while the paper cites a large number of prior specialized memory hierarchies, the core t\u00e4k\u014d architecture seems most directly influenced by the phantom address spaces of [b22], the push-based semantics of PHI [b94], the decoupled streaming of HATS [b91], the NVM optimizations of [b90], and the comparison to near-data architectures like [b82]. The caching optimizations also take inspiration from [b152] and [b153].", "62d7a7d15aee126c0f3fe03d": "Based on analyzing the text, here are the key source papers that likely inspired or motivated this work, in order of relevance:\n\n{\n  \"[b3]\": 0.95,\n  \"[b69]\": 0.85,\n  \"[b34]\": 0.7,\n  \"[b4]\": 0.65,\n  \"[b71]\": 0.6\n}\n\nRationale:\n\nThe paper cites [b3] (Biggs et al) multiple times, noting it reports a 32-bit natively flexible ARM-v6m microprocessor. The authors contrast their work against [b3], aiming for simpler 4-bit and 8-bit architectures optimized for footprint and yield instead of a complex 32-bit core. This suggests [b3] was a key motivating work.\n\n[b69] (Myny et al.) is cited as reporting an 8-bit ALU built using metal-oxide TFTs. The authors compare their ISA design choices against [b69]. This indicates [b69] likely inspired aspects of their architecture.\n\n[b34] reports a 16-bit RISC-V flexible microprocessor built using carbon nanotube TFTs. While a different technology than this work, [b34] represents pioneering prior art in flexible microprocessors that motivated this research direction.\n\n[b4] presents a design space exploration of printed microprocessors, though no physical processors were fabricated. The methodology of [b4] seems to have inspired the design space exploration done in this work for flexible processors.\n\n[b71] is mentioned as an example of another tiny, low gate count processor with high variation and static power, similar to the flexible processors targeted here. Likely [b71] provided ideas for handling variation and power constraints in resource-constrained processors.\n\nIn summary, [b3] and [b69] appear to be the works that most directly inspired the key architectural and design choices made in this flexible microprocessor work, while [b34], [b4] and [b71] provided general motivation and ideas. The many citations to [b3] in particular suggest it was the primary pioneering work that this research aimed to improve upon.", "62d7a7d15aee126c0f3fe044": "Based on analyzing the text, here are the most pertinent source papers that likely inspired this work, along with confidence scores:\n\n{\n  \"[b34]\": 0.9,\n  \"[b36]\": 0.8,\n  \"[b29]\": 0.7,\n  \"[b17]\": 0.6,\n  \"[b4]\": 0.5,\n  \"[b5]\": 0.5\n}\n\nThe key factors leading to these source papers being identified as most relevant are:\n\n1. The paper makes several direct comparisons to the SpArch [b36] and Gamma [b34] sparse matrix accelerators, noting they can use the same X-Cache microarchitecture with just reprogramming of the controller. This indicates [b34] and [b36] were significant inspirations.\n\n2. GraphPulse [b29] and Widx [b17] are called out as examples of DSAs that X-Cache was evaluated on, with X-Cache supporting their vertex id and database key meta-tags respectively. This suggests adapting ideas from [b29] and [b17].\n\n3. References [b4] and [b5] are highlighted in the context of prior work on augmenting scratchpads in FPGAs to support access-execute models. The paper notes limitations of these approaches that X-Cache aims to address.\n\n4. The two sparse GEMM examples used to motivate programmable walkers directly cite the Gamma [b34] paper.\n\n5. No other references are called out as explicitly in terms of directly inspiring or motivating this work on domain-specific caches.\n\nSo in summary, [b34] and [b36] appear to be the clearest inspirations, with [b29], [b17], [b4] and [b5] also providing important context and ideas that this work builds upon and compares against. The repeated mentions and comparisons to these papers in particular suggest they were the most influential prior works.", "62d7a7d15aee126c0f3fe059": "Based on the analysis of the paper, here are the most likely source papers that directly inspired or motivated this work, in JSON format with confidence scores:\n\n{\n  \"[15]\": 0.9,\n  \"[27]\": 0.9,\n  \"[28]\": 0.8,\n  \"[10]\": 0.7,\n  \"[11]\": 0.7,\n  \"[48]\": 0.7,\n  \"[42]\": 0.6,\n  \"[14]\": 0.5\n}\n\nThe key reasons for selecting these papers are:\n\n1. The paper directly states that prior works [15], [27] and [28] proposed record-and-replay instruction prefetchers for long-running server workloads. However, the proposed Jukebox prefetcher solves a different problem of accelerating short-running serverless functions. This indicates [15], [27] and [28] were direct inspirations but the authors aimed to address their limitations.\n\n2. Papers [10], [11], [48] and [42] are frequently cited when discussing the characteristics and trends in serverless workloads, such as short function durations, small memory footprints, providers keeping instances warm for long periods, and long inter-arrival times between invocations. Understanding these workload properties seems to have motivated the Jukebox design.\n\n3. [14] is cited as showing that web services are the most common application domain for serverless, motivating the need to accelerate such short-running interactive services.\n\nIn summary, [15], [27] and [28] were the most direct prior works on instruction prefetching that inspired Jukebox, while the serverless workload characterization studies in [10], [11], [48] and [42] motivated the problem of accelerating short lukewarm functions that Jukebox aims to solve. The application domain insight from [14] also played a motivating role.", "62d7a7d15aee126c0f3fe049": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b66]\": 0.9,\n  \"[b67]\": 0.8,\n  \"[b1]\": 0.7,\n  \"[b81]\": 0.6,\n  \"[b28]\": 0.5\n}\n\nThe key points that led me to identify these as the most relevant source papers are:\n\n1. The paper directly compares its Register File Prefetch (RFP) technique to prior load address prediction techniques like DLVP [b66] and the Composite predictor [b67]. It highlights how RFP overcomes limitations of these prior techniques. This indicates [b66] and [b67] provided direct motivation and a baseline for comparison.\n\n2. The recent work on Efficient Pipeline Prefetch (EPP) [b1] is discussed in detail, comparing it to DLVP and showing how RFP provides benefits over EPP while avoiding its complexities. This suggests [b1] is a very relevant contemporary work.\n\n3. The paper references Yoaz et al. [b81] when describing the typical OOO pipeline and how loads wake up their dependents. It uses learnings from [b81] to describe the RFP technique, indicating the paper builds upon foundations from this prior work.\n\n4. An early address calculation technique by Austin et al. [b28] is mentioned as a prior approach to reduce L1 latency, but the paper notes it does not apply to modern deep pipelines like RFP. Still, [b28] represents an early attempt in a similar vein to RFP.\n\nIn summary, the comparative analysis with [b66], [b67] and [b1], the use of [b81] as a foundation, and the mention of [b28] as early related work make these the most central prior works that the novel contributions of this paper build upon. The high confidence scores reflect the explicit comparisons drawn in the text.", "634d805b90e50fcafd4e117a": "Here is my analysis of the most pertinent source papers for the given text, in JSON format with confidence scores:\n\n{\n  \"[30]\": 0.95,\n  \"[12]\": 0.9,\n  \"[32]\": 0.85,\n  \"[23]\": 0.8\n}\n\nThe key reasons are:\n\n1. The paper makes multiple direct references to being motivated by and improving upon the compilation time of existing DNN compilers like Ansor [30], TVM [12], and FlexTensor [32]. This indicates these prior works were key inspirations.\n\n2. The system is implemented on top of TVM [12] and Rammer [23], suggesting these frameworks provided an important foundation. \n\n3. The compilation algorithm aims to address the long compilation times of ML-based search approaches used in [30] and [12].\n\n4. Methodologically, the paper adopts similar tensor expression abstractions as used in [12], [30] and [32], but proposes a new rTile abstraction and compilation flow to improve compilation efficiency.\n\nSo in summary, while many prior works are referenced, the key inspirations appear to be the DNN compilers Ansor, TVM, FlexTensor and to some extent Rammer, with the authors aiming to significantly speed up the compilation process compared to these existing approaches. The repeated comparisons to these specific works further confirm their centrality.", "62fc5c7b90e50fcafdbca64d": "Based on the provided paper text, I did not find any explicit phrases like \"inspired by\", \"motivated by\", or other language directly indicating the paper's core contributions were heavily influenced by specific previous works. The paper does not seem to directly attribute its key novel aspects to prior research.\n\nHowever, by analyzing the key challenges and novel methods introduced in the paper, I identified a few references that appear most relevant to the paper's main contributions:\n\n{\n  \"[b17]\": 0.8,\n  \"[b37]\": 0.7,\n  \"[b29]\": 0.6,\n  \"[b31]\": 0.6,\n  \"[b10]\": 0.5\n}\n\n[b17] and [b37] are cited when discussing how to apply Transformers to knowledge graphs, which is a key architectural aspect of the kgTransformer model proposed in the paper. \n\n[b29] and [b31] are referenced in the section on using the Mixture-of-Experts (MoE) strategy to improve the computational efficiency of kgTransformer's feed-forward layers, which is an important enhancement introduced by the paper.\n\n[b10] is cited when explaining how the feed-forward network in Transformers is equivalent to key-value networks, providing relevant background for the sparsity analysis and MoE enhancements.\n\nWhile not preceded by explicit \"inspiration\" phrases, these references appear to be the most central in relation to kgTransformer's architecture and the MoE efficiency improvements, which are the paper's main novel contributions. The other references provide useful context but do not seem as directly tied to the core innovations.", "634d80f190e50fcafd4ef483": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\n{\n  \"[b45]\": 0.9,\n  \"[b41]\": 0.9,\n  \"[b43]\": 0.9,\n  \"[b12]\": 0.8,\n  \"[b27]\": 0.7\n}\n\nThe key points that led me to identify these as the most relevant source papers are:\n\n1. The text mentions that \"Doc2EDAG, TDJEE and GIT [b45] [b41] [b43] contain only 5 event types and 35 argument types in financial domain.\" This directly compares the proposed DocEE dataset to these previous datasets, indicating they were important prior work that the authors aim to improve upon. The repeated mentions and direct comparison suggest these papers were highly influential.\n\n2. The text states \"WikiEvents [b27] consists of only 246 documents with very few (22% of total) cross-sentences argument annotations. RAMS [b12] limits the scope of the arguments in a 5-sentence window around its event trigger, which is not in line with the actual application\". This highlights limitations in these previous datasets that DocEE aims to address, suggesting they provided important motivation and context for this work.\n\n3. The text does not use strong phrases like \"inspired by\" for any specific references. However, the direct comparisons to the datasets in [b45], [b41], [b43], [b12] and [b27] in terms of number of event/argument types and annotation scope indicates these papers were the most influential prior work.\n\n4. Other references like [b13], [b8], [b44] are only briefly mentioned as examples of EE applications. They do not seem to be as directly related to the core dataset contributions.\n\nSo in summary, while the text does not explicitly state motivation from specific papers, the repeated direct comparisons to [b45], [b41], [b43], [b12] and [b27] throughout the related work section suggests these papers provided the most important context and inspiration for the DocEE dataset. The authors aim to improve upon the limitations of these prior datasets.", "6237ecc25aee126c0f3bef94": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b11]\": 0.9,\n  \"[b13]\": 0.9,\n  \"[b2]\": 0.8,\n  \"[b5]\": 0.6,\n  \"[b32]\": 0.6\n}\n\nConfidence scores:\n- [b11] and [b13]: 0.9. The paper directly builds upon and scales up the DimeNet++ [b11] and GemNet-T [b13] architectures. It refers to them as \"the current state-of-the-art energy-centric and force-centric models respectively\" and benchmarks the proposed scaled up versions against them.\n\n- [b2]: 0.8. The Open Catalyst 2020 (OC20) dataset [b2] is central to the paper's experiments and benchmarking. The novel large-scale models are evaluated on this dataset.\n\n- [b5] and [b32]: 0.6. These references are mentioned in the introductory context that \"Graph Neural Networks (GNNs) [b5] [b32] have emerged as the standard architecture of choice for modeling atomic systems\". While relevant for background, they seem less directly influential on the paper's main novel contributions compared to [b11], [b13] and [b2].\n\nThe paper's key contributions appear to be 1) Introducing the Extended Graph Network (EGN) framework to cover GNNs with higher-order interactions 2) Proposing a parallelization approach to scale EGNs to very large sizes 3) Demonstrating significant performance gains by scaling up two specific GNN architectures, DimeNet++ and GemNet-T, on the OC20 dataset. References [b11], [b13] and [b2] directly enable and motivate these central aspects.", "6257c5b25aee126c0f468af6": "Here are the key steps I took to identify the most relevant source papers for this paper:\n\n1. The novel contributions of the paper appear to be:\n- Proposing a spherical message passing (SMP) scheme for 3D molecular graphs that uses distance, angle and torsion information in spherical coordinates to provide an approximately complete representation. \n- Showing SMP is efficient by only using edge-based 1-hop information, enabling it to scale to large molecules.\n- Developing SphereNet, a graph neural network architecture that uses SMP along with physically meaningful basis functions to learn on 3D molecular graphs.\n\n2. Key phrases indicating inspiration from previous work:\n- \"Currently, the message passing scheme [b18] [b40] [b51] [b2] is one of the commonly used architectures for realizing GNNs.\" - This suggests the paper builds upon message passing neural networks.\n- \"Referring to theories in Griffiths & Schroeter (2018); [b9] ; [b28] , one form of the representation can be expressed as...\" - This indicates the physical representations are inspired by these references.\n\n3. References that appear most directly related to the novel contributions:\n- [b18], [b40], [b51], [b2]: These introduce the message passing framework that SMP builds upon \n- [b28]: Provides basis for the physical representations used in SphereNet\n- [b23]: Also explores basis functions for 3D information\n\n4. The introduction references like [b20], [b55] provide background but do not seem as central to the novel aspects.\n\nIn summary, based on methodological similarities and direct indications, I believe the following are likely the most influential references for the core contributions of this paper:\n\n{\n  \"[b18]\": 0.9,\n  \"[b40]\": 0.9, \n  \"[b51]\": 0.9,\n  \"[b2]\": 0.9,\n  \"[b28]\": 0.8,\n  \"[b23]\": 0.7\n}\n\nThe message passing references have the highest scores as SMP directly builds on that framework. [b28] and [b23] have slightly lower but still high scores as they help motivate the physical representation choices in SphereNet.", "6257c5b15aee126c0f468a55": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"b13\": 0.9,\n  \"b8\": 0.8,\n  \"b28\": 0.7,\n  \"b38\": 0.6,\n  \"b5\": 0.5,\n  \"b24\": 0.5\n}\n\nThe key novel contributions of this paper seem to be:\n\n1. Analyzing imbalance issues in LSH-produced sparse attention patterns in terms of unbalanced hash bucket sizes and query-key ratios. \n\n2. Proposing a new \"attention utility\" metric to quantify how well sparse patterns approximate full attention.\n\n3. Introducing Learning-to-Hash Attention (LHA), which uses separate learnable hash functions for queries and keys to optimize the attention utility metric end-to-end.\n\nThe references that most directly relate to and inspire these contributions are:\n\n- b13 (Reformer) and b8 (SMYRF): Pioneering works on using LSH for sparse attention that this paper analyzes limitations of and improves upon. The imbalance analysis is done on models from these papers.\n\n- b28 (Routing Transformer) and b38 (Cluster-former): Also use ANN methods like k-means for sparse attention and are compared to the proposed LHA approach.\n\n- b5 and b24: Provide techniques for unbiased kernelized attention that LHA leverages to efficiently approximate the attention utility during training.\n\nThe paper indicates it is directly motivated by and improves upon the LSH approaches in b13 and b8. The other references provide important context and components that enable the LHA method. Overall, b13 and b8 seem to be the most central inspiring works, with the others also making key contributions to the novel aspects of this paper.", "62c2a5625aee126c0fcca6f5": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b35]\": 0.9,\n  \"[b33]\": 0.85,\n  \"[b9]\": 0.8,\n  \"[b18]\": 0.75,\n  \"[b26]\": 0.75,\n  \"[b43]\": 0.7,\n  \"[b39]\": 0.65\n}\n\nThe key reasons for selecting these papers as the most relevant sources are:\n\n1. The paper directly states that [b35] and [b33] showed adversarial data augmentation is a data-dependent regularization that could help generalize to out-of-distribution samples. This indicates these two papers heavily inspired the core idea behind FLAG.\n\n2. The paper mentions the efficacy of adversarial augmentation has been verified in domains like computer vision [b39], language understanding [b18] [b26] [b43], and visual question answering [b9]. The phrasing suggests these papers demonstrated the potential of adversarial augmentation and motivated extending it to graphs.\n\n3. The methodology of FLAG shares strong similarities with the adversarial augmentation approaches in these source papers, like using gradient-based optimization to craft perturbations. \n\n4. The source papers focus on improving generalization and robustness, especially on out-of-distribution samples, which aligns with the main goals of FLAG.\n\n5. The confidence scores are highest for [b35] and [b33] since they are called out as direct inspiration. The scores are slightly lower for the remaining papers which provided supporting evidence for the potential of adversarial augmentation across domains.\n\nI focused on identifying the papers that the authors explicitly or implicitly indicated as inspiration for the key ideas behind FLAG, while avoiding references that just provide background context. The scores aim to quantify the centrality of each paper to the main contributions of FLAG.", "6287492a5aee126c0ffe82dd": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b33]\": 0.9,\n  \"[b16]\": 0.8,\n  \"[b25]\": 0.7,\n  \"[b45]\": 0.6,\n  \"[b7]\": 0.5\n}\n\nConfidence scores:\n- [b33] and [b16] have the highest confidence scores of 0.9 and 0.8 respectively. The paper directly states that current semantic parsing based methods like those in [b33], [b16] and [b25] suffer from \"prohibitively huge search space\" that \"hinders both the performance and speed\". This indicates the proposed approach aims to improve upon the limitations of these prior works.\n\n- [b45] and [b7] have medium confidence scores of 0.6 and 0.5. The dataset construction pipeline of synthesizing and paraphrasing questions follows the methodology in these two papers. While relevant, they seem more peripherally related to the core technical contributions.\n\n- The paper introduces a new compositional reasoning approach using a Knowledge-oriented Programming Language (KoPL). The key novelty seems to be providing explicit intermediate reasoning steps to aid in learning, which contrasts with prior reinforcement learning and search based methods that lack such supervision.\n\nSo in summary, [b33], [b16] and [b25] appear to be the most influential prior works that directly motivated this paper's novel contributions, while [b45] and [b7] provided key components of the evaluation methodology. The confidence scores reflect the centrality of each reference to the main proposals.", "6327ec7990e50fcafd928963": "Here is my analysis of the key source papers for the given paper, based on the criteria you provided:\n\n{\n  \"[b0]\": 0.9,\n  \"[b1]\": 0.85,\n  \"[b9]\": 0.7,\n  \"[b10]\": 0.6,\n  \"[b13]\": 0.5,\n  \"[b14]\": 0.5,\n  \"[b15]\": 0.5\n}\n\nConfidence scores:\n- [b0] (AlphaFold) and [b1] (AlphaFold-Multimer): Very high confidence (0.9 and 0.85). The paper directly states it is reimplementing and reproducing the training of these models, so they are the primary inspiration and foundation.\n\n- [b9]: High confidence (0.7). The paper adopts the MSA pairing technique from this reference for building cross-chain genetics in its multimer model, indicating it was an important influence on that novel aspect.\n\n- [b10]: Medium confidence (0.6). The paper replaces ReLU activations with GELUs based on this reference. While not the main focus, it shows this paper directly motivated that architectural change.\n\n- [b13], [b14], [b15]: Medium confidence (0.5). The fusion of softmax and layer norm operators for acceleration is said to be similar to these previous works on Transformer acceleration. This suggests they inspired the implementation optimizations, though perhaps less centrally than the AlphaFold papers.\n\nThe other references, while relevant for context, do not seem to be called out as direct inspirations for the primary novel contributions like the core AlphaFold reimplementation, the multimer modeling innovations, or the key acceleration techniques. The paper's own title and abstract emphasize it is an open-source platform extending AlphaFold and AlphaFold-Multimer.", "63520de390e50fcafd60ec75": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b10]\": 0.9,\n  \"[b32]\": 0.8,\n  \"[b14]\": 0.7,\n  \"[b0]\": 0.6,\n  \"[b24]\": 0.5\n}\n\nReasoning:\n\nThe paper introduces an asymmetric reverse process (Asyrp) to discover a semantic latent space (h-space) in frozen pretrained diffusion models for image editing. The key novel contributions are:\n\n1. Breaking the destructive interference between the predicted noise terms to enable semantic manipulation, by modifying only the predicted x0 term with shifted noise while preserving the direction term. This is directly inspired by and builds upon the DDPM [b10] and DDIM [b32] frameworks.\n\n2. Choosing the bottleneck features (h-space) of the U-Net noise predictor to control the noise shifting, which enables desirable properties like homogeneous, linear, and composable edits while preserving image quality. This is motivated by the limitations of naive latent manipulation approaches discussed in [b14].\n\n3. Learning an implicit neural function to map the bottleneck features to the noise shift, which improves training robustness and allows generalization to different sampling steps. The overall editing framework optimizes a loss combining the CLIP directional loss from [b24] and an image reconstruction term.\n\n4. A principled design of the generative process based on quantifiable editing strength, flexibility and quality deficiency measures.\n\nAmong these, [b10] and [b32] are the most central references that the asymmetric reverse process directly builds upon. [b14] provides key motivation by discussing the limitations of naive latent manipulation. [b24] and [b0] contribute to the editing framework in terms of the CLIP loss. The other references provide relevant background but are less directly tied to the main contributions.", "63520de890e50fcafd60f4dd": "Based on the given text, which appears to be an academic paper, I did not find any clear indications of specific source papers that directly inspired or motivated this work. The text does not contain key phrases like \"inspired by\", \"motivated by\", or references to \"pioneering/previous work\" that the authors built upon.\n\nThe paper seems to focus on studying the impact of \"instruction finetuning\" on large language models like PaLM. However, it does not cite any specific prior papers as the main inspiration or basis for their finetuning approach and experiments.\n\nSince no papers are directly referenced as the primary motivation or foundation this work builds heavily upon, I do not have enough information to confidently identify the most pertinent source papers. The text provided also does not contain a bibliography section that could help infer the centrally influential prior works.\n\nIn summary, my analysis of the given text did not surface any source papers that clearly stood out as the primary inspiration for the core novel contributions and methods of this paper. Additional sections of the paper would likely be needed to make this determination.\n\n{}", "6389d6e490e50fcafdff693f": "Here is my analysis of the most pertinent source papers for the given paper, in JSON format with reference numbers as keys and confidence scores between 0 and 1 as values:\n\n{\n  \"[b19]\": 0.95,\n  \"[b24]\": 0.9,\n  \"[b23]\": 0.85,\n  \"[b22]\": 0.8,\n  \"[b10]\": 0.75,\n  \"[b62]\": 0.7,\n  \"[b63]\": 0.65\n}\n\nThe key reasons for selecting these references as the most pertinent source papers are:\n\n1. The paper frequently cites [b19], [b22], [b23], [b24] when discussing the inefficiencies of current CPUs for data center workloads, underutilization of resources, and the need to increase thread count per socket. This indicates the paper builds heavily on the insights from these prior works.\n\n2. References [b10] is cited when mentioning that the CPU pipeline frontend consumes significant power, which the RPU aims to reduce through SIMT execution. This shows [b10] directly motivated a key contribution.\n\n3. The RPU borrows the SIMT convergence and reconvergence techniques from GPU architectures, citing [b62] and [b63] as the basis for its MinSP-PC heuristic policy for serializing divergent control flow paths. \n\n4. No other references are called out using phrases like \"inspired by\", \"motivated by\" etc. The other references provide useful background but do not seem to be as centrally tied to the novel aspects of the RPU design compared to the ones selected above.\n\n5. Based on the titles, the selected references appear to focus on analyzing CPU inefficiencies and SIMT techniques highly relevant to the RPU, while many of the other references seem more tangentially related.\n\nSo in summary, the listed references appear to be the most influential in directly inspiring and enabling the key contributions of the RPU architecture and software stack. The confidence scores reflect the strength of evidence for their influence based on the citance analysis.", "63a2c50090e50fcafdb97c2f": "Based on analyzing the paper, here are the key source papers that likely inspired this work, along with confidence scores:\n\n{\n  \"[b39]\": 0.9,\n  \"[b38]\": 0.8,\n  \"[b5]\": 0.7,\n  \"[b4]\": 0.7,\n  \"[b2]\": 0.6,\n  \"[b13]\": 0.6\n}\n\nThe key points:\n\n- The paper is heavily inspired by [b39] and [b38] for the idea of unsupervised vision-and-language pre-training (UVLP) using pseudo-parallel image-text data synthesized from object tags. The masked tag prediction (MTP) pre-training task is directly motivated by [b39].\n\n- Using vision pre-trained models like ViT [b5] as the visual encoder and BERT [b4] as the multimodal encoder are core components of their end-to-end UVLP framework. \n\n- The referring expression matching (REM) pre-training task is proposed to address limitations of MTP, drawing inspiration from studies in pre-trained language models [b2] [b13] that show performance degradation from train-test discrepancies.\n\n- Overall, [b39], [b38], [b5] and [b4] provide the key foundations that this paper builds upon for end-to-end UVLP, while [b2] and [b13] motivate the novel REM task to improve upon limitations of prior methods. The high density of citations to these papers when describing the core technical contributions indicates their centrality as source material.", "62be66205aee126c0f07d1e0": "Here are the most pertinent source papers for this paper based on the criteria you specified, along with confidence scores:\n\n{\n  \"[b4]\": 0.9,\n  \"[b3]\": 0.8,\n  \"[b10]\": 0.7,\n  \"[b33]\": 0.7,\n  \"[b24]\": 0.6,\n  \"[b34]\": 0.5\n}\n\nThe key factors for selecting these papers are:\n\n1. Papers [b4] (Codex) and [b3] (GPT-3) are heavily referenced when discussing the Transformer language models that this paper is evaluating and building upon. The authors note these papers \"recently achieved good performance in automatic program synthesis\". This indicates they are foundational to the paper's work.\n\n2. Papers [b10], [b33], and [b24] are previous studies that evaluated Codex/Copilot on specific tasks like solving math problems or LeetCode questions. This paper directly builds on their methodologies while identifying limitations they aim to address, like analyzing efficiency and repairability of solutions.\n\n3. Paper [b34] is relevant as it conducted a user study on Copilot, though this paper notes limitations in that study that motivate their own, like not examining solution quality.\n\nThe selection prioritizes papers that the authors explicitly build upon and contrast their novel contributions against. Confidence scores are highest for the foundational Codex and GPT-3 papers, followed by very relevant prior Codex/Copilot evaluations. The [b34] user study is included but at lower confidence since its relevance is more limited.", "633e477c90e50fcafde5a596": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b0]\": 0.9,\n  \"[b1]\": 0.9,\n  \"[b12]\": 0.8,\n  \"[b13]\": 0.8,\n  \"[b4]\": 0.7,\n  \"[b53]\": 0.6,\n  \"[b2]\": 0.5\n}\n\nRationale:\n- [b0] and [b1] are cited in the introduction as bringing us closer to the goal of \"task-agnostic machine learning\" and applying LLMs to new tasks \"out-of-the box\". This directly motivates the paper's focus on in-context learning and controlling LLMs via prompts. \n- [b12] and [b13] introduce the weak supervision framework that is a key component of the AMA approach proposed in this paper for aggregating predictions from multiple prompts.\n- [b4] proposes fine-tuning LMs over prompts, which this paper aims to improve upon by focusing on off-the-shelf LLMs with no additional fine-tuning. The prompt-chain approach also takes inspiration from [b4].\n- [b53] is referenced multiple times as prior work that this paper builds upon, in terms of prompt ensembling and as a model evaluated.\n- [b2] studies prompt sensitivity which motivates this paper's focus on aggregating multiple prompts to improve reliability.\n\nThe other cited papers, while relevant, do not appear to be as centrally influential on the main novel contributions and methods of this paper based on my analysis of how they are discussed in the text. The key aspects are: 1) Focusing on prompting off-the-shelf LLMs, 2) Using prompt-chains to reformat inputs, 3) Applying weak supervision to reliably aggregate multiple prompts. The selected papers most directly inspire and enable those contributions.", "62afe5495aee126c0f668b42": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\n{\n  \"[b14]\": 0.9,\n  \"[b4]\": 0.8,\n  \"[b20]\": 0.8,\n  \"[b18]\": 0.6,\n  \"[b19]\": 0.6,\n  \"[b21]\": 0.6\n}\n\nConfidence scores:\n- [b14] (score 0.9): This reference is directly mentioned as a strong baseline model \"VRBot\" that the proposed MedPIR model outperforms. It indicates [b14] is a highly relevant prior work.\n\n- [b4] (score 0.8): The text states \"We follow Chen et al. [b4] and use BERT-GPT as the backbone of our encoder and the generator.\" This shows the proposed model builds upon the architecture from [b4].\n\n- [b20] (score 0.8): The datasets MedDG from [b20] is used to evaluate the proposed MedPIR model. The entity concatenation method from [b20] is also compared as a baseline. \n\n- [b18], [b19], [b21] (score 0.6 each): These references are mentioned as prior works that \"introduce the external medical knowledge into the Seq2Seq models\". The proposed MedPIR model also leverages external medical knowledge, so these works provide relevant background.\n\nThe other references like [b8] are not included since they are only briefly mentioned as related works but do not seem to directly inspire the novel contributions of this paper based on the provided text. The key novel aspects appear to be the knowledge-aware dialogue graph encoder and recall-enhanced generator.", "634781fe90e50fcafd2c1a49": "Here is my analysis of the key source papers for this paper, based on the criteria you provided:\n\nAfter carefully reading the paper, the novel contributions appear to be:\n1) Proposing Attribution-Driven Dropout (AD-DROP) to better fine-tune pre-trained language models by dropping self-attention positions with high attribution scores. This forces the model to rely more on low-attribution positions to reduce overfitting.\n2) Designing a cross-tuning strategy that alternates between fine-tuning and AD-DROP to improve training stability.\n\nLooking at the key methodological details and phrasing in the paper, the following references seem to be the most influential in motivating and inspiring this work:\n\n[b18] - The original dropout paper which is at the core of the AD-DROP method proposed here. The paper states \"Dropout [b18], which randomly discards a proportion of units, is at the core of dropout-based methods.\"\n\n[b22] - Introduces the concept of self-attention attribution which is central to AD-DROP. The gradient and integrated gradients attribution methods from [b22] are directly used. \"Attribution [b23] is an interpretability method that attributes model predictions to input features via saliency measures such as gradient [b24] [b25]. It is also used to explain the influence patterns of self-attention in recent literature [b22] [b26] [b27].\"\n\n[b24], [b25] - Provide the gradient and integrated gradients attribution methods used to compute attribution scores in AD-DROP. \"we compute the attribution score of each attention position by gradient [b24] or integrated gradient [b25] attribution methods.\"\n\nIn summary, based on the phrasing and methodological details, the following references appear to be the key inspirations and building blocks for the novel AD-DROP method:\n\n{\n  \"[b18]\": 0.9,\n  \"[b22]\": 0.8, \n  \"[b24]\": 0.7,\n  \"[b25]\": 0.7\n}\n\nThe confidence scores reflect the centrality of these references to the main AD-DROP idea, with [b18] being the most fundamental, [b22] providing the key self-attention attribution concept, and [b24]/[b25] enabling the attribution score computation.", "62ceb9215aee126c0f4090b0": "Here is my attempt at identifying the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b6]\": 0.9,\n  \"[b10]\": 0.8,\n  \"[b9]\": 0.7,\n  \"[b12]\": 0.6,\n  \"[b13]\": 0.5\n}\n\nRationale and key points:\n\n- The paper proposes a novel method called Pruning in Time (PIT) for optimizing the architecture of Temporal Convolutional Networks (TCNs) for time-series tasks on edge devices. The key novel contributions are:\n1) Framing the optimization of receptive field and dilation as structured weight pruning using trainable masks \n2) Considering regularizers to reduce both number of parameters and operations\n3) Validating PIT on 4 real-world time-series benchmarks relevant for edge deployment\n4) Measuring memory, latency and energy of the optimized models on two edge devices\n\n- [b6] is likely the most influential reference, as the paper states \"Recently, however, new architectures have been proposed as viable alternatives to RNNs, such as Attention-based Transformers and Temporal Convolutional Networks (TCNs) [b6].\" This indicates TCNs themselves were pioneered in [b6] which directly motivates this work.\n\n- [b10], [b9] and [b12] are referenced multiple times when discussing existing differentiable NAS methods using masking ([b10], [b9]) or weight pruning ([b12]). The proposed PIT method builds upon and extends these masking approaches to the temporal dimensions of TCNs.\n\n- [b13] is the authors' own previous work that proposed optimizing TCN dilation using NAS, which this paper extends to also optimize receptive field and layer sizes. So [b13] is a direct predecessor.\n\n- The paper compares PIT favorably to [b9], [b10] and [b12], showing it can find better TCN architectures by searching temporal dimensions those methods cannot.\n\nSo in summary, [b6] pioneered the TCN architectures being optimized, while [b9], [b10], [b12] and [b13] represent the most relevant prior NAS works that PIT builds upon and surpasses. The assigned confidence scores reflect the perceived degree of importance and similarity of each reference to PIT's main contributions.", "63bb859d90e50fcafd06ee17": "Here is my analysis of the most pertinent source papers for this paper, based on the criteria you provided:\n\n{\n  \"[b47]\": 0.9,\n  \"[b35]\": 0.7,\n  \"[b36]\": 0.7,\n  \"[b49]\": 0.6,\n  \"[b43]\": 0.6\n}\n\nExplanation:\nThe key novel contributions of this paper are:\n1) Proposing the COVERT attack which plants malicious payloads in docstrings to bypass static analysis \n2) Proposing the TROJANPUZZLE attack which avoids including the malicious payload verbatim in the poisoning data by exploiting the substitution capabilities of transformer models\n\nThe source papers most directly related to these contributions are:\n\n[b47] (confidence 0.9): This paper by Schuster et al. is referenced multiple times as the main prior work and inspiration. The SIMPLE attack proposed in this paper is what the authors directly build upon and compare against. Phrases like \"We remove this limitation of Schuster et al.'s work\" indicate the current work is heavily motivated by [b47].\n\n[b35] (confidence 0.7) and [b36] (confidence 0.7): These two papers confirm the security risks of code suggestion models like Copilot and Codex, which is the key motivation for this work. The introduction states \"Security risks of code suggestions have been confirmed by recent studies [b35], [b36]\".\n\n[b49] (confidence 0.6) and [b43] (confidence 0.6): The Pythia and GPT-2 models from these papers were the targets of Schuster et al.'s original poisoning attack. While not as directly related, they provide important context.\n\nThe other referenced papers provide useful background information but do not seem to be as centrally tied to the main novel contributions based on the provided criteria. Methodologically, the proposed attacks focus on exploiting transformer language models and their ability to perform substitutions, but no specific papers are called out as inspiration for this approach.", "628749265aee126c0ffe6e73": "Here is my analysis of the most pertinent source papers for the given paper, based on the outlined criteria:\n\n{\n  \"[b23]\": 0.9,\n  \"[b82]\": 0.8,\n  \"[b111]\": 0.7,\n  \"[b63]\": 0.7,\n  \"[b62]\": 0.7,\n  \"[b110]\": 0.7\n}\n\nConfidence scores:\n[b23]: 0.9 - This reference is mentioned multiple times when discussing general task-oriented dialogue system architectures. The paper directly compares its analysis to [b23], indicating it is a key motivating work.\n\n[b82]: 0.8 - Mentioned as one of the closest prior surveys, but with important differences. This suggests [b82] is very relevant, but the current work builds novel contributions on top of it.\n\n[b111], [b63], [b62]: 0.7 each - Grouped together as they are all mentioned as prior systematic reviews of dialogue systems in healthcare. However, the current paper notes these works were written for medical audiences and did not cover the technical depth that is the focus here.\n\n[b110]: 0.7 - Also mentioned as a closely related recent survey, but the current work differentiates itself by focusing specifically on task-oriented systems and providing more technical depth.\n\nThe other references mentioned, like [b124], [b7], [b36], etc. provide relevant background information but do not seem to be as directly influential on the novel contributions of this paper based on the text provided. The key contributions appear to be the technical depth of analysis and focus on task-oriented systems in healthcare, directly building on the most relevant prior works listed above.", "63608e5090e50fcafdee1224": "Here is my analysis of the key source papers for this paper, based on the criteria you provided:\n\n{\n  \"[b33]\": 0.9,\n  \"[b11]\": 0.8,\n  \"[b39]\": 0.7,\n  \"[b10]\": 0.7,\n  \"[b41]\": 0.6,\n  \"[b46]\": 0.6\n}\n\nReasoning:\n\nThe paper focuses on using neural architecture search (NAS) to automatically design data-specific GNN architectures for temporal knowledge graph completion (TKGC). The key novel contributions are:\n\n1. Defining a generalized framework to simultaneously explore topological and temporal information in temporal knowledge graphs \n2. Designing a novel search space of GNN architectures that can capture various patterns in different temporal knowledge graphs\n3. Adopting an efficient search algorithm to find data-specific architectures\n\nThe most relevant source papers, based on the criteria, are:\n\n[b33] (TeMP) and [b11] (T-GAP): These are existing GNN-based methods for TKGC that the paper directly builds upon and aims to improve. The paper points out their limitation of using fixed architectures. Improving these methods is a key motivation.\n\n[b39] and [b10]: These are foundational papers on neural architecture search (NAS) that directly inspire the paper's use of NAS to automatically design TKGC architectures. \n\n[b41] and [b46]: Recent works that adopt NAS for static knowledge graph completion. The paper notes these as related work, but highlights that this is the first attempt to use NAS for TKGC.\n\nThe paper does not use strong phrases like \"inspired by\" for these references. But based on the problem setup and methodological similarities, these appear to be the most influential source papers that the novel contributions directly build upon. The other references provide relevant background but are less central to the main ideas.", "6306e8c890e50fcafdebd565": "Here are the most relevant source papers for this paper, based on the criteria you specified:\n\n{\n  \"[b14]\": 0.9,\n  \"[b28]\": 0.9,\n  \"[b7]\": 0.8,\n  \"[b26]\": 0.7,\n  \"[b32]\": 0.6,\n  \"[b23]\": 0.5\n}\n\nThe key points:\n\n- The paper heavily relies on the Mixture-of-Experts (MoE) architecture, directly citing [b14] and [b28] as the basis. The proposed ADMoE method is an adaptation of MoE for anomaly detection with multiple noisy label sets.\n\n- [b7] is cited as an example of how MoE has been used in other domains like machine translation to learn from diverse sources, providing inspiration for ADMoE. \n\n- The paper compares ADMoE to enhancing the DeepSAD method [b26], indicating DeepSAD is an important foundation.\n\n- Comparisons are made to existing methods for learning from multiple noisy label sets, especially CrowdLayer [b23] and UnionNet [b32]. The paper argues ADMoE provides advantages over these methods.\n\n- The high confidence scores for [b14] and [b28] reflect that the core novelty of ADMoE directly builds upon the original MoE architecture from these pioneering papers. The other references, while relevant for comparison and motivation, contribute less directly to the methodological foundations of ADMoE.", "62451c325aee126c0f47b416": "Based on analyzing the text, here are the key source papers that likely inspired or motivated this work, along with confidence scores:\n\n{\n  \"[b34]\": 0.9,\n  \"[b11]\": 0.8,\n  \"[b36]\": 0.7,\n  \"[b29]\": 0.6,\n  \"[b42]\": 0.5\n}\n\nThe key reasons for selecting these papers are:\n\n1. The paper directly states that it aims to \"address these challenges in existing self KD methods\" and specifically calls out the drawbacks of [b11], [b29], [b34], [b35] which it tries to overcome. This indicates these papers, especially [b34] and [b11], were key motivations.\n\n2. It draws direct comparisons to [b36] calling it \"more similar to our work\" and aims to improve upon its limitations, suggesting it was an important inspiration. \n\n3. The proposed DLB method is compared extensively to [b11], [b34], [b36] in tables and discussion, highlighting them as key prior works.\n\n4. While not stated as explicitly, the auxiliary head distillation approach of [b42] is discussed as one of the popular self-KD methods that requires heavy architecture changes, which DLB aims to avoid.\n\nSo in summary, [b34], [b11] and [b36] emerge as the most central motivating works, with [b29] and [b42] as other important related works that inspired key aspects of this paper's approach. The other referenced papers provide context but do not seem to be as directly influential on the main contributions.", "62725cd25aee126c0fae91f3": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b0]\": 0.9,\n  \"[b5]\": 0.8,\n  \"[b6]\": 0.7,\n  \"[b9]\": 0.6\n}\n\nConfidence scores:\n[b0] (0.9): This reference, which introduces knowledge distillation using a teacher-student learning scheme, appears to be the main inspiration and foundation for the spot-adaptive distillation method proposed in this paper. The authors build upon the core idea of knowledge distillation from [b0] while making the distillation spots adaptive.\n\n[b5] (0.8): FitNets, which distills knowledge at intermediate layers in addition to outputs, is referenced as an example of multi-spot distillation that the proposed method aims to improve upon by adaptively selecting distillation spots. The use of adaption layers to match teacher and student features also seems inspired by FitNets.\n\n[b6] (0.7): Attention transfer is another key reference for multi-spot distillation that transfers attention maps from teacher to student. The authors indicate their method can be combined with techniques like attention transfer, suggesting [b6] helped motivate their approach.\n\n[b9] (0.6): Activation boundary (AB) distillation is mentioned as another example of multi-spot distillation that transfers knowledge at different layers. While less central than [b0] and [b5], it still represents prior work that the authors aim to improve upon with adaptive distillation.\n\nThe key novel contribution is making the distillation spots adaptive to each sample and training stage by using a policy network to decide routing between teacher and student layers. This builds upon foundational distillation methods like [b0] and multi-spot approaches like [b5], [b6] while introducing a new adaptive mechanism. The routing network architecture also takes inspiration from prior work on routing and conditional computation.", "63ae56ca90e50fcafda968ed": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b37]\": 0.9,\n  \"[b25]\": 0.8,\n  \"[b14]\": 0.7,\n  \"[b0]\": 0.6,\n  \"[b8]\": 0.6,\n  \"[b6]\": 0.5\n}\n\nThe key factors I considered:\n\n1. The paper introduces the novel DEMONSTRATE-SEARCH-PREDICT (DSP) framework for in-context learning. References [b37], [b25], and [b14] are directly cited when discussing the limitations of existing \"retrieve-then-read\" pipelines that DSP aims to improve upon. This indicates they were key motivations for the DSP approach.\n\n2. The paper states that DSP \"unif[ies] techniques from the retrieval-augmented NLP and the in-context learning literatures\", citing references [b0], [b8], [b6] among others in this context. This suggests these works directly influenced the methodological foundations of DSP.\n\n3. The comparative phrase \"In contrast, a taskaware DSP program successfully decomposes the problem and produces a correct response\" highlights that DSP is positioned as an improvement over the retrieve-then-read approach of [b25] and [b14].\n\n4. The high-level DSP stages of DEMONSTRATE, SEARCH and PREDICT align closely with key aspects of the referenced in-context learning and retrieval-augmented methods.\n\n5. While other references like [b39] and [b1] provide important context, they seem less directly tied to the core novel contributions of DSP itself based on how they are discussed in the text.\n\nSo in summary, [b37], [b25], [b14], [b0], [b8] and [b6] appear to be the most central motivating and methodologically relevant previous works that inspired the key innovations in this paper, with [b37], [b25] and [b14] being particularly prominent in that regard. The scores reflect my confidence in the centrality of each reference's influence.", "63aab708a4a9066abca549f8": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b15]\": 0.9,\n  \"[b11]\": 0.8,\n  \"[b7]\": 0.7,\n  \"[b1]\": 0.6\n}\n\nConfidence scores:\n[b15] (0.9): The paper heavily relies on the Transformer architecture from this reference for its encoder, indicating it is a key inspiration. The phrase \"As various works have demonstrated that transformer is powerful in tackling sequences of varying lengths\" suggests the Transformer is central to their approach.\n\n[b11] (0.8): The concurrent softmax loss function proposed in this reference is directly used to address issues with binary cross entropy in the highly imbalanced multi-label setting. This loss function seems important for training the symptom prediction head.\n\n[b7] (0.7): The supervised contrastive learning approach generalized in this reference is adapted to pull symptoms of the same disease together while pushing apart symptoms from different diseases. This contrastive objective is a key part of their auxiliary disease prediction task.\n\n[b1] (0.6): This reference considers automatic diagnosis as a sequence generation task, which the proposed method aims to improve upon by reformulating it as multi-label classification to preserve the unordered nature of symptoms. The comparison suggests [b1] was an important motivation, even if the proposed method differs.\n\nThe paper does not seem to contain clear phrases like \"inspired by\" explicitly highlighting inspiration from specific references. However, based on the usage of key components like the Transformer, concurrent softmax, and supervised contrastive learning, along with the framing of improving upon [b1], I believe these 4 references were likely the most influential in developing the paper's main contributions. The confidence scores reflect the centrality of each reference to the novel aspects of the proposed approach.", "620e302d5aee126c0fadda4d": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\nThe key novel contributions of this paper appear to be:\n1) A new multithreading in-order pipeline microarchitecture design for RISC-V\n2) An efficient event-based issue scheduling algorithm that leverages the RISC-V ISA's ability to decode instruction type early in the pipeline\n\nThe proposed \"Early Hint-based Event (EHE)\" scheduling algorithm prioritizes issuing instructions from a thread when an upcoming instruction for that thread is detected to potentially cause a delay. This aims to propagate the instruction into the pipeline earlier to help hide latency.\n\nBased on this, the most relevant references seem to be:\n\n{\n  \"[b12]\": 0.9,\n  \"[b13]\": 0.8, \n  \"[b14]\": 0.7,\n  \"[b15]\": 0.8\n}\n\n[b12], [b13], [b14] and [b15] are directly cited when discussing the main traditional scheduling approaches of fine-grain, coarse-grain and SMT that the proposed approach builds upon. \n\nIn particular, [b13] presents an interleaved multithreaded processor that the proposed architecture is compared against. [b14] discusses the round-robin scheduling used in fine-grain multithreading which is the default mode of the proposed algorithm. And [b15] covers the coarse-grain \"switch-on-event\" technique that the EHE algorithm aims to improve upon by detecting potential delay-causing events earlier.\n\nSo in summary, while the paper cites a broad set of references to cover background on RISC-V and hardware multithreading in general, the core novel contributions seem most directly inspired by and compared to the scheduling techniques in [b12]-[b15]. The strong methodological similarities and comparative language used when citing these references justify their high relevance scores.", "625f6bf75aee126c0ffb36b6": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"[b33]\": 0.9,\n  \"[b18]\": 0.8,\n  \"[b10]\": 0.7,\n  \"[b45]\": 0.6,\n  \"[b7]\": 0.5\n}\n\nRationale:\n- [b33] is likely the most relevant source paper. The text mentions that for the Diagnosis-oriented Dialogue Policy (DDP) task, \"the training data required for DDP is exactly the structured symptom features the SLI task needs to predict.\" This indicates the DDP task builds directly upon the work in [b33] on \"datasets that contain structured symptom features to learn the dialogue policy for symptom based automatic diagnosis.\"\n\n- [b18] is also highly relevant, as it introduces the CMDD dataset \"for medical information extraction task, which is extract mentioned entities and their corresponding status.\" This relates to the Named Entity Recognition and Symptom Label Inference tasks in the current paper.\n\n- [b10] directly inspires the Medical Report Generation task, as evidenced by the line \"Medical report captures and summarizes the important parts of the medical conversation needed for clinical decision making and subsequent follow ups [b10].\"\n\n- [b45] is relevant to the Named Entity Recognition task - \"Medical NER task aims to recognize predefined medical named entities from medical texts [b45]\".\n\n- [b7] introduces the SAT dataset for medical information extraction, which has some relevance to the entity recognition and symptom inference tasks, though the connection is not as direct as [b18].\n\nThe other referenced papers like [b39], [b13], [b29] etc. provide relevant background on existing medical dialogue datasets and tasks, but do not seem to be as directly influential on the novel contributions of this specific paper compared to the ones highlighted above. The methodology and tasks build more directly on [b33], [b18], [b10] and [b45] in particular.", "63bcd73690e50fcafdefa108": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria provided:\n\nThe key novel contributions of this paper seem to be:\n1) Proposing an automated, fine-grained attribute completion framework called AutoAC for heterogeneous graphs with missing node attributes. It can search the optimal completion operation for each node.\n2) Proposing a differentiable search strategy with a continuous relaxation of the search space and discrete constraints to make the search efficient. \n3) Introducing an auxiliary unsupervised graph node clustering task to reduce the dimension of completion operation parameters and improve search efficiency.\n\nBased on this, the most relevant source papers appear to be:\n\n{\n  \"[b17]\": 0.9,\n  \"[b18]\": 0.8,\n  \"[b0]\": 0.7,\n  \"[b7]\": 0.7,\n  \"[b37]\": 0.7\n}\n\nReasoning:\n- [b17] is directly cited as proposing an attention-based attribute completion method for heterogeneous graphs, but it is coarse-grained and only considers 1-hop neighbors. This paper aims to improve upon its limitations, so [b17] is likely the most pertinent inspiration.\n- [b18] introduces proximal iteration which is leveraged in this paper to efficiently solve the constrained optimization problem in the search process. It's a key enabling technique.\n- [b0], [b7] and [b37] propose the node aggregation methods (GraphSage, GCN, APPNP) that inspire the topology-dependent attribute completion operations in the search space. They provide an important foundation.\n\nThe other referenced papers, while relevant for background and motivation, do not seem to be as directly inspirational for the core novel contributions and techniques of this particular paper based on the provided text. The auxiliary clustering task appears to be an original contribution not directly attributed to a source.", "6215a4242c356815940385b3": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b4]\": 0.9,\n  \"[b26]\": 0.85,\n  \"[b28]\": 0.8,\n  \"[b15]\": 0.7\n}\n\nConfidence scores:\n- [b4] and [b26] have the highest confidence scores of 0.9 and 0.85 respectively. The paper directly states that it draws inspiration from the \"tremendous success of DNN compression, particularly the lottery ticket (LT) finding [b4] [b15] [b26]\". It aims to leverage the lottery ticket finding to compress GCNs.\n\n- [b28] has a confidence score of 0.8. The paper compares its method to the iterative pruning method in [b28], stating \"compared with the iterative pruning method, e.g., UGS [b28] ), we for the first time show that early-bird (EB) tickets exist in both GCN graphs and networks\". So [b28] serves as an important reference point that the proposed method improves upon.\n\n- [b15] has a lower confidence score of 0.7, as it is mentioned alongside [b4] and [b26] as inspiring the lottery ticket idea, but the paper focuses more on comparing to [b4] and [b26] directly.\n\nThe paper's key novel contributions are discovering the existence of graph early-bird (GEB) tickets in GCNs and developing a graph-network co-optimization framework (GEBT) to jointly draw early-bird tickets between GCN graphs and models to boost efficiency. The selected references [b4], [b26], [b28] and [b15] directly motivated and enabled these main novel aspects of the paper.", "63a1751790e50fcafd1f4880": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b34]\": 0.9,\n  \"[b18]\": 0.8,\n  \"[b39]\": 0.7,\n  \"[b31]\": 0.6\n}\n\nConfidence scores:\n[b34] (S2G): 0.9 - The paper directly states \"Motivated by the design of S2G [b34] and FE2H [b18], we propose a simpler model called C2FM\". This indicates the proposed C2FM model takes strong inspiration from the S2G model architecture. The paper also discusses how S2G's strong performance without using graphical modeling motivated their approach.\n\n[b18] (FE2H): 0.8 - Along with S2G, FE2H is directly cited as a key inspiration for the C2FM model proposed in this paper. The authors aim to examine the impact of label smoothing in a simpler model compared to FE2H.\n\n[b39]: 0.7 - This reference explores label smoothing methods for machine reading comprehension, which directly relates to the novel \"F1 smoothing\" method proposed in this paper for answer span extraction. The authors were likely inspired by the word overlap smoothing in [b39] to develop their F1 smoothing approach.\n\n[b31] (SAE): 0.6 - The paper mentions that both SAE and S2G prioritize documents containing answer spans (gold documents) in their retrieval. This inspired the authors to introduce an answer aware distribution in their coarse retrieval label smoothing. However, the connection is less direct than the previous references.\n\nIn summary, the key source papers that inspired the main contributions appear to be S2G [b34] and FE2H [b18] for the overall C2FM model architecture, and [b39] for the novel F1 smoothing method. SAE [b31] provided some inspiration for the coarse retrieval label smoothing but has a weaker connection to the core ideas.", "63d9d87b90e50fcafd580980": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b24]\": 0.9,\n  \"[b39]\": 0.8,\n  \"[b38]\": 0.7,\n  \"[b32]\": 0.6,\n  \"[b26]\": 0.5\n}\n\nRationale:\nThe paper introduces two main novel contributions to address challenges in few-shot object detection (FSOD):\n1. Class-Agnostic Aggregation (CAA) to reduce class bias and improve robustness \n2. Variational Feature Aggregation (VFA) to model intra-class variance and sample robust class-level features\n\nFor VFA, the paper draws direct inspiration from variational feature learning approaches in [b24] and [b39]. Key phrases like \"Inspired by recent progress in variational feature learning [b24] [b39]\" indicate these are highly pertinent sources that motivated a core contribution.\n\nThe overall meta-learning framework builds upon Meta R-CNN [b38], which the authors use as a baseline. They also compare extensively to the TFA method [b32] and align hyperparameters to build a strong baseline.\n\nFinally, the classification-regression decoupling takes inspiration from DeFRCN [b26] to learn translation-invariant features for classification and translation-covariant features for regression separately.\n\nThe other referenced papers provide context on FSOD and object detection in general, but these 5 papers emerge as the most central influences based on the direct methodological inspirations drawn from them to develop the primary novel aspects of the paper. The variational feature learning papers [b24] and [b39] are prioritized at the top since the authors explicitly cite them as inspiration.", "62ce38205aee126c0f18bb75": "Based on analyzing the text, here are the key source papers that likely inspired or motivated this work, in JSON format with confidence scores:\n\n{\n  \"[b4]\": 0.9,\n  \"[b19]\": 0.8,\n  \"[b1]\": 0.7,\n  \"[b5]\": 0.6,\n  \"[b13]\": 0.5\n}\n\nThe key reasons for selecting these papers are:\n\n- [b4] is cited multiple times in the context of language models producing well-calibrated predictions, which is a core focus of this paper. The high frequency of mentions and the direct relevance to the paper's main contributions warrant a high confidence score.\n\n- [b19] provided the BIG Bench evaluation suite that was heavily used in this paper to assess calibration across diverse tasks. The authors note they are \"indebted to the BIG Bench collaboration\". This direct use and acknowledgment merits a high confidence score.  \n\n- [b1] is referenced as the source of the model architecture and training setup used in this paper, with the only difference being training for more tokens. Reusing model architectures from this prior work shows significant influence.\n\n- [b5] introduced the MMLU benchmark that revealed promising calibration results that motivated further experimentation in this paper. This catalyzing role earns a moderate confidence score.\n\n- [b13] is called out as \"perhaps the work most similar to ours\" in applying metacognition and self-evaluation to improve calibration, the core novel contribution here. This direct acknowledgement of closely related prior work deserves moderate confidence.\n\nThe other referenced papers provide relevant background and additional benchmarks, but do not seem to be as pivotal in directly inspiring the main ideas and methodologies of this paper based on the text.", "629587465aee126c0fe149f5": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b3]\": 0.9,\n  \"[b27]\": 0.8,\n  \"[b10]\": 0.7,\n  \"[b24]\": 0.6\n}\n\nRationale:\n\nThe paper introduces the novel task of \"verbalized probability\" where language models are finetuned to express epistemic uncertainty about their answers using natural language. This is directly inspired by and builds upon prior work on calibration of language models:\n\n- [b3] and [b4] are cited in the introduction as showing that current state-of-the-art language models perform well on question-answering tasks but often produce false statements when generating long-form text. This directly motivates the paper's focus on calibrating language models to express uncertainty.\n\n- [b10] is a key prior work on calibration that focuses on model log-probabilities or \"logits\". The paper contrasts this with their approach of using natural language to express epistemic uncertainty. \n\n- [b27] is cited as prior work on evaluating how well calibration generalizes under distribution shift between training and evaluation data. The paper adopts a similar approach by training on an \"Add-subtract\" dataset and evaluating on a \"Multi-answer\" dataset with a different distribution.\n\n- [b24] introduces the MAD metric for evaluating calibration which is one of the key metrics used in this paper's experiments.\n\nSo in summary, [b3], [b10], [b27] and [b24] represent the most central prior works that directly inspired and enabled the novel contributions of this paper around verbalized probabilities and evaluating their calibration. The high confidence scores reflect the explicit comparisons and connections drawn in the text between these references and the core methods and experiments.", "629587475aee126c0fe14c42": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b11]\": 0.9,\n  \"[b14]\": 0.8,\n  \"[b1]\": 0.7,\n  \"[b31]\": 0.6,\n  \"[b44]\": 0.5\n}\n\nThe key points that led me to identify these as the most relevant source papers are:\n\n1. The paper is heavily inspired by and builds upon the idea of discrete demonstration from [b11], as evidenced by phrases like \"Compared with prior discrete demonstrations described in [b11]\". This is the clearest indication of a source paper that directly motivated the current work.\n\n2. The paper adapts ideas from kNN-LM [b14] to the prompt learning setting, as stated in \"Different from kNN-LM [b14] that uses tokens to augment the language modeling directly, we explicitly take advantage of prompt-based instance representation for classification tasks\". This shows [b14] provided key methodological inspiration.\n\n3. The paper contrasts eager learners like PLMs with lazy learners like kNN, referencing [b1] when discussing the properties of kNN. This suggests [b1] provided important conceptual background.\n\n4. The loss modulation approach is compared to Focal Loss from [b31], indicating this paper offered relevant technical ideas.\n\n5. The paper mentions that their contextual representations are more informative than those from Sentence-BERT [b44], suggesting this was an important reference point in terms of representation learning.\n\nThe other referenced papers, while providing useful background, do not seem to be as centrally tied to the novel contributions and methodological approaches of this paper based on the text. The key phrases and comparisons single out the 5 papers above as the most direct sources of inspiration and technical foundations for the core ideas presented.", "624fa8db5aee126c0f3a5b79": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b27]\": 0.9,\n  \"[b21]\": 0.8,\n  \"[b24]\": 0.7,\n  \"[b3]\": 0.6\n}\n\nConfidence scores:\n[b27] (DCR): 0.9 - The paper directly builds upon the Dense Cross-modal Retrieval (DCR) method, which proves that token-wise interaction between frames and words reveals more fine-grained cross-modal knowledge. The proposed method aims to improve upon DCR by considering hierarchical interactions.\n\n[b21] (CLIP4Clip): 0.8 - CLIP4Clip is referenced multiple times as a pioneering work in transferring image-text knowledge from CLIP to the video-text domain. The temporal ensemble modules from CLIP4Clip are used as a starting point.\n\n[b24] (CLIP): 0.7 - The paper leverages CLIP as the initial visual and text encoders, similar to CLIP4Clip and DCR. CLIP provides the foundation of large-scale pre-training knowledge that the method builds upon.\n\n[b3] (CAMoE): 0.6 - CAMoE is another method that builds on the pre-trained CLIP model for video-text retrieval. It is referenced as part of the related work.\n\nThe paper aims to improve upon the limitations of considering only single cross-modal interactions in previous methods like DCR and CLIP4Clip. The novel contributions are the hierarchical modeling of frame-clip-video and word-phrase-sentence granularities, along with adaptive label denoising and marginal sample enhancement strategies. The most directly relevant references support these aspects.", "634d805690e50fcafd4e07d5": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b33]\": 0.9,\n  \"[b69]\": 0.8,\n  \"[b18]\": 0.8,\n  \"[b45]\": 0.7,\n  \"[b55]\": 0.7,\n  \"[b58]\": 0.7,\n  \"[b62]\": 0.7,\n  \"[b70]\": 0.7\n}\n\nThe reasoning is as follows:\n\n- The paper cites [b33] when discussing the popularity of JavaScript, Java and Python, indicating these references directly motivated studying the performance of these specific managed languages. \n\n- [b69] about Spark running on the JVM, [b18] about Kubernetes in Go, and [b45] about an OS kernel in Go are highlighted as examples of systems software implemented in managed languages, which seems to be a key motivation for the paper's performance study.\n\n- The shift to cloud computing and data analytics exposing interpreter startup overhead is discussed citing [b55], [b58], [b62], [b70]. This appears to be an important novel insight that guided the paper's focus on interpreter performance.\n\n- Overall, the references related to the growing use of managed languages in systems software and the impact of cloud/analytics workloads on exposing interpreter overheads seem to be called out as providing the most direct inspiration and motivation for the specific languages studied and the focus on startup/interpreter performance. The repetition of citations around these points suggests they were central to the work.\n\nI limited the list to the top 8 most pertinent references based on the criteria of being directly cited as inspiration/motivation and being connected to the paper's novel aspects of studying interpreter performance for popular managed languages in the context of modern cloud/analytics workloads. Let me know if you would like me to elaborate on the analysis further.", "62281ae45aee126c0f7aa8a8": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b33]\": 0.9,\n  \"[b20]\": 0.8,\n  \"[b37]\": 0.7,\n  \"[b11]\": 0.6,\n  \"[b19]\": 0.5\n}\n\nConfidence scores:\n- [b33] (Graphormer): 0.9 \nThe paper heavily relies on the Graphormer architecture to encode the label hierarchy. It directly states \"We model the label hierarchy with a customized Graphormer [b33].\" This indicates Graphormer is a key component of their novel approach.\n\n- [b20]: 0.8\nThe paper mentions \"[b20] proposes to regularize Transformer-based encoders for text classification tasks by FGSM [b9], an adversarial attack method based on gradient.\" Their own positive sample generation approach is inspired by and compared to the adversarial attack method in [b20].\n\n- [b37]: 0.7 \n[b37] is referenced multiple times as a state-of-the-art HTC model that encodes text and label hierarchy separately. The proposed approach aims to improve upon this by injecting the hierarchy into the text encoder instead. \n\n- [b11]: 0.6\nMoCo [b11] is mentioned as a influential contrastive learning work in computer vision. The paper adopts the general contrastive learning framework, likely taking some inspiration from [b11], though the NLP application is quite different.\n\n- [b19]: 0.5\n[b19] is cited as an example of using random masking for data augmentation to generate positive samples in contrastive learning. The proposed positive sample generation approach is a more targeted alternative to this.\n\nIn summary, the Graphormer architecture [b33] and the adversarial positive sample generation idea from [b20] emerge as the most direct influences on the novel contributions of this paper. [b37], [b11] and [b19] provide important context but are less central to the core methods introduced.", "63aa965790e50fcafd008008": "Here is my analysis to identify the most pertinent source papers for the given paper, based on the criteria you provided:\n\nThe key novel contributions of this paper are:\n1. Proposing a new semantic parsing task Text-to-CQL to translate natural language queries to Cypher Query Language (CQL) used by the Neo4j graph database\n2. Creating the first dataset SpCQL for the Text-to-CQL task, containing a Neo4j database and 10,000 query-CQL pairs\n\nThe paper mentions being inspired by or building upon the following previous works:\n- WikiSQL [b9] and Spider [b8] datasets for the Text-to-SQL task, which inspired the design of the SpCQL dataset for Text-to-CQL\n- Text-to-SPARQL task [b5] [b6] for graph data operations, but Text-to-SQL is used as a more appropriate comparison due to language similarities between SQL and SPARQL\n\nThe key source papers that directly contribute to the novel aspects, based on methodological similarities and comparative phrases, are:\n- [b8] and [b9] which introduce the WikiSQL and Spider datasets for Text-to-SQL. SpCQL is designed based on insights from these.\n- [b5] and [b6] on the Text-to-SPARQL task showing the need for graph data operation methods, but Text-to-SQL is argued as a better comparison.\n\nIn summary, here are the most pertinent source papers in JSON format with confidence scores:\n\n{\n  \"b8\": 0.9,\n  \"b9\": 0.9,\n  \"b5\": 0.7,\n  \"b6\": 0.7\n}\n\nThe papers [b8] and [b9] are given the highest confidence scores as they directly inspire the main novel contribution of creating the SpCQL dataset. [b5] and [b6] have lower confidence as they motivate the general problem of graph querying but are not used as the main basis for the Text-to-CQL methods in this paper.", "63eef09c90e50fcafda0d2a1": "Here are the top source papers for this paper based on the criteria you provided, along with confidence scores:\n\n{\n  \"[b24]\": 0.95,\n  \"[b33]\": 0.9,\n  \"[b27]\": 0.85,\n  \"[b11]\": 0.8,\n  \"[b39]\": 0.7,\n  \"[b26]\": 0.65,\n  \"[b30]\": 0.6\n}\n\nThe key reasons for selecting these papers as the most influential sources are:\n\n1. The paper directly states that it aims to address limitations of \"[b24] (SGL)\", \"[b33] (SimGCL)\", \"[b27] (HCCF)\" and \"[b11] (NCL)\" which are existing graph contrastive learning approaches for recommendation. This shows these works heavily motivated the proposed LightGCL method.\n\n2. The novel contributions of efficiently incorporating global collaborative signals via SVD is compared to the graph augmentation strategies used in \"[b39] (GCA)\", \"[b26] (SimGRACE)\" and \"[b30] (AutoGCL)\", indicating these are closely related works.\n\n3. The titles of the selected papers contain highly relevant keywords like \"graph contrastive learning\", \"self-supervised learning\", \"graph augmentation\" which match the core techniques in LightGCL.\n\n4. Less relevant historical references like traditional graph neural network models are excluded, since the focus is on finding papers that directly inspired the contrastive learning innovations in LightGCL.\n\nSo in summary, the selected sources appear to be the most pertinent prior works that motivated and are built upon by the novel lightweight graph contrastive learning approach proposed in this paper. The confidence scores reflect the degree of relevance inferred from the text.", "63eef09c90e50fcafda0cfa0": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"12\": 0.9,\n  \"15\": 0.9,\n  \"29\": 0.8,\n  \"53\": 0.8,\n  \"14\": 0.7,\n  \"38\": 0.7\n}\n\nThe key points are:\n\n- References [12], [15], [29] and [53] are cited when discussing how existing graph pre-training approaches design self-supervised tasks based on graph properties like node features, links, and local/global patterns. This directly relates to one of the main challenges the paper aims to address - unifying pre-training with various downstream tasks.\n\n- Reference [14] is cited along with [15], [29] and [53] for graph pre-training approaches, indicating it is also highly relevant. \n\n- Reference [38] (GPPT) is the pioneering work that introduced prompting to graph pre-training. The paper directly builds upon and extends the ideas in [38] to enable prompting for multiple downstream tasks, which is one of the key novel contributions.\n\n- The paper states \"Inspired by the application of pre-training models in language [6] and vision [0] domains, graph pretraining [49] emerges as a powerful paradigm\", but references [0], [6] and [49] provide more historical context rather than directly inspiring the methodological innovations.\n\nIn summary, references [12], [14], [15], [29], [38] and [53] appear to be the most central prior works that motivated and enabled the key ideas proposed in this paper for unifying graph pre-training and prompting across diverse downstream tasks. The high confidence scores reflect the clear textual evidence of their strong influence on this work.", "62620f1c5aee126c0f686d0d": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\nThe key novel contributions of this paper seem to be:\n1. Proposing DropMessage, a new random dropping method for message-passing GNNs that drops elements of the message matrix\n2. Unifying existing random dropping methods like Dropout, DropEdge, and DropNode into the DropMessage framework \n3. Theoretically analyzing how random dropping introduces regularization and improves GNN robustness\n4. Showing DropMessage has advantages in stabilizing training and preserving information diversity compared to other methods\n\nBased on this, the most relevant references appear to be:\n\n{\"14\": 0.9, \"19\": 0.9, \"48\": 0.8}\n\n[14] and [19] directly relate to the existing DropNode and Dropout methods that are unified into the DropMessage framework and compared against. The paper shows these are special cases of DropMessage.\n\n[48] introduces the DropEdge method which is also unified into the DropMessage framework and compared with the proposed method. \n\nWhile other references like [6], [12], [13] etc. provide useful background on GNNs in general, they do not seem to directly motivate or inspire the core novel contributions around DropMessage and unifying random dropping methods.\n\nThe theoretical analysis and experiments demonstrating DropMessage's advantages in regularization, training stability, and information diversity also appear to be novel contributions not directly drawing from the cited references.\n\nSo in summary, references [14], [19] and [48] introducing DropNode, Dropout and DropEdge seem to be the key source papers that most directly motivated and inspired the main innovations in this work, with very high confidence scores. The other references provide important context but are less central to the novel aspects of DropMessage.", "63a413f690e50fcafd6d1a7a": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"b31\": 0.9,\n  \"b32\": 0.8,\n  \"b0\": 0.7,\n  \"b21\": 0.6\n}\n\nConfidence scores:\nb31: 0.9\nThe paper heavily relies on the expressive power of Graph Isomorphism Networks (GINs) from [b31] to distinguish graph structures, as evidenced by this key phrase: \"Since g?(Aj) = g?(? ?(Aj)), and ?i and ?j are sums over all N nodes, hence they are permutation-invariant and we can remove all operation ?(?) in Eq. (3) and Eq. (4).\"\n\nb32: 0.8 \nThe Alignment Regularization (AReg) module uses a permutation-invariant DeepSets architecture from [b32] to guarantee injectiveness, as stated here: \"AReg implements the readout function g? with a one-layer permutation-invariant DeepSets [b32] to guarantee injectiveness\".\n\nb0: 0.7\nThe paper compares against and builds upon the SimGNN model from [b0]. For example, it states: \"EGSC [b21] speeds up the similarity learning by dropping the matching model from SimGNN [b0].\"\n\nb21: 0.6\nThe recent EGSC approach from [b21] is discussed as a point of comparison and motivation for the proposed ERIC framework. The paper notes that \"EGSC cannot capture finer-grained similarity information, and thus results in suboptimal prediction performance.\"\n\nIn summary, the key source papers that inspire and motivate the novel contributions of this paper are [b31] for the use of expressive GINs, [b32] for the permutation-invariant DeepSets architecture used in AReg, [b0] as a foundation GNN-based GSC model, and [b21] as a recent approach that the proposed ERIC framework aims to improve upon in terms of accuracy-efficiency tradeoff. The paper directly builds upon the key ideas from these sources to develop its main technical innovations.", "637c3dcf90e50fcafd77c35b": "Here is my analysis of the most pertinent source papers for this paper, based on the criteria provided:\n\nThe key novel contributions of this paper are:\n1. Showing that instruction block accesses in datacenter workloads are bursty, with significant spatial and short-term temporal locality within a burst, but potentially long reuse distances between bursts. \n2. Proposing a spatio-temporal separation using an i-Filter to capture intra-burst locality and an i-cache for inter-burst locality.\n3. Developing an admission control mechanism using a two-level predictor to determine if a block evicted from the i-Filter should be inserted into the i-cache or discarded, based on comparing its expected reuse distance to the i-cache contender.\n\nThe paper mentions being inspired by the following prior works:\n- [b60] which observes bursty accesses to data blocks and proposes cache management policies based on burstiness. This directly inspired looking at burstiness in the instruction stream.\n- [b28], [b48] which propose adding a filter buffer for instructions, whose similar form is adopted as the i-Filter in this work.\n- [b91] the two-level branch predictor, whose structure is leveraged to develop the admission control predictor.\n\nIn terms of methodological similarities, the paper compares its technique to:\n- [b63] GHRP, a recent i-cache replacement policy\n- [b22] DSB and [b57] OBM, recent d-cache bypassing policies adapted for the i-cache\n- [b43] VVC, a recent d-cache victim cache adapted for the i-cache\n\nBased on this analysis, I would rank the most pertinent source papers as follows, with confidence scores:\n\n{\n  \"[b60]\": 0.9,\n  \"[b28]\": 0.8, \n  \"[b48]\": 0.8,\n  \"[b91]\": 0.7,\n  \"[b63]\": 0.6,\n  \"[b22]\": 0.5,\n  \"[b57]\": 0.5,  \n  \"[b43]\": 0.4\n}\n\nThe papers [b60], [b28], [b48], and [b91] are given the highest scores as they directly inspire key novel aspects of the proposed technique. The other papers are relevant for comparison but less central to the main contributions.", "634d805b90e50fcafd4e1168": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b49]\": 0.9,\n  \"[b51]\": 0.9,\n  \"[b31]\": 0.8,\n  \"[b33]\": 0.7,\n  \"[b52]\": 0.7,\n  \"[b36]\": 0.6,\n  \"[b26]\": 0.5\n}\n\nRationale:\n- The paper cites [b49] and [b51] when discussing load time code randomization and binary rewriting as key functionalities the dynamic loader should provide. The phrasing \"Load time code randomization and binary rewriting, provided by the dynamic loader, are widely adopted for profiling, security hardening, and architectural adaptation [b49] [b51]\" indicates these two references directly inspired core aspects of iFed's design.\n\n- [b31] and [b33] are cited as examples of library debloating, which is listed as a key memory management feature the loader should offer. The paper seems to draw direct motivation from these works.\n\n- [b52] is referenced alongside [b51] as an example of how the loader can enable advanced binary rewriting and execution control. While not as strongly emphasized as [b51], it still appears to be an important influence.\n\n- [b36] describes the CubicleOS loader which inspired iFed's isolation functionality, though the connection is not as direct as the above references. \n\n- [b26] is cited when discussing hugepage usage, but more as background rather than direct inspiration for iFed's concatenation pass.\n\nThe other references, while providing important context, do not appear to be as centrally tied to iFed's novel contributions based on the phrasing used. The key optimization passes seem most directly motivated by [b49], [b51], [b31] and [b33].", "62d4cf305aee126c0fa9ab5a": "Here is my analysis of the most pertinent source papers for this paper, based on the criteria you provided:\n\n{\n  \"[b2]\": 0.9,\n  \"[b29]\": 0.8,\n  \"[b16]\": 0.7,\n  \"[b4]\": 0.6,\n  \"[b9]\": 0.6,\n  \"[b13]\": 0.6,\n  \"[b22]\": 0.5,\n  \"[b12]\": 0.5\n}\n\nThe key points that led me to these source papers are:\n\n1. The paper advances one-shot approaches from [b2] and [b29] to the recommendation domain to efficiently search models/subnets in their proposed NASRec search space. This indicates [b2] and [b29] were direct inspirations for their methodology.\n\n2. The paper mentions that finding a good backbone model upon multi-modality features is a common practice, citing [b4], [b9], [b13] and [b16] as examples. This suggests these papers influenced their problem formulation and approach. \n\n3. The paper compares their NASRec method to AutoCTR [b22] and DNAS [b12], indicating these are closely related prior works in applying NAS to recommender systems that motivated their work.\n\n4. The frequent comparisons to DLRM [b16] when discussing challenges and differences between vision and recommendation domains suggests [b16] was an important reference point for them.\n\nI assigned the highest confidence scores to [b2] and [b29] since the paper directly states they advanced those one-shot NAS approaches to enable their method. The other papers have lower scores but are still relevant as they represent key prior works that shaped the problem and solution space the authors were working in. Let me know if you need any clarification or have additional questions!", "63dcdb422c26941cf00b6094": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b18]\": 0.9,\n  \"[b30]\": 0.8,\n  \"[b5]\": 0.7,\n  \"[b3]\": 0.6,\n  \"[b26]\": 0.5\n}\n\nConfidence scores:\n[b18]: 0.9 - This reference is mentioned multiple times as demonstrating the importance of both selecting proper neural architectures and utilizing appropriate graph encoding strategies. The proposed AutoGT aims to jointly optimize architecture and encoding, building upon the insights from [b18].\n\n[b30]: 0.8 - Graphormer [b30] is highlighted as a state-of-the-art graph Transformer that won a KDD Cup challenge by encoding various graph structural information. Several of the graph encoding strategies in AutoGT's search space, like centrality encoding, spatial encoding and edge encoding, are inspired by [b30].\n\n[b5]: 0.7 - The Laplacian eigenvector node attribution augmentation in AutoGT's search space is directly adopted from this pioneering work on graph Transformers. \n\n[b3]: 0.6 - AutoGT's encoding-aware supernet training leverages the weight-entanglement technique from AutoFormer [b3] to improve consistency and efficiency. While AutoFormer targets vision Transformers, AutoGT extends this to the graph domain.\n\n[b26]: 0.5 - NAS-BERT [b26] demonstrates neural architecture search for Transformers in NLP. While it doesn't handle graphs, it likely provided high-level inspiration for AutoGT to automate Transformer design.\n\nThe other references, while relevant for background, seem less directly influential on AutoGT's main novel contributions around joint architecture-encoding search and the graph-specific search space and training techniques. The key phrases indicating direct inspiration focus on [b18], [b30], [b5] and [b3].", "633cf5cf90e50fcafd772e24": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b18]\": 0.9,\n  \"[b28]\": 0.8,\n  \"[b12]\": 0.7,\n  \"[b56]\": 0.7,\n  \"[b5]\": 0.6\n}\n\nConfidence scores:\n[b18] (Atlas, 0.9): The paper directly compares its approach to Atlas, highlighting that \"in contrast, we show that still a large underestimated amount of knowledge can be retrieved from LLMs' model weights through better-designed prompting.\" This indicates Atlas was a key motivation.\n\n[b28] (0.8): The paper states \"[b28] pioneer the work of solving math word problems by generating step-by-step human-readable solutions described by natural language and math equations before the final answer.\" This directly inspired their recite-and-answer approach.\n\n[b12] (0.7): Extends the work of [b28] which the paper heavily relies on. \"e.g. equations only, to illustrate the intermediate solving steps [b43] [b0] [b9] . [b12] extend [b28] by constructing a much larger dataset to finetune a pre-trained large language model to solve math word problems\"\n\n[b56] (0.7): Introduces chain-of-thought prompting which combines natural language rationales with few-shot prompting. The paper adapts this by \"instead of generating a chain of thought for multi-step reasoning questions, we decompose the process of answering a knowledge-intensive question into two steps: recite the relevant knowledge stored in the model parameters, and then answer the question.\"\n\n[b5] (0.6): Analyzes memorization capabilities of LLMs which this paper relies on. \"While these works mainly analyze the fundamental properties of memorization in the exact setting, where exactly N tokens are used as the prompt to reproduce the suffix of the prompt, our work relies on \"fuzzy memorizaiton\", where the prompts tend to not be exactly the same as the training data, but still improve the memorization accuracy.\"\n\nThe novel contributions appear to be the recite-and-answer approach for closed-book question answering, which decomposes the task into knowledge recitation and question answering steps. Key inspirations were chain-of-thought prompting [b56], step-by-step rationales [b28], and leveraging the memorization capabilities of LLMs [b5].", "635b486790e50fcafd32f8b1": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b23]\": 0.9,\n  \"[b62]\": 0.8,\n  \"[b38]\": 0.7,\n  \"[b10]\": 0.7,\n  \"[b34]\": 0.6\n}\n\nRationale and key points:\n\n- The paper takes direct inspiration from [b23] in using a contrastive loss based on entailment pairs to induce a more isotropic representation space for bias mitigation. This is a core novel contribution.\n\n- The counterfactual data augmentation approach of swapping gender terms is directly motivated by the technique proposed in [b62]. \n\n- [b38], [b10] and [b34] are highlighted as the most relevant task-agnostic baselines that the paper compares against, since they also aim to produce general-purpose debiased representations. The paper builds upon ideas from these works.\n\n- Specific methodological similarities are called out, e.g. [b38] and [b10] also use generic sentences from different domains for training, which this paper improves upon by leveraging entailment pairs.\n\n- The alignment loss proposed to minimize distance between an entailment pair and its augmented version seems to be a novel contribution building on top of the foundation of the baselines.\n\nSo in summary, [b23] and [b62] emerge as the strongest sources of inspiration for the core ideas, while [b38], [b10] and [b34] serve as the key relevant prior works that this paper improves upon in its approach to task-agnostic debiasing. The contrastive framework using NLI data and the alignment loss are the main novel contributions.", "628749485aee126c0fff0290": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\nThe key novel contributions of this paper are:\n1. Automatically searching for biased prompts that maximize disagreement in generating stereotype words between demographic groups, without relying on external corpora. This enables probing the biases encoded in the language model.\n2. Debiasing the language model by fine-tuning with an equalizing loss that minimizes the disagreement between the predicted [MASK] token distributions conditioned on different demographic words.\n\nBased on these main contributions, the most relevant references appear to be:\n\n{\n  \"[b34]\": 0.9,\n  \"[b29]\": 0.8,\n  \"[b32]\": 0.7,\n  \"[b35]\": 0.6\n}\n\n[b34], [b29] and [b32] are highly relevant because they demonstrate how prompts can be used to probe knowledge, biases and toxic content in language models. This directly inspires the automatic biased prompt search in this paper.\n\n[b35] is relevant because it provides the key motivation that a fair NLP system should produce scores independent of the choice of demographic identities mentioned. The equalizing loss in this paper aims to achieve this.\n\nThe paper also compares to prior debiasing approaches like [b13], [b20] and [b25] that rely on external corpora. But the core novelty is in not requiring such corpora by directly probing the model itself. So those references, while discussed, do not seem to be the primary inspiration based on the criteria provided.\n\nIn summary, [b34], [b29], [b32] and [b35] emerge as the most central and influential references that motivated the main technical contributions of automatically searching for biased prompts and using an equalizing loss for debiasing. The paper's key insight is combining prompt-based bias probing with prompt-based debiasing to enable a fully self-contained approach.", "627cdc8e5aee126c0f50229e": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b41]\": 0.9,\n  \"[b44]\": 0.8,\n  \"[b33]\": 0.7,\n  \"[b5]\": 0.6,\n  \"[b31]\": 0.6,\n  \"[b37]\": 0.5\n}\n\nConfidence scores:\n[b41] DProf (0.9): This paper is directly cited as the only prior work that can identify all types of cache misses for Linux kernel data structures, but requires significant manual effort. The current paper aims to overcome DProf's shortcomings.\n\n[b44] CCProf (0.8): CCProf is discussed in detail for its approach to identify conflict misses based on the portion of misses in cache sets. However, the current paper points out limitations in CCProf's method and proposes an improved technique based on memory access patterns.\n\n[b33] (0.7): This work on array regrouping is cited as a technique to reduce capacity misses, which the current paper aims to identify. \n\n[b5], [b31], [b37] (0.5-0.6): These papers propose techniques to identify specific types of cache misses like coherence misses. The current paper discusses their limitations in identifying all cache miss types in a unified way, which is the main goal here.\n\nThe key novel contributions of this paper seem to be: 1) A unified profiler to identify all major types of cache misses 2) A hybrid coarse-grained and fine-grained sampling approach 3) Techniques to differentiate cache misses caused by the allocator vs application 4) Practical filtering mechanisms to report only significant issues. The selected references above relate to and motivate these core aspects of the paper.", "62d16e895aee126c0fd68310": "Here is my analysis of the most pertinent source papers for the given paper, in JSON format with confidence scores:\n\n{\n  \"[b7]\": 0.9,\n  \"[b28]\": 0.9,\n  \"[b38]\": 0.8,\n  \"[b75]\": 0.7,\n  \"[b22]\": 0.7,\n  \"[b55]\": 0.6,\n  \"[b98]\": 0.6\n}\n\nThe key phrases \"inspired by\", \"motivated by\", \"take inspiration\", and \"based on\" do not appear in the text. However, the paper introduces novel dynamic compilation and optimization techniques for packet processing programs, which are directly inspired by and build upon similar techniques used in generic programming languages like Java, JavaScript and C/C++. This is evident from the following excerpts:\n\n\"Dynamic compilation, in contrast, enables program optimization based on invariant data computed at run time and produces code that is specialized to the input the program is processing [b7] [b28] [b38]. The idea is to continuously collect run time data about program execution and then re-compile it to improve performance. This is a wellknown practice adopted by generic programming languages (e.g., Java [b28], JavaScript [b38], and C/C++ [b7]) and often produces orders of magnitude more efficient code as shown for data-caching services [b75], data mining [b22] and databases [b55] [b98].\"\n\nThe references [b7], [b28] and [b38] are cited as the pioneering works that inspire the dynamic compilation approach taken in this paper to optimize packet processing code at runtime. [b75], [b22], [b55] and [b98] provide additional motivation by demonstrating the performance benefits of dynamic compilation in other domains.\n\nThe core novel contributions of this paper - unsupervised dynamic compilation using domain-specific optimization techniques for networking code - directly build upon the fundamental concepts from these prior works in dynamic compilation for generic languages. Therefore, I assign the highest confidence scores to [b7], [b28] and [b38] as the most pertinent source papers.", "63dcdb422c26941cf00b604a": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b21]\": 0.9,\n  \"[b52]\": 0.8,\n  \"[b50]\": 0.7,\n  \"[b58]\": 0.6,\n  \"[b22]\": 0.6\n}\n\nConfidence scores:\n[b21] (MetaD2A): 0.9 \n- This paper is directly cited as the closest related work. The proposed method builds upon and improves MetaD2A in two key ways.\n- Uses the same graph encoder for architectures and transformer-based dataset encoder as [b21].\n\n[b52]: 0.8\n- The proposed method builds upon the few-shot Bayesian optimization framework from this paper, extending it to the NAS setting.\n- Uses a similar meta-learning approach to learn the deep kernel parameters.\n\n[b50] (Deep Kernel Learning): 0.7\n- The key idea of using deep kernel learning with Gaussian processes is inspired by this pioneering work.\n\n[b58]: 0.6 \n- The directed acyclic graph encoder used for encoding neural architectures is taken from this paper.\n\n[b22]: 0.6\n- The transformer-based dataset encoder is adapted from the set transformer architecture proposed in this paper.\n\nIn summary, the proposed method heavily relies on and extends ideas from [b21] and [b52] as the two most central references. Deep kernel learning [b50] and the specific architecture and dataset encoders from [b58] and [b22] also directly enable the main novel contributions. The other cited papers, while relevant, do not appear to be as critical to the core ideas.", "640fe64790e50fcafd9e237f": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b12]\": 0.9,\n  \"[b13]\": 0.85,\n  \"[b7]\": 0.8,\n  \"[b3]\": 0.7,\n  \"[b15]\": 0.65\n}\n\nConfidence scores:\n[b12] - 0.9: This reference is directly mentioned as inspiring the key idea of using high-pass and low-pass filters to capture node dissimilarities and similarities. The paper states \"On the node level, left multiplying L L L??? and ? ? ???? filters with ? ? ? can be understood as diversification and aggregation operations, respectively [b12].\"\n\n[b13] - 0.85: The paper cites this reference when discussing how GNNs act as low-pass filters that smooth node representations. This directly motivates their approach.\n\n[b7] - 0.8: Cited when explaining how Laplacian matrices act as high-pass filters that magnify differences in node features. A key part of their methodology.\n\n[b3] - 0.7: Mentioned as prior work showing labels can help learn neighborhood aggregation for heterophily. Helps motivate the challenge they aim to solve.\n\n[b15] - 0.65: Defines the homophily ratio metric which the paper uses to quantify heterophily vs homophily. Important for problem setup.\n\nThe other references, while providing useful background, do not seem to be called out as direct inspiration for the novel contributions of this paper based on the phrasing. The key novel aspects appear to be using high-pass and low-pass filters to capture both node similarities and dissimilarities, which references [b12], [b13] and [b7] directly contribute to.", "628749355aee126c0ffec021": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b51]\": 0.9,\n  \"[b9]\": 0.85,\n  \"[b43]\": 0.8,\n  \"[b62]\": 0.75,\n  \"[b41]\": 0.7,\n  \"[b28]\": 0.65,\n  \"[b10]\": 0.6\n}\n\nConfidence scores:\n- [b51] (Transformer paper): 0.9 \nThe paper heavily relies on the Transformer architecture proposed in [b51] as the basis for the sequence models like BERT that are evaluated. The methodology and experiments are directly inspired by and compared to Transformers.\n\n- [b9] (BERT paper): 0.85\nBERT is one of the key models evaluated in the experiments. The paper aims to compare BERT's performance to other approaches, making BERT a central motivation.\n\n- [b43] (DistilBERT paper): 0.8  \nDistilBERT is the other Transformer model evaluated alongside BERT. Its inclusion is directly motivated by the goal to assess lightweight Transformer variants.\n\n- [b62] (TextGCN paper): 0.75\nTextGCN is positioned as the seminal graph-based approach that inspired many follow-up works that are discussed and evaluated like HeteGCN, TensorGCN etc. Comparing to TextGCN is a key objective.\n\n- [b41] (HeteGCN paper): 0.7\n- [b28] (TensorGCN paper): 0.65  \n- [b10] (HyperGAT paper): 0.6\nThese three papers propose graph-based models that extend TextGCN and are evaluated as part of the graph-based model family. Their inclusion is motivated by TextGCN.\n\nIn summary, the Transformer papers, especially BERT, and the TextGCN paper and its graph-based successors emerge as the most influential source works that inspired the methodology and experiments. The core novel contributions of evaluating MLPs, BERT and graph-based models in the context of text classification are directly motivated by these previous works.", "6287045b5aee126c0f5ca192": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria provided:\n\nThe key novel contributions of this paper are:\n1. Extending the B-GRAP label propagation graph partitioning algorithm with new sampling-based heuristics called RD-B-GRAP (Random Degree) and HD-B-GRAP (High Degree). These heuristics aim to improve the seed node selection and label propagation to enhance partitioning quality and reduce computation time.\n2. Evaluating the proposed sampling-based algorithms on large graphs up to 42M vertices and 1.5B edges, and analyzing their impact on vertex/edge balance, edge cut, and propagation time compared to the original B-GRAP algorithm.\n\nThe paper mentions being directly inspired by and building upon the previous B-GRAP algorithm:\n\"In this paper we focused on the label propagation approach as an extension of B-GRAP a multi-objective and scalable Balanced GRAph Partitioning (B-GRAP) algorithm, which distributes vertices B-GRAP VB or edges B-GRAP EB in a balanced way [b8].\"\n\nIt also cites several other key references that the proposed sampling approaches draw from:\n\"In order to define our initialization heuristics, we identify two main steps: (i) seed vertices selection to retrieve the initial ? vertices; (ii) sampling to take ? vertices from the neighbors of selected seeds in order to initiate the LP process. In the second step, only ? vertices are selected from the neighbors of a seed at each iteration, this allows to reduce the impact of seeds with a high degree (hub vertices) on the sampling.\"\n\"[b24] proposed a graph Rank Degree sampling method using a edge selection rule based on a node ranking.\"\n\nBased on this analysis, the most pertinent source papers appear to be:\n\n{\n  \"b8\": 1.0,\n  \"b24\": 0.8\n}\n\n[b8] is clearly the primary inspiration, as the proposed algorithms directly extend B-GRAP. [b24] also seems highly relevant, as it proposes a node ranking based sampling method that the authors adapt for their seed selection and neighbor sampling steps. The other references provide useful background on graph partitioning and sampling techniques, but do not seem as directly influential on the core novel aspects of this paper.", "62708f615aee126c0fa6920a": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b16]\": 0.9,\n  \"[b14]\": 0.8,\n  \"[b46]\": 0.7,\n  \"[b33]\": 0.6,\n  \"[b48]\": 0.5\n}\n\nConfidence scores:\n- [b16] (0.9): The paper heavily relies on and improves upon the SimCSE model from this reference for contrastive learning of sentence representations. The instance weighting method uses SimCSE as the complementary model.\n\n- [b14] (0.8): The anisotropy problem of PLM-derived representations discussed in this reference is a key motivation for generating the noise-based negatives in the proposed approach. \n\n- [b46] (0.7): The paper adopts the in-batch negative sampling strategy and positive data augmentation methods like token shuffling and cutoff from this reference as a starting point.\n\n- [b33] (0.6): The gradient-based algorithm for optimizing the noise-based negatives is directly inspired by the virtual adversarial training method proposed in this reference.\n\n- [b48] (0.5): The contrastive learning framework pulling together positives and pushing apart negatives follows the general methodology popularized by this reference.\n\nThe paper's key contributions of reducing sampling bias through instance weighting of false negatives and generating non-uniform noise-based negatives most directly build upon the specific methods from references [b16], [b14], [b33] and [b48]. The frequent comparisons to SimCSE [b16] throughout the paper further confirm it as the most influential prior work.", "63fd715990e50fcafd146bdb": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"[b7]\": 0.9,\n  \"[b20]\": 0.8,\n  \"[b13]\": 0.7,\n  \"[b17]\": 0.6,\n  \"[b18]\": 0.5\n}\n\nRationale and key phrases:\n\nThe paper heavily relies on and is directly inspired by the GPT family of large language models, especially GPT-3 ([b7]) and InstructGPT ([b20]). This is evident from phrases like:\n\n\"Inspired by the success of applying language models in text generation, we propose a new data augmentation method named ChatAug, which leverages ChatGPT to generate auxiliary samples for few-shot text classification.\"\n\n\"ChatGPT is based on GPT-3 [b7], which was trained on massive web data with diverse and rich information. Furthermore, ChatGPT was trained through Reinforcement learning from Human Feedback (RLHF).\"\n\nThe paper also builds upon existing text data augmentation methods ([b13], [b17]) but aims to improve their accuracy and diversity limitations by using ChatGPT. For example:\n\n\"Existing studies indicate that pre-trained language models can help augment a dataset with new samples with similar semantic meaning [b13], [b17], which is of significant practical value to real-world applications.\"\n\nFinally, the large parameter space of LLMs like GPT-3 ([b18]) is mentioned as enabling them to store knowledge for effective language generation that the proposed ChatAug method leverages.\n\nThe key novel contribution is using ChatGPT to generate higher quality augmented data to improve few-shot text classification compared to previous data augmentation methods. The most relevant references support this, focusing on GPT-3, InstructGPT, and limitations of existing data augmentation that motivate this work.", "62b52c635aee126c0f459d22": "Here are the key steps I took to identify the most relevant source papers for this paper:\n\n1. The main novel contributions of the paper seem to be:\n- Designing a novel offline RL algorithm, ILQL, for language models that is simple, stable, and effective at optimizing user-specified rewards in sequential decision making settings\n- Demonstrating ILQL's ability to flexibly adapt to many different utility functions more consistently and stably than prior approaches\n- Showing ILQL can optimize stochastic or subjective reward functions and discover optimal behavior from sub-optimal data distributions\n\n2. The paper does not use many explicit phrases like \"inspired by\" to indicate reliance on specific prior work. However, it does mention building on the IQL algorithm and extending it to the token-level POMDP setting for NLP tasks.\n\n3. Looking at the methodology, key aspects seem to be:\n- Integrating IQL with sequence models to handle partially observable language generation \n- Using a novel policy extraction method that directly perturbs the behavior policy with learned value functions\n- Adding a conservatism loss term to the Q-function to fix a calibration issue\n\n4. Comparing the paper's methods to prior work, it highlights that:\n- Prior dynamic programming based offline RL methods for language suffer from complexity and instability issues\n- Other conditional imitation or value learning methods are simple but lack the temporal compositionality of full RL\n- ILQL aims to combine the simplicity and stability of the latter with the optimization power of the former\n\n5. The most relevant references based on the above seem to be:\n- [b4]: The IQL algorithm that ILQL builds upon\n- [b11] [b12] [b10]: Prior dynamic programming based offline RL methods for language that ILQL aims to improve upon in terms of simplicity and stability\n- [b5]: Source of the conservatism loss term added to ILQL's Q-function\n\nIn summary, my analysis suggests the following source papers are most relevant, in JSON format with confidence scores:\n\n{\n  \"[b4]\": 0.9,\n  \"[b11]\": 0.8, \n  \"[b12]\": 0.8,\n  \"[b10]\": 0.7,\n  \"[b5]\": 0.7\n}\n\nThe paper clearly builds most directly on [b4], the IQL algorithm. [b11], [b12] and [b10] represent the main class of prior language-focused offline RL methods that ILQL aims to improve upon. And [b5] provides a key component, the conservatism loss term, that ILQL integrates to address a specific issue. The confidence scores reflect the centrality of each reference to the main contributions and methodology of the paper.", "6417d04190e50fcafd83de21": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria provided:\n\n{\n  \"[b19]\": 0.95,\n  \"[b21]\": 0.9,\n  \"[b64]\": 0.8,\n  \"[b7]\": 0.7,\n  \"[b26]\": 0.7,\n  \"[b29]\": 0.7,\n  \"[b39]\": 0.65,\n  \"[b58]\": 0.65\n}\n\nThe key points that led me to this assessment:\n\n- The paper heavily relies on the concepts of structural entropy and encoding tree from references [b19] and [b21]. The methodology is directly inspired by the structural entropy theory from these works. This indicates [b19] and [b21] are the most influential source papers.\n\n- The graph reconstruction approach is motivated by strengthening intra-cluster edges and weakening inter-cluster edges, citing the effectiveness demonstrated in [b64]. So [b64] is an important reference.\n\n- The introduction discusses how real-world graphs tend to be noisy, incomplete, adversarial and have heterophily, drastically weakening GNN performance. It cites [b7], [b26] and [b29] when making these points about the challenges the paper aims to address. \n\n- It also notes that minor perturbations can lead to inferior GNN prediction quality and GNNs are vulnerable to attacks, referencing [b39] and [b58]. These help establish the robustness issues the paper seeks to solve.\n\n- The other references seem more peripheral - either providing background information or touching on aspects not central to the main novel contributions of entropy-based graph structure learning using encoding trees for robustness and interpretability.\n\nSo in summary, [b19] and [b21] are the clearest source inspirations, followed by [b64] for the reconstruction approach, and then [b7], [b26], [b29], [b39], [b58] help motivate the overall problem of GNN sensitivity that the paper addresses. The confidence scores reflect the centrality of each reference to the main ideas.", "628d1ea25aee126c0f3e9734": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b1]\": 0.9,\n  \"[b23]\": 0.8,\n  \"[b13]\": 0.8,\n  \"[b9]\": 0.7,\n  \"[b5]\": 0.6,\n  \"[b14]\": 0.6,\n  \"[b7]\": 0.5\n}\n\nConfidence scores:\n[b1] TransE (0.9): The paper introduces their TransO model as an extension of the classic TransE translation distance model. TransE is described as \"the most representative translation distance model\" that the subsequent models aim to improve upon. This indicates TransE was a key inspiration.\n\n[b23] TransH (0.8): TransH is mentioned as addressing shortcomings of TransE by allowing entities to have different representations under different relations. The proposed TransO model builds upon this idea.\n\n[b13] TransR (0.8): TransR is described as solving TransE's problem of weak representation in the same space by modeling entities and relations in separate spaces. TransO also uses this concept of relation-specific projections.\n\n[b9] TransD (0.7): TransD is noted as simplifying TransR by decomposing the projection matrix, which is similar to how TransO defines separate projection matrices for entities and relations.\n\n[b5] SSE (0.6): SSE is an ontology model that captures semantic correlations between entities. While not a direct inspiration, the TransO model aims to incorporate ontology information to enhance representation learning like SSE and other ontology models.\n\n[b14] TransC (0.6), [b7] JOIE (0.5): These ontology models are mentioned as considering type and hierarchical information in KGs. The TransO model is motivated to seamlessly integrate such ontology information to improve knowledge representation learning.\n\nIn summary, the translation distance models, especially TransE, TransH, TransR and TransD, appear to be the most significant inspirations for the novel TransO model proposed in this paper. The ontology models provide additional motivation to incorporate richer semantic information to enhance the embedding process.", "63dcdb422c26941cf00b6413": "Based on analyzing the text, here are the key source papers that likely inspired this work, along with confidence scores:\n\n{\n  \"[b40]\": 0.9,\n  \"[b0]\": 0.8,\n  \"[b27]\": 0.7,\n  \"[b19]\": 0.7,\n  \"[b42]\": 0.6\n}\n\nThe key novel contributions of this paper seem to be:\n\n1. Introducing a task-model bank that stores performance of diverse GNN architectures and tasks to guide the search algorithm for a new task\n2. Defining a task embedding space where tasks close in the embedding have similar top-performing architectures \n3. Embedding a task using the condition number of its Fisher Information Matrix of randomly initialized models\n4. Estimating the design prior of desirable models for a new task by aggregating design distributions from close tasks\n\nThe paper cites and builds upon ideas from:\n\n- GraphGym [b40] which attempts to transfer best architecture designs with a metric space measuring task similarity. However, GraphGym computes similarities expensively by fully training models. This paper improves on that.\n\n- Task2Vec [b0] which generates task embeddings using the Fisher Information Matrix of a shared probe network. This paper extends the idea to GNNs.\n\n- [b27] and [b19] provide theoretical justification for using asymptotic statistics of the Fisher Information Matrix to represent trainability of neural networks. This is leveraged to construct the task embeddings.\n\n- Taskonomy [b42] also estimates task similarities but using final model performance. In contrast, this paper uses light-weight task embeddings.\n\nSo in summary, [b40], [b0], [b27] and [b19] provide the key conceptual and methodological foundations that this paper directly builds upon and improves, while comparing to alternate approaches like [b42]. The Fisher Information Matrix task embedding is the core novel contribution.", "6310233d90e50fcafdc2f5b3": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b21]\": 0.9,\n  \"[b56]\": 0.85,\n  \"[b62]\": 0.8,\n  \"[b4]\": 0.7,\n  \"[b33]\": 0.7,\n  \"[b27]\": 0.7,\n  \"[b13]\": 0.7\n}\n\nConfidence scores:\n- [b21] (0.9): The paper directly states \"Inspired by its recent success of open-domain question answering [b21]\" when introducing the bi-encoder architecture, indicating this reference was a key motivation.\n- [b56] (0.85): Also directly mentioned as inspiration - \"Inspired by its recent success of ... entity linking [b56]\". \n- [b62] (0.8): Also cited alongside [b56] as inspiring the bi-encoder approach for entity linking.\n- [b4], [b33], [b27], [b13] (0.7 each): These references are mentioned when discussing existing work that \"mostly formulates NER as a classification problem\". The proposed contrastive learning approach is portrayed as differing from the classification paradigm in these papers. While not stated to be direct inspirations, these references help motivate the novel aspects of this paper's methodology.\n\nThe other references, while relevant to providing context, do not seem to be called out as directly inspiring the core novel contributions as much as the ones above. Limiting to the top 7 most central references based on the criteria.", "6389d6fe90e50fcafdffc634": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b1]\": 0.9,\n  \"[b8]\": 0.8,\n  \"[b6]\": 0.7,\n  \"[b11]\": 0.6,\n  \"[b12]\": 0.5\n}\n\nRationale and confidence scores:\n\n[b1] (Bingo, confidence 0.9): The paper mentions that PMP improves upon Bingo by reducing storage overhead while maintaining performance. It also analyzes data redundancy issues in Bingo. This indicates Bingo was a key inspiration and baseline.\n\n[b8] (SMS, confidence 0.8): The paper builds its pattern capturing framework based on SMS. It analyzes the bit vector pattern form first leveraged by SMS. SMS seems to be the foundation this work improves upon.\n\n[b6] (Pythia, confidence 0.7): The results show PMP outperforms Pythia with much less storage, implying Pythia was an important point of comparison and motivation to develop a more efficient prefetcher.\n\n[b11] (BuMP, confidence 0.6): BuMP is mentioned as improving upon SMS, the baseline this paper uses. Though not the main inspiration, BuMP represents related prior art.\n\n[b12] (DSPatch, confidence 0.5): DSPatch is another prefetcher based on the bit vector form from SMS that this paper analyzes. It provides relevant context but is not a central motivation.\n\nIn summary, the Bingo and SMS prefetchers emerge as the most influential prior works that directly inspired the key contributions of this paper - using pattern merging to reduce the storage overhead of Bingo while leveraging the bit vector form of SMS. Pythia was an important benchmark to outperform. BuMP and DSPatch provided additional context on related prefetchers.", "63881b9290e50fcafd3db3f8": "Here is my analysis of the key source papers for the given paper, based on the criteria provided:\n\nThe paper introduces a novel model compiler called PEX that enables partial execution of neural networks to significantly reduce peak memory usage on microcontroller platforms. The key novel contributions are:\n\n1. Devising abstractions and execution rules that enable partial execution for arbitrary neural network architectures \n2. Automatically creating partial execution schedules that minimize peak memory usage\n3. Integrating partial execution with structured pruning to further reduce the memory footprint in a co-design approach\n\nBased on this, the most relevant source papers that directly inspired or motivated these contributions are:\n\n{\n  \"[b25]\": 0.9,\n  \"[b7]\": 0.7,\n  \"[b17]\": 0.6,\n  \"[b5]\": 0.5\n}\n\n[b25] is cited as the original source that described partial execution for the specific case of inverted residual blocks in MobileNet-v2. This paper directly inspired generalizing partial execution to arbitrary networks.\n\n[b7] and [b17] are cited as prior work on reducing memory usage of neural networks on microcontrollers, but using model-agnostic methods like buffer bin-packing and operator reordering. PEX aims to achieve further memory savings by moving beyond these techniques.\n\n[b5] is mentioned as a general model compiler that can perform graph and code optimizations for microcontrollers. PEX is positioned as a more specialized compiler that unlocks additional memory efficiency.\n\nThe paper also cites [b17] when discussing using structured pruning to change the network architecture to reduce peak memory under partial execution. This combination of pruning and partial execution appears to be a key novel contribution.\n\nIn summary, while the paper cites and discusses a broad range of related work, the key source papers that most directly inspired the core PEX methodology appear to be [b25], [b7], [b17] and [b5]. Integrating structured pruning with partial execution also seems to be a significant novel extension to ideas from [b17].", "640fe64790e50fcafd9e2811": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b2]\": 0.9,\n  \"[b11]\": 0.8,\n  \"[b0]\": 0.9,\n  \"[b19]\": 0.9,\n  \"[b20]\": 0.8,\n  \"[b28]\": 0.7\n}\n\nThe key novel contributions of this paper seem to be:\n\n1. Leveraging contrastive learning to learn discriminative image representations in an unsupervised manner and constructing a Gaussian mixture model (GMM) over its representations. \n\n2. Formulating label noise detection as an out-of-distribution (OOD) problem, utilizing a two-component GMM to model the samples with clean and wrong labels.\n\n3. A bootstrap cross-supervision method with an entropy regulation loss to reduce the impact of wrong labels.\n\nBased on this, the most relevant source papers that directly inspired or motivated these contributions are:\n\n- [b2] and [b11]: These papers on contrastive learning methods like SimCLR and MoCo seem to heavily inspire the paper's use of contrastive learning for unsupervised representation learning.\n\n- [b0] and [b19]: The mixture model approach to distinguishing clean vs noisy labels in these papers directly motivates this paper's two-component GMM for OOD label noise detection. \n\n- [b20] and [b28]: While this paper improves upon their nearest-neighbor based noise detection, these contrastive learning papers for learning with noisy labels still provide key inspiration.\n\nThe high confidence scores reflect that the key ideas of unsupervised contrastive representation learning, GMM-based noise vs clean modeling, and cross-model bootstrapping of labels from these papers very directly enable this paper's main technical innovations. The frequent comparisons to these works in the text further supports their centrality as intellectual sources.", "63d9d87390e50fcafd57e29e": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b6]\": 0.9,\n  \"[b27]\": 0.8,\n  \"[b30]\": 0.7,\n  \"[b62]\": 0.7,\n  \"[b34]\": 0.6\n}\n\nThe key points that led me to identify these as the most relevant source papers are:\n\n1. The paper introduces a novel method called Dependency Graph (DepGraph) for automatic structural pruning of neural networks. The key phrases \"To address this problem, we fully leverage the comprehensive ability of dependency modeling powered by DepGraph to learn consistent sparsity within groups\" and \"With the dependency modeling, we show in the experiments that a simple L1 norm criterion can achieve comparable performance to modern approaches\" indicate that DepGraph and dependency modeling are the core novel contributions.\n\n2. References [b6], [b27], [b30], [b62] and [b34] are directly mentioned in the context of discussing existing structural pruning approaches and their limitations in handling complex network architectures and dependencies between layers. \n\n3. The methodology of DepGraph is compared to the manual case-by-case analysis used in previous works like [b27] and [b36]. This shows DepGraph aims to improve upon the limitations of these prior methods.\n\n4. The experimental results are directly compared against state-of-the-art methods in [b6], [b30], [b53] and [b62], positioning DepGraph as an improvement over these recent techniques.\n\n5. Reference [b34] is cited when discussing the challenges and importance of modeling dependencies between layers for structural pruning, which is the key problem DepGraph aims to solve.\n\nSo in summary, these 5 references, especially [b6], [b27], [b30] and [b62], are portrayed as the most relevant prior works that the DepGraph method builds upon and aims to improve, in terms of automatically handling complex layer dependencies for effective structural pruning. The frequent comparisons to these papers' limitations and results confirm they are the most pertinent influences.", "6424fe3390e50fcafd78b58e": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b0]\": 0.9,\n  \"[b22]\": 0.8,\n  \"[b10]\": 0.7,\n  \"[b23]\": 0.7,\n  \"[b47]\": 0.6\n}\n\nConfidence scores:\n[b0], [b22], [b10], [b23]: These references are directly related to the paper's core novel contributions around using graphons to construct a graph generator space. The paper heavily relies on key properties of graphons from these prior works, as evidenced by phrases like:\n\"We utilize both perspectives in our framework. On one hand, a graphon can be considered as the limit objects of graph sequence, where the density of certain \"graph motifs\" can be preserved; and every convergent graph sequence would converge to a graphon [b22].\"\n\"Taking graphon as a graph generator, we can associate nodes ? and ? with points ? ? and ? ? in [0, 1], and then ?(? ? , ? ? ) serves as the probability to generate the edge between these two nodes.\"\n\n[b47]: This reference provides the distance metric (Gromov-Wasserstein distance) used to measure similarity between graphons, which is a key component of the paper's optimization formulation for pre-training feasibility:\n\"Following [b47], we hire the 2-order Gromov-Wasserstein (GW) distance as our distance function dist(?, ?), as GW distance is commonly used to measure the difference between structured data.\"\n\nThe paper directly builds upon the graphon theory and distance metrics from these prior works to enable its novel framework and optimization approach. The frequent citations and detailed descriptions of how these references are used indicate they provided significant inspiration and foundation for the paper's main contributions.", "62bab8f95aee126c0f6afca5": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b3]\": 0.9,\n  \"[b16]\": 0.8,\n  \"[b2]\": 0.7,\n  \"[b10]\": 0.6,\n  \"[b12]\": 0.5\n}\n\nConfidence scores:\n[b3]: 0.9 - The paper is heavily inspired by recent advancements in using the Wiener filter for deep image restoration, as evidenced by this quote: \"Motivated by recent advancement of wiener in deep image restoration [b3], we introduce the classical deconvolutional technique, wiener kernel, into GDN\". This reference is core to the paper's novel Wiener Graph Deconvolutional Network.\n\n[b16]: 0.8 - The proposed method builds upon and improves the Graph Deconvolutional Network (GDN) framework from [b16]. The paper states \"Another GDN framework [b16] is designed via a combination of inverse filters in spectral domain and denoising layers in wavelet domain, which is sub-optimal regarding signal reconstruction.\" Improving GDN is a key contribution.\n\n[b2]: 0.7 - The idea of injecting augmentations in the latent space is inspired by representation learning techniques in [b2], as indicated by \"Inspired by representation learning [b2], we inject simple augmentations (e.g., Gaussian noise) on latent graph representations\". \n\n[b10]: 0.6 - The paper mentions that \"Recently, latent augmentation is introduced in adversarial training [b10] for GNNs.\" While relevant, this reference seems less central to the main contributions compared to the others.\n\n[b12]: 0.5 - VGAE from [b12] is discussed as a related variational autoencoder method, but the proposed Wiener GDN is a distinct approach. The connection is weaker than the other references.\n\nIn summary, [b3] and [b16] emerge as the most influential source papers, directly motivating the Wiener filter and GDN aspects of the proposed method. [b2] also provides key inspiration for the latent augmentation technique. The other references, while relevant, appear to be less critical to the central novel contributions of this paper.", "6419208e90e50fcafda928aa": "Here is my analysis of the most pertinent source papers for this paper, based on the criteria you provided:\n\n{\n  \"[b35]\": 0.9,\n  \"[b52]\": 0.8,\n  \"[b4]\": 0.7,\n  \"[b47]\": 0.7,\n  \"[b50]\": 0.7,\n  \"[b7]\": 0.6,\n  \"[b49]\": 0.6,\n  \"[b36]\": 0.5\n}\n\nConfidence scores:\n- [b35] (0.9): This paper is directly cited multiple times when discussing approaches to quantify and mitigate over-smoothing, such as using the Dirichlet energy and Graph-Coupled Oscillator Networks (GraphCON). It appears to be a key inspiration.\n\n- [b52] (0.8): Cited when extending the over-smoothing definition to continuous-time GNNs. The paper also discusses GRAND and BLEND from [b52] as examples of this emerging sub-field.\n\n- [b4], [b47], [b50] (0.7 each): Grouped together as they are all cited as examples of using the Dirichlet energy to measure over-smoothing. [b50] in particular proposes Energetic Graph Neural Networks (EGNNs) to regularize based on the Dirichlet energy.\n\n- [b7], [b49] (0.6 each): Cited as examples of using Mean Average Distance (MAD) to quantify over-smoothing, though the paper argues MAD is problematic compared to the Dirichlet energy.\n\n- [b36] (0.5): Discusses the Gradient Gating (G2) method from this paper as an example of changing GNN dynamics to mitigate over-smoothing. Empirically finds G2 is able to maintain performance in deep GNNs.\n\nThe paper appears to build most directly on the Dirichlet energy-based over-smoothing analysis in [b35] and related works, while aiming to provide a unifying definition. The extension to continuous-time GNNs also draws inspiration from [b52]. Methods like G2 from [b36] are analyzed as promising ways to mitigate over-smoothing while preserving expressivity.", "63dc7b5290e50fcafdc836c3": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b13]\": 0.9,\n  \"[b9]\": 0.8,\n  \"[b1]\": 0.7,\n  \"[b5]\": 0.7,\n  \"[b27]\": 0.6\n}\n\nRationale:\n- The paper cites [b13] when introducing graph neural networks and the message passing framework, indicating this is a foundational paper that the current work builds upon. The phrase \"Graph neural networks (GNNs) are powerful frameworks that generate node embeddings for graphs via the iterative message passing (MP) scheme [b13]\" suggests [b13] was a key inspiration.\n\n- [b9] proposes the GNNAutoScale (GAS) method which is discussed extensively as a subgraph-wise sampling approach that uses historical embeddings. The current paper aims to improve upon the limitations of methods like GAS.\n\n- [b1] demonstrates how inaccurate mini-batch gradients hurt GNN convergence, which is a key challenge this paper tries to address. \n\n- Similarly, [b5] (MVS-GNN) is another recent subgraph-wise sampling method that uses historical embeddings and is relevant to the current work.\n\n- [b27] (GraphFM-IB/OB) is referenced as a recent work that tries to alleviate the staleness problem in using historical embeddings, which has similarities to this paper's approach.\n\nThe other referenced papers, while providing relevant background, do not seem to be as directly inspirational to the novel contributions of this paper based on the language used. The key aspects are compensating for discarded messages to improve mini-batch gradient accuracy and convergence speed in subgraph-wise sampling.", "64363413cf18aa11d8b55a06": "Based on the analysis of the paper, here are the most pertinent source papers that likely inspired or motivated this work, along with confidence scores:\n\n{\n  \"[b17]\": 0.95,\n  \"[b7]\": 0.8,\n  \"[b4]\": 0.7,\n  \"[b55]\": 0.7,\n  \"[b9]\": 0.6,\n  \"[b51]\": 0.6,\n  \"[b1]\": 0.5\n}\n\nRationale:\n\n- [b17] (GraphMAE) is referenced multiple times and seems to be the main inspiration for this work. The paper directly builds upon and improves GraphMAE. (confidence 0.95)\n\n- [b7] is cited when discussing how the discriminability of node features can significantly affect GNN performance. This motivates the latent representation prediction in GraphMAE2. (confidence 0.8) \n\n- [b4] and [b55] are mentioned in relation to how neural networks can serve as denoising encoders and encode high-level semantics. This inspires using a network as the target generator. (confidence 0.7)\n\n- [b9] is referenced when introducing the multi-view random re-mask decoding strategy, which resembles random propagation. (confidence 0.6)\n\n- [b51] motivates decoupling the receptive field and depth of GNNs by extracting local subgraphs, which inspires the local clustering approach for large graphs. (confidence 0.6)\n\n- [b1] introduces the PPR-Nibble algorithm used for efficiently finding local clusters in the proposed extension to large-scale graphs. (confidence 0.5)\n\nThe other referenced papers provide relevant background and techniques but do not seem to be as directly influential on the core novel contributions of GraphMAE2 based on the language used to discuss them in the text. The focus was on identifying the key source papers that inspired the main ideas.", "642525e790e50fcafdfdd202": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b70]\": 0.9,\n  \"[b25]\": 0.8,\n  \"[b71]\": 0.7,\n  \"[b66]\": 0.6,\n  \"[b24]\": 0.5\n}\n\nThe key reasons are:\n\n1. The paper is heavily inspired and motivated by the automatic schedule template generation approach in Ansor [[b70]], as evidenced by phrases like \"Automatic approaches. These approaches automatically generate schedule templates [b70] [b71] [b72] to overcome the aforementioned drawbacks of the manual approaches.\" and comparisons to Ansor throughout.\n\n2. AutoTVM [[b25]] is also a major inspiration, being referenced multiple times as a state-of-the-art exploration-based approach that the proposed Heron system improves upon. The genetic algorithm used in AutoTVM is discussed in detail.\n\n3. AMOS [[b71]] is grouped with Ansor as another automatic schedule template generation approach that motivates this work. \n\n4. The manual schedule template approaches in [[b66]] and [[b24]] provide important context and motivation for developing automatic approaches like the one proposed in this paper.\n\n5. While other papers like [[b16]], [[b49]], [[b72]] are also referenced, they do not seem to be as centrally tied to the main novel contributions and techniques of automatically generating constraints and using a constraint-based genetic algorithm for irregular DLA architectures.\n\nSo in summary, [[b70]], [[b25]], [[b71]], [[b66]], and [[b24]] emerge as the most influential prior works that inspired the key ideas and methods of this paper, with Ansor [[b70]] and AutoTVM [[b25]] being especially prominent. The novel automatic constraint generation and constrained genetic algorithm directly build upon and aim to improve these state-of-the-art approaches.", "63aaa48a90e50fcafd27b0fa": "Based on the analysis of the paper, here are the most relevant source papers that likely inspired or motivated this work, along with confidence scores:\n\n{\n  \"[b8]\": 0.9,\n  \"[b9]\": 0.85,\n  \"[b15]\": 0.8,\n  \"[b5]\": 0.7,\n  \"[b14]\": 0.65\n}\n\nThe key reasons for selecting these papers are:\n\n1. The paper frequently mentions that previous automatic mapping frameworks like [b8], [b9] can alleviate the mapping problem to some extent, but still have limitations. This indicates [b8] and [b9] were key motivations for developing the AutoMap framework to overcome those limitations.\n\n2. The proposed groupwise mapping method aims to improve upon the layerwise mapping in [b5], [b14] which caused excessive data movement. Addressing this inefficiency seems to be a main goal.\n\n3. The dynamic memory allocator (DMA) proposed is contrasted with the manual static memory allocation in [b15], implying [b15] inspired them to develop a better memory allocation strategy. \n\n4. Overall, optimizing the mapping strategy and memory allocation compared to prior accelerator frameworks appears to be the main novel contribution, and the above references are most directly tied to those aspects based on how they are discussed in the text. The phrasing around these references also suggests they were influential in motivating this work.", "6427029c90e50fcafd5d6bd8": "Based on analyzing the paper, here are the key source papers that likely inspired or motivated this work, along with confidence scores:\n\n{\n  \"[38]\": 0.9,\n  \"[153]\": 0.8,\n  \"[93]\": 0.7,\n  \"[83]\": 0.7,\n  \"[115]\": 0.6,\n  \"[140]\": 0.5\n}\n\nThe paper introduces RAMBDA, a novel system architecture that uses a cache-coherent accelerator along with an RDMA NIC to efficiently serve memory-intensive microsecond-scale datacenter applications. \n\nKey novel contributions include:\n1) A unified abstraction for inter-and intra-machine communication using lockless ring buffers, inspired by [38] and [153]. \n2) A coherence-assisted accelerator notification mechanism.\n3) A modular cc-accelerator architecture that interacts with the RNIC and CPU to process requests.\n4) An adaptive device-to-host data transfer optimization for heterogeneous memory systems.\n\nThe work is heavily motivated by the limitations of current Smart NIC based approaches, as discussed in [93], [83], [115] and [140]. These papers highlight how the limited memory capacity of Smart NICs leads to frequent accesses to host memory over slow PCIe links, degrading performance.\n\nSo in summary, while building on communication abstractions from [38] and [153], RAMBDA architects a new end-to-end system to overcome the challenges with Smart NICs identified in [93], [83], [115] and [140]. The modular cc-accelerator based approach allows efficiently accelerating a broader range of memory-intensive datacenter applications.", "63a910a390e50fcafd2a8a6e": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b26]\": 0.9,\n  \"[b17]\": 0.8,\n  \"[b18]\": 0.7\n}\n\nConfidence scores:\n[b26]: 0.9\nThe paper takes direct inspiration from differentiable architecture search [b26] to develop its personalized subgraph selector. It extends the idea from a transductive search model to an inductive subgraph selector. This reference is key to the novel methodology proposed.\n\n[b17], [b18]: 0.8, 0.7 \nThe paper compares its approach to the subgraph2link methods like SEAL [b17] and [b18]. These serve as important baselines that the proposed personalized subgraph selection aims to improve upon. The methodological similarities and differences with these prior works help motivate the novel aspects of this paper.\n\nI focused on identifying the references that seem to most directly inspire the key novel contributions of automatically selecting personalized subgraphs for link prediction in an inductive manner. The differentiable search approach from [b26] and the subgraph2link methods in [b17], [b18] emerge as the most relevant prior works that this paper builds upon and compares to.", "634f6ae490e50fcafdcb6525": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b24]\": 0.95,\n  \"[b8]\": 0.9,\n  \"[b9]\": 0.85,\n  \"[b46]\": 0.85,\n  \"[b33]\": 0.8,\n  \"[b11]\": 0.75\n}\n\nThe key points that led me to this conclusion:\n\n1. The paper introduces a new DNN compiler called Hidet based on a task-mapping-oriented programming paradigm. This is contrasted with the loop-oriented scheduling used by state-of-the-art DNN compilers like TVM [b8] and schedulers like AutoTVM [b9] and Ansor [b46]. These references are directly compared to the novel Hidet compiler, indicating they were key motivations and baselines.\n\n2. The paper discusses how template-based libraries like CUTLASS [b24] can generate tensor programs and achieve good performance, but have limited operator and fusion support compared to compilers. CUTLASS seems to be a major inspiration for the task-mapping paradigm in Hidet.\n\n3. The Halide paper [b33] is credited with originally proposing the idea of decoupling computation definition from scheduling, which existing DNN compilers adopt. Improving upon the limitations of this approach in Halide and its descendants is a key motivation.\n\n4. The efficient matrix multiplication implementation relies heavily on utilizing NVIDIA Tensor Cores [b11], so this reference helps provide important context and background for the paper's techniques.\n\n5. Overall, the references [b24], [b8], [b9], [b46] and [b33] are repeatedly mentioned when discussing the limitations of existing approaches that Hidet aims to overcome, while [b11] provides key background. The strong comparative language used, like \"Unlike kernel libraries and templates that target at a fixed set of operators and limited fusion patterns, compilers are capable of supporting more operators and more flexible fusion patterns automatically\", confirms these are the most influential references.", "64264f7b90e50fcafd68e145": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b36]\": 0.9,\n  \"[b6]\": 0.85,\n  \"[b20]\": 0.8,\n  \"[b10]\": 0.75,\n  \"[b7]\": 0.7,\n  \"[b17]\": 0.65,\n  \"[b5]\": 0.6\n}\n\nRationale and key points:\n\n- The paper heavily relies on the transformer architecture ([b36]) as the backbone of the CodeGeeX model. The use of phrases like \"Similar to recent pre-trained models, such as GPT-3 [b5] , PaLM [b7] , and Codex [b6] , CodeGeeX follows the generative pre-training (GPT) architecture [b25] with the decoder-only style for autoregressive (programming) language modeling\" indicates the transformer paper [b36] is a key inspiration.\n\n- The OpenAI Codex model [b6] is frequently referenced as pioneering the potential of large code generation models and demonstrating code generation capabilities in Python. CodeGeeX directly compares itself to Codex.\n\n- Other large code generation models like Salesforce CodeGen [b20], Meta InCoder [b10], DeepMind AlphaCode [b17] and Google PaLM [b7] are mentioned as related work that CodeGeeX builds upon and compares itself against, indicating they provide key motivation.\n\n- GPT-3 [b5] is cited multiple times as an influential model that CodeGeeX follows in terms of architecture and training approach.\n\n- The novel contributions of CodeGeeX include its multilingual capabilities, diverse downstream tasks like code explanation and translation, open-sourcing the model, and the new HumanEval-X benchmark. The key references support these by providing the foundation CodeGeeX builds off of, while CodeGeeX introduces new elements on top of them.\n\nSo in summary, the transformer architecture and previous large code models, especially Codex, provide the core inspiration and basis for CodeGeeX, while CodeGeeX makes novel contributions in the multilingual space and additional capabilities. The selected references capture this balance between the foundational work CodeGeeX relies on and the new elements it introduces.", "634d809c90e50fcafd4e77f0": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b16]\": 0.9,\n  \"[b13]\": 0.8,\n  \"[b15]\": 0.7,\n  \"[b14]\": 0.6\n}\n\nConfidence scores:\n[b16]: 0.9 - The paper makes multiple references to [b16], indicating it was a key source of inspiration. The prompt tuning paradigm employed is directly attributed to the successes shown in [b16]. This reference is prioritized due to the \"Inspired by\" phrasing.\n\n[b13]: 0.8 - [b13] is referenced multiple times when discussing limitations of prior taxonomy expansion methods that only model local substructures. The global random walk approach is presented as an improvement over the local structures used in [b13]. \n\n[b15]: 0.7 - The mini-paths approach from [b15] is discussed as another local substructure method that the proposed global random walks improve upon. The paper also mentions that [b15] showed lexical-syntactic features are important.\n\n[b14]: 0.6 - [b14] is referenced a few times, noting the ego-trees used as local structures and also that [b14] demonstrated LMs learn to summarize attributes from concept descriptions. However, the reliance on and inspiration from [b14] seems less central than the other references.\n\nThe other references like [b1], [b3], [b6] provide relevant background information but do not seem to be as directly influential on the main novel contributions of random walks for global structure and prompt-based generation. Limiting to the top 4 most pertinent references based on the criteria.", "63dcdb422c26941cf00b642d": "Here is my attempt at identifying the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"[b34]\": 0.9,\n  \"[b22]\": 0.9,\n  \"[b5]\": 0.8,\n  \"[b27]\": 0.7,\n  \"[b42]\": 0.6\n}\n\nI arrived at this by following these steps:\n\n1. The paper's main novel contributions seem to be:\n- Presenting EVAS, a new backdoor attack that leverages NAS to find neural architectures with inherent, exploitable vulnerability \n- Defining a novel metric based on neural tangent kernel to efficiently search for exploitable architectures without requiring model training\n- Demonstrating EVAS's effectiveness, evasiveness, transferability and robustness compared to conventional backdoor attacks\n\n2. Key phrases indicating inspiration from previous work:\n- \"Motivated by recent work [b27] [b42] Abdelfattah et al., 2021; [b30] on NAS using easy-to-compute metrics as proxies (without training), we present a novel method of searching for exploitable arches based on neural tangent kernel (NTK) [b17] without training the target model or trigger generator.\"\n- This directly cites [b27] and [b42] as motivation for the NAS search method without model training.\n\n3. The core novel methods leverage neural tangent kernel theory, citing [b5] heavily:\n- \"Recent work [b5] [b28] shows that NTK serves as an effective predictor of model \"trainability\" (i.e., how fast the model converges at early training stages).\"\n- \"Leveraging the insights above, we present a simple yet effective algorithm that searches for exploitable arches without training, which is a variant of regularized evolution [b36] [b27].\"\n\n4. Other frequently cited papers like [b22] and [b34] seem very relevant based on the comparisons made in the evaluation section to prior NAS methods.\n\n5. The introduction cites some papers like [b15] and [b18] when describing conventional backdoor attacks, but these seem more peripheral to the core novel contributions.\n\nIn summary, [b5], [b27] and [b42] appear to be the most central inspirations for the novel NAS search method leveraging NTK, while [b22] and [b34] are important points of comparison. The confidence scores reflect my estimate of how directly each paper influenced the key ideas. Let me know if you would like me to clarify or expand the reasoning further.", "64250fee90e50fcafdb2d890": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria provided:\n\nThe key novel contributions of this paper appear to be:\n\n1. Implementing coverage metrics as compiler passes that generate a single \"cover\" primitive, which can be easily supported by a wide range of simulators. This allows merging coverage results across different simulators.\n\n2. Providing an approach to map coverage results from the generated Verilog back to the original high-level hardware construction language (Chisel in this case).\n\n3. Demonstrating the implementation of this approach for several coverage metrics (line, toggle, FSM) and simulators (Treadle, Verilator, ESSENT, FireSim).\n\nThe paper mentions being inspired by or building upon the following key references:\n\n- [b4], [b11]: These introduce the Chisel language and FIRRTL compiler infrastructure used as the basis for this work. The paper's approach relies on FIRRTL compiler passes.\n\n- [b17]: The Treadle FIRRTL simulator is one of the key targets supported by the paper's approach. Existing code in Treadle was adapted to implement the \"cover\" primitive.\n\n- [b24]: Verilator, a popular open-source Verilog simulator, is another key target. The paper maps its \"cover\" primitives to Verilator's user-defined coverage features.\n\nBased on this analysis, I would rank the most pertinent source papers as follows, with confidence scores:\n\n{\n  \"b4\": 0.9,\n  \"b11\": 0.9, \n  \"b17\": 0.8,\n  \"b24\": 0.8\n}\n\nThe papers [b4] and [b11] are fundamental to the overall approach, while [b17] and [b24] directly enable key results. Other references provide useful context but are less central to the main contributions in my assessment.", "643e0acf0746dc40e3418ed8": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b21]\": 0.9,\n  \"[b48]\": 0.8,\n  \"[b2]\": 0.7,\n  \"[b49]\": 0.6,\n  \"[b9]\": 0.5,\n  \"[b15]\": 0.5,\n  \"[b16]\": 0.5\n}\n\nRationale:\n\nThe paper introduces a novel Multi-view Graph Encoder (MVGE) framework to learn graph representations that capture both low-frequency and high-frequency signals, in order to handle graphs with a mixture of homophily and heterophily patterns. \n\nThe key novel contributions seem to be:\n1) Analyzing the limitations of existing GRL methods that assume homophily \n2) Proposing to learn from multiple views (ego features and aggregated neighbor features) to capture high and low frequency signals\n3) Designing a mechanism to integrate the embeddings from the different views into a coherent space\n\nThe source papers most directly related to these contributions are:\n\n[b21] (confidence 0.9): The authors' own previous work PairE, which tried to encode both low and high frequency signals using node pairs. This directly inspired MVGE's goal of capturing different signal types.\n\n[b48] (confidence 0.8): H2GCN, which pointed out the limitations of GNNs on heterophilic graphs and proposed combining multi-scale information. This motivated MVGE's analysis of homophily vs heterophily.\n\n[b2] (confidence 0.7): FAGCN, a GNN for heterophily that relaxed the assumption of positive edge weights. Provided insights on the behavior of GNNs on heterophilic graphs.\n\n[b49] (confidence 0.6): GNN-LF/HF, which designed high and low pass filters for graphs. Influenced MVGE's notion of low and high frequency signals.\n\n[b9], [b15], [b16] (confidence 0.5 each): MVGRL, MERIT and other multi-view representation learning approaches provided the inspiration for MVGE's overall multi-view framework, though in the contrastive learning context.\n\nThe other referenced papers provide relevant background, but the core novel contributions of MVGE seem most directly influenced by the papers highlighted above, especially [b21] and [b48]. The key phrases indicating direct inspiration are:\n\n\"Motivated by the above discussed limitations with one pretext task, this paper proposes a novel Multi-view Graph Encoder(MVGE) framework...\"\n\n\"Our previous work, PairE [b21] tries to encode both low-frequency and high-frequency signals among connected nodes by employing node pairs as basic embedding units.\"", "634cc7a390e50fcafd162fef": "Here are the most pertinent source papers for the given paper, based on the criteria provided:\n\n{\n  \"[b44]\": 0.9,\n  \"[b18]\": 0.8,\n  \"[b40]\": 0.7,\n  \"[b7]\": 0.6,\n  \"[b27]\": 0.5\n}\n\nReasoning:\n\n1. The paper frequently cites [b44], [b27], [b7] when discussing existing methods that try to address the heterophily problem in GNNs. This suggests these papers were key motivations and inspirations for the current work.\n\n2. [b18] which introduces Graph Convolutional Networks (GCNs) is referenced multiple times, and the proposed ACM framework is built on top of GCN-like architectures. So [b18] seems to be a foundational paper that inspired the methodological approach.\n\n3. Simple Graph Convolution (SGC) [b40] is used to derive key theoretical insights about the relationship between post-aggregation node similarity and GNN performance. The frequent analysis of SGC indicates it played an important role in developing the paper's novel contributions.\n\n4. The other referenced papers, while providing relevant background knowledge, do not seem to be as directly influential on the core novel aspects of aggregation similarity and the ACM framework, based on the text.\n\n5. Overall, [b44], [b18], [b40], [b7] and [b27] emerge as the most central and inspiring previous works that the current paper builds upon in order to make its key contributions in analyzing and addressing heterophily in GNNs.", "643e0ad10746dc40e3419478": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b36]\": 0.9,\n  \"[b34]\": 0.8,\n  \"[b8]\": 0.7\n}\n\nConfidence scores:\n[b36]: 0.9 - This reference is directly mentioned as proposing the UIE framework which the current paper builds upon and improves. The paper states \"Recently, [b36] proposed UIE, which uniformly encodes different extraction structures via a structured extraction language, and captures the common IE abilities via a large-scale pretrained text-to-structure model\". This indicates [b36] is a key inspiration and foundation for the novel contributions of InstructUIE.\n\n[b34]: 0.8 - The paper discusses the limitations of the USM framework proposed in [b34], stating \"However, USM presents two major limitations.\" This suggests [b34] is highly relevant as a point of comparison and motivation for the new InstructUIE approach.\n\n[b8]: 0.7 - The BERT model from [b8] is used as a key baseline that InstructUIE is compared against in the experiments. While not necessarily an inspiration, including [b8] helps contextualize the empirical improvements of the proposed method.\n\nThe other references like [b1], [b40], [b39] provide useful background on large language models in general, but do not seem to be as directly tied to the specific novel contributions around instruction tuning and the InstructUIE framework. The experimental dataset references are also less central to the core ideas.\n\nIn summary, [b36], [b34] and to a lesser extent [b8] appear to be the most influential prior works that inspired and motivated the key technical innovations introduced in this paper. The structured analysis of their limitations and the direct empirical comparisons suggest these are the most pertinent source papers to highlight.", "629041ac5aee126c0fb5da8a": null, "63e312ef90e50fcafdc191fd": "Here is my attempt to identify the most pertinent source papers based on the given text, presented in JSON format with reference numbers as keys and confidence scores between 0 and 1 as values:\n\n{\n  \"[b32]\": 0.95,\n  \"[b43]\": 0.9,\n  \"[b23]\": 0.85,\n  \"[b58]\": 0.8,\n  \"[b44]\": 0.7,\n  \"[b46]\": 0.7,\n  \"[b59]\": 0.65\n}\n\nThe key steps in my analysis were:\n\n1. Identify the novel contributions of the paper, which are: \n- Constructing a large-scale scientific literature tagging benchmark Maple covering 19 fields\n- Comprehensively evaluating the performance of different types of multi-label classifiers after incorporating metadata features\n- Empirical findings on metadata effects across fields\n\n2. Look for methodological similarities and comparative phrases related to the key contributions. The paper uses Parabel [b32], Transformer [b43], and OAG-BERT [b23] as representative models from different categories, likely building upon or comparing to these works.\n\n3. Prioritize references that are indicated to provide direct inspiration or motivation, such as:\n- Previous metadata-aware taggers [b44] [b46] [b58] having limitations that this work addresses \n- Pre-trained language model classifiers like OAG-BERT [b23] being adopted\n- Ranking labels using lexical matching heuristic motivated by [b59]\n\n4. Verify the selected references are tightly linked to the paper's novel methods and exclude less relevant ones.\n\n5. Assign confidence scores based on the strength of connection and importance to the main contributions.\n\nThe scores reflect my confidence that these references provided significant inspiration and foundation for the key aspects of the paper. [b32], [b43] and [b23] are most central as the work directly builds upon those models. The others also clearly motivated and guided the paper's approach and analysis.", "6344dede90e50fcafd24d1af": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"b23\": 0.9,\n  \"b14\": 0.8,\n  \"b16\": 0.7,\n  \"b10\": 0.6\n}\n\nExplanation:\nThe paper draws heavy inspiration from the Sentence BERT (S-BERT) architecture proposed in [b23] for semantic textual similarity. Specifically, the authors use an S-BERT pre-trained bi-encoder as their base language model and adopt techniques like mean pooling of token representations. This reference is directly tied to one of the paper's main novel contributions of developing neural methods for text de-duplication.\n\nThe paper also relies significantly on [b14] (Facebook AI Similarity Search or FAISS) to efficiently compute nearby document pairs to evaluate with cross-attention in their re-ranking approach. FAISS enables them to avoid the quadratic cost of applying a cross-encoder to the full corpus.\n\nAdditionally, the re-ranking approach itself takes inspiration from the literature on open-domain retrieval and question answering, with [b16] specifically mentioned as using FAISS to select nearby passages to a query for cross-encoder evaluation. This ties into the paper's key contribution of developing scalable neural methods for de-duplication.\n\nFinally, the paper uses the online contrastive loss from [b10] in training their bi-encoder model, which is one of the core novel techniques introduced.\n\nThe other references, while providing useful background and context, do not appear to be as directly linked to the specific novel contributions and core methods of this paper based on the phrasing in the text. The key aspects are the neural architectures for de-duplication (bi-encoder and re-ranking) and the techniques used to make them scalable and effective (S-BERT, FAISS, contrastive loss).", "633ba44790e50fcafdfe4b50": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria provided:\n\nThe key novel contributions of this paper seem to be:\n1. Revealing that MLPs and GNNs share the same weight space, enabling weight transfer between them\n2. Proposing MLPInit - initializing GNN weights with those of a converged \"PeerMLP\" to significantly accelerate GNN training \n\nLooking at key phrases that indicate inspiration from or building upon previous work:\n- \"Motivated by this, we propose an embarrassingly simple, yet remarkably effective method to accelerate GNNs training by initializing GNN with the weights of its converged PeerMLP.\"\n- \"Recently, several approaches for more efficient GNNs training have been proposed, including novel architecture design [b51] [b60] [b29] , data reuse and partitioning paradigms [b48] [b12] [b62] and graph sparsification [b4] [b25] . However, these kinds of methods often sacrifice prediction accuracy and increase modeling complexity, while meriting sometimes significant additional engineering efforts.\"\n\nThis suggests the paper is building upon recent work on efficient GNN training, but taking a novel approach with PeerMLPs and weight transfer.\n\nThe key source papers seem to be:\n- [b51], [b60], [b29]: Novel GNN architecture designs for efficiency that the current work is compared against \n- [b48], [b12], [b62]: Data reuse and partitioning methods for GNN efficiency\n- [b4], [b25]: Graph sparsification methods for GNN efficiency\n\nThe paper also mentions:\n\"MLPs are used to accelerate GNNs [b65] [b14] [b20] by decoupling GNNs to node features learning and graph structure learning. Our work also leverages MLPs, but adopts a distinct perspective.\"\n\nSo [b65], [b14], [b20] are also relevant works using MLPs to accelerate GNNs, but the current paper takes a different approach.\n\nIn summary, here are the most pertinent source papers, with confidence scores:\n\n{\n  \"[b51]\": 0.9,\n  \"[b60]\": 0.9, \n  \"[b29]\": 0.9,\n  \"[b48]\": 0.8,\n  \"[b12]\": 0.8,\n  \"[b62]\": 0.8,\n  \"[b4]\": 0.7,\n  \"[b25]\": 0.7,\n  \"[b65]\": 0.6,\n  \"[b14]\": 0.6,\n  \"[b20]\": 0.6\n}\n\nThe papers [b51], [b60], [b29] get the highest scores as they are directly cited as alternative efficient GNN approaches that the current work improves upon. The data partitioning and graph sparsification papers are also quite relevant. Finally, the other MLP-based acceleration methods are included but with lower confidence scores, as the current paper adopts a distinct approach from them.", "628d9e805aee126c0f979841": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"[b0]\": 0.9,\n  \"[b21]\": 0.8,\n  \"[b6]\": 0.7,\n  \"[b2]\": 0.6,\n  \"[b35]\": 0.5,\n  \"[b5]\": 0.5\n}\n\nRationale:\n\nThe paper introduces RetroMAE, a novel masked auto-encoding framework for pre-training retrieval-oriented language models. The key novel contributions appear to be:\n\n1. A masked auto-encoding process where the input is polluted with two different masks - one for the encoder to generate the sentence embedding, and one for the decoder to reconstruct the original sentence. \n\n2. An asymmetric model structure with a large transformer encoder and an extremely simplified decoder.\n\n3. Asymmetric masking ratios, with a moderate 15% masking for the encoder input and an aggressive 50-90% masking for the decoder input.\n\n4. Enhanced decoding using two-stream attention and position-specific attention masking to derive training signals from all input tokens.\n\nThe references that seem most directly related to these novel aspects, based on the phrasing in the text, are:\n\n[b0] and [b21] - Mentioned in the context of existing auto-encoding based methods for retrieval-oriented pre-training, which RetroMAE aims to simplify and improve upon. The asymmetric structure and enhanced decoding likely take inspiration from these works.\n\n[b6] - Condenser, an auto-encoding method that RetroMAE is directly compared to. The masked auto-encoding process is positioned as an improvement.\n\n[b2] - DiffCSE, another auto-encoding approach that combines elements of contrastive learning that RetroMAE is compared against.  \n\n[b35] - The enhanced decoding uses two-stream attention inspired by this reference.\n\n[b5] - The enhanced decoding also leverages position-specific attention masking from this work.\n\nThe phrasing around these references, the comparisons drawn, and the methodological similarities described indicate they likely served as direct inspiration and motivation for the key contributions of RetroMAE. The other references provide useful context but do not appear as centrally tied to the novel aspects of the proposed approach.", "634d80a390e50fcafd4e7c23": "Here is my analysis of the most pertinent source papers for the given text, based on the outlined criteria:\n\nThe paper presents OneEE, a one-stage model for overlapped and nested event extraction (EE). The key novel contributions are:\n\n1. A new tagging scheme that transforms overlapped and nested EE into word-word relation recognition, using span relations (S-*) for trigger/argument identification and role relations (R-*) for argument role classification. \n\n2. An adaptive event fusion layer with attention and gate fusion modules to obtain event-aware contextual representations.\n\n3. Parallel prediction of span and role relations between word pairs using distance-aware scores, enabling one-stage EE without error propagation.\n\nBased on these contributions and the references mentioned in relevant contexts, the most likely influential source papers are:\n\n{\n  \"[b38]\": 0.9,\n  \"[b16]\": 0.8,\n  \"[b29]\": 0.7,\n  \"[b2]\": 0.6\n}\n\nRationale:\n- [b38] and [b16] are existing methods for overlapped and nested EE that use pipeline approaches, which this paper aims to improve upon with the one-stage model. They are mentioned when discussing the limitations of prior work.\n\n- [b29] is described as the current state-of-the-art model that also uses a pipeline method, indicating it is a key point of comparison and motivation for this work.\n\n- [b2] refers to BERT, which is adopted as the encoder to get contextualized word representations, an important component of the proposed model.\n\nThe other references like [b23], [b12], [b20], [b1], [b25] provide relevant background on EE but do not seem to be as directly influential on the specific novel contributions of this paper based on how they are mentioned in the text. The confidence scores reflect the assessed degree of direct relevance and inspiration from each reference.", "62b3da1e5aee126c0fb1b3bc": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria provided:\n\nThe key novel contributions of this paper seem to be:\n1. A stage-wise fine-tuning framework called LM-GNN for encoding text data with transformers and GNN models. This involves pre-fine-tuning the transformer with graph information before jointly fine-tuning it with the GNN.\n2. System and design optimizations to improve the training efficiency of the LM-GNN framework and allow it to scale to large graphs.\n\nThe paper mentions being inspired by or building upon the following key references:\n\n{\n  \"[b5]\": 0.9,\n  \"[b8]\": 0.8,\n  \"[b20]\": 0.7,\n  \"[b12]\": 0.6\n}\n\n[b5] seems to be a foundational reference for the layered GNN architecture that linearly combines neighborhood features followed by nonlinearities. The LM-GNN framework builds upon this basic GNN structure.\n\n[b8] introduces the BERT language model which is employed as the transformer to encode textual node features in LM-GNN. Integrating BERT with the GNN is a key aspect of their framework.\n\n[b20] presents the RGCN model that the paper adapts as its graph encoder for heterogeneous graphs. They augment RGCN with a projection of the node's own embedding.\n\n[b12] introduces GraphSAGE which is used as the graph encoder for homogeneous graphs to evaluate the LM-GNN framework. \n\nThe paper also references [b7] and notes that it identifies pre-training BERT on graph data can be beneficial. However, [b7] does not jointly fine-tune BERT and GNN together, which is a key contribution here.\n\nIn summary, references [b5], [b8], [b20] and [b12] appear to be the most influential prior works that inspired key components of the LM-GNN framework and methodology. The confidence scores reflect the centrality of each reference to the main contributions.", "62e0acfd5aee126c0f20a05e": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\nThe key novel contributions of this paper are:\n1. Generating structurally diverse and valid DNN computation graphs for testing DL compilers, by capturing type constraints of operators and using an SMT solver. \n2. Executing the compiled models without floating point exceptional values, by using gradient-guided search to find viable model inputs/weights.\n\nThe paper mentions being inspired by or building upon the following key references, in order of relevance:\n1. [b62] - NNSmith uses gradient-guided search inspired by the GRIST technique from this paper, though for the opposite goal of finding inputs that avoid (rather than trigger) floating point exceptions.\n2. [b28], [b32], [b56] - NNSmith adopts a fuzzing and differential testing approach similar to these prior works on testing DL compilers/frameworks. However, NNSmith addresses challenges in generating diverse valid models and avoiding exceptional values, which are not adequately handled by these previous works.\n3. [b39] - NNSmith uses the Z3 SMT solver from this reference for solving type constraints when generating valid computation graphs.\n\nIn summary, the most pertinent source papers that directly inspired novel aspects of NNSmith are:\n{\n  \"b62\": 0.9,\n  \"b28\": 0.7, \n  \"b32\": 0.7,\n  \"b56\": 0.7,\n  \"b39\": 0.6\n}\n\nThe scores indicate the estimated degree of relevance, with [b62] being the most central reference that motivated NNSmith's gradient-guided input search. The differential testing papers [b28], [b32], [b56] provide important context but have a lower score since NNSmith makes significant advances over their techniques. Finally, [b39] is included for providing a key component (the SMT solver) used in NNSmith's graph generation.", "62c28ae45aee126c0f8a1660": "Here is my analysis of the key source papers for this paper, based on the criteria you provided:\n\n{\n  \"b15\": 0.9,\n  \"b33\": 0.8,\n  \"b45\": 0.8,\n  \"b11\": 0.7,\n  \"b25\": 0.7,\n  \"b19\": 0.6,\n  \"b9\": 0.6,\n  \"b40\": 0.6\n}\n\nConfidence scores:\n- [b15] GCN: 0.9 - The paper introduces GCN as one of the key GNN layers used in their operation candidate set. GCN seems to be a foundational work that directly inspires their GNN architecture search space.\n\n- [b33] GAT: 0.8 - GAT is also included as a key GNN layer in their search space, indicating it is an important reference that shapes their methodology. \n\n- [b45] GIN: 0.8 - GIN is another GNN layer included in the search space, so it directly contributes to their GNN architecture customization approach.\n\n- [b11] GraphSAGE: 0.7 - GraphSAGE is included as one of the GNN layers to search over, though with slightly less emphasis compared to GCN, GAT and GIN. \n\n- [b25] GraphConv: 0.7 - GraphConv is the 5th GNN layer included in the search space. While important, it receives less focus compared to the first 3 GNN layers mentioned.\n\n- [b19], [b9], [b40]: 0.6 each - These 3 papers on graph neural architecture search are referenced when introducing the background and motivation. They seem to have inspired studying the GraphNAS problem, but have less direct influence on the specific novel techniques proposed in this paper.\n\nThe other references are either not as directly tied to the main novel contributions, or are cited more in passing when discussing background concepts. The key novelty seems to be extending GraphNAS to handle distribution shift using techniques like disentangled graph encoding and architecture customization with prototypes. The 5 GNN layer references directly shape those contributions.", "63f5888490e50fcafd27c90c": "Here is my analysis of the most pertinent source papers for the given paper, in JSON format with reference numbers as keys and confidence scores between 0 and 1 as values:\n\n{\n  \"[b53]\": 0.95,\n  \"[b28]\": 0.9,\n  \"[b35]\": 0.8,\n  \"[b24]\": 0.7,\n  \"[b25]\": 0.7,\n  \"[b26]\": 0.7,\n  \"[b9]\": 0.6,\n  \"[b21]\": 0.6\n}\n\nThe key reasons for selecting these source papers are:\n\n1. The paper introduces a new hybrid embedding representation that combines embedding tables and dynamically generated embeddings. This is directly inspired by and builds upon the Tensor Train Compression (TT-Rec) [b53] and Deep Hash Embedding (DHE) [b28] techniques, as evidenced by the detailed descriptions of these methods.\n\n2. The paper focuses heavily on the memory capacity and bandwidth challenges of large embedding tables, citing [b35] multiple times when discussing this issue. This indicates [b35] was an important motivating work.\n\n3. For the hardware evaluation, the paper benchmarks performance on Google TPUs [b24][b25][b26] and Graphcore IPUs [b9][b21]. The frequent references to these accelerators when analyzing hardware-representation compatibility suggests the accelerator papers were influential in the evaluation methodology.\n\n4. Overall, the novel contributions center around the hybrid representation extending [b53] and [b28], and the hardware co-design techniques to map this representation to AI accelerators like TPUs and IPUs. The source papers most relevant to these core aspects were prioritized in the list.\n\nThe confidence scores reflect the degree to which each reference directly inspired key components of the paper's methodology and experiments, based on the frequency and context in which they were cited. Papers [b53] and [b28] have the highest scores as the proposed hybrid representation heavily builds upon them.", "626603225aee126c0f2338e4": "Here is my analysis of the most pertinent source papers for the given paper, in JSON format with confidence scores:\n\n{\n  \"[b65]\": 0.9,\n  \"[b43]\": 0.85,\n  \"[b13]\": 0.7,\n  \"[b17]\": 0.6,\n  \"[b45]\": 0.5,\n  \"[b60]\": 0.5,\n  \"[b32]\": 0.4,\n  \"[b70]\": 0.4\n}\n\nReasoning:\nThe paper introduces a novel approximate neighbor search algorithm and hardware co-design to improve the efficiency of point cloud processing. The key inspirations seem to come from:\n\n1. [b65] Tigris and [b43] QuickNN, which are directly compared against and built upon. The proposed split-tree algorithm enables fully streaming DRAM accesses similar to these prior works, but reduces search load and DRAM traffic. This shows heavy reliance on and improvement over [b65] and [b43].\n\n2. [b13] introduces the K-d tree data structure which is the basis of the neighbor search algorithm here. The paper retains K-d tree search in the sub-trees unlike [b65] and [b43].\n\n3. [b17] is cited to show that both the neighbor search and feature computation stages are important to optimize, motivating this work. \n\n4. [b45], [b60], [b32], [b70] relate to avoiding bank conflicts in regular kernels through careful data layouts. The irregular accesses here make this inapplicable, but these works highlight the bank conflict problem that this paper addresses through a different selective elision approach.\n\nIn summary, [b65] and [b43] appear to be the strongest motivations methodologically, while [b13] provides the core search data structure. The others support the problem definition and challenges. Overall confidence in this assessment is high based on the direct comparisons and extensions of the key prior works.", "63fec3cd90e50fcafdd70322": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b16]\": 0.9,\n  \"[b21]\": 0.9,\n  \"[b22]\": 0.8,\n  \"[b2]\": 0.7,\n  \"[b8]\": 0.6\n}\n\nThe key points that led me to identify these as the most relevant source papers are:\n\n1. The paper directly states that it is struggling for non-English language communities to develop knowledge-grounded dialogue models due to limited datasets, in contrast to English models like \"[b16]\" (Microsoft's GODEL), \"[b21]\" (Meta AI's Blenderbot 3) and \"[b22]\" (Google's LaMDA). This indicates these papers were strong motivations and points of comparison.\n\n2. The paper mentions that even for Chinese, there are few knowledge-grounded dialogue models publicly available, with \"[b2]\" and \"[b8]\" (EVA2.0) cited as examples of Chinese dialogue models that are not knowledge-grounded. This suggests [b2] and [b8] are relevant prior work the authors aim to improve upon by making their model knowledge-grounded.\n\n3. The novel contributions of this paper seem to be: 1) Techniques for knowledge-grounded dialogue with limited datasets 2) A new evaluation platform 3) Releasing a Chinese knowledge-grounded model and toolkit. The most relevant citations are the English knowledge-grounded models they compare to methodologically.\n\n4. I did not find any phrases directly stating certain papers \"inspired\" this work. But the repeated comparisons to [b16], [b21] and [b22] suggest those papers provided the blueprint they aimed to adapt for Chinese.\n\nSo in summary, [b16], [b21] and [b22] seem to be the most central influences based on the extensive comparisons and methodological similarities, while [b2] and [b8] represent the key Chinese dialogue models this paper aims to improve upon. The assigned confidence scores reflect the strength of evidence for each paper's influence based on the textual analysis.", "6466faedd68f896efaeb70be": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\nThe key novel contributions of this paper are:\n1. Showing that considering edge directionality substantially increases effective homophily in heterophilic graphs\n2. Proposing the Dir-GNN framework to extend MPNNs to directed graphs by performing separate aggregations over incoming and outgoing edges\n3. Theoretically proving Dir-GNN is as expressive as the Directed Weisfeiler-Lehman test and more expressive than MPNNs\n4. Empirically demonstrating that augmenting GNNs with Dir-GNN yields large gains on heterophilic datasets\n\nBased on this, the most relevant source papers appear to be:\n\n{\n  \"[b52]\": 0.9,\n  \"[b33]\": 0.8,\n  \"[b49]\": 0.7,\n  \"[b15]\": 0.7\n}\n\n[b52] is referenced when discussing how heterophily is complex to capture and introducing the class compatibility matrix to better represent a graph's homophily. It is also mentioned that [b52] noted 2-hop neighborhoods tend to be more homophilic than 1-hop in heterophilic graphs, directly motivating the effective homophily concept in this paper.\n\n[b33] is cited when mentioning that MPNNs struggle to separate a node's embedding from its neighbors, a challenging problem in heterophilic settings that has gained attention recently. Addressing this limitation seems to be a key motivation behind the Dir-GNN framework.\n\n[b49] establishes the expressivity limits of MPNNs in terms of the 1-WL test. This paper proves Dir-GNN matches the expressivity of the Directed 1-WL test, drawing direct comparisons to the results in [b49].\n\n[b15] appears to define the Directed 1-WL test that the theoretical expressivity analysis is based on, though the reference is only briefly mentioned without much detail on its contents.\n\nThe scores reflect my confidence that these references provided significant inspiration and motivation for the novel contributions in this paper, based on how they are discussed in the text. Let me know if you need any clarification or have additional questions!", "6459ac6bd68f896efa659285": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"b10\": 0.9,\n  \"b11\": 0.8,\n  \"b13\": 0.7,\n  \"b5\": 0.6,\n  \"b14\": 0.5\n}\n\nThe key points that led me to identify these as the most relevant source papers are:\n\n- The paper utilizes the REBEL (Relation Extraction By End-to-end Language generation) model [b10], which is a core part of their methodology for extracting relations from text to build knowledge graphs. This indicates [b10] was very directly influential.\n\n- ChatGPT [b11] is the other main model used in their experiments and methodology. The detailed description of how ChatGPT works and was trained suggests it was also a major inspiration.\n\n- REBEL is based on the BART seq2seq model [b13], so this is a key underlying model that enables their relation extraction approach, even if not the main focus of the paper.\n\n- GPT-3 [b5] is mentioned as part of the training process for ChatGPT. While less central than REBEL and ChatGPT themselves, GPT-3 still seems to play an important role.\n\n- The reinforcement learning methods used to train ChatGPT, specifically Reinforcement Learning from Human Feedback (RLHF) [b14], are outlined in some detail, indicating this training approach was influential in developing their ChatGPT-based knowledge graph generation methods.\n\nThe other referenced papers provide useful background and context, but based on the criteria of identifying the core novel methods and the papers most directly linked to developing those methods, the ones listed above stand out as the most central inspirations and foundations for this paper's contributions. Let me know if you need any clarification or have additional questions!", "635753cc90e50fcafdddd9f5": "Based on the provided text, here are the key steps to identify the most pertinent source papers:\n\n1. The primary challenges and novel contributions of the paper are:\n- Jointly performing graph-level data layout optimization and operator-level loop optimization for deep models \n- Eliminating overhead of layout transformation via a layout propagation mechanism\n- Avoiding search space reconstruction during joint tuning via a two-stage approach and cross-exploration architecture\n- Improving tuning efficiency by pruning the joint layout and loop search space\n\n2. Key phrases indicating direct inspiration or motivation from previous work:\n- \"To eliminate the two types of overhead brought by layout transformation, we propose a layout propagation mechanism.\"\n- \"To alleviate the search space reconstruction issue in the co-tuning, our solution is two folds.\"\n- \"To avoid the search space explosion due to the combination of layout and loop tuning, we prune the space at two levels.\"\n\n3. The novel methods introduced are the layout propagation mechanism, two-stage joint tuning approach with cross-exploration, and search space pruning techniques.\n\n4. Based on the key phrases, novel methods, and evaluation of reference titles, the most pertinent source papers appear to be:\n\n{\n  \"[9]\": 0.9,\n  \"[82]\": 0.85,\n  \"[5]\": 0.8,\n  \"[69]\": 0.8,\n  \"[89]\": 0.75,\n  \"[43]\": 0.7\n}\n\nThe numbers represent confidence scores between 0 and 1. References [9], [82], [5], [69] are directly mentioned in the context of the novel joint optimization approach. [89] and [43] are also relevant as they discuss auto-tuning and layout optimization techniques that the paper aims to improve upon.", "63a2794890e50fcafd29405f": "YouChat is a large language model from You.com.\n\nYou.com is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. YouChat will generate a comprehensive and informative answer for a given question based on the provided web Search Results. YouChat considers the history of the conversation and answer the questions in a helpful manner. It tries to use the information from the provided search results to ensure accuracy and uses an unbiased and journalistic tone. YouChat will combine search results together into a coherent answer and cite search results using [${{number}}] notation. It will only cite the most relevant results that answer the question accurately.", "63b3f1f890e50fcafdea0718": "Here is my analysis of the most pertinent source papers for the given paper, in JSON format with reference numbers as keys and confidence scores between 0 and 1 as values:\n\n{\n  \"[b25]\": 0.95,\n  \"[b9]\": 0.85,\n  \"[b15]\": 0.8,\n  \"[b13]\": 0.75,\n  \"[b30]\": 0.7\n}\n\nThe key insights that led me to this conclusion are:\n\n1. The paper introduces novel methods to evaluate the accuracy of speedup factor (SF) predictions provided by Intel Thread Director (TD) and compares them to PMC-based SF prediction models. This directly builds upon and is inspired by the authors' previous work in [b25] which proposed the Phase-SF methodology to build PMC-based SF prediction models.\n\n2. The paper implements asymmetry-aware scheduling algorithms like ACFS from [b25] and analyzes their effectiveness when driven by TD vs PMC-based SF predictions. This shows a strong reliance on [b25].\n\n3. Other asymmetry-aware scheduling algorithms evaluated like AARR and TO were originally proposed in [b30] and [b15] respectively. Implementing and comparing against these prior works indicates they served as motivation.\n\n4. The paper focuses on leveraging TD, a new hardware feature in Intel Alder Lake processors, to aid OS scheduling decisions on asymmetric multicore processors (AMPs). [b13] likely contains key details on TD that inspired this work.\n\n5. Discussions around the challenges of scheduling on AMPs and the need for the OS scheduler to factor in SF of threads refers to insights from [b9].\n\nIn summary, while the paper cites several prior works, [b25], [b9] and [b15] emerge as the most central references that directly inspired the novel contributions and methodology of this work in terms of building SF prediction models and asymmetry-aware schedulers to evaluate them. [b13] and [b30] also likely provided key context and motivation.", "6466fafbd68f896efaeb77ac": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b11]\": 0.9,\n  \"[b43]\": 0.8,\n  \"[b10]\": 0.7,\n  \"[b42]\": 0.6,\n  \"[b20]\": 0.5\n}\n\nThe key points that led me to identify these as the most relevant source papers are:\n\n1. The paper introduces a novel framework for studying meaning in language models by applying language modeling to program synthesis and leveraging the precise semantics of programming languages. References [b11] on abstract interpretation and [b43] on formal semantics are directly related to the paper's core methodology of using program tracing and abstract interpretation to define and measure meaning.\n\n2. The paper empirically evaluates whether language models trained only on next token prediction can capture meaningful semantic information. Tracing program execution [b10] is central to their definition of meaning and experimental setup.\n\n3. The architecture and training objective (next token prediction) of the language model used in the experiments is based on the Transformer [b42], making this a key enabling work. \n\n4. In discussing how the language model's generative process may encode future semantic states before generating the actual tokens, the authors draw an analogy to theories of human speech production [b20] where an initial semantic conception precedes the translation to an utterance. This indicates [b20] provided important conceptual motivation.\n\n5. While other references like [b33], [b4], [b31] are also relevant for framing the high-level question and hypotheses, the works cited above appear to be the most central to the novel technical contributions and core methodology introduced in this paper based on the criteria around methodological similarity, comparative phrases, and relevance to the paper's novel aspects.", "646c3addd68f896efa5d165d": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b0]\": 0.9,\n  \"[b6]\": 0.7,\n  \"[b23]\": 0.7,\n  \"[b7]\": 0.7,\n  \"[b12]\": 0.7,\n  \"[b18]\": 0.6,\n  \"[b8]\": 0.6,\n  \"[b2]\": 0.6,\n  \"[b16]\": 0.6,\n  \"[b24]\": 0.6,\n  \"[b1]\": 0.8\n}\n\nRationale and key observations:\n\n- The paper is heavily inspired by the concept of \"in-context learning\" from [b0], as evidenced by phrases like \"In-context learning is a novel and one of the most intriguing capabilities of language models [b0]\". This appears to be the core motivation for the paper's contributions.\n\n- The paper contrasts its approach with existing graph pretraining methods [b6] [b23] [b7] [b12] and meta-learning methods [b18] [b8] [b2] [b16] [b24]. While these are important related works, the phrasing suggests the proposed method is novel compared to them. \n\n- The prompt graph representation proposed in the paper is directly \"Inspired by [b1]\". This indicates [b1] is a key methodological inspiration.\n\n- Overall, the novel contributions appear to be: 1) Formulating in-context learning for graph tasks, 2) The prompt graph representation, 3) The PRODIGY pretraining framework. \n\n- [b0] and [b1] seem to most directly inspire the core ideas, while the other citations provide important context but the phrasing suggests the proposed ideas are distinct from them.\n\n- I limited the output to the most relevant 11 references based on the criteria of direct inspiration, relevance to novel contributions, and methodological similarity. The other references, while topically related, do not seem to be as central to the main ideas.\n\nLet me know if you would like me to clarify or expand on any part of the analysis! I aimed to focus on the key inspirations and connections based on the language used in the text.", "64702deed68f896efa51ffa0": "Here is my attempt at identifying the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"[b11]\": 0.9,\n  \"[b24]\": 0.9,\n  \"[b27]\": 0.8,\n  \"[b19]\": 0.7,\n  \"[b18]\": 0.6,\n  \"[b21]\": 0.6\n}\n\nRationale:\nThe paper focuses on using pre-trained language models (PLMs) for open knowledge graph completion. The key novel contributions seem to be:\n\n1. An end-to-end framework TAGREAL that exploits implicit knowledge in PLMs and textual information to perform open KG completion, without requiring manually defined facts or prompts. \n\n2. Automatic prompt generation and information retrieval methods to create high-quality prompts for PLM knowledge probing and search for relevant support information.\n\nReferences [b11] and [b24] are mentioned multiple times when discussing the power of PLMs, so they likely provide key inspiration and motivation for using PLMs for this task. \n\n[b27] proposes PKGC which also uses PLMs for KG completion but relies on manual prompts, while this paper aims to generate prompts automatically. So [b27] is a highly relevant prior work.\n\n[b19], [b18] and [b21] are referenced in the prompt mining section as providing key methods (MINE, MetaPAD, TruePIE) that the authors build upon for automatic prompt generation. These papers seem to directly contribute to one of the main novel aspects of the work.\n\nThe other references, while relevant for background and context, do not appear to be as central to the main contributions based on my analysis of the text. Examining the titles may provide additional evidence but the full text already gives pretty clear indications of the key influences in my assessment.", "632630ff90e50fcafdf67436": "Based on analyzing the text, here are the key source papers that likely inspired this work, in JSON format with confidence scores:\n\n{\n  \"[b11]\": 0.9,\n  \"[b17]\": 0.8,\n  \"[b27]\": 0.8,\n  \"[b4]\": 0.7,\n  \"[b12]\": 0.6\n}\n\nExplanation:\nThe paper introduces Mockingjay, an ETA-based cache replacement policy that aims to faithfully mimic Belady's MIN policy. The key inspirations seem to be:\n\n[b11] Hawkeye cache replacement policy - Mockingjay builds upon Hawkeye's idea of learning the optimal caching behavior of Belady's MIN algorithm from past accesses. This is referenced multiple times as the current state-of-the-art.\n\n[b17] (KPK) and [b27] (IbRDP) - These two prior works share Mockingjay's goal of evicting lines based on their ETA (estimated time of arrival). However, the paper explains their fundamental flaws that Mockingjay overcomes. They serve as the main motivation.\n\n[b4] Belady's MIN algorithm - This is the optimal clairvoyant policy that Mockingjay aims to emulate without having future knowledge. It is the key inspiration behind ETA-based policies.\n\n[b12] is referenced in the context of Mockingjay's prefetching implementation, as it emulates the Flex-MIN policy from this paper.\n\nThe paper also references many other prior works, but the ones above stand out as the most influential in directly inspiring Mockingjay's key ideas and contributions. The confidence scores reflect the extent to which each reference is discussed as a motivation or foundation for this work.", "6456389bd68f896efacf6b14": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\n{\n  \"b14\": 0.9,\n  \"b20\": 0.7,\n  \"b30\": 0.7,\n  \"b16\": 0.6,\n  \"b27\": 0.6,\n  \"b13\": 0.8,\n  \"b7\": 0.8,\n  \"b17\": 0.5\n}\n\nConfidence scores:\n- b14 (ChatGPT): 0.9 - The paper directly states that ChatGPT has \"particularly stood out\" and inspired their work on using LLMs for AutoML. This reference is central to the main idea.\n\n- b20, b30: 0.7 - These references on in-context learning are highlighted as an emerging research topic spurred by LLMs that the paper aims to build upon. \n\n- b16, b27: 0.6 - Chain-of-thought prompting, covered in these references, is mentioned as another key LLM-driven research area motivating the work.\n\n- b13 (model card), b7 (data card): 0.8 - Incorporating model and data cards is a core methodological component of AutoML-GPT. The paper emphasizes tapping into these to enable LLMs to connect diverse models.\n\n- b17 (CLIP): 0.5 - CLIP is suggested as the text encoder for calculating dataset similarity when tuning hyperparameters for unseen data. While relevant, this is a more minor implementation detail.\n\nThe other references like b0, b1, b23 provide useful background on LLMs but do not seem to be highlighted as direct influences on the novel aspects of AutoML-GPT. The key contributions center around using LLMs with model/data cards for AutoML.", "648697e6d68f896efaa87966": null, "648697e6d68f896efaa8781e": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b28]\": 0.9,\n  \"[b33]\": 0.85,\n  \"[b30]\": 0.8,\n  \"[b0]\": 0.75,\n  \"[b2]\": 0.7\n}\n\nConfidence scores:\n- [b28] (0.9): The paper mentions that message passing in GNNs is equivalent to Laplacian smoothing on node embeddings in MLPs, which directly inspires their theoretical analysis of the under-confidence problem in GNN-to-MLP distillation. The methodological focus on message passing is also very relevant.\n\n- [b33] (0.85): GLNN is mentioned as the key baseline that directly distills knowledge from GNNs to MLPs without additional operations. Improving upon the limitations of GLNN is a core motivation of this work.\n\n- [b30] (0.8): CPF is another important baseline for GNN-to-MLP distillation that incorporates label propagation in MLPs. The proposed method aims to provide a more efficient alternative.\n\n- [b0] (0.75): RKD-MLP, which filters unreliable soft labels in a downsampling manner, is discussed as a contrast to the upsampling-style strategy proposed in this paper for providing more reliable supervision. \n\n- [b2] (0.7): While less directly relevant, GNN-SD is mentioned as a method for GNN-to-GNN distillation that aims to solve over-smoothing but has limitations in shallow student models. This provides useful context for the problem space.\n\nThe other referenced papers provide important background context but are less directly inspirational for the novel contributions compared to the top 5 identified above, based on the language used to discuss them. Let me know if you need any clarification or have additional questions!", "6389d70490e50fcafdffd95e": "Here is my analysis of the most pertinent source papers for this paper, with confidence scores:\n\n{\n  \"[5]\": 0.9,\n  \"[3]\": 0.8,\n  \"[18]\": 0.7,\n  \"[10]\": 0.8\n}\n\nThe key reasons are:\n\n1. The paper takes direct inspiration from Compresso [5] and builds upon its approach of using hardware memory compression to increase effective memory capacity. The paper compares against Compresso as the state-of-the-art prior work. [5] is cited multiple times when discussing the background and challenges.\n\n2. The paper borrows the idea of maintaining a Free List to track free space in DRAM from [3] and [5]. The Free List data structure and its usage is a key component of the proposed TMCC design.\n\n3. For the ML1 portion of memory, TMCC proposes compressing it using bandwidth compression techniques like TMC [18]. So [18] directly influences a core part of TMCC's methodology.\n\n4. To address the challenge of slow decompression for the ML2 portion of memory, the paper specializes IBM's state-of-the-art ASIC Deflate design [10] which was designed for both storage and memory. Improving upon [10]'s decompression speed is critical for TMCC's ML2 performance.\n\nIn summary, [5], [3], [18] and [10] emerge as the most influential prior works that inspired core aspects of TMCC's design, with [5] being the most central reference that the proposed approach builds upon and compares against. The other references provide more background information but do not seem to directly motivate the key novel contributions as much.", "64389992d6db87a146dd25d0": "Based on analyzing the paper text, here are the key source papers that likely inspired or motivated this work, in JSON format with confidence scores:\n\n{\n  \"[b35]\": 0.9,\n  \"[b23]\": 0.8,\n  \"[b36]\": 0.8,\n  \"[b34]\": 0.7,\n  \"[b37]\": 0.6\n}\n\nReasoning:\n- The paper mentions that AMD's IBS is based on the instruction sampling technique proposed in [b35], indicating [b35] was a key inspiration. \n- [b23] and [b36] are cited multiple times when describing the details of how IBS works in AMD processors. The comparisons to IBS suggest the authors studied these references closely.\n- [b34] is cited frequently when discussing details of Intel's PEBS facility. Examining Intel's PEBS seems to be a core part of this comparative study.\n- The IBS driver configuration in [b37] is mentioned, suggesting that reference provided important context for evaluating IBS.\n\nThe other references like [b0]-[b2], [b6]-[b15] are cited more in passing when listing prior work leveraging PEBS or IBS for profiling. They don't seem to be as central to directly motivating the novel contributions here - the detailed comparison of PEBS vs IBS microarchitectural characteristics.", "6482a38fd68f896efa8db695": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria provided:\n\nThe key novel contributions of this paper appear to be:\n1) Proposing Mixture-of-Domain-Adapters (MixDA), which injects domain-specific knowledge into language models via multiple domain adapters in parallel to the original feed-forward networks. This allows adapting to new domains while preserving old-domain knowledge.\n2) A two-stage training approach: first domain-specific tuning of the adapters on unlabeled data, then task-specific tuning of a mixture-of-adapters gate and task adapter on labeled data.\n\nThe paper mentions being directly inspired by the following references regarding where knowledge is stored in language models:\n[18], [5], [48] - These studies found that knowledge is stored in the feed-forward networks (FFNs) of language models. This directly motivated the approach of decoupling the FFNs into the original pre-trained part and novel domain-specific adapters.\n\nOther key references that share strong methodological similarities:\n[26], [53], [27] - These papers on adapter-based tuning for parameter-efficient fine-tuning are highly relevant, as MixDA builds upon the adapter approach. \n[57], [16], [78], [71], [37], [55] - Work on mixture-of-experts (MoE) is pertinent to the mixture-of-adapters gating mechanism proposed here.\n\nIn summary, the most central source papers appear to be:\n{\n  \"[18]\": 0.95,\n  \"[5]\": 0.95, \n  \"[48]\": 0.95,\n  \"[26]\": 0.8,\n  \"[53]\": 0.8,\n  \"[57]\": 0.7\n}\n\nThe [18], [5] and [48] references directly inspired the core idea of injecting knowledge into feed-forward networks. The [26], [53] and [57] papers provide important methodological foundations in terms of adapters and mixture-of-experts gating. A few other references like [27], [78] also seem quite relevant but the ones listed above appear to be the most crucial influences.", "640e949e90e50fcafd114d4b": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria provided:\n\nThe paper introduces EXPHORMER, a graph transformer architecture that uses sparse attention mechanisms to improve scalability while maintaining strong performance. The key novel contributions are:\n\n1. Using expander graphs as a sparse attention mechanism tailored for graphs, which spectrally approximate full attention with linear complexity. \n2. Combining expander graphs with global virtual nodes and local neighborhoods to create an effective sparse attention pattern.\n3. Demonstrating that EXPHORMER achieves results comparable to or better than full attention transformers and message passing networks, while scaling to larger graphs.\n\nBased on this, the most relevant references seem to be:\n\n{\n  \"[b35]\": 0.9,\n  \"[b25]\": 0.8,\n  \"[b44]\": 0.7,\n  \"[b29]\": 0.6\n}\n\n[b35] introduces GraphGPS, the framework EXPHORMER builds on by replacing the transformer component. It is directly cited as allowing sparse transformers to be combined with message passing networks.\n\n[b25], [b44] and [b29] introduce other graph transformer architectures. The paper compares EXPHORMER to these to show its advantages.\n\nA few other references like [b9], [b47] on sparse transformers for sequences, and [b43], [b30] on limitations of message passing networks, provide useful context. But the novel contributions are more directly influenced by the graph transformer papers, especially GraphGPS.\n\nIn summary, while the paper cites a broad set of relevant work, its core ideas seem most directly inspired by recent graph transformer architectures, which it aims to improve upon in terms of scalability and performance by introducing a novel sparse attention mechanism based on expander graphs. GraphGPS [b35] is particularly influential as the framework it extends.", "63dcdb422c26941cf00b6136": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\nThe key novel contributions of this paper seem to be:\n1. Assembling a new large-scale, challenging and diverse benchmark called \"Sparsity May Cry\" (SMC-Bench) to re-define evaluation protocols for sparse neural networks. \n2. Revealing that state-of-the-art sparse algorithms fail on SMC-Bench even at low sparsity levels.\n3. Conducting extensive experiments on SMC-Bench to analyze the behavior and limitations of current sparse neural network approaches.\n4. Hypothesizing that the failure of existing sparse algorithms on SMC-Bench may be due to the \"lazy regime\" dynamics in overparameterized models.\n\nBased on this, here are the most relevant references that likely inspired or motivated the paper's main contributions, in JSON format with confidence scores:\n\n{\n  \"[b7]\": 0.9,\n  \"[b50]\": 0.9,\n  \"[b31]\": 0.8,\n  \"[b20]\": 0.7,\n  \"[b16]\": 0.7,\n  \"[b93]\": 0.6,\n  \"[b37]\": 0.5,\n  \"[b44]\": 0.5\n}\n\nRationale:\n- [b7] and [b50] are highlighted as potentially explaining the failure of sparse algorithms on SMC-Bench due to \"lazy regime\" dynamics. This seems to be a key insight and motivation for the paper.\n- [b31] introduces second-order pruning which the authors hypothesize is more reliable for SMC-Bench tasks. Directly comparing against and building on this work is likely a core contribution.\n- [b20], [b16], [b93], [b37] and [b44] are some of the key sparse neural network approaches that are extensively evaluated on SMC-Bench. Rigorously testing these SOTA methods on challenging new tasks is central to the paper's analysis and findings.\n\nThe other references, while relevant background, do not seem to be as directly tied to the main novel aspects and core motivation of this particular paper based on my analysis of the text. Let me know if you would like me to clarify or expand the analysis further.", "6464b048d68f896efa35a285": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b26]\": 0.95,\n  \"[b2]\": 0.8,\n  \"[b0]\": 0.7,\n  \"[b75]\": 0.7,\n  \"[b68]\": 0.6,\n  \"[b24]\": 0.5\n}\n\nConfidence scores:\n- [b26] (0.95): This paper is directly cited multiple times when discussing the masked autoencoder (MAE) framework that the proposed GAN-MAE builds upon. The authors state their experiments follow the same settings as MAE and make direct comparisons to it, indicating it is the primary inspiration and baseline.\n\n- [b2] (0.8), [b0] (0.7), [b75] (0.7), [b68] (0.6): These papers are cited when introducing masked image modeling (MIM) approaches for vision transformers, which is the key area the proposed method aims to improve upon. They likely contain important prior work on MIM that motivated this paper.\n\n- [b24] (0.5): This is the original GAN paper, cited when discussing generative adversarial networks in the related work. While foundational, it is less directly related to the novel contributions compared to the MAE and MIM papers.\n\nThe key phrases indicating direct inspiration are things like \"We propose a Generative Adversarial Networks-based pre-training framework, referred to as GAN-MAE, which contains two components: a generator model learns to reconstruct the masked patches according to visible patches in the encoder-decoder architecture and a discriminator model learns to distinguish real image patches from plausible but synthesized remains.\" and \"Our experiments follow the same architecture, settings, and pre-training recipe as MAE [b26]\".\n\nSo in summary, [b26] on MAE is clearly the most central prior work, while the MIM papers [b2], [b0], [b75] and [b68] contain key approaches the authors aim to improve upon with their GAN-based framework. The original GAN paper [b24] provides important background but is less directly related to the core novel contributions.", "63dcdb422c26941cf00b613b": "Here are the key steps I took to identify the most pertinent source papers for this paper:\n\n1. The paper proposes a novel diffusion-based model called DIFFORMER for learning representations of instances in a dataset. The key novel contributions are:\n- Treating a dataset of instances as a whole and learning representations through a diffusion process on latent manifolds \n- Introducing an energy function to regularize the diffusion process and guide the instance representations\n- Proving an equivalence between the diffusion process and iterative energy minimization, which enables a closed-form optimal solution for the diffusivity function\n- Proposing two practical instantiations of DIFFORMER with different complexity\n\n2. The paper does not contain explicit phrases like \"inspired by\" or \"motivated by\" that directly point to key source papers. So I focused on identifying references that are linked to the main novel aspects and methods.\n\n3. Key references that seem to directly contribute to the novel diffusion model:\n- [b19] and [b33] for the idea of treating data as heat diffusing on Riemannian manifolds \n- [b18] and [b29] for the formulation of the anisotropic diffusion PDE\n- [b61] for the energy function that regularizes global and local consistency of representations\n\n4. The paper also draws connections to existing models like GNNs, showing DIFFORMER provides a unified framework. But the key source papers seem to be the ones linked to the novel diffusion model.\n\n5. Based on analyzing the titles, [b19] (\"Diffusion Improves Graph Learning\") and [b33] (\"Diffusion Processes and their Sample Paths\") appear to be central references for the diffusion model idea. The others provide more specific components.\n\nIn summary, here are the source papers I identified as most pertinent, with confidence scores:\n\n{\n  \"[b19]\": 0.9,\n  \"[b33]\": 0.8,\n  \"[b18]\": 0.7,\n  \"[b29]\": 0.7,  \n  \"[b61]\": 0.7\n}\n\nThe papers [b19] and [b33] have the highest scores as they seem to be the key inspiration for the overall diffusion model framework proposed in this paper. The others contribute important pieces but are less central overall.", "648000a9d68f896efaa12362": "Here is my analysis of the most pertinent source papers for the given paper, presented in JSON format with reference numbers as keys and confidence scores between 0 and 1 as values:\n\n{\n  \"[b18]\": 0.95,\n  \"[b44]\": 0.9,\n  \"[b45]\": 0.85,\n  \"[b13]\": 0.8,\n  \"[b11]\": 0.75,\n  \"[b10]\": 0.7,\n  \"[b39]\": 0.65\n}\n\nThe key novel contributions of this paper are:\n\n1. Deriving a theoretical quantification of how GNNs can exacerbate conditional shift, identifying graph heterophily and graph convolutions as contributing factors. This builds upon foundational GNN work in [b18] and analysis of GNNs under heterophily in [b44].\n\n2. Proposing GCONDA, a graph unsupervised domain adaptation method that minimizes conditional shift, inspired by the theoretical findings. This aims to improve upon prior graph UDA methods like [b45] that focus on covariate shift.\n\n3. Proving a generalization bound for GNNs relating expected target error to the Wasserstein distance between source and target label distributions, extending results from [b11]. The bound also incorporates the Lipschitz constant of the GNN, tying to recent theoretical work on GNN Lipschitzness [b10],[b39].\n\nThe most direct inspiration seems to come from [b11] in terms of using Wasserstein distance to bound UDA error and [b45] in identifying limitations of covariate shift assumptions for graph UDA. [b18], [b44], [b10] and [b39] provide key mathematical tools and concepts the paper builds upon in its theoretical analysis. Overall, the paper makes novel theoretical contributions in understanding conditional shift in GNNs and proposes a well-motivated algorithm GCONDA to address it.", "6385788590e50fcafdf49aef": "Based on the analysis of the paper, here are the most pertinent source papers that likely inspired or motivated this work, in JSON format with confidence scores:\n\n{\n  \"[b28]\": 0.9,\n  \"[b8]\": 0.8,\n  \"[b10]\": 0.8,\n  \"[b41]\": 0.7,\n  \"[b21]\": 0.6,\n  \"[b13]\": 0.5,\n  \"[b38]\": 0.4\n}\n\nThe key factors for selecting these papers are:\n\n1. The paper heavily relies on the T5 language model ([b28]) as the foundation of their PUnifiedNER system. This is the most direct inspiration.\n\n2. The ideas of continuously pre-training language models ([b8], [b21], [b10], [b41]) to adapt them for the downstream NER task are a core part of their methodology. \n\n3. The paper compares their unified NER approach to other recent unified NER frameworks like W2NER ([b13]) and a seq2seq method ([b38]), indicating these works motivated their design choices.\n\n4. The confidence scores are highest for the T5 paper and the language model adaptation papers, as these seem to be the most direct influences on the novel contributions of PUnifiedNER. The unified NER papers have lower scores as they are more points of comparison rather than direct inspiration.", "6391890790e50fcafd2b45f2": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b29]\": 0.9,\n  \"[b44]\": 0.8,\n  \"[b76]\": 0.8,\n  \"[b78]\": 0.7,\n  \"[b75]\": 0.6\n}\n\nConfidence scores:\n- [b29] (GraphSAGE, Hamilton et al.): 0.9. This paper is referenced multiple times and directly inspires the mini-batch neighborhood sampling approach used in the ByteGNN system. The GraphSAGE sampling configuration is also used as an example.\n\n- [b44] (Neighborhood caching): 0.8. Applying neighborhood caching to shorten sampling time is mentioned as an attempted optimization by existing systems that ByteGNN aims to improve upon. \n\n- [b76] (DistDGL): 0.8. DistDGL is discussed in detail as an existing distributed GNN training system that ByteGNN is compared against. DistDGL's fixed size prefetching and performance limitations motivate ByteGNN's design.\n\n- [b78] (GraphLearn/AliGraph): 0.7. GraphLearn is another major distributed GNN system that ByteGNN aims to outperform. Its Gremlin-based sampling approach and low CPU utilization are analyzed.\n\n- [b75] (AGL): 0.6. AGL is briefly mentioned in the context of how a suitable neighborhood sample size can approximate the full neighborhood aggregation. But it has less direct influence on ByteGNN compared to the other references.\n\nIn summary, the GraphSAGE paper [b29] and existing distributed GNN systems DistDGL [b76] and GraphLearn [b78] emerge as the most influential prior works that directly motivated the novel contributions of the ByteGNN system in terms of its mini-batch sampling optimizations and improved performance. The neighborhood caching idea from [b44] is also built upon.", "64893b17d68f896efa9826b7": "Here is my attempt at identifying the most pertinent source papers for this paper, in JSON format with confidence scores:\n\n{\n  \"[b2]\": 0.9,\n  \"[b23]\": 0.85,\n  \"[b9]\": 0.8,\n  \"[b17]\": 0.7,\n  \"[b7]\": 0.65,\n  \"[b15]\": 0.6\n}\n\nReasoning:\n\nThe paper heavily relies on and is directly inspired by the following key prior works:\n\n- GPT-3 [b2]: The paper uses GPT-3 as the backbone LLM and for in-context learning. GPT-3's ability to adopt correct references is a key insight leveraged.\n\n- WebGPT [b23]: The proposed WebGLM system aims to improve upon the WebGPT approach in terms of efficiency and cost-effectiveness. WebGPT's limitations are explicitly discussed.\n\n- Contriever [b9]: Used as the dense retriever in the two-stage retrieval process. The paper proposes distilling knowledge from GPT-3 to augment Contriever.\n\nTo a lesser extent, the paper also builds upon:\n\n- In-context learning optimization techniques [b17]\n- The idea of retrieval-augmented LMs from REALM [b7] and RAG [b15]\n\nThe novel contributions seem to be 1) the LLM-augmented two-stage retriever, 2) bootstrapping the generator with an LLM-generated dataset, and 3) training the scorer on online QA forum signals instead of expert feedback. The most relevant references directly enable or inspire these contributions.", "646aeca9d68f896efa05a572": "Here is my attempt at identifying the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b21]\": 0.8,\n  \"[b24]\": 0.9, \n  \"[b33]\": 0.7,\n  \"[b19]\": 0.6,\n  \"[b16]\": 0.6,\n  \"[b30]\": 0.8,\n  \"[b35]\": 0.8,\n  \"[b34]\": 0.8\n}\n\nReasoning:\n\nThe paper introduces a novel Three-hop Reasoning CoT (THOR) framework for implicit sentiment analysis (ISA). The key challenges and contributions are:\n\n1. ISA is much more challenging than explicit sentiment analysis (ESA) because the inputs contain only factual descriptions with no explicit opinion expressions. This challenge is outlined in [b24] and [b21], making these highly relevant references.\n\n2. The paper is inspired by fine-grained sentiment spirit from [b33] to mine implicit aspect and opinion states. This direct inspiration makes [b33] a pertinent reference. \n\n3. The paper leverages pretrained large-scale language models (LLMs) for their commonsense reasoning capabilities, citing [b19] and [b16]. While relevant, these are not the core novel contributions.\n\n4. The key novel contribution is the three-hop chain-of-thought (CoT) reasoning approach. [b30], [b35] and [b34] are cited as pioneering works on CoT that revealed LMs' multi-hop reasoning potential. The strong methodological similarity and direct influence make these highly pertinent references.\n\n5. The paper also introduces a self-consistency mechanism for CoT and a reasoning revising method for supervised fine-tuning. However, no specific references are cited for these aspects.\n\nIn summary, [b24], [b30], [b35], [b34] appear to be the most central and influential references, with [b21] and [b33] also being quite relevant. [b19] and [b16] provide important context but are less directly tied to the core contributions.", "64741c33d68f896efaa7b708": "Based on the provided text, I did not find any clear indications of specific source papers that heavily inspired or motivated this work. The paper does not seem to use key phrases like \"inspired by\", \"motivated by\", \"based on\", etc. to directly credit any prior research as the main inspiration.\n\nHowever, looking at the key challenges and novel methods introduced, a few references that seem most relevant to the paper's main contributions are:\n\n{\n  \"[b24]\": 0.8,\n  \"[b12]\": 0.7,\n  \"[b27]\": 0.7,\n  \"[b21]\": 0.6\n}\n\nThe rationale:\n\n- [b24] is cited when discussing how language models are \"trained in social isolation\", which the SANDBOX environment proposed in this paper aims to address. \n- [b12] and [b27] are referenced when explaining the concept and objective of \"social alignment\" that this work focuses on.\n- [b21] is mentioned in the context of the difficulty of creating a robust reward function for alignment, which motivates the Stable Alignment algorithm proposed here as an alternative to reward modeling.\n\nHowever, the connections are not definitive, as the paper does not explicitly state these works as direct inspiration. The confidence scores reflect this uncertainty. Overall, the paper seems to build more on broader concepts from prior work rather than heavily relying on specific source papers.", "629b0af15aee126c0fbc9a00": "Here is my attempt at identifying the most likely source papers for this paper based on the given criteria:\n\n{\n  \"[b86]\": 0.9,\n  \"[b100]\": 0.8,\n  \"[b105]\": 0.7,\n  \"[b106]\": 0.7,\n  \"[b107]\": 0.7,\n  \"[b108]\": 0.7,\n  \"[b109]\": 0.8,\n  \"[b110]\": 0.6,\n  \"[b111]\": 0.6\n}\n\nThe key points that led me to select these papers as the most likely sources are:\n\n1. The paper uses the human PPI network compiled by Menche et al. [b86] as a starting resource for the protein-protein interaction data in PrimeKG. This indicates [b86] was a key data source.\n\n2. For grouping disease concepts, the paper leverages BERT embeddings, specifically the ClinicalBERT model [b100]. This suggests the BERT paper [b100] provided important methodological inspiration.\n\n3. The use of word embeddings to resolve entities in an unsupervised way, as done for disease concepts, cites several papers [b105]-[b108] as prior work, indicating these papers influenced their approach. \n\n4. The string matching strategy used to initially group diseases references papers [b109]-[b111] as related work, suggesting these inspired their methodology.\n\n5. No other references are called out with phrases like \"inspired by\", \"based on\", \"following\", etc. to indicate heavy reliance on other specific papers for the core novel contributions.\n\nSo in summary, the PPI network from [b86] was a key data source, while the entity resolution methodology for diseases drew significant inspiration from the BERT model [b100] and prior work on using word embeddings for unsupervised entity linking [b105]-[b111]. The repeated citations of these papers when describing the novel disease grouping approach indicates they provided important motivation and methodological foundation for a core contribution of the paper.", "63e1c14790e50fcafd2dd585": "Here are the step-by-step thoughts to identify the most relevant source papers:\n\n1. Understand the novel contributions of the paper:\n- Using large language models (LLMs) as agent policies in interactive textual environments\n- Functionally grounding the LLM's knowledge through online reinforcement learning (RL) with incremental interactions in the environment\n- Studying sample efficiency, generalization to new objects and tasks, and impact of online RL vs offline behavioral cloning\n\n2. Look for key phrases indicating direct inspiration or motivation from previous work:\n- \"Building on recent works successfully using Reinforcement Learning (RL) to finetune LLMs for natural language generation tasks [b45] [b36] [b39], we propose...\"\n- \"We take inspiration from these works and also use PPO to finetune our LLMs.\"\n\n3. Identify references directly related to the novel methods:\n- Using LLMs as policies: [b1], [b52], [b14] \n- Finetuning LLMs on expert trajectories: [b49], [b41]\n- Finetuning LLMs with RL: [b45], [b36], [b39]\n\n4. Prioritize references preceded by phrases indicating direct inspiration:\n- [b45], [b36], [b39] are prioritized as the paper directly states building on and taking inspiration from these works to finetune LLMs with RL\n\n5. Select the most relevant references that directly influenced the main contributions:\n- [b45], [b36], [b39]: Finetuning LLMs with RL for language generation tasks, which directly inspired the paper's approach \n- [b49]: Showed finetuning LLMs on expert trajectories performed worse than RL, supporting the importance of interactive grounding\n- [b1], [b52], [b14]: Using LLMs as high-level planners in embodied tasks, hinting at leveraging LLM knowledge but limited by lack of grounding\n\nIn summary, the most relevant references that the paper builds upon are:\n- [b45], [b36], [b39]: Directly inspired finetuning LLMs with RL \n- [b49]: Motivated interactive grounding over offline finetuning\n- [b1], [b52], [b14]: Related to using LLM knowledge in embodied tasks\n\nThe final result in JSON format is:\n\n{\n  \"b45\": 0.95,\n  \"b36\": 0.95, \n  \"b39\": 0.95,\n  \"b49\": 0.8,\n  \"b1\": 0.7,\n  \"b52\": 0.7,\n  \"b14\": 0.7\n}", "64927546d68f896efa88a31b": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[5]\": 0.9,\n  \"[7]\": 0.85,\n  \"[9]\": 0.8,\n  \"[20]\": 0.7,\n  \"[40]\": 0.65\n}\n\nConfidence scores:\n[5], [7], [9]: These papers are directly mentioned as existing structure learning models that the proposed approach aims to improve upon in terms of generalization to unseen graphs. The phrases \"existing models limit their applicability within a closed-world hypothesis\" and \"To resolve the above dilemma, this paper attempts to explore a novel problem setting termed Open-World Graph Structure Learning\" indicate these papers serve as key motivation and inspiration.\n\n[20], [40]: These papers are cited when discussing the instantiation of the structure learner component of the proposed model. While relevant to the methodology, they seem to play a more minor role in inspiring the main contributions compared to [5], [7] and [9].\n\nThe other referenced papers provide useful background information, but do not appear to be as directly influential on the novel aspects of this paper based on the phrasing in the text. The key contributions center around enabling graph structure learning to generalize to new unseen graphs, going beyond the closed-world limitations of prior approaches [5], [7], [9].", "6482a38ed68f896efa8db612": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\nThe key novel contributions of this paper appear to be:\n1. A simple and efficient model (MUSICGEN) to generate high quality music at 32 kHz using a single-stage language model and efficient codebook interleaving strategy\n2. A single model that can perform both text and melody-conditioned music generation, with the generated audio being coherent with the provided melody and text\n3. Extensive objective and human evaluations of the key design choices\n\nThe paper introduces a novel \"codebook interleaving pattern\" framework to model the parallel discrete token sequences from the audio quantizer in an autoregressive manner. It explores and compares several interleaving patterns.\n\nKey methodological components seem to be:\n- Using an EnCodec audio tokenizer \n- Transformer decoder language model\n- Conditioning on text and/or melody\n- Codebook interleaving patterns to model parallel token sequences\n\nBased on this, the most directly relevant cited papers appear to be:\n\n{\n  \"[b6]\": 0.95,\n  \"[b12]\": 0.9,\n  \"[b7]\": 0.85, \n  \"[b9]\": 0.8,\n  \"[b11]\": 0.75\n}\n\n[b6] introduces the EnCodec audio tokenizer used as the basis, which quantizes audio into parallel token sequences. \n[b12] is the original Transformer paper, and a Transformer decoder is the core architecture used.\n[b7], [b9] and [b11] all propose strategies to model parallel token sequences in an autoregressive way, which directly inspires the codebook interleaving patterns introduced here as a generalization.\n\nThe confidence scores reflect that [b6] and [b12] are absolutely fundamental to the method, while [b7], [b9] and [b11] directly inspire the key novel codebook interleaving idea, but the paper generalizes beyond their specific approaches.", "6433f67f90e50fcafd6db326": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b11]\": 0.9,\n  \"[b34]\": 0.8,\n  \"[b32]\": 0.7,\n  \"[b1]\": 0.6\n}\n\nConfidence scores:\n- [b11]: 0.9 - The paper is directly inspired by this reference to re-examine IE tasks and unify them into token-pair classification tasks. This reference provides the core motivation for the paper's novel approach.\n\n- [b34]: 0.8 - The paper leverages the self-attention mechanism from this reference to conduct the plus-shaped interaction between token pairs in the proposed Plusformer module. This indicates a strong methodological similarity.\n\n- [b32]: 0.7 - The rotary position embedding from this reference is utilized in the Plusformer module to encode relative distances between token pairs. Adopting this technique shows the paper builds upon the ideas from this previous work.\n\n- [b1]: 0.6 - Layer normalization from this reference is used in the Plusformer layer, though the formulation is not shown for brevity. While not as central as the other references, using this technique still indicates some reliance on this previous work.\n\nThe paper's key novel contributions are the task decomposition of IE into token-pair classification, and the Plusformer module with plus-shaped self-attention to model token pair interactions. The selected references directly inspire or contribute to these main aspects. Other references like [b35], [b17] etc. provide background but are not as directly tied to the core methods proposed.", "64a29612d68f896efa28bcf5": "Here is my analysis of the source papers for the given paper, presented in JSON format with the reference number as the key and confidence score between 0 and 1 as the value:\n\n{\n  \"[b21]\": 1.0,\n  \"[b18]\": 0.9, \n  \"[b3]\": 0.9,\n  \"[b28]\": 0.9,\n  \"[b2]\": 0.7,\n  \"[b29]\": 0.7,\n  \"[b57]\": 0.8\n}\n\nThe key novel contribution of this paper is the Time-proportional Event Analysis (TEA) technique for creating accurate Per-Instruction Cycle Stacks (PICS). The paper is directly inspired and motivated by the time-proportional performance profiling work in [b21], which it builds upon. The other key source papers are:\n\n- [b18], [b3], [b28]: These represent the state-of-the-art instruction-driven performance analysis approaches that TEA is compared against. The paper's methodology and experiments are designed around demonstrating TEA's advantages over these prior techniques.\n\n- [b2], [b29]: These are referenced as examples of event-driven performance analysis approaches. Understanding their limitations helps motivate the need for TEA.\n\n- [b57]: This is the BOOM core that TEA is implemented and evaluated on. Aspects of the microarchitecture are important for the TEA design.\n\nIn summary, [b21] is the clearest direct inspiration, while the other key references provide important context in terms of the prior approaches TEA aims to improve upon. The detailed comparisons to [b18], [b3] and [b28] are central to demonstrating TEA's benefits. The BOOM core in [b57] is also important as the implementation substrate.", "646c3addd68f896efa5d1805": "Here is my attempt at identifying the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"[b21]\": 0.9,\n  \"[b1]\": 0.8,\n  \"[b62]\": 0.7,\n  \"[b14]\": 0.7,\n  \"[b33]\": 0.6,\n  \"[b7]\": 0.6,\n  \"[b45]\": 0.5\n}\n\nReasoning:\n- The paper is heavily inspired by and builds upon the idea of knowledge distillation originally proposed in [b21]. This is evident from phrases like \"Knowledge distillation [b21] aims to train a lightweight student model...\" and comparisons to conventional distillation methods. So [b21] is likely the most influential source.\n\n- The paper takes direct inspiration from the One-Image-Distill method [b1] for data-free distillation using data augmentation. It makes multiple comparisons to [b1] and positions itself as an improvement over that approach. \n\n- The paper frequently cites and compares to other data-free distillation methods that synthesize data using teacher statistics, like DeepInversion [b62], FastDFKD [b14], and the pioneering work by Lopes et al. [b33]. These likely served as key motivation.\n\n- The paper utilizes the DiT diffusion model [b45] as its main synthetic data generator. While important, [b45] seems more like a tool used rather than a conceptual inspiration.\n\n- Other data-free methods like [b7] are mentioned as related work but do not seem to be as central to the paper's main ideas.\n\nThe scores represent my confidence that each reference was a key inspiration or motivation for this paper's core contributions, based on the language used to discuss them in the text. Let me know if you would like me to explain or modify the scores further.", "635753d490e50fcafdddf4bd": "Here are the key steps I took to identify the most pertinent source papers for this paper:\n\n1. The primary novel contributions of the paper seem to be:\n- An end-to-end neural audio compression model called EnCodec that combines an encoder-decoder architecture, residual vector quantization, and adversarial losses\n- Demonstrating state-of-the-art results on speech and music compression at very low bitrates (1.5-24 kbps)\n- Extensive human evaluations comparing EnCodec to other codecs\n\n2. The paper does not use many explicit phrases like \"inspired by\", but it does cite and compare to several key previous works:\n- SoundStream [74] is cited as the most relevant prior work, as it also uses a convolutional encoder-decoder with residual vector quantization (RVQ) layers and adversarial losses\n- [67] and [57] are cited for the vector-quantized variational autoencoder (VQ-VAE) approach that inspired the quantization\n- [39], [37], [73] are cited as prior work on using multi-scale STFT discriminators as a perceptual loss\n\n3. The key novel aspects compared to prior work seem to be:\n- Combining the convolutional encoder-decoder, RVQ, and adversarial losses in a new architecture\n- Optimizing the model for very low bitrates and both speech and music \n- The extensive human evaluations\n\n4. Based on this analysis, the most pertinent source papers appear to be:\n{\"74\": 0.9, \"67\": 0.7, \"57\": 0.7, \"39\": 0.6, \"37\": 0.6, \"73\": 0.6}\n\nThe [74] SoundStream paper is clearly the biggest inspiration, while [67] and [57] provide key building blocks. The discriminator losses from [39], [37], [73] also seem quite relevant to the perceptual loss aspect. A few other papers like [64], [31], [32] are mentioned as prior neural audio codecs but don't seem as central to the key ideas.", "649a5e2ad68f896efad8460f": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\nThe key novel contributions of this paper are:\n1. Proposing the FuTex framework which utilizes the cross-paper network structure and in-paper hierarchy structure to improve weakly supervised multi-label classification of full-text scientific papers. \n2. Demonstrating the effectiveness of FuTex compared to competitive baselines on two large datasets with over 10,000 categories.\n\nThe paper does not contain many explicit phrases like \"inspired by\" or \"motivated by\" that directly point to source papers. However, based on the key methods introduced, the most relevant references seem to be:\n\n[b54] - The network-aware contrastive fine-tuning module follows the intuition of LinkBERT that connected papers are more likely to share fine-grained topics. This reference directly inspires a core part of the FuTex framework.\n\n[b10] - The hierarchy-aware aggregation module is inspired by FullMeSH's idea of checking different sections of the full paper to probe relevant topics. This reference provides key motivation for utilizing the in-paper hierarchy.\n\n[b68] - MICoL is referenced multiple times as prior work on weakly supervised multi-label classification that FuTex builds upon and compares against. Specific techniques like using meta-path based neighborhoods and the contrastive fine-tuning objective are adopted from MICoL.\n\n[b39] - Parabel is chosen as the classifier for the self-training module to utilize paper full text while avoiding the sequence length limitation. This reference provides the key model for that component.\n\nIn summary, while not always explicitly stated, references [b54], [b10], [b68] and [b39] emerge as the most central, directly inspiring the novel FuTex framework and its modules. The other references provide important context but are less pivotal to the core contributions.\n\n{\n  \"[b54]\": 0.9,\n  \"[b10]\": 0.8,\n  \"[b68]\": 0.9,\n  \"[b39]\": 0.8\n}", "6225978c5aee126c0f2d4a7c": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b0]\": 0.9,\n  \"[b64]\": 0.9,\n  \"[b60]\": 0.8,\n  \"[b7]\": 0.7,\n  \"[b47]\": 0.6,\n  \"[b48]\": 0.6\n}\n\nConfidence scores:\n- [b0] and [b64]: 0.9. These two papers describe the Loca reuse distance analysis tool which the current paper compares against and aims to improve upon. The current paper highlights Loca's high runtime and memory overheads as a key motivation.\n\n- [b60]: 0.8. This paper describes the RDX tool which is another open-source reuse distance analyzer that the current paper compares against. Improving upon the limitations of RDX is a driving factor.\n\n- [b7]: 0.7. The StatCache technique in this paper profiles reuse distance but only focuses on single-threaded applications, a shortcoming the current paper aims to address by analyzing multi-threaded codes. \n\n- [b47] and [b48]: 0.6. These papers rely on hardware simulation or binary instrumentation for reuse distance analysis, approaches that incur high overheads which motivates the current paper's PMU-based sampling technique for better performance.\n\nIn summary, the papers describing the Loca and RDX tools that the current work directly compares against and aims to improve upon in terms of overhead and multi-threaded support, emerge as the most influential source papers based on the phrasing and comparisons made in the text. The other papers provide important context on prior reuse distance analysis techniques and their limitations that this paper seeks to overcome.", "6427029c90e50fcafd5d6c03": "Here are the key steps to identify the source papers based on the given text:\n\n1. Main novel contributions of the paper:\n- Introducing Pure Miss Contribution (PMC) as a new metric to quantify the cost and performance impact of outstanding cache misses, considering both hit-miss and miss-miss overlapping\n- Presenting CARE, a concurrency-aware cache management framework that considers both locality and concurrency, augmenting existing cache policies\n- Implementing Dynamic Threshold Reconfiguration Mechanism (DTRM) in CARE to adapt to different applications and phases\n\n2. Key phrases indicating strong reliance on previous work:\nThe paper does not seem to use phrases like \"inspired by\", \"motivated by\" etc. to explicitly credit specific previous works as the main inspiration.\n\n3. References directly linked to the main contributions and methods:\n- [b43]: Introduces the C-AMAT concurrent memory access model which PMC builds upon\n- [b27], [b28], [b29], [b30]: Discuss the concept of pure miss cycles which is used to define PMC \n- [b18], [b47], [b52]: SRRIP, DRRIP, SHiP and SHiP++ which are locality-based cache management schemes that CARE aims to improve upon\n- [b33]: Proposes SBAR, an MLP-aware cache replacement policy, whose limitations motivate the PMC metric in this paper\n\n4. Inferred most relevant references based on paper titles:\n- [b43]: \"C-AMAT: A Precise Model for Concurrent Memory Access Analysis\" \n- [b33]: \"A Case for MLP-Aware Cache Replacement\"\n\nIn summary, the references [b43] and [b33] seem to be the most pertinent source papers that directly influenced the key contributions of this paper - the C-AMAT model from [b43] forms the conceptual basis for PMC, while the MLP-aware replacement policy in [b33] is what PMC and CARE aim to improve upon by considering both hit-miss and miss-miss overlapping.\n\n{\n  \"[b43]\": 0.9,\n  \"[b33]\": 0.8,\n  \"[b27]\": 0.6,\n  \"[b28]\": 0.6,\n  \"[b29]\": 0.6,\n  \"[b30]\": 0.6,\n  \"[b18]\": 0.5,\n  \"[b47]\": 0.5\n}", "629435a25aee126c0f2fecb3": "Here is my attempt at identifying the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b48]\": 0.9,\n  \"[b0]\": 0.8,\n  \"[b62]\": 0.7,\n  \"[b8]\": 0.6,\n  \"[b31]\": 0.6\n}\n\nRationale and key observations:\n\n- The paper introduces FlashAttention, a new attention algorithm that computes exact attention with far fewer memory accesses by avoiding reading/writing the large attention matrix to slow GPU memory (HBM). This seems to be the key novel contribution.\n\n- To achieve this, they apply two main techniques: \n1) Tiling - computing attention in blocks to incrementally perform the softmax without accessing the full input (seems inspired by [b48] on scaling softmax, a top candidate)\n2) Recomputation - recomputing attention on-chip in the backward pass from stored statistics to avoid storing the large intermediate attention matrix (seems motivated by gradient checkpointing ideas in [b8] and [b31])\n\n- The paper repeatedly emphasizes the importance of making the attention algorithm \"IO-aware\" ([b0]) by carefully managing reads/writes between fast on-chip SRAM and slower HBM. This IO-awareness principle seems to be a key motivation.\n\n- For numerical stability when computing softmax in blocks, they use a softmax scaling decomposition technique that appears to build upon [b62].\n\n- While many other references are cited, the ones above seem most directly linked to the core FlashAttention algorithm and its novel contributions around IO-aware exact attention with tiling and recomputation. The confidence scores reflect the centrality of each reference to the main ideas.", "64a78f10d68f896efa01dee3": "Here are the key source papers that likely inspired this work, based on the criteria specified:\n\n{\n  \"[b0]\": 0.9,\n  \"[b3]\": 0.8,\n  \"[b4]\": 0.7,\n  \"[b13]\": 0.6,\n  \"[b25]\": 0.5\n}\n\nRationale:\n\nThe paper frequently cites [b0] when discussing the performance limitations of GPUs for LLMs, indicating it was a key motivation. For example: \"State-of-the-art GPT-3 throughput on GPU is 18 tokens/sec per A100 [b0].\"\n\n[b3] and [b4] are cited when introducing GPT-3 and PaLM, the specific large language models the paper is aiming to optimize hardware for. This suggests those papers were influential in defining the target workloads.\n\nThe NRE cost model used in the paper's analysis is directly attributed to [b13]: \"We extend the NRE model from Moonwalk [b13] to use a 7nm technology node...\"\n\n[b25] is referenced multiple times when discussing performance characteristics of TPUs, implying it provided important context for the authors' work.\n\nThe key novel contributions seem to be the chiplet-based ASIC architecture and the hardware-software co-design methodology. The most relevant references to those aspects have been prioritized. Other references that provide more general background on LLMs and hardware costs are given lower confidence scores.", "6346305e90e50fcafda07ab8": "Here are the key source papers that likely inspired this work, based on my analysis of the text:\n\n{\n  \"[b49]\": 0.9,\n  \"[b20]\": 0.8,\n  \"[b17]\": 0.7,\n  \"[b21]\": 0.6,\n  \"[b59]\": 0.5,\n  \"[b61]\": 0.5\n}\n\nRationale:\n- [b49] is directly cited as inspiring the hard attention idea used for the task masks, a core novel contribution. The high confidence score reflects this direct inspiration.\n- [b20] introduces the adapter architecture that the proposed CL-plugin builds upon and modifies. This methodological similarity warrants a high score.\n- [b17] is cited as the source of the parallel adapter placement used instead of the sequential placement from [b21]. These two references provide key architectural choices.\n- [b59] and [b61] are referenced multiple times in relation to the datasets and evaluation setup, indicating they influenced important aspects of the experimental design, though perhaps less central to the core algorithmic contributions, hence the moderate score.\n\nThe paper's main novel contributions appear to be:\n1) Continual post-training of language models for sequential domains \n2) The CL-plugin architecture with task masks to prevent forgetting\n3) Identifying and addressing the \"catastrophic butterfly effect\" in fine-tuning\n\nThe selected references either directly inspired these contributions ([b49]), or provided important methodological components that were adapted and built upon ([b17], [b20], [b21]). The experimental setup also took cues from [b59] and [b61].", "6327dda690e50fcafd67dea3": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria provided:\n\nThe key novel contributions of this paper appear to be:\n\n1. Constructing a Twitter Heterogeneous Information Network (TwHIN) to unify multi-typed user engagement logs and mine socially similar tweet pairs\n2. Introducing a contrastive social objective in conjunction with masked language modeling to enforce the model to tell if tweet pairs are socially similar \n3. Building the first socially-enriched pre-trained language model for noisy user-generated text on Twitter\n\nBased on this, the most relevant references seem to be:\n\n{\n  \"[b15]\": 0.9,\n  \"[b41]\": 0.8,\n  \"[b4]\": 0.7,\n  \"[b18]\": 0.7,\n  \"[b17]\": 0.6,\n  \"[b16]\": 0.6,\n  \"[b7]\": 0.8\n}\n\nThe key reasons are:\n\n- [b15] and [b41] directly relate to the methodology of constructing and embedding the Twitter Heterogeneous Information Network, which is a core novel aspect\n- [b4], [b18], [b17] and [b16] are referenced when discussing the specific techniques used to efficiently embed and search the large-scale TwHIN \n- [b7] introduces the NT-Xent contrastive loss which is adapted into the novel social contrastive objective\n\nThe other references like [b11], [b10], [b38] provide important background context on language models and transformers in general, but do not seem to be as directly linked to the specific novel contributions of this paper based on the phrasing.", "6260bd7f5aee126c0fc6bba4": "Based on analyzing the text, here are the key source papers that likely inspired or motivated this work, along with confidence scores:\n\n{\n  \"[b6]\": 0.9,\n  \"[b44]\": 0.85,\n  \"[b27]\": 0.85,\n  \"[b36]\": 0.8,\n  \"[b25]\": 0.75\n}\n\nThe key reasons are:\n\n- The paper cites [b6], [b44] and [b27] when discussing how to achieve equivariance by constructing a local spherical coordinate system. This indicates these prior works directly inspired their approach for equivariant molecule generation.\n\n- [b36] and [b25] are discussed extensively as the most closely related prior works on machine learning for structure-based drug design. The authors highlight how their approach improves upon the limitations of [b36] and [b25], suggesting these papers provided key motivation.\n\n- The ablation studies directly compare to the approach in [b27], further confirming it as a major influence. \n\n- While other papers like [b41] and [b17] are cited when describing the GNN architecture, the connection seems less direct in terms of inspiring the core novel contributions of this paper compared to [b6], [b44], [b27], [b36] and [b25].\n\nSo in summary, [b6], [b44], [b27], [b36] and [b25] emerge as the most likely candidates for source papers that inspired this work, based on how they are positioned and discussed in the text in relation to the key ideas and contributions.", "62b2888c5aee126c0fbc731c": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b29]\": 0.9,\n  \"[b24]\": 0.8,\n  \"[b60]\": 0.7,\n  \"[b63]\": 0.7,\n  \"[b58]\": 0.6,\n  \"[b37]\": 0.6\n}\n\nConfidence scores:\n- [b29] (score 0.9): The paper builds upon the variational autoencoder (VAE) framework proposed in this pioneering work. The VAE is a core component of the LIMO approach.\n\n- [b24] (score 0.8): Junction tree VAEs proposed in this work are discussed as a key prior method. The paper compares against JT-VAE as a baseline and aims to improve upon its ability to generate molecules with desired properties.\n\n- [b60], [b63] (score 0.7): These reinforcement learning based methods GCPN and MolDQN are the state-of-the-art baselines that the LIMO approach aims to match in performance while being much faster. The methodological comparisons to these papers are central to the paper's contributions.\n\n- [b58], [b37] (score 0.6): The MARS method from [b58] and the GraphDF method from [b37] are two other important baselines compared against for the molecule optimization tasks, though the comparisons to these are slightly less central than those to the RL methods.\n\nThe key phrases \"builds on the variational autoencoder (VAE) framework\", \"contrary to previous work\", and comparisons to \"[b24]\", \"[b60]\", \"[b63]\", \"[b58]\", \"[b37]\" as baselines indicate these are the most influential prior works that inspired and motivated this paper's methodology. The novel aspects like using a decoder and predictor network in sequence and optimizing properties in the decoded molecular space are proposed to improve upon limitations of these previous approaches.", "6385788690e50fcafdf4a0f3": "Here are the most relevant source papers for this paper, based on the criteria provided:\n\n{\n  \"[b10]\": 0.9,\n  \"[b11]\": 0.95,\n  \"[b12]\": 0.8,\n  \"[b17]\": 0.7,\n  \"[b18]\": 0.7,\n  \"[b19]\": 0.8\n}\n\nThe key points that led to identifying these source papers are:\n\n1. The paper directly states it is extending the untrained subnetwork techniques from [b10] and [b11] to GNNs for the first time. This indicates [b10] and [b11] were major inspirations.\n\n2. It cites [b12] as another prior work that successfully finds untrained subnetworks in CNNs, further building on [b10] and [b11]. \n\n3. The proposed UGTs method leverages insights from the gradual sparsification technique in [b17] and [b18].\n\n4. A key aspect of UGTs is removing weights across layers based on their importance scores, inspired by the global sparse variant in [b19] which leads to better layer-wise sparsity ratios.\n\n5. The core novel contribution is extending the untrained subnetwork paradigm to GNNs for the first time and showing it mitigates issues like over-smoothing in deep GNNs. The cited papers [b10-b12, b17-b19] directly enable this main contribution.\n\nSo in summary, [b10], [b11] and [b12] are the pioneering works that inspired applying untrained subnetworks to GNNs, while [b17], [b18] and [b19] provided key techniques leveraged in the UGTs method for discovering matching untrained subnetworks in GNNs. The paper directly builds upon the ideas and approaches from these previous works.", "6466fafbd68f896efaeb7607": "Here is my analysis of the key source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b5]\": 0.9,\n  \"[b3]\": 0.7,\n  \"[b35]\": 0.7,\n  \"[b9]\": 0.8,\n  \"[b2020]\": 0.8\n}\n\nThe key points that led me to these source papers are:\n\n- The paper is heavily inspired by the BLIP-2 model [[b5]] in several key aspects - using a query mechanism to align visual and textual features, and employing a frozen pre-trained vision encoder. This is the most direct inspiration cited.\n\n- The paper also draws on the general vision-language pre-training (VLP) approaches pioneered in papers like [[b3]] and [[b35]] to learn multi-modal representations by aligning image and text features into a common space. \n\n- For the language model component, the paper chooses to use the BioMedLM model [[b9]] which is a biomedical domain-specific language model. Fine-tuning this using the LoRA technique from [[b2020]] is a key part of their lightweight architecture.\n\n- The novel contributions seem to be 1) Extending VLP ideas to 3D medical images and biomedical text, and 2) The specific MedQFormer architecture to extract 3D visual features and align them with text features. The key references that enable these contributions are [b5], [b9] and [b2020].\n\nSo in summary, [b5] is the most direct inspiration, while [b3], [b35], [b9] and [b2020] provide key components and techniques that the proposed MedBLIP model builds upon to make its novel contributions in the biomedical domain. The confidence scores reflect the centrality of each reference to the main ideas.", "622577a75aee126c0f008d4a": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b3]\": 0.9,\n  \"[b5]\": 0.9,\n  \"[b13]\": 0.9,\n  \"[b4]\": 0.95,\n  \"[b8]\": 0.95\n}\n\nThe key rationales are:\n\n1. The paper's novel contributions center around proposing a Data-efficient Interpretable Anomaly Detection (DIAD) approach. The model architecture is directly inspired by Generalized Additive Models (GAMs) as indicated by this key phrase:\n\"Our model architecture is inspired by Generalized Additive Models (GAMs) that have been shown to obtain high accuracy and interpretability for tabular data [b3] [b5] [b13]\"\n\n2. The paper employs intuitive notions of Partial Identification (PID) as the anomaly detection objective, and proposes to learn them with differentiable GAMs. This is a core novel aspect that references [b8] directly:\n\"We propose to employ intuitive notions of Partial Identification (PID) as AD objective and propose to learn them with differentiable GAMs.\"\n\n3. The specific differentiable GAM architecture used, called NodeGA2M, is the key enabler of the proposed DIAD approach. The paper indicates direct inspiration from NodeGAM for this:\n\"our work is inspired from the NodeGAM [b4], an interpretable and differentiable tree-based GAM model.\"\n\nIn summary, references [b3], [b5], [b13] inspire the use of GAMs for interpretable anomaly detection on tabular data, [b8] directly motivates the PID-based anomaly detection objective, and [b4] provides the specific differentiable GAM architecture that enables the proposed DIAD approach. These references are most central to the main novel contributions of interpretable architecture, PID objective, and differentiable GAMs for semi-supervised anomaly detection.", "64741c33d68f896efaa7b7a1": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b126]\": 0.9,\n  \"[b30]\": 0.8,\n  \"[b56]\": 0.7,\n  \"[b18]\": 0.7,\n  \"[b122]\": 0.6,\n  \"[b34]\": 0.6,\n  \"[b12]\": 0.5\n}\n\nConfidence scores and rationale:\n\n[b126] (OFA) - 0.9: The paper is heavily inspired by the OFA architecture in designing BiomedGPT as a unified encoder-decoder model that can handle diverse modalities and tasks. This is the most direct and significant inspiration.\n\n[b30] (ViT) - 0.8: The visual input processing of BiomedGPT uses ideas from the Vision Transformer (ViT) by discretizing images into patches. ViT is regarded as a key backbone for biomedical image analysis tasks.\n\n[b56], [b18] - 0.7: These papers inspire the approach of directly applying CNN backbones for visual feature extraction [b56] and using object descriptors to discretize object detections [b18]. \n\n[b122], [b34] (VQ-GAN) - 0.6: The frozen image quantization technique used to represent image outputs as a unified vocabulary is based on VQ-GAN from these papers.\n\n[b12] (GPT-3) - 0.5: The use of natural language instructions to specify tasks, eliminating task-specific modules, is motivated by the prompting techniques introduced for large language models like GPT-3.\n\nThe other referenced papers, while relevant to the general context and background, do not seem to directly inspire the core novel contributions and methods of BiomedGPT based on the phrasing in the text. The key novel aspects appear to be the unified encoder-decoder architecture for multimodal biomedical data, the input/output unification techniques, and the use of natural language instructions for multitask learning.", "63e312f590e50fcafdc1a1bb": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b7]\": 0.9,\n  \"[b1]\": 0.8,\n  \"[b22]\": 0.7,\n  \"[b2]\": 0.9,\n  \"[b6]\": 0.9,\n  \"[b25]\": 0.6,\n  \"[b14]\": 0.6,\n  \"[b15]\": 0.5\n}\n\nThe key novel contributions of this paper appear to be:\n\n1. Formulating the k-partitioning problem in terms of finding an indicator matrix \u03a0 that makes XQ close to \u03a0, where X contains the top k eigenvectors of the normalized adjacency matrix. This leads to the optimization problem in equation (14).\n\n2. Proposing an alternating algorithm (Algorithm 3) to approximately solve the optimization problem in (14) and find the indicator matrix \u03a0. \n\n3. Defining new cut functions \u03a8cut in (11) and \u03a6cut in (22) for evaluating the quality of a k-partitioning, and proving Cheeger-type inequalities relating these to the eigenvalues.\n\nThe source papers that most directly inspired and enabled these contributions are:\n\n[b7] - Fiedler's pioneering work on spectral partitioning and algebraic connectivity for the k=2 case. This heavily motivated the generalization to k>2.\n\n[b1], [b22] - These survey papers lay out the mathematical foundations of spectral partitioning that the current paper builds upon. \n\n[b2], [b6] - The current paper's alternating optimization approach (Algorithm 3) to find the indicator matrix \u03a0 seems directly inspired by the algorithms in these papers, especially [b6].\n\n[b25], [b14], [b15] - While less central, these papers also propose related optimization approaches for the k-partitioning problem that likely provided some inspiration.\n\nThe other references, while relevant for background, do not appear to be as pivotal to the paper's main novel contributions. The key phrases indicating direct inspiration, like \"inspired by\" or \"motivated by\", occur with the above references.\n\nIn summary, references [b7], [b1], [b22], [b2] and [b6] emerge as the most influential source papers that this work aims to generalize and build upon to enable spectral k-partitioning for k>2. The proposed cut functions and algorithms are the main novel elements.", "6487e9fad68f896efa482c3a": "Here is my analysis of the key source papers for this paper, based on the criteria you provided:\n\n{\n  \"[b24]\": 0.9,\n  \"[b2]\": 0.85,\n  \"[b4]\": 0.6,\n  \"[b26]\": 0.6,\n  \"[b8]\": 0.5\n}\n\nRationale:\n- The paper introduces Adaptive Graph Sparsification (AGS), a novel algorithm for sparsifying the adjacency matrices in Adaptive Spatial-Temporal Graph Neural Networks (ASTGNNs). The key inspiration seems to come from Graph WaveNet [b24], which \"proposes an AGCN layer to learn a normalized adaptive adjacency matrix without a pre-defined graph.\" This is the core foundation that AGS builds upon.\n\n- AGCRN [b2] is highlighted as a representative ASTGNN architecture that AGS is applied to. The paper notes \"Modelling the spatial network with NAPL-AGCN achieves state-of-the-art performances on multiple benchmarks, and it has been widely adopted in diverse ASTGNN variants [b2][b5][b6][b7].\" Improving the efficiency of AGCRN seems to be a key motivation.\n\n- [b4] and [b26] are referenced as prior works on graph sparsification for vanilla GNNs. The authors contrast AGS with these methods.\n\n- The hard concrete distribution from [b8] is used as a differentiable approximation to the non-differentiable L0 sparsity penalty in the AGS objective function. This enables end-to-end training.\n\nThe other referenced papers provide useful context but do not seem to be as directly influential on the core novel contributions as the ones highlighted above. The emphasis is on improving the efficiency of state-of-the-art ASTGNNs like AGCRN by sparsifying their learned adjacency matrices, building on ideas from Graph WaveNet.", "63fec3ce90e50fcafdd70610": "Based on the analysis of the paper, here are the most pertinent source papers that likely inspired or motivated this research, in JSON format with confidence scores:\n\n{\n  \"[b8]\": 0.9,\n  \"[b9]\": 0.85,\n  \"[b10]\": 0.8,\n  \"[b11]\": 0.8,\n  \"[b1]\": 0.7,\n  \"[b2]\": 0.65,\n  \"[b6]\": 0.6,\n  \"[b7]\": 0.6\n}\n\nThe key factors for selecting these papers are:\n\n1. The paper heavily focuses on the potential of ChatGPT in enabling human-bot collaborative software architecting. References [b8]-[b11] which introduce ChatGPT and explore its applications in software engineering are very likely to be the direct inspiration for this research idea. \n\n2. The architecting process and activities (analysis, synthesis, evaluation) adopted in this paper's methodology closely follow the process in [b1] and [b2]. This indicates the architecting approach itself is motivated by those previous works.\n\n3. When discussing the research context, the paper mentions that the current role of AI/bots in software architecting is limited to providing recommendations and answering questions, as shown in [b6] and [b7]. This helps highlight the research gap that this paper aims to address.\n\n4. On the other hand, the references like [b0], [b3]-[b5], [b12], [b13] provide more general background information on software architecture and AI in software engineering. They do not seem to directly inspire the core novel contributions of this paper.\n\nSo in summary, the papers specifically introducing ChatGPT and its potential, combined with the papers detailing the baseline architecting process, appear to be the most influential in motivating this research on ChatGPT-enabled collaborative architecting.", "63aa623e90e50fcafd978bc2": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b13]\": 0.9,\n  \"[b90]\": 0.8,\n  \"[b11]\": 0.7,\n  \"[b78]\": 0.7,\n  \"[b49]\": 0.6\n}\n\nThe key points that led me to identify these as the most relevant source papers are:\n\n1. The paper introduces Med-PaLM, which is an instruction prompt-tuned version of Flan-PaLM specialized for the medical domain. Flan-PaLM itself is based on the PaLM model from [b13], making this a highly relevant source paper that the current work builds upon.\n\n2. The paper uses chain-of-thought prompting, a technique that was pioneered in [b90] to improve reasoning capabilities of language models. This indicates [b90] served as important motivation and inspiration.\n\n3. The in-context few-shot learning capabilities that allow rapid adaptation of the model to new tasks without gradient updates is a key property of large language models that was demonstrated in the GPT-3 paper [b11]. \n\n4. The paper [b78] introduced a large language model called Galactica for scientific knowledge. The current paper notes this as closely related prior work in applying LLMs to specialized domains.\n\n5. The study [b49] is mentioned as the closest precedent, as it also looked at the reasoning capability of instruction-tuned LLMs for medical question answering. The current paper is motivated to expand on this.\n\nIn summary, the identified source papers provided the key conceptual and methodological foundations that the current work builds upon, refines and extends to the medical domain through the development of Med-PaLM. The high confidence scores reflect the centrality of these papers in inspiring and enabling the main contributions.", "63f2e4ae90e50fcafd283025": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b15]\": 0.9,\n  \"[b38]\": 0.8,\n  \"[b46]\": 0.7,\n  \"[b11]\": 0.6,\n  \"[b51]\": 0.5\n}\n\nRationale:\n\nThe paper introduces MiDi, a model that simultaneously generates molecular graphs and 3D conformations using diffusion. The key novel contributions are:\n\n1. Using both Gaussian diffusion for 3D coordinates and discrete diffusion for graph components in an end-to-end differentiable model. This allows generating stable molecular graphs consistent with 3D conformers.\n\n2. Introducing an adaptive noise schedule that adds noise to atom types/charges faster than coordinates/bonds. This encourages first generating realistic 3D conformations before refining atom details.\n\n3. Proposing the rEGNN layer that improves on EGNN by leveraging non-translation-invariant features while maintaining SE(3) equivariance.\n\nThe most directly relevant references based on the key phrases and novel aspects are:\n\n[b15] (score 0.9): The Equivariant Diffusion Model (EDM) which MiDi improves upon by jointly generating the graph. EDM is discussed extensively as the main baseline.\n\n[b38] (score 0.8): Introduces the EGNN layers that MiDi builds upon with the novel rEGNN layers. Highly relevant to the methodological contributions.\n\n[b46] (score 0.7): Finds that discrete diffusion is effective for graph generation, which directly motivates MiDi's use of discrete diffusion for the graph components.\n\n[b11] (score 0.6): Also finds discrete diffusion effective for graphs, further supporting MiDi's graph diffusion approach. \n\n[b51] (score 0.5): Discusses requirements for SE(3) equivariance in diffusion models that MiDi leverages, like using CoM Gaussian noise.\n\nThe other references, while topically relevant, do not seem to directly inspire or enable the main novel contributions of MiDi based on the phrasing. The early references on historical context were excluded.", "64be53113fda6d7f06326238": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"[b39]\": 0.9,\n  \"[b36]\": 0.9,\n  \"[b7]\": 0.9,\n  \"[b27]\": 0.6,\n  \"[b35]\": 0.6,\n  \"[b6]\": 0.6,\n  \"[b17]\": 0.6,\n  \"[b42]\": 0.8\n}\n\nConfidence scores:\n- [b39], [b36], [b7]: 0.9. These papers are cited when discussing the dual process model of cognitive reasoning, which directly inspires the paper's approach of modeling the full-stack reasoning process in two stages. The phrases \"Dual process models theorize that...\" and \"Within the context of Question Answering (QA), the first stage...\" indicate the paper heavily relies on these references for its core contributions.\n\n- [b27], [b35], [b6], [b17]: 0.6. These papers propose various reading comprehension datasets like SQuAD, NewsQA, DROP and Natural Questions. While relevant for background, the paper argues these datasets mostly contain factoid questions, in contrast to the more advanced questions requiring reasoning that the paper focuses on. So they are somewhat relevant but not as central.\n\n- [b42]: 0.8. This paper about discourse structure of answers to complex questions is referenced when discussing the evidential rationale generation subtask. The description of rationale types as \"(1) main answer (2) elaboration (3) auxiliary information\" is directly attributed to [b42], indicating it provides important background for one of the paper's main subtasks.\n\nIn summary, [b39], [b36], [b7] and [b42] appear to be the most influential source papers, as they directly motivate and inform the paper's key contributions around modeling cognitive reasoning and generating evidential rationales. The other papers provide relevant background but are less central to the novel aspects.", "64a63bddd68f896efaec64af": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b18]\": 0.9,\n  \"[b16]\": 0.85,\n  \"[b35]\": 0.8,\n  \"[b11]\": 0.75\n}\n\nConfidence scores:\n[b18], [b16]: These two papers propose the concept of Soft Prompts, which is a key novel contribution that the CSProm-KG model builds upon. The paper states \"Motivated by the observation that Soft Prompts in a frozen PLM is effective in solving the over-fitting issue [b35], we apply Soft Prompts in CSProm-KG to avoid the KGC models overly focusing on the textual information.\" This indicates heavy reliance and inspiration from [b18] and [b16].\n\n[b35]: This paper demonstrates that Soft Prompts can help avoid over-fitting in frozen PLMs, which directly motivates CSProm-KG's use of Soft Prompts. \n\n[b11]: The Local Adversarial Regularization technique proposed in CSProm-KG to distinguish textually similar entities is directly inspired by [b11], as stated: \"To enhance CSProm-KG's ability to distinguish textually similar entities, inspired by [b11]...\"\n\nThe other referenced papers, while relevant for background and context, do not appear to be as centrally tied to the main novel contributions of CSProm-KG based on the phrasing in the text. The key aspects - conditional soft prompts to fuse structural and textual information, and local adversarial regularization - draw clear and direct inspiration from the 4 papers listed above.", "64cc77b33fda6d7f06aebd0d": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\nThe key novel contributions of this paper appear to be:\n1. Constructing MusicLDM, a state-of-the-art text-to-music generation model adapted from Stable Diffusion and AudioLDM architectures \n2. Proposing two novel mixup strategies for music generation - beat-synchronous audio mixup (BAM) and beat-synchronous latent mixup (BLM) - to address limited training data and encourage novelty\n3. Designing new metrics using a pretrained CLAP encoder to test for plagiarism and novelty in the generated music\n\nThe paper mentions being directly inspired by or building upon the following references in developing its main contributions:\n\n- Stable Diffusion [b30]: MusicLDM adapts the Stable Diffusion architecture for text-to-music\n- AudioLDM [b23]: MusicLDM also adapts the AudioLDM architecture and is based on the open-source AudioLDM codebase\n- Mixup [b40]: The proposed BAM and BLM strategies adapt the mixup data augmentation technique to the music domain\n\nA few other references are mentioned as being methodologically similar:\n- CLAP [b38]: A pretrained text-audio encoder used in MusicLDM and for the plagiarism/novelty metrics\n- HiFi-GAN [b18]: A neural vocoder used in MusicLDM\n\n{\"b30\": 0.95, \"b23\": 0.95, \"b40\": 0.9, \"b38\": 0.7, \"b18\": 0.7}\n\nThe [b30] Stable Diffusion and [b23] AudioLDM references have the highest relevance scores, as the core MusicLDM architecture directly builds upon them. The [b40] mixup reference is also highly relevant, as the paper's BAM and BLM techniques are novel adaptations of mixup to music generation. [b38] CLAP and [b18] HiFi-GAN have moderate relevance scores, as components used in MusicLDM but not as central to the paper's main contributions.", "64a29621d68f896efa28fd65": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b27]\": 0.9,\n  \"[b52]\": 0.9,\n  \"[b22]\": 0.8,\n  \"[b30]\": 0.7,\n  \"[b9]\": 0.7,\n  \"[b49]\": 0.7,\n  \"[b50]\": 0.7\n}\n\nRationale and confidence scores:\n\nThe paper heavily relies on and is directly motivated by the techniques of using lightweight coroutines to hide memory access latency, as evidenced by this key phrase: \"there have been recent works that interleave coroutine executions to hide memory accesses for pointer-based data structures in databases [b22] [b27] [b52]\". References [b27] and [b52] are given the highest confidence scores of 0.9 since the paper directly builds upon their core idea. [b22] is also highly relevant with a score of 0.8.\n\nThe paper aims to address the limitations of prior work by leveraging sample-based profiling to guide the instrumentation of coroutine yields. References [b9], [b49] and [b50] directly relate to this key contribution, as indicated by phrases like \"Sample-based profiling leverages hardware performance counters available in modern CPUs to sample hardware events of interest in production with negligible overhead. The profiled information is then used to guide instrumentation of coroutines\". They are assigned confidence scores of 0.7.\n\n[b30] is relevant as it provides motivation for the work by quantifying the significant CPU stalls caused by memory-bound events in modern applications. It gets a score of 0.7.\n\nThe other references, while providing useful background information, do not seem to be as centrally tied to the main novel contributions of using lightweight coroutines together with sample-based profiling to efficiently hide short events in software. The key inspiration comes from combining the techniques of [b27], [b52], [b22] with [b9], [b49], [b50].", "63969ba790e50fcafdcf1d53": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b42]\": 0.9,\n  \"[b26]\": 0.8,\n  \"[b7]\": 0.8,\n  \"[b55]\": 0.8,\n  \"[b37]\": 0.7,\n  \"[b35]\": 0.6\n}\n\nThe key novel contributions of this paper are:\n1) Upcycling an existing dense model into a larger sparse Mixture-of-Experts (MoE) model with a relatively small additional compute budget \n2) A model surgery recipe for overcoming the initial performance drop when changing a trained network's structure to an MoE\n\nThe methodology heavily relies on previous work on sparsely activated MoEs. In particular:\n\n- The paper uses Expert Choice routing, directly citing \"[b55]\" as the source. \n- It also references other MoE routing approaches like Top-K \"[b42]\", BASE layers \"[b26]\", and Sinkhorn-BASE layers \"[b7]\", indicating these papers provided important context and alternatives.\n- The resultant upcycled vision models follow the \"V-MoE\" architecture from \"[b37]\" with a couple modifications.\n- For comparison to upcycling to sparse models, they also try \"dense upcycling\" by replicating layers using \"tiling\" patterns from \"[b35]\".\n\nWhile many other papers are cited for background like the model architectures (ViT, T5) and training setup, the core novel aspects of upcycling dense\u2192sparse models with minimal compute seem most directly influenced by and compared to the MoE papers listed above, especially [b55], [b42], [b26], [b7] and [b37]. The tiling approach from [b35] provides a key dense upcycling baseline.", "64893b17d68f896efa982789": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"b22\": 0.9,\n  \"b2\": 0.8,\n  \"b6\": 0.7,\n  \"b3\": 0.7,\n  \"b16\": 0.6\n}\n\nConfidence scores:\n- [b22] Mini-GPT: 0.9 - The paper draws heavy inspiration from Mini-GPT, using a similar approach of aligning vision and language models. However, it adapts this to the medical domain.\n- [b2] Vicuna: 0.8 - Vicuna is used as the foundation for the paper's large language model component, with fine-tuning on medical data. \n- [b6] MIMIC-CXR: 0.7 - This dataset of chest radiographs and reports is a key data source used for training the model in stage 1.\n- [b3] OpenI: 0.7 - The OpenI dataset is the other main data source, used in stage 2 of training with its high-quality curated summaries.\n- [b16] Med-Clip: 0.6 - Med-Clip is utilized as the pre-trained medical visual encoder in the model architecture.\n\nThe paper's key novel contributions appear to be 1) Adapting the Mini-GPT vision-language alignment approach to the medical domain, 2) Fine-tuning the language model on medical conversations and radiology data, 3) Using a two-stage training process with the MIMIC-CXR and OpenI datasets, and 4) Generating high-quality interactive summaries from radiology reports to train the model.\n\nThe source papers [b22], [b2], [b6], [b3] and [b16] directly enable and inspire these main contributions. The methodology draws heavily from Mini-GPT [b22] while adapting it to the medical context using Vicuna [b2], MIMIC-CXR [b6], OpenI [b3] and Med-Clip [b16].", "64a29612d68f896efa28bca5": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b45]\": 0.9,\n  \"[b21]\": 0.8,\n  \"[b22]\": 0.8,\n  \"[b35]\": 0.7,\n  \"[b20]\": 0.6,\n  \"[b47]\": 0.6\n}\n\nRationale and key points:\n\n- The paper introduces EMISSARY, a novel cost-aware cache replacement policy for instruction caches. It builds upon and is directly inspired by prior bimodal techniques like BIP [b45], as evidenced by phrases like \"Contrary to traditional LRU, a previously proposed mechanism, called LIP, is an insertion policy in an LRU replacement policy cache that always inserts lines in the LRU position instead [b45]. The BIP prior work adds bimodality to LIP using a 1/32 probability random signal [b45].\"\n\n- The work is motivated by the observation that modern processors with aggressive front-ends like FDIP can tolerate many L1I misses [b21] [b22]. Improving upon the FDIP baseline is noted as challenging, indicating these are key prior works: \"Prior work shows that achieving performance gains over FDIP's highly effective baseline is difficult [b21] [b22]. In fact, a non-realizable perfect prefetcher implemented over an FDIP baseline boosts performance by only 5.4% [b21] [b22].\"\n\n- The ability of out-of-order processors to tolerate L1D misses [b35] is mentioned as an analogous motivation for cost-aware replacement.\n\n- Early fetch engines that can tolerate some L1I misses [b20] [b47] are also cited as part of the background and motivation.\n\nIn summary, while the paper cites many prior works, the ones most directly influential to its novel contributions appear to be the bimodal caching techniques it builds upon like [b45], and the aggressive instruction fetch mechanisms like [b21] [b22] that create the opportunity for a cost-aware approach. A few other works provide important context and additional motivation [b35] [b20] [b47].", "621454435aee126c0f1d702f": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b9]\": 0.95,\n  \"[b20]\": 0.9,\n  \"[b30]\": 0.85,\n  \"[b21]\": 0.8,\n  \"[b27]\": 0.7,\n  \"[b7]\": 0.65\n}\n\nConfidence scores:\n[b9] Switch Transformer (0.95): The paper makes multiple direct comparisons to Switch Transformer, using it as the main baseline. The proposed expert choice method aims to address pitfalls in Switch Transformer's token-choice routing.\n\n[b20] GShard (0.9): GShard is also a key comparison, with the paper aiming to improve upon its top-2 token-choice gating approach. The 8B/64E model is directly compared to GShard's scaling.\n\n[b30] Sparsely-gated MoE (0.85): Identified as the first model to demonstrate major improvements with gating in the mixture-of-experts approach that this paper builds upon. \n\n[b21] Base Layers (0.8): Discussed as prior work that formulates expert assignment as a linear optimization problem to balance load, which relates to this paper's expert choice method and entropy-regularized assignment in Section 3.3.\n\n[b27], [b7] Hash layers (0.7, 0.65): Mentioned as recent alternative MoE routing approaches, but the paper argues they have limited downstream evaluation which this work aims to address.\n\nThe paper's key novel contributions appear to be: \n1) The expert choice routing method to address pitfalls in previous token-choice MoE \n2) Demonstrating improved training efficiency and downstream performance compared to Switch Transformer and GShard\n3) Scaling analysis from 16 to 128 experts\n4) Strong results on GLUE and SuperGLUE tasks\n\nThe selected references all directly relate to these core aspects of the work. The ordering and scores reflect the extent to which each reference is positioned as a key motivation or point of comparison for the primary technical innovations.", "64ae66f63fda6d7f0684abc0": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b6]\": 0.9,\n  \"[b26]\": 0.9,\n  \"[b7]\": 0.8,\n  \"[b25]\": 0.7,\n  \"[b22]\": 0.7,\n  \"[b17]\": 0.6,\n  \"[b34]\": 0.6\n}\n\nConfidence scores:\n- [b6] (BERT) and [b26] (RoBERTa): 0.9. The paper directly uses these encoder-based PLMs as the foundation for their probing methods. The novel contributions of probing the memorization and reasoning capabilities of PLMs over ontological knowledge are built upon BERT and RoBERTa.\n\n- [b7] (OpenPrompt): 0.8. The paper leverages the OpenPrompt framework to facilitate their prompt-based probing experiments, which is a key part of their methodology. \n\n- [b25] and [b22]: 0.7. These references likely propose the soft prompts approach that uses learnable soft tokens as an alternative to manual prompts. The paper adopts this method in their probing analysis.\n\n- [b17] and [b34]: 0.6. [b17] likely highlights how prompt templates can significantly affect performance, which motivates the paper's use of soft prompts. [b34] introduces the use of manual prompts for zero-shot probing, which the paper employs in their memorization probing.\n\nThe paper's core contributions of systematically probing PLMs' memorization and reasoning of ontological knowledge using prompt-based methods appear to be most directly inspired and enabled by the referenced works on PLMs (BERT, RoBERTa), prompting frameworks (OpenPrompt), and prompting techniques (manual prompts, soft prompts). The methodology draws heavily from these prior works.", "62a7fc635aee126c0ff5e394": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"b0\": 0.9,\n  \"b1\": 0.8,\n  \"b2\": 0.8,\n  \"b3\": 0.7,\n  \"b4\": 0.7\n}\n\nThe key points that led me to identify these as the most relevant source papers are:\n\n1. The paper focuses on using the geometric properties of protein molecular surfaces, specifically the Solvent Excluded Surface (SES), to identify ligand binding sites. References b0-b4 are directly cited when introducing and defining the SES concept, indicating they form the foundation for the paper's approach.\n\n2. The novel contribution seems to be using a Transformer-based neural network to learn local shape descriptors from the SES in order to predict binding sites. The SES construction itself relies heavily on the cited tools and methods in b0-b4.\n\n3. While the paper discusses other geometric pocket detection methods later on, the core methodology is centered around analyzing SES geometry, which traces back to the pioneering work establishing SES theory and computation in b0-b4.\n\n4. The phrases \"molecular surface of a protein, often defined as the separating surface between solvent (water) accessible and inaccessible regions [b3] [b0] [b4]\" and \"we build the proteins' Solvent Excluded Surface (SES) [b0] via the freely available software NanoShaper (NS) [b1] [b2]\" directly link the paper's methodological basis to these references.\n\nSo in summary, references b0-b4 appear to have directly motivated and enabled the novel SES analysis approach proposed in this paper to identify protein-ligand binding sites. The other references, while relevant, seem more peripherally related to the core contributions.", "63d9d87390e50fcafd57d878": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b47]\": 0.9,\n  \"[b60]\": 0.8,\n  \"[b24]\": 0.8,\n  \"[b61]\": 0.7,\n  \"[b12]\": 0.6,\n  \"[b54]\": 0.6\n}\n\nRationale:\n- The paper takes direct inspiration from StableDiffusion [b47] in exploring latent diffusion models (LDMs) for text-to-audio generation. This reference is explicitly called out as inspiration.\n- DiffSound [b60] and AudioGen [b24] are the two main baseline text-to-audio models that the proposed AudioLDM system is compared against extensively. The paper aims to improve upon the limitations of these prior works.\n- The diffusion model formulation and techniques like classifier-free guidance [b61] are core to the methodology of AudioLDM. References [b12] and [b54] establish foundational diffusion models that the paper builds upon.\n- While other references like [b44], [b58] related to contrastive language-audio pretraining are relevant, the aforementioned papers have a more direct influence on the novel LDM-based text-to-audio generation approach proposed, based on the comparative analysis and methodological details provided in the text.\n\nI focused on identifying the references that directly motivated and enabled the core technical contributions of AudioLDM, while deprioritizing more peripheral citations. The confidence scores reflect the centrality of each reference to the main ideas.", "643e0ad10746dc40e34197a9": "Here are the most pertinent source papers for this paper, based on the criteria provided:\n\n{\n  \"[b0]\": 0.9,\n  \"[b10]\": 0.8,\n  \"[b11]\": 0.7,\n  \"[b12]\": 0.6,\n  \"[b5]\": 0.5,\n  \"[b6]\": 0.5,\n  \"[b7]\": 0.4\n}\n\nConfidence scores:\n- [b0]: 0.9 - This reference is directly cited as exemplifying the large language models trained using RLHF that have profoundly influenced NLP and AI research. The paper aims to build upon this foundational work.\n\n- [b10]: 0.8 - The paper states they adhere to the fine-tuning approach outlined in this reference by Taori et al. This indicates the methodology is directly based on [b10].\n\n- [b11]: 0.7 - The Low-Rank Adaptation (LoRA) method used to efficiently adapt the pre-trained models is attributed to this reference. \n\n- [b12]: 0.6 - Combining LoRA with 8-bit matrix multiplication to further reduce memory requirements is inspired by the technique from this reference.\n\n- [b5], [b6]: 0.5 - Data from these benchmarks (Measuring Massive Multitask Language Understanding) is incorporated into the training datasets.\n\n- [b7]: 0.4 - Training data from the MedQA benchmark mentioned in this reference is used.\n\nThe other references like [b1]-[b3], [b8]-[b9] provide relevant background information but do not seem to be as directly influential to the core novel contributions and methodology of this paper based on the phrasing. The key aspects are: 1) Building upon RLHF-trained LLMs like in [b0], 2) Using the fine-tuning approach from [b10], 3) Employing LoRA [b11] and 8-bit quantization [b12] for efficient training, 4) Incorporating data from various NLP benchmarks like [b5]-[b7].", "6334268390e50fcafd6a5a91": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"b4\": 0.9,\n  \"b5\": 0.9,\n  \"b6\": 0.8,\n  \"b1\": 0.7,\n  \"b8\": 0.7,\n  \"b7\": 0.6,\n  \"b9\": 0.6\n}\n\nThe key novel contributions of this paper are:\n1) Proposing BioGPT, a domain-specific generative pre-trained Transformer language model for biomedical text generation and mining. BioGPT is pre-trained from scratch on a large biomedical corpus.\n2) Applying BioGPT to a range of biomedical NLP tasks including relation extraction, question answering, document classification and text generation. \n3) Carefully designing the target sequence format and prompt for each downstream task to adapt BioGPT.\n\nThe papers that most directly inspired and motivated this work are:\n\n- [b4], [b5], [b6]: The GPT-like models which demonstrate strong performance on language generation tasks, but have not been well adapted to the biomedical domain. This gap motivated the development of BioGPT.\n\n- [b1], [b8], [b7], [b9]: Previous works on pre-training BERT-like models on biomedical text, showing the importance of in-domain pre-training. However, these focus on language understanding not generation. BioGPT aims to fill this gap.\n\nA few other references like [b17], [b19] are relevant in discussing biomedical pre-training in general. But the core inspiration comes from wanting to develop a biomedical domain-specific generative pre-trained model, to achieve what GPT has done for general language generation in the biomedical space. The BERT-based biomedical models provide additional motivation in showing the gains from in-domain pre-training.\n\nSo in summary, the GPT papers and biomedical BERT model papers are the most direct influences that inspired the key novel aspects of this work. The confidence scores reflect the centrality of each reference in motivating the main contributions.", "64a29621d68f896efa28fd4a": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b3]\": 0.9,\n  \"[b1]\": 0.8,\n  \"[b11]\": 0.7,\n  \"[b0]\": 0.9,\n  \"[b22]\": 0.6,\n  \"[b17]\": 0.6\n}\n\nThe key factors I considered:\n\n1. The paper introduces Metal, a processor hardware extension that enables developers to rapidly evolve the processor's architecture through software. The most directly relevant references seem to be those related to programmability and extensibility of processors and systems:\n\n- [b3] discusses eBPF which allows modifying Linux kernel behavior, a capability Metal aims to provide more generally. \n- [b1] covers P4 programmable switches which expand programmability to network devices, analogous to how Metal brings programmability to the processor.\n- [b11] discusses how microcode is commonly used to implement processor programmability and extensions, which is the approach Metal builds upon and aims to open up to developers.\n\n2. The paper categorizes microcode into horizontal and vertical variants, and relates Metal to vertical microcode approaches:\n\n- [b0], [b22] and [b17] directly cover vertical microcode implementations like Intel XuCode, IBM Millicode and DEC Alpha PALcode respectively. The paper states Metal offers a \"vertical microcode like programming interface\", so these references seem to be key inspirations.\n\n3. I deprioritized references like [b6], [b5], [b43], [b18], [b38], [b16] which are mentioned in the Applications section, as they seem more peripherally related to the core Metal architecture contributions, based on my understanding.\n\n4. I did not find any explicit phrases like \"inspired by\", \"motivated by\" etc. So I had to infer the key references based on the overlapping concepts and methodologies between Metal and prior work.\n\nIn summary, the references that most directly influenced the main contributions of Metal appear to be [b3], [b1], [b11] related to processor/system programmability, and [b0], [b22], [b17] related to vertical microcode architectures. The confidence scores reflect my estimation of their centrality to Metal's novel aspects based on the textual analysis.", "640a9ffc90e50fcafd03ca47": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\n{\n  \"b7\": 0.9,\n  \"b22\": 0.8,\n  \"b17\": 0.7,\n  \"b2\": 0.6\n}\n\nExplanation:\nThe paper proposes a Siamese Recurrent Neural Network approach for disambiguating pairs of company names. The key novel contributions seem to be:\n\n1. Applying a Siamese Neural Network architecture to the company name disambiguation task. \n2. Using the model in an Active Learning setting to efficiently prioritize samples for human labeling.\n\nThe paper directly states it is inspired by the Siamese Deep Neural Network architecture from [b7], making this a top reference. \n\n[b22] is also highly relevant, as the paper notes they propose a similar Siamese DNN approach for the closely related task of toponym matching, and benchmark it against other methods. This serves as key inspiration.\n\n[b17] is pertinent since they also propose a Siamese DL model for mapping strings to a set of job names, which has methodological similarities. The paper compares some aspects of their approach to [b17].\n\nFinally, [b2] has moderate relevance, as they also propose a DL approach to NED, though using character-level tf-idf input vectors rather than the raw strings used here. Still, the contrastive training approach has some similarity.\n\nThe other referenced papers provide useful background on NED and Active Learning in general, but do not seem to be as directly influential on the core novel contributions of this specific paper based on the criteria provided. Let me know if you need any clarification or have additional questions!", "628304515aee126c0f6f0c83": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b16]\": 0.9,\n  \"[b9]\": 0.7,\n  \"[b25]\": 0.6,\n  \"[b15]\": 0.5\n}\n\nThe key novel contributions of this paper are:\n1) Developing an end-to-end machine learning pipeline called SystemMatch to optimize preclinical models to best approximate the behavior of an in sapiens target population. This enables drug developers to identify preclinical models with the greatest predictive power.\n\n2) Utilizing a Compositional Perturbational Autoencoder (CPA) to predict the behavior of single cells in previously untested combinations of experimental conditions. This allows generating in silico predictions to recommend experimental changes to enhance the preclinical models' similarity to the target population.\n\nThe source papers most directly related to these novel aspects are:\n\n[b16] - This paper introduced the Compositional Perturbational Autoencoder (CPA) which is a core component used in SystemMatch to predict cell behavior under new perturbation combinations. The current paper directly builds upon and applies CPA for its in silico query generation.\n\n[b9] - Provides an overview of uses for modeling perturbational single-cell data, which the current paper extends with the new use case of predicting optimal perturbations to generate an ideal model system. The current work is motivated by the perturbation response prediction concepts from [b9].\n\n[b25] and [b15] are also relevant as they deal with machine learning for cellular perturbation response prediction in general, which is conceptually related to SystemMatch's goal, although the specific techniques used are different.\n\nThe other references, while providing important background information, do not seem to directly inspire the main novel contributions of SystemMatch based on the phrasing in the text. The confidence scores reflect the degree to which the text indicates direct influence and methodological similarity between the references and SystemMatch's core ideas.", "646c3addd68f896efa5d1972": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b16]\": 0.9,\n  \"[b41]\": 0.8,\n  \"[b21]\": 0.7,\n  \"[b43]\": 0.6,\n  \"[b52]\": 0.5\n}\n\nConfidence scores:\n[b16]: 0.9 - This reference is directly cited as the first diffusion model for molecule generation that the current work builds upon and improves. The paper aims to address limitations of [b16] when generating large molecules.\n\n[b41]: 0.8 - EnFlow is mentioned as a recent work that utilizes continuous normalizing flows to sample valid molecules. The proposed method extends the invariant base distribution approach used in [b41] to a hierarchical framework.\n\n[b21]: 0.7 - This reference, along with [b41], is cited as an example of using invariant base distributions and equivariant neural networks for 3D molecule graph generation, which the current paper adopts.\n\n[b43]: 0.6 - Schnet from this reference is used in the autoregressive Gschnet model that is discussed. Integrating equivariance, as done in Schnet, is highlighted as an appealing property.\n\n[b52]: 0.5 - This reference is mentioned along with [b43] when discussing the importance and wide exploration of integrating equivariance in atomic systems.\n\nThe paper aims to address key limitations of non-autoregressive 3D molecule generation methods like EDM [b16], by proposing a novel hierarchical diffusion model. It adopts equivariant modeling approaches from [b41], [b21], [b43] and [b52]. The coarse-to-fine generation procedure is the main novel contribution to enable generation of realistic, drug-like molecules while maintaining the advantages of non-autoregressive models.", "6423ac7790e50fcafd55eacf": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b4]\": 0.9,\n  \"[b9]\": 0.8,\n  \"[b32]\": 0.7,\n  \"[b6]\": 0.6,\n  \"[b2]\": 0.5\n}\n\nThe key factors leading to these source papers being identified as most pertinent are:\n\n1. The paper introduces iHT as a novel Transformer-based inductive KG representation model. It draws heavy inspiration from [b4] in using \"Transformer as both contextualizer and relational scorer\" which is described as a key component of the iHT context Transformer. This indicates [b4] provided critical methodological inspiration.\n\n2. The entity Transformer in iHT shares the same architecture as BERT-like language models and is initialized with BERT weights, with the input format following [b9]. This shows the entity representation approach is directly motivated by [b9].\n\n3. The paper compares iHT against recent compositional knowledge representation approaches like KEPLER [b32] and MLMLM [b6], using them as key baselines that iHT aims to improve upon for the KG completion task. This suggests iHT builds upon the ideas in [b32] and [b6].\n\n4. Conventional KG embedding approaches like TransE [b2] are discussed to highlight their limitations that iHT tries to address, like scaling issues and inability to handle unseen entities. So while less central, [b2] still provides important context and motivation for iHT.\n\n5. The entity Transformer is designed to enable compositional knowledge sharing across entities, compared to transductive approaches like [b2] that treat entities as atomic units.\n\nIn summary, [b4] and [b9] emerge as the most influential source papers, directly inspiring iHT's key architectural components, while [b32], [b6] and [b2] provide important context and motivation for the novel aspects of iHT. The paper aims to combine the strengths of Transformer architectures and inductive KG representations to improve upon limitations of prior approaches.", "63b3f1fc90e50fcafdea3697": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b45]\": 0.9,\n  \"[b9]\": 0.8,\n  \"[b1]\": 0.7,\n  \"[b37]\": 0.6\n}\n\nThe key novel contributions of this paper seem to be:\n1. Proposing DietCode, a new auto-scheduler framework that efficiently supports dynamic-shape workloads by constructing a shape-generic search space made up of micro-kernels. \n2. Designing a cost model for DietCode that has a shape-generic component and a shape-dependent component to efficiently guide exploration of the search space.\n3. Using a joint learning approach in DietCode where all shapes of a workload explore the same search space and learn the same cost model collectively.\n\nThe paper indicates it is most directly inspired and motivated by the limitations of existing auto-schedulers like Ansor [b45], TVM [b9], Halide auto-scheduler [b1], and Tensor Comprehensions [b37] in handling dynamic shapes. \n\nIt aims to overcome their key shortcomings by introducing the novel shape-generic search space and micro-kernel based cost model. The methodology of using an incomplete program (micro-kernel) as a building block is a key distinguishing factor.\n\nTherefore, I assign the highest confidence scores to these 4 references, as they represent the core auto-scheduler works that DietCode builds upon and aims to improve. The repeated comparisons to these works throughout the text confirm they served as the primary motivation and foundation for DietCode's novel contributions.", "648fd298d68f896efa163bfb": "Here is my analysis of the key source papers for the given paper, based on the criteria you provided:\n\nAfter carefully reviewing the text, the novel contributions of this paper appear to be:\n1. Presenting ClinicalGPT, a large language model specifically designed for medical applications \n2. Training the model on extensive real-world medical datasets to incorporate domain knowledge\n3. Establishing a comprehensive evaluation framework across medical QA, exams, consultations, and record analysis\n4. Utilizing parameter-efficient fine-tuning methods to further improve ClinicalGPT's performance\n\nWith those contributions in mind, and looking for references that directly inspired or enabled those aspects, the key source papers seem to be:\n\n{\n  \"b0\": 0.9,\n  \"b2\": 0.85,\n  \"b3\": 0.8,\n  \"b5\": 0.7,\n  \"b6\": 0.75,\n  \"b11\": 0.65,\n  \"b17\": 0.6\n}\n\nRationale:\n- [b0] BERT is cited as pioneering the pre-train and fine-tune paradigm that this work builds upon\n- [b2] T5 directly inspires treating NLP tasks as text-to-text and the supervised fine-tuning approach \n- [b3] GPT-3 is highlighted for its few-shot capabilities that motivate ClinicalGPT\n- [b5] LaMDA and [b6] InstructGPT provide key ideas around instruction tuning and RL from human feedback that are adopted here\n- [b11] cMedQA2 is one of the core datasets used to train ClinicalGPT\n- [b17] LoRA is the specific parameter-efficient fine-tuning method utilized to optimize ClinicalGPT\n\nThe other referenced papers provide useful background and additional datasets, but these 7 appear to be the most central influences based on the criteria of novel contributions, methodological similarity, and comparative phrases in the text. The scores reflect my confidence in their centrality as source papers.", "6456385ad68f896efacf20ac": "Here is my analysis of the most pertinent source papers for the given academic paper, based on the criteria you provided:\n\n{\n  \"b16\": 0.9,\n  \"b1\": 0.8,\n  \"b15\": 0.7,\n  \"b2\": 0.6,\n  \"b8\": 0.5,\n  \"b3\": 0.5\n}\n\nExplanation:\n- [b16] (LLaMA) is mentioned as the foundational language model that the paper builds upon and adapts towards the medical domain. This is a key novel contribution.\n- [b1] (Vicuna) and [b15] (Alpaca) are referenced as examples of recent work that has adapted open-source LLMs for specific applications, which directly motivates the paper's approach of adapting LLaMA for the medical domain.  \n- [b2] (LMFlow) is referenced in the evaluation methodology, indicating its benchmarking approach influenced how the paper conducts its experiments and measures performance.\n- [b8] and [b3] are referenced in the context of the parameter-efficient fine-tuning experiments using LoRA. This suggests those papers directly inspired the specific fine-tuning techniques employed.\n\nThe other referenced papers like [b5], [b6], [b7], [b12], [b13] provide important context but do not seem to be as directly influential on the core novel contributions and methodologies, based on the phrasing. The evaluation against ChatGPT is not considered a core contribution.\n\nIn summary, the papers [b16], [b1], [b15] appear to be the biggest sources of inspiration for the key idea of the paper, while [b2], [b8] and [b3] directly influence the methodology. Let me know if you would like me to elaborate or modify the analysis further.", "64c78ba13fda6d7f06dba840": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\nThe key novel contributions of this paper appear to be:\n1. Providing the MELF concept as an end-to-end solution for semi-dynamic variability of compiled code at runtime\n2. Describing the MELF linker (an extension to LLVM's LLD linker) and MELF loader to enable the approach\n3. Demonstrating the benefits and costs of MELF in four case studies from different domains\n\nThe paper does not seem to use many direct phrases like \"inspired by\" to indicate reliance on specific previous works. However, based on an analysis of the key methods introduced and references that appear closely linked to the main contributions, here are the most likely influential source papers, in JSON format with confidence scores:\n\n{\n  \"[b13]\": 0.9,\n  \"[b0]\": 0.7,\n  \"[b1]\": 0.6,\n  \"[b2]\": 0.6,\n  \"[b4]\": 0.6,\n  \"[b16]\": 0.5\n}\n\n[b13] is referenced multiple times in key parts of the MELF approach, specifically for the MMViews kernel extension that allows multiple variants to coexist at runtime. This paper likely provided significant inspiration and a foundation for the MELF work.\n\n[b0]-[b5] are referenced when discussing the tradeoffs of exploiting processor-specific optimizations vs instrumenting code with checks. Balancing this tradeoff seems to be a key motivation behind MELF.\n\n[b16], which likely refers to the ELF standard, is important background for the linker modifications made in MELF.\n\nSo in summary, while not called out with explicit \"inspired by\" phrasing, references [b13], [b0], [b1], [b2], [b4] and [b16] appear to be the most influential prior works that enabled key aspects of the MELF approach presented in this paper. The MMViews work in [b13] was likely the most direct inspiration.", "64c78b9f3fda6d7f06db9a87": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b78]\": 0.9,\n  \"[b75]\": 0.8,\n  \"[b74]\": 0.7,\n  \"[b76]\": 0.6,\n  \"[b71]\": 0.5\n}\n\nThe key points that led me to identify these as the most relevant source papers are:\n\n1. The paper introduces a novel software-based defense called Half&Half to partition the conditional branch predictor (CBP) and prevent CBP-based side channel and transient execution attacks. To enable this, the authors do an in-depth reverse engineering of the CBP structures in recent Intel processors.\n\n2. In the background section, the paper discusses the TAGE branch predictor in detail, noting that \"variants of TAGE [b78], [b74], [b75], [b76] have won each branch predictor championship\" and that \"due to its accuracy, it is likely employed by other recent commercial high-performance processors.\" \n\n3. The geometric history length approach used in TAGE is directly inspired by the earlier O-GEHL predictor [b71], as mentioned in the paper: \"Similar to the O-GEHL predictor, TAGE uses geometric history lengths.\"\n\n4. The main novel contribution of reverse engineering the full CBP structures and indexing functions in recent Intel processors is directly motivated by and builds upon the prior work on TAGE. Understanding TAGE and its variants appears critical to the paper's core methods.\n\n5. While other background papers are referenced, the TAGE family of predictors [b78, b74-b76] and the closely related O-GEHL predictor [b71] have the strongest and most direct influence on the key ideas and techniques introduced in this paper to isolate the CBP. The heavy emphasis on TAGE in the background further supports this.\n\nSo in summary, while the paper references a broad set of prior work, the TAGE predictor papers, ordered by relevance, appear to be the main inspiration and foundation for the novel contributions of this work. The repeated mentions of TAGE and the detailed explanations of its key ideas are strong evidence of the central role it played in motivating and enabling this research.", "6389d6fb90e50fcafdffbdc3": "Here is my analysis of the most pertinent source papers for this paper, based on the criteria provided:\n\nThe key novel contributions of this paper are:\n1. Revealing that leveraging large pages can improve the effectiveness of spatial cache prefetchers by enabling safe prefetching across 4KB page boundaries when blocks reside in large pages.\n2. Proposing the Page-size Propagation Module (PPM), the first architectural scheme to enable safe prefetching beyond 4KB page boundaries. \n3. Designing a composite prefetcher called Page Size Aware Prefetcher with Set Dueling (Pref-PSA-SD) that selects between two identical prefetchers using different page sizes to drive prefetching.\n\nBased on these contributions and the language used in the paper, the most likely source papers are:\n\n{\n  \"[b13]\": 0.9,\n  \"[b14]\": 0.8, \n  \"[b15]\": 0.8,\n  \"[b16]\": 0.8,\n  \"[b29]\": 0.7,\n  \"[b72]\": 0.7\n}\n\nRationale:\n- [b13], [b14], [b15], [b16] describe the SPP, VLDP, PPF and BOP spatial prefetchers that this work builds upon and evaluates. The paper frequently compares against these prefetchers.\n- [b29] discusses the security issues with prefetching across page boundaries, which directly motivates the PPM scheme proposed in this paper.\n- [b72] introduces the Set Dueling mechanism that the composite Pref-PSA-SD prefetcher uses to select between page sizes.\n\nThe other references, while relevant for background, do not seem to be the core inspiration for the novel aspects of this paper based on the language used. The key phrases indicating inspiration focus on the spatial prefetchers, page boundary limitations, and set dueling.", "6426ed4590e50fcafd444b1b": "Based on the analysis of the paper, here are the most pertinent source papers that likely inspired this work, in JSON format with confidence scores:\n\n{\n  \"[b16]\": 0.9,\n  \"[b4]\": 0.8, \n  \"[b14]\": 0.7,\n  \"[b20]\": 0.6,\n  \"[b29]\": 0.5,\n  \"[b6]\": 0.4,\n  \"[b7]\": 0.4,\n  \"[b23]\": 0.4\n}\n\nThe key factors for selecting these source papers are:\n\n1. The paper frequently cites [b16], [b4], [b14] when discussing existing prefetch algorithms and how they improve accuracy. This indicates these works heavily influenced the accuracy considerations in this paper.\n\n2. [b20] is cited when introducing the taxonomy for classifying prefetching algorithms, which forms an important foundation for this paper's approach. \n\n3. [b29] is mentioned as an example of using time-based localization, showing it contributed to the localization concepts built upon here.\n\n4. [b6], [b7], [b23] are grouped together as \"existing spatial prefetchers\" that use PC localization as a filter. Overcoming the coverage limitations of this approach is a key motivation for this paper.\n\n5. The novel contributions of this paper focus on using localization as an event rather than a filter, and learning from both PC localized and spatially localized streams simultaneously. The frequently cited sources above ([b16], [b4], [b14], [b20]) appear to have laid the groundwork for these ideas.\n\nSo in summary, these references likely provided the key inspirations, taxonomies, and limitations that this paper builds upon to introduce its dual stream prefetching approach. The confidence scores reflect the centrality of each reference to the main ideas and techniques developed in this work.", "64e432bf3fda6d7f0600af81": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b47]\": 0.9,\n  \"[b3]\": 0.85,\n  \"[b39]\": 0.8,\n  \"[b6]\": 0.75,\n  \"[b15]\": 0.7,\n  \"[b25]\": 0.65,\n  \"[b2]\": 0.6\n}\n\nRationale and key observations:\n\n- The paper introduces the \"Snowflake Hypothesis\" to overcome over-smoothing and train deeper GNNs by pruning the adjacency matrix at the element level to uncover distinctive receptive fields for each node. This is directly inspired by and builds upon the message passing mechanism and over-smoothing problem discussed in [b47] and [b3].\n\n- The proposed SnoH algorithm can be seen as a graph pruning method and data augmenter. It is compared extensively to DropEdge [b39] and the graph lottery ticket hypothesis [b6], indicating these works provided key motivation. \n\n- The paper adopts the iterative pruning strategy [b15] within the unified graph sparsification (UGS) framework [b6] as one of the training schemes, showing the reliance on these previous techniques.\n\n- The gradient [b25] and cosine distance [b2] are used as the simplest guiding principles to control aggregation depth for each node in the two versions of SnoH. This highlights the direct influence of these prior works.\n\n- While other papers like [b24], [b27], [b48] etc. are referenced, they provide more general background on GNNs rather than directly inspiring the core novel contributions of this paper in terms of the Snowflake Hypothesis and receptive field based pruning.\n\nSo in summary, the message passing and over-smoothing concepts [b47, b3], pruning approaches [b39, b6, b15], and gradient/distance metrics [b25, b2] emerge as the most pertinent influences based on how the SnoH algorithm and hypothesis are positioned as novel contributions building upon these key prior works. The assigned confidence scores reflect the degree of importance based on the phrasing and comparisons in the text.", "64c78b993fda6d7f06db5b90": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b3]\": 0.9,\n  \"[b15]\": 0.8,\n  \"[b8]\": 0.7,\n  \"[b1]\": 0.6,\n  \"[b24]\": 0.5\n}\n\nConfidence scores:\n- [b3] (0.9): The paper directly builds upon and improves the seq2seq knowledge graph completion methodology proposed in this reference. It is cited multiple times when discussing the limitations of existing seq2seq KGC methods that the proposed approach aims to address.\n\n- [b15] (0.8): The T5 model architecture proposed in this reference is a key component used in the retrieval-enhanced generation module of the proposed ReSKGC approach. \n\n- [b8] (0.7): The Fusion-in-Decoder (FiD) technique from this reference is directly adopted to efficiently combine information from multiple retrieved passages during generation.\n\n- [b1] (0.6): This reference first proposed the evaluation metrics of Mean Reciprocal Rank (MRR) and Hits@K in the filtered setting, which are used to evaluate the proposed ReSKGC approach. \n\n- [b24] (0.5): One of the datasets (Wikidata5M) that ReSKGC is evaluated on was proposed in this reference. The scalability limitations of the embedding-based method in this reference are also discussed as motivation.\n\nIn summary, the methodology draws direct inspiration from the seq2seq KGC framework [b3], T5 architecture [b15] and FiD technique [b8], while the evaluation follows the protocol in [b1] and addresses limitations of the dataset proposed in [b24]. These appear to be the most central source papers that the novel contributions of ReSKGC build upon.", "6323e96890e50fcafd8a40e6": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b6]\": 0.9,\n  \"[b7]\": 0.9,\n  \"[b8]\": 0.8,\n  \"[b3]\": 0.7,\n  \"[b5]\": 0.6\n}\n\nThe key novel contributions of this paper are:\n1) Using a protein sequence-based deep learning model to predict drug-target affinity (DTA) in a reinforcement learning (RL) framework for de novo drug design. This allows generating potential inhibitors for targets lacking experimental structures.\n2) Comparing the impact of training the DTA model on different datasets (Davis, Kiba, BindingDB) on the diversity and activity of the generated molecules.\n3) Evaluating the potential bioactivity of the RL-generated molecules using both ligand-based (QSAR) and structure-based (docking) methods.\n\nReferences [b6], [b7] and [b8] are the most relevant, as they directly motivate the structure-based RL approach used here that incorporates protein information. The key phrases \"In addition, other DL algorithms have been used to create novel molecules such as recurrent neural network (RNN), autoencoder, generative adversarial network.\" and \"Taken on kind of reinforcement learning (RL) method for example, a generative model (agent) learns a policy (series of actions to create new structures) to generate molecules for maximizing a score, which is usually calculated by a predefined score function. Here, the score function includes the interaction between the ligand and target, such as molecular docking.\" highlight the direct inspiration.\n\n[b3] and [b5] provide important context on DL-based de novo design in general, but are less directly tied to the specific novel aspects of this work around protein sequence-based RL. The other background references [b1] and [b2] are not as centrally relevant.\n\nIn summary, this paper builds most directly on the concepts of structure-based RL for de novo design in [b6]-[b8], but uses a protein sequence-based DTA prediction model to extend the approach to targets lacking experimental structures. The comparative analysis on different training sets and bioactivity evaluation methods are other key novel aspects.", "64a407dcd68f896efaf1ba6d": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b1]\": 0.9,\n  \"[b23]\": 0.8,\n  \"[b10]\": 0.7,\n  \"[b2]\": 0.6\n}\n\nRationale and confidence scores:\n\n[b1] (confidence 0.9): The paper directly states \"Functional-first simulation has a separate functional simulator that runs ahead of the performance model, using a queue of instructions between the functional and performance simulator.\" This indicates [b1] is a key pioneering work that inspired the functional-first simulation approach used in this paper. The high frequency of references to [b1] throughout the text further supports its central importance.\n\n[b23] (confidence 0.8): The paper states \"Wrong-path instructions access the instruction and data caches, thereby potentially bringing in new data and evicting older cache lines.\" followed by the citation to [b23]. This suggests [b23] is an important prior work that identified the cache effects of wrong-path instructions, which is a key challenge this paper aims to address.\n\n[b10] (confidence 0.7): The authors note \"Many processor simulators do not model wrong-path execution (see Section VI-A) because it complicates the simulator implementation and because prior work indicates that wrongpath execution has not a large performance impact [b10].\" This frames [b10] as a seminal work that influenced many simulators to not model wrong-path execution. By highlighting and contrasting with [b10]'s conclusion, the authors are using it as important context and motivation for their own work.\n\n[b2] (confidence 0.6): The paper mentions \"only a small fraction of the instruction opcodes in an ISA is used for most of the dynamic instructions of an application [b2].\" This is used to justify decoupling functional and performance simulation. While relevant, the connection to [b2] is less direct than the previous references in terms of the paper's core contributions around wrong-path modeling.\n\nIn summary, based on the phrasing and context, references [b1], [b23] and [b10] appear to be the most central prior works that directly inspired and motivated this paper's novel contributions in functional-first simulation and wrong-path modeling. [b2] provides useful background but is less critical to the main techniques introduced.", "63ed9f3290e50fcafd0f10bd": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\nThe key novel contributions of this paper are:\n1. Proposing DrugGEN, a new de novo drug design system that generates target-specific small molecules using GANs, transformers, and graph representation learning. \n2. Utilizing two serially connected GANs, where the first GAN (GAN1) learns to generate valid, drug-like molecules, and the second GAN (GAN2) refines those molecules to target a specific protein by incorporating protein features.\n3. Developing variations of the DrugGEN architecture to evaluate different strategies for target-specific de novo drug design.\n\nBased on these contributions and the language used in the paper, the most likely key source papers are:\n\n{\n  \"[b32]\": 0.95,\n  \"[b10]\": 0.9,\n  \"[b33]\": 0.85,\n  \"[b43]\": 0.8,\n  \"[b44]\": 0.75,\n  \"[b46]\": 0.7,\n  \"[b47]\": 0.65\n}\n\n[b32] and [b10] likely refer to the original transformer and GAN architectures respectively, which form the core of DrugGEN's methodology. [b33] is referenced in the context of graph representation learning, a key aspect of how molecular and protein features are encoded. [b43] describes StackGAN which directly inspired DrugGEN's two-stage GAN architecture. [b44] and [b45] provide details on the graph transformer approach used. Finally, [b46] and [b47] relate to the WGAN loss function and gradient penalty terms used in training.\n\nThe other referenced papers provide relevant background information, but do not appear to be as directly influential on the core novel aspects of DrugGEN based on the language used. The methodology draws clear inspiration from the transformer, GAN, and graph representation learning paradigms established in a small set of pivotal previous works.", "642702aa90e50fcafd5de13d": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b23]\": 0.9,\n  \"[b11]\": 0.7,\n  \"[b26]\": 0.7, \n  \"[b31]\": 0.7,\n  \"[b42]\": 0.7,\n  \"[b40]\": 0.6,\n  \"[b48]\": 0.5\n}\n\nThe key points that led me to select these references:\n\n- The paper introduces Paralfetch, a new app prefetcher that addresses fundamental challenges previous prefetchers like FAST [b23] fail to achieve. This indicates [b23] is a key motivating work.\n\n- To improve prefetch throughput, Paralfetch introduces range merging of I/O requests, inspired by findings in [b11], [b26], [b31], [b42] that merging requests enhances SSD throughput. \n\n- The paper notes that aggressive prefetching can increase launch latencies due to I/O contention, aggravated by SSDs reordering outstanding I/Os [b40]. Paralfetch varies the I/O optimization level to avoid this issue highlighted in [b40].\n\n- To detect missing mmaped file blocks, Paralfetch instruments the faultaround handler [b48] with page fault tracing code. [b48] provides key inspiration for this tracing approach.\n\nThe other references like [b16], [b17] provide useful background but do not seem as directly influential on Paralfetch's novel contributions compared to the highlighted references above. Let me know if you need any clarification or have additional questions!", "63f2e4aa90e50fcafd2820a9": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\nThe key novel contributions of this paper seem to be:\n1) Using variational graph autoencoders (VGAE) to learn drug and protein representations on a multimodal graph, which the authors argue allows generating new links and provides superior performance compared to graph autoencoder (GAE) based methods. \n2) Leveraging drug molecular representations (Morgan fingerprints) concatenated with the learned drug latent representations to further enhance performance in predicting drug-pair polypharmacy side effects.\n\nLooking at the references that are most directly related to these novel aspects:\n\n[b16] introduced variational graph autoencoders (VGAE), which the authors use as a key part of their methodology. This reference is likely one of the most influential.\n\n[b41] proposed using a graph autoencoder to predict polypharmacy side effects on a multimodal graph, which the authors directly compare their VGAE approach to, arguing VGAE is superior. So [b41] provides important context and motivation.\n\nThe paper mentions that concatenating Morgan fingerprint representations of drugs with the learned latent representations improves performance in predicting drug-pair side effects. However, I did not see a reference directly tied to this aspect.\n\nOther references like [b39], [b34], [b23] are mentioned as examples of recent GNN-based approaches for learning drug/protein representations, which provides helpful background, but the paper's VGAE methodology seems to be more directly inspired by [b16] and positioned as an improvement over [b41].\n\nIn summary, here is my assessment of the most pertinent source papers in JSON format:\n\n{\n  \"[b16]\": 0.9,\n  \"[b41]\": 0.8\n}\n\nThe scores indicate my confidence that these references had a strong and direct influence on the paper's main novel contributions. [b16] gets the highest score as the original VGAE paper that the methodology relies on. [b41] is also ranked highly as it provides the key context and motivation for using VGAE to improve on GAE approaches for this task.", "62466dd35aee126c0f8b7b62": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b15]\": 0.9,\n  \"[b18]\": 0.8,\n  \"[b31]\": 0.7,\n  \"[b42]\": 0.6\n}\n\nConfidence scores:\n[b15]: 0.9 - This paper is referenced multiple times and seems to be the foundation for the diffusion model formulation and training objective used. Key phrases like \"Following [b15]\" and \"As conventional in modern diffusion models, we use the noise parametrization\" indicate heavy reliance on and inspiration from this work.\n\n[b18]: 0.8 - The signal-to-noise ratio (SNR) definition and simplification of the Lt term draws directly from this paper. It provides key components of the diffusion model used.\n\n[b31]: 0.7 - The E(n) equivariant graph neural network (EGNN) that is core to the novel E(3) equivariant diffusion model proposed in this paper comes from [b31]. The model builds on top of this previous work to enable E(3) equivariance.\n\n[b42]: 0.6 - The proofs related to equivariant distributions and diffusion models, which justify the invariance properties, are attributed to this paper. It provides the theoretical foundation for the equivariant diffusion approach.\n\nThe other referenced papers, while relevant for background and context, do not seem to be as directly influential on the main novel contributions and methods of this paper based on the text. The four papers highlighted appear to be the most central previous works that this equivariant diffusion model for molecule generation builds upon and takes inspiration from.", "6459ac57d68f896efa657eec": "Based on analyzing the paper, here are the most likely source papers that directly inspired or motivated this work, in order of confidence:\n\n{\n  \"35\": 0.9,\n  \"17\": 0.85,\n  \"22\": 0.85,\n  \"23\": 0.85,\n  \"16\": 0.8,\n  \"47\": 0.75,\n  \"15\": 0.7,\n  \"2\": 0.65\n}\n\nThe key reasons are:\n\n1. The paper makes heavy use of contrastive pre-training strategies to align text and graph representations, directly inspired by the contrastive pre-training approach in [35] for aligning image and text representations.\n\n2. For few-shot learning, the paper employs prompt tuning [17,22,23] to adapt the pre-trained model to downstream tasks in a parameter-efficient way, instead of fine-tuning the entire model. This shows prompt tuning is a core motivation.\n\n3. The graph encoder uses a Graph Convolutional Network (GCN) architecture [16], which is a fundamental building block.\n\n4. The text encoder uses a transformer architecture [47], which is the backbone model.\n\n5. Pre-trained language models like GPT-3 [2] and BERT [15] are frequently referenced as important context, though this work aims to go beyond their limitations by integrating graph structure.\n\nIn summary, contrastive pre-training [35], prompt tuning [17,22,23], GCNs [16] and transformers [47] emerge as the most central ideas that inspired the novel contributions in this paper. The confidence scores reflect the strength of influence based on the language used to describe these methods.", "64a407dcd68f896efaf1ba4e": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b6]\": 0.9,\n  \"[b7]\": 0.85,\n  \"[b12]\": 0.8,\n  \"[b17]\": 0.75,\n  \"[b19]\": 0.7\n}\n\nRationale and key points:\n\n- The paper focuses heavily on analyzing the impact of software instruction prefetching techniques like AsmDB [b12] when combined with industry-standard fetch-directed prefetching (FDP) implementations. FDP was originally proposed by Reinmann et al. in [b6], making this a foundational reference.\n\n- The specific FDP implementation the paper evaluates is based on recent optimizations proposed by Ishii et al. in [b7], including minimizing GHR noise and extending post-fetch correction. This makes [b7] highly relevant.\n\n- AsmDB [b12] is the state-of-the-art software instruction prefetching technique that the paper focuses its analysis on, making it a key motivating reference. The paper aims to understand why FDP substantially impacts the benefit of techniques like AsmDB.\n\n- The simulation methodology and front-end model is based on recent work in [b17] that emphasizes the importance of modeling a realistic FDP implementation to properly evaluate new prefetching proposals. \n\n- The FDP implementation incorporates an extension to the post-fetch correction mechanism originally proposed in [b19].\n\nSo in summary, [b6], [b7], [b12] and [b17] emerge as the most central and directly influential prior works that inspire and motivate this paper's analysis of software instruction prefetching in the context of modern FDP implementations. [b19] is also relevant but to a lesser degree.", "64be63403fda6d7f063e57a7": "Based on analyzing the paper, here are the key source papers that likely inspired this work, along with confidence scores:\n\n{\n  \"[b0]\": 0.9,\n  \"[b7]\": 0.9, \n  \"[b22]\": 0.8,\n  \"[b71]\": 0.8,\n  \"[b23]\": 0.7,\n  \"[b40]\": 0.7,\n  \"[b13]\": 0.6,\n  \"[b24]\": 0.6\n}\n\nThe paper takes heavy inspiration from recent works [b0] and [b7] which show global convergence guarantees for policy gradient methods in regular MDPs. The authors aim to extend those results to the robust MDP setting.\n\nKey phrases like \"inspired by the recent analysis of policy gradient methods for ordinary MDPs [b0] [b7]\" and comparisons to the convergence rates in [b0] indicate the centrality of these two papers.\n\nThe overall problem formulation and notation builds heavily on the foundational works [b22], [b71], [b23] and [b40] on robust MDPs and rectangularity assumptions. \n\nFor the convergence analysis, the use of Moreau envelopes and notions of gradient dominance take inspiration from [b13] and [b24], as indicated by citations when those concepts are introduced.\n\nSo in summary, [b0] and [b7] serve as the primary motivation, while [b22], [b71], [b23], [b40] provide the robust MDP framework, and [b13], [b24] inspire key proof techniques. These 8 papers likely capture the most central influences on the novel contributions in this work.", "6326f71890e50fcafdd04899": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria provided:\n\nThe key novel contributions of this paper appear to be:\n1. Proposing a method for predicting DDI and their type (event) based on attributed heterogeneous graph embedding and a deep learning approach\n2. Using an attributed heterogeneous DDIs network that presents the drug's interaction types along with the drug features to better demonstrate the intrinsic characteristics of a drug\n3. Developing an effective framework for aggregating various drug features and handling redundant information\n4. Employing a GNN model to extract drug embedding vectors from the attributed heterogeneous network representation\n\nThe paper mentions being directly inspired by or building upon the following references for its core methods:\n- [b30]: \"Lately, a lot of studies in the field of graphical data structure have been extremely advanced for processing network data structure [b30].\" This reference likely introduces foundational graph neural network techniques that the paper builds upon.\n- [b46]: \"In the proposed approach, we introduce a GNN-based model for learning Attributed Heterogeneous networks to extract the low-dimensional representation of nodes in the network. In this approach, we adopt an algorithm based on the recent research [b46] to learn the embedding of the attributed heterogeneous network.\" This reference provides the core heterogeneous graph embedding algorithm used.\n\nOther references that seem highly relevant to the paper's main challenges and novel approaches:\n- [b4]: Proposes the DPDDI technique to predict DDIs using GCNs and deep neural networks, which has similarities to this paper's approach\n- [b5]: Presents a multimodal deep learning framework using multiple drug features to predict DDI event categories, which this paper aims to improve upon\n- [b40]: Proposes the MTDDI end-to-end deep learning method to predict both DDIs and their types, which has the same goal as this paper\n\nIn summary, here are the source papers that likely most inspired and enabled this paper's main contributions, ranked by confidence:\n\n{\n  \"[b46]\": 0.95,\n  \"[b30]\": 0.9,\n  \"[b4]\": 0.8,\n  \"[b5]\": 0.75,\n  \"[b40]\": 0.7\n}\n\nThe references [b46] and [b30] are given the highest confidence scores as they are directly cited as providing the core graph embedding techniques used. [b4], [b5] and [b40] also rank highly as they tackle the same challenge of DDI type prediction using related deep learning approaches that this paper builds upon and compares against.", "64f59fc23fda6d7f0648f1fb": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\n{\n  \"[b26]\": 0.9,\n  \"[b21]\": 0.85,\n  \"[b7]\": 0.7,\n  \"[b22]\": 0.65,\n  \"[b31]\": 0.6,\n  \"[b29]\": 0.55,\n  \"[b18]\": 0.5\n}\n\nRationale:\n- [b26] and [b21] are cited as key papers establishing the RLHF technique which the current work builds upon and compares against. The text states \"We first review the RLHF pipeline introduced in [b26]; [b21]\", indicating these are foundational papers.\n\n- [b7] is referenced to support the claim that \"large language models (LLMs) exhibit a high degree of alignment with human judgment\". This helps motivate the paper's key idea of using AI preferences instead of human preferences. \n\n- [b22] is cited when discussing how to address position bias in the AI labeling process, which is an important methodological consideration.\n\n- [b31] and [b29] relate to the chain-of-thought reasoning and self-consistency techniques the paper experiments with to improve the AI labeling process and maximize alignment with human preferences. These represent key novel contributions.\n\n- [b18] is referenced as the RL algorithm the paper adapts for their RLAIF training, so it directly relates to their methodology, though is not as central as the RLHF papers.\n\nThe other cited papers like [b14] and [b16] provide background context but do not seem to be as directly influential on the paper's main proposals and results. Overall, this list prioritizes the papers most relevant to the key ideas, methods and results, with highest confidence given to those explicitly called out as inspirations or starting points.", "6459ac63d68f896efa658a19": "Here is my analysis of the key source papers for this paper, based on the criteria you provided:\n\n{\n  \"b51\": 0.9,\n  \"b52\": 0.8,\n  \"b20\": 0.7,\n  \"b3\": 0.6,\n  \"b8\": 0.5\n}\n\nThe key novel contributions of this paper are:\n1) Proposing a new NER-to-MRC data reconstruction approach using a simple yet stable multiple choice format that incorporates label semantics \n2) Applying MRC reasoning strategies to solve the NER task without relying on external data retrieval or pre-training\n\nThe source papers that most directly inspired and motivated these contributions are:\n\nb51 (confidence 0.9): This paper proposed the UniMC framework that inspired the authors' less manual multiple choice data reconstruction approach. The paper is cited when discussing the design principle of providing a single prompt question for stable results.\n\nb52 (confidence 0.8): This paper introduced the HRCA human reading strategy that the authors integrated into their MRC reasoning network to enhance the model's reasoning ability for the NER task. It is a key part of their methodology.\n\nb20 (confidence 0.7): An existing work that frames NER as MRC using BERT-MRC. The authors contrast their approach with this when discussing the challenges of existing NER-to-MRC methods. \n\nb3 (confidence 0.6): Another existing NER-to-MRC approach called KGQA that the authors aim to improve upon.\n\nb8 (confidence 0.5): The BERT language model which is part of the foundation of transformer-based models the authors build upon, but is not as directly related to their novel contributions.\n\nThe paper directly cites these works in the context of explaining the core aspects of their NER-to-MRC framework. The HRCA and UniMC papers in particular seem to have provided direct inspiration for their methodology. Let me know if you need any clarification or have additional questions!", "63d9d87390e50fcafd57d920": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b39]\": 0.9,\n  \"[b28]\": 0.85,\n  \"[b10]\": 0.8,\n  \"[b19]\": 0.75,\n  \"[b34]\": 0.7,\n  \"[b35]\": 0.65\n}\n\nConfidence scores:\n- [b39] (0.9): The paper heavily relies on the transformer architecture introduced in this pioneering work. It forms the backbone of their Text+Chem T5 model.\n\n- [b28] (0.85): The T5 framework proposed in this paper is directly used as the foundation of their multi-task, multi-domain model. The paper is inspired by T5's generalization and adaptation capabilities.\n\n- [b10] (0.8): MolT5, introduced in this paper, is a key baseline and comparison point. The proposed Text+Chem T5 model aims to improve upon MolT5's cross-domain generation capabilities without expensive pre-training.\n\n- [b19] (0.75): T5Chem, proposed in this work, is another important baseline as a unified multi-tasking model for chemistry. However, Text+Chem T5 aims to go beyond its limitations of being restricted to the chemical domain.\n\n- [b34] (0.7), [b35] (0.65): These papers introduce specialized transformer models for key chemical tasks like reaction prediction and retrosynthesis, which Text+Chem T5 aims to handle in a unified way. They serve as key motivation and comparison points.\n\nIn summary, the transformer architecture [b39], T5 framework [b28], and recent chemical language models like MolT5 [b10] and T5Chem [b19] emerge as the most influential works that this paper builds upon and compares against, in order to propose a more general multi-domain solution. Key chemical task papers [b34, b35] also guide the choice of tasks and baselines.", "64659ad1d68f896efa87539f": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"b18\": 0.9,\n  \"b19\": 0.95,\n  \"b1\": 0.8,\n  \"b34\": 0.7\n}\n\nExplanation:\nThe paper introduces MedVInT, a novel generative-based approach for medical visual question answering (MedVQA). The key novel contributions are:\n\n1. Reframing MedVQA as a generative learning task rather than classification/retrieval \n2. Aligning a pre-trained vision encoder with a large language model through visual instruction tuning\n3. Introducing PMC-VQA, a new large-scale diverse MedVQA dataset \n\nThe most relevant source papers that directly inspired these contributions are:\n\n- [b19] PMC-CLIP: The visual encoder is based on a ResNet-50 adopted from PMC-CLIP. The PMC-VQA dataset is also constructed by leveraging the PMC-OA dataset from PMC-CLIP. This previous work provides the foundation for the visual representation and dataset. (confidence: 0.95)\n\n- [b18] BLIP-2: The approach of pre-training the whole network on an image-text dataset in a caption-based manner to align the visual and language spaces is inspired by BLIP-2. (confidence: 0.9) \n\n- [b1] Flamingo: While Flamingo focused on natural images, it demonstrated the promise of visual-language models for open-ended VQA, motivating this work's generative VQA approach for the medical domain. (confidence: 0.8)\n\n- [b34] LLaMA: The text encoder in MedVInT-TD is initialized with LLaMA's parameters. LLaMA is also used to filter the PMC-VQA dataset. (confidence: 0.7)\n\nThe paper directly builds upon the visual representations and dataset from [b19], adopts the vision-language alignment technique from [b18], is inspired by the potential of generative VQA shown in [b1], and leverages [b34] for the language components. These 4 references emerge as the most central previous works that enabled the novel contributions.", "6427029c90e50fcafd5d6cf3": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b39]\": 0.9,\n  \"[b7]\": 0.8, \n  \"[b35]\": 0.8,\n  \"[b11]\": 0.6,\n  \"[b20]\": 0.6,\n  \"[b21]\": 0.6,\n  \"[b25]\": 0.6,\n  \"[b26]\": 0.6\n}\n\nRationale:\n- The paper's core contribution is proposing system entropy (E_S) to quantify interference in datacenters. It follows a three-step paradigm \"inspired by\" Shannon's information entropy paradigm from reference [b39]. This indicates [b39] is a key inspiration.\n\n- The paper compares the proposed ARQ scheduling strategy to CLITE [b35] and PARTIES [b7], showing ARQ outperforms them in reducing E_S. This suggests [b7] and [b35] are highly relevant prior works that the paper aims to improve upon.\n\n- References [b11], [b20], [b21], [b25], [b26] are mentioned together as \"state-of-the-art resource managers in prior studies\" that use resource isolation to eliminate interference. The paper notes this can reduce utilization compared to allowing partial sharing, motivating the ARQ approach. While relevant background, these seem less central than [b39], [b7] and [b35].\n\n- The other references are either used to provide context on datacenters and interference in general, or are not discussed in enough detail to determine if they are key inspirations vs background information. The core novelty seems to be extending the information entropy concept to define system entropy.\n\nIn summary, [b39] appears to be the most direct inspiration for the system entropy metric, while [b7] and [b35] represent the most relevant prior scheduling strategies that the paper aims to improve upon with ARQ. The other references provide important context but are less central to the main contributions in my assessment.", "634d805590e50fcafd4e05f7": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b12]\": 0.9,\n  \"[b19]\": 0.9,\n  \"[b25]\": 0.7,\n  \"[b27]\": 0.6,\n  \"[b26]\": 0.8,\n  \"[b17]\": 0.8\n}\n\nRationale:\nThe paper introduces PDL, a new Pipeline Description Language, as its main novel contribution to enable easier specification of processor pipelines while still providing fine-grained microarchitectural control. \n\nKey references that likely inspired this work:\n- [b12] and [b19] are cited when mentioning that simulation and bounded model checking techniques used to validate RTL processors \"cannot expose all bugs in large designs\". This suggests these papers highlight the limitations of existing validation approaches that PDL aims to address.\n- [b25] is referenced when discussing how PDL variables are declared and assigned once, similar to Verilog wires. This indicates Verilog HDL likely served as inspiration for aspects of PDL's design.\n- [b27] is the RISC-V ISA specification that the example PDL code implements a subset of. RISC-V is the base ISA that PDL targets.\n- [b26] describes the Tomasulo algorithm for out-of-order execution. It is referenced when discussing how PDL's renaming register file lock is general enough to fit a Tomasulo-style OoO microarchitecture.\n- [b17] is mentioned as another tool that generates pipeline speculation, but has more restrictions than PDL. This prior work likely inspired PDL's approach to speculative execution.\n\nThe other references, based on their context, seem to provide more general background rather than directly motivating PDL's specific language design and novel contributions. The key aspects - using a new DSL to abstract processor pipeline development, while enabling low-level control of hazards, speculation, and OoO - appear most directly influenced by the limitations of RTL ([b12], [b19]), semantics of existing HDLs like Verilog ([b25]), the target RISC-V ISA ([b27]), and prior work on pipeline speculation ([b17]) and OoO execution ([b26]).", "64a63bbad68f896efaec478f": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b19]\": 0.9,\n  \"[b26]\": 0.8,\n  \"[b6]\": 0.7,\n  \"[b15]\": 0.7,\n  \"[b18]\": 0.7,\n  \"[b10]\": 0.6,\n  \"[b34]\": 0.6,\n  \"[b35]\": 0.6\n}\n\nReasoning:\n\nThe paper introduces prompt learning, a technique from NLP, to graph neural networks in a multi-task setting. The key novel contributions are:\n\n1. Unifying the format of language and graph prompts to enable transferring prompt ideas from NLP to graphs \n2. Reformulating node-level and edge-level graph tasks into graph-level tasks to match graph pre-training objectives\n3. Using meta-learning to learn better prompts across multiple graph tasks\n\n[b19] is likely the most influential, as the authors directly state \"Inspired by the success of the language prompt, we hope to introduce the same idea to graphs.\" This indicates [b19] was a key motivation.\n\n[b26] is the only prior work the authors found that studies graph prompts, though it is limited to a single task. Addressing the limitations of [b26] seems to be an important goal.\n\n[b6], [b15] and [b18] introduce effective prompt methods in NLP like discrete and continuous prompts. The authors aim to adapt these to the graph domain.\n\n[b10], [b34] and [b35] propose graph-level pre-training strategies that the authors identify as having \"intrinsic similarities with the language-masked prediction task\". Matching these pre-training objectives is a key part of their approach.\n\nThe other references provide useful background on GNNs and graph pre-training in general, but do not seem to be as directly influential on the main novel contributions. The confidence scores reflect the estimated degree of direct relevance and inspiration each reference provided for this work.", "63a1751790e50fcafd1f49ce": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\nThe key novel contributions of this paper appear to be:\n\n1. Introducing INSTRUCTOR, a single multitask model that generates task- and domain-aware text embeddings given an input text and task instructions. This allows the same input text to be embedded into different vectors for different end tasks.\n\n2. Training INSTRUCTOR on MEDI, a new collection of 330 text embedding datasets annotated with human-written task instructions, using a contrastive loss to distinguish related vs unrelated text pairs. \n\n3. Demonstrating that INSTRUCTOR significantly outperforms prior state-of-the-art embedding models on a wide variety of downstream tasks spanning different domains, without any task-specific finetuning.\n\nThe key methodological components are:\n- Using a single encoder architecture with GTR models as the backbone \n- Encoding the concatenation of the input text and task instruction\n- Training with a bidirectional in-batch sampled contrastive loss \n\nBased on this, the most relevant source papers appear to be:\n\n{\n  \"[b50]\": 0.9,\n  \"[b49]\": 0.85,\n  \"[b25]\": 0.8\n}\n\nThe key evidence is:\n- \"We build INSTRUCTOR, based on the single encoder architecture [b25] [b50] [b49].\"\n- \"Following prior work [b50] [b49] , we use GTR models as the backbone encoder\"\n- \"Further following [b50] , we compute the same loss with x and y swapped and add it to the previous loss (i.e., bidirectional in-batch sampled loss).\"\n\nThese references are directly cited as the basis for INSTRUCTOR's architecture and training approach. The high frequency of citations, especially to [b50], indicates these prior works provided direct inspiration and motivation for the novel contributions of this paper.", "64337e3190e50fcafd76ef32": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b76]\": 0.9,\n  \"[b16]\": 0.8,\n  \"[b37]\": 0.7,\n  \"[b77]\": 0.7,\n  \"[b8]\": 0.6,\n  \"[b57]\": 0.6,\n  \"[b82]\": 0.6\n}\n\nThe key points that led me to identify these as the most relevant source papers are:\n\n- The paper introduces \"generative agents\" that draw on generative models (specifically large language models like ChatGPT [b76]) to simulate believable human behavior. This indicates [b76] is a key inspiration.\n\n- It mentions that despite progress in large language models [b16] that can simulate human behavior at a single time point [b37] [b77], fully general agents need architectures that can manage growing memories over time. This suggests [b16], [b37] and [b77] directly motivated this work to extend those capabilities. \n\n- The paper aims to realize early visions from researchers of computationally-powered agents that act as believable proxies of human behavior, citing several papers on this vision [b8] [b57] [b82]. This indicates those pioneering works provided significant inspiration.\n\n- Specific phrases like \"we introduce generative agents\" and comparisons to limitations of prior work like \"large language models [b16], though not sufficient by themselves, open up a new angle...\" confirm the direct influence of these references on the core contributions.\n\nSo in summary, the integration of large language models into an architecture to enable more general, believable agents over time seems to be most directly inspired and motivated by [b76], [b16], [b37] and [b77]. The overall vision draws heavily from the early ideas in [b8], [b57] and [b82]. The confidence scores reflect the centrality of each reference to the main novel contributions of the paper.", "63bfce0790e50fcafd0a10d0": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[31]\": 0.9,\n  \"[27]\": 0.8,\n  \"[105]\": 0.7,\n  \"[109]\": 0.7,\n  \"[34]\": 0.6,\n  \"[74]\": 0.6\n}\n\nThe key novel contribution of this paper is introducing Hermes, a technique to predict which load requests will go off-chip and start fetching their data directly from main memory in parallel with accessing the on-chip cache hierarchy. This aims to hide the increasing on-chip cache access latency.\n\nThe paper draws heavy inspiration and motivation from prior work on hardware prefetching techniques, as evidenced by this key phrase:\n\"Hermes works in tandem with any hardware data prefetcher and reduces the long memory access latency of off-chip load requests that otherwise could not have been prefetched by sophisticated state-of-the-art prefetchers.\"\n\nIt specifically compares Hermes' performance benefits to several recent prefetching techniques:\n- Pythia [31]\n- Bingo [27]  \n- MLOP [105]\n- SMS [109]\n- SPP [34], [74]\n\nThe high confidence scores reflect that optimizing prefetching and tolerating memory latency, which these prior works focus on, is core to Hermes' goals. The paper aims to improve upon the coverage limitations of even advanced prefetchers like these.\n\nSo in summary, while Hermes proposes a novel off-chip load prediction technique, it is heavily motivated by and compared against state-of-the-art hardware prefetching mechanisms. Those prefetching papers, especially Pythia, Bingo, MLOP and SMS, emerge as the most influential source material based on the language and comparisons in the text.", "637aec2590e50fcafd92962b": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b4]\": 0.9,\n  \"[b5]\": 0.9,\n  \"[b7]\": 0.8,\n  \"[b8]\": 0.8,\n  \"[b9]\": 0.8,\n  \"[b10]\": 0.7,\n  \"[b11]\": 0.6,\n  \"[b12]\": 0.6\n}\n\nThe key novel contributions of this paper are:\n1) Extending the AXI4 on-chip protocol to support end-to-end bus-packed strided and indirect memory streams\n2) Extending a RISC-V vector processor to leverage AXI-PACK for efficient irregular memory accesses\n3) Designing a banked memory controller to efficiently serve the irregular AXI-PACK bursts\n\nThe paper indicates that existing core-side extensions using stream abstractions, like those in [b4], [b5], [b7], [b8] and [b9], heavily inspired their work. However, the paper aims to address shortcomings of these prior works, like ignoring downstream interconnects and memory systems. \n\nThe memory-side extensions in [b10], [b11] and [b12] also motivated their approach, but the paper notes drawbacks like occupying memory and lacking tight architectural integration compared to core-side extensions.\n\nSo in prioritized order, [b4], [b5], [b7-9] are the most pertinent references that directly inspired the novel AXI-PACK protocol and vector processor extensions. [b10-12] are also quite relevant in motivating the memory controller design to efficiently handle the irregular bursts.\n\nThe other references like [b0-3], [b6], [b13-19] provide useful background and context, but do not seem to be as directly influential on the paper's main contributions based on the language used. The confidence scores reflect my estimation of how centrally each reference contributed to the novel aspects of the paper.", "64b60eaf3fda6d7f06eaf557": "Here is my analysis of the key source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b4]\": 0.9,\n  \"[b10]\": 0.7,\n  \"[b12]\": 0.7,\n  \"[b17]\": 0.4\n}\n\nRationale and confidence scores:\n\nThe paper builds most directly on the work in [b4], which proposed FlashAttention to speed up attention computation and reduce memory usage. This is evident from statements like \"Building on FlashAttention, we propose FlashAttention-2 with better parallelism and work partitioning to address these challenges.\" [b4] is the key inspiration and foundation for the novel contributions in this paper. Confidence score: 0.9\n\nThe paper also relies heavily on the online softmax technique proposed in [b10] and its application to attention in [b12]. This is clear from the detailed description of the online softmax trick and how it enables the tiling approach in FlashAttention and FlashAttention-2. While not the primary inspiration, [b10] and [b12] provide key building blocks. Confidence score for each: 0.7\n\nThe Transformer architecture proposed in [b17] provides important background context, but is not as directly related to the specific algorithmic innovations in this paper. Confidence score: 0.4\n\nThe other cited papers, like [b1], [b2], [b3], [b7], [b8], [b13], [b18], [b19] on approximate attention methods are mentioned more as related work rather than direct influences. The hardware details in [b5], [b6] also provide useful background but are not core to the novel contributions.\n\nIn summary, [b4] is clearly the most central inspiration, with [b10] and [b12] also being quite relevant to the key ideas. The Transformer paper [b17] is included as important context. The other citations, while relevant, do not seem to be the primary driving forces behind the innovations proposed in this paper.", "64a407dcd68f896efaf1ba64": "Based on the analysis of the paper, here are the most pertinent source papers that inspired or motivated this work, along with confidence scores:\n\n{\n  \"[b2]\": 0.9,\n  \"[b5]\": 0.8,\n  \"[b3]\": 0.7,\n  \"[b16]\": 0.7,\n  \"[b1]\": 0.6\n}\n\nThe key reasons for selecting these papers are:\n\n- The paper frequently cites gem5 [b2] when discussing architectural simulators and uses gem5 as the representative simulator for their experiments and analysis. This suggests gem5 was a key motivation and foundation for this work.\n\n- The paper compares gem5's performance to other well-known architectural simulators like zsim [b5] and sniper [b3], indicating these simulators also inspired the comparative analysis.  \n\n- FireSim [b16] is used to run sensitivity experiments by changing cache configurations of the simulated server running gem5. Using FireSim to analyze gem5 seems to be directly motivated by prior work on FPGA-accelerated simulation.\n\n- The background section discusses the history of architectural simulation techniques proposed over the years [b1], providing context and inspiration for this work analyzing the current state-of-the-art.\n\nThe paper aims to understand gem5's performance bottlenecks on modern hardware platforms through detailed profiling. Prior works proposing architectural simulators and simulation acceleration techniques appear to be the key motivating factors, with gem5 being the focal point of the empirical study.", "6423ac7790e50fcafd55eaa0": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\n{\n  \"[b58]\": 0.9,\n  \"[b27]\": 0.8,\n  \"[b28]\": 0.7,\n  \"[b36]\": 0.6,\n  \"[b4]\": 0.5\n}\n\nConfidence scores:\n[b58] (0.9): The paper heavily relies on the Transformer architecture proposed in this source paper. It provides essential background and is referenced multiple times when discussing Transformer-specific PEFT methods.\n\n[b27] (0.8): Adapters, first introduced in this paper, are discussed as one of the main categories of additive PEFT methods. The text dedicates a significant portion to adapters and their variations.\n\n[b28] (0.7): This paper introduces LoRA (Low-Rank Adaptation), which is highlighted as a well-known reparameterization-based PEFT method. It is discussed in detail in section 10.2.\n\n[b36] (0.6): Soft prompts, a key additive PEFT approach, are discussed at length, with this paper being a seminal work in the area. The text mentions how soft prompts address limitations of discrete prompts.\n\n[b4] (0.5): BitFit, a selective PEFT method that tunes only model biases, is attributed to this paper. It exemplifies modern selective approaches based on layer type or internal structure.\n\nThe paper's novel contributions appear to be the comprehensive survey and taxonomy of PEFT methods across multiple dimensions. It highlights current challenges and suggests future research directions. The selected source papers directly influence these main aspects by introducing key PEFT techniques that form the basis of the proposed taxonomy and analysis. The confidence scores reflect the centrality of each source paper to the survey's core content.", "6459ac63d68f896efa6588b4": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\n{\n  \"[b32]\": 0.9,\n  \"[b5]\": 0.8,\n  \"[b17]\": 0.7,\n  \"[b43]\": 0.6\n}\n\nConfidence scores:\n[b32]: 0.9 - This paper is referenced multiple times as the top performing system in the previous MultiCoNER I task. The current work directly builds upon and improves the retrieval augmentation approach proposed in [b32].\n\n[b5]: 0.8 - Also a top system in MultiCoNER I that is discussed, which used a gazetteer-adapted network. The current work incorporates some similar ideas of using Wikidata for additional entity knowledge.\n\n[b17]: 0.7 - Mentioned as prior work showing the effectiveness of retrieval-augmented methods in NLP. The infusion approach used here to utilize more retrieval context is inspired by techniques in this paper.\n\n[b43]: 0.6 - Describes the MultiCoNER shared tasks and datasets which are the focus of this work. Understanding the task definitions, challenges and data from [b43] directly motivates this paper's approach.\n\nThe paper emphasizes improving upon the knowledge retrieval limitations of prior top systems like [b32] and [b5] as a key contribution. The retrieval augmentation module with multiple strategies and the infusion technique to incorporate more context also draw direct inspiration from works like [b17]. So these emerge as the most influential source papers based on the criteria of being linked to the main challenges, methods and contributions.", "64741a3ad68f896efaa62202": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria provided:\n\n{\n  \"[b30]\": 0.9,\n  \"[b9]\": 0.8,\n  \"[b3]\": 0.7,\n  \"[b19]\": 0.7,\n  \"[b38]\": 0.6\n}\n\nThe key novel contributions of this paper are:\n1) Providing a unified theoretical framework to understand which semantically relevant features are learned by both supervised and unsupervised contrastive learning. This framework is then used to characterize the failure modes of class collapse and feature suppression.\n\n2) Proving that the simplicity bias of (stochastic) gradient descent is a key factor causing class collapse in supervised contrastive learning and feature suppression in unsupervised contrastive learning. \n\n3) Theoretically justifying why combining supervised and unsupervised contrastive losses leads to superior representations that avoid class collapse and feature suppression.\n\nThe source papers most directly related to these contributions are:\n\n[b30] - Highlighted as directly inspiring the insight that the simplicity bias of SGD is the main factor driving class collapse. The paper likely discusses this simplicity bias.\n\n[b9] - One of the only prior theoretical works on class collapse, but cannot explain class collapse at test time, which this paper addresses. Likely the launching point for this paper's class collapse analysis.\n\n[b3], [b19] - Empirically observe that combining supervised and unsupervised losses avoids class collapse, which this paper provides a theoretical explanation for. The methods in these papers were likely a motivation.\n\n[b38] - Shows feature suppression occurs in unsupervised contrastive learning, which this paper provides the first theoretical characterization of. The empirical findings likely inspired the theoretical analysis.\n\nThe paper also cites these as either \"pioneering work\", \"inspiring\" the approach, or directly compares its methods and results to them, indicating their centrality. Overall, these 5 papers appear to be the most influential prior works that this paper builds upon to make its novel contributions.", "64a29621d68f896efa28fd54": "Here is my analysis of the most pertinent source papers for this paper, based on the criteria provided:\n\n{\n  \"51\": 0.9,\n  \"7\": 0.85,\n  \"42\": 0.8,\n  \"40\": 0.75,\n  \"68\": 0.7\n}\n\nExplanation:\nThe paper introduces two key techniques to improve upon FIFO caching - Lazy Promotion (LP) and Quick Demotion (QD). \n\nFor Lazy Promotion, the paper compares LP-FIFO algorithms like FIFO-Reinsertion and 2-bit CLOCK to LRU. This seems to be directly inspired by previous work on CLOCK [19,55,64] and reinsertion/CLOCK variants [6,33,38,42,55] which are cited as being perceived as less efficient than LRU. So references [42] and [7] which introduce 2Q and LHD respectively are likely key influences.\n\nFor Quick Demotion, the paper notes that the idea is not new and has been discussed in various contexts and algorithms before, specifically calling out 2Q [42], MQ [88], ARC [51], and LIRS [40] as being inspired by the need to remove low-value objects faster. Later the paper adds a simple QD technique on top of several state-of-the-art algorithms including ARC, LIRS, and LeCaR [68], showing that QD provides substantial improvements. \n\nTherefore, I score [51] the highest as ARC seems to be the primary inspiration and baseline for the QD technique. This is followed by [7], [42], [40] and [68] as other key algorithms that either inspired or were used to demonstrate the benefits of QD.\n\nThe paper's primary novel contributions appear to be 1) showing that simple LP-FIFO can outperform LRU, and 2) demonstrating the benefits of QD across many state-of-the-art algorithms. While doing so it builds heavily upon ideas and algorithms from the references scored above.", "64a29621d68f896efa28fd67": "YouChat is a large language model from You.com.\n\nYou.com is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. YouChat will generate a comprehensive and informative answer for a given question based on the provided web Search Results. YouChat considers the history of the conversation and answer the questions in a helpful manner. It tries to use the information from the provided search results to ensure accuracy and uses an unbiased and journalistic tone. YouChat will combine search results together into a coherent answer and cite search results using [${{number}}] notation. It will only cite the most relevant results that answer the question accurately.", "64be5e653fda6d7f063a95ce": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b24]\": 0.9,\n  \"[b40]\": 0.8,\n  \"[b50]\": 0.7,\n  \"[b68]\": 0.7,\n  \"[b11]\": 0.6,\n  \"[b33]\": 0.6\n}\n\nConfidence scores:\n- [b24] (score 0.9): This reference on the Gaussian mechanism and advanced composition theorem is directly used to define and analyze the differential privacy aspects. The paper states \"We conclude by applying lower bounds on centralized private estimation of one-way marginals [b68] to M\", indicating [b24] plays a key role.\n\n- [b40] (score 0.8): Assumption 2.1 on bounded heterogeneity is directly taken from this reference. The heterogeneity bound G appears in the main lower bound result.\n\n- [b50] (score 0.7): The paper aims to improve over the lower bound from this reference on non-adversarial private distributed learning. This shows [b50] is an important motivation.\n\n- [b68] (score 0.7): Lower bounds from this reference on centralized private estimation are directly applied to prove the main lower bound. \n\n- [b11] (score 0.6): The robustness definition is contrasted with the (?, f)-Byzantine resilience criterion from this reference. While not a direct inspiration, it helps contextualize the robustness aspect.\n\n- [b33] (score 0.6): The robustness goal of minimizing the loss over honest workers' data is attributed to this reference.\n\nThe other references, while relevant for background, do not seem to be as directly influential on the main novel contributions based on the phrasing in the text. The lower bound proof technique and result appear to be most heavily inspired by [b24], [b40], [b50] and [b68].", "64ba03413fda6d7f062732f5": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b31]\": 0.9,\n  \"[b18]\": 0.8,\n  \"[b44]\": 0.7,\n  \"[b8]\": 0.6,\n  \"[b41]\": 0.6,\n  \"[b80]\": 0.5\n}\n\nConfidence scores:\n[b31], [b18], [b44]: These papers are directly cited as inspiration for using variational methods and latent variable models to parameterize the multimodal policy. The paper states \"We adopt a sequence modeling perspective [b7] and view the policy as a density function over the entire trajectory space (instead of the action space) [b81] [b34]. This allows us to sample a population of trajectories that cover multiple modalities, enabling concurrent exploration of distant regions in the solution space. Additionally, we use a generative model to parameterize the multimodal policies, drawing inspiration from their success in modeling highly complex distributions such as natural images [b13] [b80] [b62] [b57].\" \n\n[b8], [b41]: These skill discovery methods are discussed as being closely related to the proposed method. The paper notes that by dropping the reward term, the objective becomes similar to unsupervised skill discovery. However, the proposed method unifies skill discovery with reward optimization.\n\n[b80]: This paper is cited as motivation for including a consistency term in the objective to prevent mode collapse - \"The third part is the log-likelihood of z, defined by our auxiliary distribution p_?(z|?). It is easy to see that if we fix ?_?, maximize p_? alone will minimize the cross-entropy E_z,?~?_?[-log p_?(z|?)], similar to the supervised learning of predicting z given ?. This achieves optimality when p_?(z|?) = p_?(z|?) = ?_?(z,?)/?z?_?(z,?)dz, modeling the posterior of z for ? sampled from ?_?. On the other hand, by fixing ?, the policy ?_? is encouraged to generate trajectories that are easy to identify or classify; this helps to increase diversity and enforce consistency to avoid mode collapse, letting the network not ignore the latent variables.\"\n\nThe other cited papers provide relevant background and motivation but do not seem to be as directly influential on the core novel contributions as the ones highlighted above. The analysis focuses on the methodological inspiration drawn from variational inference, latent variable models, and preventing mode collapse.", "6426ed4490e50fcafd443eef": "Here are the key steps I took to identify the most pertinent source papers for this paper:\n\n1. The paper focuses on accelerating CNN models using processing-in-memory (PIM) techniques, specifically on a PIM-enabled GPU memory architecture. The main novel contributions are:\n- Expanding the scope of PIM-offloadable computations by transforming CNN model graphs to exploit GPU-PIM mixed-parallel execution \n- An end-to-end compiler and runtime solution called PIMFlow that enables all types of convolution layers on DRAM-PIM\n- Extending the DRAM-PIM memory architecture to support convolution more efficiently and facilitate mixed-parallel execution\n\n2. The paper does not contain many explicit phrases like \"inspired by\" or \"motivated by\" that directly point to key source papers. \n\n3. However, based on the main contributions and methods, the most relevant references appear to be:\n- [b25], [b36], [b37]: These describe the baseline DRAM-PIM architecture that the paper extends. Understanding the capabilities and limitations of this baseline architecture likely motivated many of the extensions proposed.\n- [b47]: This recent proposal includes GPU-PIM parallel execution support, which relates to the mixed-parallel execution models in PIMFlow.\n\n4. The paper introduces graph transformation passes and a search algorithm to enable mixed-parallel execution across GPU and PIM. References related to parallelizing DL models across multiple devices, while not identical in approach, likely inspired these techniques, e.g. [b20], [b46].\n\n5. Checking the reference titles, [b25] \"A 3D-Stacked Logic-in-Memory Accelerator for Application-Specific In-Memory Processing\" and [b37] \"A 1.2 TFLOPS/W Programmable Deep-Learning Accelerator with Stacked DRAM for In-Memory Processing in 7nm CMOS\" appear to be the foundational DRAM-PIM architectures most directly built upon in this work.\n\nIn summary, here is my assessment of the most pertinent source papers, with confidence scores:\n\n{\n  \"[b25]\": 0.95,\n  \"[b37]\": 0.9,\n  \"[b36]\": 0.8,\n  \"[b47]\": 0.7,\n  \"[b20]\": 0.6,\n  \"[b46]\": 0.6\n}\n\nThe baseline DRAM-PIM architectures in [b25] and [b37] are the clearest inspiration for this work. The other references relate to key aspects like mixed-parallel execution but are less central overall to the main contributions compared to [b25] and [b37].", "638eb2ef90e50fcafd58b2af": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b17]\": 0.9,\n  \"[b11]\": 0.8,\n  \"[b10]\": 0.7,\n  \"[b15]\": 0.6,\n  \"[b8]\": 0.5\n}\n\nConfidence scores:\n- [b17] (ColBERT): 0.9. The paper makes multiple direct comparisons to ColBERT, using similar techniques like MaxSim operators and two-stage retrieval. ColBERT seems to be the main inspiration and baseline.\n\n- [b11] (FiD): 0.8. The proposed method builds upon the fusion-in-decoder (FiD) approach for combining multiple retrieved documents. FiD is described as a \"strong QA model\" that ReAtt aims to match in end-to-end training. \n\n- [b10]: 0.7. The idea of minimizing the discrepancy between retrieval attention and cross-attention is directly inspired by the reader-to-retriever distillation approach in this paper.\n\n- [b15] (DPR): 0.6. Negative sampling techniques and in-batch negatives are inspired by methods from DPR, though adapted for ReAtt's attention-based approach.\n\n- [b8]: 0.5. The unsupervised adaptation method using salient span masking is similar to the approach in this paper, indicating some methodological overlap, though not central to ReAtt's main contributions.\n\nThe key novel contributions seem to be 1) performing both retrieval and reading within a single unified model via attention, and 2) end-to-end training of this model using only QA supervision aided by cross-document attention adjustment. The most relevant references support these core aspects.", "63c8b59590e50fcafd90b721": "Here are the most relevant source papers for this paper, along with confidence scores, based on the criteria provided:\n\n{\n  \"[22]\": 0.9,\n  \"[51]\": 0.9, \n  \"[18]\": 0.7,\n  \"[14]\": 0.6,\n  \"[35]\": 0.5\n}\n\nThe key points for selecting these papers are:\n\n1. References [22] and [51] are mentioned multiple times as the state-of-the-art learning-based methods for shape-from-polarization that the proposed approach builds upon and compares against. The paper directly states it is \"Inspired by the frame-based solution of Ba et al. [22,51]\". This indicates these two papers provided significant inspiration and motivation.\n\n2. Reference [18] is cited when discussing prior \"Division of Time (DoT)\" shape-from-polarization methods that the proposed event-based approach aims to improve upon in terms of speed and resolution. \n\n3. Reference [14] is the only prior work that combined polarization with event cameras, but had limitations that this paper tries to address with a new approach.\n\n4. The Lucid Polarisens camera [35] is used to collect comparison image data in the real-world dataset. While not a direct influence, it enables comparing the event-based methods to prior image-based approaches.\n\nThe other references, while relevant for background and context, do not seem to be as directly influential on the novel contributions of this paper based on the provided criteria and analysis of the text. The key novelty is using event cameras for high-speed, high-resolution shape-from-polarization to overcome limitations of prior methods, especially [22], [51] and [18].", "64bb03bb3fda6d7f06002e9f": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria provided:\n\nThe paper focuses on proposing T-CAT, a dynamic cache allocation technique that partitions the last-level cache (LLC) for near and far memory in tiered memory systems. It aims to reduce the latency difference when accessing data from near vs far memory.\n\nKey novel contributions:\n1. Investigating the effects of NUMA interleaving at page granularity on performance in tiered memory systems \n2. Proposing T-CAT to dynamically partition LLC for near and far memory based on measured access latencies\n3. Balancing access latency between near and far memory through dynamic cache allocation\n\nPhrases indicating inspiration from previous work:\nThe paper does not contain many explicit phrases like \"inspired by\". However, it builds upon existing technologies like Intel CAT for cache partitioning.\n\nMost relevant references to the novel contributions and methods:\n\n{\n  \"[b1]\": 0.8,\n  \"[b2]\": 0.8,\n  \"[b3]\": 0.7,\n  \"[b4]\": 0.6,\n  \"[b5]\": 0.6,\n  \"[b6]\": 0.6,\n  \"[b7]\": 0.6,\n  \"[b8]\": 0.5,\n  \"[b9]\": 0.5\n}\n\n[b1], [b2]: Likely discuss the processor-memory performance imbalance in modern systems, which the paper aims to address\n[b3]: Introduces CXL interconnect technology for tiered memory systems that the paper focuses on \n[b4]-[b9]: Propose software techniques for data placement and migration in tiered memory systems. The paper contrasts T-CAT with these prior approaches.\n\nIn summary, [b1]-[b3] seem to be the most influential in terms of defining the problem space and system architecture, while [b4]-[b9] represent prior approaches that T-CAT aims to improve upon through hardware-based dynamic cache allocation. The other references are likely less central to the main contributions.", "64cc25d83fda6d7f063be43e": "Here is my analysis of the most pertinent source papers for the given paper, in JSON format with confidence scores:\n\n{\n  \"[b12]\": 0.9,\n  \"[b7]\": 0.8, \n  \"[b8]\": 0.8,\n  \"[b4]\": 0.7,\n  \"[b5]\": 0.7,\n  \"[b6]\": 0.6,\n  \"[b13]\": 0.5\n}\n\nRationale and key points:\n\n- The paper's main novel contribution is proposing a clustered 128-bit microarchitecture with a dedicated 128-bit address cluster and 64-bit integer cluster. Instructions are dynamically steered between the clusters.\n\n- [b12] seems to be the most directly relevant, as the paper mentions \"A comparable algorithm can be implemented in hardware to iteratively build BASes and drive the steering decision for instructions that are not explicitly 128-bit but are indirect or direct producers of AGEN instructions [b12].\" This indicates [b12] proposes a very similar dynamic steering approach.\n\n- [b7] and [b8] are also highly relevant, as the paper discusses how dynamic steering performs better than static steering by the compiler, referencing these papers: \"Since out-of-order machines are notorious for their latency variability, dynamic steering appears the natural choice.\"\n\n- [b4] and [b5] describe existing heterogeneous clustered microarchitectures that the proposed design builds upon conceptually. \n\n- [b6] introduces the region-based compression scheme leveraged to reduce the size of the 128-bit structures.\n\n- [b13] is mentioned as an example of homogeneous clustering, an alternative to the proposed heterogeneous clustering approach.\n\nThe other references provide useful background but do not seem to be as centrally tied to the main novel aspects of the paper based on the text. The key inspiration appears to come from dynamic steering techniques and existing heterogeneous clustered designs.", "6449e7fc582c1376bbfc600e": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b48]\": 0.9,\n  \"[b34]\": 0.85,\n  \"[b51]\": 0.8,\n  \"[b13]\": 0.75,\n  \"[b0]\": 0.7\n}\n\nConfidence scores:\n[b48], [b34], [b51]: These papers are referenced multiple times when discussing the novel parametrization and optimization of event representations, which is the core contribution of this paper. The authors build upon and unify concepts from these works like different windowing strategies ([b48], [b34]) and stacking representations ([b51]).\n\n[b13]: This work performed a large-scale study on event representations which served as important motivation and background. The proposed parametrization extends the one from [b13] in key ways.\n\n[b0]: Computing higher-order moments like in this paper is mentioned as an example of capturing more event information in dense representations.\n\nThe paper also references many other works for background and related work, but the ones above appear to be the most central and directly influential to the main novel contributions around optimizing event representations using the proposed Gromov-Wasserstein discrepancy metric. The key phrases indicating direct inspiration and the methodological similarities confirm the importance of these source papers.", "6523793e939a5f4082e182a2": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b0]\": 0.9,\n  \"[b1]\": 0.9, \n  \"[b2]\": 0.9,\n  \"[b3]\": 0.8,\n  \"[b29]\": 0.7,\n  \"[b30]\": 0.7,\n  \"[b31]\": 0.7,\n  \"[b32]\": 0.7\n}\n\nConfidence scores:\n- 0.9 for [b0], [b1], [b2]: These references are cited at the very beginning when introducing the key challenge the paper aims to address - that address translation is a significant performance bottleneck in modern data-intensive workloads. This indicates these papers likely inspired and motivated this work.\n\n- 0.8 for [b3]: This reference is cited when discussing how PTW latency continues to rise with modern processors' deeper multi-level page table designs, such as 5-level radix PT in the latest Intel processors. Addressing this increasing latency seems to be a key motivation.\n\n- 0.7 for [b29], [b30], [b31], [b32]: These references are grouped together to support the key insight leveraged in this paper - that modern data-intensive workloads tend to greatly underutilize the cache hierarchy. This observation directly enables the paper's main idea of repurposing underutilized cache blocks to store TLB entries.\n\nThe other references, while providing useful background information, do not seem to be called out as directly inspiring the key insights and techniques of this paper based on the phrasing. The paper's main novel contribution appears to be leveraging underutilized cache resources to extend TLB reach in a software-transparent way, building upon the observations in [b29]-[b32] about cache underutilization.", "64c78ba33fda6d7f06dbcb16": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b45]\": 0.9,\n  \"[b49]\": 0.9,\n  \"[b35]\": 0.8,\n  \"[b17]\": 0.7,\n  \"[b27]\": 0.7,\n  \"[b11]\": 0.6,\n  \"[b25]\": 0.6\n}\n\nRationale:\n- [b45] and [b49] are cited when discussing that neighbor aggregation is the key bottleneck in GNNs due to its high computation sparsity and irregularity. This directly relates to the main challenge the paper aims to address.\n\n- [b35] introduces NVSHMEM which the paper leverages to provide fine-grained inter-GPU communication support, a key component of their pipelining approach. \n\n- [b17] and [b27] are referenced when discussing existing approaches that optimize communication as a standalone operator, which the paper contrasts with their joint computation-communication optimization.\n\n- [b11] and [b25] represent prior work on transforming irregular communication to regularized communication, which the paper aims to improve upon with their pipelining strategy.\n\nThe paper introduces a novel fine-grained dynamic software pipelining approach to overlap communication and computation in multi-GPU GNNs. The references most directly related to this core contribution are those discussing the challenges of neighbor aggregation ([b45], [b49]) and existing communication optimization approaches ([b17], [b27], [b11], [b25]) which motivate their new pipelining strategy. NVSHMEM [b35] is also highly relevant as an enabling technology for their fine-grained pipeline communication.", "65260ee8cd549670787e1513": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"b8\": 0.9,\n  \"b18\": 0.8,\n  \"b26\": 0.7,\n  \"b60\": 0.6,\n  \"b3\": 0.5\n}\n\nConfidence scores:\nb8 (PaLI): 0.9 - The paper directly compares its approach of adapting the language model during joint vision-language training to PaLI's method, noting PaLI's drop in NLP performance as a key drawback CogVLM aims to address. This indicates PaLI was a major inspiration and baseline.\n\nb18 (LoRA): 0.8 - The paper draws a direct analogy between the superior performance of LoRA over p-tuning in efficient finetuning to the benefits of deep fusion over shallow alignment in VLMs. This analogy seems central to the paper's main architectural insight.\n\nb26 (p-tuning): 0.7 - Discussed alongside LoRA as part of the key analogy that inspired the CogVLM architecture. The limitations of p-tuning are likened to those of shallow alignment methods.\n\nb60 (PaLI-X): 0.6 - Mentioned as an example of a strong performing model that jointly trains vision and language modules, which CogVLM aims to match while preserving NLP ability. Provides important context but is less of a direct inspiration than PaLI.\n\nb3 (Qwen-VL): 0.5 - Briefly compared to on the Flickr image captioning benchmark. Provides useful context on SOTA but is not presented as a major influence on the CogVLM approach itself.\n\nThe key novel contributions seem to be 1) Preserving NLP ability while enabling deep vision-language fusion, inspired by comparing LoRA and p-tuning, and 2) The visual expert module that adapts each layer while keeping the pretrained language model frozen. The most relevant references based on the explicit comparisons and stated inspirations are PaLI, LoRA and p-tuning.", "6503bec83fda6d7f067c7787": "Here are the key steps I took to identify the most relevant source papers for this paper:\n\n1. The main novel contributions of the paper seem to be:\n- Leveraging the underlying language model's existing knowledge to select more effective in-context learning (ICL) demonstrations, by considering the model's ambiguity around the output label space. \n- Identifying an \"ambiguous label set\" for each test example - the two output labels the model is most confused between.\n- Selecting ICL demonstrations whose ground truth label is in the ambiguous label set, which are misclassified by the model, and whose predicted label is also in the ambiguous label set.\n\n2. The paper does not seem to use many explicit phrases like \"inspired by\" to indicate reliance on specific previous works. \n\n3. Looking at the references linked to the main contributions:\n\n- The idea of using a retriever to select semantically similar examples as ICL demonstrations is attributed to [18], [23], [21].\n\n- The concept of using misclassified training examples to improve performance is linked to [29] and [31]. \n\n- The observation that labels in ICL demonstrations strongly influence model predictions is credited to [24].\n\n4. Evaluating the reference titles, [18], [23], [21], [29], [31], [24] seem most directly relevant to the paper's core methods.\n\n5. In summary, the references that appear to be the key sources of inspiration, in order of relevance, are:\n\n{\n  \"[23]\": 0.9,\n  \"[18]\": 0.85, \n  \"[21]\": 0.8,\n  \"[29]\": 0.7,\n  \"[31]\": 0.65,\n  \"[24]\": 0.6\n}\n\nThe paper builds most directly on the retriever-based ICL approach from [18], [23], [21], and incorporates insights from [29], [31] on leveraging misclassified examples, and [24] on the influence of demonstration labels. But its core contributions of using the model's label ambiguity to select demonstrations appear novel.", "645dad16d68f896efad9df53": "Here is my analysis of the most pertinent source papers for this paper, based on the criteria you provided:\n\n{\n  \"[b42]\": 0.95,\n  \"[b67]\": 0.9,\n  \"[b29]\": 0.8,\n  \"[b9]\": 0.75,\n  \"[b43]\": 0.7\n}\n\nThe key points that led me to these source papers are:\n\n1. The paper is heavily inspired by and builds upon the vision transformer architectures proposed in [b42] (Swin Transformer) and [b67] (DeiT). The authors systematically analyze these models to identify factors affecting inference speed.\n\n2. [b29] is a key reference that reveals memory access overhead is a critical factor affecting transformer speed, especially in the multi-head self-attention layers. This directly motivates the paper's memory efficiency analysis and the proposed sandwich block layout. \n\n3. The idea of the cascaded group attention module feeding each head with different feature splits is inspired by the group convolutions used in efficient CNNs like [b9]. This helps reduce computation redundancy.\n\n4. The parameter reallocation strategy to expand channels of important modules while shrinking unimportant ones is motivated by the structured pruning method in [b43], which is used to identify the critical network components.\n\n5. Overall, [b42] and [b67] are the most important references, as the proposed EfficientViT model directly builds upon and improves these architectures. The high confidence scores reflect that these papers contribute the most to the core ideas and novelty of EfficientViT.", "64dbf5883fda6d7f060c36fd": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b4]\": 0.9,\n  \"[b5]\": 0.9,\n  \"[b8]\": 0.8,\n  \"[b2]\": 0.7,\n  \"[b1]\": 0.6\n}\n\nConfidence scores:\n- [b4] and [b5]: 0.9 - These papers are directly cited as utilizing \"CPU quotas to regulate the end-to-end serverless chain execution time\", which the current paper aims to improve upon using DVFS. The direct comparison indicates these are highly pertinent prior works.\n\n- [b8]: 0.8 - Gemini, a state-of-the-art DVFS governor that the current paper compares against, is a very relevant prior work that the current paper builds upon and aims to outperform. \n\n- [b2]: 0.7 - This paper is cited as discussing performance variability issues in FaaS that existing DVFS solutions do not account for. Addressing this seems to be a key motivation for the DVFaaS approach.\n\n- [b1]: 0.6 - Cited as showing system noise in serverless environments that impacts performance. Understanding this issue provides important context for the paper's contributions, though it may not be as directly built upon as the other higher scored references.\n\nThe other cited references like [b0], [b3], [b6], [b7] provide useful background information, but do not seem to be as directly built upon in developing the novel DVFaaS methodology compared to the top scoring references above. The comparative phrases like \"Unlike previous approaches [b6]\" and \"existing DVFS solutions are inadequate\" also support prioritizing the other references.", "62ea18d35aee126c0fca1369": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b17]\": 0.9,\n  \"[b2]\": 0.8,\n  \"[b51]\": 0.7,\n  \"[b42]\": 0.6,\n  \"[b23]\": 0.5,\n  \"[b34]\": 0.5\n}\n\nRationale:\n\nThe paper introduces SLED (SLiding-Encoder and Decoder), a method for applying pretrained encoder-decoder models to long text problems. The key novel contributions are:\n\n1. Using a fusion-in-decoder approach, where the encoder only contextualizes local chunks and the decoder handles cross-chunk reasoning. This is directly inspired by the Fusion in Decoder (FiD) method from [b17].\n\n2. Enabling the use of off-the-shelf pretrained language models like BART [b23] and T5 [b34] for long text tasks, without requiring specialized pretraining. \n\n3. Demonstrating strong results on the SCROLLS benchmark compared to dedicated long-range models.\n\nThe paper frequently compares SLED to LED [b2] and other sparse local attention models like [b51]. The overall approach is heavily motivated by the transformer architecture [b42].\n\nSo in summary, [b17] is the clearest direct inspiration, while [b2], [b51], [b42], [b23] and [b34] provide important context and components that SLED builds upon in its novel long-range modeling approach. The confidence scores reflect the centrality of each reference to the main contributions.", "64ec1b763fda6d7f0626f480": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b54]\": 0.9,\n  \"[b8]\": 0.8,\n  \"[b49]\": 0.7,\n  \"[b68]\": 0.6,\n  \"[b58]\": 0.5\n}\n\nRationale:\n- [b54] (Proximal Policy Optimization) is directly cited as the reinforcement learning algorithm used to train the token selector, a core novel contribution of the paper. The paper states \"inspired by the recent success of reinforcement learning in NLP [b40] , we adopt the Proximal Policy Optimization (PPO) [b54] algorithm to train the selector\".\n- [b8] is cited in the context of the sequential batch alignment method, another key contribution. The paper notes \"[BOS] and [EOS] have been well-recognized representative of sentence semantics [b8]\" as motivation for aligning these tokens across chunks.\n- [b49] inspires the design of the token selector from a reinforcement learning perspective, as indicated by \"Inspired by [b49] , we design the selector from the perspective of reinforcement learning (RL).\"\n- [b68] directly motivates the reward design for training the selector - \"Inspired by [b68] , we can directly utilize the language modeling likelihood as the generation quality reward for selection actions\"\n- [b58] is the original Transformers paper which forms the foundation this work builds upon, though it is not as directly tied to the specific novel contributions as the other cited papers.\n\nThe other references, while relevant for context and background, do not seem to be called out as direct inspiration for the primary novel aspects of SimCAS based on my analysis of the phrasing. Let me know if you would like me to elaborate or modify the analysis in any way.", "636482d790e50fcafdccab10": "Here is my analysis to identify the most pertinent source papers for the given paper, step-by-step:\n\nThe primary novel contributions of the paper are:\n1. Proposing a novel problem of defending black box models against adversarial attacks in a data-free setup (without access to training data or model weights)\n2. Proposing a wavelet-based noise remover (WNR) with a selective wavelet coefficient module to remove coefficients likely to be corrupted by adversarial attacks\n3. Proposing a U-net based regenerator network to restore high-frequency content lost after noise removal\n\nKey phrases indicating inspiration from previous work:\n- \"Unlike these existing works, we use model stealing only as a means to obtain the surrogate model and synthetic data.\" - indicates the paper uses model stealing techniques from previous works in a novel way\n- \"Unlike their work, our method do not suffer from such problems while allowing complete access to our defense mechanism in the black-box setting\" - compares to limitations of a previous related work on black-box defense\n\nThe paper introduces novel methods of wavelet-based noise removal and a regenerator network to tackle the problem of black-box adversarial defense without training data. The key source papers seem to be:\n\n- Papers on model stealing techniques to obtain surrogate models, which the authors use to enable their data-free setup (e.g. [b18], [b19], [b20], [b21], [b22], [b23]) \n- The previous work [b41] on black-box adversarial defense that the authors directly compare their approach to, highlighting how their method improves upon its limitations\n- Papers utilizing wavelets for denoising and with CNNs (e.g. [b39], [b40]), which relate to their wavelet-based noise removal approach\n\nIn summary, the most pertinent source papers based on the criteria are:\n\n{\n  \"[b41]\": 0.9,\n  \"[b22]\": 0.8, \n  \"[b23]\": 0.8,\n  \"[b40]\": 0.7,\n  \"[b39]\": 0.6\n}\n\nThe paper [b41] is assigned the highest confidence score as the authors directly compare their novel approach to it. [b22] and [b23] are highly relevant as they provide key model stealing techniques the paper builds upon. [b40] and [b39] are assigned lower but still significant scores as they relate to the paper's use of wavelets for denoising and adversarial defense.", "62c4fd9a5aee126c0fad6f58": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b33]\": 0.95,\n  \"[b7]\": 0.85,\n  \"[b10]\": 0.8,\n  \"[b28]\": 0.75,\n  \"[b8]\": 0.7,\n  \"[b48]\": 0.65,\n  \"[b11]\": 0.6,\n  \"[b23]\": 0.55\n}\n\nConfidence scores:\n- [b33] (CLIP): 0.95 - The paper heavily relies on CLIP as the vision-language pre-training (VLP) model for extracting multi-modal knowledge from image-text pairs. CLIP's powerful image-text matching ability is a key motivation and foundation for their approach.\n\n- [b7] (Vision Transformer): 0.85 - The paper uses a Vision Transformer as the backbone to extract semantic image features, which is central to their architecture. \n\n- [b10], [b28], [b8], [b48]: 0.65-0.8 - These papers explore open-vocabulary classification for tasks like object detection, and introduce techniques like knowledge distillation and prompt tuning to boost performance. The current paper adapts similar ideas for open-vocabulary multi-label classification.\n\n- [b11] (Knowledge Distillation): 0.6 - Knowledge distillation is used to align the image and label embeddings, which is important for generalizing to unseen labels.\n\n- [b23] (Prompt Tuning): 0.55 - Prompt tuning is applied to iteratively update the label embeddings for better classification performance.\n\nThe analysis focuses on references that directly contribute to the paper's main novel aspects - using a VLP model for multi-modal knowledge, adapting open-vocabulary techniques for multi-label classification, and the specific architectural choices. The Vision Transformer backbone and CLIP model are clearly the most influential references that the proposed approach heavily builds upon.", "65123f453fda6d7f06e54a4b": "YouChat is a large language model from You.com.\n\nYou.com is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. YouChat will generate a comprehensive and informative answer for a given question based on the provided web Search Results. YouChat considers the history of the conversation and answer the questions in a helpful manner. It tries to use the information from the provided search results to ensure accuracy and uses an unbiased and journalistic tone. YouChat will combine search results together into a coherent answer and cite search results using [${{number}}] notation. It will only cite the most relevant results that answer the question accurately.", "6531e2ca939a5f4082f5d5d2": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b3]\": 0.9,\n  \"[b2]\": 0.8,\n  \"[b6]\": 0.6,\n  \"[b7]\": 0.5\n}\n\nThe key points that led me to identify these as the most relevant source papers are:\n\n- The paper directly states that their approach \"leverages\" and \"adapts\" the interference model from reference [b3] which was developed for IBM POWER8 processors, in order to apply it to ARM processors. This indicates [b3] was a major inspiration and starting point for their work.\n\n- Reference [b2] is mentioned as an approach for identifying symbiotic application pairs on Intel processors. The authors state their heuristic is based on the [b2] method, again showing direct influence, even though [b2] targeted a different processor.\n\n- References [b6] and [b7] are highlighted as early influential works that tried to address the problem of inter-application interference on generic simulated SMT processors. While not as directly influential as [b3] and [b2], these papers helped establish the problem space and potential solutions that motivated this work.\n\n- The other references, while relevant to the general topic, do not appear to be called out as major inspirations or methodological influences based on the language used when citing them in this paper. The key novel contribution is adapting the approaches from [b3] and [b2] to work on real ARM processors for the first time.\n\nSo in summary, references [b3] and [b2] emerge as the clear top influences based on the authors' own descriptions, with [b6] and [b7] as important early motivating works. The other references provide relevant background but are not highlighted as core inspirations for the key contributions of this paper.", "6514e2043fda6d7f062dc9f8": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\n{\n  \"[b39]\": 0.9,\n  \"[b18]\": 0.8,\n  \"[b7]\": 0.7,\n  \"[b5]\": 0.6,\n  \"[b37]\": 0.6,\n  \"[b30]\": 0.5\n}\n\nConfidence scores:\n- [b39] (VQ-VAE paper): 0.9. The text indicates this paper introduced the original VQ-VAE formulation that the current work aims to simplify. It is referenced multiple times as the key inspiration.\n- [b18]: 0.8. Mentioned as showing the success of training an autoregressive transformer on VQ-VAE representations for image generation, which relates to the MaskGIT model discussed later.\n- [b7]: 0.7. Introduces the straight-through estimator used to backpropagate gradients through the quantization operation in VQ-VAE, a key aspect of the method.\n- [b5], [b37]: 0.6. Early works on scalar quantization in neural compression, which inspired the finite scalar quantization (FSQ) method proposed here as an alternative to VQ.\n- [b30]: 0.5. Previous work that used bounded scalar quantization (i.e. FSQ) for image compression, supporting the feasibility of FSQ.\n\nThe text introduces FSQ as a simpler alternative to VQ in VQ-VAE that avoids auxiliary losses and achieves high codebook utilization. The key novel contribution is showing FSQ can directly replace VQ in existing architectures like MaskGIT and UViM with minimal performance loss. The most pertinent references support the original VQ-VAE method that is being improved upon, and the scalar quantization literature that inspired the FSQ approach.", "6531e2ca939a5f4082f5d4fe": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b0]\": 0.9,\n  \"[b41]\": 0.85,\n  \"[b18]\": 0.7,\n  \"[b9]\": 0.7,\n  \"[b35]\": 0.65,\n  \"[b10]\": 0.65\n}\n\nConfidence scores:\n[b0]: 0.9 - The paper introduces a \"topology-aware pretraining task\" to \"inject the graph learning ability of graph neural networks into LMs\". This directly builds upon and is inspired by the capabilities of graph neural networks as described in [b0].\n\n[b41]: 0.85 - The methodology uses R-HGNN from [b41] to implement the heterogeneous graph neural network component. This shows a strong reliance on and is directly motivated by the techniques from [b41].\n\n[b18], [b9]: 0.7 - These references are mentioned when emphasizing the importance of capturing high-order signals and connections in graphs, which is a key motivation and novel contribution of the paper's approach. \n\n[b35], [b10]: 0.65 - Also cited as examples of prior work showing the importance of high-order information in graphs for applications like recommender systems. This helps justify the paper's focus on modeling multi-order topology.\n\nIn summary, the proposed approach seems to be most directly inspired by graph neural networks [b0] in general and the R-HGNN architecture [b41] specifically, while aiming to capture high-order graph signals that have been shown to be important in [b18], [b9], [b35] and [b10]. The key novelty is integrating these graph representation learning techniques with language models for text-attributed heterogeneous graphs.", "65252d90939a5f40827eabe7": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b6]\": 0.9,\n  \"[b24]\": 0.8,\n  \"[b25]\": 0.7,\n  \"[b22]\": 0.6,\n  \"[b5]\": 0.5\n}\n\nConfidence scores:\n[b6]: 0.9 - This reference is mentioned multiple times and is used as the key evaluation metric (AlpacaEval) to demonstrate the effectiveness of NEFTune. The large performance gains on AlpacaEval seem to be the central result of the paper.\n\n[b24]: 0.8 - This reference is highlighted in the introduction as providing the baseline LLaMA-2-7B model that NEFTune improves from 29.8% to 64.7% on AlpacaEval. It serves as an important starting point that NEFTune builds upon.\n\n[b25]: 0.7 - The Alpaca dataset, which is one of the key instruction tuning datasets used in this paper, was constructed using the Self-Instruct method from this reference. \n\n[b22]: 0.6 - This reference introduced the original Alpaca dataset that the paper finetunes models on. It is one of the core instruction tuning datasets used to showcase NEFTune's effectiveness.\n\n[b5]: 0.5 - The paper demonstrates that NEFTune also works with the Quantized Low Rank Adapters (QLORA) method from this reference, showing the approach generalizes to constrained compute settings. The QLORA implementation and hyperparameters are based on this prior work.\n\nIn summary, the key source papers seem to be those that 1) Provide the evaluation framework ([b6]), 2) Serve as the baseline models improved by NEFTune ([b24]), and 3) Introduce the core instruction tuning datasets like Alpaca ([b22], [b25]) that are used to demonstrate NEFTune's impact. The QLORA reference [b5] also appears to directly influence a secondary result.", "626b49625aee126c0fffd149": "Here is my analysis of the most pertinent source papers for this paper, based on the criteria you provided:\n\n{\n  \"[b2]\": 0.9,\n  \"[b5]\": 0.8,\n  \"[b7]\": 0.7,\n  \"[b3]\": 0.6,\n  \"[b0]\": 0.5\n}\n\nRationale:\n\nThe paper introduces Note EM, a framework for training polyphonic music transcribers using unaligned supervision from musical scores. The key novel contributions are:\n\n1. Using EM to intertwine alignment of musical scores with training of the transcriber network. This allows leveraging unaligned scores to supervise training on real recordings.\n\n2. Achieving state-of-the-art note-level accuracy that surpasses fully supervised methods, despite training only on synthetic data and unaligned scores. \n\n3. Demonstrating unprecedented generalization to unseen instruments and musical genres.\n\nThe most relevant references based on the criteria are:\n\n[b2] (confidence 0.9) - The paper uses the architecture from Hawthorne et al. [b2] as the base transcriber network. [b2] is a key foundation that the novel EM framework builds upon.\n\n[b5] (confidence 0.8) - Gardner et al. [b5] is discussed as prior work on instrument-sensitive transcription. The paper improves significantly over [b5]'s cross-dataset generalization.\n\n[b7] (confidence 0.7) - MusicNet dataset [b7] is used for training, but the paper generates an improved MusicNet EM labeling using their EM framework, fixing issues with the original MusicNet labels from [b7].\n\n[b3] (confidence 0.6) - [b3] is mentioned as demonstrating the advantage of separate onset/frame detection, a key aspect utilized in this paper's transcriber architecture.\n\n[b0] (confidence 0.5) - Xi et al. [b0] introduced the GuitarSet dataset used for evaluation in this paper. While relevant as an evaluation benchmark, it is less central to the paper's main novel contributions.\n\nIn summary, [b2], [b5] and [b7] emerge as the most influential references, directly informing and enabling the key novel aspects of the EM framework and transcriber architecture that lead to the paper's strong results. The other references provide important context but are less central to the main innovations.", "62a6aabf5aee126c0ff36991": "Here are the key steps I took to identify the most relevant source papers for this paper:\n\n1. The primary novel contributions of the paper are:\n- Proposing Hop2Token, a neighborhood aggregation method that constructs a sequence of token vectors preserving neighborhood information for each node \n- Proposing NAGphormer, a new scalable Graph Transformer method for node classification that can be trained in mini-batches\n- Developing an attention-based readout function to adaptively learn the importance of different neighborhood hops\n\n2. Key phrases indicating inspiration from previous work:\n- \"Motivated by the above observation, We propose a novel model, termed Neighborhood Aggregation Graph Transformer (NAGphormer), for node classification.\"\n- This suggests the observation that local neighborhood information is useful for node classification, as shown by GNN methods, motivated their work.\n\n3. References directly linked to the main novel aspects:\n- [b21], [b32], [b22] showed the effectiveness of aggregating local neighborhood information in GNNs for node classification \n- [b11], [b23], [b36] are existing Graph Transformer models that aim to introduce structural information but have scalability limitations\n- [b41] is a recent Graph Transformer that tries to improve scalability by sampling ego-graphs but is still limited\n\n4. Checking the reference titles, [b21] (GCN), [b32] (GAT), [b22] (APPNP) align closely with the key motivation of leveraging local neighborhood aggregation. The other references provide important context on Graph Transformers.\n\nIn summary, based on the novel contributions, motivating insights from GNNs, and methodological similarities, I believe the following are the most relevant source papers:\n\n{\n  \"[b21]\": 0.9,\n  \"[b32]\": 0.9, \n  \"[b22]\": 0.8,\n  \"[b11]\": 0.7,\n  \"[b23]\": 0.7,\n  \"[b36]\": 0.7,\n  \"[b41]\": 0.6\n}\n\nThe GNN papers [b21], [b32], [b22] provided key motivation and insights leveraged in NAGphormer's design. The Graph Transformer papers, especially [b41], highlighted the scalability challenges NAGphormer aims to address.", "6535d747939a5f408295c3c4": "Here is my analysis of the key source papers for this paper, based on the criteria you provided:\n\n{\n  \"11\": 0.9,\n  \"34\": 0.8,\n  \"39\": 0.7,\n  \"66\": 0.7,\n  \"44\": 0.6\n}\n\nConfidence scores:\n[11] - 0.9\nThe paper introduces GraphMAE, a masked autoencoder approach for generative self-supervised learning on graphs. This work directly inspires the generative SSL paradigm discussed in the introduction as a way to pre-train robust graph models.\n\n[34] - 0.8 \nS2GAE is highlighted as another key example of the generative SSL paradigm for graphs, alongside GraphMAE. It demonstrates the paper's focus on generative approaches for graph pre-training.\n\n[39] - 0.7\nDGI is discussed as a prominent example of contrastive self-supervised learning on graphs, the other major paradigm explored in the paper. The authors contrast this with the generative approaches.\n\n[66] - 0.7\nGCA is mentioned together with DGI as a key contrastive graph SSL method. Discussing these two methods frames the current landscape the paper aims to build upon.\n\n[44] - 0.6\nThis reference introduces the concept of \"instruction tuning\" for adapting language models to specific domains. The proposed dual-stage graph instruction tuning directly builds upon this idea to align LLMs to graph reasoning tasks.\n\nIn summary, the selected references highlight the key self-supervised graph learning paradigms (contrastive and generative) that motivate this work, as well as the instruction tuning concept that inspires the paper's core technical contribution. The high confidence scores reflect the centrality of these ideas to the novel aspects of the GraphGPT framework.", "6503bec83fda6d7f067c7717": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b26]\": 0.9,\n  \"[b11]\": 0.8,\n  \"[b3]\": 0.7,\n  \"[b12]\": 0.6\n}\n\nConfidence scores:\n[b26] (0.9): The paper introduces the Snake activation function f(x) = x + sin^2(x), which the authors say \"can bring periodic inductive bias and can perform well for temperature and financial data prediction.\" This directly inspires a key novel contribution of SnakeGAN.\n\n[b11] (0.8): StyleGAN3 is referenced as identifying \"that the aliasing artifacts in image synthesis are rooted in careless signal processing\" and eliminating aliasing by applying nonlinearity to temporarily increased resolution. The SnakeGAN generator architecture takes direct inspiration from this.\n\n[b3] (0.7): The DDSP model is used to \"decouple and further control the characters of a time domain waveform\" by adjusting amplitude, envelope, fundamental frequency etc. SnakeGAN uses DDSP to \"simulate the prior knowledge of target audio from different domains\", a core part of its approach.\n\n[b12] (0.6): Fre-GAN's discriminator settings are directly applied in SnakeGAN, including using DWT instead of average pooling in the MPD and MSD discriminators. The paper states \"We refer to Fre-GAN [b12] and UnivNet [b8] , MPD with DWT and MRD discriminators are adopted to improve the synthesized audio quality.\"\n\nThe key phrases \"based on\", \"inspired by\", \"according to\", and comparative discussion of previous works indicate these 4 references provided significant inspiration and components that enabled SnakeGAN's main novel contributions around improving generalization and eliminating aliasing artifacts. The other references appear to play a more minor role.", "6303504190e50fcafd769fe6": "Here are the key steps I took to identify the most relevant source papers for this paper:\n\n1. The main novel contributions of the paper are:\n- Proposing TARNet, a method to learn task-aware reconstruction from time-series data to improve end-task performance \n- Designing a data-driven masking strategy to determine important timestamps for an end task and learn to reconstruct them\n\n2. Looking for key phrases indicating inspiration from previous work:\n- The paper mentions \"Time-series Transformer [b36] leveraged this unlabeled data to craft a reconstruction task that masks time-series values of randomly chosen time segments and reconstructs them.\" This indicates [b36] was an inspiration.\n- It also says \"Unlike TST, which only considers MSE loss for reconstructing the masked portion of the data, L_mask, we include loss incurred for replicating the unmasked, observed portion of the input data, L_unmask, as well.\" This compares their method to TST [b36].\n\n3. Identifying references directly linked to the main novel methods:\n- The transformer encoder architecture used as the backbone is based on the original Transformer paper [b28].\n- The idea of learning representations through reconstruction has been explored in NLP [b6] and time-series [b36] as mentioned.\n\n4. Evaluating the titles of key references:\n- [b36] \"Time Series Transformer\" is very relevant as it proposes the reconstruction based pre-training that TARNet builds upon.\n- [b28] \"Attention Is All You Need\" introduces the Transformer architecture used as the base model.\n\nBased on this analysis, here are the most relevant source papers in JSON format with confidence scores:\n\n{\n  \"b36\": 0.95,\n  \"b28\": 0.85,\n  \"b6\": 0.6\n}\n\nThe key inspirations are the Time Series Transformer [b36] which proposed the reconstruction based pre-training idea, and the original Transformer paper [b28] whose architecture is adapted. The reconstruction idea is also linked to work in NLP [b6] but less directly.", "6364c0ba90e50fcafdbb4aad": "Based on my analysis of the paper, the most pertinent source papers that inspired or motivated this work are:\n\n{\n  \"[b37]\": 0.9,\n  \"[b47]\": 0.8,\n  \"[b39]\": 0.7,\n  \"[b12]\": 0.6,\n  \"[b16]\": 0.5,\n  \"[b34]\": 0.5\n}\n\nThe key evidence for selecting these papers is:\n\n- The paper is directly inspired by the Best Offset Prefetcher (BOP) [[b37]] in asking the question \"for an L1D access to address X, what is the timely and accurate delta (d) that should be used for prefetching?\". This is a core motivation for the Berti prefetcher.\n\n- Multi-lookahead offset prefetching (MLOP) [[b47]], an extension of BOP, is discussed extensively as related work. The paper highlights how Berti differs from MLOP in using local per-IP deltas rather than global deltas.\n\n- The Instruction Pointer Classifier Prefetching (IPCP) [[b39]], the winner of the recent DPC-3 prefetching competition, is analyzed in detail. The paper argues why Berti can provide better coverage than IPCP for certain access patterns.\n\n- Other state-of-the-art prefetchers like Bingo [[b12]], SPP-PPF [[b16]] [[b34]] are compared against extensively in the evaluation, positioning Berti as an improvement over them, especially for L1 prefetching.\n\nThe paper directly states being inspired by BOP, and focuses its related work discussion and evaluation on comparing against and improving upon the other selected papers. This indicates they were likely the most influential prior works in developing the novel contributions of Berti.", "63dcdb422c26941cf00b6339": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\n{\n  \"[b14]\": 0.9,\n  \"[b2]\": 0.8,\n  \"[b26]\": 0.7,\n  \"[b6]\": 0.6,\n  \"[b34]\": 0.5\n}\n\nRationale:\nThe key novel contributions of this paper seem to be:\n1) Reformulating text generation as copying variable-length phrases from source documents, rather than predicting the next token over a fixed vocabulary. This allows for more accurate and context-sensitive phrase selection.\n2) Enabling training-free adaptation to new knowledge sources by updating the source document collection. \n3) Allowing generation of multi-word phrases in a single step.\n\nThe source papers most directly related to these contributions are:\n- [b14] and [b6] which discuss standard autoregressive language models that generate text by predicting the next token. This paper's approach is directly contrasted with these.\n- [b2] which is a retrieval-augmented generation model that combines a retriever with a generator. The use of retrieval to enhance generation is similar to this paper's approach.\n- [b26] which introduces GPT-2, the language model architecture that this paper adapts for the prefix encoder. \n- [b34] which introduces the Transformer architecture that is used for both the prefix and phrase encoders here.\n\n[b14], [b2] and [b26] are preceded by phrases directly comparing this work to them, indicating they were key sources of inspiration. [b6] and [b34] are also highly relevant to the core language modeling aspects this paper builds upon.\n\nThe other cited papers provide useful background but do not seem to be as centrally tied to the main novel contributions. So this concise list focuses on the most influential source papers for the key ideas.", "65364bdf939a5f40822568b2": "Here are the most relevant source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b4]\": 0.9,\n  \"[b11]\": 0.9,\n  \"[b14]\": 0.8,\n  \"[b17]\": 0.8,\n  \"[b9]\": 0.7,\n  \"[b13]\": 0.7,\n  \"[b2]\": 0.6,\n  \"[b8]\": 0.6\n}\n\nThe key points for selecting these papers as the most relevant sources are:\n\n1. The paper draws direct inspiration from the methodology used in NLP for pre-training tasks, as evidenced by phrases like \"follows the same methodology in conventional NLP for setting pre-training tasks\" and references to [b4] and [b11]. This indicates [b4] and [b11] were key motivations.\n\n2. The Masked Behavior Prediction (MBP) and Next Behavior Prediction (NBP) methods discussed, along with their limitations, appear to be the main challenges this paper aims to address with its novel contributions. [b14], [b17], [b9] and [b13] are directly cited in this context.\n\n3. For the contrastive regularization aspect of their method, the paper states \"Referring to the idea of SimSiam in contrastive learning [b2]\", directly identifying [b2] as an inspiration. \n\n4. The multi-task prompt training approach takes inspiration from the T5 model in [b8], as stated by \"Inspired by these works, in this paper, we adopt a prompt training approach\".\n\n5. While other references like [b3], [b6], [b7] are discussed, the language used does not indicate they were as directly influential on the core novel contributions compared to the ones selected above. The references selected appear more tightly linked to the specific methodology innovations proposed.\n\nSo in summary, these 8 references emerge as the most likely \"source papers\" that directly inspired and motivated the primary novel aspects of this paper's approach, based on a close reading of how they are discussed in the text. The confidence scores reflect the strength of the textual evidence for their influence.", "623184035aee126c0f4848ed": null, "656d3a42939a5f4082629226": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\n{\n  \"[b19]\": 0.8,\n  \"[b4]\": 0.7,\n  \"[b54]\": 0.7,\n  \"[b30]\": 0.6,\n  \"[b18]\": 0.5,\n  \"[b26]\": 0.5,\n  \"[b31]\": 0.5,\n  \"[b59]\": 0.5,\n  \"[b64]\": 0.5,\n  \"[b77]\": 0.5\n}\n\nThe key novel contributions of this paper are:\n1) Integrating \"future modeling\" into the learning frameworks of MLLMs to enable them to foresee future events. This is done through two proposed methods:\n   - Foresight Pre-Training (FPT) which jointly trains various trajectory-centric tasks \n   - Foresight Instruction-Tuning (FIT) which uses trajectory modeling as a bridge in future reasoning\n2) Introducing trajectory, a highly structured representation, as a learning objective to help MLLMs obtain dynamic clues from multi-image observations\n3) Providing Merlin, a unified MLLM that can handle spatial coordinate inputs/outputs and perform inductive future reasoning\n\nThe most relevant references that directly inspired or motivated these contributions are:\n- [b19] and [b54] which discuss predictive processing and foresight in neuroscience theory, providing the conceptual basis\n- [b4] and [b30] which analyze the human process of foreseeing the future, informing the two-stage approach \n- [b18], [b26], [b31], [b59], [b64], [b77] which contain the richly labeled datasets used for the multitask pre-training in FPT\n\nThe confidence scores reflect the level of direct inspiration and methodological similarity between these references and the core novel aspects introduced in this paper. The other references, while relevant, do not appear to be as central to the main proposals.", "6427029c90e50fcafd5d6bdb": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"b43\": 0.9,\n  \"b48\": 0.8,\n  \"b49\": 0.8,\n  \"b63\": 0.8,\n  \"b16\": 0.7,\n  \"b69\": 0.6,\n  \"b60\": 0.6,\n  \"b75\": 0.6\n}\n\nConfidence scores:\nb43, b48, b49, b63: These references are directly mentioned as prior proposals that AstriFlash builds upon and improves, by addressing both core-side and memory-side overheads synergistically. The paper states \"While prior proposals [b43], [b48], [b49], [b63] typically focus on optimizing one class of overheads, AstriFlash achieves better performance by addressing core-side and memory-side overheads synergistically.\" This indicates these are highly pertinent source papers.\n\nb16, b69: The paper mentions that \"switch-on-miss architectures have been studied for ns-scale memory stalls [b16] and ms-scale disk stalls [b69]\", but AstriFlash extends this to \u03bcs-scale flash accesses. This suggests these papers provide important background and motivation for AstriFlash's novel switch-on-miss design.\n\nb60, b75: The paper states that accommodating frequent DRAM-cache misses requires reverting committed stores, and \"AstriFlash extends existing speculation proposals [b75] to cover the SB, thus allowing reverting stores without OS support.\" It also mentions that while \"previous proposals [b60] rely on the OS to handle the trap\", AstriFlash provides microarchitectural support instead. This indicates these papers are important prior work that AstriFlash builds upon.\n\nIn summary, references b43, b48, b49 and b63 appear to be the most central source papers, as the authors explicitly state that AstriFlash improves upon the techniques in these papers. References b16, b60, b69 and b75 also seem quite relevant as they provide key background ideas and motivation that AstriFlash extends in novel ways to enable its \u03bcs-scale switch-on-miss design. The confidence scores reflect the centrality of each reference to AstriFlash's main contributions.", "64a29612d68f896efa28bc9c": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b17]\": 0.9,\n  \"[b47]\": 0.8,\n  \"[b33]\": 0.7,\n  \"[b38]\": 0.7,\n  \"[b43]\": 0.6,\n  \"[b61]\": 0.6\n}\n\nConfidence scores:\n- [b17] (Tangram): 0.9. The paper makes direct comparisons to Tangram, saying \"To ensure a fair comparison with it\" when discussing HW-tile allocation strategies. This indicates Tangram was a key inspiration and baseline.\n\n- [b47] (Simba): 0.8. The Simba accelerator architecture is referenced multiple times when discussing the hardware abstraction and requirements for the inter-layer scheduling framework. \n\n- [b33], [b38], [b43], [b61]: 0.6-0.7. These papers propose various notations for the intra-layer scheduling space. The current work aims to do something analogous for inter-layer scheduling, directly inspired by these prior works on systematizing intra-layer scheduling.\n\nThe paper's key novel contributions appear to be:\n1) Proposing the RA Tree notation to systematically define the space of inter-layer scheduling on tiled DNN accelerators \n2) Analyzing how different inter-layer scheduling choices in this space impact hardware efficiency\n3) Developing the SET framework to automatically explore this space\n\nThe Tangram and Simba papers, as well as the various intra-layer scheduling space notations, directly motivated and enabled these contributions. The many other references provide useful background but do not seem to be as centrally tied to the novel aspects of this work based on the text.", "654f510b939a5f408289af51": "Here is my analysis to identify the most pertinent source papers based on the given text:\n\n1. Understand the novel contributions of the paper:\n- Adding RISC-V support to the SimEng microarchitectural simulator\n- Performing a path length analysis comparing AArch64 and RISC-V\n- Providing a critical path analysis to determine theoretical minimum runtimes \n- Showing results from scaled and windowed critical path analysis simulating structural hazard effects\n\n2. Look for methodological similarities and comparative phrases:\n- The paper uses the SimEng simulator, which is \"a cycle accurate microarchitectural simulator modelling simple atomic cores up to superscalar, out of order cores\". References related to SimEng or similar microarchitectural simulators may be relevant.\n- The analysis focuses on path lengths and critical paths. Look for references that employ similar analysis techniques.\n- Phrases like \"To dig deeper into the reason for these differences...\" indicate comparisons with previous work.\n\n3. Evaluate reference titles:\n- References [b0]-[b4] provide background on RISC-V and Arm adoption, but likely do not directly contribute to the novel aspects.\n- [b7]-[b10] describe the benchmarks used. These provide important context but are not the core inspiration.\n- [b11] about NVIDIA's Arm-based Grace CPU and [b12] on molecular docking relate to the background motivation but not the specific techniques.\n\nBased on this analysis, here is my assessment of the most likely source papers in JSON format:\n\n{\n  \"[b8]\": 0.2,\n  \"[2]\": 0.1, \n  \"[4]\": 0.1,\n  \"SimEng paper (uncited)\": 0.8,\n  \"Papers on path length analysis (uncited)\": 0.6,\n  \"Papers on critical path analysis (uncited)\": 0.7\n}\n\nThe key takeaways are:\n- The SimEng simulator itself is likely the biggest direct influence, but the specific paper is not cited. \n- The STREAM benchmark [b8] may have provided some inspiration for the memory bandwidth analysis.\n- Path length and critical path analysis techniques are central to the methodology but their origin papers are not directly cited.\n- The other cited references provide important background and motivation but do not seem to be the core inspiration for the novel contributions.", "646aecaad68f896efa05a6c5": "Based on the analysis of the paper, here are the most pertinent source papers that inspired or motivated this work, along with confidence scores:\n\n{\n  \"[b64]\": 0.9,\n  \"[b49]\": 0.9,\n  \"[b52]\": 0.8,\n  \"[b46]\": 0.8,\n  \"[b61]\": 0.7,\n  \"[b6]\": 0.7\n}\n\nThe key reasons are:\n\n1. The paper draws heavy inspiration from in-context learning methods like ReAct [b64] and other works [b49] that use in-context demonstrations to teach LLMs to use tools. This is a core motivation for ToolkenGPT.\n\n2. Fine-tuning LLMs on tools, as done in works like Toolformer [b52] and TALM [b46], is discussed as a related but limited approach compared to ToolkenGPT. These papers directly motivated the ToolkenGPT methodology.\n\n3. Chain-of-Thought prompting [b61] is used in combination with ToolkenGPT to enhance reasoning abilities, indicating it was an important related technique.\n\n4. LangChain [b6] is mentioned as a successful application of in-context learning for tools, which ToolkenGPT aims to improve upon.\n\nThe paper cites these works repeatedly in key parts of the methodology and positions ToolkenGPT as an approach to overcome their limitations. Therefore, these appear to be the most central inspirations and motivations for the novel contributions of ToolkenGPT.", "62c64f2e5aee126c0f6cf0f9": "Here are the key source papers identified for the given paper, with confidence scores:\n\n{\n  \"[b16]\": 0.9,\n  \"[b9]\": 0.85,\n  \"[b11]\": 0.8,\n  \"[b12]\": 0.8,\n  \"[b5]\": 0.75,\n  \"[b41]\": 0.7,\n  \"[b30]\": 0.65\n}\n\nThe reasoning for selecting these papers as the key sources:\n\n1. The paper makes several direct comparisons to DPR ([b16]), indicating it is a major source of inspiration. Phrases like \"In contrast to previous approaches, our proposed pipeline is relatively straightforward and does not require joint training [b33] or re-building index periodically [b41]\" show the paper is building upon and comparing to DPR. \n\n2. BERT ([b9]) is referenced as the backbone architecture the proposed SimLM model is built on top of. The encoder is initialized with BERT parameters.\n\n3. Condenser ([b11]) and coCondenser ([b12]) are discussed extensively as prior bottleneck-based pre-training approaches. The proposed SimLM method is compared to them with the goal of addressing their limitations.\n\n4. ELECTRA ([b5]) provides the replaced token detection pre-training approach that inspires the replaced language modeling objective in SimLM. \n\n5. ANCE ([b41]) and RocketQA ([b30]) are referenced as strong baseline dense retrieval methods that SimLM outperforms, indicating the paper builds upon ideas from those works.\n\nIn summary, the paper draws heavy inspiration from foundational works like BERT and DPR, while aiming to improve upon more recent bottleneck-based pre-training methods like Condenser. The comparisons to state-of-the-art dense retrieval systems demonstrate how SimLM fits into the broader context of retrieval research.", "641137fd90e50fcafd17b84e": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b2]\": 0.9,\n  \"[b27]\": 0.8,\n  \"[b11]\": 0.7,\n  \"[b12]\": 0.6,\n  \"[b15]\": 0.6,\n  \"[b18]\": 0.5\n}\n\nThe key factors I considered:\n\n1. The paper's main novel contributions appear to be using large language models (LLMs) for query expansion in information retrieval by generating pseudo-documents. This is most directly inspired by the GPT-3 model from OpenAI ([b2]), which the authors use to generate the pseudo-documents.\n\n2. The paper compares their dense retrieval results to strong baselines like SimLM ([b27]) and DPR ([b11]). The methodologies of these papers, especially using dense vector representations, seem to have heavily influenced this work. \n\n3. For sparse retrieval, the paper frequently references query expansion methods like RM3 ([b12], [b15]) and document expansion methods like doc2query ([b18]). Their proposed query2doc approach is portrayed as an alternative to these previous query/document expansion techniques.\n\n4. I prioritized the GPT-3 reference ([b2]) as it directly enables their main contribution. The dense retrieval papers ([b27], [b11]) and query/document expansion papers ([b12], [b15], [b18]) provide important context and baselines, but are not as central to the paper's novel aspects.\n\n5. I limited the list to avoid including more peripheral references. Other papers like those describing datasets or evaluation metrics seemed less directly influential on the core ideas.\n\nLet me know if you would like me to elaborate on the reasoning behind any part of this analysis. I tried to focus on identifying the key enabling works and methodological inspirations based on the details provided in the text.", "657c181a939a5f4082ab98e2": "Here are the most likely source papers for this paper based on the given criteria, along with confidence scores:\n\n{\n  \"[b20]\": 0.95,\n  \"[b22]\": 0.9, \n  \"[b34]\": 0.85,\n  \"[b42]\": 0.8,\n  \"[b43]\": 0.8,\n  \"[b54]\": 0.8\n}\n\nThe key reasons are:\n\n1. The paper introduces a new graph model formulation based on critical path analysis to characterize microexecution. References [b20], [b22], [b34], [b42], [b43], [b54] are directly mentioned as prior work on critical path analysis that this new formulation builds upon and aims to improve.\n\n2. The authors summarize two design principles for their approach based on lessons learned from scrutinizing the previous DEG (dynamic event-dependence graph) formulation used in the cited critical path analysis works. This shows those references provided direct inspiration and motivation for the novel contributions.\n\n3. The methodological similarities are very strong, as this paper is fundamentally employing critical path analysis on graph models of microexecution, just as the highlighted references did. The novel aspects are improvements to the graph model to enable more accurate critical path construction.\n\n4. Some of the references like [b20] are called out as the \"original proposal\" or \"first appearance\" of concepts used here, indicating their foundational influence.\n\nSo in summary, the set of references related to prior work on critical path analysis seem to be the clear top candidates for direct inspiration of the core methods, as evidenced by the detailed comparisons, lessons learned, and incremental improvements proposed with respect to those specific works. The confidence scores are high because of how explicitly the authors connect their novel contributions to building on and enhancing the techniques from those papers.", "6326303790e50fcafdf36ca9": "Here is my analysis to identify the most pertinent source papers for this paper, in JSON format with reference numbers as keys and confidence scores between 0 and 1 as values:\n\n{\n  \"[b4]\": 0.9,\n  \"[b2]\": 0.8,\n  \"[b31]\": 0.7\n}\n\nExplanation:\nThe paper introduces a novel analytical framework to optimize prefetching strategies based on application runtime characteristics. The key novel contributions are:\n\n1. A mathematical framework to understand and predict prefetcher performance by abstracting prefetching techniques. This is directly inspired and builds upon the work in [b4] and [b2] which use dynamic analysis to identify problematic loads and examine application runtime latencies for prefetching. \n\n2. A methodology to evaluate the prefetching capabilities of an application on a given hardware configuration. This is motivated by the insight from [b31] that prefetching distance can lead to significant performance variation, necessitating adaptation to dynamic application behavior.\n\n3. Closing the gap to optimal performance by leveraging memory-level parallelism (MLP) for prefetching.\n\nThe paper cites [b4], [b2] and [b31] in key parts of the text, indicating they provide direct inspiration and foundation for the novel techniques:\n\n\"Previous work [b2], [b4] has used dynamic analysis to identify problematic loads, however, to the best of our knowledge, we are the first to push this analysis further by examining application runtime latencies.\"\n\n\"For example, different prefetching distances, i.e. how far ahead the prefetcher sends requests, can lead to up to 10\u00d7 variation in performance [b31]. Therefore, we argue that prefetching needs to be driven by dynamic application behavior [b2], [b4], [b31].\"\n\nSo in summary, [b4], [b2] and [b31] emerge as the most pertinent source papers that directly inspire and enable the novel contributions of this paper. The other references provide useful background but are not as central to the paper's main techniques.", "64b60eaa3fda6d7f06eaea30": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you provided:\n\nThe paper does not contain any explicit phrases like \"inspired by\", \"motivated by\", etc. that directly point to key source papers. Therefore, I focused on identifying the novel methods and approaches introduced in the paper to tackle its main challenges and contributions.\n\nThe paper's main novel contribution seems to be proposing the GeoGPT framework that combines large language models (LLMs) with mature GIS tools to enable non-professional users to solve geospatial tasks using natural language instructions. \n\nTo achieve this, the paper takes inspiration from and builds upon some key recent works:\n\n1. The paper mentions being inspired by studies like AutoGPT [16], Visual ChatGPT [17] and HuggingGPT [48] which extend LLMs with external tools and knowledge to solve tasks in computer vision, chemistry, public health etc. This directly motivates the paper's approach of combining LLMs with GIS tools.\n\n2. The paper uses the LangChain framework [49] to connect the LLM with the GIS tools. LangChain seems to be a key enabling technology for the proposed GeoGPT system.\n\n3. The paper tests GeoGPT on some classical GIS tasks like geospatial data collection, spatial analysis, and mapping. The GIS tools it uses for these tasks (listed in Table 1) can be considered foundational to the paper's contributions, though the paper doesn't go into details on them.\n\nIn summary, based on my analysis, the most likely source papers that inspired this work are:\n\n{\n  \"16\": 0.9,\n  \"17\": 0.8, \n  \"48\": 0.8,\n  \"49\": 0.95\n}\n\nThe numbers represent my confidence score that each of these references played a key role in inspiring and enabling the main contributions of this paper, based on how they are discussed in the text. A couple other references like [47] may have also influenced the high-level approach, but the above 4 seem to be the most central based on the provided criteria.", "6584f922939a5f408236fc36": "Here is my analysis of the most pertinent source papers for the given text, based on the criteria you specified:\n\n{\n  \"[b8]\": 0.9,\n  \"[b15]\": 0.7,\n  \"[b12]\": 0.7,\n  \"[b21]\": 0.7,\n  \"[b14]\": 0.6,\n  \"[b10]\": 0.5,\n  \"[b16]\": 0.4,\n  \"[b7]\": 0.4\n}\n\nThe key novel contributions of this paper seem to be:\n\n1. Introducing the concept of \"time vectors\", which represent directions in weight space that improve performance on text from specific time periods. This is directly inspired by the concept of \"task vectors\" from [b8].\n\n2. Demonstrating that interpolating between time vectors allows customizing language models to new time periods in a computationally efficient way, without requiring data from the target time period. \n\n3. Showing that the structure of time vectors encodes temporal relationships, with vectors from closer time periods being more similar. This enables generalizing to future times through vector arithmetic.\n\nThe methodology heavily relies on the parameter interpolation techniques introduced in [b8], [b15], [b12] and [b21]. The time vector approach is a direct extension of the task vector concept from [b8].\n\n[b14] and [b10] provide important context and motivation by demonstrating the problem of temporal misalignment that the time vector method aims to address. \n\nSome implementation details like using the T5 model [b16] and LoRA [b7] are included but not as central to the main contributions.\n\nSo in summary, [b8] is the most crucial reference that directly inspired the core time vector idea. [b15], [b12] and [b21] also seem very relevant to the methodology. The others provide important context but are less central to the novel aspects of the paper.", "642e38e090e50fcafd6abcf8": null, "64f7fc6a3fda6d7f06f43fbe": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b19]\": 0.9,\n  \"[b20]\": 0.85,\n  \"[b17]\": 0.7,\n  \"[b16]\": 0.6,\n  \"[b22]\": 0.5,\n  \"[b24]\": 0.5,\n  \"[b28]\": 0.5\n}\n\nThe key points that led me to select these papers as the most relevant sources are:\n\n1. The paper takes direct inspiration from the equivariant pitch estimation [b19] and equivariant tempo estimation [b20] algorithms, as explicitly stated: \"We take inspiration from the equivariant pitch estimation [b19] and the equivariant tempo estimation [b20] algorithms which we describe in part 2.\" This indicates [b19] and [b20] were central in motivating the paper's approach.\n\n2. The architecture is inspired by [b17], suggesting methodological similarity: \"The architecture of f \u03b8 is illustrated in Figure 3. It is inspired by [b17].\"\n\n3. The paper compares its approach to CREPE [b16], a major prior work on pitch estimation using deep learning. \n\n4. The self-supervised learning approach draws upon ideas from SimCLR [b22], VICReg [b24], and BYOL [b28], but these are more general influences rather than works that directly inspired the novel contributions.\n\n5. The titles of [b19], [b20], [b17] and [b16] all relate closely to pitch estimation, the core focus of this paper.\n\nIn summary, [b19] and [b20] emerge as the clearest direct inspirations, with [b17] and [b16] also being highly relevant prior works that the authors build upon. The other references inform the general approach but are less central to the specific novel aspects introduced in this paper on self-supervised pitch estimation.", "6257c63c5aee126c0f47280f": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b9]\": 0.9,\n  \"[b17]\": 0.8,\n  \"[b41]\": 0.7,\n  \"[b26]\": 0.6,\n  \"[b5]\": 0.5\n}\n\nRationale:\nThe paper builds most directly on the work of [b9], which introduced the (EG+) algorithm that this paper generalizes and improves upon. Key phrases like \"Building on the analysis in [b9]\" and comparisons to the range of parameters allowed by [b9] indicate this is the most influential reference.\n\n[b17] is also highly relevant, as it provides the difficult counterexample that the proposed (CurvatureEG+) algorithm is shown to converge on, unlike previous methods. The paper states \"Our main contributions are summarized below. 1. We propose an adaptive extragradient-type algorithm that converges even in the difficult counter example of [b17]\".\n\nThe forward-backward-forward (FBF) algorithm of [b41] is shown to be a special case of the non-adaptive variant of the proposed algorithm for certain parameter choices. This unification under the new framework makes [b41] an important related work.\n\n[b26] and [b5] are relevant for bringing extragradient methods to the attention of the machine learning community and studying them in the monotone/convex-concave setting which this work builds upon to address the nonconvex-nonconcave case. However, they are less central to the main novel contributions here compared to [b9] and [b17].\n\nThe key novel aspects appear to be 1) Relaxing restrictions on the range of the problem parameter ? compared to [b9]'s (EG+) algorithm, 2) Proving convergence on the challenging counterexample of [b17] using the adaptive (CurvatureEG+) method, and 3) Unifying (EG+) and FBF [b41] as special cases. References [b9], [b17] and [b41] directly enable or motivate these main contributions.", "63dcdb422c26941cf00b61c5": "Here is my attempt at identifying the most pertinent source papers for the given paper, based on the criteria you provided:\n\nAfter analyzing the text, the key novel contributions of this paper appear to be:\n\n1. Proposing a bias-corrected stochastic extragradient algorithm (BC-SEG+) that converges for weak Minty variational inequalities (MVIs) without increasing the batch size. The bias correction allows using a fixed large stepsize.\n\n2. Generalizing the bias-corrected scheme to constrained and regularized settings, providing a unified analysis. This includes stochastic variants of forward-backward-forward and primal-dual hybrid gradient algorithms for weak MVIs.\n\n3. Proving almost sure convergence of the proposed algorithms under classical stochastic approximation stepsizes.\n\nThe paper does not contain many explicit phrases like \"inspired by\" that directly point to source papers. However, based on the key methods and the references linked to the main contributions, the most pertinent source papers appear to be:\n\n{\n  \"[b29]\": 0.9,\n  \"[b13]\": 0.8,\n  \"[b33]\": 0.7,\n  \"[b32]\": 0.6,\n  \"[b9]\": 0.6\n}\n\n[b29] and [b13] are the most relevant as they establish key properties of extragradient methods for weak MVIs in the deterministic setting that this paper builds upon. [b33] provides relevant background on difficulties of last-iterate convergence even for monotone stochastic variational inequalities. [b32] introduces the forward-backward-forward method that is generalized here, while [b9] is the original paper on the primal-dual hybrid gradient method, a variant of which is analyzed.\n\nThe paper also cites other relevant work on stochastic methods for variational inequalities, but the ones above appear to be the most central to the specific novel contributions around bias-corrected stochastic extragradient methods for weak MVIs. Let me know if you need any clarification or have additional questions!", "640fe64790e50fcafd9e276e": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b0]\": 0.9,\n  \"[b2]\": 0.8,\n  \"[b29]\": 0.7,\n  \"[b14]\": 0.6,\n  \"[b31]\": 0.6\n}\n\nRationale:\n- [b0] (DeepSpeed Zero-Inference) and \"HuggingFace, 2022\" (Hugging Face Accelerate) are mentioned multiple times as state-of-the-art offloading-based inference systems that the paper directly compares against and aims to improve upon. The paper states \"Compared to DeepSpeed Zero-Inference [b0] and Hugging Face Accelerate (HuggingFace, 2022), two state-of-the-art offloading-based inference systems, FlexGen often allows a batch size that is orders of magnitude larger. As a result, FlexGen can achieve much higher throughputs.\" This indicates [b0] is a key motivation and baseline.\n\n- [b2] (Petals) is a decentralized collective inference system that the paper directly compares FlexGen against, stating \"We also compare offloading and decentralized collective inference based on FlexGen and Petals [b2] as two representative systems.\" \n\n- [b29] (PaLM inference) is mentioned as an example of a specialized inference system for LLMs that \"focus on latency-oriented scenarios with high-end accelerators, limiting their deployment for throughput-oriented inference on easily accessible hardware.\" Overcoming this limitation seems to be a key motivation for FlexGen.\n\n- [b14] and [b31] are training systems whose offloading techniques are inherited by current inference systems like DeepSpeed Zero-Inference and Hugging Face Accelerate. The paper argues these techniques are suboptimal for inference.\n\nIn summary, [b0], [b2], and [b29] emerge as the most relevant prior works that directly motivated FlexGen's key contributions, while [b14] and [b31] represent key prior techniques that FlexGen aims to improve upon for the inference setting. The other cited papers, while relevant as background, do not seem to be as centrally tied to FlexGen's main proposals based on the text.", "6493c733d68f896efad19c1d": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b14]\": 0.9,\n  \"[b10]\": 0.8,\n  \"[b23]\": 0.7,\n  \"[b3]\": 0.7,\n  \"[b21]\": 0.6,\n  \"[b22]\": 0.6,\n  \"[b20]\": 0.6,\n  \"[b24]\": 0.8\n}\n\nConfidence scores:\n- [b14] (0.9): The paper mentions that \"multimodal GPT-4 [b14] and open-sourced efforts such as LLaVA [b10] , generative pretraining has effectively leveraged their performance\", indicating GPT-4 was a key inspiration. \n- [b10] (0.8): LLaVA is mentioned alongside GPT-4 as open-source efforts that inspired the paper's approach to generative pretraining.\n- [b23], [b3], [b21], [b22], [b20] (0.6-0.7): These biomedical large language models like ChatDoctor, Med-Alpaca, PMC-LLaMA, DoctorGLM, Huatuo are discussed as related work that the paper builds upon for developing OphGLM. The paper follows a similar approach of fine-tuning open-source LLMs on domain-specific datasets.\n- [b24] (0.8): The paper directly states \"In our model, ChatGLM [b24] was used as the LLM and was fine-tuned on an ophthalmology dialogue dataset.\" This indicates ChatGLM was core to their methodology.\n\nThe key novel contributions seem to be 1) Constructing an ophthalmic instruction and dialogue dataset using knowledge graphs and ChatGPT, 2) Developing multiple fundus image based disease diagnosis models, and 3) Proposing OphGLM by combining the visual models with a fine-tuned ChatGLM. \n\nThe references most directly tied to these contributions are [b14], [b10] for the inspiration behind the multimodal generative pretraining approach, [b23]-[b22] for the related biomedical LLMs that were fine-tuned in a similar way, and [b24] ChatGLM which was the specific LLM used as the base for OphGLM.", "62283c435aee126c0fd5de60": "Based on the criteria outlined, here are the most pertinent source papers for the given paper, along with confidence scores:\n\n{\n  \"[b29]\": 0.9,\n  \"[b3]\": 0.8, \n  \"[b0]\": 0.7,\n  \"[b14]\": 0.6,\n  \"[b17]\": 0.6,\n  \"[b49]\": 0.5\n}\n\nThe key reasons for selecting these papers are:\n\n- [b29] and [b3] are highly relevant as they focus on using optical flow and dense motion fields to generate temporally coherent videos, which directly inspires the Video Consistency Network (VCN) proposed in this paper. The paper adopts similar techniques of optical flow to ensure inter-frame consistency.\n\n- [b0] and [b17] are relevant as they also utilize facial landmarks as an intermediate representation to map audio to facial movements. This paper builds upon that general approach. \n\n- [b14] is pertinent as it uses LSTM to convert audio to mouth shapes, which inspires the landmark prediction part of this paper, although this paper uses a different Multiple Synergy Network (MSN) architecture.\n\n- [b49] introduces Temporal Convolutional Networks (TCNs) which are employed in the encoder of the MSN network proposed here for audio/text encoding.\n\nSo in summary, the selected papers provide direct inspiration for the key novel components proposed in this work - the VCN network for video frame generation using optical flow, and the MSN network for landmark prediction from multimodal audio/text inputs. The paper builds upon the general paradigm of using landmarks from prior work but proposes new architectures for improved results.", "639a90a690e50fcafdf0a150": "Here is my analysis of the most pertinent source papers for this paper, based on the criteria you provided:\n\n{\n  \"[b29]\": 0.9,\n  \"[b23]\": 0.8,\n  \"[b44]\": 0.7,\n  \"[b20]\": 0.6,\n  \"[b61]\": 0.6\n}\n\nRationale:\n- The paper incorporates the temporal shift module (TSM) from [b29] in the discriminator as a key improvement to capture more temporal information. This is highlighted as providing \"better guidance to the generator\". So [b29] seems to be a very direct inspiration.\n\n- The alias-free technique from [b23] (StyleGAN3) is deployed to tackle the texture sticking problem and preserve frame quality. Adopting this technique from image generation to video is presented as an important enhancement. \n\n- The work aims to improve upon StyleGAN-V [b44], the \"prior art\" that the paper makes a \"comprehensive overhaul\" of. [b44] is referenced extensively as the foundation.\n\n- [b20] and [b61] are cited to support the idea that a GAN's deep layers control details and this knowledge can be pre-learned, a technique the paper employs.\n\nThe paper emphasizes these novel contributions: 1) Adapting alias-free technique and pre-learned knowledge from image GANs to video, 2) Adding explicit temporal reasoning to the discriminator, 3) Improving motion embedding continuity with B-Splines.\n\nThe selected references [b29], [b23] and [b44] tie most directly to enabling these main contributions. [b20] and [b61] also provide key insights leveraged in the approach.", "6233f88c5aee126c0f94b3c4": "Based on the analysis of the paper, here are the most pertinent source papers that likely inspired or motivated this work, along with confidence scores:\n\n{\n  \"[b41]\": 0.9,\n  \"[b44]\": 0.8,\n  \"[b33]\": 0.7,\n  \"[b5]\": 0.6,\n  \"[b31]\": 0.6,\n  \"[b37]\": 0.5\n}\n\nThe key factors for selecting these papers are:\n\n1. The paper directly states that DProf [b41] is \"the only tool that can identify all types of cache misses for data structures of Linux kernel\", but requires significant manual effort. This strongly indicates [b41] was a key inspiration and the current work aims to automate that process.\n\n2. CCProf [b44] is discussed in detail in section 2.1.2 on identifying conflict misses. The authors point out limitations of CCProf's approach and propose an alternative, signaling [b44] was an important prior work they aim to improve upon.\n\n3. The data-centric analysis to attribute cache misses to data objects is directly credited to [b33]. \n\n4. Several prior works on identifying specific types of cache misses, like [b5], [b31], [b37] for coherence misses, are discussed and contrasted, indicating they provided important context and inspiration, even if the current paper takes a more unified approach.\n\n5. Overall, the introduction and background sections contain the strongest signals through direct comparisons and statements of building upon or automating techniques from these specific prior works. The novel contributions are then framed in contrast to the shortcomings of these previous approaches.", "634967f890e50fcafdb5120a": "Here are the key steps I took to identify the most pertinent source papers for this paper:\n\n1. The primary novel contributions of the paper are:\n- Proposing a high-order degradation model to synthesize more practical degradations for training, extending the classical first-order degradation model \n- Incorporating sinc filters in the degradation process to simulate common ringing and overshoot artifacts\n- Improving the ESRGAN discriminator to a U-Net design with spectral normalization to increase discriminative power and training stability\n\n2. Looking for key inspiration phrases, the most relevant ones are:\n- \"Inspired by [42] [53], we also improve the VGG-style discriminator in ESRGAN to an U-Net design with skip connections\"\n- \"We employ the spectral normalization regularization [38] to stabilize the training dynamics.\"\n\n3. The references most directly linked to the main novel contributions are:\n- [42], [53]: Directly inspired the U-Net discriminator design \n- [38]: Spectral normalization is employed to stabilize training\n- [51]: The powerful ESRGAN is extended in this work for blind SR\n- [56]: Proposes a random shuffling degradation strategy that this work builds upon\n\n4. Evaluating the centrality of references to the novel contributions, here are the most pertinent source papers in JSON format with confidence scores:\n\n{\n  \"42\": 0.95,\n  \"53\": 0.9,\n  \"38\": 0.85, \n  \"51\": 0.8,\n  \"56\": 0.75\n}\n\nThe papers [42], [53] and [38] are given the highest scores as they directly inspired the discriminator and training improvements central to the work. [51] is the base ESRGAN method extended here so is also highly relevant. [56] proposed a related degradation synthesis approach built upon in this paper.", "6287493f5aee126c0ffedf11": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"[b40]\": 0.9,\n  \"[b2]\": 0.8,\n  \"[b24]\": 0.7,\n  \"[b22]\": 0.6,\n  \"[b7]\": 0.5\n}\n\nThe key factors leading to these source papers being identified as most pertinent are:\n\n1. The paper takes direct inspiration from the extractive formulation proposed in [b40] for the related task of Word Sense Disambiguation. This is evident from phrases like \"Inspired by recent trends in other semantic tasks [b40], we formulate Entity Disambiguation as a text extraction problem\".\n\n2. The Longformer architecture [b2] is a core component of the proposed EXTEND model. The paper adapts the Longformer's attention mechanism to suit the entity disambiguation task.\n\n3. The BART model [b24] is used as the underlying architecture in one variant of EXTEND. \n\n4. The candidate sets and mention-candidate frequencies used for evaluation are taken from [b22].\n\n5. The paper frames entity disambiguation as a multi-label classification problem, following the paradigm established in [b7].\n\nThe novel contributions of this paper are proposing an extractive formulation of entity disambiguation and the EXTEND model architectures that implement this formulation. References that directly enable or inspire these core aspects are given high pertinence scores. Methodological and dataset similarities are also considered.", "62286c865aee126c0fa71627": "Here is my analysis of the most pertinent source papers for the given paper, based on the criteria you specified:\n\n{\n  \"b4\": 0.9,\n  \"b5\": 0.8,\n  \"b10\": 0.7,\n  \"b11\": 0.7,\n  \"b12\": 0.7,\n  \"b13\": 0.7\n}\n\nRationale:\nThe paper seems to be most directly inspired and motivated by references [b4] and [b5]. Key phrases like \"Inspired by [b4]\" and \"Different from [b4]\" indicate the paper builds upon and improves the methods in [b4]. [b5] is also mentioned as extending [b4]'s approach.\n\nReferences [b10]-[b13] are also highly relevant, as they represent the state-of-the-art methods that formulate the legalization problem as a quadratic program and solve it using MMSIM. The paper analyzes their limitations and aims to improve upon them.\n\nThe novel contributions of this paper seem to be:\n1) Reformulating legalization as a resource allocation problem and using a negotiation-based method to solve it\n2) Introducing an isolation point and adaptive penalty function in the negotiation process \n3) Limiting each cell's searchable space to accelerate the algorithm\n4) Using multithreading to speed up convergence\n\nSo the references that share the strongest methodological similarities and that the paper positions its work against are [b4], [b5], [b10]-[b13]. The other references, while providing useful background, do not seem to be as centrally tied to the main novel aspects contributed by this paper based on my analysis of the text."}