{"621454435aee126c0f1d7065": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of automated fact-checking in the presence of adversarially generated disinformation, specifically focusing on how synthetically generated false documents can contaminate evidence repositories and degrade the performance of fact verification systems. The paper evaluates the impact of adversarial attacks, defined as AD-VERSARIAL ADDITION (ADVADD) and ADVERSARIAL MODIFICATION (ADVMOD), on existing fact-checking systems. The authors demonstrate that these attacks significantly lower the performance of fact verification models and highlight the sensitivity of these systems to even small amounts of evidence contamination.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b42\": 0.9,\n    \"b36\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.75,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automated fact-checking in the context of misinformation and disinformation, focusing on how adversarially injected synthetic disinformation can degrade the performance of fact-checking systems. The authors propose two settings for adversarial attacks: AD-VERSARIAL ADDITION (ADVADD) and ADVERSARIAL MODIFICATION (ADVMOD), and demonstrate that these attacks significantly impact the performance of fact verification systems.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1, \n    \"b42\": 1, \n    \"b36\": 1,\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.9, \n    \"b33\": 0.9,\n    \"b4\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.8, \n    \"b21\": 0.8, \n    \"b12\": 0.8, \n    \"b46\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automated fact-checking in the presence of synthetic disinformation. It proposes adversarial attacks by injecting synthetically-generated false documents into fact-checking evidence repositories, significantly impacting the performance of these systems.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1,\n    \"b42\": 1,\n    \"b36\": 1,\n    \"b33\": 0.9,\n    \"b46\": 0.9,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b4\": 0.8,\n    \"b21\": 0.8,\n    \"b31\": 0.8,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b44\": 0.6,\n    \"b3\": 0.5,\n    \"b13\": 0.5,\n    \"b19\": 0.5,\n    \"b24\": 0.5,\n    \"b15\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automated fact-checking in the presence of synthetically generated disinformation that contaminates evidence repositories. The primary focus is on evaluating how adversarial disinformation can reduce the performance of fact verification systems. The proposed approach involves two settings: adversarial addition and adversarial modification, where synthetic disinformation is injected into the evidence bases of existing systems to assess their vulnerability.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b42\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.8,\n    \"b33\": 0.7,\n    \"b46\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of misinformation in online ecosystems and explores automated fact-checking systems. It proposes methods to inject adversarial disinformation into fact-checker evidence repositories and assesses the impact on these systems' performance.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1,\n    \"b42\": 1,\n    \"b36\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b46\": 0.9,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7,\n    \"b41\": 0.6\n  }\n}\n```"], "6221834e5aee126c0f23c25c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges associated with training deep neural networks (DNNs) on large datasets by proposing a novel strategy for dataset condensation called CAFE. This method aims to condense a large dataset into a smaller synthetic one while preserving its performance. The approach involves aligning layer-wise features between real and synthetic data and incorporating a discrimination loss to ensure the synthetic data's discriminative power. Additionally, a dynamic bi-level optimization scheme is introduced to adaptively alter the number of SGD steps, thus improving generalization and robustness.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1,\n    \"b53\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b28\": 0.8,\n    \"b51\": 0.7,\n    \"b52\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of condensing large-scale datasets for training deep neural networks by introducing a novel method called CAFE. This method aims to overcome the biases and generalization issues of previous gradient-based methods by aligning layer-wise features and incorporating a discrimination loss. The proposed dynamic bi-level optimization further enhances the performance and adaptability of the synthetic datasets.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9,\n    \"b53\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b51\": 0.7,\n    \"b28\": 0.6,\n    \"b52\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b5\": 0.5,\n    \"b1\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing dataset condensation methods, specifically the bias towards hard examples in gradient-based methods, and proposes a novel approach called CAFE (Condense dataset by Aligning FEatures). CAFE uses layer-wise feature alignment and discrimination loss to better capture the distribution of the original dataset, thereby improving generalization and performance. A new dynamic bi-level optimization scheme is also introduced to adaptively alter the number of SGD steps, mitigating under- and over-fitting issues.\",\n  \"Direct Inspiration\": [\"b37\", \"b53\"],\n  \"Indirect Inspiration\": [\"b52\", \"b51\"],\n  \"Other Inspiration\": [\"b0\", \"b28\", \"b1\", \"b32\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the training inefficiency of deep neural networks (DNNs) due to the reliance on large datasets. The proposed algorithm, CAFE (Condense dataset by Aligning FEatures), aims to condense large datasets into smaller synthetic ones while maintaining performance. CAFE introduces a novel strategy that includes layer-wise feature alignment, discrimination loss, and dynamic bi-level optimization to address issues related to gradient matching methods, such as overfitting to hard examples and lack of generalization.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1,\n    \"b53\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b51\": 0.8,\n    \"b52\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b28\": 0.7,\n    \"b31\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a novel approach called CAFE for condensing datasets by aligning features between real and synthetic data, and explicitly encoding discriminant power into synthetic clusters. The approach addresses issues of learning bias and generalization in existing methods, particularly the gradient matching method.\",\n  \"Direct Inspiration\": [\n    \"b53\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b37\",\n    \"b51\",\n    \"b52\"\n  ],\n  \"Other Inspiration\": [\n    \"b1\",\n    \"b28\",\n    \"b32\"\n  ]\n}\n```"], "621ee1835aee126c0f26a902": ["```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include the limited scope of existing GNN benchmark datasets, architectural overfitting, and the high cost and accessibility issues related to large-scale benchmark datasets. The proposed solution, GraphWorld, introduces a tunable, scalable, and reproducible method for generating synthetic benchmark datasets to evaluate GNN models across a diverse range of tasks and datasets. This method addresses the problems of inadequate generalization, incremental overfitting, and un-scalable development of GNNs.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b17\": 0.8,\n        \"b31\": 0.85,\n        \"b33\": 0.85,\n        \"b34\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.75,\n        \"b29\": 0.75,\n        \"b36\": 0.7,\n        \"b49\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b30\": 0.65,\n        \"b10\": 0.6,\n        \"b27\": 0.6,\n        \"b38\": 0.6,\n        \"b45\": 0.6,\n        \"b12\": 0.65,\n        \"b20\": 0.65,\n        \"b28\": 0.65,\n        \"b50\": 0.65,\n        \"b8\": 0.6,\n        \"b35\": 0.6,\n        \"b42\": 0.6,\n        \"b43\": 0.6,\n        \"b52\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Limited scope of benchmark datasets making it difficult to infer generalizability of GNN models.\",\n      \"Architectural overfitting due to new models being proposed only when they outperform existing methods on limited datasets.\",\n      \"High cost of evaluating models on expanded benchmark datasets.\"\n    ],\n    \"algorithm_proposed\": \"GraphWorld, a tunable, scalable, and reproducible method for analyzing GNN performance on synthetic benchmark data.\"\n  },\n  \"Direct Inspiration\": [\"b31\", \"b33\", \"b17\"],\n  \"Indirect Inspiration\": [\"b36\", \"b49\", \"b13\", \"b29\"],\n  \"Other Inspiration\": [\"b7\", \"b34\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces GraphWorld, a scalable and reproducible method for analyzing GNN performance on synthetic benchmark data. It addresses key challenges such as inadequate generalization, incremental overfitting, and un-scalable development in GNN benchmarking. The proposed GraphWorld allows for diverse, tunable graph datasets and provides insights into GNN model performance across a wide range of tasks.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b47\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b17\": 0.8,\n    \"b49\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b29\": 0.6,\n    \"b31\": 0.5,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces GraphWorld, a tunable, scalable, and reproducible method for analyzing GNN model performance on synthetic benchmark data for various GNN tasks. The primary challenges include the limited scope of current benchmark datasets, which leads to difficulties in generalization, potential overfitting, and scaling issues for researchers with limited computational resources. GraphWorld addresses these challenges by generating diverse synthetic datasets, enabling comprehensive benchmarking and analysis of GNN models.\",\n  \"Direct Inspiration\": {\n    \"b47\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b31\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b29\": 0.6,\n    \"b36\": 0.5,\n    \"b49\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces GraphWorld, a tunable, scalable, and reproducible method for analyzing the performance of Graph Neural Networks (GNNs) on synthetic benchmark data. It addresses the challenges of inadequate generalization, incremental overfitting, and un-scalable development inherent in current GNN benchmarking methods. GraphWorld facilitates comparisons between GNN models by generating diverse synthetic datasets, enabling large-scale experimental studies, and providing novel insights into GNN performance across various tasks.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b31\": 1.0,\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b30\": 0.7,\n    \"b34\": 0.7,\n    \"b46\": 0.7,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b19\": 0.6,\n    \"b29\": 0.6,\n    \"b36\": 0.6,\n    \"b43\": 0.6,\n    \"b52\": 0.6\n  }\n}\n```"], "621635aa91e011b46d7ce15d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of capturing local distinctions and preserving global coherence in large-scale graph neural networks (GNNs). It proposes a novel model-agnostic framework, Ada-GNN, which generates personalized models at the subgroup level by leveraging the Model-Agnostic Meta-Learning (MAML) framework.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b14\": 0.8,\n    \"b16\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of personalized model learning at the subgroup-level in large-scale graph neural networks (GNNs). The main challenges include effectively capturing and representing distinctions among subgroups (local distinction) and ensuring that common knowledge among subgroups is preserved (global coherence). The proposed solution is Ada-GNN, a model-agnostic framework inspired by Model-Agnostic Meta-Learning (MAML), which generates different models for different subgraphs while maintaining global coherence and local distinction.\",\n  \"Direct Inspiration\": [\n    \"b2\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b1\",\n    \"b14\",\n    \"b7\"\n  ],\n  \"Other Inspiration\": [\n    \"b3\",\n    \"b25\",\n    \"b9\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of capturing local distinctions and preserving global coherence in large-scale graphs by proposing a model-agnostic framework, Ada-GNN. This framework generates personalized models for subgroups within the graph, inspired by the Model-Agnostic Meta-Learning (MAML) framework. Key methods include node tagging, meta adapter for global to local model adaptation, and a feature enhancement module.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b2\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b1\", \"b7\", \"b14\", \"b16\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": []\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of effectively capturing and representing the distinctions among different subgroups (local distinction) and ensuring the preservation of useful common knowledge among subgroups (global coherence) in large-scale graphs. The proposed algorithm, Ada-GNN, is inspired by the Model-Agnostic Meta-Learning (MAML) framework to generate different models for different subgraphs.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b7\": 0.8,\n    \"b16\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Graph Neural Networks (GNNs) on large-scale graphs by proposing a framework called Ada-GNN, which generates personalized models at the subgroup level. The primary challenges include capturing local distinctions among subgroups and preserving global coherence. The proposed solution is inspired by the Model-Agnostic Meta-Learning (MAML) framework and involves a meta adapter module and a feature enhancement module to facilitate subgroup-level personalization while maintaining global coherence.\",\n  \"Direct Inspiration\": [\n    \"b2\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b7\",\n    \"b14\",\n    \"b16\"\n  ],\n  \"Other Inspiration\": [\n    \"b1\"\n  ]\n}\n```"], "620f0e735aee126c0fec4734": ["```json\n{\n  \"Summary\": \"The paper proposes S3-CL, a graph contrastive learning framework that addresses three primary challenges in existing GCL methods: shallow encoding architecture, arbitrary augmentation design, and semanticless contrastive objectives. S3-CL uses a parameterless encoding backbone with structural and semantic contrastive learning modules to improve node representations by exploiting both local and global structure knowledge and enhancing intra-cluster compactness and inter-cluster separability without human-annotated labels.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b18\": 0.8,\n    \"b16\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7,\n    \"b30\": 0.7,\n    \"b21\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of high-dimensional, complex, and heterogeneous real-world graphs in self-supervised node representation learning. It proposes a novel GCL framework, S3-CL, which uses structural and semantic contrastive learning to improve node-wise discrimination and capture data semantics without relying on human-annotated labels.\",\n    \"Direct Inspiration\": [\"b8\", \"b10\", \"b18\"],\n    \"Indirect Inspiration\": [\"b2\", \"b6\", \"b16\", \"b21\", \"b32\"],\n    \"Other Inspiration\": [\"b33\", \"b35\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in self-supervised node representation learning for graphs, which involve shallow encoding architectures, arbitrary augmentation designs, and semanticless contrastive objectives. The proposed method, S3-CL, introduces a simple yet effective framework utilizing structural and semantic contrastive learning to enhance node representation by considering both local and global structure information and ensuring intra-cluster compactness and inter-cluster separability.\",\n  \"Direct Inspiration\": [\n    \"b8\",\n    \"b10\",\n    \"b35\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b2\",\n    \"b18\",\n    \"b21\"\n  ],\n  \"Other Inspiration\": [\n    \"b33\",\n    \"b9\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of shallow encoding architecture, arbitrary augmentation design, and semanticless contrastive objectives in graph contrastive learning (GCL) methods. It proposes a new framework, S3-CL, which integrates structural and semantic contrastive learning. This framework enhances node-wise discrimination and captures both local and global structure knowledge using a simpler, parameterless encoding backbone.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b10\": 1,\n    \"b18\": 1,\n    \"b33\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.9,\n    \"b32\": 0.9,\n    \"b35\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b16\": 0.7,\n    \"b21\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in self-supervised node representation learning in graphs, specifically issues with shallow encoding architectures, arbitrary augmentation designs, and semanticless contrastive objectives. The proposed solution, S3-CL, introduces novel structural and semantic contrastive learning modules to improve node representation by capturing both local and global structure information and enhancing intra-cluster compactness and inter-cluster separability.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b8\": 0.9,\n    \"b10\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b14\": 0.7,\n    \"b16\": 0.7,\n    \"b23\": 0.7,\n    \"b26\": 0.6,\n    \"b37\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b15\": 0.6,\n    \"b33\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```"], "622183525aee126c0f23c7c2": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of short text matching, particularly in scenarios where contextual information is lacking, leading to ambiguous meanings. It proposes a context-aware BERT matching model (CBM) that enhances the semantic representation of short texts using external sentence-level knowledge. The framework includes modules for context crawling, context selection, and context-enhanced text matching, achieving state-of-the-art performance on multiple datasets.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b3\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b12\": 0.75,\n    \"b17\": 0.7,\n    \"b18\": 0.65,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.55,\n    \"b9\": 0.5,\n    \"b5\": 0.45,\n    \"b15\": 0.4,\n    \"b14\": 0.35,\n    \"b11\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of short text matching, particularly in Chinese contexts where character-level and word-level tokenization introduces semantic errors. The proposed solution is a context-aware BERT matching model (CBM) that enriches short text representations by incorporating external sentence-level knowledge. The framework includes modules for context crawling, selection, and enhancement.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b3\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.75,\n    \"b9\": 0.7,\n    \"b5\": 0.65,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge addressed in the paper is the lack of sufficient contextual information in short text matching tasks, particularly in Chinese scenarios where both character-level and word-level tokenization introduce semantic errors or omissions.\",\n    \"Inspirations\": \"The paper proposes a context-aware BERT matching model (CBM) that enriches the semantic representation of short texts by incorporating external sentence-level knowledge.\"\n  },\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b8\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.65,\n    \"b9\": 0.6,\n    \"b14\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of short text matching, especially in Chinese language contexts where character-level and word-level tokenization can introduce semantic errors. The proposed solution is a context-aware BERT matching model (CBM) that enhances short text representation by incorporating external sentence-level knowledge. The approach involves crawling contextual sentences from search engines, selecting relevant contexts, and using these to improve text matching accuracy.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b3\": 0.85,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b6\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b9\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of short text matching, particularly in the context of Chinese language processing, where character-level and word-level tokenization introduce semantic errors. It proposes a context-aware BERT matching model (CBM) that leverages external sentence-level knowledge to enhance semantic representation. The model includes three main components: Context Crawler, Context Selector, and Context-enhanced Text Matcher. The CBM model outperforms existing methods by incorporating additional contextual information from search engines to improve matching accuracy.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b3\": 0.90,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.80,\n    \"b12\": 0.75,\n    \"b17\": 0.70,\n    \"b18\": 0.70,\n    \"b6\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.65,\n    \"b9\": 0.60,\n    \"b5\": 0.55,\n    \"b15\": 0.50,\n    \"b14\": 0.45,\n    \"b11\": 0.40\n  }\n}\n```"], "62451c2b5aee126c0f47ac08": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the issue of few-shot node classification in graph neural networks (GNNs) due to skewed label distribution. The paper proposes a novel supervised contrastive learning based pretraining strategy, combined with a transductive fine-tuning approach, to enhance the generalization ability of GNNs. Key aspects include self-supervised graph contrastive learning, balance sampling strategy, and robust data augmentation methods.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b7\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.8,\n    \"b6\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b9\": 0.6,\n    \"b11\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of few-shot node classification in graph neural networks (GNNs) by proposing a novel supervised contrastive learning framework. The main contributions include a pretraining strategy tailored for few-shot learning, a balanced sampling strategy to handle skewed class distributions, and a robust data augmentation method using node connectivity. The effectiveness of the proposed methods is demonstrated through comprehensive experiments.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b7\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b5\": 0.8,\n        \"b6\": 0.8,\n        \"b9\": 0.8,\n        \"b12\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.6,\n        \"b11\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot node classification in graphs, particularly dealing with the issue of skewed class distributions. The authors propose a supervised contrastive learning framework combined with a transductive fine-tuning approach to enhance the generalizability and discriminative power of GNN encoders for this task. Key inspirations include the use of self-supervised graph contrastive learning and the adaptation of methods from the image domain to the graph domain.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.85,\n    \"b3\": 0.8,\n    \"b5\": 0.75,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.65,\n    \"b10\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot node classification in Graph Neural Networks (GNNs) by proposing a transductive fine-tuning framework combined with a supervised contrastive pretraining strategy. The novel approach aims to overcome the issue of skewed label distributions and enhance the discriminative power of node representations.\",\n  \"Direct Inspiration\": [\"b1\", \"b7\"],\n  \"Indirect Inspiration\": [\"b2\", \"b6\", \"b9\", \"b12\"],\n  \"Other Inspiration\": [\"b3\", \"b5\", \"b8\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot node classification in graphs, particularly when labels are scarce and distributed in a highly skewed manner. It proposes a supervised contrastive learning based framework with a transductive fine-tuning strategy to generate discriminative representations for unseen classes. The solution involves a balanced sampling strategy and robust data augmentation techniques using node connectivity measures.\",\n  \"Direct Inspiration\": [\"b1\", \"b7\"],\n  \"Indirect Inspiration\": [\"b2\", \"b6\", \"b9\"],\n  \"Other Inspiration\": [\"b3\", \"b12\"]\n}\n```"], "6226c93d5aee126c0fd57ba8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving entity alignment (EA) in knowledge graphs (KGs) by framing it as a sequential decision-making task using reinforcement learning (RL). The proposed RL-based entity alignment (RLEA) framework aims to improve the evaluation process of embedding-based entity alignment (EEA) methods by leveraging trained embeddings, a curriculum learning strategy, and a policy network to maximize alignment rewards while tackling issues like accumulated errors and long-term dependencies.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b12\": 0.85,\n    \"b18\": 0.8,\n    \"b7\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b20\": 0.65,\n    \"b21\": 0.6,\n    \"b34\": 0.55,\n    \"b24\": 0.5,\n    \"b31\": 0.45,\n    \"b22\": 0.4\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.35,\n    \"b35\": 0.3,\n    \"b6\": 0.25,\n    \"b28\": 0.2,\n    \"b17\": 0.15,\n    \"b32\": 0.1\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the entity alignment (EA) challenge in knowledge graphs (KGs) by proposing a novel reinforcement learning (RL) based approach to improve the evaluation strategy of embedding-based entity alignment (EEA) methods. The core contribution is modeling EA as a sequential decision-making task and implementing an end-to-end RL-based framework (RLEA) to solve it.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b7\": 0.8,\n    \"b11\": 0.85,\n    \"b25\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.6,\n    \"b21\": 0.65,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b12\": 0.55,\n    \"b18\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the evaluation process in embedding-based entity alignment (EEA) methods, which often fail to discriminate true aligned entities from other candidates due to similar embeddings. The novel contribution is an RL-based entity alignment (RLEA) framework that models entity alignment as a sequential decision-making task. The methodology involves training an agent to maximize rewards by aligning entities sequentially and using curriculum learning to manage the difficulty of candidate pairs.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b13\": 1,\n    \"b12\": 0.9,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.75,\n    \"b9\": 0.7,\n    \"b34\": 0.7,\n    \"b24\": 0.7,\n    \"b31\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b38\": 0.65,\n    \"b35\": 0.65,\n    \"b26\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the improvement of entity alignment (EA) in knowledge graphs (KGs) by considering the evaluation process and reducing the confusion in discriminating true aligned entities from other candidates. The paper proposes a novel reinforcement learning (RL)-based framework to model EA as a sequential decision-making task, which is the first of its kind to provide a general solution for improving the evaluation strategy. The contributions include the introduction of a policy network that accounts for self-embedding, neighborhood, and long-term rewards, and an end-to-end RL-based entity alignment framework (RLEA).\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b13\": 0.9,\n    \"b12\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b38\": 0.6,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b20\": 0.5,\n    \"b9\": 0.5,\n    \"b34\": 0.5,\n    \"b24\": 0.5,\n    \"b31\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of entity alignment (EA) in knowledge graphs (KGs) by proposing a reinforcement learning-based framework (RLEA) to model it as a sequential decision-making task. The novel approach is designed to improve the evaluation strategy by considering the long-term dependencies and accumulated errors in sequential decision processes. The paper also adopts a curriculum learning strategy to provide candidate entity pairs with increasing difficulty.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b13\": 0.8,\n    \"b12\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b9\": 0.6,\n    \"b34\": 0.6,\n    \"b24\": 0.6,\n    \"b31\": 0.6,\n    \"b22\": 0.6,\n    \"b38\": 0.7,\n    \"b35\": 0.6,\n    \"b6\": 0.6,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b4\": 0.5,\n    \"b40\": 0.5,\n    \"b30\": 0.5,\n    \"b3\": 0.5,\n    \"b11\": 0.5,\n    \"b14\": 0.5\n  }\n}\n```"], "6243ca915aee126c0fbd0aa0": ["```json\n{\n  \"Summary\": \"The paper proposes a unified framework, TGL, for large-scale offline Temporal Graph Neural Network (TGNN) training, addressing the challenges of scaling TGNNs to large graphs. Key contributions include a novel random chunk scheduling technique, a Temporal-CSR data structure, and the introduction of large-scale datasets for evaluation.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b14\": 1,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b16\": 0.8,\n    \"b9\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper proposes TGL, a unified framework for large-scale offline Temporal Graph Neural Network (TGNN) training. TGL addresses the challenges of efficiently training TGNNs on large dynamic graphs by introducing components such as a temporal sampler, node memory module, and random chunk scheduling. The framework supports multiple TGNN variants and is evaluated on both small and large-scale datasets, demonstrating significant speedup and comparable or better accuracy compared to baseline methods.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1,\n        \"b14\": 1,\n        \"b22\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.8,\n        \"b16\": 0.8,\n        \"b9\": 0.8,\n        \"b17\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.7,\n        \"b15\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently training Temporal Graph Neural Networks (TGNNs) on large-scale dynamic graphs with billions of edges. It proposes TGL, a unified framework that supports various TGNN architectures, introduces novel techniques like a random chunk scheduling technique, and evaluates its performance on new large-scale datasets.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b12\": 0.9,\n    \"b14\": 1,\n    \"b16\": 0.9,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b17\": 0.8,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b5\": 0.6,\n    \"b8\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of training Temporal Graph Neural Networks (TGNNs) on large-scale dynamic graphs. It proposes a unified framework called TGL, which includes components like a temporal sampler, mailbox, node memory module, memory updater, and message passing engine. Key contributions include a CSR-based data structure for rapid access to temporal neighbors, a parallel sampler, and a novel random chunk scheduling technique.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b9\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Temporal Graph Neural Networks (TGNNs) on large-scale dynamic graphs. The proposed TGL framework introduces novel components such as a CSR-based data structure for efficient sampling, a parallel temporal sampler, and a random chunk scheduling technique to mitigate node memory obsolescence. The framework enables efficient multi-GPU training and is evaluated on both small-scale and large-scale datasets, showing significant speedups and similar or better accuracy compared to baseline methods.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b14\": 0.9,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b12\": 0.6,\n    \"b16\": 0.7,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b3\": 0.4,\n    \"b5\": 0.4,\n    \"b15\": 0.3\n  }\n}\n```"], "62296c7b5aee126c0f57d7aa": ["```json\n{\n  \"Summary\": \"The paper outlines the primary challenges in enhancing Graphormer, a deep learning model for graph representations, by addressing limitations in expressiveness and over-smoothing of conventional graph neural networks. The authors propose improvements in layer normalization and extend Graphormer for 3D molecular graph modeling, achieving better baselines on molecular datasets. They also offer theoretical insights into Graphormer's expressiveness compared to classic message-passing-based GNNs.\",\n  \"Direct Inspiration\": [\"b16\", \"b6\"],\n  \"Indirect Inspiration\": [\"b12\", \"b1\", \"b11\"],\n  \"Other Inspiration\": [\"b14\", \"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on improving the Graphormer framework by investigating the placement of layer normalization, adapting it for 3D molecular modeling, and enhancing its theoretical understanding through distributed computing theory. The primary challenges addressed include enhancing model expressiveness, better generalization, and adapting to 3D molecular data.\",\n  \"Direct Inspiration\": [\"b6\", \"b11\", \"b12\"],\n  \"Indirect Inspiration\": [\"b15\", \"b16\", \"b17\"],\n  \"Other Inspiration\": [\"b3\", \"b14\", \"b19\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper discusses improvements to the Graphormer framework, focusing on enhancing its architecture for better performance in molecular property prediction tasks and 3D molecular modeling. The key challenges addressed include optimizing layer normalization placement, developing new structural encodings for 3D molecular graphs, and understanding the theoretical expressiveness of Graphormer compared to classic message-passing-based GNNs.\",\n    \"Direct Inspiration\": {\n        \"b16\": 0.9,\n        \"b6\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.8,\n        \"b17\": 0.8,\n        \"b11\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.7,\n        \"b14\": 0.7,\n        \"b19\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": [\n    \"The paper addresses the limitations of conventional graph neural networks (GNNs) in terms of expressiveness and over-smoothing by proposing several improvements to the Graphormer framework. These improvements include changes to the placement of layer normalization, extending Graphormer to 3D molecule graph modeling, and better theoretical understanding through distributed computing theory.\"\n  ],\n  \"Direct Inspiration\": [\n    \"b18\",\n    \"b6\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b15\",\n    \"b16\",\n    \"b14\",\n    \"b11\"\n  ],\n  \"Other Inspiration\": [\n    \"b7\",\n    \"b17\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper include improving the expressiveness and overcoming the over-smoothing issues of traditional graph neural networks for graph representation tasks, and extending the Graphormer model to 3D molecular graph modeling.\",\n    \"Inspirations\": \"The paper takes significant inspiration from the Transformer architecture and aims to incorporate graph structural information effectively. Direct inspirations include works on layer normalization, spatial encoding, centrality encoding, and attention mechanisms.\"\n  },\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b18\": 1.0,\n    \"b14\": 0.9,\n    \"b3\": 0.9,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b7\": 0.8,\n    \"b16\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.6,\n    \"b1\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```"], "624fa8db5aee126c0f3a5be7": ["```json\n{\n  \"Summary\": \"The paper focuses on addressing the challenges of video generation using diffusion models, extending their application from image and audio domains. The main contributions include architectural modifications to accommodate video data, a new method for conditional video generation, and the introduction of a gradient method for autoregressive video extension.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b12\": 1.0,\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.9,\n    \"b17\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.7,\n    \"b14\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality videos using diffusion models, extending the success of these models in image and audio generation to video data. The authors propose a novel approach that incorporates straightforward architectural changes to the standard Gaussian diffusion model to accommodate video data, enabling both unconditional and text-conditioned video generation. Key contributions include the introduction of a new gradient method for conditional generation and the application of classifier-free guidance for sample quality improvement.\",\n  \"Direct Inspiration\": [\"b10\", \"b12\", \"b34\"],\n  \"Indirect Inspiration\": [\"b1\", \"b4\", \"b13\", \"b21\"],\n  \"Other Inspiration\": [\"b7\", \"b29\", \"b30\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality videos using diffusion models, extending their application from image and audio generation to video data. It proposes architectural modifications to the standard Gaussian diffusion model to accommodate video data and introduces a new gradient method for conditional generation to improve temporal coherence in generated videos.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b12\": 0.95,\n    \"b31\": 0.9,\n    \"b34\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.75,\n    \"b17\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b23\": 0.65,\n    \"b24\": 0.5,\n    \"b29\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of video generation using diffusion models, extending the standard Gaussian diffusion model with minimal modifications to handle video data. Key contributions include a novel gradient method for conditional generation, joint training on video and image modeling, and the application of classifier-free guidance for text-conditioned video generation. The paper aims to validate the efficacy of diffusion models in the video domain, achieving state-of-the-art results in both unconditional and text-conditioned settings.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1,\n    \"b12\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.9,\n    \"b22\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges of video generation using diffusion models, proposing a method for both unconditional and text-conditioned settings. It introduces a novel method for conditional generation and extends the standard image diffusion model architecture to accommodate video data.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b34\": 1.0,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b10\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b4\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```"], "6243ca9b5aee126c0fbd1cfd": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of aligning Graph Neural Networks (GNNs) with Dynamic Programming (DP) algorithms to improve sample complexity and generalization in neural algorithmic reasoning. The authors propose a framework using category theory and abstract algebra to better understand and optimize the GNN-DP connection, focusing on integral transforms and commutative monoids.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b12\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b20\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of aligning graph neural networks (GNNs) with dynamic programming (DP) through a framework based on category theory and abstract algebra. It aims to better understand the GNN-DP connection to improve algorithmic alignment and sample complexity.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b0\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b20\": 0.6,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of linking graph neural networks (GNNs) with dynamic programming (DP) algorithms, aiming to leverage their connection to improve neural algorithmic reasoning. The primary contribution lies in using category theory and abstract algebra to create a framework that aligns the operations of GNNs and DP, thereby enhancing sample complexity and generalization.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b0\": 0.9,\n    \"b26\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b17\": 0.7,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training neural networks to execute algorithmic computation in a high-dimensional latent space, focusing on the use of graph neural networks (GNNs) to align with dynamic programming (DP) algorithms. It aims to create a framework that identifies GNNs that align well with certain classes of DP algorithms using category theory and abstract algebra.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b26\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7,\n    \"b17\": 0.7,\n    \"b14\": 0.6,\n    \"b0\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of aligning Graph Neural Networks (GNNs) with Dynamic Programming (DP) to improve algorithmic reasoning. The authors propose a framework using category theory and abstract algebra to elucidate the GNN-DP connection, aiming to create GNN architectures that align better with specific classes of DP algorithms.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b0\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b26\": 0.7,\n    \"b17\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```"], "623d33155aee126c0f100f50": ["```json\n{\n    \"Summary\": \"The paper proposes a novel Graph Complementary Contrastive Learning (GraphCoCo) approach to address issues in graph contrastive learning (GCL) by introducing non-maximum erasing to learn complementary embeddings. The challenge identified is that certain 'highlighted' dimensions dominate the embeddings, reducing their expressiveness in downstream tasks. The paper draws inspiration from various contrastive learning methods in computer vision and natural language processing.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1.0,\n        \"b7\": 1.0,\n        \"b19\": 1.0,\n        \"b22\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.8,\n        \"b20\": 0.8\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is the limitation of graph contrastive learning where embeddings of positive pairs share common 'highlighted' dimensions, reducing the expressiveness in downstream tasks. The proposed GraphCoCo approach introduces non-maximum erasing to learn complementary embeddings by erasing significant dimensions in one view based on the other, thus improving performance in graph classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b7\": 0.9,\n    \"b22\": 0.85,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b14\": 0.7,\n    \"b17\": 0.65,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.55,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of limited expressiveness in graph contrastive learning due to 'highlighted' dimensions in embeddings. It proposes a novel approach called Graph Complementary Contrastive Learning (GraphCoCo) which uses non-maximum erasing to learn complementary embeddings from two augmented views of the same graph.\",\n  \"Direct Inspiration\": [\"b2\", \"b22\", \"b7\"],\n  \"Indirect Inspiration\": [\"b19\", \"b14\"],\n  \"Other Inspiration\": [\"b10\", \"b25\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of 'highlighted' dimensions in graph contrastive learning, which limits the expressiveness of embeddings in downstream tasks. The proposed GraphCoCo approach uses non-maximum erasing to mitigate this issue by learning complementary embeddings of graphs with two augmented views.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b22\": 0.8,\n    \"b19\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b14\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.65,\n    \"b20\": 0.6,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of limited expressiveness in graph contrastive learning due to prominent 'highlighted' dimensions in embeddings. The proposed GraphCoCo algorithm uses non-maximum erasing to create complementary embeddings from two augmented views of the same graph, enhancing representation for downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b7\": 0.9,\n    \"b19\": 1,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b14\": 0.7,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6\n  }\n}\n```"], "625e1a335aee126c0feca4ba": ["```json\n{\n  \"Summary\": \"The paper discusses the challenges of solving machine learning tasks using graph neural networks (GNNs) with a focus on message passing GNNs (MPNNs). The primary challenges include representational power, graph isomorphism testing, and limitations of MPNNs. The paper proposes algorithms and solutions to overcome these challenges, taking inspiration from various previous works in these areas.\",\n  \"Direct Inspiration\": {\n    \"b42\": 0.9,\n    \"b43\": 0.9,\n    \"b46\": 0.9,\n    \"b58\": 0.9,\n    \"b76\": 0.9,\n    \"b95\": 0.9,\n    \"b107\": 0.8,\n    \"b79\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b71\": 0.7,\n    \"b84\": 0.7,\n    \"b85\": 0.7,\n    \"b5\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b51\": 0.7,\n    \"b19\": 0.7,\n    \"b57\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.6,\n    \"b24\": 0.6,\n    \"b93\": 0.6,\n    \"b50\": 0.6,\n    \"b31\": 0.6,\n    \"b41\": 0.6,\n    \"b109\": 0.6,\n    \"b110\": 0.6,\n    \"b106\": 0.6,\n    \"b18\": 0.5,\n    \"b30\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of learning from graph-structured data using Graph Neural Networks (GNNs). The authors focus on Message Passing Neural Networks (MPNNs) and their representational power, particularly in the context of graph isomorphism testing and limitations that motivate higher-order GNNs.\",\n  \"Direct Inspiration\": {\n    \"b42\": 0.9,\n    \"b43\": 0.9,\n    \"b46\": 0.9,\n    \"b58\": 0.9,\n    \"b76\": 0.9,\n    \"b95\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b31\": 0.7,\n    \"b71\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b87\": 0.6,\n    \"b107\": 0.6,\n    \"b109\": 0.6,\n    \"b110\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the development of effective graph neural networks (GNNs) for various machine learning tasks involving graph-structured data. The algorithm proposed is a Message Passing Neural Network (MPNN) which iteratively updates node embeddings based on their neighbors' embeddings and edge attributes. The paper also discusses the representational power of GNNs in terms of graph isomorphism and computational limitations, proposing enhancements like higher-order functions and node IDs to improve GNN capabilities.\",\n  \"Direct Inspiration\": {\n    \"b42\": 0.9,\n    \"b43\": 0.9,\n    \"b46\": 0.9,\n    \"b58\": 0.9,\n    \"b76\": 0.9,\n    \"b95\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b87\": 0.8,\n    \"b107\": 0.8,\n    \"b110\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.7,\n    \"b71\": 0.7,\n    \"b109\": 0.7,\n    \"b93\": 0.7,\n    \"b39\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of graph-based machine learning tasks, focusing on the development of Graph Neural Networks (GNNs) and their representational power. It introduces Message Passing Neural Networks (MPNNs) and discusses their limitations, particularly in distinguishing certain graph structures and properties. The authors propose methods to enhance GNNs' representational power by incorporating node IDs and higher-order functions.\",\n  \"Direct Inspiration\": {\n    \"b42\": 1.0,\n    \"b43\": 1.0,\n    \"b46\": 1.0,\n    \"b58\": 1.0,\n    \"b76\": 1.0,\n    \"b95\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b93\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b71\": 0.6,\n    \"b41\": 0.6,\n    \"b107\": 0.6,\n    \"b110\": 0.6,\n    \"b109\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper focuses on the challenges of solving machine learning tasks using graph neural networks (GNNs), particularly message passing neural networks (MPNNs). The paper addresses issues such as representational power, permutation invariance, and the limitations of MPNNs in distinguishing certain graph properties. It proposes enhancements to GNNs to overcome these challenges.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b42\", \"b43\", \"b46\", \"b58\", \"b76\", \"b95\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b109\", \"b107\", \"b110\", \"b87\", \"b106\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b93\", \"b39\", \"b31\", \"b71\"]\n    }\n}\n```"], "62393e7f5aee126c0f126162": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of communication volume, memory requirement, and imbalanced memory consumption in distributed GCN training. It proposes BNS-GCN, a method that samples boundary nodes to reduce communication and memory costs while maintaining accuracy. This work is motivated by the inefficiencies in current partition-parallel training methods and aims to provide a scalable solution.\",\n    \"Direct Inspiration\": [\"b11\", \"b2\", \"b24\", \"b31\", \"b30\", \"b33\"],\n    \"Indirect Inspiration\": [\"b37\", \"b5\", \"b15\"],\n    \"Other Inspiration\": [\"b13\", \"b6\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of training Graph Convolutional Networks (GCNs) at scale, specifically focusing on reducing communication and memory overheads in distributed GCN training. The proposed BNS-GCN framework introduces boundary node sampling to tackle these issues effectively.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b2\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b15\": 0.8,\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges addressed in the paper are the overwhelming communication volume, prohibitive memory requirement, and imbalanced memory consumption in distributed GCN training. The proposed algorithm, BNS-GCN, aims to alleviate these issues by randomly sampling features of boundary nodes to reduce communication and memory costs while maintaining accuracy.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b11\": 0.9,\n        \"b30\": 0.8,\n        \"b31\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.7,\n        \"b24\": 0.7,\n        \"b33\": 0.7,\n        \"b37\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b13\": 0.6,\n        \"b42\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalable and efficient training of Graph Convolutional Networks (GCNs) on large graphs. It identifies three main challenges in partition-parallel training of GCNs: overwhelming communication volume, prohibitive memory requirement, and imbalanced memory consumption. The proposed method, BNS-GCN, aims to reduce these issues by sampling boundary nodes during training, thus reducing communication and memory costs while improving accuracy.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b2\": 0.8,\n    \"b24\": 0.85,\n    \"b31\": 0.8,\n    \"b30\": 0.85,\n    \"b33\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b37\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in training graph convolutional networks (GCNs) at scale, focusing on memory and communication overheads due to boundary nodes in distributed training. The proposed solution, BNS-GCN, uses boundary node sampling to reduce memory and communication costs while maintaining accuracy.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b31\": 0.8,\n    \"b30\": 0.75,\n    \"b33\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b15\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```"], "6241273e5aee126c0f292ae3": ["```json\n{\n  \"Summary\": \"The paper presents the P5 framework, a unified approach integrating multiple recommendation tasks into a shared sequence-to-sequence language model using personalized prompts. The primary challenge is to bridge the gap between current solutions and a comprehensive recommender system, accommodating diverse features and tasks. The framework is inspired by recent advancements in multitask prompt-based training.\",\n  \"Direct Inspiration\": [\"b1\", \"b48\", \"b63\"],\n  \"Indirect Inspiration\": [\"b2\", \"b43\", \"b52\", \"b53\", \"b68\"],\n  \"Other Inspiration\": [\"b5\", \"b62\", \"b67\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating a comprehensive recommender system that can accommodate diverse features and different types of tasks. The proposed solution is the 'Pretrain, Personalized Prompt & Predict Paradigm' (P5), which integrates various recommendation tasks into a unified sequence-to-sequence framework using personalized prompts.\",\n  \"Direct Inspiration\": [\"b1\", \"b48\", \"b63\"],\n  \"Indirect Inspiration\": [\"b2\", \"b43\"],\n  \"Other Inspiration\": [\"b52\", \"b53\", \"b68\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a comprehensive recommender system that accommodates diverse features and a wide spectrum of application scenarios. The proposed solution is a unified 'Pretrain, Personalized Prompt & Predict Paradigm' (P5), which integrates various recommendation tasks into a shared conditional language generation framework using prompt-based training.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b48\": 0.9,\n    \"b63\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b43\": 0.7,\n    \"b52\": 0.6,\n    \"b68\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.4,\n    \"b4\": 0.4,\n    \"b18\": 0.4,\n    \"b70\": 0.4,\n    \"b7\": 0.4,\n    \"b57\": 0.4,\n    \"b72\": 0.4,\n    \"b29\": 0.4,\n    \"b58\": 0.4,\n    \"b66\": 0.4,\n    \"b73\": 0.4,\n    \"b20\": 0.3,\n    \"b56\": 0.3,\n    \"b59\": 0.3,\n    \"b76\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating a comprehensive recommender system that accommodates diverse features and various recommendation tasks. The proposed solution, P5, utilizes a unified 'Pretrain, Personalized Prompt & Predict Paradigm' to integrate multiple recommendation tasks into a shared conditional language generation framework inspired by recent progress in multitask prompt-based training.\",\n  \"Direct Inspiration\": [\"b1\", \"b48\", \"b63\"],\n  \"Indirect Inspiration\": [\"b2\", \"b43\"],\n  \"Other Inspiration\": [\"b52\", \"b53\", \"b68\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a unified 'Pretrain, Personalized Prompt & Predict Paradigm' (P5) to integrate various recommendation-related tasks into a shared conditional language generation framework. The main challenges addressed include the integration of diverse features and different types of tasks into a comprehensive recommender system. P5 is inspired by the recent progress in multitask prompt-based training and aims to achieve zero-shot generalization to unseen personalized prompts and new items.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b48\": 1.0,\n    \"b63\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b43\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b52\": 0.7,\n    \"b53\": 0.7,\n    \"b68\": 0.7\n  }\n}\n```"], "6271e0e75aee126c0f574720": ["```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies in existing matrix multiplication accelerators (MXUs) for broader matrix-based applications. It proposes SIMD 2 architecture to support a wider set of matrix operations that follow a semiring-like structure, including eight new types of instructions. The primary challenges include the need for algorithm optimizations and under-utilization of MXUs for non-GEMM operations. The proposed architecture aims to minimize overhead by extending existing MXUs and sharing infrastructure to support the new operations.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b51\": 0.8,\n    \"b52\": 0.8,\n    \"b58\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b46\": 0.6,\n    \"b61\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of performing efficient matrix operations for a wide range of applications, particularly those that follow a semiring-like structure. The proposed SIMD 2 architecture extends existing matrix-multiplication units (MXUs) to support a broader set of matrix operations with minimal hardware overhead, improving the performance of various matrix-based applications.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b51\": 1,\n    \"b52\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b10\": 0.7,\n    \"b22\": 0.8,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b20\": 0.6,\n    \"b61\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The main challenges addressed in the paper involve optimizing matrix operations to improve performance for a broader set of applications beyond GEMM, by proposing the SIMD 2 architecture. This architecture introduces a wider set of matrix-based instructions that fit application demands without requiring sophisticated code transformations and minimizes overhead by reusing existing MXU functions and data paths.\",\n    \"Direct Inspiration\": {\n        \"b26\": 0.9,\n        \"b51\": 0.8,\n        \"b52\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b22\": 0.7,\n        \"b4\": 0.65,\n        \"b58\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b9\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inefficiency of modern MXUs to handle semiring-like structured matrix operations beyond GEMM, causing under-utilization and necessitating complex algorithm optimizations. The proposed SIMD 2 architecture aims to address this by extending the functionality of existing MXUs to support a wider range of matrix operations, thereby improving efficiency and reducing overhead.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b51\": 1.0,\n    \"b52\": 1.0,\n    \"b58\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b22\": 0.8,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b6\": 0.6,\n    \"b9\": 0.6,\n    \"b23\": 0.6,\n    \"b39\": 0.6,\n    \"b53\": 0.6,\n    \"b69\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Efficiently supporting a broader set of matrix operations beyond GEMMs on hardware accelerators, minimizing overheads, and improving performance for semiring-like matrix applications.\",\n    \"inspirations\": \"The commonality among matrix problems with semiring-like structures and the potential of existing GEMM accelerators to support these structures with minimal changes.\"\n  },\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b51\": 1.0,\n    \"b52\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b22\": 0.7,\n    \"b53\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```"], "6274c91a5aee126c0f71246a": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of incomplete and noisy data in knowledge graphs and proposes an Uncertainty-aware Pseudo Label Refinery (UPLR) framework for unsupervised entity alignment. The key contributions include a new framework for mining confident samples from unlabeled data, a novel non-sampling calibration strategy, and a gradual enhancement strategy for domain similarity.\",\n  \"Direct Inspiration\": {\n    \"Inspired by\": [\"b1\"],\n    \"Motivated by\": [\"b30\", \"b31\"],\n    \"Inspired us\": [],\n    \"Motivated us\": [],\n    \"take inspiration\": [],\n    \"the pioneering/previous work\": [],\n    \"following.. we adopt ... to solve the challenge/problem\": [],\n    \"we use... based on to achieve...\": []\n  },\n  \"Indirect Inspiration\": {\n    \"Inspired by\": [],\n    \"Motivated by\": [],\n    \"Inspired us\": [],\n    \"Motivated us\": [],\n    \"take inspiration\": [],\n    \"the pioneering/previous work\": [],\n    \"following.. we adopt ... to solve the challenge/problem\": [],\n    \"we use... based on to achieve...\": []\n  },\n  \"Other Inspiration\": [\"b13\", \"b35\", \"b32\", \"b38\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of entity alignment in Knowledge Graphs (KGs) without relying on labeled entity pairs. It introduces the Uncertainty-aware Pseudo Label Refinery (UPLR) framework, which generates high-quality pseudo-labels in an unsupervised manner. The main contributions include a novel loss function for pseudo-labeling, a non-sampling calibration strategy, and a gradual enhancement strategy to improve label quality. The framework leverages Graph Attention Networks (GAT) and introduces a Gate Graph Attention Network (GateGAT) to handle noisy labels effectively.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b13\": 0.9,\n    \"b35\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b21\": 0.65,\n    \"b32\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.5,\n    \"b45\": 0.45\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of unsupervised entity alignment in knowledge graphs, proposing the Uncertainty-aware Pseudo Label Refinery (UPLR) framework. This framework iteratively refines pseudo-labels for cross-graph entity pairs based on semantic embeddings. The key innovations include reducing the impact of noisy pseudo-labels through a novel calibration strategy and enhancing the alignment process without relying on negative sampling.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1, \n        \"b13\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.9, \n        \"b36\": 0.9, \n        \"b21\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b23\": 0.7, \n        \"b35\": 0.8, \n        \"b32\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised entity alignment in knowledge graphs, proposing an Uncertainty-aware Pseudo Label Refinery (UPLR) framework that iteratively refines pseudo-labels for improved alignment accuracy without relying on negative pairs.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b30\": 0.85,\n    \"b31\": 0.8,\n    \"b36\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b32\": 0.65,\n    \"b40\": 0.6,\n    \"b41\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b7\": 0.5,\n    \"b21\": 0.5,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of entity alignment in knowledge graphs, which is crucial for integrating multiple KGs but is hindered by the need for labeled entity pairs. The authors propose an Uncertainty-aware Pseudo Label Refinery (UPLR) framework that uses a completely unsupervised approach to entity alignment, leveraging pseudo-labels and a Gate Graph Attention Network (GateGAT) to iteratively improve alignment accuracy without relying on negative pairs.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b13\": 0.9,\n    \"b35\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.8,\n    \"b21\": 0.75,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.65,\n    \"b17\": 0.6,\n    \"b37\": 0.55\n  }\n}\n```"], "626b868a6750f822a0a5e16d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of creating a multimodal generative model that can perform various vision-language tasks (e.g., captioning, visual question answering) with few-shot learning. The Flamingo model is proposed, which leverages large-scale pretrained vision and language models, combining them with novel architectural components and training strategies to enable effective multimodal task adaptation without fine-tuning.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b47\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b20\": 0.7,\n    \"b55\": 0.7,\n    \"b93\": 0.7,\n    \"b94\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b53\": 0.6,\n    \"b61\": 0.6,\n    \"b62\": 0.6,\n    \"b73\": 0.5,\n    \"b85\": 0.5,\n    \"b115\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in multimodal generative modeling, particularly integrating vision and language models without extensive fine-tuning. It proposes the Flamingo model, which uses a novel architecture to interleave text, images, and videos, leveraging pretrained models and specific training strategies to achieve state-of-the-art few-shot learning performance across various tasks.\",\n  \"Direct Inspiration\": [\n    \"b11\",\n    \"b47\",\n    \"b94\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b1\",\n    \"b55\",\n    \"b93\",\n    \"b142\",\n    \"b153\"\n  ],\n  \"Other Inspiration\": [\n    \"b19\",\n    \"b28\",\n    \"b53\",\n    \"b61\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces the Flamingo family of Visual Language Models (VLMs) that tackle various multimodal tasks using few-shot learning. The primary challenges addressed include integrating pretrained language models with visual inputs, efficiently handling high-dimensional visual data, and acquiring heterogeneous training data. Key innovations include novel architectural components for interleaving visual and text data, and specific training strategies for enhancing performance.\",\n    \"Direct Inspiration\": {\n        \"b11\": 1,\n        \"b20\": 0.9,\n        \"b47\": 1,\n        \"b94\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b55\": 0.8,\n        \"b93\": 0.8,\n        \"b15\": 0.7,\n        \"b28\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b53\": 0.6,\n        \"b62\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of creating a multimodal generative model that integrates both visual and language inputs. The proposed model, Flamingo, combines large language models (LLMs) with visual embeddings and introduces architectural innovations to handle the multimodal input. The main challenges include unifying strong single-modal models, supporting both images and videos, and obtaining heterogeneous training data. The model is inspired by recent advancements in large-scale generative language models and aims to achieve few-shot learning without fine-tuning.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b20\": 0.9,\n    \"b47\": 0.95,\n    \"b94\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b53\": 0.8,\n    \"b55\": 0.75,\n    \"b93\": 0.75,\n    \"b124\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b28\": 0.6,\n    \"b62\": 0.7,\n    \"b85\": 0.65,\n    \"b95\": 0.7,\n    \"b127\": 0.65,\n    \"b152\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of multimodal generative modeling, specifically integrating strong single-modal models, supporting both images and videos, and obtaining heterogeneous training data to induce generalist capabilities. The proposed Flamingo model combines pretrained language models with visual embeddings, using novel architectural components and training strategies to perform well in few-shot learning tasks without fine-tuning.\",\n  \"Direct Inspiration\": [\"b11\", \"b20\", \"b47\", \"b94\"],\n  \"Indirect Inspiration\": [\"b15\", \"b28\", \"b85\", \"b93\"],\n  \"Other Inspiration\": [\"b53\", \"b61\", \"b73\", \"b142\"]\n}\n```"], "626754bb5aee126c0fbccbaa": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of query autocompletion (QAC) systems being trained to mimic user behavior rather than optimizing for the utility of query suggestions. It proposes a novel utility-aware QAC approach that focuses on ranking performance of queries by using downstream feedback (like purchases) instead of previous user queries. This involves a counterfactual learning approach to develop an unbiased estimator of ranking performance under position-bias models.\",\n    \"Direct Inspiration\": {\n        \"b28\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b53\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b32\": 0.6,\n        \"b34\": 0.6,\n        \"b41\": 0.5,\n        \"b56\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Query Auto-Completion (QAC) systems by presenting a novel utility-aware QAC framework. This framework optimizes query suggestions based on the utility (ranking performance) and directly improves retrieval performance by using downstream feedback from user interactions.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b53\": 0.7,\n    \"b56\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b32\": 0.6,\n    \"b34\": 0.6,\n    \"b55\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the suboptimality of current Query Auto-Completion (QAC) systems, which are typically trained to mimic user behavior rather than optimize for query efficacy. The novel approach proposed, called utility-aware QAC, aims to directly optimize the retrieval performance of the queries by incorporating downstream feedback, such as user purchases or content streams, into the training objective. This involves formulating the task as learning a ranking of rankings, developing an unbiased estimator of ranking performance using counterfactual learning, and providing a theoretical foundation for the approach. The system architecture includes utility-aware query retrieval and re-ranking using techniques from eXtreme multi-label learning and gradient boosting.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0,\n    \"b53\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b34\": 0.7,\n    \"b56\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b15\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents a novel utility-aware query autocompletion (QAC) system that aims to optimize the retrieval performance of queries by incorporating downstream user feedback into the training process. The proposed framework leverages a counterfactual learning approach to estimate the quality of item rankings from biased, item-level feedback, and introduces a new training objective that directly optimizes the efficacy of the suggested queries. The system architecture involves both retrieval and re-ranking stages, and the approach is validated through theoretical analysis and empirical evaluation on public and proprietary datasets.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b53\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b32\": 0.75,\n    \"b34\": 0.75,\n    \"b55\": 0.75,\n    \"b56\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b39\": 0.65,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of traditional Query AutoCompletion (QAC) systems by introducing a utility-aware QAC approach that aims to improve retrieval performance by ranking query suggestions based on downstream feedback. Key contributions include a novel training framework, theoretical analysis ensuring consistency, and empirical validation on public and real-world datasets.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b18\": 0.7,\n    \"b41\": 0.7,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b53\": 0.5\n  }\n}\n```"], "627332775aee126c0f18d585": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the handling of out-of-vocabulary seeds in seed-guided topic discovery and leveraging the power of pre-trained language models (PLMs) for better topic coherence. The SEETOPIC framework is proposed to address these challenges by combining general linguistic knowledge from PLMs with local corpus-specific embeddings through an iterative ensemble ranking process.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b2\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in seed-guided topic discovery, especially for out-of-vocabulary seeds, using a framework called SEETOPIC. This framework leverages pre-trained language models (PLMs) and embedding learning to improve the coherence and accuracy of discovered topics.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b13\": 0.9,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b12\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b24\": 0.5,\n    \"b26\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of seed-guided topic discovery in the presence of out-of-vocabulary seeds. It proposes the SEETOPIC framework, which utilizes pre-trained language models (PLMs) to handle out-of-vocabulary seeds and learn their semantics effectively. The framework consists of a general representation module using PLMs and a seed-guided local representation module for embedding learning, optimized for topic coherence.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.95,\n    \"b13\": 0.9,\n    \"b10\": 0.85,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b26\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Handling out-of-vocabulary seeds in topic discovery\",\n      \"Leveraging pre-trained language models for better topic coherence\"\n    ],\n    \"inspirations\": [\n      \"Seed-guided topic discovery\",\n      \"Pre-trained language models\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b10\": 0.9,\n    \"b22\": 0.95,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b26\": 0.75,\n    \"b32\": 0.6,\n    \"b34\": 0.6,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.85,\n    \"b12\": 0.8,\n    \"b30\": 0.65,\n    \"b37\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of seed-guided topic discovery in the presence of out-of-vocabulary seeds. The proposed SEETOPIC framework leverages pre-trained language models (PLMs) to handle out-of-vocabulary seeds and combines this with local text semantics from the input corpus.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.95,\n    \"b22\": 0.85,\n    \"b13\": 0.80,\n    \"b10\": 0.75,\n    \"b24\": 0.70\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.65,\n    \"b12\": 0.60,\n    \"b30\": 0.55,\n    \"b37\": 0.50,\n    \"b32\": 0.45\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.40,\n    \"b14\": 0.35,\n    \"b29\": 0.30,\n    \"b17\": 0.25,\n    \"b34\": 0.20\n  }\n}\n```"], "621ee1895aee126c0f26af67": ["```json\n{\n    \"Summary\": \"The paper addresses the scalability challenges of Graph Neural Networks (GNNs) by proposing a novel Scalable Graph Neural Architecture Paradigm (SGAP). The main contributions include re-designing the GNN pipeline to separate data aggregation from training, creating a general design space for scalable GNNs, and implementing an auto-search system to find optimal GNN architectures. The proposed system, PaSca, demonstrates improved training scalability, efficiency, and performance on various graph datasets.\",\n    \"Direct Inspiration\": {\n        \"b11\": 0.95,\n        \"b51\": 0.9,\n        \"b67\": 0.9,\n        \"b5\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.8,\n        \"b14\": 0.75,\n        \"b42\": 0.7,\n        \"b49\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.6,\n        \"b32\": 0.6,\n        \"b61\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the scalability challenges of GNNs in large-scale graphs by proposing a novel paradigm, the Scalable Graph Neural Architecture Paradigm (SGAP). The SGAP paradigm decouples the data aggregation and training processes and introduces a design space with adaptive aggregation and a complementary post-processing stage. Additionally, the paper introduces an auto-search system to explore scalable GNN designs automatically.\",\n  \"Direct Inspiration\": [\"b51\", \"b11\", \"b67\", \"b5\"],\n  \"Indirect Inspiration\": [\"b14\", \"b42\", \"b49\"],\n  \"Other Inspiration\": [\"b19\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the major challenge of large-scale Graph Neural Networks (GNNs) computation, particularly the exponential IO overhead caused by recursive neighborhood aggregation. The authors propose a novel Scalable Graph Neural Architecture Paradigm (SGAP) that separates data aggregation from the training process to enhance scalability. They introduce a general design space for scalable GNNs and an auto-search system named PaSca to explore this design space.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b51\": 0.9,\n    \"b67\": 0.9,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b19\": 0.7,\n    \"b20\": 0.7,\n    \"b22\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.6,\n    \"b49\": 0.6,\n    \"b58\": 0.6,\n    \"b66\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the scalability challenges of Graph Neural Networks (GNNs) by proposing a novel scalable GNN pipeline called the Scalable Graph Neural Architecture Paradigm (SGAP). This paradigm decouples the data aggregation from the training process, which is different from traditional GNN pipelines. The paper introduces a design space for scalable GNNs and implements an auto-search system named PaSca to explore this space. The primary challenge addressed is the exponential growth of neighborhood size in GNNs, leading to high IO overhead and communication costs in distributed settings.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b51\": 1.0,\n    \"b67\": 1.0,\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b42\": 0.8,\n    \"b49\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b18\": 0.6,\n    \"b45\": 0.6,\n    \"b54\": 0.6,\n    \"b22\": 0.6,\n    \"b32\": 0.6,\n    \"b61\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the exponential growth of neighborhood size leading to IO overhead and high communication costs during the training of GNNs. The authors propose the Scalable Graph Neural Architecture Paradigm (SGAP) with pre/post-processing stages to address scalability, introducing three operation abstractions and a general design space for scalable GNNs. They also implement an auto-search system to automate the design space exploration.\",\n  \"Direct Inspiration\": [\"b11\", \"b51\", \"b67\"],\n  \"Indirect Inspiration\": [\"b14\", \"b49\", \"b42\"],\n  \"Other Inspiration\": [\"b5\", \"b19\", \"b20\"]\n}\n```"], "628749425aee126c0ffee615": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Label noise in large-scale annotated training data for fine-grained entity typing (FET)\",\n      \"Reliance on prediction distribution and instance-level information which can be biased\",\n      \"Need for robust methods that leverage both global and local information\"\n    ],\n    \"Inspirations\": [\n      \"Weighted training and loss correction methods in machine learning\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"References\": [\n      \"b12\"\n    ]\n  },\n  \"Indirect Inspiration\": {\n    \"References\": [\n      \"b26\",\n      \"b29\",\n      \"b39\"\n    ]\n  },\n  \"Other Inspiration\": {\n    \"References\": []\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of label noise in Fine-grained Entity Typing (FET) and proposes a novel framework, FCLC, inspired by methods like weighted training and loss correction. The framework utilizes feature clustering and a cluster-level confusion matrix to improve noise handling in FET tasks.\",\n  \"Direct Inspiration\": [\"b12\", \"b26\"],\n  \"Indirect Inspiration\": [\"b29\"],\n  \"Other Inspiration\": [\"b30\", \"b37\", \"b39\", \"b41\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of label noise in fine-grained entity typing (FET) tasks by proposing a novel framework, FCLC, which involves feature clustering, estimating cluster-level confusion matrices, and loss correction. The proposed method aims to leverage both global and local information to improve the robustness of FET models against noisy labels.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of label noise in fine-grained entity typing (FET) by proposing a novel framework (FCLC) that relies on feature representations and learns both global and local information through clustering and loss correction. The framework aims to alleviate noise by using cluster-level label confusion matrices and is inspired by weighted training and loss correction methods.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.9,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b23\": 0.7,\n    \"b30\": 0.7,\n    \"b37\": 0.7,\n    \"b41\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of label noise in fine-grained entity typing (FET) by proposing a novel framework called FCLC. The key challenges include handling noisy labels resulting from distant supervision and the limitations of existing methods that rely heavily on prediction distribution and instance-level information. The proposed method leverages feature clustering, a cluster-level confusion matrix, and loss correction to improve noise estimation and label correction. The main contributions are the novel training method, the design of the FCLC framework, and experimental validation showing significant improvements over state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b26\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.8,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b23\": 0.6,\n    \"b37\": 0.6,\n    \"b41\": 0.6,\n    \"b42\": 0.6\n  }\n}\n```"], "62451c2b5aee126c0f47ab40": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"Incorporating multi-hop knowledge from document links into language model pretraining, improving multi-document understanding and question answering.\",\n    \"inspirations\": \"Previous works on language models like BERT and GPTs, usage of masked language modeling, and graph-based self-supervised learning.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b18\", \"b32\", \"b74\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b26\", \"b77\", \"b48\", \"b35\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b2\", \"b12\", \"b44\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating multi-hop knowledge from document links into language model (LM) pretraining, which existing models like BERT and GPTs typically do not include. The proposed algorithm, LinkBERT, leverages hyperlinks and citation links to create LM inputs and introduces a Document Relation Prediction (DRP) objective to enhance understanding of relations between documents.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b25\": 0.9,\n    \"b77\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b2\": 0.8,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b58\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper proposes LinkBERT, a language model pretraining method that incorporates document link knowledge through hyperlinks and citation links to improve multi-hop reasoning, multi-document understanding, and few-shot question answering. The primary challenges addressed include the inability of existing LMs to model links between documents and the need for better pretraining methods to internalize multi-hop knowledge.\",\n    \"Direct Inspiration\": {\n        \"b18\": 1.0,\n        \"b48\": 0.9,\n        \"b35\": 0.9,\n        \"b74\": 0.8,\n        \"b32\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b55\": 0.7,\n        \"b8\": 0.7,\n        \"b58\": 0.7,\n        \"b25\": 0.6,\n        \"b51\": 0.6,\n        \"b77\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b37\": 0.5,\n        \"b54\": 0.5,\n        \"b73\": 0.5,\n        \"b26\": 0.5,\n        \"b44\": 0.5,\n        \"b2\": 0.5,\n        \"b14\": 0.4,\n        \"b12\": 0.4,\n        \"b60\": 0.4,\n        \"b50\": 0.4,\n        \"b57\": 0.4,\n        \"b76\": 0.4,\n        \"b5\": 0.4,\n        \"b38\": 0.4,\n        \"b16\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces LinkBERT, a novel language model pretraining method that incorporates document link knowledge to overcome limitations in existing LM pretraining methods. The key challenges addressed include the inability of traditional LMs to model links between documents and leverage multi-hop knowledge. The proposed solution involves viewing the pretraining corpus as a graph of documents and using two self-supervised objectives: masked language modeling (MLM) and document relation prediction (DRP).\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b77\": 0.8,\n    \"b32\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.6,\n    \"b35\": 0.6,\n    \"b55\": 0.6,\n    \"b8\": 0.5,\n    \"b58\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b48\": 0.4,\n    \"b25\": 0.4,\n    \"b2\": 0.3,\n    \"b11\": 0.3,\n    \"b44\": 0.3,\n    \"b26\": 0.3,\n    \"b50\": 0.3,\n    \"b12\": 0.3,\n    \"b57\": 0.3,\n    \"b76\": 0.3,\n    \"b5\": 0.3,\n    \"b38\": 0.3,\n    \"b16\": 0.3,\n    \"b7\": 0.2,\n    \"b49\": 0.2,\n    \"b67\": 0.2,\n    \"b75\": 0.2,\n    \"b36\": 0.2,\n    \"b42\": 0.2,\n    \"b21\": 0.2,\n    \"b66\": 0.2,\n    \"b59\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating multi-hop knowledge into language model pretraining by leveraging document links (e.g., hyperlinks). The proposed model, LinkBERT, uses a graph-based approach to combine linked documents within the same context, aiming to improve performance on tasks requiring multi-document understanding, such as question answering. Key techniques include masked language modeling (MLM) and a novel Document Relation Prediction (DRP) objective.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b74\": 0.9,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b48\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b77\": 0.6,\n    \"b55\": 0.6\n  }\n}\n```"], "6233f88d5aee126c0f94b61a": ["```json\n{\n    \"Summary\": \"The paper addresses the inefficiencies in Data Augmentation (DA) methods, particularly in natural language processing (NLP), where current methods either lack sample efficiency or incur high training costs. The proposed solution, Glitter, is a universal DA method designed to enhance sample efficiency and adapt to various training setups without additional training costs. The methodology involves a minimax approach to select the most impactful augmented samples from a pre-generated pool, aiming to optimize the task objective. The paper demonstrates the effectiveness of Glitter through empirical evaluations on popular datasets.\",\n    \"Direct Inspiration\": {\n        \"b9\": 0.9,\n        \"b45\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b36\": 0.75,\n        \"b38\": 0.8,\n        \"b52\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.6,\n        \"b19\": 0.65,\n        \"b42\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the sample inefficiency and training inefficiency of current Data Augmentation (DA) methods in NLP. The proposed algorithm, Glitter, aims to be a universal DA method that improves sample efficiency and can be applied to any DA method without sacrificing performance. Glitter uses a minimax approach to select a subset of augmented samples with maximal expected loss to optimize the task objective during training.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b45\": 1,\n    \"b52\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.7,\n    \"b36\": 0.6,\n    \"b22\": 0.5,\n    \"b51\": 0.5,\n    \"b42\": 0.5,\n    \"b33\": 0.5,\n    \"b18\": 0.5,\n    \"b16\": 0.5,\n    \"b19\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.4,\n    \"b41\": 0.4,\n    \"b14\": 0.4,\n    \"b1\": 0.4,\n    \"b7\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in data augmentation (DA) for natural language processing (NLP), specifically focusing on sample efficiency and training efficiency. It proposes a universal DA method called Glitter that can be integrated into any DA method to enhance sample efficiency and task-awareness without compromising performance. The method employs a minimax approach to select a small subset of augmented samples with maximal expected loss and optimizes the task objective on this subset.\",\n  \"Direct Inspiration\": {\n    \"1\": \"b9\",\n    \"2\": \"b45\"\n  },\n  \"Indirect Inspiration\": {\n    \"1\": \"b52\",\n    \"2\": \"b36\",\n    \"3\": \"b38\"\n  },\n  \"Other Inspiration\": {\n    \"1\": \"b42\",\n    \"2\": \"b22\",\n    \"3\": \"b48\",\n    \"4\": \"b33\"\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of data augmentation (DA) in natural language processing (NLP), focusing on sample efficiency and training efficiency. It proposes a novel method called Glitter, which is a universal DA method that can be applied to any DA method to enforce sample efficiency while maintaining or improving performance. The key contributions include strategies to adapt Glitter for various training setups and empirical evaluations demonstrating superior performance over state-of-the-art DA methods.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b45\": 0.85,\n    \"b52\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.75,\n    \"b36\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b42\": 0.6,\n    \"b51\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the inefficiency and high computational cost of existing Data Augmentation (DA) methods in Natural Language Processing (NLP). The proposed algorithm, Glitter, aims to improve both sample efficiency and universality without sacrificing performance. It uses a minimax approach to select a small subset of augmented samples with maximal expected loss during training, thereby optimizing the task objective for this selected subset.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b45\": 1.0,\n    \"b52\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.8,\n    \"b36\": 0.7,\n    \"b42\": 0.6,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b51\": 0.5,\n    \"b33\": 0.5,\n    \"b8\": 0.5\n  }\n}\n```"], "628c4ce15aee126c0ff596d4": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the refinement of 3D protein structures, particularly protein complexes, which is difficult due to the large search space and physical constraints. The authors propose a novel E(3)-equivariant graph deep learning model (EGR) for multi-task structure refinement and quality assessment of protein complexes. The EGR model combines insights from Equivariant Graph Neural Networks and other methods to refine and assess protein complex structures concurrently, achieving significant speed-ups in inference time compared to traditional methods.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b7\": 0.5,\n    \"b40\": 0.5,\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of refining and assessing the quality of protein complex structures using a novel E(3)-equivariant graph deep learning model, EGR. The model incorporates E(3)-equivariant graph neural networks and is designed to predict refined structures and assess their quality in a single forward pass, significantly improving inference time and accuracy compared to traditional methods.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b34\": 0.8,\n    \"b18\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.75,\n    \"b51\": 0.7,\n    \"b52\": 0.7,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b59\": 0.6,\n    \"b60\": 0.6,\n    \"b61\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve the refinement and quality assessment of 3D protein complex structures, which is a difficult task due to the size of the search space and the physical constraints of atomic systems. The proposed algorithm, EGR (Equivariant Graph Refiner), is a novel E(3)-equivariant graph deep learning model designed to refine and assess protein complex structures. The method introduces an efficient single forward pass prediction for the refined structure and per-residue quality, leveraging equivariant graph neural networks and input regularization.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b18\": 0.85,\n    \"b34\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.7,\n    \"b40\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b13\": 0.5,\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper tackles the challenge of refining and assessing the quality of 3D structures of protein complexes using a novel E(3)-equivariant graph deep learning model, EGR. The model improves upon existing molecular dynamics and relaxation protocols by significantly speeding up inference time and refining the positions of all atoms.\",\n    \"Direct Inspiration\": {\n        \"b11\": 0.9,\n        \"b34\": 0.85,\n        \"b18\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b35\": 0.75,\n        \"b40\": 0.7,\n        \"b39\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.6,\n        \"b13\": 0.55\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in the refinement and quality assessment of 3D protein complex structures. It introduces the EGR model, an E(3)-equivariant graph deep learning model, which significantly speeds up inference while refining all atom positions in proteins. The model leverages equivariant graph neural networks (EGNNs) and regularizes input features based on geometric priors for 3D molecules.\",\n    \"Direct Inspiration\": {\n        \"b11\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.8,\n        \"b34\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b35\": 0.6,\n        \"b39\": 0.6\n    }\n}\n```"], "62466dd35aee126c0f8b79e7": ["```json\n{\n  \"Summary\": \"The paper investigates how Pre-trained Language Models (PLMs) capture factual knowledge using a two-fold analysis: dependence measure and effectiveness measure. The authors propose a causal-inspired method to quantify the word-level dependence in mask-filling tasks and evaluate the effectiveness of this dependence on factual knowledge capture. Key findings include that PLMs depend more on positional close and highly co-occurred associations than knowledge-dependent associations, and that knowledge-dependent associations are more effective for factual knowledge capture.\",\n  \"Direct Inspiration\": [\"b4\"],\n  \"Indirect Inspiration\": [\"b18\", \"b10\", \"b21\", \"b27\"],\n  \"Other Inspiration\": [\"b1\", \"b17\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding how Pre-trained Language Models (PLMs) capture factual knowledge. It proposes a two-fold analysis: measuring the dependence of PLMs on different word associations and evaluating the effectiveness of these dependencies in capturing factual knowledge. The authors develop a causal-inspired method to quantify word-level dependence and effectiveness. Key contributions include quantifying the word-level dependence in mask filling, comparing the effectiveness of different associations, and introducing causal theories into PLM analysis.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b4\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b21\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b6\": 0.6,\n    \"b3\": 0.6,\n    \"b16\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper are understanding how Pre-trained Language Models (PLMs) capture factual knowledge and determining the effectiveness of this capture. The algorithm proposed includes a causal-inspired method to quantify word-level dependence and a measure of effectiveness based on the correlation between dependence and factual knowledge capture performance.\",\n    \"Inspirations\": \"The paper is inspired by the need to quantify factual knowledge capture in PLMs and the methods to improve this capture. It draws on causal theories and structured causal models for its methodology.\"\n  },\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b18\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b21\": 0.8,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding how Pre-trained Language Models (PLMs) capture factual knowledge. It does so by analyzing the dependence of PLMs on different associations (Knowledge-Dependent, Positionally Close, Highly Co-occurred) in predicting missing words during mask-filling tasks. The key contributions include quantifying word-level dependence using a causal-inspired method and comparing the effectiveness of various associations in factual knowledge capture.\",\n  \"Direct Inspiration\": [\"b4\", \"b18\"],\n  \"Indirect Inspiration\": [\"b1\", \"b10\", \"b21\", \"b27\"],\n  \"Other Inspiration\": [\"b2\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary research questions: which associations do Pre-trained Language Models (PLMs) rely on to capture factual knowledge, and how effective are these associations in capturing factual knowledge. The novel contributions include quantifying word-level dependence using a causal-inspired method, comparing the effectiveness of different associations in factual knowledge capture, and introducing causal theories into PLM analysis.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b4\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b21\": 0.9,\n    \"b27\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.8,\n    \"b17\": 0.7,\n    \"b20\": 0.7\n  }\n}\n```"], "6296d90e5aee126c0f730b92": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of graph anomaly detection with Graph Neural Networks (GNNs), focusing on the 'right-shift' phenomenon in spectral energy distributions caused by anomalies. The authors propose the Beta Wavelet Graph Neural Network (BWGNN) to better tackle high-frequency anomalies via band-pass and spectral-localized filters.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b22\": 0.8,\n    \"b39\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b23\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of anomaly detection in graph data using Graph Neural Networks (GNNs) and proposes a novel approach called Beta Wavelet Graph Neural Network (BWGNN). It is inspired by the observation that anomalies cause a 'right-shift' in the spectral energy distribution of graph signals. The authors develop BWGNN based on Hammond's graph wavelet theory to better capture high-frequency anomalies.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b22\": 0.9,\n    \"b51\": 0.8,\n    \"b5\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.7,\n    \"b44\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of anomaly detection in graphs using Graph Neural Networks (GNNs). Traditional GNNs face issues like over-smoothing, which makes anomalies less distinguishable. The authors propose a novel approach called Beta Wavelet Graph Neural Network (BWGNN) that leverages spectral properties of graphs, specifically focusing on the 'right-shift' phenomenon in spectral energy distribution caused by anomalies.\",\n  \"Direct Inspiration\": [\"b20\"],\n  \"Indirect Inspiration\": [\"b1\", \"b22\"],\n  \"Other Inspiration\": [\"b2\", \"b26\", \"b29\", \"b50\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of anomaly detection in graph data using Graph Neural Networks (GNNs). Traditional GNNs struggle with anomaly detection due to the over-smoothing issue. The authors propose a novel approach by analyzing anomalies through the graph spectrum and introducing the Beta Wavelet Graph Neural Network (BWGNN) to address the 'right-shift' phenomenon of spectral energy distribution in anomalous graphs.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b22\": 0.7,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b29\": 0.6,\n    \"b50\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the issue of effectively detecting anomalies in graph data using Graph Neural Networks (GNNs), particularly addressing the over-smoothing issue that makes anomalies less distinguishable. The authors propose a novel approach to anomaly detection by focusing on the spectral domain and introducing the Beta Wavelet Graph Neural Network (BWGNN) to tackle the 'right-shift' phenomenon of graph anomalies effectively.\",\n    \"Direct Inspiration\": {\n        \"b20\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8,\n        \"b22\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b31\": 0.6,\n        \"b44\": 0.5,\n        \"b51\": 0.4\n    }\n}\n```"], "62997c0b5aee126c0f77cecc": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of hard negative sampling in contrastive learning, specifically focusing on unsupervised instance selection, avoiding false hardest negatives, and ensuring the representativeness of negative samples. The proposed method, UnReMix, incorporates model uncertainty, anchor similarity, and representativeness to select informative hard negative examples. The effectiveness of UnReMix is demonstrated through empirical evaluations across multiple domains including image, text, and graph data.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b18\": 0.8,\n    \"b42\": 0.85,\n    \"b36\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.6,\n    \"b46\": 0.6,\n    \"b22\": 0.65,\n    \"b3\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b8\": 0.5,\n    \"b56\": 0.4,\n    \"b23\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in hard negative sampling for contrastive learning, proposing UnReMix, a method that combines importance scores capturing model uncertainty, representativeness, and anchor similarity. The method aims to improve downstream performance by selecting informative hard negatives that are representative of the data population, confident under the model, and close to the anchor in feature space.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b3\": 0.9,\n    \"b42\": 0.95,\n    \"b36\": 0.9,\n    \"b22\": 0.85,\n    \"b31\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b8\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b34\": 0.7,\n    \"b37\": 0.7,\n    \"b57\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three primary challenges in hard negative example selection for contrastive learning: lack of label information, avoidance of false hardest negatives, and representativeness of the negative examples. The proposed method, UnReMix, combines importance scores incorporating model uncertainty, representativeness, and anchor similarity to improve the quality of negative example selection. The method demonstrates improved performance across various benchmarks in image, text, and graph datasets.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b42\": 0.9,\n    \"b36\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b22\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of selecting appropriate informative hard negative examples in contrastive learning. It introduces Uncertainty and Representativeness Mixing (UnReMix) to improve negative sampling by combining model uncertainty, representativeness, and anchor similarity. The proposed method aims to enhance downstream task performance and select high-quality negatives that are diverse and informative.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b18\": 0.9,\n    \"b42\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b31\": 0.7,\n    \"b22\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b23\": 0.6,\n    \"b46\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of selecting appropriate informative hard negative examples in contrastive learning to improve downstream performance. It introduces a novel method called UnReMix, which combines importance scores that capture model uncertainty, representativeness, and anchor similarity to select hard negatives. The method is verified on multiple benchmarks, showing its effectiveness in improving performance.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b42\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b18\": 0.7,\n    \"b36\": 0.7,\n    \"b22\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b46\": 0.6\n  }\n}\n```"], "622183525aee126c0f23c770": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of label noise in deep neural networks and proposes a novel contrastive regularization function to mitigate the memorization effect of noisy labels. The main contributions include theoretical analysis of the proposed method's benefits, an empirical demonstration of its effectiveness, and a novel algorithm for learning contrastive representations from noisy data.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b7\": 1,\n    \"b8\": 1,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b13\": 0.8,\n    \"b23\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.7,\n    \"b20\": 0.6,\n    \"b41\": 0.6,\n    \"b54\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of label noise in deep neural networks, proposing a novel contrastive regularization function to improve robustness and combat the memorization effect. It introduces new theoretical insights and empirically demonstrates the method's effectiveness on various datasets.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b8\": 0.95,\n    \"b19\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b26\": 0.8,\n    \"b31\": 0.7,\n    \"b54\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.65,\n    \"b13\": 0.65,\n    \"b23\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of label noise in deep neural networks by proposing a novel contrastive regularization function. This method aims to keep representations related to true labels and discard information related to corrupted labels, making the classifier robust to noisy labels. The paper highlights the inadequacy of existing noise-robust loss functions and introduces a contrastive learning approach to mitigate the memorization effect of noisy labels.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b7\": 1.0,\n    \"b8\": 1.0,\n    \"b15\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b13\": 0.8,\n    \"b23\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b31\": 0.7,\n    \"b26\": 0.6,\n    \"b28\": 0.6,\n    \"b30\": 0.6,\n    \"b54\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of label noise in deep neural networks, which leads to poor performance due to the memorization effect. The authors propose a novel contrastive regularization function that helps learn noise-robust representations by pulling together representations of data with the same true labels and pushing apart those with different labels. The main contributions include theoretical analysis of the proposed method, a novel algorithm for learning contrastive representations, and empirical validation on various datasets.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.9,\n        \"b8\": 0.9,\n        \"b15\": 0.8,\n        \"b19\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.7,\n        \"b13\": 0.7,\n        \"b23\": 0.7,\n        \"b24\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b25\": 0.6,\n        \"b26\": 0.6,\n        \"b31\": 0.6,\n        \"b52\": 0.6,\n        \"b54\": 0.7,\n        \"b32\": 0.6,\n        \"b28\": 0.6,\n        \"b11\": 0.5,\n        \"b30\": 0.5,\n        \"b41\": 0.5,\n        \"b18\": 0.5,\n        \"b36\": 0.5,\n        \"b3\": 0.5,\n        \"b4\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of label noise in training deep neural networks and proposes a novel contrastive regularization function to improve robustness to noisy labels. The approach leverages the strengths of contrastive learning to maintain information related to true labels while discarding corrupted label information.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b6\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b13\": 0.7,\n    \"b23\": 0.65,\n    \"b24\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b15\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```"], "628d9e795aee126c0f9791fd": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of over-parameterization in large-scale Graph Neural Networks (GNNs) by proposing a novel adversarial knowledge distillation framework named GraphAKD. The key innovation is the use of a trainable discriminator to distinguish between teacher and student GNNs, thereby improving the efficiency and performance of compact student models.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b45\": 0.85,\n    \"b46\": 0.85,\n    \"b36\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b52\": 0.7,\n    \"b47\": 0.65,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b53\": 0.5,\n    \"b55\": 0.5,\n    \"b59\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of compressing deep and over-parameterized Graph Neural Networks (GNNs) for large-scale graphs using a novel adversarial knowledge distillation framework named GraphAKD. This framework includes a trainable discriminator to distinguish between teacher and student networks from the views of node representations and logits, aiming to transfer both inter-node and inter-class correlations from a complicated teacher GNN to a compact student GNN.\",\n    \"Direct Inspiration\": {\n        \"b36\": 0.9,\n        \"b45\": 0.85,\n        \"b46\": 0.85,\n        \"b52\": 0.85,\n        \"b55\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.7,\n        \"b47\": 0.75,\n        \"b20\": 0.7,\n        \"b7\": 0.75,\n        \"b53\": 0.7,\n        \"b59\": 0.7,\n        \"b42\": 0.7,\n        \"b25\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b24\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of over-parameterized GNNs on large-scale graphs by proposing an adversarial knowledge distillation framework named GraphAKD. GraphAKD introduces a discriminator to distinguish between student and teacher GNNs from the views of node representations and logits, optimizing the student GNN to mimic the teacher GNN more effectively in a topology-aware manner.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.9,\n    \"b45\": 0.9,\n    \"b46\": 0.9,\n    \"b52\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b53\": 0.7,\n    \"b55\": 0.7,\n    \"b59\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b7\": 0.6,\n    \"b54\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of over-parameterization in Graph Neural Networks (GNNs) for large-scale graphs and proposes a novel adversarial knowledge distillation framework (GraphAKD) to compress deep GNNs while preserving their expressive power. The approach involves a trainable discriminator within a teacher-student architecture, using generative adversarial networks (GANs) to transfer knowledge effectively. The framework aims to distinguish between student and teacher outputs from node representations and logits, enabling the student model to mimic the teacher's performance while being computationally efficient.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.9,\n    \"b45\": 0.9,\n    \"b52\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.75,\n    \"b47\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b55\": 0.65,\n    \"b59\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of compressing deep Graph Neural Networks (GNNs) while maintaining their expressive power without relying on predefined distance functions. It proposes a novel adversarial knowledge distillation framework named GraphAKD, which uses a trainable discriminator to distinguish between student and teacher node representations and logits, making the teacher-student architecture more flexible and efficient.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.9,\n    \"b47\": 0.85,\n    \"b52\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b55\": 0.75,\n    \"b20\": 0.7,\n    \"b53\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.6,\n    \"b59\": 0.55\n  }\n}\n```"], "6287492a5aee126c0ffe8231": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in neural topic modeling, particularly the limitations of bag-of-words (BoWs) representations and the computational overhead of using pre-trained language models (PLMs). It proposes a new method called Pre-trained Neural Topic Model (PT-NTM) that pre-trains topic models on a large corpus and then fine-tunes them on specific datasets to achieve better performance with lower computational cost.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b31\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b14\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving topic modeling by incorporating pre-trained language models (PLMs). Traditional methods using bag-of-words (BoWs) representations fail to encode rich word semantics. The proposed solution, Pre-trained Neural Topic Model (PT-NTM), pre-trains the topic model on a large corpus and fine-tunes it on specific datasets, reducing computational overhead and training data requirements.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b30\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b2\": 0.75,\n    \"b23\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b34\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in neural topic modeling, particularly the computational overhead and the gap in training objectives between Pre-trained Language Models (PLMs) and topic models. The proposed solution, Pre-trained Neural Topic Model (PT-NTM), involves pre-training a neural topic model on a large corpus and then fine-tuning it on specific datasets to reduce computational complexity and improve performance.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.95,\n    \"b2\": 0.9,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b21\": 0.8,\n    \"b31\": 0.8,\n    \"b24\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b30\": 0.7,\n    \"b23\": 0.7,\n    \"b34\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in this paper are the limitations of BoWs representation in encoding rich word semantics and the computational overhead introduced by using deep PLMs in neural topic models. The proposed algorithm, Pre-trained Neural Topic Model (PT-NTM), addresses these challenges by pre-training the topic model on a large corpus and then fine-tuning it on a specific dataset.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b13\": 0.9,\n    \"b30\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b31\": 0.75,\n    \"b34\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.7,\n    \"b6\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of inferring high-quality topics from documents using neural topic models. It proposes a novel approach called Pre-trained Neural Topic Model (PT-NTM) that pre-trains the topic model on a large corpus and then fine-tunes it on specific datasets. This method aims to reduce computational complexity and improve performance over traditional methods that utilize BoWs or PLMs.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b2\": 0.90,\n    \"b13\": 0.92,\n    \"b30\": 0.88\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.70,\n    \"b21\": 0.72,\n    \"b31\": 0.75,\n    \"b34\": 0.74\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.65,\n    \"b7\": 0.60,\n    \"b8\": 0.62,\n    \"b9\": 0.58,\n    \"b24\": 0.66,\n    \"b26\": 0.64,\n    \"b33\": 0.68\n  }\n}\n```"], "62982a9a5aee126c0f6f5f99": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of recognizing objects in images with irregular shapes using a graph representation of the image. The proposed Vision Graph Neural Network (ViG) models process the image in a flexible and effective way by treating patches of the image as nodes and using a combination of Graph Convolutional Network (GCN) and Feed-Forward Network (FFN) modules. The ViG models outperform traditional CNN, MLP, and transformer models on visual tasks like image classification and object detection.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b15\": 1,\n    \"b27\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b46\": 0.8,\n    \"b47\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b26\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively processing images with irregularly shaped objects using a novel Vision Graph Neural Network (ViG) that treats image patches as nodes in a graph. The method aims to overcome the limitations of traditional grid or sequence-based representations used in CNNs and transformers.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b27\": 1.0,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b46\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b53\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge addressed in the paper is the inflexibility and redundancy of the commonly-used grid or sequence structures in previous networks like ResNet and ViT for processing irregular-shaped objects in images.\",\n    \"Inspirations\": \"The paper is inspired by the limitations of CNNs, transformers, and MLPs in handling non-quadrate objects and proposes the use of graph neural networks (GNNs) to represent images more flexibly and effectively.\"\n  },\n  \"Direct Inspiration\": {\n    \"References\": [\"b7\", \"b15\", \"b32\", \"b27\", \"b46\"]\n  },\n  \"Indirect Inspiration\": {\n    \"References\": [\"b3\", \"b53\"]\n  },\n  \"Other Inspiration\": {\n    \"References\": [\"b9\", \"b41\", \"b35\", \"b36\", \"b8\", \"b1\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents a novel vision graph neural network (ViG) for visual tasks, addressing the challenge of handling irregular object shapes in images more flexibly and effectively than traditional grid or sequence representations. The ViG model processes images as graphs where patches are nodes, and it includes GCN modules for graph processing and FFN modules for node feature transformation. The model demonstrates improved performance on tasks like image classification and object detection compared to CNNs, MLPs, and transformers.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b32\": 0.7,\n    \"b46\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b3\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively processing images with irregular object shapes by representing images as graphs instead of regular grids or sequences. The proposed Vision Graph Neural Network (ViG) utilizes graph convolutional networks (GCNs) to process these image graphs, achieving higher accuracy in visual tasks like image classification and object detection compared to traditional CNNs, MLPs, and transformers.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b15\": 0.9,\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b32\": 0.8,\n    \"b46\": 0.7,\n    \"b47\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b17\": 0.6,\n    \"b29\": 0.6,\n    \"b45\": 0.6,\n    \"b48\": 0.6,\n    \"b53\": 0.6\n  }\n}\n```"], "626f3dd05aee126c0f8f76a7": ["```json\n{\n    \"Summary\": \"The paper addresses three primary challenges in text-to-image generation using auto-regressive models: slow generation speed, expensive high-resolution training, and uni-directional generation. To overcome these challenges, the authors propose a new pretraining method called Cross-Modal general Language Model (CogLM) and develop a hierarchical generation process in CogView2. CogLM incorporates masked token prediction for bidirectional context handling, text-guided infilling, and image captioning. CogView2 leverages this versatility for efficient and high-quality image generation through a three-step hierarchical process.\",\n    \"Direct Inspiration\": [\"b2\", \"b23\"],\n    \"Indirect Inspiration\": [\"b6\", \"b11\", \"b29\"],\n    \"Other Inspiration\": [\"b9\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three primary challenges in text-to-image generation using large-scale pretrained transformers: slow generation, expensive high-resolution training, and the limitation of uni-directional token generation. The authors propose a novel algorithm, CogLM, which unifies auto-regressive generation and bidirectional context-aware mask prediction. This new approach improves the versatility and efficiency of text-to-image generation by leveraging a hierarchical model, CogView2, which includes three steps: generating low-resolution images, direct super-resolution, and iterative super-resolution.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b23\": 1,\n    \"b10\": 0.9,\n    \"b0\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b11\": 0.7,\n    \"b22\": 0.6,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5,\n    \"b9\": 0.5,\n    \"b26\": 0.4,\n    \"b3\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three main challenges in text-to-image generation: slow generation speed, high training costs for high-resolution images, and uni-directional token generation. The proposed solution, Cross-Modal General Language Model (CogLM), is versatile and capable of handling various tasks such as text-to-image generation, image captioning, and image infilling. The hierarchical design of CogView2 speeds up the generation process and improves image quality through direct and iterative super-resolution methods.\",\n  \"Direct Inspiration\": [\"b2\", \"b23\"],\n  \"Indirect Inspiration\": [\"b6\", \"b9\", \"b11\", \"b26\"],\n  \"Other Inspiration\": [\"b0\", \"b25\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the slow generation speed of auto-regressive models, the high expense of high-resolution training, and the uni-directional generation limitation. The authors propose the Cross-Modal general Language Model (CogLM) to address these challenges by enabling bidirectional masked prediction, hierarchical design for local coherence, and a local parallel auto-regressive generation method. This results in faster and more efficient high-resolution text-to-image generation.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b26\": 0.8,\n    \"b6\": 0.8,\n    \"b11\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three primary challenges in text-to-image generation: slow generation speed, expensive high-resolution training, and uni-directional token generation. To tackle these issues, the authors propose a novel algorithm called Cross-Modal general Language Model (CogLM) and its hierarchical extension, CogView2. CogLM combines auto-regressive generation with bidirectional context-aware mask prediction. CogView2 leverages a hierarchical generation process involving low-resolution image generation, direct super-resolution, and iterative super-resolution to improve both the speed and quality of image generation.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b5\": 0.9,\n    \"b11\": 0.85,\n    \"b19\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b26\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```"], "628c6264f66cd1000c54e4e9": ["```json\n{\n  \"Summary\": \"The paper introduces Imagen, a text-to-image diffusion model combining transformer language models with high-fidelity diffusion models to achieve unprecedented photorealism and language understanding in text-to-image synthesis. The main challenges addressed include capturing semantic complexity of text and improving image-text alignment and image fidelity. Key novel methods include the use of pretrained text encoders, classifier-free guidance, dynamic thresholding, and a robust cascaded diffusion pipeline.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b15\": 0.9,\n    \"b28\": 0.9,\n    \"b40\": 1.0,\n    \"b51\": 1.0,\n    \"b53\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b27\": 0.8,\n    \"b39\": 0.7,\n    \"b48\": 0.8,\n    \"b52\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b56\": 0.6,\n    \"b62\": 0.5,\n    \"b63\": 0.5,\n    \"b64\": 0.5,\n    \"b74\": 0.4,\n    \"b79\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces Imagen, a text-to-image diffusion model that leverages transformer language models (LMs) and high-fidelity diffusion models to achieve photorealistic image generation with deep language understanding. Key challenges addressed include the need for powerful semantic text encoders and effective text conditioning in diffusion models. Innovations include the use of pretrained LMs like T5-XXL, dynamic thresholding for better photorealism, and a robust cascaded diffusion model pipeline.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b15\": 0.9,\n    \"b27\": 0.8,\n    \"b51\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.85,\n    \"b40\": 0.8,\n    \"b48\": 0.75,\n    \"b53\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b52\": 0.6,\n    \"b56\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of enhancing text-to-image synthesis by integrating transformer language models with high-fidelity diffusion models. The novel contributions include using pretrained text embeddings from large language models, classifier-free guidance for effective text conditioning, dynamic thresholding for improved photorealism, and a robust cascaded diffusion model pipeline for generating high-resolution images.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b51\": 0.9,\n    \"b27\": 0.8,\n    \"b28\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.7,\n    \"b52\": 0.7,\n    \"b53\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b48\": 0.6,\n    \"b56\": 0.6,\n    \"b26\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the development of 'Imagen', a text-to-image diffusion model that leverages transformer language models (LMs) and high-fidelity diffusion models to achieve photorealistic image generation and deep language understanding. It introduces novel methods such as dynamic thresholding for sampling with large guidance weights, and robust cascaded diffusion models for high-resolution image generation.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b27\": 1.0,\n    \"b28\": 1.0,\n    \"b40\": 1.0,\n    \"b51\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.9,\n    \"b48\": 0.9,\n    \"b53\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b56\": 0.8,\n    \"b57\": 0.8,\n    \"b26\": 0.8,\n    \"b21\": 0.8,\n    \"b63\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces Imagen, a text-to-image diffusion model that combines transformer language models with high-fidelity diffusion models to achieve photorealism and deep language understanding. It addresses challenges in text-to-image synthesis by leveraging pretrained text embeddings from large language models and introduces novel techniques such as dynamic thresholding and robust cascaded diffusion models.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b40\": 0.85,\n    \"b53\": 0.85,\n    \"b51\": 0.9,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b52\": 0.75,\n    \"b27\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b56\": 0.7,\n    \"b48\": 0.75\n  }\n}\n```"], "629c4e2a5aee126c0f6f8d0b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of protein sequence design, proposing a deep learning-based method called ProteinMPNN that improves upon existing physically-based approaches like Rosetta. The primary goal is to design amino acid sequences that fold into desired structures, applicable to various protein design problems including monomers, cyclic oligomers, protein nanoparticles, and protein-protein interfaces.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b7\": 1,\n    \"b9\": 1,\n    \"b10\": 1,\n    \"b11\": 1,\n    \"b14\": 1,\n    \"b15\": 1,\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.9,\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.9,\n    \"b8\": 0.9,\n    \"b17\": 0.9,\n    \"b19\": 0.9,\n    \"b23\": 0.9,\n    \"b31\": 0.9,\n    \"b32\": 0.9,\n    \"b33\": 0.9,\n    \"b34\": 0.9,\n    \"b35\": 0.9,\n    \"b36\": 0.9,\n    \"b37\": 0.9,\n    \"b38\": 0.9,\n    \"b39\": 0.9,\n    \"b40\": 0.9,\n    \"b41\": 0.9,\n    \"b42\": 0.9,\n    \"b43\": 0.9,\n    \"b44\": 0.9,\n    \"b45\": 0.9,\n    \"b46\": 0.9,\n    \"b47\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b48\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of protein sequence design given a backbone structure, introducing a deep learning-based method, ProteinMPNN, to improve sequence recovery and design performance across various protein design problems including monomers, oligomers, nanoparticles, and protein-protein interfaces. The approach builds upon and improves a previously described message passing neural network (MPNN).\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b7\": 1,\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the protein sequence design problem using a deep learning approach called ProteinMPNN. The main challenges include improving sequence recovery, making the model applicable to various design problems, and ensuring robust performance under noisy conditions. The authors draw inspiration from and build on previous machine learning methods and physically based approaches like Rosetta.\",\n    \"Direct Inspiration\": [\"b0\", \"b7\", \"b8\"],\n    \"Indirect Inspiration\": [\"b6\", \"b9\", \"b10\", \"b11\"],\n    \"Other Inspiration\": [\"b14\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in protein sequence design, specifically for designing sequences that fold to a given protein backbone structure. The authors propose a deep learning-based method called ProteinMPNN, which aims to improve sequence recovery and applicability across various design problems, including monomers, cyclic oligomers, protein nanoparticles, and protein-protein interfaces. Key improvements include the addition of interatomic distances, edge updates in the neural network, and a flexible decoding order. The method is experimentally validated to show higher sequence recovery and success rates compared to traditional methods like Rosetta.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.75,\n    \"b8\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b10\": 0.65,\n    \"b11\": 0.65,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the protein sequence design problem, proposing a novel deep learning-based method, ProteinMPNN. The main challenges include improving sequence recovery for various protein structures and enabling broader applicability to different protein design problems.\",\n  \"Direct Inspiration\": [\"b0\", \"b7\", \"b9\"],\n  \"Indirect Inspiration\": [\"b1\", \"b2\", \"b3\", \"b4\", \"b5\"],\n  \"Other Inspiration\": [\"b6\", \"b8\", \"b10\", \"b14\", \"b15\", \"b16\", \"b17\", \"b19\"]\n}\n```"], "62aa9fb55aee126c0fa5cbb7": ["```json\n{\n  \"Summary\": \"The paper introduces a Hybrid Graph Network Simulator (HGNS) for subsurface simulation, addressing two primary challenges: multi-scale dynamics and large grid sizes. The HGNS incorporates a Subsurface Graph Neural Network (SGNN) to model fluid flow and a 3D-U-Net to model pressure dynamics, utilizing sector-based training to handle large grid sizes and multi-step rollout for long-term prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.95,\n    \"b6\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.80,\n    \"b11\": 0.75,\n    \"b3\": 0.70,\n    \"b7\": 0.70,\n    \"b25\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.65,\n    \"b15\": 0.60,\n    \"b20\": 0.60,\n    \"b16\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of modeling subsurface flow for large-scale applications, specifically focusing on multi-scale dynamics and scaling to extremely large grids. The proposed HGNS (Hybrid Graph Network Simulator) combines a Subsurface Graph Neural Network (SGNN) to model fluid flow and a 3D-U-Net to model pressure dynamics. The model is designed to handle grids with millions of cells and significantly outperforms existing methods in accuracy and computational efficiency.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b25\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b7\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a Hybrid Graph Network Simulator (HGNS) to address challenges in subsurface simulation, particularly the multi-scale problem of fluid flow through porous media and scaling to large grids. The HGNS combines a Subsurface Graph Neural Network (SGNN) for local fluid dynamics and a 3D-U-Net for global pressure dynamics, overcoming limitations of previous models.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b7\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Modeling multi-scale subsurface fluid flow dynamics\",\n      \"Scaling simulations to extremely large grids\"\n    ],\n    \"inspirations\": [\n      \"Data-driven surrogate models\",\n      \"Graph Neural Networks\",\n      \"3D-U-Net\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b6\",\n    \"b18\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b7\",\n    \"b19\"\n  ],\n  \"Other Inspiration\": [\n    \"b3\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are modeling the multi-scale dynamics of subsurface fluid flow and scaling to extremely large grids. The proposed Hybrid Graph Network Simulator (HGNS) addresses these challenges by combining a Subsurface Graph Neural Network (SGNN) for fine-scale fluid dynamics and a 3D-U-Net for global pressure dynamics. Key advances include sector-based training for handling large grids and multi-step rollout for accurate long-term predictions.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b18\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.75,\n    \"b20\": 0.7,\n    \"b25\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b7\": 0.6,\n    \"b19\": 0.55\n  }\n}\n```"], "6260bd7f5aee126c0fc6bbc3": ["```json\n{\n  \"Summary\": \"The main challenges outlined in the paper are grasping abstract concepts and their relationships in morals, capturing inter-event discourse relations in stories, and aligning value preferences between stories and morals. The authors propose tasks for evaluating these abilities and introduce a new dataset called STORAL. They present a retrieval-augmented algorithm to improve model performance by retrieving related concepts or events as additional guidance.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b40\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b19\": 0.5,\n    \"b43\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling machines to grasp abstract concepts and relate them to concrete events in stories for moral understanding and generation. It proposes four tasks (two understanding and two generation tasks) and introduces a new dataset called STORAL to facilitate these tasks. The paper also presents a retrieval-augmented algorithm to enhance model performance.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b7\": 1.0,\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.8,\n    \"b13\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling machines to understand and generate moral stories, focusing on the ability to grasp abstract moral concepts, capture inter-event discourse relations, and align value preferences. It proposes four tasks (two understanding and two generation tasks) and introduces a new dataset named STORAL for these tasks. The paper also presents a retrieval-augmented algorithm to improve model performance by retrieving related concepts or events from training sets.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b33\": 1,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b41\": 0.7,\n    \"b42\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on the challenge of enabling machines to grasp abstract morals from stories and generate coherent stories that convey specific morals. The key challenges include understanding abstract concepts, capturing inter-event discourse relations, and aligning value preferences. The authors propose a series of tasks (MOCPT, MOPREF, ST2MO, MO2ST) and introduce a new dataset named STORAL. They also present a retrieval-augmented algorithm to improve model performance.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b33\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.75,\n    \"b40\": 0.7,\n    \"b13\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.6,\n    \"b25\": 0.55,\n    \"b41\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling machines to grasp abstract morals from stories, which involves understanding abstract concepts, capturing inter-event discourse relations, and aligning value preferences. The authors propose four tasks (two understanding and two generation) to evaluate these abilities and introduce a new dataset named STORAL to facilitate this research. They also present a retrieval-augmented algorithm to improve model performance.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b7\": 0.85,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b45\": 0.7,\n    \"b19\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b6\": 0.55\n  }\n}\n```"], "628304515aee126c0f6f0fed": ["```json\n{\n  \"Summary\": \"The paper introduces Directed Acyclic Transformer (DA-Transformer) to address the multimodality problem in Non-Autoregressive Transformers (NATs) for machine translation. The key contribution is the Directed Acyclic Decoder, which organizes hidden states as a Directed Acyclic Graph (DAG) to capture multiple translation modalities directly, avoiding the reliance on knowledge distillation (KD). The proposed method aims to improve translation quality and inference efficiency by generating sentences along predicted paths of the DAG.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b13\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b31\": 0.7,\n    \"b38\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b50\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies in inference of the Non-autoregressive Transformers (NATs) due to the multimodality problem and reliance on knowledge distillation (KD). The proposed Directed Acyclic Transformer (DA-Transformer) introduces a Directed Acyclic Decoder to capture multiple translation modalities directly, thereby avoiding the issues associated with KD and improving translation quality while maintaining low latency.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b13\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.7,\n    \"b17\": 0.7,\n    \"b30\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b37\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the inefficiency and multimodality problem in Non-Autoregressive Transformers (NATs) for machine translation. The proposed solution, Directed Acyclic Transformer (DA-Transformer), utilizes a Directed Acyclic Graph (DAG) structure to capture multiple translation modalities directly, avoiding the use of Knowledge Distillation (KD) and improving translation quality and inference latency.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b13\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.75,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b31\": 0.6,\n    \"b50\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of inefficiency and multimodality in Non-Autoregressive Transformers (NATs) for machine translation. It proposes the Directed Acyclic Transformer (DA-Transformer) which captures multiple translation modalities through a Directed Acyclic Graph (DAG) structure, avoiding the need for knowledge distillation and improving translation quality and diversity while maintaining high inference speed.\",\n  \"Direct Inspiration\": [\"b14\", \"b13\"],\n  \"Indirect Inspiration\": [\"b3\", \"b26\", \"b9\", \"b37\"],\n  \"Other Inspiration\": [\"b10\", \"b45\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiency of Non-Autoregressive Transformers (NATs) in machine translation due to the multimodality problem. The proposed Directed Acyclic Transformer (DA-Transformer) aims to solve this by using a Directed Acyclic Graph (DAG) to capture multiple translation modalities simultaneously, avoiding the reliance on knowledge distillation (KD). The DA-Transformer generates sentences along predicted paths in parallel, which preserves translation quality while significantly reducing inference latency.\",\n  \"Direct Inspiration\": [\"b14\", \"b13\"],\n  \"Indirect Inspiration\": [\"b26\", \"b38\", \"b0\"],\n  \"Other Inspiration\": [\"b15\", \"b31\", \"b3\"]\n}\n```"], "62c28ae55aee126c0f8a1954": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of link prediction in graph-structured data by leveraging causal inference through the introduction of counterfactual links. The proposed method, CFLP, aims to improve link prediction accuracy by training GNN-based link predictors on both factual and counterfactual data to learn the causal relationships between graph structure and link existence.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b42\": 0.85,\n    \"b81\": 0.8,\n    \"b90\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b27\": 0.65,\n    \"b52\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.55,\n    \"b69\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge of the paper is to improve link prediction in graph-structured data by addressing the causal relationship between graph structure and link existence. The proposed method, CFLP (Counterfactual Link Prediction), aims to generate counterfactual links to answer counterfactual questions about link existence and uses these links as augmented data for graph representation learning.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b42\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.75,\n    \"b71\": 0.7,\n    \"b90\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b81\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the inability of existing graph machine learning methods to capture essential factors for accurate link prediction due to the lack of causal relationship consideration between graph structure and link existence. The proposed CFLP method introduces the concept of 'counterfactual links' to improve link prediction by leveraging causal inference. It involves generating counterfactual links as augmented data for graph representation learning, thereby enhancing the learning of node representations and link prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b42\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b27\": 0.9,\n    \"b71\": 0.7,\n    \"b81\": 0.8,\n    \"b90\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b3\": 0.5,\n    \"b8\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in this paper is the lack of causal inference in existing link prediction methods. The proposed method, CFLP, introduces the concept of 'counterfactual links' to improve link prediction by answering counterfactual questions about link existence. The method uses GNN-based link predictors and leverages the causal relationship between global graph structure and link existence. The novel approach enriches the training data with counterfactual links and enhances the accuracy of link prediction.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b90\": 0.9,\n    \"b42\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.7,\n    \"b81\": 0.7,\n    \"b18\": 0.6,\n    \"b71\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.4,\n    \"b53\": 0.4,\n    \"b52\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving link prediction in graph-structured data by leveraging causal inference. The central contribution is the introduction of the CFLP method, which generates counterfactual links to better understand and predict link existence by asking counterfactual questions. This method is novel in that it incorporates causal relationships into graph learning models, specifically using GNN-based predictors to estimate the effect of treatment variables on link existence and learning from both factual and counterfactual outcomes to enhance prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b42\": 0.9,\n    \"b90\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.7,\n    \"b81\": 0.6,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b71\": 0.5\n  }\n}\n```"], "620f0e735aee126c0fec46ff": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The paper highlights challenges in graph analysis using Graph Neural Networks (GNNs), specifically the over-smoothing and over-squashing problems associated with the Message Passing paradigm. The paper also addresses the need for systematic evaluations of Transformer variants in graph-structured data.\",\n    \"inspirations\": \"The paper is inspired by the rich expressive power of deep learning models and the recent successful applications of Transformers in various domains including NLP, CV, and other graph-related tasks.\"\n  },\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b24\": 0.9,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b23\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b13\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the integration of graph structures into Transformer models, categorizing the methods into three groups: GNNs as auxiliary modules, improved positional embeddings from graphs, and improved attention matrices from graphs. The paper also conducts comprehensive evaluations to determine the effectiveness of these methods on various graph tasks, highlighting the performance gains and limitations.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b22\": 1,\n    \"b24\": 1,\n    \"b6\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b21\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b14\": 0.7,\n    \"Dwivedi and Bresson, 2020\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the integration of Transformer models with graph-structured data. It categorizes existing solutions into three primary approaches: using Graph Neural Networks (GNNs) as auxiliary modules, improving positional embeddings from graphs, and enhancing attention matrices with graph information. The paper conducts extensive experiments to evaluate the effectiveness of these methods on various graph-based tasks.\",\n  \"Direct Inspiration\": [\"b4\", \"b7\", \"b19\"],\n  \"Indirect Inspiration\": [\"b6\", \"b9\", \"b22\", \"b24\"],\n  \"Other Inspiration\": [\"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies the primary challenges in utilizing Graph Neural Networks (GNNs) for graph analysis, specifically the over-smoothing and over-squashing problems. It proposes incorporating Transformer models to address these issues and enhance performance on graph-based tasks. The paper systematically reviews and categorizes different methods of integrating Transformers with graph data, including using GNNs as auxiliary modules, improved positional embeddings from graphs, and improved attention matrices from graphs.\",\n  \"Direct Inspiration\": {\n    \"GraphTrans\": \"b6\",\n    \"Grover\": \"b14\",\n    \"GraphiT\": \"b9\",\n    \"Graphormer\": \"b22\"\n  },\n  \"Indirect Inspiration\": {\n    \"Mesh Graphormer\": \"b23\",\n    \"Dwivedi and Bresson, 2020\": \"b24\",\n    \"Lin et al., 2021\": \"b21\"\n  },\n  \"Other Inspiration\": {\n    \"Xu et al., 2018\": \"b7\",\n    \"Ying et al., 2021\": \"b10\"\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the incorporation of Transformers in graph-structured data analysis, addressing challenges such as over-smoothing and over-squashing in Graph Neural Networks (GNNs). It categorizes existing methods into GNNs as auxiliary modules, improved positional embeddings from graphs, and improved attention matrices from graphs. The study conducts comprehensive ablation studies to evaluate the effectiveness of these methods on various graph tasks.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b6\": 1,\n    \"b24\": 0.95,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.85,\n    \"b19\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.65,\n    \"b9\": 0.6\n  }\n}\n```"], "628603bc970707000cbf14a8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of extending Masked Autoencoders (MAE) to spatiotemporal data for self-supervised learning on videos. The proposed method involves randomly masking spacetime patches in videos and reconstructing them using a simple autoencoder framework with minimal domain knowledge. The approach achieves strong empirical results and significant computation speedup, demonstrating the potential of learning useful representations from data with fewer inductive biases.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1,\n    \"b17\": 0.9,\n    \"b14\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b67\": 0.75,\n    \"b8\": 0.7,\n    \"b66\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b76\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extending Masked Autoencoders (MAE) to spatiotemporal representation learning for videos. The key inspiration is to use minimal domain knowledge, relying on a simple method of randomly masking spacetime patches in videos and reconstructing them. The proposed method uses Vision Transformers as both encoder and decoder, achieving significant computational efficiency and strong empirical results.\",\n  \"Direct Inspiration\": [\"b30\", \"b14\", \"b17\"],\n  \"Indirect Inspiration\": [\"b66\", \"b67\"],\n  \"Other Inspiration\": [\"b8\", \"b2\", \"b76\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the challenge of extending the Masked Autoencoders (MAE) method to spatiotemporal data for video representation learning. The authors propose a method that randomly masks spacetime patches in videos and reconstructs them using a vanilla Vision Transformer (ViT) architecture. The core contribution is the adaptation of the MAE approach to video data with minimal domain-specific inductive biases, resulting in a highly efficient and scalable solution for video research.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1.0,\n    \"b17\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b66\": 0.7,\n    \"b67\": 0.6,\n    \"b76\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b2\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of spatiotemporal representation learning in videos using a minimal domain knowledge approach, inspired by the success of Masked Autoencoders (MAE) in image and language representation learning. The authors propose a method that masks spacetime patches in videos and learns an autoencoder to reconstruct them. The method uses vanilla Vision Transformers with a high masking ratio, leading to significant computation reduction and strong empirical results.\",\n  \"Direct Inspiration\": [\"b30\", \"b14\", \"b17\"],\n  \"Indirect Inspiration\": [\"b66\", \"b8\", \"b67\"],\n  \"Other Inspiration\": [\"b76\", \"b34\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extending Masked Autoencoders (MAE) to spatiotemporal representation learning for videos. The key inspiration comes from the philosophy of minimizing domain-specific inductive biases and learning useful knowledge almost purely from data. The method proposed involves randomly masking out spacetime patches in videos and reconstructing them using an autoencoder, leveraging Vision Transformers (ViTs) with minimal modifications.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1.0,\n    \"b17\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b66\": 0.8,\n    \"b67\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.5\n  }\n}\n```"], "628ef0495aee126c0f82d966": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing a general, powerful, and scalable (GPS) architecture for Graph Transformers (GTs) that incorporates positional and structural encodings with local message passing and global attention. The proposed framework aims to solve issues related to node identifiability, computational costs, and the integration of efficient attention mechanisms, such as Performer and BigBird, into the graph domain.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b33\": 0.9,\n    \"b58\": 0.9,\n    \"b61\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b12\": 0.7,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b7\": 0.6,\n    \"b14\": 0.6,\n    \"b35\": 0.6,\n    \"b60\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are over-smoothing, over-squashing, and expressiveness bounds in Graph Transformers (GTs). The proposed solution is a scalable, powerful GT architecture (GPS) that incorporates positional encodings (PE) and structural encodings (SE) with local message passing and global attention. The approach leverages efficient linear attention mechanisms such as Performer and BigBird to handle large graphs, eliminating the need for explicit edge features.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b33\": 1.0,\n    \"b58\": 1.0,\n    \"b61\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b40\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b14\": 0.6,\n    \"b35\": 0.6,\n    \"b60\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of traditional Graph Transformers (GTs) such as over-smoothing, over-squashing, and computational inefficiency. It proposes a General, Powerful, Scalable (GPS) architecture for GTs, which integrates positional and structural encodings with local message passing and global attention. The GPS architecture aims to balance between message-passing graph neural networks (MPNNs) and Transformer-like global attention, including linear attention mechanisms like Performer and BigBird.\",\n  \"Direct Inspiration\": [\n    \"b10\",\n    \"b61\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b33\",\n    \"b58\",\n    \"b40\",\n    \"b9\"\n  ],\n  \"Other Inspiration\": [\n    \"b12\",\n    \"b60\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the fundamental limitations of graph transformers (GTs) such as over-smoothing, over-squashing, and expressiveness bounds by allowing nodes to attend to all other nodes in a graph using global attention. The authors propose a novel GPS (general, powerful, scalable) graph transformer architecture that combines positional encodings (PE) and structural encodings (SE) with local message passing and global attention. The key contributions include a blueprint for GPS GT with linear global attention, a better definition and organization of PEs and SEs, evaluation of GPS with extensive ablation studies, and the implementation within the GRAPHGPS package.\",\n    \"Direct Inspiration\": {\n        \"b10\": 0.9,\n        \"b33\": 0.8,\n        \"b58\": 0.8,\n        \"b61\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.7,\n        \"b40\": 0.7,\n        \"b50\": 0.6,\n        \"b60\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.5,\n        \"b7\": 0.5,\n        \"b14\": 0.5,\n        \"b35\": 0.5,\n        \"b9\": 0.4\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of over-smoothing, over-squashing, and expressiveness bounds in Graph Transformers (GTs) by proposing a general, powerful, and scalable (GPS) architecture. It introduces modules for embedding positional and structural encodings (PE and SE) and processing modules that combine local message passing with global attention. The paper highlights the efficiency of linear attention mechanisms like Performer and BigBird, and provides an extensive ablation study on the contributions of different components.\",\n    \"Direct Inspiration\": {\n        \"b10\": 0.95,\n        \"b61\": 0.95\n    },\n    \"Indirect Inspiration\": {\n        \"b33\": 0.75,\n        \"b58\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.6,\n        \"b12\": 0.6,\n        \"b40\": 0.6\n    }\n}\n```"], "628d9e795aee126c0f979247": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of building effective recommendation systems for e-commerce on Pinterest by leveraging multi-modal and multi-task learning. The proposed solution, ItemSage, creates product embeddings that use both image and text features to provide relevant shopping recommendations, optimizing for various engagement types like clicks, saves, add-to-cart actions, and purchases.\",\n  \"Direct Inspiration\": {\n    \"b32\": 0.95,\n    \"b0\": 0.9,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b18\": 0.75,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper outlines the development of Pinterest's learned embedding representation for products, ItemSage, designed to enhance shopping recommendations by leveraging multi-modal (image and text) data and optimizing for various engagement outcomes like purchases and add-to-cart actions. The approach includes multi-modal representation learning, multi-task learning, and compatibility with existing Pinterest embeddings like PinSage and SearchSage.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1.0,\n    \"b0\": 0.9,\n    \"b33\": 0.85,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.75,\n    \"b21\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.65,\n    \"b35\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces Pinterest's learned embedding representation for products, called ItemSage, designed to enhance the shopping recommendation experience on the platform. ItemSage leverages multi-modal features (images and text) and multi-task learning to optimize for various engagement outcomes, such as purchases and add-to-cart actions. The embeddings are designed to be compatible with Pinterest's existing PinSage and SearchSage models to support effective candidate generation and retrieval tasks.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1.0,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b20\": 0.7,\n    \"b27\": 0.7,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper outlines the challenges of building effective shopping recommendation systems on Pinterest, focusing on multi-modal information integration and optimizing for various user engagement metrics. The proposed solution, ItemSage, introduces a transformer-based architecture to combine text and image features, and leverages multi-task learning to enhance the recommendation pipeline.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1,\n    \"b33\": 0.9,\n    \"b20\": 0.8,\n    \"b18\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.6,\n    \"b21\": 0.6,\n    \"b6\": 0.5,\n    \"b27\": 0.5,\n    \"b35\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.4,\n    \"b7\": 0.4,\n    \"b28\": 0.4,\n    \"b4\": 0.4,\n    \"b1\": 0.3,\n    \"b14\": 0.3,\n    \"b15\": 0.3,\n    \"b22\": 0.3,\n    \"b26\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a recommendation system for Pinterest that leverages multi-modal information (images and text) to optimize for various engagement outcomes such as purchases, add-to-cart actions, clicks, and saves. The proposed solution, named ItemSage, introduces a transformer-based architecture for embedding representations that are compatible with existing models like PinSage and SearchSage, enabling efficient and versatile recommendations across different Pinterest surfaces.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.9,\n    \"b34\": 0.9,\n    \"b27\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b26\": 0.7\n  }\n}\n```"], "62bd48b80cd9e8000cfc9dc5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing an efficient search space for Transformer-like architectures by proposing a micro-meso-macro search space. This new search space includes rich atomic operations at the micro level, a novel Hamburger structure at the meso level, and multi-stage architecture adjustments at the macro level. The authors leverage One-Shot NAS and a hybrid sampling method for effective supernet training.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b54\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b28\": 0.6,\n    \"b29\": 0.7,\n    \"b46\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.5,\n    \"b44\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on designing a search space at the micro-meso-macro level to search for more efficient Transformer-like architectures. It introduces the Norm-Op-Norm-Act and Hamburger structures to offer more diverse operations and block designs. The paper utilizes One-Shot NAS for searching and proposes a hybrid sampling method to effectively train the supernet, demonstrating its effectiveness on the ImageNet and COCO datasets.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b54\": 0.8,\n    \"b29\": 0.7,\n    \"b44\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.6,\n    \"b46\": 0.6,\n    \"b26\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b43\": 0.6,\n    \"b53\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Designing an efficient search space for Transformer-like architectures at multiple levels (micro, meso, macro).\",\n    \"contributions\": [\n      \"Introduction of a micro-meso-macro search space.\",\n      \"Norm-Op-Norm-Act and Hamburger structure for diverse operations and block design.\",\n      \"One-Shot NAS for efficient searching with a hybrid sampling method.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b54\": 0.85,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.75,\n    \"b29\": 0.7,\n    \"b44\": 0.7,\n    \"b46\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b43\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper aims to design a micro-meso-macro search space to search for more efficient Transformer-like architectures, introducing Norm-Op-Norm-Act and Hamburger structures. It utilizes One-Shot NAS with a hybrid sampling method to train the supernet. The primary challenges addressed include the design of search space for high-performance architectures, the diversity of block structures, and comprehensive consideration of different granularities.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b12\": 1,\n    \"b44\": 1,\n    \"b54\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b29\": 0.8,\n    \"b46\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.7,\n    \"b26\": 0.7,\n    \"b15\": 0.6,\n    \"b53\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on designing a comprehensive micro-meso-macro search space to develop more efficient Transformer-like architectures. It introduces a Norm-Op-Norm-Act and Hamburger structure to offer diverse operations and block designs, and utilizes One-Shot NAS with a hybrid sampling method to train the supernet, aiming to improve performance on tasks such as recognition, detection, and segmentation.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b12\": 0.8,\n    \"b28\": 0.7,\n    \"b29\": 0.8,\n    \"b54\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.6,\n    \"b26\": 0.6,\n    \"b40\": 0.5,\n    \"b46\": 0.6,\n    \"b47\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.4,\n    \"b19\": 0.4,\n    \"b23\": 0.4,\n    \"b43\": 0.4,\n    \"b48\": 0.4\n  }\n}\n```"], "62bd48b2cb97d2000c50c6a6": ["```json\n{\n  \"Summary\": \"The paper addresses the efficiency and performance issues in Neural Architecture Search (NAS), particularly focusing on leveraging attention mechanisms for guiding both micro and macro searches within a unified framework. The proposed method aims to enhance the search process by emphasizing the importance of candidate operations and choice blocks using attention weights, which leads to more reliable and efficient architecture discovery.\",\n  \"Direct Inspiration\": [\"b28\", \"b37\"],\n  \"Indirect Inspiration\": [\"b32\", \"b42\", \"b10\", \"b22\", \"b6\", \"b49\"],\n  \"Other Inspiration\": [\"b13\", \"b9\", \"b48\", \"b29\", \"b39\", \"b47\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of Neural Architecture Search (NAS) by proposing a novel paradigm that leverages the attention mechanism to search for both micro and macro neural architectures within one framework. This method aims to improve efficiency and performance over previous NAS methods that suffer from high computational costs and issues in bi-level optimization.\",\n  \"Direct Inspiration\": [\"b28\", \"b6\", \"b37\"],\n  \"Indirect Inspiration\": [\"b32\", \"b13\", \"b54\"],\n  \"Other Inspiration\": [\"b42\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of the computational cost of Neural Architecture Search (NAS) and proposes a novel paradigm leveraging the attention mechanism to efficiently search for both micro and macro neural architectures in one framework. The proposed method aims to optimize the search process by using attention weights to indicate the importance of candidate operations and blocks, thus improving the selection process and overall network performance.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b37\": 0.9,\n    \"b43\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b29\": 0.6,\n    \"b39\": 0.5,\n    \"b47\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.4,\n    \"b13\": 0.4,\n    \"b9\": 0.4,\n    \"b48\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Neural Architecture Search (NAS) by proposing a novel paradigm that leverages the attention mechanism to efficiently search both micro and macro neural architectures within a single framework. The primary challenges include the high computational cost and instability associated with existing methods like DARTS. The proposed method uses accumulated channel attention weights to indicate the importance of candidate operations, thus optimizing the architecture search process and improving performance.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b29\": 1,\n    \"b39\": 1,\n    \"b47\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b42\": 0.8,\n    \"b10\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b37\": 0.6,\n    \"b43\": 0.6,\n    \"b9\": 0.6,\n    \"b48\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Neural Architecture Search (NAS) by proposing a novel paradigm that leverages the attention mechanism to efficiently search for both micro and macro neural architectures within a unified framework. The approach aims to mitigate issues with performance collapse and the computational burden traditionally associated with NAS.\",\n  \"Direct Inspiration\": [\"b28\", \"b42\", \"b10\", \"b6\", \"b37\"],\n  \"Indirect Inspiration\": [\"b13\", \"b9\", \"b48\", \"b32\"],\n  \"Other Inspiration\": [\"b30\", \"b29\", \"b39\", \"b47\"]\n}\n```"], "6253cbd75aee126c0f191b95": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge is designing novel amino acid sequences for proteins with desired properties using de novo protein design. The paper addresses the limitation of experimental data in deep learning approaches by utilizing predicted structures from AlphaFold2.\",\n    \"Inspirations\": \"The paper is inspired by recent advancements in deep learning for protein design, particularly generative models, and the success of AlphaFold2 in predicting protein structures.\"\n  },\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b54\": 0.7,\n    \"b67\": 0.6,\n    \"b50\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.4,\n    \"b22\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of de novo protein design, specifically focusing on predicting amino acid sequences that fold into desired protein structures. The authors propose leveraging predicted protein structures from AlphaFold2 to augment training data for an autoregressive inverse folding model. They explore different model architectures, including GVP-GNN and GVP-Transformer, and implement techniques such as span masking to improve model performance on partially masked backbones. The approach aims to overcome limitations in experimental data by utilizing large-scale predicted structures.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b30\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.75,\n    \"b54\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b67\": 0.55,\n    \"b50\": 0.5,\n    \"b53\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper tackles the challenge of de novo protein design using an autoregressive inverse folding model trained with predicted structures from AlphaFold2. The primary goals are to evaluate if predicted structures can enhance protein design models and to compare different model architectures, including GVP-GNN and GVP-Transformer.\",\n    \"Direct Inspiration\": {\n        \"b28\": 1,\n        \"b24\": 0.9,\n        \"b30\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b54\": 0.8,\n        \"b15\": 0.8,\n        \"b67\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b50\": 0.7,\n        \"b31\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses de novo protein design, focusing on predicting amino acid sequences from backbone structures using deep learning models. The main challenge is the limited number of experimentally determined protein structures, which the authors aim to overcome by using predicted structures from AlphaFold2. The novel contribution includes training an autoregressive inverse folding model with a large dataset of predicted structures and comparing different model architectures for sequence recovery and generalization capabilities.\",\n    \"Direct Inspiration\": {\n        \"b28\": 1.0,\n        \"b30\": 1.0,\n        \"b24\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b50\": 0.8,\n        \"b54\": 0.7,\n        \"b15\": 0.7,\n        \"b31\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b44\": 0.5,\n        \"b53\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Designing novel amino acid sequences that encode proteins with desired properties is a central challenge in bioengineering. Existing deep learning approaches are limited by the relatively small number of experimentally determined protein structures.\",\n    \"inspirations\": \"The paper explores whether predicted structures can be used to overcome the limitation of experimental data and trains an autoregressive inverse folding model to perform fixed-backbone protein sequence design using predicted structures.\"\n  },\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b24\": 0.9,\n    \"b30\": 0.9,\n    \"b54\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b21\": 0.7,\n    \"b53\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b62\": 0.6,\n    \"b39\": 0.6,\n    \"b10\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```"], "620c6b655aee126c0fe29013": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the difficulty of applying Mixup, a data augmentation technique, to graph data due to its irregular, unaligned, and non-Euclidean characteristics. The proposed solution, G-Mixup, involves mixing graphons, which are graph generators, to create synthetic graphs that preserve the key characteristics of the original graphs. The method theoretically and experimentally demonstrates improved generalization and robustness in graph neural networks.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b32\": 1,\n    \"b44\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.8,\n    \"b50\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b39\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the difficulty of applying Mixup to graph data due to the irregular, unaligned, and non-Euclidean nature of graphs. The paper proposes G-Mixup, a class-level graph data augmentation method, to mix up graph data based on graphons. The main contributions include proposing G-Mixup to augment training graphs, theoretically proving the synthetic graph's mixture of original graphs, and demonstrating the effectiveness of G-Mixup on various graph neural networks and datasets.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1.0,\n    \"b44\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b35\": 0.8,\n    \"b38\": 0.8,\n    \"b42\": 0.8,\n    \"b47\": 0.8,\n    \"b50\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.9\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the generalization and robustness of Graph Neural Networks (GNNs) through a novel graph data augmentation method called G-Mixup. The method leverages graphons to mix up graph data at the class level, generating synthetic graphs that retain key characteristics of the original graphs. G-Mixup addresses the irregularity, unalignment, and non-Euclidean nature of graph data, which makes traditional Mixup strategies inapplicable.\",\n  \"Direct Inspiration\": {\n    \"b44\": 0.95,\n    \"b32\": 0.90,\n    \"b26\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.80,\n    \"b50\": 0.80,\n    \"b47\": 0.80,\n    \"b40\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.75,\n    \"b35\": 0.75,\n    \"b45\": 0.70,\n    \"b12\": 0.70,\n    \"b11\": 0.70,\n    \"b22\": 0.65,\n    \"b39\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the difficulty of mixing up graph data due to its irregular, unaligned, and non-Euclidean nature. The proposed G-Mixup algorithm tackles these challenges by using graphons to mix up graph data and generate synthetic graphs for improving the generalization and robustness of Graph Neural Networks (GNNs). The method involves estimating graphons for each class of graphs, mixing these graphons, and generating synthetic graphs based on the mixed graphons.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b44\": 1.0,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b50\": 0.8,\n    \"b47\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the generalization and robustness of Graph Neural Networks (GNNs) by proposing a novel data augmentation method called G-Mixup. Inspired by the Mixup strategy used in image and text classification, G-Mixup aims to mix up graph data based on graphons to generate synthetic graphs. The method involves estimating graphons for different classes of graphs, mixing these graphons, and then generating synthetic graphs from the mixed graphons. Theoretical analysis and experimental results demonstrate the effectiveness of G-Mixup in enhancing GNN performance.\",\n  \"Direct Inspiration\": {\n    \"b44\": 0.9,\n    \"b32\": 0.85,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.6,\n    \"b50\": 0.6,\n    \"b47\": 0.6\n  }\n}\n```"], "62d8c4565aee126c0f762dba": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of generating arbitrarily-sized high-quality images and videos, focusing on maintaining consistency between patches and handling different generation orders for images and videos. The proposed NUWA-Infinity model uses an autoregressive over autoregressive mechanism, a Nearby Context Pool (NCP), and an Arbitrary Direction Controller (ADC) to solve these challenges.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b19\": 0.85,\n    \"b27\": 0.88,\n    \"b29\": 0.87\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b5\": 0.78,\n    \"b21\": 0.72,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b6\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.65,\n    \"b14\": 0.65,\n    \"b30\": 0.68\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of generating arbitrarily-sized high-quality images or videos, which includes the difficulty of modeling dependencies between patches and determining suitable generation orders and directions. The authors propose NUWA-Infinity, an autoregressive over autoregressive model, introducing a Nearby Context Pool (NCP) for efficient context caching and an Arbitrary Direction Controller (ADC) for order-aware positional embeddings. The model is evaluated on various high-resolution visual synthesis tasks and shows significant improvements over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b18\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b5\": 0.8,\n    \"b21\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b6\": 0.6,\n    \"b9\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of generating high-resolution images and long-duration videos with arbitrary sizes. It proposes NUWA-Infinity, a visual synthesis model based on an autoregressive over autoregressive generation mechanism, incorporating Nearby Context Pool (NCP) and Arbitrary Direction Controller (ADC) to handle dependencies and generation orders respectively.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of generating arbitrarily-sized high-quality images or videos. It proposes the NUWA-Infinity model, which uses an autoregressive over autoregressive generation mechanism to handle dependencies between patches and within patches. Key innovations include the Nearby Context Pool (NCP) and the Arbitrary Direction Controller (ADC) to manage generation order and context effectively.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b18\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b5\": 0.7,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are: (1) generating arbitrarily-sized high-quality images or videos while maintaining consistency across patches, and (2) handling different generation orders and directions for images and videos. The proposed algorithm, NUWA-Infinity, addresses these challenges using an autoregressive over autoregressive generation mechanism, a Nearby Context Pool (NCP) to cache related patches, and an Arbitrary Direction Controller (ADC) to decide suitable generation orders.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b19\": 0.9,\n    \"b27\": 0.8,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b5\": 0.7,\n    \"b21\": 0.6,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b6\": 0.5,\n    \"b9\": 0.5,\n    \"b10\": 0.4,\n    \"b14\": 0.4,\n    \"b24\": 0.4,\n    \"b28\": 0.5\n  }\n}\n```"], "622eb2495aee126c0f62b12a": ["```json\n{\n  \"Summary\": \"The paper addresses the limitations of traditional topic models like LDA and NMF in capturing semantic relationships among words. It proposes BERTopic, which utilizes document embeddings, dimensionality reduction, and a class-based variation of TF-IDF to generate coherent topic representations. The main challenges include accurately representing documents semantically and improving clustering techniques to enhance topic modeling.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1,\n    \"b27\": 1,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b26\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.7,\n    \"b40\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces BERTopic, a novel topic modeling algorithm that leverages clustering techniques and a class-based variation of TF-IDF to generate coherent topic representations. The key challenges addressed include the limitations of conventional models like LDA and NMF, which disregard semantic relationships among words, and the centroid-based perspective in current embedding-based topic models. The proposed method builds on clustering embeddings and extends it by incorporating a class-based variant of TF-IDF.\",\n    \"Direct Inspiration\": {\n        \"b37\": 1.0,\n        \"b27\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.8,\n        \"b34\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b43\": 0.6,\n        \"b25\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces BERTopic, a topic model that leverages clustering techniques and a class-based variation of TF-IDF to generate coherent topic representations. The primary challenges addressed include the limitations of bag-of-words representations in conventional models, and the centroid-based perspective in clustering embeddings for topic representation. The proposed algorithm overcomes these challenges by embedding documents using a pre-trained language model, reducing dimensionality with UMAP, clustering with HDBSCAN, and extracting topic representations using a custom class-based TF-IDF.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1.0,\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b34\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b8\": 0.7,\n    \"b24\": 0.7,\n    \"b26\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the limitation of conventional topic models like LDA and NMF, which disregard semantic relationships among words in their bag-of-words representations. The proposed algorithm, BERTopic, leverages clustering techniques and a class-based variation of TF-IDF to generate coherent topic representations. It involves creating document embeddings using a pre-trained language model, reducing the dimensionality of these embeddings, and then clustering them to form distinct topics. The paper also introduces a dynamic topic modeling approach and a smoothing technique to handle evolving topics over time.\",\n    \"Direct Inspiration\": {\n        \"b37\": 1.0,\n        \"b27\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.8,\n        \"b43\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b34\": 0.7,\n        \"b19\": 0.6,\n        \"b14\": 0.6,\n        \"b23\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of conventional topic models like LDA and NMF in capturing semantic relationships among words due to their bag-of-words representation. It introduces BERTopic, a novel topic model that leverages clustering techniques and a class-based variation of TF-IDF to generate coherent topic representations. The key steps involve creating document embeddings using pre-trained language models, reducing the dimensionality of these embeddings, and developing a class-based TF-IDF to extract topic representations from clusters.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1.0,\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b34\": 0.8,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b15\": 0.6,\n    \"b23\": 0.5\n  }\n}\n```"], "628c4ce15aee126c0ff597ca": ["```json\n{\n    \"Summary\": \"The main challenge addressed in the paper is to improve the performance of text retrieval systems that use a two-stage retrieval-then-reranking architecture. The authors propose a novel lightweight reranking module named Hybrid List Aware Transformer Ranker (HLATR) that effectively fuses features from both stages. This method aims to enhance the final retrieval performance while maintaining computational efficiency.\",\n    \"Direct Inspiration\": {\n        \"b25\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b6\": 0.7,\n        \"b23\": 0.7,\n        \"b27\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.6,\n        \"b18\": 0.6,\n        \"b26\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving text retrieval systems by proposing the Hybrid List Aware Transformer Ranker (HLATR), which aims to couple features from both retrieval and reranking stages. It introduces a lightweight reranking model that utilizes transformer architecture to fuse retrieval and reranking features, ultimately enhancing retrieval performance with minimal additional computational cost.\",\n  \"Direct Inspiration\": [\"b3\", \"b25\", \"b6\"],\n  \"Indirect Inspiration\": [\"b27\", \"b23\", \"b16\"],\n  \"Other Inspiration\": [\"b10\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses challenges in text retrieval, particularly the need to balance efficiency and effectiveness in a retrieval-then-reranking system. The authors propose a novel Hybrid List Aware Transformer Ranker (HLATR) that couples features from both retrieval and reranking stages using a lightweight transformer model to improve final retrieval performance.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b23\": 0.7,\n    \"b26\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b18\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is optimizing the effectiveness and efficiency of text retrieval systems, particularly focusing on the retrieve-then-rerank architecture. The novel approach introduced by the authors is the Hybrid List Aware Transformer Ranker (HLATR), which adds an additional lightweight reranking stage that fuses features from both the retrieval and reranking stages. This approach is inspired by the need to improve performance while minimizing computational costs.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.9,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b16\": 0.6,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving text retrieval systems by proposing a novel Hybrid List Aware Transformer Ranker (HLATR) that incorporates features from both retrieval and reranking stages. The model aims to enhance performance by using a lightweight reranking module that couples coarse retrieval and fine reranking features. The proposed solution leverages the transformer model architecture to effectively fuse these features and model the list-aware text ranking process.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b25\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```"], "6292aa4a8c0a46000c95808f": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of large displacements and motion occlusions in optical flow estimation by proposing a self-attention-based multiscale feature learning method with occlusion feature map prediction. The novel methods include a multiscale feature learning module using self-attention, an occlusion feature map prediction using a self-learning module, and a hybrid loss function integrating photometric and smoothness losses with the classical endpoint error (EPE) loss.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b52\", \"b56\", \"b57\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b33\", \"b46\", \"b49\", \"b53\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b42\", \"b43\", \"b45\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in optical flow estimation, specifically large displacements and motion occlusions, by proposing a self-attention-based multiscale feature learning method with occlusion feature map prediction. Key contributions include the self-attention mechanism for capturing long-range dependencies, a self-learning-based occlusion prediction module, and a hybrid loss function that integrates photometric and smoothness losses with classic endpoint error (EPE) loss.\",\n  \"Direct Inspiration\": {\n    \"b52\": 1.0,\n    \"b42\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b56\": 0.8,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7,\n    \"b48\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of large displacements and motion occlusions in optical flow estimation by proposing a self-attention-based multiscale feature learning method with occlusion feature map prediction. The proposed method utilizes a self-attention mechanism to capture long-range dependencies and a self-learning module to predict occlusion feature maps. A hybrid loss function is also employed to ensure accuracy and robustness.\",\n  \"Direct Inspiration\": {\n    \"b42\": 0.9,\n    \"b52\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.8,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b56\": 0.7,\n    \"b59\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of large displacements and motion occlusions in optical flow estimation by proposing a self-attention-based multiscale feature learning method combined with an occlusion feature map prediction module and a hybrid loss function. The proposed method improves optical flow accuracy and robustness in challenging scenarios.\",\n  \"Direct Inspiration\": {\n    \"b52\": 0.9,\n    \"b42\": 0.85,\n    \"b43\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b57\": 0.7,\n    \"b56\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.6,\n    \"b37\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Large displacements and motion occlusions in optical flow estimation\",\n    \"inspirations\": \"Self-attention mechanism, multiscale feature learning, occlusion feature map prediction, hybrid loss function\"\n  },\n  \"Direct Inspiration\": {\n    \"b52\": 1.0,\n    \"b64\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.8,\n    \"b43\": 0.8,\n    \"b45\": 0.7,\n    \"b48\": 0.7,\n    \"b57\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.6,\n    \"b56\": 0.6\n  }\n}\n```"], "62a013785aee126c0ff695ef": ["```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in self-supervised learning on molecular graphs: ill-defined pre-training tasks and limited model capacity. The proposed solution is a novel framework called Knowledge-guided Pre-training of Graph Transformer (KPGT), which includes a high-capacity Line Graph Transformer (LiGhT) and a knowledge-guided pre-training strategy that leverages molecular descriptors and fingerprints to enhance the prediction of masked nodes, capturing both structural and semantic information from molecular graphs.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b24\": 0.95,\n    \"b43\": 0.9,\n    \"b61\": 0.85,\n    \"b62\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.75,\n    \"b31\": 0.8,\n    \"b57\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b27\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7,\n    \"b60\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of ill-defined pre-training tasks and limited model capacity in self-supervised learning for molecular graphs. It proposes the Knowledge-guided Pre-training of Graph Transformer (KPGT) framework, which includes a Line Graph Transformer (LiGhT) and a knowledge-guided pre-training strategy. The approach leverages additional molecular knowledge to guide the prediction of masked nodes, thereby capturing both structural and semantic information from molecular graphs. Extensive experiments show that KPGT outperforms current state-of-the-art methods in molecular property prediction.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b24\": 0.9,\n    \"b43\": 0.9,\n    \"b61\": 0.8,\n    \"b62\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.7,\n    \"b45\": 0.7,\n    \"b57\": 0.7,\n    \"b54\": 0.6,\n    \"b60\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b7\": 0.5,\n    \"b16\": 0.5,\n    \"b55\": 0.5,\n    \"b59\": 0.5,\n    \"b63\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in molecular property prediction using self-supervised learning on molecular graphs: ill-defined pre-training tasks and limited model capacity. The authors propose a novel self-supervised learning framework called Knowledge-guided Pre-training of Graph Transformer (KPGT), which includes a high-capacity model named Line Graph Transformer (LiGhT) and a knowledge-guided pre-training strategy. The proposed method leverages additional molecular knowledge (descriptors and fingerprints) to guide the prediction of masked nodes, thus capturing abundant structural and semantic information from large-scale unlabeled molecules. Extensive experiments demonstrate superior performance compared to state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b43\": 1.0,\n    \"b61\": 1.0,\n    \"b62\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b31\": 0.8,\n    \"b45\": 0.8,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b27\": 0.7,\n    \"b34\": 0.7,\n    \"b60\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\"Ill-defined pre-training tasks\", \"Limited model capacity\"],\n    \"inspirations\": [\"Self-supervised learning in NLP and CV\"]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b9\", \"b24\", \"b43\", \"b61\", \"b62\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b6\", \"b27\", \"b34\", \"b35\", \"b60\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b31\", \"b45\", \"b57\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges of ill-defined pre-training tasks and limited model capacity in self-supervised learning methods for molecular graphs. It introduces the Knowledge-guided Pre-training of Graph Transformer (KPGT) framework, which includes the Line Graph Transformer (LiGhT) and a knowledge-guided pre-training strategy to capture both structural and semantic information from molecular graphs. The proposed method demonstrates superior performance on multiple molecular property prediction tasks.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b24\": 0.9,\n    \"b43\": 0.85,\n    \"b61\": 0.85,\n    \"b62\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.8,\n    \"b31\": 0.8,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.75,\n    \"b60\": 0.75,\n    \"b6\": 0.75,\n    \"b27\": 0.75,\n    \"b35\": 0.75\n  }\n}\n```"], "62e744545aee126c0f33c1ec": ["```json\n{\n    \"Summary\": \"The paper addresses key challenges in graph-based collaborative filtering (CF) recommender systems, specifically data noise, data sparsity, and skewed distribution. The proposed algorithm, Self-Supervised Hypergraph Transformer (SHT), integrates hypergraph neural networks with topology-aware Transformers to enhance the robustness and generalization performance of these systems. The model aims to maintain global cross-user collaborative relations while unifying the local and global learning views for collaborative relations under a generative self-supervised learning framework.\",\n    \"Direct Inspiration\": {\n        \"b5\": 1.0,\n        \"b25\": 1.0,\n        \"b30\": 1.0,\n        \"b32\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b24\": 0.8,\n        \"b38\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.6,\n        \"b28\": 0.6,\n        \"b34\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Data noise in user-item interactions\",\n      \"Data sparsity and skewed distribution\",\n      \"Effectiveness of current graph-based CF models impacted by noisy and sparse supervision signals\"\n    ],\n    \"Inspirations\": [\n      \"Hypergraph neural network integration\",\n      \"Topology-aware Transformer\",\n      \"Self-supervised learning for robust user embeddings\"\n    ]\n  },\n  \"Direct Inspiration\": [\"b5\", \"b30\"],\n  \"Indirect Inspiration\": [\"b24\", \"b29\", \"b28\"],\n  \"Other Inspiration\": [\"b25\", \"b37\"]\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges addressed in the paper are data noise and data sparsity in recommender systems, which lead to inaccurate user representations and biased recommendations. The proposed algorithm, Self-Supervised Hypergraph Transformer (SHT), integrates hypergraph neural networks with topology-aware Transformers to enhance the robustness and generalization of graph-based collaborative filtering. The SHT framework includes local and global learning views, a graph-based meta transformation layer, and a generative self-supervised learning framework for graph topological denoising.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b25\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.9,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b24\": 0.7,\n    \"b38\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of data noise and sparsity in graph-based collaborative filtering (CF) recommender systems. It proposes a Self-Supervised Hypergraph Transformer (SHT) that integrates hypergraph neural networks with topology-aware Transformers to improve robustness and performance. SHT enhances user embeddings by combining local and global collaborative relations and introduces a graph topological denoising scheme for data augmentation.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.9,\n    \"b28\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b24\": 0.7,\n    \"b38\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of data noise, data sparsity, and skewed distribution in graph-based collaborative filtering (CF) recommender systems. It proposes a Self-Supervised Hypergraph Transformer (SHT) to enhance the robustness and generalization performance by integrating hypergraph neural networks with a topology-aware Transformer. SHT aims to maintain global cross-user collaborative relations and unify local and global learning views for better user/item representation through self-supervised learning and graph topological denoising.\",\n    \"Direct Inspiration\": {\n        \"b5\": 1.0,\n        \"b25\": 1.0,\n        \"b30\": 1.0,\n        \"b32\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.9,\n        \"b24\": 0.9,\n        \"b38\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.8,\n        \"b21\": 0.8,\n        \"b22\": 0.8,\n        \"b29\": 0.8\n    }\n}\n```"], "623be1965aee126c0f37abe2": ["```json\n{\n  \"Summary\": \"The paper proposes UIE, a unified text-to-structure generation architecture for Information Extraction (IE). The primary challenges the paper addresses are the need for a universal architecture to model diverse IE tasks, the requirement to adaptively control the extraction process, and the necessity of effective knowledge sharing across different IE settings. The novel approaches introduced include the Structural Extraction Language (SEL) for uniform structure encoding, the Structural Schema Instructor (SSI) for schema-based prompting, and a comprehensive pre-training strategy on large-scale heterogeneous datasets.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b41\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b14\": 0.7,\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b48\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on addressing the challenges of developing a universal Information Extraction (IE) architecture that can handle diverse IE tasks and structures. The proposed solution, UIE, aims to uniformly model text-to-structure transformations across different IE tasks using a unified framework. Key components include the Structural Extraction Language (SEL) for encoding heterogeneous structures, the Structural Schema Instructor (SSI) for adaptive schema-based prompts, and a large-scale pre-training on diverse datasets to enhance the model's generalization and transferability.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b41\": 0.9\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.7,\n    \"b34\": 0.7,\n    \"b12\": 0.7,\n    \"b14\": 0.8,\n    \"b3\": 0.8,\n    \"b35\": 0.8,\n    \"b33\": 0.8,\n    \"b48\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of developing a universal Information Extraction (IE) architecture that can model various IE tasks uniformly, adaptively predict different structures, and effectively learn from diverse resources. The proposed algorithm, UIE, employs a unified text-to-structure generation framework, a structural extraction language (SEL), and a structural schema instructor (SSI) to encode and generate different IE structures. The UIE model is pre-trained on large-scale heterogeneous datasets to enhance its ability to share and transfer knowledge across different IE tasks.\",\n    \"Direct Inspiration\": {\n        \"b14\": 1.0,\n        \"b3\": 1.0,\n        \"b35\": 1.0,\n        \"b33\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.8,\n        \"b10\": 0.7,\n        \"b34\": 0.7,\n        \"b12\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.5,\n        \"b18\": 0.5,\n        \"b41\": 0.5,\n        \"b48\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Information Extraction (IE) by proposing a unified text-to-structure generation architecture called UIE. The primary challenges include the diversity of IE tasks, the need for adaptive control in the extraction process, and the complications of developing dedicated architectures for various tasks. UIE aims to model different IE tasks uniformly, adaptively generate targeted structures, and collaboratively learn general IE abilities from various knowledge sources. Key components include a structural extraction language (SEL) and a schema-based prompt mechanism (SSI). UIE is pre-trained on large-scale datasets to enhance knowledge sharing and quick adaptation across different IE settings.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b33\": 1,\n    \"b35\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b12\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b9\": 0.7,\n    \"b18\": 0.7,\n    \"b41\": 0.7,\n    \"b48\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of developing dedicated architectures for varied IE tasks, limited knowledge sharing between isolated models, and the high cost and time of constructing specialized datasets. It proposes a unified text-to-structure generation architecture called UIE, which uses a Structural Extraction Language (SEL) and Structural Schema Instructor (SSI) to encode and adaptively generate different IE structures. The model is pre-trained on large-scale heterogeneous datasets to enhance its general IE abilities, improving performance in supervised, low-resource, and few-shot settings.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b41\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b33\": 0.6,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b9\": 0.5,\n    \"b10\": 0.5,\n    \"b12\": 0.5,\n    \"b14\": 0.5,\n    \"b34\": 0.5,\n    \"b48\": 0.5\n  }\n}\n```"], "6281b2a35aee126c0ffd514a": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include restoring high-quality facial details from low-quality faces with unknown degradations, and balancing the trade-off between detail generation and fidelity preservation. The proposed algorithm, VQFR, leverages a vector-quantized (VQ) codebook as a comprehensive dictionary for all facial areas and a parallel decoder structure to fuse input features and texture features, achieving superior facial detail generation while maintaining fidelity.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b22\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b18\": 0.7,\n    \"b36\": 0.7,\n    \"b42\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b16\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of blind face restoration, particularly in real-world scenarios with diverse and complicated degradations. It proposes a novel method called VQFR, which leverages a Vector Quantized (VQ) codebook and a parallel decoder to generate high-quality facial details while preserving fidelity. The method is inspired and motivated by various prior works on geometric, generative, and reference priors but introduces significant improvements.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b32\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b30\": 0.6,\n    \"b36\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.4,\n    \"b42\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of blind face restoration in real-world scenarios with complex degradations. It proposes a novel Vector Quantized (VQ) codebook and a parallel decoder to achieve high-quality facial details while preserving fidelity. The VQ codebook serves as a comprehensive texture dictionary, and the parallel decoder structure separates the tasks of generating realistic details and maintaining fidelity, incorporating a texture warping module for better alignment.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b32\": 1,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.8,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of blind face restoration, particularly in real-world scenarios with various degradations. The proposed method, VQFR, leverages a Vector-Quantized (VQ) codebook as a facial dictionary and introduces a parallel decoder structure to generate high-quality facial details while preserving fidelity. The main contributions include the use of a VQ dictionary for HQ facial details and a parallel decoder to fuse input features and texture features effectively.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b32\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.8,\n    \"b7\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.7,\n    \"b23\": 0.65,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of restoring low-quality (LQ) faces with unknown degradations, diverse face poses, and expressions. It proposes a novel face restoration method called VQFR, which combines a Vector-Quantized (VQ) dictionary of high-quality (HQ) facial details and a parallel decoder to achieve both high-fidelity and realistic facial details. The method leverages the VQ codebook for comprehensive low-level feature banking and introduces network designs that balance detail generation and identity preservation.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b22\": 0.85,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.75,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.65,\n    \"b23\": 0.6,\n    \"b17\": 0.55\n  }\n}\n```"], "624802f36750f84d00082030": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the low adoption of Source Address Validation (SAV) by network operators to mitigate Distributed Denial of Service (DDoS) attacks. The paper proposes a randomized control trial (RCT) to measure the effectiveness of various notification methods, including social and reciprocity nudges, and the role of national CERTs in encouraging SAV adoption. The study finds no significant effect of these notifications compared to a control group, highlighting the complexity of incentivizing SAV implementation.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b11\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Source Address Validation (SAV) adoption among network operators to mitigate Distributed Denial of Service (DDoS) attacks. It introduces a randomized control trial (RCT) to measure the effectiveness of various notification methods and messages on remediation rates. The study highlights the need for more effective notification systems and explores the roles of different communication channels and behavioral nudges.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b11\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b13\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of encouraging network operators to adopt Source Address Validation (SAV) to mitigate Distributed Denial of Service (DDoS) attacks. The study is motivated by the lack of SAV adoption and explores different notification mechanisms to improve SAV implementation rates. The authors conduct the first randomized control trial (RCT) to measure the impact of notifications on SAV remediation rates. They also test social and reciprocity nudges, different communication channels, and the effect of notifications sent by a national CERT.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b11\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.7,\n    \"b27\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the persistent issue of Distributed Denial of Service (DDoS) attacks facilitated by IP spoofing. The authors propose a randomized control trial (RCT) to measure the impact of different notification mechanisms on Source Address Validation (SAV) adoption among network operators. They explore various channels (direct emails, CERT notifications, NOG mailing lists) and nudging mechanisms (social and reciprocity nudges) to improve SAV adoption. The study reveals that none of the treatments performed better than the control group.\",\n  \"Direct Inspiration\": [\"b6\", \"b11\", \"b12\"],\n  \"Indirect Inspiration\": [\"b21\", \"b23\"],\n  \"Other Inspiration\": [\"b13\", \"b10\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of distributed denial of service (DDoS) attacks, specifically focusing on the lack of source-address validation (SAV) leading to IP spoofing. The authors conducted the first randomized control trial (RCT) to measure the impact of notifications on SAV remediation rates among network operators, utilizing misconfigured open resolvers as vantage points. They explored the effectiveness of different notification channels and message designs, including social and reciprocity nudges, and partnered with a Brazilian CERT for direct communication. Despite their comprehensive approach, the study found no significant impact of the notifications compared to the control group.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b11\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.5\n  }\n}\n```"], "63034ea190e50fcafd73e0b0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of cross-domain few-shot learning on heterogeneous graphs (HGs) to tackle label scarcity. It proposes a novel model, CrossHG-Meta, which aggregates multiple fixed-size sets of semantic contexts to produce node embeddings and employs a cross-domain meta-learning strategy with a domain critic to handle domain shifts. The model also incorporates self-supervised signals for contrastive regularization to further alleviate data scarcity.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b41\": 1.0,\n    \"b47\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.8,\n    \"b28\": 0.75,\n    \"b36\": 0.75,\n    \"b30\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b32\": 0.7,\n    \"b44\": 0.7,\n    \"b40\": 0.7,\n    \"b34\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of cross-domain few-shot learning on heterogeneous graphs (HGs). The primary challenges include handling graph heterogeneity, extracting transferable meta-knowledge without accessing the target domain, and performing prediction with few-shot annotations. The proposed CrossHG-Meta model aggregates semantic contexts using meta-paths to create node embeddings and includes a domain critic to manage domain shifts. Furthermore, it incorporates self-supervised signals to aid meta-optimization.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b41\": 1,\n    \"b47\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.9,\n    \"b17\": 0.9,\n    \"b19\": 0.8,\n    \"b30\": 0.8,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b36\": 0.6,\n    \"b43\": 0.6,\n    \"b42\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of cross-domain few-shot learning on heterogeneous graphs (HGs), which involves leveraging meta-knowledge from data-rich source domains to improve prediction in a target domain with few labeled examples. The proposed model, CrossHG-Meta, uses a heterogeneous graph encoder and a cross-domain meta-learning strategy with domain shift awareness, combined with self-supervised contrastive regularization, to enhance generalizability and tackle label scarcity.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b41\": 0.9,\n    \"b47\": 0.9,\n    \"b16\": 0.85,\n    \"b17\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.8,\n    \"b30\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.75,\n    \"b20\": 0.75,\n    \"b46\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenges of few-shot learning on heterogeneous graphs (HGs), addressing issues such as graph heterogeneity, domain shift, and labeled data scarcity. The proposed CrossHG-Meta model incorporates a novel approach using cross-domain meta-learning and self-supervised contrastive regularization to enhance generalization and adaptation capabilities.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b41\": 1.0,\n    \"b47\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.6,\n    \"b39\": 0.6,\n    \"b44\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of cross-domain few-shot learning on heterogeneous graphs (HGs) by proposing a novel model called CrossHG-Meta. The key challenges include graph heterogeneity, domain shifts without accessing target domain data, and prediction on novel categories with few-shot annotations. The proposed model aggregates semantic contexts to produce node embeddings and utilizes a cross-domain meta-learning methodology with domain critic and self-supervised signals to improve generalizability.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b17\": 0.9,\n    \"b19\": 0.8,\n    \"b41\": 0.85,\n    \"b47\": 0.82\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b13\": 0.78,\n    \"b20\": 0.77\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.65,\n    \"b34\": 0.7,\n    \"b39\": 0.68,\n    \"b43\": 0.72\n  }\n}\n```"], "62c2a9595aee126c0fcf0a32": ["```json\n{\n  \"Summary\": \"This paper addresses the robustness of Graph Neural Networks (GNNs) against adversarial attacks. It proposes an approach named STABLE that uses unsupervised learning to derive reliable representations for graph structure refinement.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b44\", \"b39\", \"b53\", \"b54\"],\n    \"notes\": \"These references are highlighted as directly inspiring the methods and techniques proposed in the paper.\"\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b23\", \"b51\", \"b43\"],\n    \"notes\": \"These references are indirectly related to the proposed methods and provide foundational techniques or background.\"\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b30\", \"b42\"],\n    \"notes\": \"These references provide additional context or methodologies that support the paper's contributions.\"\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of defending Graph Neural Networks (GNNs) against non-targeted adversarial attacks that aim to reduce the overall performance of GNNs by perturbing the graph structure. The proposed method, STABLE, introduces a contrastive learning-based approach to obtain robust representations for structure refining and an advanced GCN module for improved robustness.\",\n  \"Direct Inspiration\": {\n    \"b39\": 0.9,\n    \"b44\": 0.85,\n    \"b53\": 0.8,\n    \"b54\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.7,\n    \"b30\": 0.75,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b24\": 0.65,\n    \"b36\": 0.65,\n    \"b42\": 0.6,\n    \"b52\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving the robustness of Graph Neural Networks (GNNs) against non-targeted adversarial attacks. The authors propose a method named STABLE, which refines the graph structure by learning more reliable representations through an unsupervised contrastive learning approach. The method also includes an advanced GCN classifier with a modified renormalization trick to further enhance robustness.\",\n    \"Direct Inspiration\": {\n        \"b54\": 1.0,\n        \"b39\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b44\": 0.8,\n        \"b43\": 0.75,\n        \"b23\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b51\": 0.6,\n        \"b42\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of defending Graph Neural Networks (GNNs) against non-targeted adversarial attacks that perturb the graph structure. It introduces the STABLE approach, which involves a contrastive learning method with robustness-oriented augmentations to learn reliable representations for refining graph structures. The method emphasizes the importance of carrying correct structural information and being insensitive to perturbations.\",\n  \"Direct Inspiration\": {\n    \"b44\": 0.9,\n    \"b39\": 0.85,\n    \"b54\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b51\": 0.8,\n    \"b30\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.7,\n    \"b23\": 0.7,\n    \"b52\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the robustness of Graph Neural Networks (GNNs) against adversarial attacks, particularly those that perturb the graph structure. The proposed method, STABLE (STructure leArning GNN via more reliaBLe rEpresentations), introduces an unsupervised contrastive learning approach with robustness-oriented augmentations to obtain reliable representations for structure refining. The learned representations are used to refine the graph structure, enhancing the GNN's robustness against adversarial attacks.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1,\n    \"b39\": 0.9,\n    \"b54\": 0.9,\n    \"b53\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b51\": 0.7,\n    \"b43\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```"], "62f07ec290e50fcafde5ad10": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of whether few-shot learning requires models to store a large amount of information in their parameters and investigates if memorization can be decoupled from generalization. It proposes Atlas, a retrieval-augmented language model, capable of strong few-shot learning, despite having fewer parameters than other models. The approach leverages a non-parametric memory source, enhancing parametric language models using retrieval-augmented architectures. The paper emphasizes training strategies, pre-training tasks, and fine-tuning strategies to improve few-shot performance.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b86\": 0.9,\n    \"b59\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.8,\n    \"b64\": 0.75,\n    \"b26\": 0.7,\n    \"b34\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents Atlas, a retrieval-augmented language model designed to achieve strong few-shot learning capabilities with a lower parameter count compared to other models. The primary challenge addressed is whether few-shot learning requires extensive parameter-based memorization, which the authors propose can be mitigated by using external non-parametric knowledge sources. The model leverages a dual-encoder architecture and the Fusion-in-Decoder technique, and demonstrates state-of-the-art performance on various tasks through innovative training strategies and pre-training tasks.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b27\": 0.9,\n    \"b86\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b59\": 0.6,\n    \"b21\": 0.6,\n    \"b24\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.4,\n    \"b46\": 0.4,\n    \"b1\": 0.4,\n    \"b34\": 0.3,\n    \"b64\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include investigating whether few-shot learning requires models to store large amounts of information in their parameters and decoupling memorization from generalization. The proposed algorithm leverages a retrieval-augmented architecture, introducing Atlas, which uses retrieval during both pre-training and fine-tuning to achieve strong few-shot performance on knowledge tasks. Key contributions include designing and training retrieval-augmented language models, exploring fine-tuning strategies, and conducting extensive downstream experiments demonstrating state-of-the-art results.\",\n  \"Direct Inspiration\": {\n    \"b29\": 0.95,\n    \"b27\": 0.9,\n    \"b86\": 0.85,\n    \"b21\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b59\": 0.8,\n    \"b64\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b46\": 0.5,\n    \"b1\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper investigates whether few-shot learning in large language models (LLMs) requires extensive memorization within the model parameters and proposes Atlas, a retrieval-augmented language model to decouple memorization from generalization. Atlas uses a retriever to access an external non-parametric knowledge source, enhancing the model's adaptability, interpretability, and efficiency. The study explores various training techniques and pre-training tasks to improve Atlas's few-shot learning performance on downstream tasks, achieving state-of-the-art results.\",\n  \"Direct Inspiration\": [\"b86\", \"b6\"],\n  \"Indirect Inspiration\": [\"b29\", \"b27\"],\n  \"Other Inspiration\": [\"b20\", \"b46\", \"b1\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of whether few-shot learning requires large amounts of memorized information in model parameters by introducing Atlas, a retrieval-augmented language model. Atlas leverages a non-parametric memory through a retrieval mechanism to improve few-shot learning capabilities. The core contributions include designing and training retrieval-augmented models, exploring fine-tuning strategies, and demonstrating strong performance across various tasks.\",\n  \"Direct Inspiration\": {\n    \"b86\": 0.9,\n    \"b29\": 0.85,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.75,\n    \"b26\": 0.7,\n    \"b34\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b59\": 0.6,\n    \"b64\": 0.55\n  }\n}\n```"], "628464625aee126c0faca44e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving knowledge graph (KG) link prediction by proposing CASCADER, a novel cross-modal ensemble that balances ranking accuracy and efficiency by using a tiered ranking architecture with dynamic answer pruning. CASCADER leverages increasingly complex language models (LMs) to adaptively reweight and rerank outputs of more efficient base KGEs, significantly improving accuracy and efficiency over existing models.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b8\": 0.85,\n    \"b25\": 0.9,\n    \"b39\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.75,\n    \"b4\": 0.7,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b32\": 0.65,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper are high precision but low coverage in KGs, impractically expensive models for cross-modal ensembles, and inefficient dual-encoder architectures.\",\n    \"Algorithm\": \"The proposed algorithm, CASCADER, is a tiered ranking architecture that uses increasingly complex language models to rerank the outputs of more efficient base KGEs. It employs an adaptive answer selection strategy to balance accuracy and efficiency.\"\n  },\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b25\": 1,\n    \"b39\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b27\": 0.9,\n    \"b19\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.8,\n    \"b30\": 0.8,\n    \"b32\": 0.8,\n    \"b45\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the accuracy and efficiency of knowledge graph link prediction. The proposed algorithm, CASCADER, is a tiered ranking architecture that integrates more complex language models with efficient base KGEs to enhance ranking accuracy while minimizing computational costs. The method involves progressively refining a smaller subset of candidates through multiple tiers, balancing the trade-off between accuracy and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b25\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b30\": 0.7,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b32\": 0.6,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the low coverage of knowledge graphs (KGs) despite their high precision, and the inefficiency and computational expense of cross-modal ensembles for link prediction. CASCADER is proposed as a novel tiered ranking architecture that balances ranking accuracy and efficiency by progressively refining candidate rankings using increasingly complex language models (LMs).\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b25\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b19\": 0.7,\n    \"b30\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of enhancing link prediction in knowledge graphs (KGs) by integrating structure-based knowledge graph embeddings (KGEs) and text-based language models (LMs). The proposed algorithm, CASCADER, is a tiered ranking architecture that aims to balance accuracy and efficiency by using progressively complex LMs to reweight and rerank candidates scored by a more efficient base KGE.\",\n    \"Direct Inspiration\": {\n        \"b8\": 0.9,\n        \"b25\": 0.9,\n        \"b39\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.8,\n        \"b19\": 0.7,\n        \"b30\": 0.7,\n        \"b32\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b45\": 0.6\n    }\n}\n```"], "6303545e90e50fcafd7d3b71": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the diverse local patterns in graph neural networks (GNNs), necessitating node-wise architecture instead of applying the same architecture to all nodes. The proposed algorithm introduces context-aware controllers to determine optimal GNN architectures for each node based on local patterns, including depth, aggregator, and resolution controllers.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b16\": 1,\n    \"b25\": 1,\n    \"b32\": 1,\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b10\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b21\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying a single GNN architecture uniformly across all nodes in a graph, despite the diverse local patterns present in different nodes' neighborhoods. The authors propose a novel framework that uses parametric controllers to adapt the GNN architecture in a node-wise manner, optimizing aspects such as depth, aggregator, and resolution based on each node's local patterns. This approach enhances the performance of GNNs on large-scale graphs and diverse datasets.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b5\": 0.9,\n    \"b16\": 0.9,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b25\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b20\": 0.7,\n    \"b30\": 0.7,\n    \"b36\": 0.7,\n    \"b37\": 0.7,\n    \"b38\": 0.7,\n    \"b39\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying a single GNN architecture to all nodes in a graph, which can be unsuitable due to diverse local patterns among nodes. It proposes a node-wise architecture for GNNs using parametric controllers to adapt the depth, aggregator, and resolution based on the local context of each node. This framework aims to improve performance on large-scale graphs under both transductive and inductive settings.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b16\": 0.7,\n    \"b25\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inadequacy of applying a uniform GNN architecture to all nodes within a graph due to the diverse local patterns, both topological and attribute-based, among nodes. The authors propose a framework that utilizes parametric controllers to predict suitable architectures for each node, addressing aspects such as depth, aggregators, and resolution. The proposed method ensures adaptability and efficiency in handling large-scale graphs under both transductive and inductive settings.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b16\": 1.0,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b10\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing Graph Neural Networks (GNNs) by tailoring their architecture to individual nodes, rather than applying a single architecture across all nodes. This is motivated by the diverse local patterns in graphs, which necessitate different GNN depths, aggregators, and resolutions for different nodes. The proposed solution involves a framework with parametric controllers that predict suitable architectures for each node based on its local context, aiming to improve performance on large-scale graphs.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.95,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b10\": 0.85,\n    \"b32\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.7,\n    \"b39\": 0.65\n  }\n}\n```"], "62d16f8d5aee126c0fd82ddd": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing the Linux kernel for data center applications to reduce I-cache and I-TLB misses, which significantly impact performance. The authors propose a profile-guided optimization (PGO) approach using a universal kernel profile derived from multiple applications to achieve notable performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b6\": 0.8,\n    \"b9\": 0.7,\n    \"b12\": 0.7,\n    \"b27\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.65,\n    \"b26\": 0.65,\n    \"b40\": 0.6,\n    \"b41\": 0.6,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing the Linux kernel for data center applications due to frequent I-cache and I-TLB misses. The authors propose a universal profile-guided optimization approach, combining kernel profiles from multiple applications to enhance performance.\",\n  \"Direct Inspiration\": [\"b6\", \"b8\", \"b26\"],\n  \"Indirect Inspiration\": [\"b5\", \"b9\", \"b12\", \"b27\"],\n  \"Other Inspiration\": [\"b4\", \"b22\", \"b40\", \"b41\", \"b42\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of frequent I-cache and I-TLB misses in modern data center applications, which are exacerbated by the large instruction footprints of these applications. The proposed solution is to optimize the Linux kernel using a universal profile generated from multiple application-specific profiles. This approach aims to improve instruction locality and reduce misses, achieving an average end-to-end speedup of 8.02% across various data center applications.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.9,\n    \"b27\": 0.85,\n    \"b26\": 0.8,\n    \"b40\": 0.75,\n    \"b41\": 0.75,\n    \"b42\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b9\": 0.65,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing I-cache and I-TLB misses in data center applications, which have large instruction footprints. The authors propose a novel method of profile-guided kernel optimizations by combining kernel profiles from multiple applications to create a universal profile, which is then used to optimize the Linux kernel. This approach aims to improve data center performance and reduce energy costs.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b8\": 1,\n    \"b26\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b40\": 0.8,\n    \"b41\": 0.8,\n    \"b42\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b9\": 0.6,\n    \"b12\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of frequent I-cache and I-TLB misses in modern data center applications due to their large instruction footprints. It proposes optimizing the Linux kernel using a universal profile generated from multiple application profiles to achieve significant performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b9\": 0.9,\n    \"b12\": 0.9,\n    \"b27\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.8,\n    \"b26\": 0.8\n  }\n}\n```"], "62393e7f5aee126c0f12607f": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of document-level relation extraction (DocRE), including the complexity of handling multiple entities, imbalance in relation types, and the lack of effective use of distantly supervised data. The proposed solutions include a semi-supervised learning framework with an axial attention module for improved reasoning, Adaptive Focal Loss for handling imbalanced labels, and knowledge distillation to better utilize distantly supervised data.\",\n    \"Direct Inspiration\": {\n        \"b22\": 1,\n        \"b27\": 0.9,\n        \"b30\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b23\": 0.8,\n        \"b25\": 0.7,\n        \"b26\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b16\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in document-level relation extraction (DocRE), focusing on the complexity of handling multiple entities, imbalance in relation types, and the need for better utilization of distantly supervised data. The proposed solution includes a semi-supervised learning framework with an axial attention module for two-hop reasoning, Adaptive Focal Loss for imbalanced label distribution, and knowledge distillation to better leverage distantly supervised data.\",\n    \"Direct Inspiration\": {\n        \"b27\": 0.9,\n        \"b22\": 0.95,\n        \"b30\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.75,\n        \"b25\": 0.7,\n        \"b26\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b23\": 0.8,\n        \"b5\": 0.65\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of document-level relation extraction (DocRE), such as high complexity, imbalanced relation types, and the limited use of distantly supervised data. The proposed solutions include a semi-supervised learning framework incorporating an axial attention module, Adaptive Focal Loss, and knowledge distillation techniques.\",\n    \"Direct Inspiration\": {\n        \"b22\": 0.9,\n        \"b27\": 0.85,\n        \"b30\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b10\": 0.75,\n        \"b26\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.65,\n        \"b20\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in document-level relation extraction (DocRE), specifically focusing on the complexity of entity interactions, class imbalance in relation types, and the adaptation of distantly supervised data. The authors propose a semi-supervised learning framework that includes an axial attention module for better reasoning, Adaptive Focal Loss for addressing imbalanced label distribution, and knowledge distillation for leveraging distantly supervised data more effectively.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b22\": 1.0,\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.9,\n    \"b26\": 0.8,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of document-level relation extraction (DocRE), focusing on the complexity of entity pair classification, class imbalance in relation types, and the adaptation of distantly supervised data. It proposes a semi-supervised learning framework featuring an axial attention module for two-hop reasoning, Adaptive Focal Loss for class imbalance, and knowledge distillation for better utilization of distantly supervised data.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b27\": 0.9,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.7,\n    \"b25\": 0.7\n  }\n}\n```"], "6293c3025d72d8000db42919": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the optimization of Deep Neural Networks (DNN) by co-designing the neural network architecture and scheduling policy for hardware deployment. The proposed CHaNAS framework integrates both the DNN architecture and compiler-level scheduling optimizations to achieve superior performance on target hardware. Key inspirations are drawn from hardware-aware Neural Architecture Search (NAS) techniques and efficient DNN model designs.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b20\": 0.85,\n    \"b21\": 0.8,\n    \"b26\": 0.75,\n    \"b35\": 0.7,\n    \"b42\": 0.65\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.6,\n    \"b15\": 0.55,\n    \"b22\": 0.5,\n    \"b23\": 0.45,\n    \"b24\": 0.4,\n    \"b52\": 0.35\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.3,\n    \"b50\": 0.25\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of co-designing Deep Neural Network (DNN) architectures and scheduling policies to optimize performance across varied hardware platforms. The proposed CHaNAS framework integrates both DNN architecture search and compiler-level scheduling strategies to achieve higher performance. Key challenges include constructing a large joint search space, efficiently evaluating each design pair, and reducing the search space using evolutionary techniques.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b20\": 0.9,\n    \"b21\": 0.85,\n    \"b35\": 0.85,\n    \"b42\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b28\": 0.75,\n    \"b48\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.7,\n    \"b50\": 0.7,\n    \"b57\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the efficient design and deployment of DNN models on various hardware, incorporating both neural network architecture and scheduling policy optimization. The proposed CHaNAS framework aims to jointly optimize these aspects to achieve better performance than existing methods.\",\n  \"Direct Inspiration\": [\"b12\", \"b13\", \"b21\"],\n  \"Indirect Inspiration\": [\"b3\", \"b42\", \"b50\"],\n  \"Other Inspiration\": [\"b10\", \"b22\", \"b28\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing Deep Neural Network (DNN) performance by integrating both neural architecture search (NAS) and compiler-level scheduling strategies to fit target hardware. It introduces the Compiler and Hardware aware Network Architecture Search (CHaNAS) framework, which constructs a joint search space for both DNN architectures and scheduling policies, aiming to find the optimal co-design solution for various hardware platforms.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b20\": 1,\n    \"b21\": 1,\n    \"b26\": 1,\n    \"b35\": 1,\n    \"b42\": 1,\n    \"b57\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b15\": 0.8,\n    \"b22\": 0.8,\n    \"b23\": 0.8,\n    \"b24\": 0.8,\n    \"b52\": 0.8,\n    \"b3\": 0.8,\n    \"b28\": 0.8,\n    \"b50\": 0.8,\n    \"b13\": 0.8,\n    \"b17\": 0.8,\n    \"b27\": 0.8,\n    \"b18\": 0.8,\n    \"b47\": 0.8,\n    \"b46\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.7,\n    \"b56\": 0.7,\n    \"b6\": 0.7,\n    \"b32\": 0.7,\n    \"b40\": 0.7,\n    \"b10\": 0.7,\n    \"b55\": 0.7,\n    \"b8\": 0.7,\n    \"b39\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The paper addresses the challenge of optimizing DNN model design and deployment by incorporating cross-stack co-design approaches that consider both neural network architectures and compiler-level scheduling strategies.\",\n    \"inspirations\": \"The paper is inspired by hardware-aware Neural Architecture Search (NAS) methods and compiler-level optimization techniques.\"\n  },\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b21\": 1,\n    \"b20\": 0.9,\n    \"b26\": 0.9,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b42\": 0.8,\n    \"b46\": 0.8,\n    \"b50\": 0.8,\n    \"b56\": 0.8,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b13\": 0.7,\n    \"b18\": 0.7,\n    \"b31\": 0.7,\n    \"b32\": 0.7,\n    \"b39\": 0.7,\n    \"b40\": 0.7,\n    \"b48\": 0.7\n  }\n}\n```"], "62de84a55aee126c0f96fbb9": ["```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are understanding the underlying factors of the performance gains in contrastive learning (CL) for recommendation systems, particularly questioning the necessity of graph augmentations. The authors propose a novel graph-augmentation-free CL method that uses random uniform noises to directly regularize the embedding space towards a more uniform distribution, leading to better recommendation accuracy and model training efficiency.\",\n    \"Direct Inspiration\": [\"b13\", \"b17\", \"b23\", \"b27\"],\n    \"Indirect Inspiration\": [\"b3\", \"b8\", \"b24\"],\n    \"Other Inspiration\": [\"b7\", \"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the necessity and efficacy of graph augmentations in contrastive learning (CL) for recommendation systems. It proposes a graph-augmentation-free CL method that adds random uniform noise to the original representations, thereby simplifying the process and making it more efficient while maintaining or improving performance.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b27\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b8\": 0.65,\n    \"b19\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving recommendation performance using contrastive learning (CL) without relying on graph augmentations. It proposes a novel graph-augmentation-free CL method that adds random uniform noises to the original representations, making the process more efficient and controllable.\",\n  \"Direct Inspiration\": [\"b17\", \"b27\", \"b23\"],\n  \"Indirect Inspiration\": [\"b8\", \"b7\"],\n  \"Other Inspiration\": [\"b10\", \"b11\", \"b15\", \"b39\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenge of understanding why contrastive learning (CL) can boost recommendation performance and proposes a novel graph-augmentation-free CL method that adds random uniform noises to representations instead of relying on dropout-based graph augmentations. The proposed method aims to regulate the uniformity of the representation distribution more effectively and efficiently.\",\n  \"Direct Inspiration\": [\"b7\", \"b23\"],\n  \"Indirect Inspiration\": [\"b8\", \"b17\", \"b27\"],\n  \"Other Inspiration\": [\"b4\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving recommendation performance using Contrastive Learning (CL). It discovers that the InfoNCE loss, rather than graph augmentation, is the critical factor in enhancing recommendation performance. The paper proposes a graph-augmentation-free CL method that adds random uniform noises to representations, enhancing efficiency and effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b23\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b10\": 0.5,\n    \"b11\": 0.5,\n    \"b15\": 0.5\n  }\n}\n```"], "63180bf590e50fcafded784e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the efficiency and effectiveness of contrastive learning (CL) in recommendation systems. The authors propose a novel method called XSimGCL, which builds on their previous work SimGCL by eliminating graph augmentations and using noise-based augmentation instead. This approach aims to achieve a more evenly distributed representation and reduce computational complexity.\",\n  \"Direct Inspiration\": [\"b11\", \"b23\", \"b27\"],\n  \"Indirect Inspiration\": [\"b21\", \"b30\", \"b35\"],\n  \"Other Inspiration\": [\"b16\", \"b19\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving contrastive learning (CL) based recommendation systems by proposing a new method, XSimGCL, which replaces traditional graph augmentations with noise-based augmentations and reduces computational complexity.\",\n  \"Direct Inspiration\": [\"b23\"],\n  \"Indirect Inspiration\": [\"b11\", \"b27\"],\n  \"Other Inspiration\": [\"b21\", \"b35\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper proposes a new method called XSimGCL for recommendation systems, addressing the inefficiencies of graph augmentations in contrastive learning (CL) models. The primary challenges include the computational expense and complexity of existing CL-based recommendation models, and the need for more effective data augmentation approaches. XSimGCL simplifies the architecture by sharing the forward/backward propagation for recommendation and contrastive tasks, and introduces a noise-based augmentation to enhance the uniformity of representation distribution, thereby improving recommendation accuracy and efficiency.\",\n    \"Direct Inspiration\": {\n        \"b11\": 0.9,\n        \"b23\": 0.95,\n        \"b27\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b24\": 0.75,\n        \"b21\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.65,\n        \"b12\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in contrastive learning (CL) for recommendation systems, specifically focusing on the inefficiency and complexity of existing graph augmentation methods. The authors propose a new method, XSimGCL, which simplifies the architecture by introducing noise-based augmentation to achieve more evenly distributed user/item representations, thus enhancing recommendation performance and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of current graph contrastive learning (CL) methods in recommendation systems, particularly focusing on the computational inefficiencies and the need for better augmentation techniques. It proposes a novel method, XSimGCL, which uses noise-based augmentation instead of graph augmentations to achieve more evenly distributed representations, thereby improving both recommendation accuracy and training efficiency.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.95,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.75,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b16\": 0.6\n  }\n}\n```"], "628c4ce25aee126c0ff59b39": ["```json\n{\n  \"Summary\": \"The paper addresses the under-explored expressive power of spectral Graph Neural Networks (GNNs) and proposes a novel model named JacobiConv. The main challenges revolve around whether nonlinearity is necessary for spectral GNNs and the differences in empirical performance due to different filter bases. The paper proves that linear spectral GNNs are universal under certain conditions and proposes JacobiConv, which uses Jacobi polynomial bases and a Polynomial Coefficient Decomposition technique to improve optimization.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b30\": 0.85,\n    \"b40\": 0.8,\n    \"b9\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b15\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in the expressive power of spectral Graph Neural Networks (GNNs) and proposes a novel method called JacobiConv. It removes nonlinearity from spectral GNNs and explores the expressive power of linear spectral GNNs. The paper also builds a bridge between the expressivity analyses of spectral and spatial GNNs and introduces a Polynomial Coefficient Decomposition (PCD) technique for optimization.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b15\": 0.8,\n    \"b24\": 0.75,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the expressive power of spectral Graph Neural Networks (GNNs) and proposes a novel linear spectral GNN called JacobiConv. The primary challenges include understanding the role of nonlinearity in spectral GNNs and optimizing spectral filter coefficients. The authors explore the universality of linear spectral GNNs and propose methods to enhance their performance, particularly focusing on polynomial filter bases and optimization techniques.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b13\": 0.7,\n    \"b15\": 0.6,\n    \"b40\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the under-researched expressive power of spectral Graph Neural Networks (GNNs), particularly focusing on the necessity of nonlinearity in these models. It investigates whether spectral filters alone suffice for high expressivity and introduces a novel linear spectral GNN model, JacobiConv, which uses Jacobi polynomial bases and a new optimization technique called Polynomial Coefficient Decomposition (PCD). The paper also bridges the gap between spectral and spatial GNN expressivity analyses and explores the optimization properties of spectral GNNs using different filter bases.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.85,\n    \"b24\": 0.8,\n    \"b9\": 0.75,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.6,\n    \"b32\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the lack of systematic explanation for the differences in spectral GNN models and their basis choices. It explores the expressive power of spectral GNNs, particularly focusing on linear spectral GNNs without nonlinearity. The authors introduce a novel spectral GNN, JacobiConv, which uses Jacobi polynomial bases and a new optimization technique called Polynomial Coefficient Decomposition (PCD). The paper presents theoretical analyses and experimental validations to show that JacobiConv outperforms existing models.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.9,\n    \"b32\": 0.8,\n    \"b15\": 0.8,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b13\": 0.6,\n    \"b2\": 0.6\n  }\n}\n```"], "62c28ae45aee126c0f8a182f": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of understanding the effects of the graph's spectrum in GNNs and proposes a novel correlation-free architecture to decouple the correlation issue from the filter design. This allows for the use of more sophisticated filters without performance degradation.\",\n    \"Direct Inspiration\": {\n        \"b28\": 0.9,\n        \"b37\": 0.9,\n        \"b38\": 0.9,\n        \"b18\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b30\": 0.8,\n        \"b48\": 0.8,\n        \"b29\": 0.8,\n        \"b23\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.7,\n        \"b10\": 0.7,\n        \"b26\": 0.7,\n        \"b24\": 0.7,\n        \"b16\": 0.7,\n        \"b46\": 0.7,\n        \"b57\": 0.7,\n        \"b8\": 0.7,\n        \"b1\": 0.7,\n        \"b40\": 0.7,\n        \"b22\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the lack of understanding of the effects of the graph's spectrum in GNNs, highlighting the correlation issue caused by unsmooth spectra. It proposes a correlation-free architecture to decouple the correlation issue from filter design, allowing the exploration of more sophisticated filters without performance degradation.\",\n  \"Direct Inspiration\": {\n    \"b46\": 1,\n    \"b57\": 1,\n    \"b23\": 1,\n    \"b8\": 0.9,\n    \"b1\": 0.9,\n    \"b30\": 0.8,\n    \"b48\": 0.8,\n    \"b29\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.7,\n    \"b15\": 0.7,\n    \"b10\": 0.7,\n    \"b18\": 0.6,\n    \"b37\": 0.6,\n    \"b38\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b22\": 0.5,\n    \"b24\": 0.5,\n    \"b26\": 0.5,\n    \"b16\": 0.5,\n    \"b32\": 0.4,\n    \"b36\": 0.4,\n    \"b12\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the correlation issue in Graph Neural Networks (GNNs) tied to the graph's spectrum. It proposes a correlation-free architecture to decouple the correlation issue from filter design, enabling the use of more sophisticated filters without performance degradation. The paper also introduces new graph matrix representations to leverage more bases and learnable filter coefficients for better handling complex signal patterns.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b30\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b23\": 0.7,\n    \"b46\": 0.7,\n    \"b57\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b24\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies the correlation issue in general GNN architectures, which is caused by an unsmooth spectrum of the underlying graph, and proposes a correlation-free architecture to decouple this issue from graph convolution. This approach allows the use of more sophisticated filters without performance degradation due to correlation problems.\",\n  \"Direct Inspiration\": {\n    \"b46\": 1.0,\n    \"b23\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b10\": 0.8,\n    \"b48\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b24\": 0.6,\n    \"b16\": 0.6,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper investigates the relationship between the graph's spectrum and the performance of graph neural networks (GNNs), highlighting challenges such as oversmoothing and signal correlation. It proposes a novel correlation-free architecture that decouples filter design from correlation issues, enabling the use of more sophisticated polynomial filters. The paper introduces new graph matrix representations to better approximate complex signal patterns, significantly improving performance in learning graph representations.\",\n  \"Direct Inspiration\": [\"b28\", \"b29\", \"b30\", \"b23\", \"b8\", \"b10\"],\n  \"Indirect Inspiration\": [\"b15\", \"b24\", \"b22\"],\n  \"Other Inspiration\": [\"b1\", \"b46\", \"b57\"]\n}\n```"], "62d0db155aee126c0f9f111a": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of parameter inefficiency and weak generalizability in neural text retrieval. It proposes leveraging parameter-efficient (PE) learning methods, such as prompt tuning, adapters, and hybrid methods, to improve both parameter efficiency and generalizability. The paper demonstrates that PE prompt tuning can achieve comparable performance to full-parameter fine-tuning in in-domain settings and significantly better performance in cross-domain and cross-topic benchmarks. Additionally, the paper provides an understanding of the generalization advantages of PE learning, such as better confidence calibration and query-length robustness, and introduces a new dataset, OAG-QA, for testing cross-topic generalizability.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b27\": 1,\n    \"b52\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b40\": 0.8,\n    \"b22\": 0.7,\n    \"b16\": 0.7,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.9,\n    \"b10\": 0.6,\n    \"b24\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include parameter inefficiency and weak generalizability of neural text retrievers. The proposed solution leverages parameter-efficient (PE) learning methods, such as prompt tuning, adapters, and hybrid methods, to achieve comparable performance to fine-tuning while using fewer parameters. The paper systematically examines PE methods in various settings and demonstrates their effectiveness, particularly in cross-domain and cross-topic generalization. The paper constructs a new dataset, OAG-QA, to test the cross-topic generalizability of neural text retrievers.\",\n  \"Direct Inspiration\": [\"b23\", \"b27\"],\n  \"Indirect Inspiration\": [\"b15\", \"b40\", \"b52\"],\n  \"Other Inspiration\": [\"b19\", \"b10\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include parameter inefficiency and weak generalizability of neural text retrievers. The authors propose leveraging parameter-efficient (PE) learning to address these issues, specifically through methods like prompt tuning, adapters, and hybrid methods. They aim to achieve comparable performance to fine-tuning while using fewer parameters and improving cross-domain and cross-topic generalization.\",\n    \"Direct Inspiration\": {\n        \"b23\": 1,\n        \"b27\": 1,\n        \"b40\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.8,\n        \"b52\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.6,\n        \"b10\": 0.6,\n        \"b24\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of parameter inefficiency and weak generalizability in neural text retrieval methods. It proposes leveraging parameter-efficient (PE) learning methods, such as prompt tuning and adapters, to improve generalization and reduce the number of parameters needed. The study systematically examines mainstream PE methods and demonstrates their effectiveness, particularly highlighting P-Tuning v2 for its significant gains in out-of-domain performance. The paper also introduces a new cross-topic retrieval dataset, OAG-QA, to test these methods.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b27\": 1.0,\n    \"b40\": 1.0,\n    \"b52\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b15\": 0.8,\n    \"b19\": 0.9,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b9\": 0.7,\n    \"b22\": 0.6,\n    \"b51\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses two primary challenges in neural text retrieval: parameter inefficiency and weak generalizability. The authors propose leveraging parameter-efficient (PE) learning methods, such as prompt tuning and adapters, to tackle these issues. The study systematically evaluates these PE methods across in-domain, cross-domain, and cross-topic settings, demonstrating that PE methods can achieve competitive performance with much fewer tuning parameters. The paper also introduces a new dataset, OAG-QA, to test cross-topic generalizability.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b27\": 1.0,\n    \"b40\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.8,\n    \"b52\": 0.8\n  }\n}\n```"], "628749345aee126c0ffeb827": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying human values in written arguments, a complex task due to the implicit nature of these values and their vague definitions. The authors propose a multilevel taxonomy of 54 human values, create a dataset of 5270 arguments annotated for these values, and present initial classification results. The work is inspired by various social science studies on human values and aims to apply these insights to computational linguistics and argument mining.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b32\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.6,\n    \"b16\": 0.6,\n    \"b20\": 0.6,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.5,\n    \"b35\": 0.5,\n    \"b2\": 0.4,\n    \"b19\": 0.4,\n    \"b18\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatically identifying human values in written arguments. It proposes a multilevel taxonomy of human values, creates a dataset of annotated arguments from diverse cultures, and establishes a baseline for value classification. The primary inspiration comes from cross-cultural social science studies and advances in natural language understanding and argument mining.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.7,\n    \"b27\": 0.6,\n    \"b37\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying human values in arguments, which is difficult due to the implicit nature and large number of values. The authors propose a multilevel taxonomy of values, create a dataset of arguments annotated for values, and achieve baseline classification results. The research aims to bridge social science insights on values with computational argument mining.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.7,\n    \"b32\": 0.6,\n    \"b12\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.4,\n    \"b16\": 0.4,\n    \"b20\": 0.4,\n    \"b24\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying human values in arguments computationally, an area previously unexplored in argument mining. It proposes a multilevel taxonomy of 54 human values, presents a dataset of 5270 annotated arguments from diverse cultures, and provides baseline classification results.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1,\n    \"b30\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.9,\n    \"b27\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b35\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying human values behind arguments using computational methods. It proposes a multilevel taxonomy of human values, compiles a diverse dataset, and establishes baseline classification results. Inspired by thorough social science research, it aims to bridge the gap between formal argumentation and natural language processing, leveraging advancements in argument mining and large-scale datasets.\",\n  \"Direct Inspiration\": [\n    \"b33\",\n    \"b32\",\n    \"b30\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b10\",\n    \"b35\"\n  ],\n  \"Other Inspiration\": [\n    \"b27\",\n    \"b37\",\n    \"b1\"\n  ]\n}\n```"], "621635aa91e011b46d7ce129": ["```json\n{\n    \"Summary\": {\n        \"challenges\": \"The primary challenges outlined in the paper include the difficulty in retaining valuable information for clustering when using autoencoders for dimensionality reduction, and the high spatial and/or computational complexity of attributed graph clustering methods. The paper proposes a novel Graph Convolutional Clustering (GCC) model that alternates iteratively between embedding and clustering tasks to address these challenges.\",\n        \"inspirations\": \"The paper is inspired by recent advances in Graph Convolutional Networks (GCNs) and joint representation learning and clustering methods.\"\n    },\n    \"Direct Inspiration\": {\n        \"b4\": 1,\n        \"b41\": 0.9,\n        \"b47\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.8,\n        \"b32\": 0.75,\n        \"b42\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.7,\n        \"b21\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of clustering in attributed graphs by proposing a novel Graph Convolutional Clustering (GCC) model that simultaneously performs graph representation learning and clustering. The key contributions include a variant of the GCN propagation matrix, a new formulation combining graph convolutional representation learning and clustering processes, and an efficient algorithm for GCC.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b47\": 0.9,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b22\": 0.7,\n    \"b41\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of clustering in the context of attributed graphs by proposing a novel Graph Convolutional Clustering model (GCC). The main contributions include a variant of the GCN propagation matrix, a new formulation for joint graph convolutional representation learning and clustering, and an efficient algorithm for GCC.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b12\": 1,\n    \"b47\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.7,\n    \"b9\": 0.85,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b8\": 0.7,\n    \"b43\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of simultaneous graph embedding and clustering in attributed graphs, proposing a novel Graph Convolutional Clustering (GCC) model. It aims to overcome limitations in existing methods by alternating iteratively between embedding and clustering tasks, leveraging a new variant of the Graph Convolutional Network (GCN) propagation matrix.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b12\": 0.8,\n    \"b47\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.75,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.65,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in clustering attributed graphs by simultaneously performing graph embedding and clustering using a novel Graph Convolutional Clustering (GCC) model. The main contributions include a variant of the GCN propagation matrix, a new formulation combining graph convolutional representation learning and clustering, and an efficient algorithm (GCC 2). The approach aims to overcome high spatial and computational complexity in existing methods and improve clustering performance by alternating between embedding and clustering tasks.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b12\": 0.8,\n    \"b47\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b22\": 0.7,\n    \"b41\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b32\": 0.5,\n    \"b43\": 0.5\n  }\n}\n```"], "62feff8d90e50fcafd599888": ["```json\n{\n  \"Summary\": \"The paper addresses the exposure bias problem in GNN-based recommender systems. It proposes a novel method called Navip, which applies inverse propensity scoring (IPS) to the neighbor aggregation process in GNNs to reduce bias and improve recommendation accuracy. The method emphasizes less popular neighbors to balance the biased local structure of each target node.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b4\": 0.7,\n    \"b19\": 0.6,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of exposure bias in user-item graphs used in recommender systems. It proposes a novel method called Navip, which applies inverse propensity scoring (IPS) to the neighbor aggregation process in Graph Neural Networks (GNNs) to mitigate the bias. Navip aims to upweight interactions with low propensity and downweight those with high propensity during aggregation, balancing the biased local structure of each target node.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b16\": 0.95,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b18\": 0.8,\n    \"b19\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of exposure bias in user-item graphs used by Graph Neural Networks (GNNs) for recommender systems. The proposed method, Navip, applies inverse propensity scoring (IPS) during the neighbor aggregation process to mitigate this bias, emphasizing less popular items to obtain a more accurate representation of user preferences.\",\n  \"Direct Inspiration\": [\"b2\", \"b16\"],\n  \"Indirect Inspiration\": [\"b4\", \"b5\", \"b19\"],\n  \"Other Inspiration\": [\"b3\", \"b18\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of exposure bias in user-item graphs within recommender systems that use Graph Neural Networks (GNNs). It proposes a novel method called Navip, which applies Inverse Propensity Scoring (IPS) to the neighbor aggregation process to reduce the impact of biased interactions. This method emphasizes less popular neighbors to balance the biased local structure of each target node, thereby improving the accuracy of recommendations.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1,\n        \"b16\": 1,\n        \"b18\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b19\": 0.8,\n        \"b20\": 0.8,\n        \"b4\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.7,\n        \"b24\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of exposure bias in GNN-based recommender systems, which stems from the system's policy-driven user-item interactions. The proposed solution, Navip, mitigates this bias by incorporating inverse propensity scoring in the neighbor aggregation process, thereby emphasizing less popular neighbors to better reflect true user interests.\",\n  \"Direct Inspiration\": [\"b2\", \"b16\", \"b18\"],\n  \"Indirect Inspiration\": [\"b19\", \"b20\", \"b22\", \"b24\"],\n  \"Other Inspiration\": [\"b4\", \"b3\"]\n}\n```"], "62cce6795aee126c0f2a7fcc": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of tensorized program optimization in machine learning, particularly focusing on creating an abstraction called TensorIR that effectively separates tensorized computation from loop transformations. The goal is to automate optimization processes that currently require significant domain expert intervention.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b19\": 0.6,\n    \"b2\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing tensorized programs for machine learning models. It introduces TensorIR, an abstraction for automatic tensor program optimization that isolates tensorized computation from loop transformations. The proposed approach aims to make use of specialized tensor instructions and hardware constraints effectively, overcoming the complexities of manual optimizations by domain experts.\",\n  \"Direct Inspiration\": [\n    \"b8\",\n    \"b33\",\n    \"b4\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b15\",\n    \"b16\",\n    \"b43\"\n  ],\n  \"Other Inspiration\": [\n    \"b19\",\n    \"b10\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the need for an abstraction that can represent tensorized computations and loop transformations, and the complexity of optimizing tensorized programs for modern hardware. The authors propose TensorIR, a new abstraction for automatic tensor program optimization, and introduce a novel scheduling algorithm to handle tensorized computations effectively.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b33\": 1.0,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b19\": 0.7,\n    \"b2\": 0.7,\n    \"b10\": 0.7,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.5,\n    \"b39\": 0.4,\n    \"b43\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of optimizing tensorized programs for machine learning by introducing a new abstraction called TensorIR. It aims to automate the process of optimizing tensorized computations using a compilation approach, overcoming the limitations of manual optimizations and large design spaces. The key contributions include the TensorIR abstraction, a novel automatic scheduling algorithm, and an end-to-end compilation framework.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b33\": 1.0,\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b43\": 0.7,\n    \"b46\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the abstraction for tensorized programs and the large design space of possible tensorized program optimizations. The paper introduces TensorIR, an abstraction for automatic tensor program optimization, and a novel automatic scheduling algorithm to address these challenges.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b2\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```"], "628749495aee126c0fff087a": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of complex multi-hop question answering (QA) by proposing a novel model, PATHFID. PATHFID extends the existing Fusion-in-Decoder (FID) approach to explicitly model the reasoning path required for multi-hop QA, thus improving interpretability and enabling better performance on tasks that require sequential reasoning across multiple documents.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.95,\n    \"b40\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b22\": 0.7,\n    \"b10\": 0.7,\n    \"b23\": 0.8,\n    \"b44\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the complexity of multi-hop question answering (QA), which requires combining multiple pieces of evidence from various documents. The proposed solution, PATHFID, is a generative model that extends the Fusion-in-Decoder (FID) approach to explicitly model the reasoning path for multi-hop QA. This model aims to improve interpretability and performance by jointly generating the reasoning path and the answer.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b40\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.75,\n    \"b44\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.7,\n    \"b42\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of complex multi-hop question answering (QA) which requires aggregating evidence from multiple documents to answer a query. The primary contribution is the PATHFID model, which extends the Fusion-in-Decoder (FID) model by explicitly modeling the reasoning path, thereby improving multi-hop QA performance and interpretability.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b40\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b44\": 0.8,\n    \"b35\": 0.7,\n    \"b42\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b22\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multi-hop QA, where the system must combine evidence from multiple documents. Inspired by the limitations of the FID model in multi-hop contexts, the authors propose PATHFID, a generative QA model that generates answers along with a reasoning path. PATHFID aims to improve multi-hop reasoning by explicitly modeling the reasoning path, thereby making the answer generation process more interpretable.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.7,\n    \"b44\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b22\": 0.6,\n    \"b10\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multi-hop question answering (QA), where a QA system must combine multiple pieces of evidence from different documents to generate an answer. The proposed model, PATHFID, extends the Fusion-in-Decoder (FID) model to improve multi-hop reasoning by explicitly modeling the reasoning path and allowing for cross-passage interactions.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b40\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.9,\n    \"b44\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.8,\n    \"b38\": 0.8,\n    \"b42\": 0.8\n  }\n}\n```"], "62bbc3865aee126c0fa68605": ["```json\n{\n    \"Summary\": \"The paper addresses challenges in Graph Neural Networks (GNNs), specifically issues related to hierarchical pooling methods that cause information loss and local structure damage. The proposed method, SEP, inspired by structural entropy, aims to provide a globally optimized hierarchical pooling approach to minimize information loss and retain local graph structures. The effectiveness of SEP is validated through experiments on graph and node classification tasks, showing superior performance compared to state-of-the-art methods.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8,\n        \"b2\": 0.8,\n        \"b8\": 0.8,\n        \"b12\": 0.8,\n        \"b16\": 0.8,\n        \"b20\": 0.8,\n        \"b30\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.6,\n        \"b24\": 0.6,\n        \"b29\": 0.6,\n        \"b32\": 0.6,\n        \"b34\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two key issues in hierarchical pooling for graph neural networks (GNNs): local structure damage and suboptimal clustering due to fixed compression quotas. The authors propose a novel hierarchical pooling method called SEP, inspired by structural entropy, to minimize these issues. SEP provides globally optimized cluster assignments without relying on a fixed compression quota, thus preserving the local structure of graphs. The method is validated through extensive experiments on graph classification and node classification tasks, demonstrating superior performance compared to state-of-the-art methods.\",\n  \"Direct Inspiration\": [\"b13\"],\n  \"Indirect Inspiration\": [\"b16\"],\n  \"Other Inspiration\": [\"b8\", \"b12\", \"b2\", \"b20\", \"b30\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in previous hierarchical pooling works for GNNs: local structure damage and suboptimal clustering due to fixed compression quotas. It proposes a novel hierarchical pooling method, SEP, inspired by structural entropy, which is globally optimized and does not rely on a fixed layer-specific compression quota. SEP aims to retain local structures and avoid suboptimal clustering across layers.\",\n  \"Direct Inspiration\": [\"b13\"],\n  \"Indirect Inspiration\": [\"b12\", \"b2\"],\n  \"Other Inspiration\": [\"b8\", \"b20\", \"b16\"]\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is the loss of local structure and suboptimal results in hierarchical pooling methods used in Graph Neural Networks (GNNs). The authors propose a novel hierarchical pooling method, SEP, inspired by structural entropy, to address these issues and improve graph classification and node classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b12\": 0.7,\n    \"b2\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b1\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in hierarchical pooling for graph neural networks (GNNs): (1) local structure damage caused by node compression quotas, and (2) suboptimal pooling due to layer-specific design. The authors propose a novel hierarchical pooling method termed SEP, inspired by structural entropy minimization, to overcome these issues. The method aims to retain local graph structures and provides globally optimized cluster assignments. The effectiveness of SEP is experimentally validated on graph classification and node classification tasks.\",\n  \"Direct Inspiration\": [\"b13\", \"b16\"],\n  \"Indirect Inspiration\": [\"b8\", \"b12\", \"b20\", \"b2\", \"b30\", \"b1\"],\n  \"Other Inspiration\": [\"b10\", \"b24\", \"b29\", \"b22\", \"b14\"]\n}\n```"], "630359f590e50fcafd88dc06": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of domain shifts and adversarial attacks on DNNs. It proposes a Structural Causal Model (DICM) and a causal inference pipeline (DICE) to improve robustness and generalization by focusing on causal factors. The main challenges are constructing a causal graph and efficiently inferring causal relations. The authors introduce DICM to split latent factors into causative and spurious parts and use DICE to remove spurious bias by causal intervention.\",\n    \"Direct Inspiration\": {\n        \"b20\": 0.9,\n        \"b24\": 0.95,\n        \"b29\": 0.85,\n        \"b46\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.8,\n        \"b23\": 0.8,\n        \"b30\": 0.8,\n        \"b40\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.75,\n        \"b26\": 0.75,\n        \"b27\": 0.75,\n        \"b45\": 0.75,\n        \"b32\": 0.7,\n        \"b37\": 0.7,\n        \"b58\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are: (i) constructing a causal graph to describe causal relationships between latent factors and observed variables in the context of attack and defense, and (ii) efficiently inferring unobserved causal relations from observed variables. The algorithm proposed, Domain-attack Invariant Causal Learning (DICE), aims to remove spurious bias through causal intervention using a backdoor adjustment method. The inspiration comes from adversarial training and causal reasoning principles.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b29\": 1.0,\n    \"b23\": 0.9,\n    \"b40\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b30\": 0.8,\n    \"b20\": 0.7,\n    \"b46\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b42\": 0.6,\n    \"b27\": 0.5,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the challenge of domain shifts in DNNs and their vulnerability to adversarial and delusive attacks. The authors propose a novel approach, Domain-attack Invariant Causal Model (DICM) and Domain-attack Invariant Causal Learning (DICE), to improve robustness by focusing on causal factors.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b29\": 1,\n    \"b30\": 1,\n    \"b40\": 1,\n    \"b46\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.9,\n    \"b23\": 0.9,\n    \"b45\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b26\": 0.7,\n    \"b27\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of domain shifts impacting the performance of Deep Neural Networks (DNNs) and proposes a novel algorithm called Domain-attack Invariant Causal Learning (DICE). The main challenges identified are constructing a causal graph to describe the relationships between latent factors and observed variables in the context of attack and defense, and efficiently inferring unobserved causal relations from observed variables. The proposed method uses a Structural Causal Model (SCM) and the backdoor adjustment method to mitigate spurious bias through causal intervention.\",\n    \"Direct Inspiration\": {\n        \"b23\": 1.0,\n        \"b40\": 0.9,\n        \"b29\": 0.8,\n        \"b11\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b24\": 0.7,\n        \"b30\": 0.65,\n        \"b20\": 0.6,\n        \"b46\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.5,\n        \"b42\": 0.5,\n        \"b27\": 0.45,\n        \"b45\": 0.45,\n        \"b4\": 0.4,\n        \"b26\": 0.4\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The main challenges addressed in this paper are: (i) constructing a causal graph to describe causal relationships between latent factors and observed variables in the context of attack and defense, and (ii) inferring unobserved causal relations efficiently. The paper proposes the Domain-attack Invariant Causal Model (DICM) and the Domain-attack Invariant Causal Learning (DICE) algorithm to address these challenges.\",\n    \"Direct Inspiration\": {\n        \"b29\": 1.0,\n        \"b11\": 1.0,\n        \"b23\": 1.0,\n        \"b40\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b24\": 0.9,\n        \"b30\": 0.8,\n        \"b12\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.7,\n        \"b46\": 0.7\n    }\n}\n```"], "628748bc5aee126c0ffc3ecd": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of cross-lingual named entity recognition (NER) for zero-resource languages, where labeled training data is scarce. The authors propose a novel multiple-task and multiple-teacher model (MTMT) that utilizes knowledge distillation and multitask learning to leverage entity similarity between tokens from different languages, thereby improving NER performance.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1.0,\n        \"b3\": 0.9,\n        \"b20\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.7,\n        \"b2\": 0.7,\n        \"b13\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b17\": 0.6,\n        \"b1\": 0.6,\n        \"b8\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The main challenges addressed in the paper are the difficulty of performing named entity recognition (NER) in zero-resource languages due to the lack of labeled training data and the need for effective cross-lingual transfer of NER models. The proposed solution is a multi-task and multi-teacher model (MTMT) that incorporates entity similarity evaluation as an auxiliary task to boost the performance of the student recognizer on target languages using a knowledge distillation framework.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.9,\n        \"b3\": 0.8,\n        \"b20\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.7,\n        \"b2\": 0.7,\n        \"b13\": 0.7,\n        \"b5\": 0.6,\n        \"b17\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.5,\n        \"b9\": 0.5,\n        \"b21\": 0.5,\n        \"b18\": 0.5,\n        \"b1\": 0.5,\n        \"b8\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of cross-lingual named entity recognition (NER), particularly for zero-resource languages. It proposes a multiple-task and multiple-teacher model (MTMT) that leverages entity similarity evaluation and knowledge distillation to improve the performance of NER on target languages. The model incorporates multitask learning by using a similarity evaluation task as an auxiliary task to the entity recognition classifier.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b7\": 0.65,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b5\": 0.55,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Named Entity Recognition (NER) in zero-resource languages by utilizing cross-lingual approaches. It introduces a novel multiple-task and multiple-teacher model (MTMT) which leverages entity similarity evaluation alongside entity recognition to improve performance. The model is built upon knowledge distillation and multitask learning techniques.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b3\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b20\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b16\": 0.65,\n    \"b18\": 0.6,\n    \"b19\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of cross-lingual named entity recognition (NER) where labeled data is only available in a source language, and not in the target languages. The authors propose a novel multiple-task and multiple-teacher (MTMT) model that leverages similarity between tokens across languages to improve NER performance. The model incorporates knowledge distillation and multitask learning, where entity recognition and similarity evaluation are conducted simultaneously.\",\n  \"Direct Inspiration\": [\"b6\"],\n  \"Indirect Inspiration\": [\"b3\", \"b20\"],\n  \"Other Inspiration\": [\"b1\", \"b2\", \"b5\", \"b8\", \"b13\"]\n}\n```"], "62bab8f95aee126c0f6afb82": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving collaborative filtering (CF) methods by focusing on the alignment and uniformity of user and item representations. The authors propose a novel learning objective, DirectAU, which directly optimizes these two properties to enhance recommendation performance. The theoretical and empirical analyses demonstrate that achieving better alignment and uniformity contributes to higher recommendation accuracy.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b26\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.85,\n    \"b33\": 0.8,\n    \"b6\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b17\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in collaborative filtering (CF) for recommender systems, specifically focusing on the properties of alignment and uniformity in user and item representations. Inspired by recent advances in contrastive representation learning, the authors propose a novel learning objective named DirectAU, which directly optimizes these two properties to improve recommendation performance.\",\n    \"Direct Inspiration\": [\"b4\", \"b26\"],\n    \"Indirect Inspiration\": [\"b18\", \"b6\"],\n    \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are improving the alignment and uniformity of user and item representations in collaborative filtering (CF) systems. The paper proposes a new learning objective called DirectAU that directly optimizes these two properties to enhance recommendation performance. The authors are inspired by recent progress in contrastive representation learning and theoretical analyses of the Bayesian personalized ranking (BPR) loss.\",\n  \"Direct Inspiration\": [\"b4\", \"b26\"],\n  \"Indirect Inspiration\": [\"b18\", \"b6\", \"b1\"],\n  \"Other Inspiration\": [\"b33\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in collaborative filtering (CF) by focusing on the alignment and uniformity of user and item representations. Inspired by recent progress in contrastive representation learning, the authors propose a new learning objective, DirectAU, to directly optimize these properties. Theoretical and empirical analyses demonstrate that the proposed method significantly improves recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9,\n    \"b6\": 0.8,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b17\": 0.5,\n    \"b1\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in collaborative filtering (CF) by theoretically analyzing and empirically validating the alignment and uniformity properties of user and item representations. It proposes a novel learning objective called DirectAU to directly optimize these properties, showing significant improvements in recommendation performance.\",\n  \"Direct Inspiration\": [\"b4\", \"b26\"],\n  \"Indirect Inspiration\": [\"b18\", \"b6\"],\n  \"Other Inspiration\": [\"b15\", \"b33\", \"b17\"]\n}\n```"], "628749125aee126c0ffe0f2d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating accurate and coherent answers in Open-domain question answering (ODQA) using generative models. The primary inspiration for the proposed model, FiD-PGN, comes from the need to mitigate the issue of hallucinated text produced by generative models. The authors enhance the generative model with a pointer network to enable the model to copy text from retrieved passages while retaining the ability to generate new words. The proposed model achieves better accuracy on standard benchmarks with less data.\",\n  \"Direct Inspiration\": [\"b22\", \"b23\"],\n  \"Indirect Inspiration\": [\"b7\", \"b8\", \"b10\", \"b19\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the issue of generative models producing hallucinated text that is factually inaccurate or inconsistent with the input. The proposed solution is to enhance the generative model with a pointer network, allowing it to copy text from retrieved passages while retaining the ability to generate new words. This approach aims to improve the faithfulness of the generated answers.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b6\": 0.7,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b8\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper focuses on Open-domain question answering (ODQA) and addresses the challenge of generative models producing hallucinated text. The authors propose a fusion-in-decoder pointer-generator network (FiD-PGN) model to enhance the generative model FiD by incorporating a pointer network to improve answer faithfulness.\",\n    \"Direct Inspiration\": [\"b22\", \"b23\"],\n    \"Indirect Inspiration\": [\"b7\", \"b8\", \"b1\", \"b10\", \"b19\"],\n    \"Other Inspiration\": [\"b6\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the problem of factual inaccuracy or inconsistency in generative models for Open-Domain Question Answering (ODQA). The authors propose a novel model, FiD-PGN, which integrates a pointer network into the FiD reader to enable direct copying of text from retrieved passages while retaining the ability to generate new words when necessary. This approach aims to enhance the faithfulness and accuracy of generated answers.\",\n    \"Direct Inspiration\": [\"b22\", \"b23\"],\n    \"Indirect Inspiration\": [\"b7\", \"b8\", \"b10\", \"b19\", \"b1\"],\n    \"Other Inspiration\": [\"b9\", \"b6\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of factual inaccuracies in generative models for open-domain question answering (ODQA). The authors propose enhancing a generative model with a pointer-generator network to directly copy text from retrieved passages while retaining the ability to generate new words. This approach is built upon the FiD model and aims to improve accuracy and efficiency on standard ODQA benchmarks.\",\n  \"Direct Inspiration\": [\"b22\", \"b23\"],\n  \"Indirect Inspiration\": [\"b7\", \"b10\", \"b19\"],\n  \"Other Inspiration\": [\"b1\", \"b8\"]\n}\n```"], "62cce67a5aee126c0f2a86f3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of author name disambiguation in digital libraries due to the high number of authors sharing the same names. It proposes a novel approach using semantic and symbolic representations of titles, sources, and co-authors, leveraging Char2Vec and BERT models to capture the necessary representations. The method is applied to a challenging dataset obtained from DBLP.\",\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [\n    \"b1\",\n    \"b2\",\n    \"b33\",\n    \"b31\"\n  ],\n  \"Other Inspiration\": [\n    \"b20\",\n    \"b17\",\n    \"b34\",\n    \"b29\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of author name disambiguation in digital libraries, where many authors share the same or similar names. It proposes a novel approach leveraging semantic and symbolic representations of titles, sources, and co-authors using models like Char2Vec and BERT. The method aims to improve accuracy without exhaustive manual corrections by associating author names with their respective fields of research and co-authors.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b1\", \"b2\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b6\", \"b9\", \"b33\", \"b31\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b3\", \"b30\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the disambiguation of author names in bibliographic records, especially when authors share the same names or when names are substituted by initials. The proposed model leverages co-authors, titles, and sources of publications, using Char2Vec for author names and BERT for titles and sources, to improve the accuracy of linking author names to their real-world counterparts.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b17\": 0.7,\n    \"b33\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of author name disambiguation in digital libraries, especially when dealing with high numbers of homonymous authors and incomplete metadata. It proposes a novel approach that leverages semantic and symbolic representations of titles, sources, and co-authors using Char2Vec and BERT models.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b26\": 0.7,\n    \"b33\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of author name disambiguation in digital libraries by leveraging bibliographic data such as titles, sources, and co-authors' names. The proposed model employs Char2Vec for co-authors' names and BERT for titles and sources to capture semantic and symbolic representations.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b34\": 0.8,\n    \"b33\": 0.8,\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7,\n    \"b35\": 0.7,\n    \"b25\": 0.7,\n    \"b17\": 0.7\n  }\n}\n```"], "62f3220a90e50fcafd115bd6": ["```json\n{\n    \"Summary\": \"The paper addresses the limitations of existing Named Entity Recognition (NER) methods, particularly for nested NER scenarios. It proposes a novel approach using Convolutional Neural Networks (CNNs) to model spatial correlations between adjacent spans, improving performance on nested NER tasks. The paper also releases a pre-processing script to ensure comparable results across different datasets.\",\n    \"Direct Inspiration\": [\n        \"b3\",\n        \"b26\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b4\",\n        \"b21\",\n        \"b12\",\n        \"b28\"\n    ],\n    \"Other Inspiration\": [\n        \"b11\",\n        \"b27\"\n    ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of nested Named Entity Recognition (NER) by leveraging spatial correlations between spans. The authors propose using a Convolutional Neural Network (CNN) on a 3D feature matrix derived from a Biaffine decoder to model local interactions between spans. This approach is shown to significantly improve performance on three widely used nested NER datasets.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b12\": 0.6,\n    \"b21\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.4,\n    \"b27\": 0.3,\n    \"b28\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of nested Named Entity Recognition (NER) where traditional sequence labeling methods fall short due to token overlap in multiple entities. The authors propose using a Biaffine decoder to create a 3D feature matrix and applying a Convolutional Neural Network (CNN) to model the local interaction between spans, which improves performance on three widely-used nested NER datasets. They also provide a pre-processing script to standardize comparisons across different datasets.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b21\": 0.7,\n    \"b27\": 0.7,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of nested Named Entity Recognition (NER), which traditional sequence labeling methods struggle with due to the overlap of entities. The authors propose a method that uses a Biaffine decoder and Convolutional Neural Network (CNN) to model the spatial correlation between adjacent spans, improving performance on nested NER tasks. Additionally, they provide preprocessing scripts for common datasets to facilitate fair comparisons in future research.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b21\": 0.8,\n    \"b28\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the nested Named Entity Recognition (NER) task, where entities may overlap within the same sentence. The authors propose a method that uses a Biaffine decoder to generate a 3D feature matrix and then applies Convolutional Neural Network (CNN) to model the local interactions between spans. Their approach is designed to leverage the spatial correlations between adjacent spans, which were previously ignored in other methods.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b28\": 0.6,\n    \"b11\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```"], "628748e05aee126c0ffd1130": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficiently incorporating syntactic information into pre-trained language models (LMs), proposing a novel Syntax-guided Contrastive Language Model (SynCLM) that uses syntactic structures for contrastive learning. The approach introduces phrase-guided and tree-guided contrastive objectives to enhance attention learning and word representations without adding computational complexity during downstream tasks.\",\n    \"Direct Inspiration\": {\n        \"b17\": 0.9,\n        \"b39\": 0.9,\n        \"b0\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b20\": 0.7,\n        \"b13\": 0.7,\n        \"b32\": 0.6,\n        \"b26\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.5,\n        \"b19\": 0.5,\n        \"b22\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the inefficiency and complexity of incorporating syntactic information into pre-trained language models (LMs) and the inadequate encoding of syntactic knowledge in existing models like BERT and RoBERTa. The proposed solution, Syntax-guided Contrastive Language Model (SynCLM), leverages contrastive learning to integrate syntactic information during the pre-training stage, using phrase-guided and tree-guided contrastive objectives to enhance syntactic knowledge in LMs without adding computational complexity during downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.9,\n    \"b39\": 0.9,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b19\": 0.7,\n    \"b0\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The challenges outlined include the insufficient encoding of syntactic knowledge in pre-trained transformer-based neural language models and the complexity of adding syntax-driven components. The proposed algorithm, SynCLM, leverages contrastive learning to incorporate syntactic information during the pre-training stage without adding computational complexity during downstream tasks. The key innovations include phrase-guided and tree-guided contrastive learning objectives to enhance attention learning and word representations.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b13\": 0.85,\n    \"b39\": 0.9,\n    \"b17\": 0.85,\n    \"b0\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b19\": 0.75,\n    \"b22\": 0.7,\n    \"b32\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.6,\n    \"b23\": 0.6,\n    \"b44\": 0.6,\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively and efficiently incorporating syntactic information into pre-trained language models (LMs) without increasing computational complexity. It proposes a Syntax-guided Contrastive Language Model (SynCLM) that leverages contrastive learning to incorporate syntactic information during the pre-training stage. SynCLM introduces two novel contrastive learning objectives, phrase-guided and tree-guided, to enhance attention learning and word representations, respectively. The model ensures effective and efficient utilization of syntax without introducing additional parameters or requiring syntax parsing during testing.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.95,\n    \"b13\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.85,\n    \"b39\": 0.80,\n    \"b0\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.70,\n    \"b26\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper tackles the issue of insufficient syntactic knowledge in pre-trained transformer-based neural language models (LMs) such as BERT and RoBERTa. It proposes a novel Syntax-guided Contrastive Language Model (SynCLM) that incorporates syntactic information into the pre-training stage using contrastive learning. The model introduces two new objectives: phrase-guided and tree-guided contrastive objectives, which aim to enhance attention distributions and hidden representations, respectively. The contributions include the introduction of these contrastive objectives without adding computational complexity during downstream tasks and demonstrating consistent performance improvements across various NLP tasks.\",\n    \"Direct Inspiration\": [\"b13\", \"b20\", \"b39\", \"b17\", \"b0\"],\n    \"Indirect Inspiration\": [\"b32\", \"b26\", \"b22\"],\n    \"Other Inspiration\": [\"b5\", \"b19\", \"b31\", \"b28\", \"b23\", \"b44\"]\n}\n```"], "628afb4c5aee126c0f04e3aa": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating learned models into trajectory optimization for reinforcement learning. It proposes a denoising diffusion probabilistic model called Diffuser, which is trained to predict entire trajectories non-autoregressively, ensuring long-horizon accuracy and task compositionality.\",\n  \"Direct Inspiration\": {\n    \"b49\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b55\": 0.9,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.6,\n    \"b8\": 0.6,\n    \"b35\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of model-based reinforcement learning in planning and trajectory optimization. It proposes a novel approach using a diffusion probabilistic model, named Diffuser, which generates trajectories non-autoregressively and incorporates temporal and task compositionality. This model aims to overcome the issues of compounding rollout errors and adversarial example generation in traditional methods.\",\n  \"Direct Inspiration\": {\n    \"b49\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b53\": 0.6,\n    \"b28\": 0.6,\n    \"b57\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in reinforcement learning and data-driven decision-making, specifically the inefficacies of combining learned models with classical trajectory optimization. It proposes a novel approach, Diffuser, a trajectory-level diffusion probabilistic model that improves long-horizon scalability, task compositionality, temporal compositionality, and non-greedy planning.\",\n  \"Direct Inspiration\": {\n    \"b49\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b57\": 0.7,\n    \"b38\": 0.7,\n    \"b4\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of trajectory optimization in reinforcement learning with learned models. Traditional methods often struggle due to compounding errors and adversarial examples. The proposed algorithm, Diffuser, leverages a denoising diffusion probabilistic model to generate trajectories in a way that is more robust and scalable over long horizons. This model allows for flexible conditioning, task compositionality, and improved planning capabilities.\",\n  \"Direct Inspiration\": {\n    \"b49\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b55\": 0.8,\n    \"b8\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b9\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of planning with learned models in reinforcement learning by proposing a new approach using a trajectory-level diffusion probabilistic model called Diffuser. This model aims to integrate the planning and modeling processes closely, making sampling and planning nearly identical and addressing issues like long-horizon scalability, task compositionality, and effective non-greedy planning.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b49\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.7,\n    \"b28\": 0.6,\n    \"b38\": 0.6,\n    \"b53\": 0.6,\n    \"b57\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b29\": 0.5,\n    \"b35\": 0.5,\n    \"b45\": 0.5,\n    \"b55\": 0.5\n  }\n}\n```"], "6344dede90e50fcafd24ceec": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generalizing Transformer models to large graph data. It identifies two main issues: fixed node sampling strategies that do not account for graph properties and neglect of long-range dependencies and global contexts. To tackle these, the authors propose Adaptive Node Sampling for Graph Transformer (ANS-GT), which adapts the Exp4.P method to sample informative nodes, and a hierarchical attention scheme that combines fine-grained local attention with coarse-grained global attention.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0, \n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.9, \n    \"b6\": 0.8, \n    \"b7\": 0.8, \n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7, \n    \"b40\": 0.7, \n    \"b29\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the scalability of Transformer architectures for large graphs and the necessity of incorporating global information while maintaining computational efficiency. The authors propose two main solutions: (1) Adaptive Node Sampling for Graph Transformer (ANS-GT), which uses a multi-armed bandit algorithm to adaptively sample nodes, and (2) a hierarchical attention scheme that combines fine-grained local attention with coarse-grained global attention through graph coarsening.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b25\": 0.9,\n    \"b37\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b8\": 0.7,\n    \"b40\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b10\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include handling the quadratic computational and storage complexity of vanilla Transformers on large graphs, and the need to incorporate both local and global graph information. The proposed methods to address these challenges are Adaptive Node Sampling for Graph Transformer (ANS-GT), which adaptively samples nodes for attention using a modified Exp4.P method, and a hierarchical attention scheme that combines fine-grained local attention with coarse-grained global attention using graph coarsening.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b25\": 0.85,\n    \"b37\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b8\": 0.70,\n    \"b10\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.60,\n    \"b29\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of adapting Transformer architectures for large graphs by proposing two main contributions: Adaptive Node Sampling for Graph Transformer (ANS-GT) and a hierarchical attention scheme. The ANS-GT modifies the Exp4.P method to adaptively sample nodes, while the hierarchical attention scheme incorporates both fine-grained local and coarse-grained global attention to capture long-range dependencies and improve efficiency.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b25\": 0.9,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b40\": 0.6,\n    \"b45\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.4,\n    \"b18\": 0.3\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of generalizing Transformer architecture to large graphs, specifically focusing on adaptive node sampling and hierarchical attention schemes. The proposed methods, Adaptive Node Sampling for Graph Transformer (ANS-GT) and hierarchical attention, aim to improve computational efficiency and capture long-range dependencies in graph data.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b20\": 0.85,\n        \"b37\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.75,\n        \"b40\": 0.7,\n        \"b45\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.6,\n        \"b29\": 0.55\n    }\n}\n```"], "628749415aee126c0ffee49c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in Fine-Grained Entity Typing (FGET), particularly focusing on 'hard' mentions where contextual information alone is insufficient for accurate type prediction. The authors propose a novel heterogeneous graph model that leverages sibling mentions to improve representation learning and type prediction for such hard mentions.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of Fine-Grained Entity Typing (FGET) by focusing on 'hard' mentions that existing models fail to handle effectively. It proposes a novel approach using sibling mentions to improve the representation of these hard mentions. The methodology involves constructing a heterogeneous graph composed of mentions and types, and employing an attentive graph neural network to learn effective representations by aggregating information from sibling mentions and type neighbors.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b7\": 0.7,\n    \"b11\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Fine-Grained Entity Typing (FGET) by proposing a novel approach that leverages sibling mentions to improve representation learning for hard mentions. The authors model FGET as a heterogeneous graph learning problem and employ an attentive graph neural module to enhance mention representations. This methodology aims to handle complex and ambiguous contexts by aggregating supplementary information from sibling mentions.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Fine-Grained Entity Typing (FGET) by focusing on 'hard' mentions with complex or ambiguous contexts. It proposes a novel method using sibling mentions within a heterogeneous graph model to enhance mention representations and improve typing accuracy. This involves constructing a mention-type graph, defining sibling relationships, and employing an attentive graph neural network for representation learning.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the difficulty in learning effective representations for contextualized mentions in Fine-Grained Entity Typing (FGET), particularly for 'hard' mentions with complex or ambiguous contexts. The authors propose using sibling mentions, which share the same or semantically similar types, to improve mention representations. The novel method involves constructing a heterogeneous graph that incorporates sibling relationships and employs an attentive graph neural module for representation learning.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.9,\n    \"b32\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b7\": 0.7,\n    \"b11\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```"], "623004305aee126c0f9b3299": ["```json\n{\n  \"Summary\": \"The paper addresses the scalability and generalization challenges of Graph Neural Networks (GNNs) for semi-supervised learning on large-scale graphs. It introduces the GRAND+ framework, which incorporates efficient approximation techniques and a confidence-aware loss to improve performance and scalability over the existing GRAND model.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b7\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b3\": 0.8,\n    \"b19\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the scalability and generalization challenges in semi-supervised learning on graphs using Graph Neural Networks (GNNs). It proposes the GRAND+ framework, which improves upon the existing GRAND method by introducing efficient approximation techniques for random propagation in a mini-batch manner and a confidence-aware loss for better generalization.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b7\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b8\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the scalability challenges of graph neural networks (GNNs) in semi-supervised learning by introducing the GRAND+ framework. GRAND+ incorporates efficient approximation techniques and a confidence-aware loss to enhance the generalization and scalability of GNNs. Key contributions include a generalized feature propagation matrix, the Generalized Forward Push (GFPush) approximation method, and a confidence-aware loss for regularization.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b7\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b19\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalability and generalization in graph neural networks (GNNs) for semi-supervised learning on large-scale graphs. The proposed solution, GRAND+, introduces efficient approximation techniques and a confidence-aware loss to improve scalability and generalization over the existing GRAND framework.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b7\": 0.8,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.6,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.4,\n    \"b19\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the scalability challenges of graph neural networks (GNNs) for semi-supervised learning on large-scale graphs. It proposes the GRAND+ framework, which introduces efficient approximation techniques for random propagation and a confidence-aware loss to improve generalization and scalability.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b7\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b13\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b20\": 0.7\n  }\n}\n```"], "629587485aee126c0fe15059": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating expressive emotional talking face animations from a single image and audio input, a task that is complex due to the need for accurate facial deformation and emotion extraction. The proposed solution, Emotion-Aware Motion Model (EAMM), consists of an Audio2Facial-Dynamics module and an Implicit Emotion Displacement Learner, which work together to generate realistic emotional face animations by leveraging self-learned key-points and local affine transformations.\",\n  \"Direct Inspiration\": {\n    \"b44\": 0.9,\n    \"b65\": 0.85,\n    \"b63\": 0.87\n  },\n  \"Indirect Inspiration\": {\n    \"b51\": 0.8,\n    \"b24\": 0.75,\n    \"b43\": 0.78\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b53\": 0.73\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating expressive emotional talking face animations in a one-shot setting, overcoming limitations of previous methods that either rely on long video recordings or focus solely on mouth shapes without considering emotions. The proposed solution, Emotion-Aware Motion Model (EAMM), integrates dynamic emotion patterns from a source video using unsupervised motion representations and an Implicit Emotion Displacement Learner to achieve realistic emotional animations.\",\n  \"Direct Inspiration\": {\n    \"b44\": 0.9,\n    \"b63\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b51\": 0.75,\n    \"b24\": 0.7,\n    \"b53\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.65,\n    \"b1\": 0.6,\n    \"b41\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper include generating emotional information in audio-driven talking face animation, handling the non-rigid facial structures, and extracting emotion patterns entangled with other factors. The proposed solution, Emotion-Aware Motion Model (EAMM), leverages unsupervised zero-and first-order motion representations and introduces the Audio2Facial-Dynamics (A2FD) module and the Implicit Emotion Displacement Learner to achieve one-shot talking face generation with emotion control.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1,\n    \"b53\": 0.9,\n    \"b63\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b51\": 0.8,\n    \"b65\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is enabling expressive emotional editing in one-shot talking face animation, which involves generating emotional facial expressions using only a single frame of a source image, audio input, and an emotional video source. The proposed algorithm, Emotion-Aware Motion Model (EAMM), introduces a novel approach combining an Audio2Facial-Dynamics (A2FD) module and an Implicit Emotion Displacement Learner, leveraging unsupervised motion representations and emotion-feature conditioned implicit functions to achieve realistic emotional dynamics.\",\n  \"Direct Inspiration\": {\n    \"b44\": 0.9,\n    \"b63\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b55\": 0.7,\n    \"b53\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b51\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating expressive emotional talking face animations from a single image, overcoming limitations of previous methods that either rely on extensive video data or produce limited emotional control. The proposed Emotion-Aware Motion Model (EAMM) combines an Audio2Facial-Dynamics (A2FD) module for neutral expression generation and an Implicit Emotion Displacement Learner for emotional dynamics.\",\n  \"Direct Inspiration\": [\"b44\", \"b53\"],\n  \"Indirect Inspiration\": [\"b63\", \"b10\"],\n  \"Other Inspiration\": [\"b51\", \"b25\", \"b33\"]\n}\n```"], "634d809490e50fcafd4e683f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatically estimating group memberships of students based on leaked speech recorded by close-talk microphones during active learning sessions. The proposed method utilizes cross-spectrum phase analysis (CSP) to detect correlations in leaked speech, followed by graph clustering to identify groupings.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b1\": 0.6,\n    \"b15\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of estimating groupings in a classroom setting where each student is fitted with a close-talk microphone. The proposed method focuses on the 'leaked speech' of surrounding speakers that is mixed into each person's microphone. By using cross-spectrum phase analysis (CSP), the authors aim to detect correlations without depending on volume or tone, and then perform graph clustering to estimate multiple subgroups.\",\n    \"Direct Inspiration\": {\n        \"b14\": 1,\n        \"b18\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.7,\n        \"b10\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.8,\n        \"b8\": 0.5,\n        \"b16\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include managing the workload for teachers in grouping students for interactive learning sessions, and accurately estimating group memberships based on audio recordings from close-talk microphones. The proposed algorithm focuses on using 'leaked speech' from surrounding speakers to determine group memberships by employing cross-spectrum phase analysis (CSP) and graph clustering techniques.\",\n    \"Direct Inspiration\": {\n        \"b14\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b10\": 0.7,\n        \"b18\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.6,\n        \"b5\": 0.6,\n        \"b6\": 0.5,\n        \"b15\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of estimating group membership among students in a classroom using close-talk microphones, focusing on 'leaked speech' to estimate groupings. The proposed method leverages cross-spectrum phase analysis (CSP) for detecting correlations between audio tracks and utilizes graph clustering for grouping estimation.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b18\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b16\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of estimating student groupings in a classroom setting where students use close-talk microphones. The primary contribution is an acoustic-based method utilizing leaked speech and cross-spectrum phase analysis (CSP) to automatically estimate groupings, reducing the workload of teachers.\",\n    \"Direct Inspiration\": {\n        \"b14\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.7,\n        \"b18\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.5,\n        \"b11\": 0.5,\n        \"b12\": 0.5,\n        \"b13\": 0.5\n    }\n}\n```"], "630359ec90e50fcafd88ceba": ["```json\n{\n  \"Summary\": \"The paper primarily addresses the challenge of uncertainty quantification in Graph Convolutional Networks (GCNs), especially in high-stake scenarios like financial fraud detection. The authors propose the JuryGCN problem, a frequentist-based approach leveraging jackknife resampling and influence functions to quantify uncertainty without retraining the GCN.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.9,\n    \"b33\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.8,\n    \"b20\": 0.7,\n    \"b39\": 0.7,\n    \"b48\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of uncertainty quantification in Graph Convolutional Networks (GCNs), a crucial aspect in applications like financial fraud detection and drug discovery. The authors propose a novel frequentist-based method called JuryGCN, which leverages jackknife resampling and influence functions to quantify the uncertainty of GCN predictions without additional training or parameters.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 0.9,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b12\": 0.7,\n    \"b20\": 0.6,\n    \"b39\": 0.6,\n    \"b48\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of uncertainty quantification in Graph Convolutional Networks (GCNs), proposing a novel frequentist-based approach called JuryGCN. This method leverages jackknife resampling and influence functions to provide post-hoc uncertainty quantification without additional parameters or re-training of the GCN.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b27\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b29\": 0.5,\n    \"b39\": 0.5,\n    \"b48\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of uncertainty quantification in Graph Convolutional Networks (GCNs), introducing the JuryGCN algorithm based on frequentist principles using jackknife resampling and influence functions. This approach aims to provide deterministic uncertainty quantification without additional parameters or re-training of the GCN model.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1,\n    \"b33\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.6,\n    \"b48\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b47\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of uncertainty quantification in Graph Convolutional Networks (GCNs), which is crucial for applications like financial fraud detection and node classification. The proposed method, JuryGCN, leverages frequentist-based jackknife resampling and influence functions to quantify GCN uncertainty without the need for additional parameters or re-training.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 1.0,\n    \"b27\": 1.0,\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b39\": 0.8,\n    \"b48\": 0.8,\n    \"b47\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```"], "629435a05aee126c0f2fe317": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the robustness and generalization of spectral Graph Neural Networks (GNNs) across different levels of graph homophily, particularly in the presence of adversarial attacks. The authors propose EvenNet, a novel spectral GNN that discards messages from odd-order neighbors and incorporates balance theory to derive a graph filter with only even-order terms. They also introduce Spectral Regression Loss (SRL) to evaluate graph filters' performance in the spectral domain. Comprehensive experiments validate EvenNet's superiority in generalizing across homophily levels without additional computational complexity.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b8\": 0.85,\n    \"b17\": 0.8,\n    \"b7\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b14\": 0.65,\n    \"b2\": 0.6,\n    \"b4\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.5,\n    \"b21\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the robustness and generalization of Graph Neural Networks (GNNs) across different homophily levels, especially under adversarial attacks. The authors propose EvenNet, a novel spectral GNN that discards messages from odd-order neighbors, inspired by balance theory. They also introduce Spectral Regression Loss (SRL) to evaluate graph filters' performance in the spectral domain. The paper's contributions include theoretical analysis and comprehensive experiments validating EvenNet's effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b7\": 0.85,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b2\": 0.7,\n    \"b14\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b21\": 0.55,\n    \"b36\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the robustness of spectral Graph Neural Networks (GNNs) against adversarial attacks and their ability to generalize across different levels of graph homophily. The proposed novel method is EvenNet, which discards messages from odd-order neighbors and introduces Spectral Regression Loss (SRL) for evaluating graph filters.\",\n  \"Direct Inspiration\": {\n    \"Balance theory\": \"b5\",\n    \"GPRGNN\": \"b8\"\n  },\n  \"Indirect Inspiration\": {\n    \"Spectral GNNs with learnable polynomial filters\": \"b2\",\n    \"ChebNet\": \"b9\",\n    \"BernNet\": \"b14\"\n  },\n  \"Other Inspiration\": {\n    \"Robust GNNs against adversarial attacks\": \"b11\",\n    \"Robust GNNs against adversarial attacks\": \"b15\",\n    \"Robust GNNs against adversarial attacks\": \"b29\",\n    \"Robust GNNs against adversarial attacks\": \"b33\",\n    \"Robust GNNs against adversarial attacks\": \"b34\",\n    \"Homophily metrics\": \"b19\",\n    \"Homophily metrics\": \"b21\",\n    \"Homophily metrics\": \"b36\",\n    \"Adversarial attacks on GNNs\": \"b37\"\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"GNNs' vulnerability to adversarial attacks\",\n      \"Performance of spectral GNNs under structural perturbation\",\n      \"Generalization across graphs of different homophily\"\n    ],\n    \"Inspirations\": [\n      \"Balance theory\",\n      \"Graph homophily and spectral methods\",\n      \"Spectral Regression Loss (SRL)\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b35\": 0.8,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b8\": 0.75,\n    \"b14\": 0.7,\n    \"b36\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b21\": 0.6,\n    \"b11\": 0.6,\n    \"b15\": 0.6,\n    \"b29\": 0.6,\n    \"b33\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving the robustness of spectral Graph Neural Networks (GNNs) against adversarial attacks and their ability to generalize across different levels of graph homophily. The proposed solution, EvenNet, discards messages from odd-order neighbors inspired by balance theory, achieves a more robust performance under varying homophily conditions, and introduces a Spectral Regression Loss (SRL) to evaluate graph filters.\",\n    \"Direct Inspiration\": [\"b5\", \"b8\", \"b17\"],\n    \"Indirect Inspiration\": [\"b7\", \"b36\"],\n    \"Other Inspiration\": [\"b2\", \"b14\", \"b21\"]\n}\n```"], "626754c85aee126c0fbcdd75": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of high computational and memory complexity in feature matching using graph neural networks (GNNs) by proposing a coarse-to-fine cluster-based GNN method. The key contributions include a learnable clustering method to reduce redundancy, and the ClusterGNN architecture which achieves state-of-the-art results while improving efficiency.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.9,\n    \"b36\": 0.8,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b5\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient and accurate feature matching between images, which is crucial for various computer vision applications. The authors propose a novel coarse-to-fine clustering method integrated into a graph neural network (GNN) architecture to reduce computational redundancy and improve matching performance, leading to significant improvements in runtime and memory efficiency while maintaining state-of-the-art accuracy.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.7,\n    \"b5\": 0.7,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient and accurate feature matching between images, which is crucial for various computer vision applications like SLAM, SfM, and camera pose estimation. The authors propose a novel ClusterGNN architecture that uses a coarse-to-fine clustering method to reduce computational and memory complexity while maintaining state-of-the-art accuracy. The method is inspired by the Routing Transformer and aims to overcome the limitations of existing attention-based GNNs, such as SuperGlue, by introducing a sparse attention mechanism within local graphs.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.8,\n    \"b41\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b23\": 0.7,\n    \"b26\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of feature matching in computer vision tasks such as SLAM, SfM, and camera pose estimation. The proposed algorithm, ClusterGNN, introduces a coarse-to-fine clustering method to establish local graphs for feature matching, significantly reducing redundancy and computational complexity. The contributions include a learnable clustering method, the ClusterGNN architecture, and achieving state-of-the-art results with improved efficiency.\",\n  \"Direct Inspiration\": [\"b27\"],\n  \"Indirect Inspiration\": [\"b30\", \"b41\", \"b26\", \"b36\"],\n  \"Other Inspiration\": [\"b5\", \"b8\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient and accurate feature matching between images, which is crucial for applications like SLAM, SfM, and camera pose estimation. The authors propose a coarse-to-fine cluster-based GNN (ClusterGNN) to reduce redundancy and computational complexity in feature matching, inspired by the Routing Transformer. The novel contributions include a learnable clustering method to establish local graphs, an attentional GNN architecture for feature matching, and achieving state-of-the-art results with reduced runtime and memory consumption.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b8\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```"], "634f6ae390e50fcafdcb62af": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are effectively combining text and knowledge graphs (KGs) for pretraining, requiring a deeply bidirectional model and a self-supervised objective to learn joint reasoning over text and KG at scale. The proposed algorithm DRAGON addresses these challenges by using a cross-modal encoder (GreaseLM) and a joint self-supervised objective that unifies masked language modeling and link prediction to facilitate mutual information flow between text and KG, leading to improved performance on diverse downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1,\n    \"b8\": 1,\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b11\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of combining text data and knowledge graphs (KGs) for pretraining a deeply interactive, bidirectional model. It proposes DRAGON, which integrates a cross-modal encoder and a bidirectional self-supervised objective to learn joint reasoning over text and KG. The model is pretrained using masked language modeling and link prediction tasks, demonstrating improved performance on diverse downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.9,\n    \"b8\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b13\": 0.6,\n    \"b46\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.5,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes DRAGON, a Deep Bidirectional Language-Knowledge Graph Pretraining method that combines text data and knowledge graphs (KGs) to improve natural language processing (NLP) tasks. The primary challenges addressed are effectively combining text and KGs and learning joint reasoning over the two modalities. The approach uses a cross-modal encoder and a bidirectional self-supervised objective involving masked language modeling (MLM) and link prediction tasks.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b17\": 0.8,\n    \"b44\": 0.7,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b54\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively combining text data and knowledge graphs (KGs) for pretraining. It introduces the DRAGON approach, which uses a cross-modal encoder to fuse text and KG representations bidirectionally and employs a joint self-supervised objective of masked language modeling and link prediction to enable mutual information flow between text and KG.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b7\", \"b8\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b1\", \"b2\", \"b11\", \"b12\", \"b13\", \"b14\", \"b15\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b3\", \"b6\", \"b17\", \"b45\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of effectively combining text data and knowledge graphs (KGs) to pretrain deeply interactive representations. It introduces DRAGON, a model that performs deeply bidirectional, self-supervised pretraining of a language-knowledge model from text and KG. The core innovation is the use of a cross-modal encoder that bidirectionally fuses text and KG, along with a unified self-supervised objective that combines masked language modeling and KG link prediction.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b8\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b2\": 0.7,\n        \"b6\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.6,\n        \"b12\": 0.6,\n        \"b15\": 0.6\n    }\n}\n```"], "6369c8cd90e50fcafde87ef2": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot learning with pretrained language models (PLMs) by focusing on the generation of quality training data, rather than proposing new fine-tuning methods. The authors propose a meta-learning-based approach to automatically learn token weights that emphasize label-discriminative tokens, facilitating the generation of clearer training samples for downstream classification tasks.\",\n  \"Direct Inspiration\": [\"b66\"],\n  \"Indirect Inspiration\": [\"b6\", \"b17\", \"b41\", \"b84\", \"b79\", \"b83\"],\n  \"Other Inspiration\": [\"b30\", \"b52\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving few-shot learning with pretrained language models (PLMs) by focusing on the generation of quality training data. Instead of proposing new methods for fine-tuning on few-shot samples, the authors emphasize the importance of generating label-discriminative texts to benefit the final classification task. The proposed methodology involves the use of meta-learning to automatically learn token weights, which enhances the label-discriminativeness of the generated text.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b41\": 0.9,\n    \"b66\": 0.9,\n    \"b84\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.8,\n    \"b65\": 0.8,\n    \"b79\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b60\": 0.7,\n    \"b87\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving few-shot learning with pretrained language models (PLMs) by generating quality training data. It introduces a method to generate label-discriminative texts using a meta-learning approach to automatically learn token weights and fine-tunes classifiers with noise-robust training procedures.\",\n  \"Direct Inspiration\": {\n    \"b66\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b17\": 0.8,\n    \"b41\": 0.7,\n    \"b84\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b30\": 0.6,\n    \"b52\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses few-shot learning with pretrained language models (PLMs) by focusing on generating quality training data. It introduces a method for label-discriminative text generation using meta-learned token weights. The novel contributions include a meta objective for token-wise weighted training and a noise-robust training procedure for classifier fine-tuning.\",\n    \"Direct Inspiration\": {\n        \"b66\": 0.9,\n        \"b17\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b60\": 0.75,\n        \"b84\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b41\": 0.65,\n        \"b79\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot learning with pretrained language models (PLMs) by focusing on generating high-quality training data through a novel meta-learning based approach. The proposed method, FewGen, emphasizes label-discriminative token generation to improve the performance of classification tasks under limited training samples.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b66\": 0.85,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b41\": 0.7,\n    \"b84\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b68\": 0.6\n  }\n}\n```"], "6304456b90e50fcafd12fe4b": ["```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are cross-graph heterogeneity and graph-signal heterogeneity in GNN pre-training. The paper proposes a novel method, MentorGNN, which uses a multi-scale encoder-decoder architecture and a curriculum learning paradigm to address these challenges.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b65\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b31\": 0.7,\n    \"b45\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in pre-training Graph Neural Networks (GNNs), specifically cross-graph heterogeneity and graph-signal heterogeneity. It proposes MentorGNN, an end-to-end framework that includes a multi-scale encoder-decoder architecture and a curriculum learning paradigm to improve knowledge transfer and generalization performance.\",\n  \"Direct Inspiration\": [\"b23\"],\n  \"Indirect Inspiration\": [\"b16\", \"b20\", \"b21\", \"b45\"],\n  \"Other Inspiration\": [\"b32\", \"b54\", \"b60\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are cross-graph heterogeneity and graph-signal heterogeneity, which affect the performance of pre-training Graph Neural Networks (GNNs). The proposed algorithm, MentorGNN, addresses these challenges using a multi-scale encoder-decoder architecture for cross-graph adaptation and a curriculum learning paradigm for graph signal comprehension.\",\n  \"Direct Inspiration\": [\"b0\", \"b16\", \"b20\", \"b21\", \"b31\", \"b45\", \"b65\"],\n  \"Indirect Inspiration\": [\"b22\", \"b23\", \"b32\", \"b33\", \"b54\", \"b60\"],\n  \"Other Inspiration\": [\"b1\", \"b4\", \"b19\", \"b52\", \"b63\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges: Cross-Graph Heterogeneity and Graph-Signal Heterogeneity in the context of pre-training Graph Neural Networks (GNNs). The proposed solution, MentorGNN, includes a multi-scale encoder-decoder architecture for cross-graph adaptation and a curriculum learning paradigm to manage graph signal heterogeneity.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b20\": 0.8,\n    \"b21\": 0.8,\n    \"b31\": 0.8,\n    \"b45\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.7,\n    \"b32\": 0.7,\n    \"b54\": 0.7,\n    \"b60\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b65\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in pre-training Graph Neural Networks (GNNs): cross-graph heterogeneity and graph-signal heterogeneity. It proposes an end-to-end framework called MentorGNN to enable knowledge transfer and improve generalization performance. The framework includes a multi-scale encoder-decoder architecture and a curriculum learning paradigm to tackle these challenges.\",\n  \n  \"Direct Inspiration\": {\n    \"b23\": 0.9\n  },\n\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b21\": 0.8,\n    \"b45\": 0.8,\n    \"b31\": 0.7\n  },\n\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b16\": 0.6\n  }\n}\n```"], "637aec2590e50fcafd929667": ["```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are the high memory and computational costs associated with serving large-scale language models (LLMs) like GPT-3, which contain billions of parameters. The authors propose SmoothQuant, an efficient post-training quantization (PTQ) method that migrates the quantization difficulty from activations to weights, making LLMs quantization-friendly while maintaining accuracy and reducing resource usage.\",\n    \"Direct Inspiration\": [\"b7\", \"b35\"],\n    \"Indirect Inspiration\": [\"b33\", \"b37\"],\n    \"Other Inspiration\": [\"b13\", \"b38\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing the cost and improving the efficiency of serving large-scale language models (LLMs) by developing a novel post-training quantization (PTQ) method called SmoothQuant. SmoothQuant migrates the quantization difficulty from activations to weights using a per-channel scaling transformation, which makes the model quantization-friendly. The proposed method achieves significant memory savings and speedup while maintaining accuracy, and it is compatible with various quantization schemes.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b35\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.9,\n    \"b37\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.8,\n    \"b27\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the efficient quantization of large-scale language models (LLMs) to reduce serving costs while maintaining accuracy. The proposed algorithm, SmoothQuant, addresses this by migrating quantization difficulty from activations to weights using a per-channel scaling transformation, making the model more quantization-friendly. This approach is shown to be hardware-efficient and easy to implement, significantly reducing the memory and computational resources required for large models.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.7,\n    \"b37\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.8,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the efficient and accurate quantization of large-scale language models (LLMs) to reduce the computational and memory costs while maintaining performance. The proposed algorithm, SmoothQuant, introduces a novel post-training quantization solution that migrates quantization difficulty from activations to weights using a per-channel scaling transformation. This approach aims to achieve hardware efficiency and maintain model accuracy even for very large models like OPT-175B, BLOOM-176B, and GLM-130B.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b35\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b37\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b38\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the difficulty of quantizing large-scale language models (LLMs) due to the presence of outliers in activations, which leads to accuracy degradation. The proposed method, SmoothQuant, introduces a novel approach that migrates quantization difficulty from activations to weights using a per-channel smoothing factor. This method is hardware-efficient, maintains model accuracy, and reduces memory usage and latency, making it feasible to serve large LLMs with fewer resources.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b35\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.8,\n    \"b33\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b30\": 0.5\n  }\n}\n```"], "6350bc6d90e50fcafdecf145": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of performing self-supervised learning (SSL) on heterogeneous information networks (HINs) without relying on positive or negative examples, which is a common limitation in existing methods. The proposed solution, SHGP, integrates an attention-based heterogeneous graph neural network (HGNN) module (Att-HGNN) and a structural clustering method (Att-LPA) to generate pseudo-labels that guide the learning process, resulting in more generalizable and flexible embeddings.\",\n  \"Direct Inspiration\": [\"b21\"],\n  \"Indirect Inspiration\": [\"b33\", \"b23\", \"b18\", \"b15\", \"b37\", \"b13\", \"b14\", \"b10\"],\n  \"Other Inspiration\": [\"b29\", \"b7\", \"b41\", \"b25\", \"b20\", \"b19\", \"b43\", \"b36\", \"b35\", \"b30\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of conducting self-supervised learning (SSL) on heterogeneous information networks (HINs) without relying on positive or negative examples, which are typically dataset-specific and limit the generalization ability of existing methods. The proposed algorithm, SHGP, innovatively integrates an attention-based heterogeneous graph neural network (Att-HGNN) and a structural clustering method (Att-LPA) to generate pseudo-labels for learning effective embeddings. This novel approach aims to enhance flexibility and generalization in SSL for HINs.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.9,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7,\n    \"b23\": 0.7,\n    \"b18\": 0.7,\n    \"b15\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b37\": 0.7,\n    \"b10\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the dependency on high-quality supervision labels in graph neural networks (GNNs) and the difficulty in generating effective positive and negative examples for self-supervised learning (SSL) on heterogeneous information networks (HINs). The proposed algorithm, SHGP, introduces a novel SSL approach that avoids the need for positive or negative examples by combining an attention-based HGNN with a label propagation algorithm (LPA) to generate pseudo-labels and improve embeddings.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b41\": 0.6,\n    \"b36\": 0.6,\n    \"b32\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.4,\n    \"b23\": 0.4,\n    \"b18\": 0.4,\n    \"b15\": 0.4,\n    \"b37\": 0.4,\n    \"b13\": 0.4,\n    \"b14\": 0.4,\n    \"b10\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"Challenges\": \"The primary challenge addressed in the paper is the reliance on high-quality supervision labels for semi-supervised graph neural networks (GNNs), which are often expensive or impossible to acquire due to privacy concerns. The paper focuses on heterogeneous information networks (HINs) and proposes a novel self-supervised learning (SSL) approach named SHGP, which does not require positive or negative examples, thus circumventing dataset-specific example generation strategies.\",\n        \"Inspirations\": \"The paper is inspired by existing SSL methods on homogeneous graphs and HINs, the attention-aggregation scheme used in HGNN models, and the structural clustering method LPA. It seeks to improve the generalization ability and flexibility of SSL methods for HINs.\"\n    },\n    \"Direct Inspiration\": {\n        \"b21\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.7,\n        \"b8\": 0.8,\n        \"b19\": 0.7,\n        \"b25\": 0.7,\n        \"b29\": 0.8,\n        \"b36\": 0.8,\n        \"b41\": 0.8,\n        \"b43\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.6,\n        \"b13\": 0.6,\n        \"b15\": 0.6,\n        \"b18\": 0.6,\n        \"b23\": 0.6,\n        \"b33\": 0.6,\n        \"b37\": 0.6,\n        \"b40\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of performing self-supervised learning (SSL) on heterogeneous information networks (HINs) without relying on positive or negative examples. It proposes a novel SSL method called SHGP, which consists of two modules: Att-HGNN and Att-LPA. Att-HGNN is an attention-based HGNN encoder, while Att-LPA uses structural clustering to generate pseudo-labels. The two modules enhance each other to learn better embeddings. This method avoids the laborious process of generating example strategies, improving generalizability and flexibility.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b41\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.6,\n    \"b16\": 0.6,\n    \"b33\": 0.6,\n    \"b23\": 0.6,\n    \"b18\": 0.6,\n    \"b15\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b37\": 0.6,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.5,\n    \"b7\": 0.5,\n    \"b25\": 0.5,\n    \"b20\": 0.5,\n    \"b19\": 0.5,\n    \"b43\": 0.5,\n    \"b31\": 0.5,\n    \"b1\": 0.5,\n    \"b24\": 0.5,\n    \"b32\": 0.5,\n    \"b11\": 0.5,\n    \"b42\": 0.5,\n    \"b38\": 0.5\n  }\n}\n```"], "6346f67490e50fcafd950453": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing performance in disaggregated memory systems with hybrid access mechanisms. The proposed solution, HotBox, aims to maximize local memory hit rate through finer-grain management mechanisms, avoiding inefficiencies of previous approaches such as swap-only access, huge pages, and batch migrations.\",\n  \"Direct Inspiration\": {\n    \"b47\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.9,\n    \"b9\": 0.85,\n    \"b21\": 0.85,\n    \"b23\": 0.85,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.75,\n    \"b14\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include optimizing the performance of disaggregated memory systems with a hybrid access mechanism, reducing memory costs by shifting memory usage into slower and cheaper tiers with minimal impact on application performance, and addressing the inefficiencies of previous approaches such as swapping, huge pages, and batch migration. The paper introduces HotBox, a memory management subsystem for Linux, to tackle these challenges by using a hybrid access mechanism, eliminating hotness fragmentation, and avoiding batching for page migration.\",\n  \"Direct Inspiration\": [\"b6\", \"b9\", \"b21\", \"b23\", \"b41\", \"b47\"],\n  \"Indirect Inspiration\": [\"b25\", \"b27\"],\n  \"Other Inspiration\": [\"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing performance in disaggregated memory systems with a hybrid access mechanism. It highlights the inefficiencies of swap-only approaches, huge pages, and batch migrations, and introduces HotBox, a novel memory management subsystem for the Linux kernel, which uses a hybrid access mechanism, base pages, and on-demand migration to maximize local memory hit rate and reduce overheads.\",\n  \"Direct Inspiration\": {\n    \"b47\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b21\": 0.8,\n    \"b23\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b27\": 0.7,\n    \"b19\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing disaggregated memory systems with a hybrid access mechanism. The proposed solution, HotBox, aims to maximize local memory hit rate while managing the overheads associated with migration and access monitoring. Key design choices include using a hybrid access mechanism, eliminating hotness fragmentation by using base pages, and migrating pages on demand.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b9\": 1,\n    \"b14\": 1,\n    \"b21\": 1,\n    \"b23\": 1,\n    \"b25\": 1,\n    \"b27\": 1,\n    \"b47\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.9,\n    \"b41\": 0.8,\n    \"b42\": 0.8,\n    \"b7\": 0.7,\n    \"b48\": 0.7,\n    \"b32\": 0.6,\n    \"b3\": 0.6,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b36\": 0.5,\n    \"b39\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance challenges in disaggregated memory systems, particularly with high-latency remote memory. The authors propose HotBox, a novel memory management subsystem for Linux, to improve local memory hit rates by using a hybrid access mechanism, avoiding huge pages to prevent hotness fragmentation, and migrating pages on demand rather than in batches.\",\n  \"Direct Inspiration\": {\n    \"b47\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b21\": 0.8,\n    \"b23\": 0.8,\n    \"b41\": 0.8,\n    \"b6\": 0.7,\n    \"b14\": 0.7,\n    \"b19\": 0.7,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.6\n  }\n}\n```"], "6344dedd90e50fcafd24cdcd": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of leveraging contrastive self-supervised learning for hypergraph neural networks (HyperGNNs) in few-shot scenarios. It proposes novel methods for constructing contrastive views of hypergraphs by designing augmented views through both manual and generative approaches. The main contributions include fabricated augmentations based on domain knowledge and a novel variational hypergraph auto-encoder for generative augmentations, which enhance performance and preserve higher-order information.\",\n  \"Direct Inspiration\": [\"b11\", \"b13\"],\n  \"Indirect Inspiration\": [\"b10\", \"b12\", \"b16\", \"b17\"],\n  \"Other Inspiration\": [\"b18\", \"b19\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot learning in hypergraphs where task-specific labels are scarce, by leveraging contrastive self-supervised learning. The main contributions are the exploration of hypergraph view augmentations and the introduction of a variational hypergraph auto-encoder for generative augmentations.\",\n  \"Direct Inspiration\": [\"b11\", \"b13\"],\n  \"Indirect Inspiration\": [\"b41\", \"b42\"],\n  \"Other Inspiration\": [\"b10\", \"b12\", \"b16\", \"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of hypergraph representation learning in few-shot scenarios using contrastive self-supervised learning. Inspired by the success of contrastive learning on images and graphs, the authors propose HyperGCL, which leverages augmented views of hypergraphs to preserve higher-order relations. The paper introduces novel methods of hypergraph augmentations including manual perturbations and a generative model using a variational hypergraph auto-encoder.\",\n  \"Direct Inspiration\": [\"b11\", \"b13\"],\n  \"Indirect Inspiration\": [\"b16\", \"b17\"],\n  \"Other Inspiration\": [\"b10\", \"b20\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the generalizability of hypergraph neural networks (HyperGNNs) in few-shot scenarios through contrastive learning. The core contributions include designing effective hypergraph augmentations for contrastive views and introducing a novel variational hypergraph auto-encoder for generative augmentations.\",\n  \"Direct Inspiration\": [\"b11\", \"b13\"],\n  \"Indirect Inspiration\": [\"b16\", \"b21\", \"b22\"],\n  \"Other Inspiration\": [\"b10\", \"b12\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of applying contrastive self-supervised learning to hypergraphs in few-shot scenarios, focusing on building effective contrastive views for hypergraphs. The authors propose HyperGCL, which includes both manually designed and generative augmentations for hypergraphs to preserve higher-order relations and improve performance.\",\n    \"Direct Inspiration\": [\"b11\", \"b13\"],\n    \"Indirect Inspiration\": [\"b10\", \"b12\", \"b16\", \"b17\"],\n    \"Other Inspiration\": [\"b27\", \"b41\", \"b42\"]\n}\n```"], "633e476490e50fcafde590a1": ["```json\n{\n  \"Summary\": \"The paper addresses the unfaithfulness problem in abstractive summarization, which includes intrinsic errors due to inadequate semantic understanding and extrinsic errors due to overconfidence in language models. The proposed Faithfulness Enhanced Summarization (FES) model tackles these issues through a multi-task learning framework incorporating a QA-based faithfulness evaluation task and a max-margin loss to handle overconfidence.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b8\": 1.0,\n    \"b9\": 1.0,\n    \"b24\": 1.0,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b10\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b33\": 0.6,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the unfaithfulness problem in abstractive summarization by proposing a novel Faithfulness Enhanced Summarization (FES) model. The key challenges include intrinsic errors caused by inadequate semantic understanding and extrinsic errors caused by language model overconfidence. The FES model employs a multi-task learning framework combining summarization and QA tasks to enhance semantic understanding and uses a max-margin loss to mitigate overconfidence.\",\n  \"Direct Inspiration\": [\"b7\", \"b6\", \"b9\"],\n  \"Indirect Inspiration\": [\"b8\", \"b24\"],\n  \"Other Inspiration\": [\"b5\", \"b20\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the unfaithfulness problem in abstractive summarization by proposing a novel Faithfulness Enhanced Summarization model (FES). The primary challenges include intrinsic errors caused by inadequate encoder-decoder mechanisms and extrinsic errors due to overconfidence in language models. The FES model integrates a multi-task learning paradigm involving a QA-based faithfulness evaluation task and introduces a max-margin loss to control the language model's overconfidence.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b20\": 0.9,\n    \"b24\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b8\": 0.75,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.65,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the unfaithfulness problem in abstractive summarization models by proposing a novel Faithfulness Enhanced Summarization (FES) model. The FES model tackles intrinsic errors with a multi-task learning paradigm incorporating QA-based faithfulness evaluation, and extrinsic errors with a max-margin loss to prevent overconfidence of the language model. The paper validates the effectiveness of this approach through extensive experiments on CNN/DM and XSum datasets.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.9,\n        \"b9\": 0.9,\n        \"b31\": 0.8,\n        \"b37\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.7,\n        \"b8\": 0.7,\n        \"b24\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b10\": 0.6,\n        \"b20\": 0.6,\n        \"b39\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the unfaithfulness problem in abstractive summarization by proposing a Faithfulness Enhanced Summarization model (FES). The primary challenges are intrinsic errors caused by inadequate document-level inference and extrinsic errors due to excessive reliance on language models (LM). The proposed FES model employs a multi-task learning framework with a QA-based faithfulness evaluation task, a QA attention-enhanced decoder, and a max-margin loss to mitigate overconfidence of the LM.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b9\": 1.0,\n    \"b36\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.9,\n    \"b8\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.8,\n    \"b20\": 0.8,\n    \"b39\": 0.8\n  }\n}\n```"], "632812a590e50fcafd1a6a98": ["```json\n{\n    \"Summary\": \"The paper addresses challenges in small molecule drug discovery, particularly the need for efficient and accurate prediction of molecular properties. The proposed CoAtGIN model integrates k-hop convolution, virtual nodes, and linear transformers to enhance message passing and embedding representation for molecule graphs.\",\n    \"Direct Inspiration\": {\n        \"b16\": 1,\n        \"b18\": 0.9,\n        \"b21\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.7,\n        \"b12\": 0.7,\n        \"b13\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in small molecule drug discovery, specifically the need to efficiently predict molecular properties. Inspired by limitations of string-based methods and existing graph neural networks (GNNs), the authors propose a novel graph-based architecture, CoAtGIN. Key contributions include the introduction of k-hop convolution for faster message aggregation and a new global message passing mechanism using linear transformers.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b18\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b15\": 0.6,\n    \"b20\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of small molecule drug discovery, particularly the difficulty of predicting molecular properties efficiently and accurately. It introduces a novel graph-based architecture, CoAtGIN, which incorporates k-hop convolution, virtual nodes, and linear transformers to enhance message passing and global representation of molecular structures.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b18\": 1.0,\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient and accurate small molecule drug discovery. It proposes a novel graph-based architecture called CoAtGIN, which incorporates k-hop convolution, virtual nodes, and linear attention to achieve better local topology structure information and global representation. The model aims to balance efficiency and performance in predicting molecular properties.\",\n  \"Direct Inspiration\": [\"b16\", \"b18\", \"b21\"],\n  \"Indirect Inspiration\": [\"b20\"],\n  \"Other Inspiration\": [\"b23\", \"b25\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently predicting molecular properties by proposing a novel graph-based architecture called CoAtGIN. The main contributions include a k-hop convolution for faster message aggregation, a new global message-passing method using linear transformers, and enhancements to virtual node mechanisms to improve global representation.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b18\": 0.8,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b13\": 0.65,\n    \"b21\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b25\": 0.55\n  }\n}\n```"], "634e194790e50fcafd24f33e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of parameter-efficient fine-tuning of large pre-trained models, specifically for vision tasks, which currently suffer from issues like overfitting and high storage demands when fully fine-tuned. The authors propose a new method named SSF (Scale and Shift your Features), inspired by feature modulation methods, to fine-tune models effectively without introducing additional parameters during the inference phase. The method involves scaling and shifting the features extracted by a pre-trained model to match the distribution of a target dataset, achieving state-of-the-art performance on various classification tasks with minimal trainable parameters.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1,\n    \"b48\": 1,\n    \"b37\": 1,\n    \"b29\": 1,\n    \"b43\": 1,\n    \"b79\": 1,\n    \"b39\": 1,\n    \"b65\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b70\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of parameter-efficient fine-tuning of large pre-trained models for downstream tasks. It proposes a new method called SSF (Scale and Shift Features), which introduces scale and shift parameters to modulate features without adding extra parameters or computational cost during inference. This approach is inspired by feature modulation methods and aims to provide a task-independent, efficient fine-tuning strategy.\",\n  \"Direct Inspiration\": {\n    \"b43\": 1,\n    \"b35\": 0.9,\n    \"b59\": 0.8,\n    \"b39\": 0.8,\n    \"b65\": 0.8\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b79\": 0.7,\n    \"b14\": 0.7,\n    \"b70\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of parameter-efficient fine-tuning of large pre-trained models for downstream tasks without introducing additional parameters during inference. Inspired by the success of parameter-efficient strategies in NLP, the paper proposes SSF (Scale and Shift Features) that modulates features extracted by a pre-trained model through linear transformations, achieving state-of-the-art performance while maintaining a unified learnable parameter space and avoiding extra computational costs during inference.\",\n  \"Direct Inspiration\": [\"b35\", \"b43\", \"b65\", \"b79\"],\n  \"Indirect Inspiration\": [\"b39\", \"b29\", \"b37\", \"b48\"],\n  \"Other Inspiration\": [\"b14\", \"b70\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges of fine-tuning large pre-trained models for downstream tasks, particularly the issues of overfitting and storage space requirements. It proposes a novel parameter-efficient fine-tuning method called SSF (Scale and Shift Features), which modulates deep features using linear transformations to match the distribution of the target dataset. The method is inspired by feature modulation techniques and aims to avoid additional parameters and computational costs during inference.\",\n  \"Direct Inspiration\": {\n    \"b43\": 1,\n    \"b35\": 0.8,\n    \"b79\": 0.9,\n    \"b39\": 0.9,\n    \"b65\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.7,\n    \"b37\": 0.7,\n    \"b48\": 0.7,\n    \"b59\": 0.7,\n    \"b70\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b40\": 0.6,\n    \"b41\": 0.6,\n    \"b42\": 0.5,\n    \"b5\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of parameter-efficient fine-tuning of large pre-trained models for downstream tasks without introducing additional inference parameters. Traditional fine-tuning approaches either lead to overfitting or require significant storage for task-specific parameters. Inspired by methods from natural language processing, the paper proposes a new method called SSF (Scale and Shift deep Features), which modulates features using linear transformations. This approach achieves state-of-the-art performance with fewer trainable parameters and no additional inference costs, making it suitable for deployment on edge devices.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b43\": 1.0,\n    \"b65\": 1.0,\n    \"b79\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.8,\n    \"b37\": 0.8,\n    \"b48\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b39\": 0.6,\n    \"b70\": 0.6\n  }\n}\n```"], "62982a9a5aee126c0f6f5ecb": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving molecular property prediction from 3D structures using self-supervised pre-training via denoising. This approach is inspired by noise regularization techniques in GNNs and the connection between denoising and score-matching. The proposed method aims to generate useful representations from large datasets like PCQM4Mv2 to improve performance on smaller datasets such as DES15K.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b69\": 0.8,\n    \"b60\": 0.7,\n    \"b24\": 0.7,\n    \"b49\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.5,\n    \"b65\": 0.5,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving molecular property prediction from 3D structures by leveraging large datasets through a novel self-supervised pre-training method based on denoising. The proposed method generates useful representations for downstream tasks and demonstrates significant performance improvements on multiple datasets. The paper also introduces enhancements to a common GNN architecture, applying Tailored Activation Transformation (TAT) to Graph Network Simulators (GNS) to further boost performance.\",\n  \"Direct Inspiration\": [\"b22\"],\n  \"Indirect Inspiration\": [\"b36\", \"b69\", \"b60\", \"b24\", \"b49\"],\n  \"Other Inspiration\": [\"b79\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving molecular property prediction from 3D structures using a novel pre-training approach based on denoising. This method targets 3D point clouds and aims to generate useful representations for downstream tasks. The key contributions include the development of a denoising objective related to learning a specific force field, significant performance improvements on multiple datasets, and enhancements to a common GNN architecture using Tailored Activation Transformation (TAT).\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b69\": 0.9,\n    \"b60\": 0.8,\n    \"b24\": 0.8,\n    \"b61\": 0.7,\n    \"b49\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b79\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving molecular property prediction from 3D structures using large datasets. The proposed approach is a form of self-supervised pre-training based on denoising in the space of structures. This method is shown to improve performance on multiple challenging datasets and set new state-of-the-art results on the QM9 dataset. The technique is architecture-agnostic and can be applied to various models, including GNN-based and Transformer-based architectures.\",\n    \"Direct Inspiration\": {\n        \"b22\": 1.0,\n        \"b69\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b60\": 0.8,\n        \"b61\": 0.8,\n        \"b24\": 0.8,\n        \"b56\": 0.8,\n        \"b41\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b49\": 0.6,\n        \"b79\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving molecular property prediction from 3D structures using self-supervised pre-training through denoising. The main contributions include demonstrating significant performance improvements on various datasets by leveraging denoising as a pre-training objective, introducing enhancements to GNN architectures, and analyzing the benefits of pre-training.\",\n    \"Direct Inspiration\": {\n        \"b22\": 1.0,\n        \"b69\": 0.95,\n        \"b60\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b49\": 0.85,\n        \"b65\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b31\": 0.75,\n        \"b51\": 0.7,\n        \"b20\": 0.65\n    }\n}\n```"], "63520de890e50fcafd60f43e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance and efficiency of prompt tuning in pre-trained models (PTMs) for NLP tasks. The authors propose Late Prompt Tuning (LPT), which includes late and instance-aware prompts to shorten the propagation path of task-related information and improve model convergence and performance. LPT shows significant improvements in both few-shot and full-data scenarios across various text classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b44\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b19\": 0.7,\n    \"b41\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b24\": 0.6,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the poor performance and slow convergence rate of prompt tuning due to the long propagation path of task-related information from label signals to the prompt. The paper introduces Late Prompt Tuning (LPT), which inserts the prompt into an intermediate hidden layer of the pre-trained model (PTM) to shorten the propagation path and enhance performance. LPT also incorporates a prompt generator to produce instance-aware prompts, further improving effectiveness and efficiency. The proposed method is shown to outperform existing prompt-based tuning methods and is comparable to adapter-based tuning methods.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b10\": 0.95,\n    \"b19\": 0.85,\n    \"b44\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b31\": 0.75,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b11\": 0.7,\n    \"b23\": 0.7,\n    \"b13\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies in prompt tuning for pre-trained models (PTMs) used in natural language processing tasks. The primary challenges are the long propagation path of task-related information, which leads to poor performance and slow convergence rates. The proposed solution, Late Prompt Tuning (LPT), involves inserting prompts into an intermediate hidden layer of the PTM and using a prompt generator to create instance-aware prompts. This method aims to improve performance, convergence rates, and reduce training costs.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1.0,\n    \"b10\": 0.9,\n    \"b19\": 0.85,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.75,\n    \"b31\": 0.7,\n    \"b9\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b32\": 0.55,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the poor performance of prompt tuning in pre-trained models (PTMs) due to the long propagation path of task-related information. To solve this, the authors propose Late Prompt Tuning (LPT), which inserts prompts into intermediate layers of PTMs to shorten the propagation path and use instance-aware prompts generated by a prompt generator to further improve performance and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b44\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b41\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of prompt tuning in pre-trained models, particularly focusing on the long propagation path from label signals to the prompt that leads to poor performance. They propose a new method called Late Prompt Tuning (LPT) which inserts the prompt into an intermediate hidden layer and introduces a prompt generator to optimize training speed and memory cost.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b44\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b19\": 0.65,\n    \"b41\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.4,\n    \"b17\": 0.4,\n    \"b33\": 0.4\n  }\n}\n```"], "6327dda690e50fcafd67dfe7": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of learning robust and transferable graph representations with limited labeled data. It proposes a novel Graph Contrastive Learning framework, GraphCV, guided by the Information Bottleneck (IB) principle to minimize redundant information while maintaining predictive features.\",\n    \"Direct Inspiration\": [\"b29\", \"b38\", \"b39\"],\n    \"Indirect Inspiration\": [\"b21\", \"b45\", \"b47\"],\n    \"Other Inspiration\": [\"b24\", \"b32\"]\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges addressed in the paper are the feature suppression issue in Graph Contrastive Learning (GCL) and the need for robust and transferable graph representations. The proposed method, GraphCV, aims to alleviate these issues using a cross-view reconstruction mechanism guided by the Information Bottleneck (IB) principle.\",\n  \"Direct Inspiration\": [\"b29\", \"b38\"],\n  \"Indirect Inspiration\": [\"b47\", \"b45\"],\n  \"Other Inspiration\": [\"b19\", \"b43\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of feature suppression in Graph Contrastive Learning (GCL) by proposing a novel method called Graph Contrastive Learning with Cross-View Reconstruction (GraphCV). The primary challenge is the reliance on augmentation techniques that can either introduce redundant information or lose predictive features. The proposed method aims to balance representation robustness and sufficiency by using the Information Bottleneck (IB) principle and cross-view reconstruction to disentangle predictive from non-predictive features.\",\n  \"Direct Inspiration\": [\"b29\", \"b39\", \"b38\"],\n  \"Indirect Inspiration\": [\"b28\", \"b47\", \"b45\"],\n  \"Other Inspiration\": [\"b19\", \"b43\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of feature suppression in Graph Contrastive Learning (GCL) by proposing a novel framework, GraphCV, which uses cross-view reconstruction to disentangle and preserve predictive features while suppressing non-predictive information. The method is guided by the Information Bottleneck (IB) principle to ensure robust and transferable representations.\",\n  \"Direct Inspiration\": {\n    \"b29\": 0.9,\n    \"b38\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.75,\n    \"b13\": 0.75,\n    \"b47\": 0.75,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the feature suppression issue in Graph Contrastive Learning (GCL), where redundant and biased information affects the robustness and transferability of representations. The proposed algorithm, GraphCV, aims to alleviate this issue by using a novel cross-view reconstruction mechanism guided by the Information Bottleneck (IB) principle. This method disentangles predictive and non-predictive features to enhance representation robustness and sufficiency.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b38\": 1,\n    \"b39\": 1,\n    \"b47\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b43\": 0.8,\n    \"b28\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b25\": 0.6,\n    \"b46\": 0.6,\n    \"b45\": 0.5\n  }\n}\n```"], "63896cd690e50fcafde7a0f0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of Cross-Lingual Summarization (CLS) for long documents, which has not been well-explored due to the lack of corresponding datasets. The proposed Perseus dataset contains 94K Chinese scientific papers paired with English summaries, covering various disciplines. The paper evaluates different CLS methods, including summarize-then-translate and end-to-end approaches, and highlights the main challenges in long-document CLS.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1.0,\n    \"b0\": 1.0,\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b8\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b17\": 0.6,\n    \"b27\": 0.6,\n    \"b36\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in this paper is the development of a cross-lingual summarization (CLS) system for long documents, which is less explored due to the lack of corresponding datasets. The paper introduces Perseus, a long-document CLS dataset containing 94K Chinese scientific papers paired with English summaries. The authors propose and evaluate various summarize-then-translate and end-to-end methods based on Perseus, with the end-to-end baseline performing best. They also provide an out-of-domain test set to evaluate the generalization of CLS models. The main challenges identified include missing information, redundancy, wrong references, and semantically unclear generation.\",\n    \"Direct Inspiration\": {\n        \"b40\": 1.0,\n        \"b0\": 0.9,\n        \"b31\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.8,\n        \"b19\": 0.7,\n        \"b3\": 0.7,\n        \"b5\": 0.7,\n        \"b11\": 0.7,\n        \"b32\": 0.6,\n        \"b35\": 0.6\n    },\n    \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Cross-Lingual Summarization (CLS) for long documents, which is under-explored due to the lack of corresponding datasets. The authors construct a new long-document CLS dataset named Perseus, containing Chinese scientific papers paired with English summaries. They evaluate various summarize-then-translate and end-to-end methods based on this dataset, finding that the end-to-end models perform best. The main challenges identified include missing information, redundancy, wrong references, and semantically unclear generation in the summaries.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b31\": 0.9,\n    \"b40\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b5\": 0.8,\n    \"b11\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b8\": 0.7,\n    \"b19\": 0.7,\n    \"b9\": 0.6,\n    \"b16\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Cross-Lingual Summarization (CLS) for long documents, introducing the Perseus dataset, which consists of 94K Chinese scientific papers paired with English summaries. It highlights the significance of long-document CLS, details the dataset construction, and evaluates various CLS methods, emphasizing the superiority of end-to-end approaches.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.95,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.85,\n    \"b19\": 0.8,\n    \"b40\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b15\": 0.65,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Cross-Lingual Summarization (CLS) for long documents, constructing a new dataset named Perseus which consists of 94K Chinese scientific papers paired with English summaries. The authors propose and evaluate various summarize-then-translate and end-to-end methods based on Perseus. The end-to-end method performs best in generating logical, informative, and concise summaries. The paper also provides an out-of-domain test set to evaluate generalization and highlights key challenges such as missing information, redundancy, wrong references, and semantically unclear generation.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1,\n    \"b31\": 0.9,\n    \"b4\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b19\": 0.75,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.65,\n    \"b17\": 0.65,\n    \"b27\": 0.65,\n    \"b36\": 0.65,\n    \"b37\": 0.65,\n    \"b16\": 0.6,\n    \"b28\": 0.6,\n    \"b3\": 0.55,\n    \"b5\": 0.55,\n    \"b11\": 0.55,\n    \"b32\": 0.55,\n    \"b35\": 0.55\n  }\n}\n```"], "62a165475aee126c0f509e38": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing effective self-supervised learning schemes for non-homophilous graphs, where linked nodes may have dissimilar features and class labels. The authors propose a new framework, Decoupled Self-Supervised Learning (DSSL), which leverages both local structural and global semantic information to learn meaningful node representations. The key challenges include capturing local neighborhood distributions in a self-supervised manner and managing long-range semantic dependencies.\",\n  \"Direct Inspiration\": [\"b45\", \"b14\", \"b52\", \"b58\", \"b40\"],\n  \"Indirect Inspiration\": [\"b24\", \"b6\", \"b52\", \"b31\", \"b56\"],\n  \"Other Inspiration\": [\"b57\", \"b42\", \"b9\", \"b15\", \"b16\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in designing self-supervised learning (SSL) schemes for non-homophilous graphs, proposing a decoupled self-supervised learning (DSSL) framework. The key inspiration is that nodes with similar neighborhood patterns should have similar representations. The DSSL framework aims to model the distribution of node neighbors through a mixture generative process and capture both local structure and global semantic information.\",\n  \"Direct Inspiration\": [\"b45\", \"b14\", \"b52\", \"b58\", \"b40\"],\n  \"Indirect Inspiration\": [\"b56\", \"b5\", \"b50\", \"b31\", \"b26\"],\n  \"Other Inspiration\": [\"b12\", \"b23\", \"b10\", \"b39\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of self-supervised learning for node representation learning in non-homophilous graphs. It introduces a novel framework, Decoupled Self-Supervised Learning (DSSL), which leverages both local neighborhood structures and global semantic information to achieve better downstream performance without relying on labeled data.\",\n    \"Direct Inspiration\": {\n        \"b45\": 1,\n        \"b52\": 1,\n        \"b14\": 0.9,\n        \"b40\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b23\": 0.8,\n        \"b39\": 0.8,\n        \"b33\": 0.7,\n        \"b10\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.6,\n        \"b15\": 0.6,\n        \"b12\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing effective self-supervised learning schemes for non-homophilous graphs. It proposes a new framework called decoupled self-supervised learning (DSSL) that leverages latent variables to decouple heterogeneous and diverse patterns in local neighborhood distributions and capture global semantic dependencies.\",\n  \"Direct Inspiration\": {\n    \"b45\": 0.95,\n    \"b52\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.85,\n    \"b40\": 0.8,\n    \"b58\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b12\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper include designing self-supervised learning schemes for non-homophilous graphs, capturing local neighborhood distributions, and capturing long-range semantic dependencies in a self-supervised learning manner.\",\n    \"inspirations\": \"The authors are inspired by previous works on self-supervised learning, graph neural networks, and various methods addressing non-homophilous graphs. They propose a new framework called decoupled self-supervised learning (DSSL) which leverages latent variables to decouple heterogeneous patterns in local neighborhood distributions and capture global semantic dependencies.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b12\", \"b24\", \"b45\", \"b52\", \"b40\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b31\", \"b56\", \"b58\", \"b5\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b4\", \"b8\", \"b9\", \"b15\"]\n  }\n}\n```"], "62393e7f5aee126c0f1260e9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality paraphrases with high semantic similarity and high lexical/syntactic diversity. The proposed model, QCPG, allows for direct control of these three quality dimensions using a simpler mechanism compared to previous methods, avoiding over-specificity and enhancing flexibility and scalability.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b26\": 0.7,\n    \"b31\": 0.68\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.65,\n    \"b21\": 0.6,\n    \"b27\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality paraphrases that maintain high semantic similarity while also exhibiting high lexical and syntactic diversity. It proposes a Quality Controlled Paraphrase Generation model (QCPG) that uses a simple control mechanism to manage paraphrase quality along three dimensions: semantic similarity, syntactic diversity, and lexical diversity.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b31\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b21\": 0.75,\n    \"b26\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b11\": 0.55,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality paraphrases characterized by high semantic similarity, and high lexical and syntactic diversity. The proposed Quality Controlled Paraphrase Generation (QCPG) model introduces a simpler control mechanism for paraphrase quality without over-complicating the process. The model is trained to generate paraphrases based on a three-dimensional quality vector and is tested on datasets like MSCOCO, WikiAnswers, and ParaBank2.0.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.9,\n    \"b18\": 0.8,\n    \"b8\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b21\": 0.7,\n    \"b30\": 0.7,\n    \"b22\": 0.7,\n    \"b11\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in generating high-quality paraphrases characterized by high semantic similarity and high lexical and syntactic diversity. The proposed QCPG model allows direct control over these three quality dimensions, offering a simpler mechanism compared to previous control-based methods that required detailed syntactic or lexical information.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b31\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.65,\n    \"b8\": 0.6,\n    \"b14\": 0.6,\n    \"b4\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces the Quality Controlled Paraphrase Generation (QCPG) model, which aims to generate high-quality paraphrases by controlling three dimensions: semantic similarity, syntactic diversity, and lexical diversity. The challenges addressed include the difficulty of maintaining semantic similarity while increasing linguistic diversity and the scarcity of high-quality paraphrase data. The proposed solution simplifies control mechanisms compared to previous methods, allowing for flexible and scalable paraphrase generation.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b31\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b8\": 0.7,\n    \"b14\": 0.7,\n    \"b27\": 0.7,\n    \"b4\": 0.7\n  }\n}\n```"], "630ed16690e50fcafd793a2d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient DNN model quantization to reduce computation and memory costs while maintaining accuracy. The authors propose an adaptive numeric data type called ANT, which adapts to individual value intervals within tensors and the distribution of different tensors. ANT combines the advantages of float and int types, resulting in large quantization benefits and low hardware overheads. The paper also introduces a unified TypeFusion processing element to handle different primitive types within ANT and demonstrates significant performance and energy improvements.\",\n  \"Direct Inspiration\": {\n    \"b65\": 1.0,\n    \"b85\": 1.0,\n    \"b72\": 1.0,\n    \"b77\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b63\": 0.8,\n    \"b45\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b57\": 0.7,\n    \"b93\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the escalating computational and memory demands of Deep Neural Networks (DNNs) by proposing a novel adaptive numeric data type called ANT. This type adapts to the importance and distribution of values in DNN tensors, achieving higher quantization benefits and lower hardware overheads. The main challenges include handling the non-uniform distribution and importance of tensor values, and ensuring compatibility with existing DNN accelerators.\",\n  \"Direct Inspiration\": {\n    \"b65\": 1.0,\n    \"b85\": 0.9,\n    \"b72\": 0.85,\n    \"b77\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.75,\n    \"b63\": 0.7,\n    \"b57\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b60\": 0.6,\n    \"b93\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient quantization for deep neural networks (DNNs), proposing an adaptive numeric data type called ANT that adapts both intra-tensor and inter-tensor distributions. This reduces bit length while maintaining accuracy and hardware efficiency. The ANT framework leverages a novel data type primitive called flint, combining float and int advantages, and introduces a TypeFusion processing element to handle different primitive types.\",\n  \"Direct Inspiration\": {\n    \"b65\": 1.0,\n    \"b85\": 1.0,\n    \"b72\": 0.95,\n    \"b77\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b63\": 0.7,\n    \"b45\": 0.7,\n    \"b39\": 0.65,\n    \"b46\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b57\": 0.6,\n    \"b93\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the growing computation gap in DNN models due to the increasing size of these models, which outpaces hardware improvements. The authors propose a novel adaptive numeric data type called ANT, designed to optimize memory and computation through both intra-tensor and inter-tensor adaptivity. This method aims to maintain model accuracy while reducing bit length and hardware overhead.\",\n  \"Direct Inspiration\": {\n    \"b65\": 1.0,\n    \"b85\": 1.0,\n    \"b72\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b63\": 0.8,\n    \"b45\": 0.8,\n    \"b77\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b57\": 0.7,\n    \"b93\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently quantizing DNN models to reduce memory and computation costs while maintaining model accuracy. The authors propose an adaptive numeric data type called ANT, which adapts to the importance of different value intervals within a tensor and the distribution of different tensors. The ANT framework achieves high hardware efficiency and supports mixed-precision quantization with minimal hardware modifications.\",\n  \"Direct Inspiration\": {\n    \"b65\": 1,\n    \"b85\": 1,\n    \"b72\": 0.95,\n    \"b77\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.8,\n    \"b63\": 0.8,\n    \"b57\": 0.75,\n    \"b93\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b56\": 0.7,\n    \"b73\": 0.7,\n    \"b80\": 0.7,\n    \"b94\": 0.7\n  }\n}\n```"], "62a2b6955aee126c0f4d8e7b": ["```json\n{\n  \"Summary\": \"The primary challenges identified include the difficulty in fine-tuning large vision models due to overfitting, high computation, and storage costs. The authors propose a novel approach called Neural prOmpt seArcH (NOAH), which uses neural architecture search (NAS) to automatically find the optimal design for parameter-efficient tuning methods such as Adapter, LoRA, and VPT. The NOAH approach aims to address the inefficiencies and performance inconsistency of these individual tuning methods.\",\n  \"Direct Inspiration\": [\n    \"b17\",\n    \"b18\",\n    \"b19\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b4\",\n    \"b9\",\n    \"b47\"\n  ],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency and inconsistency in fine-tuning large vision models due to their size, which can lead to overfitting and increased computational costs. The authors propose a novel Neural prOmpt seArcH (NOAH) algorithm to automatically search for optimal prompt designs using neural architecture search (NAS), incorporating Adapter, LoRA, and VPT modules.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b18\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the difficulty of fine-tuning large vision models due to issues like overfitting, increased compute, and storage costs. The algorithm proposed, Neural prOmpt seArcH (NOAH), aims to address this by using a neural architecture search (NAS) to automatically find the optimal prompt design from existing parameter-efficient tuning methods (Adapter, LoRA, and VPT).\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b18\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b47\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of fine-tuning large vision models, particularly Transformers, to avoid overfitting and reduce compute and storage costs. The authors propose a novel neural architecture search (NAS) algorithm called Neural prOmpt seArcH (NOAH) that integrates three existing parameter-efficient tuning methods: Adapter, Low-Rank Adaptation (LoRA), and Visual Prompt Tuning (VPT). The primary contribution is the automatic search for optimal prompt design within a Transformer block using NAS, leading to better performance in transfer learning, few-shot learning, and domain generalization tasks.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b18\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b47\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the difficulty of fine-tuning large vision models due to overfitting, increased compute, and storage costs. The paper introduces a novel method, Neural prOmpt seArcH (NOAH), which leverages neural architecture search (NAS) to find the optimal prompt module design for efficiently tuning large vision models.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b18\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b47\": 0.7\n  }\n}\n```"], "6389d6b590e50fcafdfead12": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are to enhance microarchitectural instruction fusion, especially focusing on non-consecutive and non-contiguous memory fusion. The proposed algorithm is the Helios microarchitecture, which supports non-consecutive (NCS) and non-contiguous (NCT) memory fusion, and relies on predictive mechanisms to identify fusion candidates and handle dependencies.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b28\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is improving microarchitectural fusion, specifically focusing on non-consecutive and non-contiguous memory operations within the RISC-V ISA. The proposed Helios microarchitecture introduces novel techniques to increase the number of fused memory instructions, aiming to enhance performance by reducing latency and pipeline resource usage.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving microarchitectural fusion, specifically focusing on non-consecutive and non-contiguous memory fusion within the RISC-V ISA. The proposed Helios microarchitecture aims to optimize fusion to enhance performance and pipeline resource utilization.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b2\": 0.7,\n    \"b3\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses microarchitectural fusion, particularly focusing on non-consecutive and non-contiguous memory fusion within the RISC-V ISA. The proposed Helios microarchitecture aims to enhance pipeline resource utilization by optimizing fusion techniques beyond conventional methods.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b13\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper focuses on microarchitectural fusion to improve processor performance, specifically targeting the RISC-V ISA. It proposes the Helios microarchitecture to enable non-consecutive and non-contiguous fusion of memory operations, which can lead to significant performance improvements.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1.0,\n        \"b16\": 0.9,\n        \"b1\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b28\": 0.7,\n        \"b12\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.5,\n        \"b2\": 0.5,\n        \"b3\": 0.5,\n        \"b11\": 0.5\n    }\n}\n```"], "63608e5090e50fcafdee1257": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving fine-tuning techniques for pretrained language models, specifically focusing on the initialization of classification heads. The authors propose Efficient Head Finetuning (EH-FT), a two-stage method that leverages parameter-efficient tuning methods in the first stage to create well-initialized classification heads, which are then used in full fine-tuning in the second stage.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b32\": 1.0,\n    \"b55\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.8,\n    \"b12\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b65\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is improving the performance of pretrained language models for specific NLP tasks by optimizing the initialization of the classification heads. The proposed algorithm, Efficient Head Finetuning (EH-FT), is a two-stage method that combines parameter-efficient tuning with full finetuning to ensure better generalization and performance.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b33\": 0.9,\n    \"b36\": 0.9,\n    \"b12\": 0.9,\n    \"b65\": 0.9,\n    \"b21\": 0.9,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b55\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b54\": 0.6,\n    \"b60\": 0.6,\n    \"b58\": 0.6,\n    \"b57\": 0.6,\n    \"b56\": 0.6,\n    \"b64\": 0.6,\n    \"b35\": 0.6,\n    \"b61\": 0.6,\n    \"b39\": 0.6,\n    \"b29\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the optimization of the classification head initialization in fine-tuning pretrained language models for specific NLP tasks. The authors propose a method called Efficient Head Finetuning (EH-FT) which involves a two-stage tuning process with parameter-efficient tuning methods in the first stage to improve the performance of the downstream task.\",\n    \"Direct Inspiration\": {\n        \"b27\": 1,\n        \"b33\": 1,\n        \"b36\": 1,\n        \"b12\": 1,\n        \"b65\": 1,\n        \"b21\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b32\": 0.9,\n        \"b44\": 0.8,\n        \"b55\": 0.9,\n        \"b58\": 0.7,\n        \"b57\": 0.7,\n        \"b35\": 0.6,\n        \"b61\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b25\": 0.5,\n        \"b54\": 0.5,\n        \"b9\": 0.5,\n        \"b56\": 0.5,\n        \"b64\": 0.5,\n        \"b29\": 0.5,\n        \"b60\": 0.5,\n        \"b39\": 0.5,\n        \"b16\": 0.5,\n        \"b5\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of pretrained language models in NLP tasks through an effective finetuning approach. The authors propose a new method called Efficient Head Finetuning (EH-FT), which combines parameter-efficient tuning with full finetuning to achieve better generalization and performance, especially in low-resource settings. The primary inspiration comes from the limitations of existing methods like LP-FT and the potential of parameter-efficient tuning methods.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b32\": 0.9,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.7,\n    \"b36\": 0.7,\n    \"b12\": 0.7,\n    \"b65\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b54\": 0.6,\n    \"b16\": 0.6,\n    \"b44\": 0.6,\n    \"b55\": 0.6,\n    \"b56\": 0.6,\n    \"b64\": 0.6,\n    \"b39\": 0.6,\n    \"b35\": 0.5,\n    \"b61\": 0.5,\n    \"b5\": 0.5,\n    \"b58\": 0.5,\n    \"b57\": 0.5,\n    \"b60\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the fine-tuning process of pretrained language models for specific NLP tasks. The main contribution is the proposal of Efficient Head Finetuning (EH-FT), a two-stage method that leverages parameter-efficient tuning in the first stage to create a well-initialized classification head, which is then used in the second stage for full fine-tuning. This method aims to enhance the performance on downstream tasks while reducing the change in the model's features, thereby addressing issues like catastrophic forgetting and overfitting.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b55\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.9,\n    \"b36\": 0.8,\n    \"b12\": 0.8,\n    \"b65\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.6,\n    \"b9\": 0.5,\n    \"b44\": 0.5,\n    \"b16\": 0.4,\n    \"b56\": 0.4,\n    \"b64\": 0.4,\n    \"b58\": 0.3,\n    \"b57\": 0.3\n  }\n}\n```"], "628ef0495aee126c0f82db2f": ["```json\n{\n  \"Summary\": \"The primary challenge in the paper is addressing the false negative problem in document-level relation extraction (RE), particularly in the DocRED dataset. The authors propose an iterative human-in-the-loop approach to revise the DocRED dataset by generating relation candidates using state-of-the-art document-level RE models and involving human annotators to examine these triples. The goal is to recover incomplete annotations and provide a comprehensive evaluation for document-level RE research.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b15\": 0.7,\n    \"b24\": 0.6,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the false negative problem in relation extraction datasets, particularly in the DocRED dataset. The proposed algorithm involves revising the DocRED dataset to recover incomplete annotations using an iterative approach with human annotators and state-of-the-art document-level RE models.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b5\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the false negative problem in document-level relation extraction (RE) datasets, specifically the DocRED dataset. It identifies the incompleteness of annotations as a primary challenge and proposes an iterative human-in-the-loop approach to revise DocRED. The novel method involves using state-of-the-art RE models to generate relation candidates, which are then examined by human annotators.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b28\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.75,\n    \"b24\": 0.7,\n    \"b5\": 0.65,\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b11\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the false negative problem in the DocRED dataset for relation extraction (RE). The authors propose to revise the DocRED dataset to recover incomplete annotations through an iterative human-in-the-loop approach. They use state-of-the-art document-level RE models to generate relation candidates and involve human annotators to examine the recommended triples.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b15\": 0.7,\n    \"b24\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the false negative problem in document-level relation extraction (RE) datasets, particularly in DocRED. The authors propose revising the DocRED dataset to recover incomplete annotations through an iterative approach involving human annotators who examine recommended triples generated by state-of-the-art RE models.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.95,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.85,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.75,\n    \"b30\": 0.7\n  }\n}\n```"], "622819cdd18a2b26c7ab496a": ["```json\n{\n  \"Summary\": \"The paper introduces a new pretraining framework called GLM (General Language Model) designed to unify autoregressive and autoencoding methodologies using autoregressive blank infilling. This model aims to handle both NLU and text generation tasks more efficiently and effectively than existing models like BERT, XLNet, T5, and UniLM.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b39\": 0.8,\n    \"b27\": 0.8,\n    \"b9\": 0.7,\n    \"b1\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing pretraining frameworks for large-scale language models, which include autoregressive, autoencoding, and encoder-decoder models. The paper proposes a novel framework called GLM (General Language Model) that integrates autoregressive and autoencoding approaches. The key innovations include autoregressive blank infilling, 2D positional encoding, and multi-task pretraining for both NLU and text generation tasks.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b14\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b27\": 0.6,\n    \"b9\": 0.6,\n    \"b1\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The paper addresses the limitations of existing pretraining frameworks (autoregressive, autoencoding, and encoder-decoder models) in handling all NLP tasks. It proposes a new framework called GLM (General Language Model) based on autoregressive blank infilling to unify bidirectional and unidirectional attention mechanisms.\",\n    \"inspirations\": \"The paper is inspired by previous works on pretraining frameworks, particularly those utilizing autoregressive and autoencoding objectives, and aims to combine their strengths.\"\n  },\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b39\": 0.8,\n    \"b27\": 0.8,\n    \"b9\": 0.7,\n    \"b1\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b3\": 0.6,\n    \"b35\": 0.6,\n    \"b2\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b14\": 0.5,\n    \"b13\": 0.5,\n    \"b34\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces GLM (General Language Model), a novel pretraining framework based on autoregressive blank infilling. The main challenges addressed are unifying autoregressive and autoencoding models to handle both NLU and text generation tasks and achieving consistency between pretraining and finetuning. Key innovations include the autoregressive blank infilling objective, a 2D positional encoding technique, and a multi-task pretraining setup.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b14\": 0.8,\n    \"b27\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b9\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is to develop a general pretraining framework that can handle both natural language understanding (NLU) and text generation tasks. The authors propose a novel algorithm called GLM (General Language Model) based on autoregressive blank infilling. GLM combines the strengths of autoregressive and autoencoding models by dividing the input text into two parts and using a 2D positional encoding technique. GLM is shown to significantly outperform existing models like BERT, RoBERTa, T5, and BART on various benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.95,\n    \"b32\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.85,\n    \"b27\": 0.85,\n    \"b39\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.75,\n    \"b9\": 0.75,\n    \"b14\": 0.75,\n    \"b25\": 0.75,\n    \"b26\": 0.75\n  }\n}\n```"], "623d90d46750f864fe4cafd6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of inductive node classification by combining GNNs and CRFs. It proposes a novel method called Structured Proxy Network (SPN) that uses GNNs to parameterize potential functions in CRFs. This approach aims to leverage the high capacity of GNNs while modeling the joint dependency of node labels to achieve better performance in node classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b42\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.6,\n    \"b25\": 0.7,\n    \"b30\": 0.7,\n    \"b27\": 0.6,\n    \"b17\": 0.5,\n    \"b14\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.4,\n    \"b38\": 0.4,\n    \"b37\": 0.4,\n    \"b43\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the limitation of Graph Neural Networks (GNNs) in modeling the joint dependency of node labels in inductive node classification. The proposed algorithm, Structured Proxy Network (SPN), combines GNNs and Conditional Random Fields (CRFs) to achieve higher capacity and structured prediction. This is done by parameterizing potential functions in CRFs with GNNs and solving a new optimization problem as a proxy, which is easier and more efficient to solve compared to the traditional maximin game.\",\n    \"Direct Inspiration\": {\n        \"b25\": 1,\n        \"b30\": 1,\n        \"b42\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.9,\n        \"b23\": 0.8,\n        \"b44\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.6,\n        \"b38\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge in the paper is the inductive node classification problem in graph-structured data. The authors propose the Structured Proxy Network (SPN) which combines Graph Neural Networks (GNNs) and Conditional Random Fields (CRFs) to model the joint dependency of node labels efficiently.\",\n  \"Direct Inspiration\": {\n    \"b42\": 0.9,\n    \"b25\": 0.85,\n    \"b30\": 0.85,\n    \"b43\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inductive node classification in graph-structured data. It proposes a novel approach called Structured Proxy Network (SPN), which combines Graph Neural Networks (GNNs) and Conditional Random Fields (CRFs) to model the joint dependency of node labels while maintaining high model capacity. The key innovation is the introduction of a proxy optimization problem that simplifies the learning process, making it more stable and efficient.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b30\": 1.0,\n    \"b44\": 1.0,\n    \"b23\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b42\": 0.8,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b41\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inductive node classification in graph-structured data by proposing a novel Structured Proxy Network (SPN) that combines Graph Neural Networks (GNNs) and Conditional Random Fields (CRFs). The key innovation is to parameterize potential functions in CRFs with GNNs, and solve a proxy optimization problem for model learning, which simplifies the learning process while maintaining high model capacity.\",\n  \"Direct Inspiration\": {\n    \"b42\": 0.9,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b30\": 0.7,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```"], "634d805690e50fcafd4e07bc": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of memory disaggregation, focusing on high latency and performance degradation associated with existing RDMA-based and page/object-based approaches. It proposes a new solution, DIRECTCXL, which uses Compute Express Link (CXL) to directly connect host processors and remote memory resources, eliminating the need for data copies and improving performance significantly.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1.0,\n        \"b18\": 1.0,\n        \"b19\": 1.0,\n        \"b20\": 1.0,\n        \"b21\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b4\": 0.8,\n        \"b5\": 0.8,\n        \"b6\": 0.8,\n        \"b7\": 0.8,\n        \"b8\": 0.8,\n        \"b9\": 0.8,\n        \"b10\": 0.8,\n        \"b11\": 0.8,\n        \"b12\": 0.8,\n        \"b13\": 0.8,\n        \"b14\": 0.8,\n        \"b15\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b24\": 0.5,\n        \"b25\": 0.5,\n        \"b26\": 0.5,\n        \"b27\": 0.5,\n        \"b28\": 0.5,\n        \"b29\": 0.5,\n        \"b30\": 0.5,\n        \"b31\": 0.5,\n        \"b32\": 0.5,\n        \"b33\": 0.5,\n        \"b34\": 0.5,\n        \"b35\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of memory disaggregation, particularly focusing on latency and performance inefficiencies associated with current RDMA-based approaches. The authors propose DIRECTCXL, a solution leveraging Compute Express Link (CXL) technology to provide direct access to disaggregated memory without data copies and with lower latency compared to RDMA.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b20\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b22\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b27\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing latency and improving performance in memory disaggregation by leveraging Compute Express Link (CXL) technology. DIRECTCXL is introduced as a novel approach to connect host processors and remote memory resources directly over CXL, eliminating redundant data copies and software interventions.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b18\": 0.8,\n    \"b20\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of memory disaggregation, specifically focusing on the high latency and redundant memory copies associated with current RDMA-based solutions. It proposes DIRECTCXL, a system that leverages Compute Express Link (CXL) to enable direct access to disaggregated memory without data copies, achieving significantly lower latency and higher performance compared to RDMA-based approaches.\",\n  \"Direct Inspiration\": [\"b17\", \"b18\", \"b19\", \"b20\", \"b21\"],\n  \"Indirect Inspiration\": [\"b3\", \"b4\", \"b5\", \"b6\", \"b7\", \"b8\", \"b9\", \"b10\", \"b11\", \"b12\", \"b13\", \"b14\", \"b15\"],\n  \"Other Inspiration\": [\"b24\", \"b25\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of memory disaggregation, focusing on the high latency and redundancy issues posed by traditional methods like RDMA. The proposed solution, DIRECTCXL, leverages Compute Express Link (CXL) to provide a low-latency, direct access memory disaggregation system. It highlights the novel use of CXL for integrating disaggregated memory into processor-side system memory without data copies, implementing a CXL controller and switch, and providing a software runtime for direct access.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b18\": 0.8,\n    \"b19\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5,\n    \"b29\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5,\n    \"b33\": 0.5,\n    \"b34\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```"], "621ee1845aee126c0f26a9df": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of reducing memory stranding in public cloud environments, specifically focusing on the inefficiency caused by unrented memory in servers with fully rented cores. The proposed solution is a CXL-based memory disaggregation system that aims to pool memory across multiple servers to reduce hardware costs and improve resource utilization. The design includes a multi-ported external memory controller, management of pooled memory as zero-core virtual NUMA nodes, and a control plane that predicts VM memory latency sensitivity for efficient scheduling.\",\n    \"Direct Inspiration\": {\n        \"b10\": 0.9,\n        \"b11\": 0.8,\n        \"b12\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.7,\n        \"b29\": 0.6,\n        \"b46\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b36\": 0.8,\n        \"b40\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of memory stranding in public cloud environments and proposes a full-stack memory disaggregation design based on the Compute Express Link (CXL) standard. The core contributions include identifying stranding hotspots, designing a CXL-based memory pool, managing pooled memory, and optimizing VM scheduling based on latency sensitivity predictions.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b11\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b29\": 0.7,\n    \"b46\": 0.7,\n    \"b47\": 0.7,\n    \"b48\": 0.7,\n    \"b49\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.8,\n    \"b40\": 0.8,\n    \"b37\": 0.8,\n    \"b38\": 0.8,\n    \"b39\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of DRAM inefficiency in public cloud environments due to memory stranding. It proposes a CXL-based full-stack memory disaggregation design to mitigate this issue, ensuring performance and compatibility with existing virtualization technologies without requiring modifications to customer workloads.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b11\": 1.0,\n    \"b12\": 1.0,\n    \"b36\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b13\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b29\": 0.6,\n    \"b37\": 0.6,\n    \"b38\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the high costs and inefficiencies of DRAM in public cloud environments, specifically due to memory stranding. The authors propose a CXL-based full-stack memory disaggregation design to address these challenges. This design aims to reduce memory stranding, ensure compatibility with existing virtualization technologies, and manage memory latency effectively.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b11\": 0.85,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b14\": 0.7,\n    \"b36\": 0.65,\n    \"b37\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.55,\n    \"b46\": 0.5,\n    \"b47\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of memory stranding in public cloud providers, particularly focusing on the inefficiency of DRAM usage. They propose a CXL-based full-stack memory disaggregation design to reduce these inefficiencies. Their solution includes a multi-ported external memory controller, a system software layer that manages pooled memory, and a distributed system software layer that predicts VM memory latency sensitivity and manages memory allocation dynamically.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b11\": 0.9,\n    \"b12\": 0.9,\n    \"b36\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.8,\n    \"b38\": 0.8,\n    \"b39\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.6,\n    \"b44\": 0.6,\n    \"b45\": 0.6,\n    \"b46\": 0.6,\n    \"b47\": 0.6,\n    \"b48\": 0.6,\n    \"b49\": 0.6,\n    \"b50\": 0.6,\n    \"b51\": 0.6\n  }\n}\n```"], "63438d2990e50fcafd4ebd66": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper include the complexity of Map Space Exploration (MSE) for DNN accelerators, the vast search space for legal mappings, and the inefficiency of current search algorithms given the increasing size and complexity of DNN models.\",\n    \"inspirations\": \"The paper is inspired by the need to demystify MSE, understand why certain optimization techniques work, and develop more efficient and scalable MSE techniques.\"\n  },\n  \"Direct Inspiration\": [\n    \"b18\",\n    \"b27\",\n    \"b43\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b8\",\n    \"b35\",\n    \"b47\"\n  ],\n  \"Other Inspiration\": [\n    \"b15\",\n    \"b37\",\n    \"b39\",\n    \"b50\",\n    \"b67\",\n    \"b68\",\n    \"b79\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Map Space Exploration (MSE) for Deep Neural Network (DNN) accelerators. It aims to demystify MSE by analyzing various search algorithms and their trade-offs. The authors propose two novel heuristic techniques: a warm-start technique to speed up convergence and a sparsity-aware technique for optimized mapping across different sparsity levels.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b18\": 1,\n    \"b43\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b35\": 0.8,\n    \"b47\": 0.8,\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b38\": 0.7,\n    \"b46\": 0.7,\n    \"b60\": 0.7,\n    \"b15\": 0.7,\n    \"b37\": 0.7,\n    \"b39\": 0.7,\n    \"b50\": 0.7,\n    \"b67\": 0.7,\n    \"b68\": 0.7,\n    \"b79\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.6,\n    \"b22\": 0.6,\n    \"b45\": 0.6,\n    \"b33\": 0.6,\n    \"b5\": 0.6,\n    \"b11\": 0.6,\n    \"b51\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing the mapping of Deep Neural Networks (DNNs) over Neural Processing Units (NPUs) to achieve high performance and energy efficiency. It proposes novel heuristic techniques to improve the efficiency of Map Space Exploration (MSE), including a warm-start technique and a sparsity-aware technique.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b27\": 1.0,\n    \"b43\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b35\": 0.8,\n    \"b47\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b38\": 0.7,\n    \"b46\": 0.7,\n    \"b60\": 0.7,\n    \"b15\": 0.6,\n    \"b37\": 0.6,\n    \"b39\": 0.6,\n    \"b50\": 0.6,\n    \"b67\": 0.6,\n    \"b68\": 0.6,\n    \"b79\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of map space exploration (MSE) for deep neural network (DNN) accelerators. It proposes two novel heuristic techniques: a warm-start technique to initialize MSE with prior optimal solutions, and a sparsity-aware technique to handle mappings across different sparsity levels. The paper aims to demystify MSE by comparing three categories of mappers: random-based, feedback-based, and gradient-based, analyzing their trade-offs and effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b27\": 1.0,\n    \"b43\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b35\": 0.7,\n    \"b47\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Map Space Exploration (MSE) for Deep Neural Network (DNN) accelerators, specifically focusing on understanding and improving the mapping of DNN workloads to hardware resources to achieve high efficiency. The paper proposes two novel heuristic techniques: a warm-start technique and a sparsity-aware technique, to scale MSE to more complex workloads.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b27\": 0.9,\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b38\": 0.6,\n    \"b46\": 0.6,\n    \"b60\": 0.6,\n    \"b15\": 0.5,\n    \"b37\": 0.5,\n    \"b39\": 0.5,\n    \"b50\": 0.5,\n    \"b67\": 0.5,\n    \"b68\": 0.5,\n    \"b79\": 0.5\n  }\n}\n```"], "63a413f790e50fcafd6d1f93": ["```json\n{\n  \"Summary\": \"The paper introduces NODEFORMER, a new scalable model for node-level prediction tasks on large graphs. Major challenges addressed include the scalability of message passing and differentiable optimization for discrete structures. The model leverages a kernelized Gumbel-Softmax operator for efficient message passing and incorporates input topology as relational bias.\",\n  \"Direct Inspiration\": [\"b10\", \"b3\"],\n  \"Indirect Inspiration\": [\"b35\", \"b18\", \"b38\", \"b46\"],\n  \"Other Inspiration\": [\"b25\", \"b5\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses key challenges in graph neural networks (GNNs) such as scalability, heterophily, over-squashing, long-range dependencies, and graph incompleteness. The proposed NODEFORMER model is a novel Transformer-based approach that optimizes latent graph topology for efficient node-level prediction, demonstrating significant improvements over state-of-the-art GNN models and structure learning methods.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b25\": 0.85,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b3\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalability and differentiability in learning latent graph structures for node-level prediction tasks. The authors propose NODEFORMER, a model that leverages a kernelized Gumbel-Softmax operator to enable efficient, scalable, and differentiable message passing over large graphs.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b7\": 0.9,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b25\": 0.6,\n    \"b15\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.4,\n    \"b51\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the scalability and differentiability of learning discrete graph structures for node-level prediction tasks, particularly in large-scale graphs. The proposed method, NODEFORMER, introduces a new class of graph networks with layer-wise message passing operating over latent graphs that connect all nodes, optimized in an end-to-end differentiable fashion. The method aims to achieve efficient and scalable structure learning, reducing time and space complexity significantly compared to previous models.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b25\": 1.0,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b35\": 0.8,\n    \"b1\": 0.7,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b51\": 0.6,\n    \"b7\": 0.6,\n    \"b26\": 0.5,\n    \"b38\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in graph neural networks (GNNs) such as heterophily, over-squashing, long-range dependencies, and graph incompleteness. It proposes a novel model, NODEFORMER, which optimizes latent graphs in an end-to-end differentiable manner and scales all-pair message passing to large node classification graphs. The model demonstrates significant improvements over existing GNN models and structure learning methods while reducing time and space consumption.\",\n\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b10\": 0.85,\n        \"b35\": 0.8\n    },\n\n    \"Indirect Inspiration\": {\n        \"b3\": 0.75,\n        \"b18\": 0.7,\n        \"b46\": 0.65,\n        \"b51\": 0.7\n    },\n\n    \"Other Inspiration\": {\n        \"b25\": 0.6,\n        \"b5\": 0.6,\n        \"b15\": 0.55,\n        \"b21\": 0.55\n    }\n}\n```"], "632bd2a990e50fcafdb7a24d": ["```json\n{\n  \"Summary\": \"The paper addresses several challenges in knowledge-intensive tasks such as open-domain question answering and fact-checking, focusing on improving the retrieve-then-read pipeline by introducing a novel Generate-then-Read (GENREAD) pipeline. This approach integrates large language models to generate contextual documents for a given query and then uses these generated documents to predict the final answer. The paper also explores zero-shot and supervised settings and proposes methods to ensure diverse and rich content in the generated documents.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b6\": 0.9,\n    \"b21\": 0.9,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b17\": 0.8,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b27\": 0.6,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in knowledge-intensive tasks such as open-domain question answering and fact checking, specifically focusing on the limitations of existing dense retrieval models. The authors propose a novel Generate-then-Read (GENREAD) pipeline that leverages large language models to generate contextual documents in response to a query, which are then read to predict the final answer. The method is evaluated in zero-shot and supervised settings, with additional techniques like nucleus sampling, diverse human prompts, and clustering-based prompts to enhance document diversity and knowledge coverage.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b17\": 0.85,\n    \"b15\": 0.8,\n    \"b3\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.7,\n    \"b28\": 0.65,\n    \"b27\": 0.6,\n    \"b32\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.5,\n    \"b36\": 0.45,\n    \"b41\": 0.4,\n    \"b53\": 0.35\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses primary challenges in knowledge-intensive tasks such as open-domain question answering and fact-checking. The main innovation is the proposed Generate-then-Read (GENREAD) pipeline, which prompts a large language model to generate contextual documents based on a given query and then reads these documents to predict the final answer. Key challenges addressed include shallow interactions between question and document representations, potential loss of fine-grained information, and the high computational cost of dense retrievers. The paper proposes solutions like diverse human prompts and clustering-based prompts to improve knowledge coverage in generated documents.\",\n    \"Direct Inspiration\": {\n        \"b17\": 0.9,\n        \"b21\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b6\": 0.8,\n        \"b15\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b31\": 0.6,\n        \"b28\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenges of knowledge-intensive tasks such as open-domain question answering (QA) and fact-checking by proposing a novel Generate-then-Read (GENREAD) pipeline. The algorithm first prompts a large language model (e.g., GPT-3) to generate contextual documents based on a given query and then reads these generated documents to predict the final answer. The proposed method aims to address the limitations of existing retrieval methods, particularly the expensive and parameter-limited dense retrieval models. The paper also explores both zero-shot and supervised settings and introduces solutions like nucleus sampling, diverse human prompts, and clustering-based prompts to enhance the diversity and knowledge coverage of the generated documents.\",\n  \"Direct Inspiration\": [\"b31\", \"b21\", \"b3\", \"b17\"],\n  \"Indirect Inspiration\": [\"b7\", \"b36\", \"b41\", \"b15\"],\n  \"Other Inspiration\": [\"b27\", \"b28\", \"b37\", \"b0\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in knowledge-intensive tasks such as open-domain question answering and fact checking. It highlights the limitations of existing dense retrieval models and proposes a novel Generate-then-Read (GENREAD) pipeline. The approach involves prompting large language models to generate contextual documents for a given query, then using these documents to predict the final answer. The method aims to enhance the performance by leveraging the generative capabilities of large language models, particularly under zero-shot and supervised settings.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b28\": 0.9,\n    \"b15\": 0.85,\n    \"b17\": 0.8,\n    \"b3\": 0.75,\n    \"b6\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.68,\n    \"b49\": 0.65,\n    \"b24\": 0.6,\n    \"b30\": 0.58\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.55,\n    \"b4\": 0.5,\n    \"b27\": 0.45,\n    \"b7\": 0.4,\n    \"b32\": 0.35\n  }\n}\n```"], "62fa0d1390e50fcafd246228": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in performing representation learning on attributed multiplex heterogeneous networks (AMHENs). The proposed method, MHGCN, aims to tackle the issues of automated meta-path learning, effective meta-path dependency encoding, and efficiency in handling large-scale network data. The authors introduce a novel approach that decouples the multiplex network into multiple sub-networks and re-aggregates them, alongside a multilayer graph convolution module to capture meta-path information.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b26\": 1.0,\n    \"b19\": 1.0,\n    \"b36\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b22\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of performing representation learning on attributed multiplex heterogeneous networks (AMHENs) which have complex network structures and node attributes. It introduces a new Multiplex Heterogeneous Graph Convolutional Network (MHGCN) that decouples the multiplex network into multiple sub-networks and re-aggregates them. The MHGCN is designed to automatically capture meta-path information across multi-relations and improve model efficiency with a simplified convolution-based message passing mechanism.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b26\": 0.9,\n    \"b29\": 0.8,\n    \"b4\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b22\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b32\": 0.6,\n    \"b35\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in performing representation learning on attributed multiplex heterogeneous networks (AMHENs). It proposes a new Multiplex Heterogeneous Graph Convolutional Network (MHGCN) to address these challenges by decoupling the network into sub-networks and re-aggregating them, and by designing a multilayer graph convolution module to capture meta-path information efficiently.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b19\": 0.9,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b15\": 0.8,\n    \"b29\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b22\": 0.7,\n    \"b36\": 0.7,\n    \"b40\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of performing representation learning on attributed multiplex heterogeneous networks (AMHENs), which have complex network structures and node attributes. The proposed solution, MHGCN, aims to capture meta-path information automatically and efficiently learn network representations by decoupling the multiplex network into homogeneous and bipartite sub-networks, and then re-aggregating them using a multilayer graph convolution module.\",\n    \"Direct Inspiration\": {\n        \"b3\": 1.0,\n        \"b26\": 1.0,\n        \"b19\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.9,\n        \"b39\": 0.9,\n        \"b22\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.7,\n        \"b13\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of performing representation learning on Attributed Multiplex Heterogeneous Networks (AMHENs), which involve complex network structures with multi-typed nodes and multiplex relationships. The proposed MHGCN framework aims to automatically capture meta-path information across multi-relations and improve model efficiency through a simplified graph convolution method.\",\n  \"Direct Inspiration\": [\n    \"b3\",\n    \"b26\",\n    \"b4\",\n    \"b39\",\n    \"b19\",\n    \"b22\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b29\",\n    \"b30\",\n    \"b0\"\n  ],\n  \"Other Inspiration\": [\n    \"b15\",\n    \"b33\",\n    \"b6\",\n    \"b28\",\n    \"b25\",\n    \"b8\",\n    \"b31\"\n  ]\n}\n```"], "634d805490e50fcafd4e02c8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning effective representations for electronic circuit netlists, highlighting the limitations of existing methods that rely solely on structural information. It introduces a novel contrastive learning-based framework combined with a customized graph neural network (GNN) to encode the functional semantics of netlists, aiming to improve generalization to unseen designs.\",\n  \"Direct Inspiration\": [\"b5\", \"b22\", \"b23\"],\n  \"Indirect Inspiration\": [\"b6\", \"b7\", \"b8\", \"b17\", \"b18\", \"b19\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is the inadequacy of existing netlist representation learning methods that focus solely on structural information, ignoring the boolean functionality critical for understanding the semantics of netlists. The proposed solution is a novel contrastive learning-based framework for netlist representation learning, coupled with a customized graph neural network architecture (FGNN) that encodes functional information of gate-level netlists, thereby improving generalization to unseen designs.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b7\": 0.8,\n    \"b15\": 0.75,\n    \"b17\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b6\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of representing electronic circuit netlists by proposing a novel contrastive learning-based framework and a customized graph neural network (FGNN) to capture the functional semantics of netlists, thereby improving generalization to unseen designs.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b17\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of learning representations for electronic circuit netlists, emphasizing the need to capture both structural and functional information. It proposes a novel contrastive learning-based framework combined with a customized graph neural network (FGNN) to achieve better generalization and performance in netlist representation learning.\",\n  \"Direct Inspiration\": [\"b5\"],\n  \"Indirect Inspiration\": [\"b4\", \"b17\", \"b18\", \"b20\", \"b21\", \"b22\"],\n  \"Other Inspiration\": [\"b3\", \"b7\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of representation learning for electronic circuit netlists, proposing a novel contrastive learning-based framework and a customized graph neural network (GNN) to capture the functional semantics of netlists, rather than just their structural information.\",\n  \"Direct Inspiration\": [\"b5\"],\n  \"Indirect Inspiration\": [\"b0\", \"b4\"],\n  \"Other Inspiration\": [\"b3\", \"b7\", \"b12\", \"b13\"]\n}\n```"], "63a413f790e50fcafd6d24b3": ["```json\n{\n  \"Summary\": \"The primary challenge in the paper is to learn invariant graph representations under distribution shifts, which is crucial for achieving out-of-distribution (OOD) generalization. The proposed Graph Invariant Learning method (GIL) aims to capture invariant graph patterns in a mixture of latent environments and generate graph representations capable of OOD generalization. The method involves three main modules: invariant subgraph identification, environment inference, and invariant learning.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b8\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b11\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning graph representations under distribution shifts, which is critical for applications such as medical diagnosis and financial analysis. The authors propose the Graph Invariant Learning method (GIL) to capture invariant graph patterns in a mixture of latent environments, enabling out-of-distribution (OOD) generalization. The methodology involves three modules: invariant subgraph identification, environment inference using variant subgraphs, and invariant learning to optimize the maximal invariant subgraph generator criterion.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b8\": 1,\n    \"b9\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of out-of-distribution (OOD) generalization for graph representation learning, focusing on invariant learning to capture invariant graph patterns amid distribution shifts. The proposed method, Graph Invariant Learning (GIL), identifies invariant subgraphs, infers environment labels from variant subgraphs, and optimizes a maximal invariant subgraph generator to achieve robust OOD generalization.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b8\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b11\": 0.8,\n    \"b12\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning graph representations that generalize well under distribution shifts, which is critical for high-stake applications like medical diagnosis and financial analysis. The authors propose a novel Graph Invariant Learning (GIL) method that captures invariant graph patterns from a mixture of latent environments to achieve out-of-distribution (OOD) generalization. The GIL method includes an invariant subgraph identification module, an environment inference module, and an invariant learning module.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of learning invariant graph representations under distribution shifts, which is crucial for applications like medical diagnosis and financial analysis. The proposed Graph Invariant Learning (GIL) method aims to capture invariant graph patterns from a mixture of latent environments and achieve out-of-distribution (OOD) generalization. The method involves three key modules: invariant subgraph identification, environment inference by clustering variant subgraphs, and optimizing the maximal invariant subgraph generator criterion.\",\n    \"Direct Inspiration\": {\n        \"b7\": 1.0,\n        \"b8\": 1.0,\n        \"b10\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.8,\n        \"b11\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b14\": 0.5\n    }\n}\n```"], "62d16e8a5aee126c0fd6847a": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the substantial performance overhead in nested address translation in virtualized systems. The proposed solution, Nested ECPTs, aims to speed up nested address translation by exploiting parallelism through the use of Elastic Cuckoo Page Tables (ECPTs) and additional hardware caches like the Shortcut Translation Cache (STC).\",\n  \"Direct Inspiration\": {\n    \"b77\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b87\": 0.8,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.6,\n    \"b42\": 0.6,\n    \"b49\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the substantial performance overhead introduced by address translation in virtualized systems, particularly focusing on nested address translation. The proposed solution is Nested ECPTs, which leverages parallelism and introduces hardware caches to reduce the number of memory accesses required during address translation.\",\n  \"Direct Inspiration\": {\n    \"b77\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b87\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b39\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of high overhead in nested address translation in virtualized environments, proposing Nested Elastic Cuckoo Page Tables (ECPTs) to exploit parallelism and reduce the number of memory accesses. The design introduces new hardware caches, such as the Shortcut Translation Cache (STC), and optimizes the caching of metadata to improve performance.\",\n    \"Direct Inspiration\": {\n        \"b77\": 1,\n        \"b87\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.8,\n        \"b25\": 0.8,\n        \"b34\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.7,\n        \"b39\": 0.7,\n        \"b42\": 0.7,\n        \"b48\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of high overhead in nested address translation in virtualized environments. The proposed solution, Nested ECPTs, leverages parallelism through hashed page tables to reduce the number of sequential memory accesses required for address translation. Key innovations include the Shortcut Translation Cache (STC) and adaptive caching strategies to minimize bandwidth consumption.\",\n    \"Direct Inspiration\": [\"b15\", \"b77\"],\n    \"Indirect Inspiration\": [\"b1\", \"b13\", \"b25\", \"b34\", \"b87\"],\n    \"Other Inspiration\": [\"b6\", \"b31\", \"b32\", \"b55\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing the substantial performance overhead caused by address translation in virtualized systems. The authors propose a novel method called Nested ECPTs, which exploits parallelism to speed up nested address translation.\",\n  \"Direct Inspiration\": {\n    \"b77\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b48\": 0.7,\n    \"b87\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b25\": 0.6,\n    \"b34\": 0.6,\n    \"b39\": 0.6,\n    \"b42\": 0.6\n  }\n}\n```"], "628704275aee126c0f5b583a": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the inefficiency of traditional page walks in systems with large memory capacities, leading to significant TLB misses and delays. The proposed solution involves flattening the page table to reduce memory indirections and prioritizing the caching of page table entries to decrease latency.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.95,\n    \"b3\": 0.90,\n    \"b46\": 0.85,\n    \"b33\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.75,\n    \"b47\": 0.70,\n    \"b4\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.60,\n    \"b40\": 0.55,\n    \"b39\": 0.50,\n    \"b51\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiency of traditional page walks with large datasets, proposing a solution to flatten the page table and prioritize caching page table entries to reduce latency. The primary challenges are the significant overheads of traditional page table trees and the latency incurred from multiple memory indirections during page walks. The proposed algorithm combines flattening the page table structure and prioritizing caching page table entries to reduce the number of memory accesses and the latency of each access.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.95,\n    \"b24\": 0.90,\n    \"b33\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.75,\n    \"b34\": 0.70,\n    \"b46\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.60,\n    \"b47\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inefficiency of traditional page walks with large data sets, leading to significant TLB misses and resulting delays. The novel method proposed is to flatten the page table to reduce indirections and prioritize caching the page table entries to reduce latency.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b24\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.75,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.65,\n    \"b47\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies of traditional page table walks in systems with large memory capacities, particularly in virtualized environments. The main contributions include flattening the page table to reduce the number of memory accesses and prioritizing caching of page table entries to reduce latency. These methods significantly improve the performance and energy efficiency of page table walks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b15\": 0.8,\n    \"b24\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b40\": 0.75,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.65,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of traditional page walks not scaling well with large datasets, leading to significant TLB misses and page walk delays. The proposed solution involves flattening the page table to reduce indirections and prioritizing caching page table entries to reduce latency, significantly improving performance in both non-virtualized and virtualized environments.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b24\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.75,\n    \"b33\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.65,\n    \"b40\": 0.6\n  }\n}\n```"], "628d27f85aee126c0f4de037": ["```json\n{\n  \"Summary\": \"The paper addresses the problem of front-end bottlenecks in server workloads caused by instruction cache and BTB misses. It introduces 'Shotgun,' a BTB-directed front-end prefetcher that uses a new BTB organization to efficiently track instruction working sets and improve prefetching accuracy and timeliness. Shotgun focuses on utilizing the BTB to map the program's instruction footprint, dedicating most of the BTB capacity to unconditional branches and their spatial footprints, and maintaining conditional branches in a smaller, separate BTB.\",\n  \"Direct Inspiration\": [\"b8\", \"b14\"],\n  \"Indirect Inspiration\": [\"b13\"],\n  \"Other Inspiration\": [\"b6\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the front-end bottleneck in server workloads due to large instruction working sets. The authors propose a novel BTB-directed front-end prefetcher called Shotgun, which leverages a new BTB organization to efficiently track and prefetch both instruction cache and branch target buffer entries. Shotgun focuses on global control flow through unconditional branches and uses spatial footprints to prefetch local control flow regions, significantly improving prefetch accuracy and reducing stalls.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b8\": 0.8,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The main challenge addressed by the paper is overcoming the front-end bottleneck in server workloads caused by instruction cache and BTB misses. The proposed solution, Shotgun, is a BTB-directed front-end prefetcher that uses a specialized BTB organization to track global control flow through unconditional branches and their spatial footprints, thereby enabling more effective and timely prefetching.\",\n    \"Direct Inspiration\": {\n        \"b8\": 0.95,\n        \"b13\": 0.90,\n        \"b14\": 0.90\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.80,\n        \"b10\": 0.75,\n        \"b11\": 0.75,\n        \"b12\": 0.70,\n        \"b15\": 0.70\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the front-end bottleneck in server workloads caused by large instruction working sets and branch prediction misses. It introduces Shotgun, a BTB-directed instruction cache and BTB prefetcher. Shotgun improves upon existing methods by using a specialized BTB organization that focuses on unconditional branches and their spatial footprints, enabling more effective prefetching with limited storage.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b13\": 0.8,\n    \"b14\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b11\": 0.65,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of mitigating the front-end bottleneck in server workloads caused by large instruction working sets and frequent BTB misses. It proposes a novel BTB-directed prefetcher called Shotgun, which introduces a specialized BTB organization to efficiently track control flow and prefetch instructions.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b13\", \"b14\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b8\", \"b6\", \"b12\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b11\"]\n  }\n}\n```"], "632630ff90e50fcafdf67484": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the difficulties of simulating large, multi-threaded applications due to synchronization complexities and the long simulation times required by existing methods. The paper proposes the LoopPoint methodology, which reduces applications to representative regions called looppoints, focusing on loop iterations as the unit of work. Key contributions include a methodology for selecting representative simulation regions, a technique for filtering out spin-loops, the development of a process for accurate analysis and unconstrained simulation, and a comprehensive evaluation of the methodology's speed and accuracy.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b10\": 1,\n    \"b11\": 1,\n    \"b12\": 1,\n    \"b13\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.9,\n    \"b14\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b21\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the difficulty of simulating large, multi-threaded applications and the limitations of existing sampling methodologies in handling synchronization and application diversity. The proposed LoopPoint methodology addresses these challenges by using loop iterations as the unit of work, filtering out spin-loops, and employing constrained checkpoints for accurate simulation. This approach aims to provide a synchronization-agnostic sampling method that scales with the application characteristics.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in this paper is the difficulty of simulating large, multi-threaded applications efficiently and accurately. The authors propose the LoopPoint methodology, which aims to reduce an application to a few representative regions called looppoints. This approach involves understanding where to simulate through accurate analysis and precise clustering, filtering out spin-loops, and using loop iterations as the unit of work. The methodology also includes capturing constrained application checkpoints for accurate analysis and using both binary-driven and checkpoint-driven simulations to achieve fast and accurate workload evaluation.\",\n  \"Direct Inspiration\": [\"b0\", \"b1\", \"b9\", \"b10\", \"b11\", \"b12\", \"b13\"],\n  \"Indirect Inspiration\": [\"b16\", \"b18\", \"b21\"],\n  \"Other Inspiration\": [\"b15\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of simulating large, multi-threaded applications, which are difficult due to thread synchronization and interference. The proposed LoopPoint methodology aims to provide a synchronization-agnostic sampling solution using loop iterations as the unit of work to reduce the workload into representative components, accurately extrapolate performance characteristics, and simulate unconstrained behavior for accurate analysis.\",\n  \"Direct Inspiration\": [\"b9\", \"b10\", \"b11\", \"b12\", \"b13\"],\n  \"Indirect Inspiration\": [\"b4\", \"b5\", \"b6\", \"b7\", \"b8\"],\n  \"Other Inspiration\": [\"b0\", \"b1\", \"b14\", \"b15\", \"b16\", \"b18\", \"b19\", \"b20\", \"b21\", \"b22\", \"b23\", \"b24\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the difficulties in simulating large, multi-threaded applications due to synchronization issues, complexity, and simulation time. The LoopPoint methodology is proposed to overcome these limitations by reducing an application to representative regions called looppoints, using loop iterations as the unit of work. The methodology focuses on accurate region selection, filtering out spin-loops, and enabling unconstrained simulation for accurate workload evaluation.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b1\": 1,\n    \"b9\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b19\": 0.6,\n    \"b21\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```"], "62d16e8a5aee126c0fd684cf": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of fully exploiting the Channel-Level Parallelism (CLP) offered by 3D memory. The proposed Software-Defined Address Mapping (SDAM) combines hardware and software mechanisms to achieve fine-grained data placement and support multiple access patterns. This approach includes a runtime memory allocator, machine learning for access pattern identification, and necessary modifications to the memory controller and system software.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b16\": 0.85,\n    \"b29\": 0.8,\n    \"b48\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b13\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b36\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of effectively utilizing the Channel-Level Parallelism (CLP) in 3D-stacked memory by proposing Software-Defined Address Mapping (SDAM). SDAM is a collaborative hardware-software technique that allows fine-grained data placement to fully exploit the CLP. It combines the strengths of hardware-only and software-only methods to adapt address mappings to different data access patterns, ensuring higher bandwidth utilization and performance.\",\n    \"Direct Inspiration\": {\n        \"b0\": 0.9,\n        \"b22\": 0.8,\n        \"b36\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.7,\n        \"b13\": 0.7,\n        \"b16\": 0.7,\n        \"b29\": 0.7,\n        \"b48\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b35\": 0.6,\n        \"b45\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of fully exploiting the Channel-Level Parallelism (CLP) in 3D-stacking memory to improve memory bandwidth utilization. The proposed solution, Software-Defined Address Mapping (SDAM), is a cooperative hardware-software mechanism that enables fine-grained data placement control. SDAM leverages machine learning to automatically learn access patterns of variables and uses chunk-based address mapping to manage address mappings at a coarse-grained chunk level while achieving fine-grained data placement in hardware. The paper demonstrates the effectiveness of SDAM on an FPGA-based full-system prototyping platform, achieving significant performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b29\": 0.8,\n    \"b7\": 0.7,\n    \"b13\": 0.7,\n    \"b22\": 0.9,\n    \"b36\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b48\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the critical performance bottleneck in data centers caused by DDR-based main memory's inability to keep pace with increasing data and processor technology trends. It proposes a Software-Defined Address Mapping (SDAM) that combines hardware-software mechanisms to fully exploit the Channel-Level Parallelism (CLP) in 3D memory. Key contributions include the introduction of SDAM, necessary architectural support, machine learning methods for optimization, and demonstration of benefits on FPGA-based hardware.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b36\": 1,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b16\": 0.8,\n    \"b29\": 0.8,\n    \"b48\": 0.8,\n    \"b7\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b21\": 0.6,\n    \"b28\": 0.6,\n    \"b45\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the limitations of DDR-based main memory in handling high bandwidth demands due to increasing processor core counts and data-intensive applications. The proposed solution, Software-Defined Address Mapping (SDAM), aims to exploit the Channel-Level Parallelism (CLP) offered by 3D-stacking memory technologies such as HBM and HMC. SDAM is a collaborative hardware-software mechanism that allows fine-grained data placement based on access patterns, achieved through a runtime memory allocator and architectural support in the memory controller.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b7\": 0.9,\n    \"b13\": 0.9,\n    \"b16\": 0.9,\n    \"b29\": 0.9,\n    \"b48\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b22\": 0.8,\n    \"b35\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b19\": 0.6,\n    \"b21\": 0.6,\n    \"b23\": 0.6,\n    \"b28\": 0.6,\n    \"b41\": 0.6\n  }\n}\n```"], "628704555aee126c0f5c7953": ["```json\n{\n  \"Summary\": \"The paper addresses the Von-Neumann bottleneck and the difficulties of memory access latency in modern applications. It proposes a novel profile-guided mechanism, APT-GET, to optimize software prefetch timeliness by determining the optimal prefetch-distance and prefetch injection site. The approach aims to significantly improve performance over existing static software prefetching techniques.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b26\": 0.7,\n    \"b36\": 0.7,\n    \"b55\": 0.6,\n    \"b64\": 0.7,\n    \"b84\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b104\": 0.5,\n    \"b110\": 0.5,\n    \"b117\": 0.5,\n    \"b118\": 0.5,\n    \"b111\": 0.4,\n    \"b66\": 0.4,\n    \"b43\": 0.3,\n    \"b95\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the Von-Neumann bottleneck, the increasing data footprints of modern applications, and the limitations of existing software prefetching mechanisms in terms of timeliness. The paper proposes APT-GET, a profile-guided mechanism to ensure the timeliness of software prefetch operations by optimizing prefetch-distance and prefetch injection site.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b66\": 0.8,\n    \"b115\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b48\": 0.6,\n    \"b62\": 0.6,\n    \"b92\": 0.6,\n    \"b93\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the issue of prefetching timeliness in automated software data prefetching mechanisms. The authors propose a novel profile-guided mechanism called APT-GET to optimize the timeliness of software prefetch operations by determining optimal prefetch-distance and prefetch injection site. The proposed approach leverages dynamic profiling information to improve the effectiveness of software prefetching, achieving significant performance improvements over state-of-the-art techniques.\",\n    \"Direct Inspiration\": {\n        \"b5\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b111\": 0.8,\n        \"b66\": 0.8,\n        \"b67\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5,\n        \"b123\": 0.5,\n        \"b14\": 0.5,\n        \"b74\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of the memory latency caused by the Von-Neumann bottleneck in modern software systems. It develops APT-GET, a novel profile-guided mechanism to ensure the timeliness of software prefetch operations by identifying optimal prefetch-distance and prefetch injection site. The primary inspiration is derived from existing automated software data prefetching mechanisms and their limitations.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b26\": 0.7,\n    \"b36\": 0.7,\n    \"b55\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b10\": 0.6,\n    \"b30\": 0.6,\n    \"b123\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance challenges caused by the Von-Neumann bottleneck and the limitations of existing software prefetching techniques in handling memory access latency. The authors propose APT-GET, a novel profile-guided mechanism that optimizes software prefetching by determining the optimal prefetch-distance and prefetch injection site through dynamic profiling. This approach aims to achieve timely prefetching and improve performance in memory-latency-bound applications.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b44\": 0.8,\n    \"b66\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b10\": 0.6,\n    \"b61\": 0.6,\n    \"b79\": 0.6,\n    \"b104\": 0.6\n  }\n}\n```"], "6287044a5aee126c0f5c33ff": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the reliability of uncertainty estimates in multimodal stress detection models under distribution shifts. The authors propose and evaluate three approaches: deep ensembles, Mixup, and Focal Loss, in addition to a baseline deep neural network.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b25\": 1.0,\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the calibration and uncertainty quantification of machine learning models for multimodal stress detection, especially under distribution shifts. The authors propose and evaluate three methods: deep ensembles, Focal Loss, and Manifold Mixup, against a baseline model, focusing on their performance in terms of calibration error and uncertainty estimation reliability.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b25\": 1.0,\n    \"b33\": 1.0,\n    \"b29\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of reliability in uncertainty estimates for multimodal stress detection under data distribution shifts. The proposed algorithm explores deep ensembles, Mixup, and Focal Loss to improve calibration and uncertainty estimates compared to a baseline deep neural network.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b25\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the robustness and reliability of uncertainty estimates in multimodal stress detection models under distribution shifts. The paper proposes approaches like deep ensembles, Mixup, and Focal Loss to improve the calibration of uncertainty estimates.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b25\": 1.0,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b7\": 0.8,\n    \"b26\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of estimating affective states like stress using multimodal data in the presence of distribution shifts. It aims to improve the calibration and reliability of uncertainty estimates in machine learning models for this purpose. The authors propose and evaluate deep ensembles, Mixup, and Focal Loss as methods to tackle this problem.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.95,\n    \"b25\": 0.90,\n    \"b33\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.80,\n    \"b29\": 0.75,\n    \"b32\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.65,\n    \"b24\": 0.60\n  }\n}\n```"], "62d7a7d15aee126c0f3fe03e": ["```json\n{\n  \"Summary\": \"The paper addresses significant stalls in the frontend of the processor pipeline caused by large instruction footprints in modern data center applications. It introduces 'Thermometer,' a novel BTB replacement technique leveraging both holistic and transient branch patterns to improve instruction cache utilization and reduce frontend stalls.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.95,\n    \"b59\": 0.90,\n    \"b61\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b72\": 0.75,\n    \"b82\": 0.75,\n    \"b26\": 0.70,\n    \"b105\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve mitigating frontend stalls in data center applications due to large instruction footprints, which lead to frequent BTB misses. The paper proposes Thermometer, a novel BTB replacement technique that combines holistic and transient branch behavior to achieve near-optimal BTB performance.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b59\": 1,\n    \"b61\": 1,\n    \"b72\": 0.8,\n    \"b82\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.9,\n    \"b74\": 0.85,\n    \"b83\": 0.8,\n    \"b131\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b26\": 0.7,\n    \"b66\": 0.7,\n    \"b105\": 0.7,\n    \"b106\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the significant performance losses in data center applications due to large instruction footprints causing frontend stalls in the processor pipeline. It proposes a novel BTB replacement technique called Thermometer, which considers both holistic and transient branch patterns to make near-optimal replacement decisions, thereby improving performance significantly.\",\n  \"Direct Inspiration\": [\"b19\", \"b59\", \"b61\"],\n  \"Indirect Inspiration\": [\"b72\", \"b82\"],\n  \"Other Inspiration\": [\"b28\", \"b60\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the significant performance losses in data center applications caused by large instruction footprints leading to frontend stalls. It proposes a novel BTB replacement technique, Thermometer, which leverages both holistic and transient branch behaviors to make near-optimal replacement decisions and significantly reduce BTB misses.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b59\": 1,\n    \"b61\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.9,\n    \"b72\": 0.8,\n    \"b82\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.7,\n    \"b74\": 0.7,\n    \"b105\": 0.8,\n    \"b106\": 0.8,\n    \"b131\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the frontend stalls in processors due to large instruction footprints of modern data center applications. The novel approach introduced is the Thermometer BTB replacement technique, which combines holistic and transient branch behavior to make near-optimal BTB replacement decisions.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b59\": 1,\n    \"b61\": 1,\n    \"b72\": 0.9,\n    \"b82\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 1,\n    \"b74\": 0.9,\n    \"b131\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.8,\n    \"b105\": 0.8,\n    \"b106\": 0.8\n  }\n}\n```"], "62d7a7d15aee126c0f3fe05a": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency of current memory hierarchies in optimizing data movement due to a lack of software visibility and control. The proposed solution is a polymorphic cache hierarchy realized through the t\u00e4k\u014d architecture, which enables software-defined data movement optimizations via cache-triggered callbacks.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b82\": 0.9,\n    \"b104\": 0.9,\n    \"b141\": 0.9,\n    \"b149\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.7,\n    \"b58\": 0.7,\n    \"b102\": 0.7,\n    \"b131\": 0.7,\n    \"b137\": 0.7,\n    \"b142\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b94\": 0.6,\n    \"b91\": 0.6,\n    \"b90\": 0.6,\n    \"b80\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing data movement in current systems where software lacks visibility and control over data movement. The proposed solution is a polymorphic cache hierarchy named t\u00e4k\u014d, which allows software to control and optimize data movement through cache-triggered callbacks, thus reducing the need for custom hardware and enabling significant performance and energy improvements.\",\n  \"Direct Inspiration\": [\"b5\", \"b82\", \"b104\", \"b141\", \"b149\"],\n  \"Indirect Inspiration\": [\"b42\", \"b58\", \"b102\", \"b131\", \"b137\", \"b142\"],\n  \"Other Inspiration\": [\"b94\", \"b91\", \"b90\", \"b80\", \"b8\", \"b35\", \"b89\", \"b105\", \"b106\", \"b117\", \"b135\", \"b145\", \"b46\", \"b64\", \"b114\", \"b7\", \"b39\", \"b152\", \"b153\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the inefficiency of current memory hierarchies, which limits software's ability to optimize data movement due to a lack of visibility and control over these movements. The paper proposes a new architecture, t\u00e4k\u014d, which exposes data movement to software through a polymorphic cache hierarchy, enabling software to implement optimizations that otherwise require custom hardware.\",\n    \"Direct Inspiration\": {\n        \"b5\": 0.9,\n        \"b82\": 0.9,\n        \"b104\": 0.85,\n        \"b141\": 0.85,\n        \"b149\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b42\": 0.8,\n        \"b58\": 0.8,\n        \"b102\": 0.8,\n        \"b131\": 0.8,\n        \"b137\": 0.8,\n        \"b142\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.75,\n        \"b80\": 0.75,\n        \"b90\": 0.75,\n        \"b91\": 0.75,\n        \"b94\": 0.75\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the lack of visibility and control over data movement in current systems, leading to conservative and wasteful solutions. The proposed solution is t\u00e4k\u014d, a polymorphic cache hierarchy that exposes data movement to software, enabling optimizations that would normally require custom hardware.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b82\": 0.9,\n    \"b104\": 0.8,\n    \"b141\": 0.8,\n    \"b149\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.7,\n    \"b58\": 0.7,\n    \"b102\": 0.7,\n    \"b131\": 0.7,\n    \"b137\": 0.7,\n    \"b142\": 0.7,\n    \"b90\": 0.6,\n    \"b94\": 0.6,\n    \"b91\": 0.6,\n    \"b80\": 0.6,\n    \"b20\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b35\": 0.5,\n    \"b89\": 0.5,\n    \"b105\": 0.5,\n    \"b106\": 0.5,\n    \"b117\": 0.5,\n    \"b135\": 0.5,\n    \"b145\": 0.5,\n    \"b46\": 0.4,\n    \"b64\": 0.4,\n    \"b114\": 0.4,\n    \"b130\": 0.4,\n    \"b148\": 0.4,\n    \"b7\": 0.3,\n    \"b39\": 0.3,\n    \"b152\": 0.3,\n    \"b153\": 0.3,\n    \"b107\": 0.3,\n    \"b88\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inefficient data movement in current memory hierarchies due to the lack of software control. It proposes the t\u00e4k\u014d architecture, a polymorphic cache hierarchy that allows software to observe and optimize data movement through cache-triggered callbacks. This approach aims to break the innovation deadlock by providing a general-purpose architecture that supports various data-movement features and optimizations, reducing the need for custom hardware.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b82\": 0.9,\n    \"b104\": 0.9,\n    \"b141\": 0.9,\n    \"b149\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.7,\n    \"b58\": 0.7,\n    \"b102\": 0.7,\n    \"b131\": 0.7,\n    \"b137\": 0.7,\n    \"b142\": 0.7,\n    \"b94\": 0.8,\n    \"b91\": 0.8,\n    \"b90\": 0.8,\n    \"b80\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b35\": 0.6,\n    \"b89\": 0.6,\n    \"b105\": 0.6,\n    \"b106\": 0.6,\n    \"b117\": 0.6,\n    \"b135\": 0.6,\n    \"b145\": 0.6,\n    \"b46\": 0.6,\n    \"b64\": 0.6,\n    \"b114\": 0.6,\n    \"b130\": 0.6,\n    \"b148\": 0.6,\n    \"b7\": 0.6,\n    \"b39\": 0.6,\n    \"b152\": 0.6,\n    \"b153\": 0.6\n  }\n}\n```"], "62d7a7d15aee126c0f3fe03d": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of creating low-cost, thin, and conformal computing solutions for applications where traditional silicon-based electronics are limited. It proposes and tests flexible microprocessors, named FlexiCores, designed to be low footprint and high yielding using metal-oxide TFT technology. The novel approach includes optimizing ISA and microarchitecture for low gate count and high yield, demonstrating the feasibility of manufacturing flexible microprocessors at scale, and presenting performance and energy data over multiple applications.\",\n    \"Direct Inspiration\": [\n        \"b3\",\n        \"b34\",\n        \"b69\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b12\",\n        \"b42\",\n        \"b53\",\n        \"b87\",\n        \"b94\",\n        \"b74\"\n    ],\n    \"Other Inspiration\": [\n        \"b67\",\n        \"b38\",\n        \"b97\",\n        \"b41\",\n        \"b56\",\n        \"b79\",\n        \"b10\"\n    ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of manufacturing low-cost, natively flexible microprocessors with high yield and low footprint, optimized for applications requiring thinness, conformality, and low power consumption. The authors propose FlexiCores, optimized 4-bit and 8-bit flexible microprocessors, and demonstrate their feasibility, performance, and energy efficiency. The work also includes a design space exploration to identify efficient design points for flexible microprocessors.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.95,\n    \"b69\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.8,\n    \"b74\": 0.75,\n    \"b77\": 0.7,\n    \"b56\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.65,\n    \"b42\": 0.65,\n    \"b53\": 0.65,\n    \"b87\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of silicon-based electronics in terms of thinness, conformality, and cost, particularly for applications requiring ultra-low-cost, flexibility, and high yield. It introduces FlexiCores, a family of natively flexible, low gate-count microprocessors optimized for low footprint, high yield, and energy efficiency. The work demonstrates the feasibility of manufacturing flexible microprocessors at scale, supports multiple applications, and presents design space explorations for further optimizations.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.7,\n    \"b69\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b10\": 0.6,\n    \"b38\": 0.6,\n    \"b97\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing low-cost, natively flexible microprocessors that meet the needs for ultra-low-cost, thinness, and conformality in various applications. The proposed solution includes the design, fabrication, and testing of FlexiCore processors optimized for footprint and yield. The paper demonstrates the feasibility of building flexible processors at high yield and presents performance and energy data for these processors. The work also includes a design space exploration to identify optimal design points for flexible processors.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b34\": 0.9,\n    \"b69\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b67\": 0.7,\n    \"b79\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b60\": 0.6,\n    \"b101\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the limitations of silicon-based electronics in terms of thinness, conformality, and cost, which prevent their use in ultra-low-cost applications such as wearable patches, smart bandages, and disposable sensors. The paper proposes flexible microprocessors (FlexiCores) using 0.8 \u00b5m IGZO-based TFTs to address these challenges. The main contributions include designing, fabricating, and testing natively flexible 4-bit and 8-bit microprocessors optimized for low footprint and high yield, demonstrating their feasibility for commercial volume production, and presenting performance, energy data, and process variation data of FlexiCores.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b69\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.8,\n    \"b94\": 0.7,\n    \"b67\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.5,\n    \"b29\": 0.6,\n    \"b71\": 0.7\n  }\n}\n```"], "62d7a7d15aee126c0f3fe044": ["```json\n{\n  \"Summary\": \"This paper proposes X-Cache, an architectural template for domain-specific caches that addresses the challenges of dynamic access patterns and irregular data structures in Domain-Specific Accelerators (DSAs). The key novel contributions include the introduction of DSA-specific tags (meta-tags), DSA-programmable walking (X-Routine), and a DSA-agnostic controller architecture using coroutines.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.8,\n    \"b34\": 0.85,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.75,\n    \"b29\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing domain-specific caches (DSAs) that handle dynamic access patterns and irregular data structures. The authors introduce X-Cache, a novel architectural template that includes domain-specific meta-tags, programmable walkers, and a DSA-agnostic controller architecture. The primary contributions are the reusable caching idiom, generalization of meta-tags, and implementation of coroutines for parallelism.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b29\", \"b34\", \"b36\"],\n    \"phrases\": [\"include fields that mirror the functionality of meta-tags\", \"can use the same X-Cache microarchitecture\", \"we are the first to define and generalize the concept\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b4\", \"b5\"],\n    \"phrases\": [\"used blocking threads\", \"used threads for static access patterns\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": []\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Handling non-affine data structures with minimal DRAM footprint.\",\n      \"Managing dynamic and irregular access patterns in DSAs.\",\n      \"Designing efficient data walkers for preloading caches.\",\n      \"Explicit orchestration of cache replacement and refill mechanisms.\"\n    ],\n    \"Inspirations\": [\n      \"Exploiting extreme locality for energy efficiency.\",\n      \"Creating a reusable caching idiom for dynamic access patterns and irregular data structures in DSAs.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b34\": 1.0,\n    \"b36\": 1.0,\n    \"b29\": 0.9,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b13\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.4,\n    \"b6\": 0.4,\n    \"b7\": 0.4,\n    \"b9\": 0.3,\n    \"b12\": 0.3,\n    \"b16\": 0.3,\n    \"b20\": 0.3,\n    \"b27\": 0.3,\n    \"b28\": 0.3,\n    \"b30\": 0.3,\n    \"b31\": 0.3\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of integrating domain-specific caches (DSAs) with dynamic and irregular access patterns. The proposed solution, X-Cache, introduces novel concepts like meta-tags and programmable walkers to enhance performance and energy efficiency. Key contributions include a reusable caching idiom, domain-specific meta-tags, and a programmable cache controller.\",\n    \"Direct Inspiration\": {\n        \"b34\": 1.0,\n        \"b36\": 1.0,\n        \"b29\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b17\": 0.8,\n        \"b4\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.6,\n        \"b31\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Exploiting extreme locality, making fewer DRAM accesses, and maximizing bandwidth utilization\",\n      \"Handling non-affine data structures and dynamic accesses\",\n      \"Creating custom walkers and managing cache replacement and refill\"\n    ],\n    \"inspirations\": [\n      \"Developing an architectural template for domain-specific caches\",\n      \"Generalizing the concept of domain-specific tags (meta-tags)\",\n      \"Implementing coroutines for controller architecture\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b5\": 1,\n    \"b34\": 1,\n    \"b36\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b13\": 0.8,\n    \"b29\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b9\": 0.6,\n    \"b12\": 0.6,\n    \"b16\": 0.6,\n    \"b30\": 0.6,\n    \"b3\": 0.6,\n    \"b27\": 0.6,\n    \"b20\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```"], "62d7a7d15aee126c0f3fe059": ["```json\n{\n  \"Summary\": \"The paper introduces Jukebox, an instruction prefetcher designed to address the performance degradation of serverless functions due to 'lukewarm' executions. The main challenge is the obliteration of on-chip microarchitectural state between function invocations, resulting in high instruction fetch latency. Jukebox records instruction cache misses during an initial invocation and replays these accesses in subsequent invocations to prefetch instructions into the L2 cache.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b27\": 1.0,\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the performance degradation in serverless computing, specifically the 'lukewarm execution' phenomenon where serverless functions experience significant performance loss due to the loss of microarchitectural state between invocations. The authors propose Jukebox, a record-and-replay instruction prefetcher, to mitigate this issue by prefetching instructions into L2 cache, thus accelerating lukewarm executions.\",\n    \"Direct Inspiration\": {\n        \"b15\": 1,\n        \"b27\": 1,\n        \"b28\": 1\n    },\n    \"Indirect Inspiration\": {},\n    \"Other Inspiration\": {\n        \"b6\": 0.8,\n        \"b32\": 0.8,\n        \"b42\": 0.7,\n        \"b48\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper revolve around the performance degradation of serverless functions due to lukewarm executions caused by high degrees of interleaving and the obliteration of on-chip microarchitectural states. The paper proposes Jukebox, a record-and-replay instruction prefetcher, to address these challenges by reducing instruction fetch latency and improving execution times for lukewarm functions.\",\n    \"Direct Inspiration\": {\n        \"b15\": 0.9,\n        \"b27\": 0.9,\n        \"b28\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.7,\n        \"b10\": 0.7,\n        \"b11\": 0.7,\n        \"b14\": 0.7,\n        \"b42\": 0.7,\n        \"b48\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b51\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performance degradation in lukewarm executions of serverless functions due to obliterated on-chip microarchitectural state. It proposes Jukebox, a record-and-replay instruction prefetcher to accelerate these executions by storing metadata in main memory and prefetching instructions into the L2 cache.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b27\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b42\": 0.6,\n    \"b48\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b14\": 0.5,\n    \"b19\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the performance degradation of serverless functions due to lukewarm executions, where the function instances have their on-chip microarchitectural state obliterated between invocations due to high interleaving. The proposed solution, Jukebox, is a record-and-replay instruction prefetcher designed to accelerate these lukewarm executions by leveraging the high commonality in instruction footprints across invocations.\",\n    \"Direct Inspiration\": [\"b15\", \"b27\", \"b28\"],\n    \"Indirect Inspiration\": [\"b10\", \"b11\", \"b24\", \"b48\", \"b42\", \"b51\"],\n    \"Other Inspiration\": [\"b18\", \"b4\", \"b19\", \"b44\", \"b53\"]\n}\n```"], "62d7a7d15aee126c0f3fe049": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the significant latency of L1 data cache in Out-of-Order (OOO) processors, despite its lower latency compared to main memory. The paper proposes a novel Register File Prefetch (RFP) technique to prefetch load data from the L1 data cache to the Register File, aiming to hide the performance-critical latency of the L1 data cache. The proposed RFP leverages existing OOO scheduling logic without adding bandwidth/power overheads or requiring expensive pipeline modifications. The paper demonstrates RFP's superiority to prior techniques and its synergy with load value prediction.\",\n  \"Direct Inspiration\": {\n    \"b66\": 0.95,\n    \"b21\": 0.9,\n    \"b81\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.7,\n    \"b67\": 0.7,\n    \"b64\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b55\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of mitigating latency between the L1 data cache and the Register File in modern Out-of-Order (OOO) processors, proposing a novel Register File Prefetch (RFP) mechanism. The RFP aims to hide L1 latency by prefetching load data into the Register File, thus improving performance without additional bandwidth/power overheads or expensive pipeline modifications.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b66\": 0.9,\n    \"b67\": 0.8,\n    \"b39\": 0.7,\n    \"b55\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b6\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the latency of load instructions in modern Out-of-Order (OOO) processors, focusing on the memory wall between the L1 data cache and the Register File (RF). The proposed solution, Register File Prefetch (RFP), aims to prefetch load data from the L1 data cache to the RF, mitigating the performance-critical L1 data cache latency.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b66\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.9,\n    \"b39\": 0.8,\n    \"b55\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b70\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of the memory wall in modern Out-of-Order (OOO) processors, specifically focusing on the latency of L1 data cache. It proposes a novel Register File Prefetch (RFP) algorithm to prefetch load data from the L1 data cache to the Register File, aiming to hide the performance-critical latency of the L1 data cache. The work highlights the significant impact of L1 data cache latency on performance and demonstrates RFP's superiority over previous techniques such as load address prediction and load value prediction.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b21\": 0.9,\n        \"b66\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.7,\n        \"b45\": 0.7,\n        \"b57\": 0.7,\n        \"b58\": 0.7,\n        \"b62\": 0.7,\n        \"b64\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.6,\n        \"b25\": 0.6,\n        \"b28\": 0.6,\n        \"b39\": 0.6,\n        \"b55\": 0.6,\n        \"b67\": 0.6,\n        \"b81\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high latency in modern Out-of-Order (OOO) processors, specifically focusing on the latency of L1 data cache and its impact on performance. The proposed solution, Register File Prefetch (RFP), aims to prefetch data from the L1 data cache to the Register File to hide this latency, leveraging existing OOO scheduling logic without additional bandwidth or power overheads. The paper also compares RFP with previous techniques like load address prediction and value prediction, highlighting RFP\u2019s advantages in terms of performance and power efficiency.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b66\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b39\": 0.8,\n    \"b55\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b30\": 0.6,\n    \"b64\": 0.6,\n    \"b67\": 0.6\n  }\n}\n```"], "634d805b90e50fcafd4e117a": ["```json\n{\n    \"Summary\": \"The primary challenges in the paper are excessive compilation time and inefficiencies in the DNN model development cycle due to frequent re-compilations. The proposed algorithm, ROLLER, addresses these by rethinking the problem of DNN operator computation as a data processing pipeline rather than nested loops, using a new abstraction called rTile. This approach significantly reduces the complexity and time required for kernel generation, achieving three orders of magnitude improvement in compilation time while maintaining or improving kernel performance.\",\n    \"Direct Inspiration\": {\n        \"b12\": 0.9,\n        \"b30\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b32\": 0.7,\n        \"b23\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b24\": 0.6,\n        \"b18\": 0.5,\n        \"b19\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes ROLLER, a deep learning tensor compiler that addresses the excessive compilation time of DNN models by using a novel approach that treats DNN operators as data processing pipelines. ROLLER uses a new abstraction called rTile to align data tile shapes with hardware features, leading to efficient kernel generation in seconds.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.95,\n    \"b30\": 0.95,\n    \"b32\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b4\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the excessive compilation time required for DNN models using state-of-the-art compilers, the need for frequent recompilation when model configurations change, and the inefficient handling of custom operators. The proposed ROLLER algorithm introduces a novel approach by treating DNN computation as a data processing pipeline, focusing on aligning data tiles with hardware characteristics to maximize throughput and simplify performance evaluation. The key contributions include the rTile abstraction, a recursive construction algorithm for efficient tensor programs, and an efficient micro-performance model.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b30\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b23\": 0.6,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b0\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces ROLLER, a deep learning tensor compiler designed to address the excessive compilation times and inefficiencies in current DNN compilers. ROLLER redefines the problem by treating DNN computation as a data processing pipeline, and introduces rTile, an abstraction that aligns data tiles with hardware characteristics to optimize performance and reduce compilation time.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b30\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b23\": 0.75,\n    \"b32\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b4\": 0.5,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper include the excessive compilation time and inefficiencies of existing deep neural network (DNN) compilers. The proposed algorithm, ROLLER, introduces a radically different approach by treating DNN computation as a data processing pipeline and using a new abstraction called rTile to align data tile shapes with hardware characteristics. This method simplifies performance evaluation, significantly reduces compilation time, and generates highly-optimized kernels.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b30\": 1,\n    \"b32\": 0.9,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```"], "62fc5c7b90e50fcafdbca64d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of incomplete knowledge and inefficient queries in Knowledge Graphs (KGs). It proposes a Transformer-based GNN architecture called kgTransformer, along with self-supervised pretraining strategies to handle complex logical queries effectively.\",\n  \"Direct Inspiration\": [\"b17\", \"b29\", \"b31\"],\n  \"Indirect Inspiration\": [\"b10\", \"b18\", \"b27\", \"b37\"],\n  \"Other Inspiration\": [\"b2\", \"b35\", \"b0\", \"b11\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of incomplete knowledge and inefficient queries in knowledge graphs (KGs), particularly focusing on complex logical queries. The authors propose the kgTransformer, a Transformer-based graph neural network (GNN) with a novel architecture and training objective, including the Triple Transformation strategy and Mixture-of-Experts strategy. The kgTransformer is designed to handle complex EPFO queries on KGs with a high-capacity and computationally efficient architecture, further enhanced by a masked pre-training framework.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.95,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b35\": 0.75,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b11\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in knowledge graph (KG) representation learning, particularly for complex logical queries. The authors propose a novel Transformer-based Graph Neural Network (GNN) architecture called kgTransformer, which incorporates a Triple Transformation strategy and a Mixture-of-Experts (MoE) strategy to improve model capacity and computational efficiency. They also introduce a masked pre-training framework to enhance the model's generalizability.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b29\": 0.7,\n    \"b31\": 0.7,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b13\": 0.6,\n    \"b18\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of answering complex logical queries on knowledge graphs (KGs) by proposing a novel Transformer-based GNN architecture called kgTransformer. The primary challenges include handling incomplete knowledge, exponential query complexity, and the need for models to generalize well to out-of-domain queries. The proposed solution leverages a Triple Transformation strategy to convert KGs into directed graphs without edge attributes and employs a Mixture-of-Experts strategy for computational efficiency. Additionally, a masked pre-training framework is introduced to further enhance the model's generalizability.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b29\": 1,\n    \"b31\": 1,\n    \"b20\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b27\": 0.8,\n    \"b37\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b7\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"Challenges\": [\n            \"Incomplete knowledge and inefficient queries in KGs.\",\n            \"Exponential complexity of EPFO queries.\",\n            \"Limited expressiveness and capacity of existing KGE-based approaches.\"\n        ],\n        \"Algorithm\": \"kgTransformer\u2014a Transformer-based GNN architecture with self-supervised pretraining strategies for handling complex logical queries.\"\n    },\n    \"Direct Inspiration\": [\n        \"b17\",\n        \"b29\",\n        \"b31\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b0\",\n        \"b10\",\n        \"b13\",\n        \"b20\",\n        \"b27\"\n    ],\n    \"Other Inspiration\": [\n        \"b18\",\n        \"b19\",\n        \"b26\"\n    ]\n}\n```"], "634d80f190e50fcafd4ef483": ["```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the need for effective document-level event extraction (EE) due to limitations of existing sentence-level EE datasets and methods. The authors propose DocEE, a large-scale, human-annotated dataset designed to address the challenges of extracting event arguments scattered across multiple sentences. Key contributions include large-scale manual annotations, fine-grained argument types, and application-oriented settings to test cross-domain transfer capabilities of EE models.\",\n    \"Direct Inspiration\": {\n        \"b16\": 1.0,\n        \"b27\": 1.0,\n        \"b12\": 1.0,\n        \"b45\": 1.0,\n        \"b41\": 1.0,\n        \"b43\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.8,\n        \"b44\": 0.8,\n        \"b17\": 0.8,\n        \"b5\": 0.8,\n        \"b6\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.7,\n        \"b7\": 0.7,\n        \"b36\": 0.7,\n        \"b38\": 0.7,\n        \"b19\": 0.7,\n        \"b15\": 0.7,\n        \"b14\": 0.7,\n        \"b6\": 0.7,\n        \"b17\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of moving Event Extraction (EE) from sentence-level to document-level due to the dispersed nature of event arguments in text. The authors introduce DocEE, a large-scale human-annotated document-level EE dataset, highlighting its extensive annotations, fine-grained argument types, and application-oriented settings.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.95,\n    \"b6\": 0.95,\n    \"b12\": 0.9,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b41\": 0.75,\n    \"b43\": 0.75,\n    \"b45\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b17\": 0.7,\n    \"b44\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces DocEE, a large-scale human-annotated document-level Event Extraction (EE) dataset. It addresses the limitations of existing datasets by providing a larger scale, fine-grained argument types, and application-oriented settings. The main challenges include the need for reasoning over multiple sentences, modeling long-distance dependencies, and adapting to new domains. The paper highlights three main contributions: large-scale manual annotations, fine-grained argument types, and application-oriented settings.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b6\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b27\": 0.8,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.65,\n    \"b43\": 0.65,\n    \"b45\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of moving Event Extraction (EE) from sentence-level to document-level, as current datasets are limited in scale, domain coverage, and argument type refinement. The proposed solution is DocEE, a large-scale, manually annotated document-level EE dataset with 27,485 events and 180,528 arguments, designed to provide comprehensive training and testing data. DocEE features fine-grained argument types and application-oriented settings to better simulate real-world scenarios.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b8\": 0.8,\n    \"b17\": 0.85,\n    \"b27\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.75,\n    \"b45\": 0.75,\n    \"b41\": 0.75,\n    \"b43\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.7,\n    \"b45\": 0.7,\n    \"b41\": 0.7,\n    \"b43\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges outlined in the paper are the limitations of existing datasets for document-level Event Extraction (EE), which include small data scale, limited domain coverage, and insufficient refinement of argument types. The proposed algorithm, DocEE, aims to provide a large-scale, human-annotated document-level EE dataset with fine-grained argument types and application-oriented settings to overcome these challenges.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b12\": 0.9,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b45\": 0.7,\n    \"b41\": 0.7,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b16\": 0.6,\n    \"b44\": 0.6\n  }\n}\n```"], "6237ecc25aee126c0f3bef94": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the scaling of Graph Neural Networks (GNNs) to handle very large models with billions of parameters, specifically for atomic simulation datasets containing millions of smaller graphs. The paper introduces Graph Parallelism to address this challenge by splitting the input graph across multiple GPUs to scale up GNNs with higher-order interactions. The proposed methods are benchmarked on the Open Catalyst 2020 (OC20) dataset using two recent GNN architectures, DimeNet++ and GemNet-T, leading to significant improvements in predicting per-atom forces and system energies.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b13\": 0.9,\n    \"b2\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b32\": 0.7,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling Graph Neural Networks (GNNs) with higher-order interactions to billions of parameters for atomic simulations. The authors introduce the Graph Parallelism approach to distribute computations across multiple GPUs. They benchmark their method using DimeNet++ and GemNet-T on the Open Catalyst (OC20) dataset, achieving significant improvements in prediction accuracy. The paper builds on the Extended Graph Network (EGN) framework, which includes higher-order interactions such as triplets and quadruplets.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve scaling Graph Neural Networks (GNNs) to billions of parameters for datasets consisting of many moderately-sized graphs, especially those modeling higher-order interactions like triplets or quadruplets. The paper introduces Graph Parallelism to address these challenges, and benchmarks this approach using two recent GNN architectures, DimeNet++ and GemNet-T, on the Open Catalyst (OC20) dataset.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b13\": 1,\n    \"b2\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b31\": 0.8,\n    \"b9\": 0.8,\n    \"b17\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b32\": 0.7,\n    \"b20\": 0.7,\n    \"b3\": 0.7,\n    \"b10\": 0.7,\n    \"b19\": 0.7,\n    \"b27\": 0.7,\n    \"b23\": 0.7,\n    \"b7\": 0.7,\n    \"b1\": 0.7,\n    \"b29\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling Graph Neural Networks (GNNs) with higher-order interactions to billions of parameters for atomic simulations. The main contribution is the introduction of Graph Parallelism, which splits the input graph across multiple GPUs to handle large-scale models. This method is benchmarked using two recent GNN architectures, DimeNet++ and GemNet-T, on the Open Catalyst (OC20) dataset. The results show significant performance improvements in predicting per-atom forces and system energies.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.95,\n    \"b13\": 0.90,\n    \"b2\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.80,\n    \"b20\": 0.75,\n    \"b10\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b19\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling Graph Neural Networks (GNNs) with higher-order interactions to billions of parameters, particularly for applications in atomic systems. The authors propose a novel approach called Graph Parallelism, which distributes the computation across multiple GPUs to handle large-scale GNNs efficiently. They benchmark this approach on two recent GNN architectures, DimeNet++ and GemNet-T, using the Open Catalyst (OC20) dataset, achieving significant improvements in predicting forces and relaxed structures.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b11\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b5\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```"], "6257c5b25aee126c0f468af6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently and accurately representing 3D molecular graphs using a novel message passing scheme called spherical message passing (SMP). SMP is designed to be computationally efficient by using edge-based 1-hop information while maintaining approximately complete representations of 3D molecules. The approach ensures predictions invariant to translation and rotation and demonstrates superior performance on various datasets.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b23\": 0.9,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.6,\n    \"b18\": 0.7,\n    \"b24\": 0.7,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an efficient and approximately complete message passing method for 3D molecular graphs, named spherical message passing (SMP). The novel SMP method leverages spherical coordinates to reduce computational complexity while maintaining accurate representations of 3D molecular structures.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b24\": 0.85,\n    \"b45\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b28\": 0.75,\n    \"b5\": 0.75,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.65,\n    \"b47\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an efficient and accurate message passing method for 3D molecular graphs. It proposes the Spherical Message Passing (SMP) scheme, which uses the spherical coordinate system to represent 3D molecular structures and reduces computational complexity by using edge-based 1-hop information. The SMP is further extended to SphereNet, which incorporates physical representations based on spherical Bessel and harmonic functions for meaningful molecular learning.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b24\": 0.9,\n    \"b45\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b28\": 0.7,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an efficient and scalable message passing method for 3D molecular graphs. The authors propose the Spherical Message Passing (SMP) scheme, which utilizes spherical coordinates (distance, angle, torsion) for molecular representation. This method reduces computational complexity while maintaining high accuracy in molecular learning, leading to the development of SphereNet, which achieves superior performance on various datasets.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b17\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.75,\n    \"b28\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b5\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a novel message passing method for 3D molecular graphs, focusing on reducing computational complexity while ensuring accurate molecular representation. The proposed Spherical Message Passing (SMP) method uses relative 3D information (distance, angle, torsion) in the Spherical Coordinate System (SCS) to achieve efficient and invariant molecular learning, leading to the development of SphereNet.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b24\": 0.8,\n    \"b45\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b23\": 0.6,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```"], "6257c5b15aee126c0f468a55": ["```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies in self-attention mechanisms in transformer architectures, particularly focusing on the limitations of existing Approximate Nearest Neighbors (ANN)-based methods for attention sparsification. It proposes a novel solution, Learning-to-Hash Attention (LHA), which dynamically optimizes the attention utility metric through separate learnable hash functions for queries and keys.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b8\": 0.9,\n    \"b28\": 0.9,\n    \"b38\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b24\": 0.7,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiency of self-attention mechanisms in handling long sequences, particularly focusing on the limitations of existing ANN-based methods for attention sparsification. It introduces a novel Learning-to-Hash Attention (LHA) model, which uses separate learnable hash functions for queries and keys to optimize a newly defined attention utility metric and improve computational efficiency.\",\n  \"Direct Inspiration\": [\"b8\", \"b13\", \"b28\", \"b38\"],\n  \"Indirect Inspiration\": [\"b5\", \"b24\", \"b33\"],\n  \"Other Inspiration\": [\"b37\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses inefficiencies in self-attention mechanisms for long sequences by proposing a novel Learning-to-Hash Attention (LHA) model. This model directly optimizes a newly defined attention utility metric using separate learnable hash functions for queries and keys to overcome limitations of existing ANN-based methods.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b8\": 0.85,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.75,\n    \"b5\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.65,\n    \"b20\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the inefficiency of self-attention mechanisms in Transformers by proposing a novel Learning-to-Hash Attention (LHA) model. The primary challenges include the quadratic time and space complexity of self-attention and the limitations of existing Approximate Nearest Neighbor (ANN)-based methods for attention sparsification. The proposed LHA model introduces learnable hash functions for queries and keys to optimize a newly defined attention utility metric, reducing computational complexity while improving performance.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1.0,\n        \"b8\": 1.0,\n        \"b28\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.9,\n        \"b24\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b34\": 0.8,\n        \"b33\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiency of the self-attention mechanism in Transformers, particularly its quadratic time and space complexity. The proposed solution, Learning-to-Hash Attention (LHA), aims to optimize attention utility via learnable hash functions for queries and keys, thus improving model expressiveness and reducing complexity. The paper also analyzes the limitations of existing Approximate Nearest Neighbors (ANN) methods and introduces techniques to overcome these issues.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b8\": 0.85,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b24\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.65,\n    \"b33\": 0.6\n  }\n}\n```"], "62c2a5625aee126c0fcca6f5": ["```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the overfitting problem in GNNs when trained on large-scale datasets, exacerbated by out-of-distribution test nodes. The paper proposes FLAG, a feature-based adversarial augmentation method, to tackle this challenge by adding adversarial perturbations to node features while keeping graph structures unchanged. FLAG leverages 'free' adversarial training methods to ensure scalability and incorporates multi-scale techniques to enhance augmentation effectiveness. Extensive experiments demonstrate FLAG's efficacy and scalability across various tasks such as node, link, and graph property prediction.\",\n    \"Direct Inspiration\": {\n        \"b30\": 1.0,\n        \"b35\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.8,\n        \"b16\": 0.8,\n        \"b25\": 0.8,\n        \"b34\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.7,\n        \"b9\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the overfitting problem in Graph Neural Networks (GNNs) by proposing a novel method called FLAG (Free Large-scale Adversarial Augmentation on Graphs). FLAG introduces adversarial perturbations in the node feature space to augment data, aiming to improve generalization and robustness without modifying graph structures. The method leverages 'free' adversarial training techniques to be computationally efficient and scalable to large datasets.\",\n    \"Direct Inspiration\": {\n        \"b30\": 1.0,\n        \"b16\": 0.9,\n        \"b35\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.75,\n        \"b25\": 0.75,\n        \"b34\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.65,\n        \"b15\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the problem of overfitting in Graph Neural Networks (GNNs) when dealing with large-scale datasets, particularly focusing on the challenge of applying data augmentation in the node feature space. The proposed method, FLAG (Free Large-scale Adversarial Augmentation on Graphs), utilizes adversarial perturbations in the node feature space to enhance generalization and robustness of GNNs.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1,\n    \"b35\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b16\": 0.7,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of overfitting in training Graph Neural Networks (GNNs) on large-scale datasets and proposes a novel method called FLAG (Free Large-scale Adversarial Augmentation on Graphs). FLAG works by adding adversarial perturbations to the input node features, leveraging 'free' adversarial training methods to be scalable and efficient. It demonstrates significant improvements in generalization across various graph-related tasks, such as node classification, link prediction, and graph classification.\",\n    \"Direct Inspiration\": {\n        \"b14\": 1.0,\n        \"b25\": 1.0,\n        \"b30\": 1.0,\n        \"b35\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.9,\n        \"b34\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.8,\n        \"b39\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of overfitting in Graph Neural Networks (GNNs) and introduces FLAG, a method for feature-based adversarial data augmentation to improve generalization in GNNs. FLAG applies adversarial perturbations to node features while keeping graph structures unchanged, making it scalable and applicable to large datasets.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b25\": 0.9,\n    \"b30\": 0.9,\n    \"b35\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b27\": 0.75,\n    \"b34\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b12\": 0.65,\n    \"b39\": 0.65\n  }\n}\n```"], "6287492a5aee126c0ffe82dd": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges in answering complex questions over a knowledge base (Complex KBQA), which require compositional reasoning and diverse, large-scale datasets. The authors propose KQA Pro, a large-scale benchmark that includes a novel Knowledge-oriented Programming Language (KoPL) for explicit reasoning processes and provide SPARQL queries for comparison. They also introduce a methodology for generating diverse questions and analyze the performance of state-of-the-art models on KQA Pro.\",\n    \"Direct Inspiration\": {\n        \"b17\": 0.9,\n        \"b33\": 0.85,\n        \"b16\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.7,\n        \"b31\": 0.65,\n        \"b25\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.5,\n        \"b42\": 0.5,\n        \"b46\": 0.5,\n        \"b34\": 0.4,\n        \"b20\": 0.4,\n        \"b45\": 0.3,\n        \"b7\": 0.3\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in complex question answering (Complex KBQA) over knowledge bases (KBs) and introduces KQA Pro, a large-scale benchmark for Complex KBQA. It highlights the shortcomings of existing benchmarks, such as lack of explicit reasoning processes and insufficient diversity and scale. The authors propose a novel Knowledge-oriented Programming Language (KoPL) to improve compositional reasoning and provide a more explicit reasoning process compared to SPARQL. The KQA Pro dataset aims to enhance the field by addressing these challenges and providing a more comprehensive benchmark.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b39\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.7,\n    \"b16\": 0.6,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b46\": 0.4,\n    \"b42\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in complex question answering over knowledge bases (Complex KBQA), particularly the need for compositional reasoning and the lack of diverse and scalable datasets. It proposes KQA Pro, a large-scale benchmark that introduces a novel Knowledge-oriented Programming Language (KoPL) to explicitly describe reasoning processes, aiming to improve human understanding and control over the model's behavior.\",\n  \"Direct Inspiration\": [\"b16\", \"b25\", \"b33\", \"b39\", \"b45\"],\n  \"Indirect Inspiration\": [\"b7\", \"b43\"],\n  \"Other Inspiration\": [\"b23\", \"b17\", \"b10\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of complex question answering over knowledge bases (Complex KBQA), including the need for compositional reasoning and diversity in questions. It introduces KQA Pro, a large-scale benchmark dataset with a novel Knowledge-oriented Programming Language (KoPL) designed for explicit reasoning processes. The dataset aims to improve the performance and diversity of Complex KBQA by providing intermediate supervision and a variety of question types.\",\n    \"Direct Inspiration\": {\n        \"b17\": 0.95,\n        \"b39\": 0.9,\n        \"b7\": 0.85,\n        \"b45\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b25\": 0.75,\n        \"b33\": 0.8,\n        \"b16\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.65,\n        \"b31\": 0.6,\n        \"b13\": 0.55,\n        \"b42\": 0.55,\n        \"b46\": 0.55,\n        \"b34\": 0.5,\n        \"b20\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": [\n            \"Unsatisfied capability of compositional reasoning in Complex KBQA\",\n            \"Lack of explicit reasoning processes in existing benchmarks\",\n            \"Limited diversity and scale in existing datasets\"\n        ],\n        \"inspirations\": [\n            \"The need for intermediate supervision for learning compositional reasoning\",\n            \"Existing benchmarks' limitations in diversity and scale\",\n            \"The effectiveness of combining KoPL and SPARQL for complex question answering\"\n        ]\n    },\n    \"Direct Inspiration\": {\n        \"b25\": 0.9,\n        \"b33\": 0.85,\n        \"b16\": 0.8,\n        \"b39\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b45\": 0.7,\n        \"b7\": 0.7,\n        \"b23\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b31\": 0.6,\n        \"b17\": 0.6,\n        \"b46\": 0.55,\n        \"b42\": 0.55,\n        \"b13\": 0.5,\n        \"b20\": 0.5,\n        \"b34\": 0.5\n    }\n}\n```"], "6327ec7990e50fcafd928963": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve the limitations of the AlphaFold system's accessibility and its performance on certain protein types. The proposed Uni-Fold platform aims to address these challenges by being a fully open-source platform that reimplements AlphaFold and AlphaFold-Multimer in the PyTorch framework, making it more accessible and adaptable for the research community.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b1\": 1\n  },\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b16\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of making advanced protein structure prediction models, like AlphaFold, more accessible to smaller research groups. Uni-Fold, an open-source platform, reimplements AlphaFold and AlphaFold-Multimer using the PyTorch framework and introduces several improvements to enhance accuracy, efficiency, and accessibility.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b1\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Predicting 3D protein structures via computational methods.\",\n      \"Limitations in training and accessibility of AlphaFold for smaller research groups.\"\n    ],\n    \"Inspirations\": [\n      \"AlphaFold and AlphaFold-Multimer for their success in protein structure prediction.\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b0\",\n    \"b1\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b9\"\n  ],\n  \"Other Inspiration\": [\n    \"b11\",\n    \"b12\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of making the AlphaFold system more accessible and adaptable by presenting Uni-Fold, an open-source platform for developing protein folding models. Uni-Fold reimplements AlphaFold and AlphaFold-Multimer in the PyTorch framework, making it easier for the wider research community to use and contribute. Key contributions include reimplementation with equivalent or better performance, and several alterations and improvements for better accuracy and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in protein folding prediction, particularly the limitations of AlphaFold in terms of accessibility and adaptability for smaller research groups. The authors propose Uni-Fold, an open-source platform that reimplements AlphaFold and AlphaFold-Multimer in the PyTorch framework, making it more accessible and modifiable. Key contributions include reimplementation in a widely-used framework, alterations for improved performance, and methods for efficient training and inference.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```"], "63520de390e50fcafd60ec75": ["```json\n{\n  \"Summary\": \"The paper presents an asymmetric reverse process (Asyrp) to discover the semantic latent space of frozen diffusion models, enabling various attribute modifications on input images. The proposed h-space allows linear and simultaneous attribute changes without degrading image quality. The authors also introduce implicit neural directions to optimize h-space efficiently.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b25\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b32\": 0.8,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b6\": 0.5,\n    \"b8\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of semantic latent manipulation in pre-trained diffusion models, particularly focusing on the degradation of results when editing latent variables and the computational costs of existing methods. The authors propose an asymmetric reverse process (Asyrp) to discover a semantic latent space (h-space) in frozen diffusion models, allowing versatile image editing without quality degradation. Their method is applicable to various architectures and datasets.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1,\n        \"b14\": 1,\n        \"b24\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b6\": 0.8,\n        \"b8\": 0.8,\n        \"b10\": 0.8,\n        \"b18\": 0.8,\n        \"b20\": 0.8,\n        \"b21\": 0.8,\n        \"b25\": 0.8,\n        \"b26\": 0.8,\n        \"b32\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in image synthesis using diffusion models, focusing on improving the semantic latent space for better image editing. It proposes an asymmetric reverse process (Asyrp) that modifies the semantic latent space, named h-space, to achieve consistent and high-quality attribute changes in images. The method also introduces an implicit neural direction function to optimize the latent space effectively.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b0\": 0.8,\n    \"b24\": 0.7,\n    \"b25\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.6,\n    \"b32\": 0.65,\n    \"b26\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.4,\n    \"b20\": 0.4,\n    \"b21\": 0.4,\n    \"b6\": 0.35,\n    \"b18\": 0.35,\n    \"b8\": 0.3,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve achieving effective image editing using diffusion models without degrading image quality. The proposed algorithm, Asymmetric Reverse Process (Asyrp), introduces a semantic latent space called h-space within frozen diffusion models to facilitate versatile and high-quality image editing. Key innovations include a principled design of the generative process, the introduction of h-space, and the use of implicit neural directions to control attribute changes during image synthesis.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b14\": 1,\n    \"b25\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b6\": 0.8,\n    \"b8\": 0.8,\n    \"b18\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b32\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of semantic latent manipulation in pretrained and frozen diffusion models for image synthesis. The authors introduce an asymmetric reverse process (Asyrp) to discover a semantic latent space, named h-space, which allows for versatile image editing without degrading image quality. They propose a principled design of the generative process and demonstrate their method's applicability across various architectures and datasets.\",\n    \"Direct Inspiration\": {\n        \"b10\": 0.9,\n        \"b14\": 0.8,\n        \"b24\": 0.85,\n        \"b25\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.7,\n        \"b2\": 0.7,\n        \"b20\": 0.7,\n        \"b21\": 0.7,\n        \"b26\": 0.75,\n        \"b32\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b8\": 0.65,\n        \"b18\": 0.65\n    }\n}\n```"], "63520de890e50fcafd60f4dd": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of scaling instruction finetuning in language models, particularly focusing on improving reasoning abilities through chain-of-thought (CoT) annotations. The primary contributions include the development of Flan-PaLM, a model finetuned with CoT data, which demonstrates significant improvements in reasoning tasks, multilingual capabilities, and overall performance across various benchmarks.\",\n  \"Direct Inspiration\": [\"b61\", \"b58\"],\n  \"Indirect Inspiration\": [\"b32\", \"b59\", \"b66\"],\n  \"Other Inspiration\": [\"b39\", \"b50\", \"b11\", \"b14\", \"b7\", \"b9\", \"b15\", \"b46\"]\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges discussed in the paper include improving the generalization ability of language models to unseen tasks and enhancing their reasoning capabilities. The proposed solution involves instruction finetuning on a diverse set of tasks and incorporating chain-of-thought (CoT) finetuning to improve reasoning performance. The paper evaluates the effectiveness of these techniques on various benchmarks, demonstrating substantial improvements over previous models.\",\n  \"Direct Inspiration\": {\n    \"b61\": 1,\n    \"b58\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b66\": 0.8,\n    \"b32\": 0.7,\n    \"b59\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b10\": 0.5,\n    \"b46\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of generalizing AI models to unseen tasks by advancing instruction finetuning. Key contributions include scaling instruction finetuning to larger models and more tasks, and incorporating chain-of-thought (CoT) annotations to improve reasoning abilities.\",\n  \"Direct Inspiration\": [\"b32\", \"b59\", \"b61\"],\n  \"Indirect Inspiration\": [\"b10\", \"b15\", \"b46\"],\n  \"Other Inspiration\": [\"b4\", \"b9\", \"b28\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper aims to enhance instruction finetuning for language models by scaling the number of tasks and model size, and by including chain-of-thought (CoT) data to improve reasoning abilities. Key challenges include improving performance on unseen tasks and reasoning capabilities.\",\n  \"Direct Inspiration\": {\n    \"b61\": 1.0,\n    \"b66\": 0.9,\n    \"b32\": 0.8,\n    \"b59\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b58\": 0.7,\n    \"b15\": 0.7,\n    \"b49\": 0.6,\n    \"b10\": 0.6,\n    \"b46\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b14\": 0.5,\n    \"b7\": 0.5,\n    \"b39\": 0.5,\n    \"b9\": 0.5,\n    \"b48\": 0.5,\n    \"b45\": 0.5,\n    \"b41\": 0.5,\n    \"b4\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of language models on unseen tasks by advancing instruction finetuning with a focus on scaling the number of tasks and model size, and incorporating chain-of-thought (CoT) annotations.\",\n  \"Direct Inspiration\": {\n    \"b61\": 1.0,\n    \"b58\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b59\": 0.8,\n    \"b66\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.7,\n    \"b15\": 0.7,\n    \"b46\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```"], "6389d6e490e50fcafdff693f": ["```json\n{\n  \"Summary\": \"The paper addresses the inefficiency and high energy consumption of current CPUs in data centers and proposes the Request Processing Unit (RPU) as a solution. The RPU leverages the energy-efficient nature of SIMT processors while maintaining the single-thread latency and programmability of OoO CPUs. Key observations highlight the benefits of batching and SIMT execution for microservices, and the RPU's design includes a SIMT-aware software stack and hardware optimizations to improve energy efficiency and thread density.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.9,\n    \"b16\": 0.85,\n    \"b19\": 0.8,\n    \"b24\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b45\": 0.65,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.55,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the high energy consumption of CPUs in hyperscale data centers, proposing a new architecture called Request Processing Unit (RPU) that combines the energy efficiency of SIMT processors and the single-thread latency and programmability of OoO CPUs to improve energy efficiency and thread density for microservices.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b19\": 1,\n    \"b24\": 0.9,\n    \"b36\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.85,\n    \"b16\": 0.8,\n    \"b23\": 0.75,\n    \"b24\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the energy inefficiency and latency issues in contemporary data centers by proposing a novel architecture called the Request Processing Unit (RPU). The RPU leverages the frontend and memory system design of SIMT processors while maintaining the single-thread latency and programmability of OoO CPUs, specifically tailored to microservices.\",\n    \"Direct Inspiration\": [\"b16\", \"b19\", \"b24\", \"b36\"],\n    \"Indirect Inspiration\": [\"b8\", \"b23\", \"b45\"],\n    \"Other Inspiration\": [\"b10\", \"b25\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the energy inefficiency of current CPU architectures in data centers, particularly for microservices. The proposed solution, the Request Processing Unit (RPU), leverages SIMT execution from GPUs while maintaining CPU-like single-thread performance and programmability.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.9,\n        \"b16\": 0.9,\n        \"b19\": 0.9,\n        \"b24\": 0.9,\n        \"b36\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.8,\n        \"b23\": 0.8,\n        \"b41\": 0.8,\n        \"b45\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.7,\n        \"b42\": 0.7,\n        \"b93\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is improving the energy-efficiency and performance of CPUs in data centers, particularly for microservices. The paper proposes a novel architecture called the Request Processing Unit (RPU), which leverages the energy-efficient SIMT execution model of GPUs while maintaining the single-thread performance and programmability of CPUs.\",\n  \"Direct Inspiration\": [\"b16\", \"b19\", \"b23\", \"b24\", \"b36\"],\n  \"Indirect Inspiration\": [\"b10\", \"b14\", \"b22\", \"b25\", \"b33\", \"b34\"],\n  \"Other Inspiration\": [\"b6\", \"b8\", \"b28\"]\n}\n```"], "63a2c50090e50fcafdb97c2f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of bridging the gap between vision and language modalities in vision-and-language pre-training (VLP) without relying on costly and sub-optimal region features. The proposed solution is an end-to-end unsupervised vision-and-language pre-training (UVLP) framework called E2E-UVLP, which leverages a vision encoder and a pre-trained language model (PLM) connected by a linear projection layer. The key contributions include the introduction of a masked tag prediction (MTP) pre-training task and a novel referring expression matching (REM) pre-training task to improve model performance and generalization.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b38\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.8,\n    \"b8\": 0.8,\n    \"b11\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b2\": 0.7,\n    \"b13\": 0.7,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of bridging the gap between vision and language modalities in vision-and-language pre-training (VLP) without relying on costly and sub-optimal region features. It proposes an end-to-end unsupervised vision-and-language pre-training (UVLP) framework named E2E-UVLP, which integrates a vision encoder and a pre-trained language model (PLM) connected by a linear projection layer. The novel contributions include the masked tag prediction (MTP) pre-training task and a new referring expression matching (REM) pre-training task, aiming to reduce training-inference discrepancy and improve generalization to richer natural language expressions.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b38\": 1.0,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b2\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b11\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenge of bridging the gap between vision and language representations in vision-and-language pre-training (VLP), especially in an unsupervised manner (UVLP). It proposes an end-to-end UVLP framework named E2E-UVLP that eliminates the reliance on region features and introduces a novel pre-training task named referring expression matching (REM) to alleviate training-inference discrepancies and improve generalization to natural language expressions.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b38\": 0.9,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b18\": 0.6,\n    \"b12\": 0.6,\n    \"b30\": 0.6,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.5,\n    \"b8\": 0.5,\n    \"b11\": 0.5,\n    \"b5\": 0.5,\n    \"b29\": 0.4,\n    \"b37\": 0.4,\n    \"b2\": 0.4,\n    \"b13\": 0.4,\n    \"b6\": 0.3,\n    \"b9\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of bridging the gap between vision and language modalities in vision-and-language pre-training (VLP) without relying on region-based image features. The proposed solution is an end-to-end unsupervised vision-and-language pre-training (UVLP) framework named E2E-UVLP. Key contributions include the introduction of a novel referring expression matching (REM) pre-training task to reduce training-inference discrepancy and improve generalization to natural language expressions, and demonstrating the superiority of the framework over existing UVLP methods through extensive experiments.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b38\": 1.0,\n    \"b17\": 0.9,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b5\": 0.8,\n    \"b33\": 0.8,\n    \"b8\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in vision-and-language pre-training (VLP), particularly the difficulty of bridging the gap between vision and language representations using unsupervised methods. The proposed solution, an end-to-end unsupervised vision-and-language pre-training framework named E2E-UVLP, employs a novel pre-training task called referring expression matching (REM) to reduce training-inference discrepancy and improve generalization to richer natural language expressions. The framework eliminates the need for region features and uses raw images, making it computationally efficient and capable of leveraging vision pre-trained models (V-PTMs).\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b38\": 0.9,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.5,\n    \"b11\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.4,\n    \"b8\": 0.3\n  }\n}\n```"], "62be66205aee126c0f07d1e0": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The paper seeks to evaluate Copilot's capability as an AI pair programmer, tackling fundamental algorithmic problems, comparing its solutions with human solutions, and assessing reproducibility, correctness, and efficiency of the generated solutions.\",\n    \"inspirations\": \"The study is inspired by the broader goal of automating code generation using deep learning and transformer models, assessing the practical application and effectiveness of Copilot in real-world programming tasks.\"\n  },\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b7\": 0.85,\n    \"b10\": 0.8,\n    \"b24\": 0.75,\n    \"b34\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.65,\n    \"b5\": 0.6,\n    \"b13\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b12\": 0.45,\n    \"b26\": 0.4,\n    \"b31\": 0.35,\n    \"b33\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper undertakes an empirical evaluation of GitHub Copilot, focusing on its performance and functionality in solving fundamental algorithmic problems and comparing its solutions to human-provided solutions. The study assesses Copilot's strengths and weaknesses, providing guidelines for its effective adoption as a pair programming tool. The main challenges addressed include the correctness, reproducibility, and efficiency of Copilot's generated solutions, especially compared to human solutions.\",\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [\"b4\", \"b12\", \"b7\"],\n  \"Other Inspiration\": [\"b10\", \"b33\", \"b24\", \"b31\", \"b34\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper primarily addresses the challenges of evaluating the performance and functionality of GitHub Copilot, an AI-based code generation tool, in solving fundamental programming tasks. It explores the correctness, reproducibility, and efficiency of Copilot's solutions compared to human-provided solutions.\",\n    \"Direct Inspiration\": [\"b4\", \"b10\", \"b24\", \"b33\"],\n    \"Indirect Inspiration\": [\"b3\", \"b7\", \"b12\", \"b31\", \"b34\"],\n    \"Other Inspiration\": [\"b16\", \"b27\", \"b32\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper aims to evaluate GitHub Copilot's performance in solving fundamental algorithmic problems and compare its solutions with human-provided solutions. The primary challenges include assessing the correctness, reproducibility, and efficiency of Copilot's solutions, as well as its ability to mimic human pair programmers.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b3\": 0.9,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b33\": 0.7,\n    \"b24\": 0.65,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.55,\n    \"b23\": 0.5,\n    \"b36\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include assessing the correctness, reproducibility, and efficiency of Copilot's solutions for fundamental algorithmic problems, and comparing these solutions with human-generated ones. The authors aim to understand Copilot's strengths and weaknesses as an AI pair programmer and provide guidelines for its effective adoption.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b7\": 0.85,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b10\": 0.6,\n    \"b24\": 0.65,\n    \"b33\": 0.6,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.5,\n    \"b31\": 0.55,\n    \"b36\": 0.5\n  }\n}\n```"], "633e477c90e50fcafde5a596": ["```json\n{\n  \"Summary\": \"The paper addresses three main challenges: creating effective prompts, scalable collection of prompts, and reliable prompt aggregation. The authors propose the AMA (ASK ME ANYTHING) approach, which involves reformatting tasks to effective question-answering formats, using prompt-chains to generate multiple prompts, and applying weak supervision for aggregation. This method aims to enhance the performance of off-the-shelf LLMs, making them competitive with larger models like GPT-3-175B.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b4\": 0.9,\n    \"b12\": 0.95,\n    \"b13\": 0.9,\n    \"b24\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b7\": 0.75,\n    \"b8\": 0.75,\n    \"b9\": 0.8,\n    \"b11\": 0.85,\n    \"b30\": 0.7,\n    \"b53\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b14\": 0.65,\n    \"b16\": 0.6,\n    \"b18\": 0.65,\n    \"b20\": 0.65,\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b25\": 0.6,\n    \"b28\": 0.65,\n    \"b29\": 0.65,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three main challenges in prompting large language models (LLMs): creating effective prompts that work across various tasks and models, scalably collecting these prompts, and aggregating the predictions from multiple prompts to improve performance. The proposed solution, ASK ME ANYTHING PROMPTING (AMA), involves identifying effective prompt formats, using a scalable strategy for reformatting task inputs, and employing weak supervision for prompt aggregation.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b9\": 0.8,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b11\": 0.9,\n    \"b12\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b14\": 0.6,\n    \"b16\": 0.6,\n    \"b18\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.7,\n    \"b25\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b53\": 0.9\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three main challenges in improving the performance of large language models (LLMs) for diverse tasks: creating effective prompts, scalable prompt collection, and reliable prompt aggregation. The proposed algorithm, AMA (ASK ME ANYTHING PROMPTING), involves identifying optimal prompt formats, using a recursive LLM-based method to generate prompts, and applying weak supervision for prompt aggregation.\",\n  \"Direct Inspiration\": {\n    \"inspired by\": [\"b4\", \"b24\"],\n    \"motivated by\": [\"b0\", \"b12\", \"b13\"],\n    \"take inspiration\": [\"b4\", \"b24\"]\n  },\n  \"Indirect Inspiration\": [\"b5\", \"b9\", \"b7\", \"b8\", \"b53\", \"b11\", \"b30\", \"b31\"],\n  \"Other Inspiration\": [\"b14\", \"b16\", \"b18\", \"b20\", \"b21\", \"b3\", \"b22\", \"b23\", \"b2\", \"b10\", \"b25\", \"b28\", \"b29\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are: (1) the need for effective prompts that work across various tasks and models, (2) scalable collection of multiple effective prompts, and (3) reliable aggregation of predictions from multiple prompts. The AMA algorithm proposes using a two-step prompting pipeline, recursive reformatting of task inputs, and weak supervision for aggregation to improve the performance of large language models.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b12\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b24\": 0.8,\n    \"b53\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are: 1. Identifying effective prompt formats that work across various tasks and models. 2. Scalably collecting multiple prompts in effective formats. 3. Aggregating the predictions of these prompts to improve prompting performance. The proposed algorithm is ASK ME ANYTHING PROMPTING (AMA), which uses open-ended prompts to improve effectiveness, a strategy to scalably reformat task inputs, and weak supervision (WS) to aggregate predictions reliably.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b11\": 0.7,\n    \"b53\": 0.7,\n    \"b2\": 0.6,\n    \"b14\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```"], "62afe5495aee126c0f668b42": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of generating accurate medical responses in dialogue systems by proposing a model called MedPIR. The core contributions include a knowledge-aware dialogue graph encoder to exploit relationships between medical entities in different utterances and a recall-enhanced generator to focus on pivotal information from long dialogue histories.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b14\": 0.9,\n    \"b20\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b6\": 0.75,\n    \"b18\": 0.85,\n    \"b19\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b3\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include effectively utilizing complex medical relationships between different utterances in medical dialogues and recalling pivotal information from long dialogue history to generate accurate medical responses. The algorithm proposed, MedPIR, addresses these challenges with a knowledge-aware dialogue graph encoder and a recall-enhanced generator.\",\n  \"Direct Inspiration\": {\n    \"References\": [\"b4\", \"b14\", \"b20\"]\n  },\n  \"Indirect Inspiration\": {\n    \"References\": [\"b18\", \"b19\", \"b21\"]\n  },\n  \"Other Inspiration\": {\n    \"References\": [\"b9\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to effectively model the complex medical relationships between different utterances in long medical dialogues and recall pivotal information for accurate response generation. The proposed solution, MedPIR, includes a knowledge-aware dialogue graph encoder and a recall-enhanced generator to address these challenges.\",\n  \"Direct Inspiration\": [\"b4\", \"b14\", \"b20\"],\n  \"Indirect Inspiration\": [\"b2\", \"b6\", \"b18\"],\n  \"Other Inspiration\": [\"b1\", \"b9\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is how to acquire pivotal information from long medical dialogue history to generate accurate medical responses. The proposed solution, MedPIR, employs a knowledge-aware dialogue graph encoder to exploit the medical relationships between utterances and a recall-enhanced generator to recall pivotal information during response generation.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b14\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b18\": 0.7,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in Medical Dialogue Systems (MDS) by proposing a new model, MedPIR, which focuses on recalling pivotal information from long dialogue histories through a knowledge-aware dialogue graph encoder and a recall-enhanced generator. The model aims to improve the accuracy of responses by fully utilizing the complex medical relationships between utterances.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1,\n        \"b14\": 1,\n        \"b20\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.9,\n        \"b19\": 0.8,\n        \"b21\": 0.8,\n        \"b6\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.7,\n        \"b30\": 0.7,\n        \"b9\": 0.7\n    }\n}\n```"], "634781fe90e50fcafd2c1a49": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of overfitting in pre-trained language models (PrLMs) when fine-tuned on downstream tasks due to the overparameterization and limited annotated data. It introduces a novel dropout regularizer, Attribution-Driven Dropout (AD-DROP), which strategically drops self-attention positions with high attribution scores to reduce overfitting. The paper also proposes a cross-tuning strategy to improve training stability.\",\n    \"Direct Inspiration\": {\n        \"b22\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b24\": 0.9,\n        \"b25\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.7,\n        \"b18\": 0.8\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The paper addresses the issue of overfitting in pre-trained large language models (PrLMs) when fine-tuned on downstream tasks due to the mismatch between their overparameterization and limited annotated data.\",\n    \"Inspirations\": \"The authors propose a novel dropout regularizer, Attribution-Driven Dropout (AD-DROP), motivated by self-attention attribution to reduce overfitting.\"\n  },\n  \"Direct Inspiration\": {\n    \"References\": [\"b22\", \"b24\", \"b25\"]\n  },\n  \"Indirect Inspiration\": {\n    \"References\": [\"b18\", \"b3\"]\n  },\n  \"Other Inspiration\": {\n    \"References\": [\"b1\", \"b10\", \"b12\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of overfitting in fine-tuning pre-trained large language models (PrLMs) due to the redundancy and overparameterization of self-attention networks. It proposes a novel dropout regularizer called Attribution-Driven Dropout (AD-DROP) based on self-attention attribution to reduce overfitting. The AD-DROP method selectively drops self-attention positions with high attribution scores to force the model to rely on low-attribution positions. The paper also introduces a cross-tuning strategy to improve training stability.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b24\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b26\": 0.6,\n    \"b27\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of overfitting in fine-tuning pre-trained large language models (PrLMs) on downstream tasks, specifically targeting the redundancy and overparameterization of self-attention networks. The authors propose a novel dropout regularizer, Attribution-Driven Dropout (AD-DROP), which strategically drops self-attention positions with high attribution scores to minimize over-reliance on particular features and improve generalization. The proposed method leverages gradient-based attribution to compute attribution scores and introduces a cross-tuning strategy to enhance training stability.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b24\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b12\": 0.7,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the overfitting of pre-trained language models (PrLMs) during fine-tuning on downstream tasks, due to the mismatch between their overparameterization and limited annotated data. The authors propose a novel dropout regularizer, Attribution-Driven Dropout (AD-DROP), which selectively drops self-attention positions with high attribution scores to improve generalization and training stability. The approach is motivated by the observation that not all attention positions are equally important in preventing overfitting, and that dropping low-attribution positions accelerates overfitting.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b24\": 1.0,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b3\": 0.7,\n    \"b26\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b27\": 0.4\n  }\n}\n```"], "62ceb9215aee126c0f4090b0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing Temporal Convolutional Networks (TCNs) for efficient time-series processing on edge devices. The proposed method, Pruning In Time (PIT), extends previous work by optimizing the receptive field, dilation, number of channels in convolutional layers, and neurons in fully connected layers. The main contributions include the introduction of structured weight pruning and regularizers to reduce model complexity while preserving accuracy.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b10\": 0.7,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b14\": 0.4,\n    \"b15\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of optimizing Temporal Convolutional Networks (TCNs) for time-series processing on edge devices. It proposes a novel NAS tool called Pruning in Time (PIT) to optimize TCN hyperparameters like receptive field and dilation. This approach aims to reduce model complexity, memory footprint, and computational cost while maintaining accuracy.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b12\": 0.75,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses optimizing Temporal Convolutional Networks (TCNs) for time-series processing on edge devices. It proposes a new tool, Pruning In Time (PIT), which extends Neural Architecture Search (NAS) techniques to optimize TCN hyperparameters like dilation and receptive field. The primary challenges include reducing memory footprint, energy consumption, and search time while maintaining or improving accuracy.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b12\": 0.85,\n    \"b13\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b9\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b15\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of optimizing Temporal Convolutional Networks (TCNs) for time-series processing on edge devices. The core contribution is the development of the Pruning In Time (PIT) tool, which extends Neural Architecture Search (NAS) methodologies to optimize TCNs by tuning hyperparameters such as dilation, receptive field, and number of channels to create Pareto-optimal models.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b9\": 0.6,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.4,\n    \"b15\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing Temporal Convolutional Networks (TCNs) for edge device deployment through a novel Neural Architecture Search (NAS) tool called Pruning in Time (PIT). The primary inspirations include the need for efficient model architecture for time-series processing on edge devices, leveraging the unique features of TCNs, and extending existing NAS methodologies to optimize TCN-specific hyperparameters such as receptive fields and dilation.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b13\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.85,\n    \"b9\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b14\": 0.75\n  }\n}\n```"], "63bb859d90e50fcafd06ee17": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of data poisoning in automatic code suggestion models, specifically targeting the vulnerability of using untrusted sources for training data. The authors propose novel poisoning attacks, COVERT and TROJANPUZZLE, that hide malicious payloads in docstrings and comments to evade detection by static analysis tools. The primary inspiration comes from the limitations of previous work by Schuster et al., which the authors aim to overcome by avoiding direct inclusion of malicious payloads in the training data.\",\n  \"Direct Inspiration\": {\n    \"b47\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b3\": 0.8,\n    \"b28\": 0.8,\n    \"b11\": 0.8,\n    \"b2\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.7,\n    \"b36\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the security challenges of automatic code-suggestion models trained on large, untrusted datasets. It highlights the risks of data poisoning attacks and introduces novel methods (COVERT and TROJANPUZZLE) to bypass existing detection mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b47\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b3\": 0.8,\n    \"b28\": 0.8,\n    \"b11\": 0.8,\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.7,\n    \"b43\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training code-suggestion models on potentially untrusted data, which can be exploited through poisoning attacks. It introduces novel attacks, COVERT and TROJANPUZZLE, to bypass detection mechanisms and achieve malicious payload suggestion without explicit inclusion of payload in the training data.\",\n  \"Direct Inspiration\": {\n    \"b47\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.8,\n    \"b36\": 0.8,\n    \"b43\": 0.7,\n    \"b49\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b34\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies the primary challenge of security risks in code suggestion models trained on public data, focusing on poisoning attacks where adversaries inject malicious data to influence model outputs. The authors propose novel data poisoning attacks\u2014COVERT and TROJANPUZZLE\u2014that evade detection by static analysis tools. COVERT hides malicious code in docstrings, while TROJANPUZZLE masks parts of the payload, making the attack even stealthier.\",\n  \"Direct Inspiration\": {\n    \"b47\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.9,\n    \"b36\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b19\": 0.6,\n    \"b11\": 0.6,\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of poisoning attacks on code-suggestion models, particularly focusing on how adversaries can inject malicious code into training data to manipulate the model's output. The primary contributions are the introduction of the COVERT and TROJANPUZZLE attacks, which aim to bypass static analysis detection by hiding malicious payloads in out-of-context regions such as docstrings and comments.\",\n    \"Direct Inspiration\": {\n        \"b47\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b43\": 0.8,\n        \"b11\": 0.7,\n        \"b34\": 0.7,\n        \"b2\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.5,\n        \"b42\": 0.5,\n        \"b19\": 0.5\n    }\n}\n```"], "628749265aee126c0ffe6e73": ["```json\n{\n  \"Summary\": \"The paper addresses the translational gap between foundational dialogue systems research and applied task-oriented dialogue agents in healthcare. The main contributions include a comprehensive analysis of 70 papers, identification of system limitations, and practical suggestions for future work.\",\n  \"Direct Inspiration\": {\n    \"b62\": 0.9,\n    \"b63\": 0.9,\n    \"b111\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b82\": 0.7,\n    \"b110\": 0.7,\n    \"b124\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b85\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the translational gap between foundational dialogue system research and its application in healthcare settings. It aims to explore the use of task-oriented dialogue systems in healthcare, identify their shortcomings, and provide opportunities for future work. The paper systematically reviews 70 papers, analyzing them based on various factors such as system objectives, architecture, modality, and evaluation methods.\",\n    \"Direct Inspiration\": {\n        \"b124\": 0.95,\n        \"b23\": 0.9,\n        \"b82\": 0.85,\n        \"b110\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b111\": 0.75,\n        \"b63\": 0.7,\n        \"b62\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b14\": 0.6,\n        \"b7\": 0.6,\n        \"b36\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the translational gap between foundational work in dialogue systems and their application in healthcare settings. It proposes a systematic analysis of task-oriented healthcare dialogue systems, identifying key limitations and opportunities for future work. The paper aims to provide a comprehensive review and practical suggestions for advancing the design and implementation of these systems.\",\n  \"Direct Inspiration\": {\n    \"b124\": 0.9,\n    \"b23\": 0.9,\n    \"b82\": 0.85,\n    \"b110\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b111\": 0.7,\n    \"b63\": 0.7,\n    \"b62\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b7\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper include the translational gap between fundamental dialogue systems research and applied systems in healthcare settings, incomplete exploration of system architectures, replicability concerns, ethical and privacy issues, and minimal investigation of usability or engagement.\",\n    \"Inspirations\": \"The paper is inspired to address these challenges by conducting a comprehensive, scientifically rigorous analysis of task-oriented healthcare dialogue systems, exploring how these systems have been employed, mapping out their characteristics and shortcomings, and offering practical suggestions for future work.\"\n  },\n  \"Direct Inspiration\": {\n    \"b82\": 1,\n    \"b110\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b124\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b62\": 0.7,\n    \"b63\": 0.7,\n    \"b111\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the translational gap between fundamental dialogue systems research and their application in healthcare settings. It provides a systematic review of task-oriented dialogue systems in healthcare, analyzing 70 papers based on various factors like system objective, architecture, and evaluation methods. The paper identifies common limitations and offers practical suggestions for future work.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b82\": 0.85,\n    \"b110\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b111\": 0.75,\n    \"b63\": 0.75,\n    \"b62\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b124\": 0.7,\n    \"b14\": 0.7\n  }\n}\n```"], "63608e5090e50fcafdee1224": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing data-specific GNN architectures for temporal knowledge graph completion (TKGC). The proposed method, SPA, aims to automatically design these architectures by exploring topological and temporal information in TKGs. The paper highlights the limitations of existing embedding-based and GNN-based methods and introduces a novel search space and algorithm to efficiently search for optimal architectures.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b11\": 0.8,\n    \"b39\": 0.85,\n    \"b10\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.75,\n    \"b4\": 0.7,\n    \"b8\": 0.7,\n    \"b13\": 0.7,\n    \"b40\": 0.75,\n    \"b25\": 0.75,\n    \"b42\": 0.75,\n    \"b46\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.65,\n    \"b22\": 0.65,\n    \"b19\": 0.65,\n    \"b5\": 0.65,\n    \"b31\": 0.65,\n    \"b18\": 0.65,\n    \"b34\": 0.65,\n    \"b9\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Temporal Knowledge Graph Completion (TKGC) by proposing a novel method called Search to PAss messages (SPA). The primary challenge is to design data-specific GNN architectures that can simultaneously explore topological and temporal information in TKGs. The proposed method introduces a generalized framework, a novel search space, and an efficient search algorithm to achieve state-of-the-art performance on TKGC tasks.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b4\": 0.8,\n    \"b8\": 0.8,\n    \"b13\": 0.8,\n    \"b39\": 0.8,\n    \"b10\": 0.8,\n    \"b41\": 0.8,\n    \"b46\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b23\": 0.7,\n    \"b45\": 0.7,\n    \"b26\": 0.7,\n    \"b22\": 0.7,\n    \"b19\": 0.7,\n    \"b5\": 0.7,\n    \"b31\": 0.7,\n    \"b25\": 0.7,\n    \"b40\": 0.7,\n    \"b42\": 0.7,\n    \"b7\": 0.7,\n    \"b15\": 0.7,\n    \"b38\": 0.7,\n    \"b27\": 0.7,\n    \"b0\": 0.7,\n    \"b24\": 0.7,\n    \"b1\": 0.7,\n    \"b28\": 0.7,\n    \"b3\": 0.7,\n    \"b29\": 0.7,\n    \"b36\": 0.7,\n    \"b16\": 0.7,\n    \"b18\": 0.7,\n    \"b34\": 0.7,\n    \"b9\": 0.7,\n    \"b21\": 0.7,\n    \"b35\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Temporal Knowledge Graph Completion (TKGC) by proposing a novel method called Search to Pass messages (SPA). It aims to design data-specific GNN architectures for TKGC, leveraging both topological and temporal information. The key inspiration is drawn from the limitations of existing fixed GNN architectures and the success of Neural Architecture Search (NAS) in other domains.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b11\": 1.0,\n    \"b39\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b22\": 0.8,\n    \"b19\": 0.8,\n    \"b5\": 0.8,\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b4\": 0.6,\n    \"b8\": 0.6,\n    \"b13\": 0.6,\n    \"b27\": 0.6,\n    \"b18\": 0.6,\n    \"b34\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the incompletion of Temporal Knowledge Graphs (TKGs), which hampers their application in downstream tasks. The authors propose a novel method called Search to PAss messages (SPA) to automatically design data-specific architectures for TKG completion (TKGC). The method involves defining a generalized framework to explore both topological and temporal information in TKGs and employs a search algorithm to efficiently find optimal architectures within this defined search space.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b11\": 0.9,\n    \"b39\": 0.8,\n    \"b10\": 0.8,\n    \"b41\": 0.8,\n    \"b46\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b4\": 0.7,\n    \"b8\": 0.7,\n    \"b13\": 0.7,\n    \"b40\": 0.7,\n    \"b25\": 0.7,\n    \"b42\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b22\": 0.6,\n    \"b19\": 0.6,\n    \"b5\": 0.6,\n    \"b31\": 0.6,\n    \"b18\": 0.5,\n    \"b34\": 0.5,\n    \"b9\": 0.5,\n    \"b24\": 0.5,\n    \"b1\": 0.5,\n    \"b28\": 0.5,\n    \"b3\": 0.5,\n    \"b29\": 0.5,\n    \"b16\": 0.5,\n    \"b36\": 0.5,\n    \"b7\": 0.5,\n    \"b15\": 0.5,\n    \"b38\": 0.5,\n    \"b27\": 0.5,\n    \"b0\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of temporal knowledge graph completion (TKGC), which involves inferring missing facts in temporal knowledge graphs. The proposed method, Search to PAss messages (SPA), aims to automatically design data-specific graph neural network (GNN) architectures for TKGC. The paper introduces a generalized framework that jointly models topological and temporal information, defines a novel search space for different combinations of operations, and employs an efficient search algorithm to reduce GPU memory cost. The effectiveness of SPA is validated through experiments on three benchmark datasets.\",\n    \"Direct Inspiration\": {\n        \"b33\": 0.9,\n        \"b11\": 0.9,\n        \"b39\": 0.9,\n        \"b41\": 0.9,\n        \"b46\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.8,\n        \"b34\": 0.8,\n        \"b9\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b14\": 0.7,\n        \"b4\": 0.7,\n        \"b8\": 0.7,\n        \"b13\": 0.7,\n        \"b40\": 0.7,\n        \"b25\": 0.7,\n        \"b42\": 0.7,\n        \"b24\": 0.6,\n        \"b1\": 0.6,\n        \"b28\": 0.6,\n        \"b3\": 0.6,\n        \"b29\": 0.6,\n        \"b36\": 0.6,\n        \"b16\": 0.6,\n        \"b27\": 0.6\n    }\n}\n```"], "6306e8c890e50fcafdebd565": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of anomaly detection (AD) with multiple sets of noisy labels, proposing a novel ADMoE framework that enhances existing neural-network-based AD methods using a Mixture-of-experts (MoE) architecture. The key contributions include defining the problem of MNLAD, introducing the ADMoE framework, and demonstrating its effectiveness on various datasets.\",\n    \"Direct Inspiration\": {\n        \"b14\": 0.95,\n        \"b28\": 0.90\n    },\n    \"Indirect Inspiration\": {\n        \"b32\": 0.70,\n        \"b23\": 0.65,\n        \"b10\": 0.60\n    },\n    \"Other Inspiration\": {\n        \"b26\": 0.55,\n        \"b39\": 0.50\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of anomaly detection (AD) with multiple sets of noisy labels. It proposes ADMoE, a novel framework that enhances existing neural-network-based AD algorithms using a Mixture-of-Experts (MoE) architecture. This approach allows joint learning from multiple noisy label sets, providing scalability and specialization without requiring explicit mapping from noisy labels to network parameters.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b32\": 0.8,\n    \"b23\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of anomaly detection (AD) with multiple sets of noisy labels, proposing a novel framework, ADMoE, that enhances existing neural-network-based AD algorithms using a Mixture-of-Experts (MoE) architecture. This approach allows for scalable and specialized learning from noisy labels without requiring an explicit mapping of noisy labels to network parameters.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b10\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.7,\n    \"b39\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of anomaly detection (AD) with multiple sets of noisy/weak labels, proposing ADMoE, a novel framework that leverages Mixture-of-Experts (MoE) architecture. The main contributions include defining the problem of using multiple noisy labels for AD, introducing the ADMoE framework for scalable and specialized learning, and demonstrating its effectiveness on benchmark datasets and real-world applications.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b32\": 0.7,\n    \"b23\": 0.6,\n    \"b39\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the anomaly detection (AD) with multiple sets of weak/noisy labels (MNLAD). The proposed ADMoE framework enhances existing neural-network-based AD algorithms by incorporating a Mixture-of-Experts (MoE) network to leverage multiple noisy labels effectively. ADMoE aims to provide specialization and scalability while improving model flexibility and robustness.\",\n    \"Direct Inspiration\": {\n        \"b14\": 1,\n        \"b28\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b32\": 0.8,\n        \"b26\": 0.7,\n        \"b23\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b39\": 0.5,\n        \"b33\": 0.4\n    }\n}\n```"], "62451c325aee126c0f47b416": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of low knowledge transferring efficiency, high computational and memory costs in existing Knowledge Distillation (KD) methods, and introduces a novel self-distillation approach named Self-Distillation from Last Mini-Batch (DLB). DLB is designed to be computationally efficient, easy to implement for parallelization, and does not require network architecture modifications. It leverages the most immediate historically generated soft targets to improve training consistency and stability.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b11\": 0.85,\n    \"b34\": 0.8,\n    \"b35\": 0.83\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.75,\n    \"b42\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b40\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies and high computational costs of existing self-knowledge distillation (self-KD) methods, proposing a new approach called Self-Distillation from Last Mini-Batch (DLB). DLB improves training consistency and stability by using soft targets generated from the last mini-batch, without requiring a pre-trained teacher model or complex network modifications.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.8,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies and high computational costs associated with existing knowledge distillation (KD) methods, particularly self-KD. It proposes a novel self-distillation approach called Self-Distillation from Last Mini-Batch (DLB), which leverages the soft targets generated in the last mini-batch to improve training consistency and efficiency without requiring architectural modifications.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b34\": 0.9,\n    \"b36\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b42\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b35\": 0.6,\n    \"b43\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of low knowledge transferring efficiency and high computational/memory costs in existing knowledge distillation (KD) methods. It proposes a novel self-distillation approach named Self-Distillation from Last Mini-Batch (DLB) to improve training efficiency and reduce memory usage without requiring complex network architecture modifications.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1.0, \n        \"b11\": 1.0, \n        \"b35\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8, \n        \"b9\": 0.8, \n        \"b34\": 0.8, \n        \"b36\": 0.8, \n        \"b42\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6, \n        \"b3\": 0.6, \n        \"b8\": 0.6, \n        \"b10\": 0.6, \n        \"b12\": 0.6, \n        \"b13\": 0.6, \n        \"b16\": 0.6, \n        \"b18\": 0.6, \n        \"b20\": 0.6, \n        \"b24\": 0.6, \n        \"b29\": 0.6, \n        \"b30\": 0.6, \n        \"b31\": 0.6, \n        \"b32\": 0.6, \n        \"b37\": 0.6, \n        \"b38\": 0.6, \n        \"b40\": 0.6, \n        \"b43\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies and high computational costs of conventional knowledge distillation (KD) methods by proposing a new self-distillation approach called Self-Distillation from Last Mini-Batch (DLB). This method improves training efficiency by utilizing soft targets from the last mini-batch, which reduces computational redundancy and memory usage. The approach is model-agnostic and task-agnostic, making it easy to implement and parallelize. Experimental results demonstrate significant improvements in generalization and robustness across various neural network models and datasets.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b11\": 1.0,\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b13\": 0.8,\n    \"b32\": 0.8,\n    \"b36\": 0.8,\n    \"b38\": 0.8,\n    \"b42\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b29\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```"], "62725cd25aee126c0fae91f3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently deploying deep neural networks (DNNs) on resource-limited edge devices. It proposes a novel spot-adaptive knowledge distillation (SAKD) strategy that dynamically determines the optimal distillation spots for each sample during training. This approach contrasts with existing methods that use fixed distillation spots, which can either insufficiently or overly supervise the student model. The proposed method integrates a multi-path routing network and a policy network to adaptively manage distillation spots, enhancing the performance of both one-spot and multi-spot distillation strategies.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b5\": 0.85,\n    \"b6\": 0.85,\n    \"b1\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b9\": 0.75,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Excessive size of deep neural networks (DNNs) for deployment on resource-limited edge devices.\",\n      \"Inefficiency in manually designing distillation spots for knowledge distillation (KD) in networks with many layers.\",\n      \"Fixed global distillation strategy that assumes optimal distillation spots for the entire data distribution, which is often not true.\"\n    ],\n    \"algorithm\": \"Spot-adaptive Knowledge Distillation (SAKD) which merges teacher and student models into a multi-path routing network and uses a lightweight policy network to determine optimal distillation spots per sample at different training iterations.\"\n  },\n  \"Direct Inspiration\": [\n    \"b0\",\n    \"b1\",\n    \"b5\",\n    \"b6\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b2\",\n    \"b9\"\n  ],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The paper proposes a new distillation strategy termed spot-adaptive KD (SAKD) to make the distillation spots adaptive to training samples and distillation stages. By merging the student and teacher models into a multi-path routing network and employing a policy network to determine optimal propagation paths per sample, the method aims to address the inefficiencies of manually designed distillation spots in current KD approaches.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1.0,\n        \"b1\": 0.9,\n        \"b5\": 0.85,\n        \"b6\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.7,\n        \"b9\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b27\": 0.5,\n        \"b28\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The main challenge is to make the distillation spots adaptive to training samples and distillation stages, instead of manually setting them, to improve the performance and efficiency of knowledge distillation (KD).\",\n    \"inspirations\": \"The paper proposes a novel spot-adaptive distillation (SAKD) strategy that automatically determines the distillation spots, making them adaptive to the training samples and distillation stages.\"\n  },\n  \"Direct Inspiration\": [\"b0\", \"b1\", \"b5\", \"b6\", \"b9\"],\n  \"Indirect Inspiration\": [\"b2\", \"b8\", \"b11\", \"b12\"],\n  \"Other Inspiration\": [\"b27\", \"b28\", \"b30\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently distilling knowledge from large deep neural networks (DNNs) to smaller models, particularly focusing on the automatic determination of distillation spots. The proposed method, Spot-Adaptive Knowledge Distillation (SAKD), utilizes a multi-path routing network combined with a policy network to dynamically decide where to distill knowledge based on training samples and stages. The approach is novel in its focus on 'where to distill' instead of 'what to distill', allowing it to be integrated with existing distillation techniques to enhance performance.\",\n  \"Direct Inspiration\": [\"b0\", \"b5\", \"b6\", \"b9\"],\n  \"Indirect Inspiration\": [\"b1\", \"b2\"],\n  \"Other Inspiration\": []\n}\n```"], "63ae56ca90e50fcafda968ed": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the inadequacy of existing 'retrieve-then-read' pipelines for knowledge-intensive tasks, which often fail when simple search cannot find an answer. The paper introduces the DEMONSTRATE-SEARCH-PREDICT (DSP) framework, which decomposes complex questions and retrieves supporting information over multiple hops before generating a grounded response. The approach leverages both language models (LM) and retrieval models (RM) to systematically unify techniques from retrieval-augmented NLP and in-context learning literatures.\",\n  \"Direct Inspiration\": [\"b25\", \"b37\", \"b14\"],\n  \"Indirect Inspiration\": [\"b27\", \"b18\", \"b0\", \"b8\", \"b6\", \"b58\", \"b59\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of in-context learning for knowledge-intensive tasks by introducing the DEMONSTRATE-SEARCH-PREDICT (DSP) framework. This framework systematically unifies techniques from retrieval-augmented NLP and in-context learning, leveraging both language models (LM) and retrieval models (RM) to create sophisticated interactions that improve task performance. The DSP framework uses composable functions to bootstrap training examples (DEMONSTRATE), gather information (SEARCH), and generate outputs (PREDICT), achieving state-of-the-art results in knowledge-intensive tasks.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.9,\n    \"b25\": 0.9,\n    \"b27\": 0.8,\n    \"b37\": 0.8,\n    \"b43\": 0.8,\n    \"b59\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b8\": 0.7,\n    \"b12\": 0.7,\n    \"b14\": 0.7,\n    \"b18\": 0.7,\n    \"b20\": 0.7,\n    \"b34\": 0.7,\n    \"b39\": 0.7,\n    \"b44\": 0.7,\n    \"b58\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.6,\n    \"b58\": 0.6,\n    \"b59\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the limitation of simple 'retrieve-then-read' pipelines in knowledge-intensive tasks. The proposed algorithm, DEMONSTRATE-SEARCH-PREDICT (DSP), introduces a structured approach where a frozen language model (LM) and retrieval model (RM) work together to decompose and solve complex queries through composable functions, improving in-context learning. DSP aims to enable task-aware strategies without significant overhead, using novel techniques like bootstrapping annotations, iterative question rewriting, and generating grounded responses.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.8,\n    \"b0\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b20\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of unreliable outputs from language models (LMs) and the limitations of simple retrieve-then-read pipelines in knowledge-intensive tasks. The authors propose the DEMONSTRATE-SEARCH-PREDICT (DSP) framework which integrates LMs and retrieval models (RMs) to enhance in-context learning by systematically bootstrapping training examples, gathering information, and generating grounded outputs.\",\n  \"Direct Inspiration\": [\n    \"b39\",\n    \"b1\",\n    \"b25\",\n    \"b37\",\n    \"b20\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b14\",\n    \"b44\",\n    \"b12\"\n  ],\n  \"Other Inspiration\": [\n    \"b27\",\n    \"b18\",\n    \"b0\",\n    \"b8\",\n    \"b6\",\n    \"b58\",\n    \"b59\",\n    \"b34\",\n    \"b43\",\n    \"b37\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the unreliability of language models (LMs) in making factual assertions, particularly in knowledge-intensive tasks such as question answering and fact-checking. The proposed solution is the DEMONSTRATE-SEARCH-PREDICT (DSP) framework, which uses a combination of frozen LMs and retrieval models (RMs) to decompose complex questions and generate grounded responses. The DSP framework systematically unifies techniques from retrieval-augmented in-context learning and employs novel transformations to achieve state-of-the-art results.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.9,\n    \"b14\": 0.8,\n    \"b18\": 0.85,\n    \"b27\": 0.85,\n    \"b37\": 0.8,\n    \"b59\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b8\": 0.75,\n    \"b25\": 0.7,\n    \"b43\": 0.7,\n    \"b58\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.65,\n    \"b20\": 0.65,\n    \"b34\": 0.65,\n    \"b44\": 0.65\n  }\n}\n```"], "63aab708a4a9066abca549f8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of automatic diagnosis in the medical domain, focusing on symptom checking and disease diagnosis. It proposes a multi-task framework (MTDiag) that reformulates symptom checking as a multi-label classification task and employs a multi-task learning strategy with attention-based models to improve the accuracy and efficiency of the diagnosis process.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b17\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b12\": 0.5,\n    \"b15\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of automatic diagnosis in the medical domain, particularly focusing on symptom checking and disease diagnosis. It proposes a novel approach called MTDiag, which reformulates symptom checking as a multi-label classification task and employs a multi-task learning strategy to capture the relationship between disease and symptom. The framework also leverages contrastive learning to better distinguish symptoms among different diseases.\",\n  \"Direct Inspiration\": [\"b1\", \"b9\", \"b17\", \"b18\"],\n  \"Indirect Inspiration\": [\"b3\", \"b10\", \"b12\", \"b15\"],\n  \"Other Inspiration\": [\"b11\", \"b14\", \"b16\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of automatic diagnosis in the medical domain, particularly focusing on the limitations of reinforcement learning-based methods. It proposes a novel multi-task framework, MTDiag, which reformulates symptom checking as a multi-label classification task and employs a multi-task learning strategy using attentional pooling heads based on a Transformer encoder.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b17\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b12\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of automatic medical diagnosis, particularly the inefficiencies and data scarcity issues associated with RL-based methods. The authors propose an effective multi-task framework, MTDiag, which reformulates symptom checking as a multi-label classification task and employs a multi-task learning strategy to capture the relationship between symptoms and diseases.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b12\": 0.8,\n    \"b18\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b9\": 0.6,\n    \"b11\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of automatic diagnosis through interactions between an agent and a patient, where the agent collects necessary symptoms for diagnosis. The proposed MTDiag framework reformulates symptom checking as a multi-label classification task and employs a multi-task learning strategy to better capture the relationship between disease and symptom. Key improvements include transforming the sequential decision process into multiple independent training samples and using contrastive learning to differentiate symptoms of different diseases.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b17\": 0.85,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b3\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.65,\n    \"b7\": 0.6\n  }\n}\n```"], "620e302d5aee126c0fadda4d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving pipeline utilization in hardware multithreading for RISC-V architectures. The authors propose a new microarchitecture design and an efficient event-based issue scheduling algorithm that leverages the unique RISC-V ISA for early instruction decoding. The proposed solution is evaluated through simulations, demonstrating improved performance over traditional algorithms like Round Robin and Coarse Grain.\",\n  \"Direct Inspiration\": [\"b29\", \"b32\"],\n  \"Indirect Inspiration\": [\"b23\", \"b24\", \"b25\"],\n  \"Other Inspiration\": [\"b5\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include improving pipeline utilization and reducing CPU stalls in hardware multithreaded RISC-V architectures. The proposed solution is an efficient event-based issue scheduling algorithm that leverages the unique RISC-V ISA to decode instruction types early in the pipeline. This new algorithm is evaluated using simulations and benchmarks, showing significant performance improvements over classical scheduling algorithms.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving pipeline utilization in hardware multithreaded RISC-V pipeline architectures through a novel issue scheduling algorithm. The proposed Efficient Hint-Based Event (EHE) Issue Scheduling algorithm seeks to outperform classical Round Robin and coarse-grain algorithms by leveraging early instruction decoding specific to the RISC-V ISA.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b24\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a new multithreading in-order pipeline microarchitecture design for RISC-V and an efficient event-based issue scheduling algorithm, which improves pipeline utilization by up to 26% in terms of IPC using four threads. The primary challenges addressed include enhancing hardware multithreading performance, efficient issue scheduling, and reducing pipeline stalls.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b20\": 0.4,\n    \"b21\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving hardware multithreading for RISC-V pipelines by proposing a novel event-based issue scheduling algorithm. This approach aims to better utilize pipeline stages by preemptively addressing potential delays caused by specific instruction types.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b5\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b19\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b29\": 0.65\n  }\n}\n```"], "625f6bf75aee126c0ffb36b6": ["```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in automatic medical consultation: the lack of unified frameworks and tasks, and the lack of benchmark datasets. The authors propose a unified framework for automatic medical consultation and introduce DialoAMC, a comprehensive annotated medical dialogue corpus. They also establish several tasks based on this corpus and report benchmark results.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b18\": 0.9,\n    \"b33\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.7,\n    \"b29\": 0.7,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of creating an automatic medical consultation system, focusing on the lack of unified frameworks and benchmark datasets. It introduces DialoAMC, a large-scale annotated medical dialogue corpus, and proposes a unified design of frameworks and tasks for automatic medical consultation, including dialogue understanding and task-oriented interaction frameworks.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b45\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b39\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in online medical consultation: the lack of a unified framework for automatic consultation and the absence of comprehensive benchmark datasets. The authors propose a unified design of frameworks and tasks for automatic medical consultation, introduce DialoAMC, a large-scale annotated medical dialogue corpus, and develop neural-based models to establish benchmark results.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b18\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.8,\n    \"b34\": 0.8,\n    \"b22\": 0.8,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b7\": 0.6,\n    \"b42\": 0.6,\n    \"b37\": 0.5,\n    \"b15\": 0.5,\n    \"b17\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of automating the medical consultation process by proposing a unified framework and tasks for automatic medical consultation. The authors introduce DialoAMC, a large-scale annotated medical dialogue corpus, and establish a series of tasks to evaluate its effectiveness. The main contributions include the design of both static and dynamic frameworks for dialogue understanding and task-oriented interaction, and the release of the DialoAMC corpus for future research.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b45\": 0.9,\n    \"b34\": 0.9,\n    \"b18\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.8,\n    \"b29\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.7,\n    \"b31\": 0.7,\n    \"b35\": 0.7,\n    \"b5\": 0.7,\n    \"b3\": 0.7,\n    \"b8\": 0.7,\n    \"b44\": 0.7,\n    \"b22\": 0.7,\n    \"b37\": 0.7,\n    \"b15\": 0.7,\n    \"b42\": 0.7,\n    \"b17\": 0.7,\n    \"b7\": 0.7,\n    \"b13\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a unified framework for automatic medical consultation and the lack of benchmark datasets. It proposes a framework with two modes (dialogue understanding and task-oriented interaction) and introduces a large-scale annotated medical dialogue corpus called DialoAMC. The contributions include the design of frameworks, tasks, and the establishment of benchmark results.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.8,\n    \"b34\": 0.7,\n    \"b22\": 0.7,\n    \"b18\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.6,\n    \"b13\": 0.6,\n    \"b29\": 0.6,\n    \"b42\": 0.6,\n    \"b37\": 0.6,\n    \"b15\": 0.6,\n    \"b17\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```"], "63bcd73690e50fcafdefa108": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the problem of missing node attributes in heterogeneous graphs, which significantly impacts the performance of heterogeneous graph neural networks (HGNNs). The proposed solution, AutoAC, is a differentiable attribute completion framework designed to automate the search for optimal completion operations for each no-attribute node in heterogeneous GNNs. The algorithm integrates different heterogeneous GNNs and employs a continuous relaxation scheme for the search space, making the search process differentiable via gradient descent. Additionally, it uses proximal iteration for efficient optimization and an auxiliary unsupervised clustering task to reduce the dimension of completion parameters.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b7\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b12\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of missing node attributes in heterogeneous graphs and proposes a differentiable attribute completion framework called AutoAC for automated completion operation search in heterogeneous GNNs. The proposed methodology includes a continuous relaxation scheme, proximal iteration for constrained optimization, and an auxiliary unsupervised clustering task to reduce the dimension of completion parameters.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1,\n        \"b18\": 0.9,\n        \"b37\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.7,\n        \"b10\": 0.7,\n        \"b12\": 0.6,\n        \"b23\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.5,\n        \"b4\": 0.5,\n        \"b6\": 0.5,\n        \"b7\": 0.5,\n        \"b8\": 0.5,\n        \"b13\": 0.5,\n        \"b16\": 0.5,\n        \"b22\": 0.5,\n        \"b24\": 0.5,\n        \"b25\": 0.5,\n        \"b26\": 0.5,\n        \"b27\": 0.5,\n        \"b28\": 0.5,\n        \"b29\": 0.5,\n        \"b30\": 0.5,\n        \"b31\": 0.5,\n        \"b32\": 0.5,\n        \"b33\": 0.5,\n        \"b34\": 0.5,\n        \"b35\": 0.5,\n        \"b36\": 0.5,\n        \"b38\": 0.5,\n        \"b39\": 0.5,\n        \"b40\": 0.5,\n        \"b41\": 0.5,\n        \"b42\": 0.5,\n        \"b43\": 0.5,\n        \"b44\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of missing node attributes in heterogeneous graph neural networks (HGNNs). The proposed solution is the AutoAC framework, which automates the search for optimal attribute completion operations for nodes with missing attributes. The framework integrates various topology-dependent and topology-independent attribute completion methods and employs a differentiable search strategy using gradient descent. Additionally, the framework includes optimization techniques like discrete constraints and an auxiliary unsupervised clustering task to enhance search efficiency.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.9,\n    \"b37\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.7,\n    \"b16\": 0.8,\n    \"b36\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of missing attributes in heterogeneous graphs and proposes a differentiable attribute completion framework called AutoAC. The framework integrates different heterogeneous GNNs and automates the search for optimal completion operations using a continuous relaxation scheme and proximal iteration for efficient optimization.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.85,\n    \"b37\": 0.75,\n    \"b7\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.60,\n    \"b36\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of completing missing node attributes in heterogeneous graph neural networks (HGNNs). The proposed solution, AutoAC, introduces a differentiable attribute completion framework that searches for optimal completion operations for nodes with missing attributes. This is achieved through a continuous relaxation scheme, constrained bi-level joint optimization, and an auxiliary unsupervised clustering task to improve search efficiency.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b8\": 0.8,\n    \"b9\": 0.75,\n    \"b10\": 0.75,\n    \"b12\": 0.75,\n    \"b16\": 0.75,\n    \"b22\": 0.6,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b27\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6,\n    \"b36\": 0.7,\n    \"b37\": 0.7,\n    \"b38\": 0.6,\n    \"b39\": 0.6,\n    \"b40\": 0.6,\n    \"b41\": 0.6,\n    \"b42\": 0.6,\n    \"b43\": 0.6,\n    \"b44\": 0.6\n  }\n}\n```"], "6215a4242c356815940385b3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of training and inference in Graph Convolutional Networks (GCNs) due to their high computational costs and proposes a novel method inspired by DNN compression techniques, specifically the lottery ticket (LT) hypothesis, to discover 'graph early-bird (GEB) tickets'. The paper aims to minimize GCN complexity through a co-optimization framework that simultaneously sparsifies both the GCN graphs and models.\",\n  \"Direct Inspiration\": [\"b4\", \"b15\", \"b26\"],\n  \"Indirect Inspiration\": [\"b19\", \"b28\"],\n  \"Other Inspiration\": [\"b6\", \"b24\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper are the high computational cost, large memory requirements, and complexity of training and inference for GCNs due to large and irregular graph data, high-dimensional node feature vectors, and sparse adjacency matrices.\",\n    \"inspirations\": \"The paper is inspired by the success of DNN compression, particularly the lottery ticket hypothesis, to develop methods for reducing the complexity of GCNs without compromising their performance.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b4\", \"b15\", \"b26\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b19\", \"b28\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b6\", \"b24\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high computational cost in training and inference of Graph Convolutional Networks (GCNs) due to large graph sizes, high-dimensional node features, and sparse adjacency matrices. Inspired by the success of DNN compression, specifically the lottery ticket hypothesis, the authors propose a novel method to identify and leverage Graph Early-Bird (GEB) tickets and joint-EB tickets to significantly improve the efficiency of GCN training and inference while maintaining performance.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b15\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b19\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the significant challenges of training and inference in Graph Convolutional Networks (GCNs) due to their large memory and computational demands. The authors propose a novel method inspired by DNN compression, specifically the lottery ticket hypothesis, to introduce Graph Early-Bird (GEB) tickets and develop a co-sparsification framework called GEBT to optimize both GCN graphs and networks simultaneously.\",\n  \"Direct Inspiration\": [\"b4\", \"b15\", \"b26\"],\n  \"Indirect Inspiration\": [\"b28\", \"b19\"],\n  \"Other Inspiration\": [\"b12\", \"b24\", \"b6\"]\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the computational challenges of training and inference for Graph Convolutional Networks (GCNs) on large and irregular graphs. It proposes a novel approach for GCN compression inspired by the success of DNN compression techniques, particularly the lottery ticket hypothesis. The paper introduces the concept of Graph Early-Bird (GEB) tickets and a new GCN training framework called GEBT that significantly improves training and inference efficiency by co-sparsifying GCN graphs and models.\",\n  \"Direct Inspiration\": [\"b4\", \"b15\", \"b26\"],\n  \"Indirect Inspiration\": [\"b28\", \"b19\"],\n  \"Other Inspiration\": [\"b6\", \"b24\"]\n}\n```"], "63a1751790e50fcafd1f4880": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the application of label smoothing in multi-hop question answering (MHQA) tasks. The authors propose a novel method, F1 smoothing, specifically tailored for machine reading comprehension tasks to improve the performance of MHQA models. The paper aims to systematically analyze the role of label smoothing across various MHQA modules, including document retrieval, supporting evidence prediction, answer type selection, and answer span extraction. The effectiveness of the proposed method is evaluated on the HotpotQA dataset.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b34\": 0.9,\n    \"b31\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.7,\n    \"b39\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b22\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the application of label smoothing in multi-hop question answering (MHQA) tasks and the need for systematic analysis of its impact on various modules such as document retrieval, supporting evidence prediction, answer type selection, and answer span extraction. The paper proposes a novel method called F1 smoothing tailored for machine reading comprehension (MRC) tasks and evaluates its performance on the HotpotQA dataset. The paper also introduces a simpler model called C2FM for studying label smoothing in MHQA.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1,\n    \"b18\": 1\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b11\": 0.9,\n    \"b22\": 0.9,\n    \"b37\": 0.8,\n    \"b39\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively applying label smoothing to various modules of multi-hop question answering (MHQA), a task that requires aggregating information from multiple sources and performing complex reasoning steps. The authors propose a novel label smoothing method, F1 Smoothing, tailored for machine reading comprehension (MRC) tasks. They systematically analyze the role of label smoothing in document retrieval, supporting evidence prediction, answer type selection, and answer span extraction, demonstrating its effectiveness on the Hot-potQA dataset.\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.9,\n    \"b18\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.75,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.65,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving the performance of multi-hop question answering (MHQA) by systematically analyzing and applying label smoothing techniques. The authors propose a novel method, F1 smoothing, tailored for machine reading comprehension (MRC) tasks and evaluate its effectiveness on the HotpotQA dataset. The main contributions include a systematic analysis of label smoothing's impact on various MHQA modules and demonstrating the effectiveness of the proposed method.\",\n    \"Direct Inspiration\": {\n        \"b34\": 1,\n        \"b18\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b30\": 0.8,\n        \"b37\": 0.7,\n        \"b39\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b31\": 0.6,\n        \"b41\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in this paper is the application of label smoothing techniques to improve the performance of multi-hop question answering (MHQA) tasks. The authors propose a novel label smoothing method called F1 Smoothing and systematically analyze its impact on different modules of MHQA, including document retrieval, supporting evidence prediction, answer type selection, and answer span extraction.\",\n    \"Direct Inspiration\": {\n        \"b34\": 0.9,\n        \"b18\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b37\": 0.8,\n        \"b31\": 0.75,\n        \"b39\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.6,\n        \"b22\": 0.6,\n        \"b30\": 0.65\n    }\n}\n```"], "63d9d87b90e50fcafd580980": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of few-shot object detection (FSOD), which involves detecting objects in novel classes with only a few examples by leveraging knowledge from base classes. To enhance recognition capabilities and reduce class bias, the authors propose a meta-learning framework with novel methods such as Class-Agnostic Aggregation (CAA) and Variational Feature Aggregation (VFA). These methods aim to improve the robustness to example variances and reduce the confusion between novel and base classes.\",\n    \"Direct Inspiration\": {\n        \"b38\": 1.0,\n        \"b24\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.8,\n        \"b32\": 0.85,\n        \"b26\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b23\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot object detection (FSOD), which involves improving recognition of novel classes with limited examples by transferring knowledge from base classes. It proposes a meta-learning framework with key innovations, including Class-Agnostic Aggregation (CAA) and Variational Feature Aggregation (VFA), to reduce class bias and improve robustness to example variance.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1.0,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.7,\n    \"b23\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge in the paper is the few-shot object detection (FSOD) problem, where there are abundant samples for base classes but only a few for novel classes. The paper proposes a meta-learning framework, including Class-Agnostic Aggregation (CAA) and Variational Feature Aggregation (VFA), to reduce class bias and improve robustness to example variance. The key inspirations come from previous works on FSOD, meta-learning, and variational feature learning.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1.0,\n    \"b32\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b39\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of few-shot object detection (FSOD) by transferring knowledge from base classes to novel classes. The proposed meta-learning framework includes Class-Agnostic Aggregation (CAA) and Variational Feature Aggregation (VFA) to reduce class bias and improve robustness to example variances.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.9,\n    \"b24\": 0.85,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.75,\n    \"b10\": 0.7,\n    \"b23\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of few-shot object detection (FSOD) by proposing a meta-learning framework. Key challenges include class bias due to imbalance between base and novel classes and sensitivity to the variance of novel examples. The proposed methods, Class-Agnostic Aggregation (CAA) and Variational Feature Aggregation (VFA), aim to reduce class bias and improve robustness to example variance.\",\n  \"Direct Inspiration\": {\n    \"b38\": 0.9,\n    \"b32\": 0.8,\n    \"b24\": 0.7,\n    \"b26\": 0.6\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5\n  }\n}\n```"], "62ce38205aee126c0f18bb75": ["```json\n{\n  \"Summary\": \"The paper primarily addresses the challenge of enabling large language models to self-evaluate their own responses. It introduces methods for improving calibration and self-assessment of the models' knowledge and responses, emphasizing the importance of accurate and honest AI systems.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.85,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.75,\n    \"b9\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b18\": 0.7,\n    \"b20\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the calibration of language models, specifically their ability to self-evaluate the correctness of their own generated answers. The paper introduces methods to improve model calibration and self-evaluation, highlighting improvements with model size and few-shot prompting. The core contributions include techniques for self-evaluation without finetuning, the use of multiple-choice formats for calibration, and training models to predict their ability to answer questions correctly (P(IK)).\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.95,\n    \"b11\": 0.9,\n    \"b19\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.75,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.65,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper investigates the ability of a 52B parameter language model to self-evaluate its responses to various types of questions. The main challenges addressed include improving model calibration and enabling models to assess their own knowledge and confidence. The proposed methods involve few-shot learning, brainstorming multiple answers, and training models to predict their confidence in answering correctly (P(IK)).\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b10\": 0.6,\n    \"b9\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to train AI systems, specifically large language models, to be honest and accurately evaluate their own confidence in their knowledge and reasoning. The paper proposes methods to improve and assess the calibration of language models on various tasks, and introduces techniques for self-evaluation where models assess the correctness of their own generated answers.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b9\": 0.75,\n    \"b18\": 0.75,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b12\": 0.6,\n    \"b13\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper explores the ability of large language models (LLMs) to evaluate their own responses to various tasks and questions. The primary challenge is to train AI systems that can accurately and faithfully assess their confidence in their knowledge and reasoning. The study focuses on calibration and self-evaluation methods to improve model performance in identifying correct answers and their own knowledge gaps.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b1\": 0.8,\n    \"b18\": 0.85,\n    \"b11\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b5\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.65,\n    \"b3\": 0.6,\n    \"b12\": 0.6,\n    \"b8\": 0.55\n  }\n}\n```"], "629587465aee126c0fe149f5": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include addressing the issue of 'hallucinations' or false statements produced by state-of-the-art language models when generating long-form text, and the lack of epistemic uncertainty in model outputs. The proposed algorithm involves finetuning GPT-3 to express epistemic uncertainty using 'verbalized probability' to make models' confidence levels more aligned with human-like expressions of uncertainty.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b33\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of language models producing false statements or 'hallucinations' when generating long-form text. It proposes an algorithm for finetuning models to express epistemic uncertainty using natural language, termed 'verbalized probability'. The goal is calibration\u2014ensuring that models' expressed uncertainty about their statements is aligned with their actual correctness.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b14\": 0.75,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the issue of 'hallucinations' or false statements produced by state-of-the-art language models when generating long-form text. It proposes a novel approach called 'verbalized probability' to express epistemic uncertainty in a human-like manner. This involves finetuning models to produce calibrated probabilities using natural language, focusing on the calibration of confidence rather than the accuracy of answers. The paper introduces a new test suite called CalibratedMath to evaluate this method.\",\n    \"Direct Inspiration\": {\n        \"b19\": 0.9,\n        \"b20\": 0.9,\n        \"b24\": 0.9,\n        \"b29\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b14\": 0.8,\n        \"b27\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.7,\n        \"b4\": 0.7,\n        \"b13\": 0.7,\n        \"b8\": 0.7,\n        \"b33\": 0.7,\n        \"b5\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the issue of language models, such as GPT-3, producing false statements or 'hallucinations' when generating long-form text. This undermines their reliability as users cannot discern the truthfulness of the models' outputs. To address this, the paper introduces the concept of 'verbalized probability,' where models express their confidence in natural language. The goal is to improve the calibration of language models, making them more honest and reliable. The authors propose finetuning GPT-3 to express calibrated verbalized probabilities and introduce a new test suite called CalibratedMath to evaluate the model's performance.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1,\n        \"b33\": 0.9,\n        \"b5\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b14\": 0.75,\n        \"b27\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.6,\n        \"b20\": 0.55,\n        \"b24\": 0.65,\n        \"b29\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge outlined in the paper is the issue of language models, like GPT-3, generating false statements or 'hallucinations' when producing long-form text, thereby reducing their trustworthiness. The paper aims to tackle this by finetuning models to express epistemic uncertainty using natural language, called 'verbalized probability'.\",\n    \"Inspirations\": \"The paper is inspired by previous works on model calibration and uncertainty, particularly focusing on methods to improve how models express uncertainty in a human-like way.\"\n  },\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b24\": 1,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b20\": 0.7,\n    \"b29\": 0.7\n  }\n}\n```"], "629587475aee126c0fe14c42": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of unstable generalization in prompt learning with pre-trained language models (PLMs) in low-resource or emerging domains. The authors propose a novel retrieval-augmented framework called RETROPROMPT, which integrates an open-book knowledge store, kNN-guided training, and neural demonstrations to decouple knowledge from memorization and improve generalization.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b34\": 0.6,\n    \"b36\": 0.6,\n    \"b55\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b18\": 0.5,\n    \"b31\": 0.5,\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unstable generalization in prompt learning with pre-trained language models (PLMs) in low-resource or emerging domains. The proposed solution, RETROPROMPT, aims to decouple knowledge from memorization by introducing a retrieval-augmented framework that incorporates neural demonstrations and k-nearest neighbors (kNN) to enhance input sequences and guide training. This approach helps improve model generalization, especially in few-shot and zero-shot tasks.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b39\": 0.8,\n    \"b58\": 0.85,\n    \"b51\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b31\": 0.65,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b11\": 0.55,\n    \"b32\": 0.5,\n    \"b46\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the generalization ability of prompt learning in large parametric language models, particularly in low-resource settings or emerging domains. The proposed solution, RETROPROMPT, incorporates a novel retrieval-augmented framework that uses an open-book knowledge store to decouple knowledge from memorization, thereby enhancing model performance. Key inspirations include leveraging retrieval mechanisms, neural demonstrations, and k-nearest neighbors (kNN) to focus on hard examples and improve few-shot/zero-shot learning capabilities.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.95,\n    \"b24\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.75,\n    \"b36\": 0.75,\n    \"b55\": 0.75,\n    \"b9\": 0.70,\n    \"b10\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.65,\n    \"b31\": 0.60,\n    \"b14\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unstable generalization in prompt learning with pre-trained language models (PLMs) in low-resource settings and emerging domains. It proposes the RETROPROMPT framework, which enhances generalization by decoupling knowledge from memorization through a retrieval-augmented approach. This involves creating an open-book knowledge store, using k-nearest neighbors (kNN) for guiding training and inference, and incorporating neural demonstrations to improve few-shot and zero-shot learning.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b24\": 0.8,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b14\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the unstable generalization of prompt learning with PLMs in low-resource settings or emerging domains. The proposed solution, RETROPROMPT, aims to improve generalization by utilizing a retrieval-augmented framework that decouples knowledge from memorization, incorporating neural demonstrations, kNN guided training, and kNN-based probability for cloze-style prediction.\",\n    \"Direct Inspiration\": {\n        \"b11\": 0.9,\n        \"b34\": 0.85,\n        \"b36\": 0.85,\n        \"b55\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.75,\n        \"b10\": 0.75,\n        \"b18\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b24\": 0.6,\n        \"b31\": 0.55\n    }\n}\n```"], "624fa8db5aee126c0f3a5b79": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of bridging the semantic gap between video and text modalities in Text-Video Retrieval (TVR). It proposes the HunYuan_tvr method, which hierarchically explores video-sentence, clip-phrase, and frame-word interactions. HunYuan_tvr leverages large-scale pretraining knowledge from CLIP and introduces novel strategies such as adaptive label denoising and marginal sample enhancement to improve feature discrimination and retrieval performance.\",\n    \"Direct Inspiration\": [\"b21\", \"b27\"],\n    \"Indirect Inspiration\": [\"b3\", \"b24\"],\n    \"Other Inspiration\": [\"b9\", \"b17\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the semantic gap challenge in Text-Video Retrieval (TVR) by proposing a novel method named HunYuan_tvr. This method hierarchically explores video-sentence, clip-phrase, and frame-word interactions to understand text-video contents comprehensively. It leverages large-scale pretraining knowledge from CLIP and designs adaptive label denoising and marginal sample enhancement strategies to discover potential positive pairs and improve feature discrimination, achieving state-of-the-art performance on various benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b27\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b15\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of bridging the semantic gap between video and text in Text-Video Retrieval (TVR) by proposing a novel hierarchical method named HunYuan_tvr. This method leverages hierarchical cross-modal interactions at different granularities (video-sentence, clip-phrase, and frame-word) and includes adaptive label denoising and marginal sample enhancement strategies to improve retrieval performance.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b24\": 0.9,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b9\": 0.75,\n    \"b10\": 0.7,\n    \"b15\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b19\": 0.6,\n    \"b23\": 0.6,\n    \"b4\": 0.6,\n    \"b8\": 0.6,\n    \"b18\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of the semantic gap between video and text in Text-Video Retrieval (TVR). It proposes a novel hierarchical method, HunYuan_tvr, that explores interactions at video-sentence, clip-phrase, and frame-word levels. Key contributions include adaptive label denoising and marginal sample enhancement strategies to improve retrieval accuracy.\",\n    \"Direct Inspiration\": {\n        \"b21\": 1.0,\n        \"b27\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b24\": 0.9,\n        \"b3\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.7,\n        \"b29\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of the semantic gap between video and text modalities in Text-Video Retrieval (TVR). It proposes a novel method named HunYuan_tvr, which hierarchically explores video-sentence, clip-phrase, and frame-word interactions. The method leverages CLIP as visual and text encoders and introduces hierarchical cross-modal contrastive learning, adaptive label denoising, and marginal sample enhancement strategies to improve retrieval performance.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b27\": 1,\n    \"b24\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```"], "634d805690e50fcafd4e07d5": ["```json\n{\n    \"Summary\": \"The paper addresses the performance challenges of managed languages and introduces novel runtime instrumentation and a benchmark suite called LangBench. The primary contributions are runtime instrumentations for OpenJDK, V8, and CPython, and the development of LangBench for evaluating performance across different languages.\",\n    \"Direct Inspiration\": {\n        \"b21\": 0.9,\n        \"b55\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.75,\n        \"b43\": 0.7,\n        \"b66\": 0.7,\n        \"b49\": 0.65,\n        \"b24\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b68\": 0.6,\n        \"b58\": 0.6,\n        \"b70\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance and scalability challenges of managed programming languages (JavaScript, Java, Python, and Go) by providing a quantitative performance analysis and introducing runtime instrumentation and a benchmark suite (LangBench) to evaluate these languages. Key contributions include runtime instrumentations for OpenJDK, V8, and CPython, a comprehensive benchmark suite, and comparative performance analysis relative to C++.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b45\": 0.85,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.7,\n    \"b35\": 0.65,\n    \"b0\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b69\": 0.55,\n    \"b54\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of understanding the performance and scalability implications of managed languages. It proposes runtime instrumentations for OpenJDK, V8, and CPython, a new benchmark suite called LangBench, and a comparative performance analysis of these languages against C++.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b21\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b19\", \"b23\", \"b36\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b47\", \"b69\", \"b45\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges of using managed languages for performance-critical systems and proposes a thorough quantitative performance analysis of popular managed languages (CPython, OpenJDK, Node.js/V8, and Go) compared to C++. It introduces runtime instrumentations for profiling, a benchmark suite called LangBench, and conducts a comparative analysis to understand the performance implications along dimensions such as typing, execution modes, and concurrency models.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b13\": 1,\n    \"b20\": 1,\n    \"b31\": 1,\n    \"b33\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b22\": 0.8,\n    \"b36\": 0.8,\n    \"b39\": 0.8,\n    \"b34\": 0.8,\n    \"b28\": 0.8,\n    \"b29\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b67\": 0.6,\n    \"b32\": 0.6,\n    \"b55\": 0.6,\n    \"b58\": 0.6,\n    \"b70\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper tackles the challenge of understanding the performance and scalability of managed languages (JavaScript, Java, Python, and Go) as compared to C++. It introduces novel contributions including runtime instrumentations, a new benchmark suite called LangBench, and a comprehensive comparative analysis of these languages.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b45\": 0.85,\n    \"b36\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.75,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.65,\n    \"b47\": 0.6,\n    \"b69\": 0.55\n  }\n}\n```"], "62281ae45aee126c0f7aa8a8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of Hierarchical Text Classification (HTC), particularly focusing on modeling the large-scale, imbalanced, and structured label hierarchy. The authors propose a novel approach called Hierarchy-Guided Contrastive Learning (HGCLR) to obtain hierarchy-aware text representation by injecting constant hierarchy representation into the text encoder. The methodology includes using a customized Graphormer as the graph encoder and constructing positive samples by retaining key tokens, guided by label hierarchy.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1.0,\n    \"b11\": 0.9,\n    \"b20\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b8\": 0.7,\n    \"b33\": 0.8,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.7,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of Hierarchical Text Classification (HTC) by proposing a novel approach called Hierarchy-Guided Contrastive Learning (HGCLR). The key challenges are modeling the large-scale, imbalanced, and structured label hierarchy. The authors propose injecting constant hierarchy representation into the text encoder and using contrastive learning to generate hierarchy-aware text representations. They also introduce a modified Graphormer for encoding the label hierarchy.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1.0,\n        \"b37\": 1.0,\n        \"b33\": 1.0,\n        \"b35\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.9,\n        \"b14\": 0.9,\n        \"b11\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.8,\n        \"b20\": 0.8,\n        \"b19\": 0.8,\n        \"b5\": 0.8,\n        \"b25\": 0.8,\n        \"b8\": 0.8\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of Hierarchical Text Classification (HTC), which involves categorizing text into labels organized in a structured hierarchy. The key challenge is modeling the large-scale, imbalanced, and structured label hierarchy. This paper proposes Hierarchy-Guided Contrastive Learning (HGCLR) to obtain hierarchy-aware text representations. The paper introduces a novel approach for generating positive samples guided by label hierarchy and employs a modified Graphormer for encoding the label hierarchy.\",\n    \"Direct Inspiration\": {\n        \"b33\": 0.9,\n        \"b37\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.7,\n        \"b20\": 0.7,\n        \"b35\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5,\n        \"b5\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Hierarchical Text Classification (HTC) by proposing a novel method called Hierarchy-Guided Contrastive Learning (HGCLR). This method integrates label hierarchy into text encoding using contrastive learning to generate hierarchy-aware text representations. The key contributions include the construction of high-quality positive samples guided by label hierarchy and the use of a modified Graphormer as the graph encoder.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1.0,\n    \"b35\": 0.9,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b20\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in Hierarchical Text Classification (HTC) by proposing a novel approach called Hierarchy-Guided Contrastive Learning (HGCLR). This approach aims to achieve hierarchy-aware text representation by integrating label hierarchy into text encoding through contrastive learning.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b37\": 1.0,\n    \"b35\": 1.0,\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b11\": 0.8,\n    \"b8\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b2\": 0.6,\n    \"b5\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```"], "63aa965790e50fcafd008008": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of translating natural language queries into CQL (Cypher Query Language) for graph databases, particularly Neo4j, due to the complexities and differences between CQL and SQL. The authors introduce the Text-to-CQL task, propose the first dataset for this task named SpCQL, and highlight their approach to data collection and annotation to ensure realistic and diverse query-CQL pairs.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b3\": 0.8,\n    \"b7\": 0.8,\n    \"b0\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n    \"Summary\": \"This paper addresses the challenge of translating natural language queries into CQL (Cypher Query Language) for graph databases, specifically Neo4j. It proposes a new task, Text-to-CQL, and introduces the SpCQL dataset, which contains 10,000 query-CQL pairs. The dataset and task aim to facilitate automatic translation from natural language to CQL, addressing the unique complexities of graph data queries that differ from SQL.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1.0,\n        \"b9\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b4\": 0.8,\n        \"b7\": 0.8,\n        \"b0\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.7,\n        \"b6\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of translating natural language queries into Cypher Query Language (CQL) for graph databases, specifically Neo4j. Due to the complexity and differences between CQL and SQL, existing Text-to-SQL models are not directly applicable. The authors propose a new semantic parsing task, Text-to-CQL, and introduce a dataset named SpCQL containing a Neo4j database and 10,000 query-CQL pairs.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed by the paper is the difficulty in translating natural language queries into Cypher (CQL) for graph databases, specifically Neo4j. The authors propose a new semantic parsing task, Text-to-CQL, and introduce the first dataset for this task, named SpCQL, which includes a Neo4j database and 10,000 query-CQL pairs. The paper outlines the differences between SQL and CQL and explains why existing Text-to-SQL models are not applicable for Text-to-CQL.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge is the difficulty of translating natural language queries into Cypher Query Language (CQL) due to its complex operations and syntax, which are different from SQL.\",\n    \"inspiration\": \"The need for an effective semantic parsing technology for translating natural language queries into CQL, inspired by the successes of Text-to-SQL models and the datasets like WikiSQL and Spider.\"\n  },\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b0\": 0.8,\n    \"b5\": 0.7,\n    \"b6\": 0.7\n  }\n}\n```"], "63eef09c90e50fcafda0d2a1": ["```json\n{\n  \"Summary\": \"The key challenges of the paper include the data sparsity issue in learning high-quality user and item representations and the limitations of existing graph contrastive learning (GCL) methods, such as random perturbation losing useful information, reliance on heuristic-guided schemes, and over-smoothing issues. The proposed LightGCL model aims to address these challenges by introducing a lightweight and robust GCL framework that leverages singular value decomposition (SVD) to distill useful information and inject global collaborative context into the representation alignment.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b33\": 1,\n    \"b27\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b35\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b10\": 0.8,\n    \"b29\": 0.8,\n    \"b3\": 0.8,\n    \"b14\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b38\": 0.6,\n    \"b20\": 0.6,\n    \"b4\": 0.6,\n    \"b15\": 0.6,\n    \"b37\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses several primary challenges in graph neural network (GNN)-based recommender systems, particularly the data sparsity issue, the limitations of current heuristic-based graph contrastive learning (GCL) methods, and the over-smoothing problem. It proposes a lightweight and robust graph contrastive learning framework called LightGCL, which uses singular value decomposition (SVD) for graph augmentation to retain useful structural information and inject global collaborative context.\",\n    \"Direct Inspiration\": [\"b24\", \"b33\", \"b27\", \"b11\"],\n    \"Indirect Inspiration\": [\"b6\", \"b35\", \"b12\", \"b29\"],\n    \"Other Inspiration\": [\"b16\", \"b17\", \"b3\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Data sparsity issue in learning high-quality user and item representations.\",\n      \"Misleading representation learning due to random perturbation in graph augmentation.\",\n      \"Model generality and vulnerability to noisy user behaviors.\",\n      \"Over-smoothing issue in GNN-based contrastive recommenders.\"\n    ],\n    \"inspirations\": [\n      \"Contrastive learning for data augmentation and addressing label scarcity.\",\n      \"Singular value decomposition (SVD) for guided graph augmentation.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b24\", \"b33\", \"b27\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b6\", \"b12\", \"b31\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b1\", \"b13\", \"b16\", \"b17\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses key challenges in graph neural networks (GNNs) for recommender systems, focusing on data sparsity, noisy user behaviors, and over-smoothing issues. The authors propose a novel contrastive learning paradigm, LightGCL, which uses singular value decomposition (SVD) for graph augmentation to enhance representation learning by preserving global collaborative signals.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b33\": 1.0,\n    \"b27\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b30\": 0.7,\n    \"b26\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"data sparsity in learning user and item representations\",\n      \"limitations of heuristic-based contrastive view generators\",\n      \"over-smoothing issue in GNN-based contrastive recommenders\"\n    ],\n    \"inspirations\": [\n      \"contrastive learning for data augmentation\",\n      \"singular value decomposition (SVD) for graph augmentation\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\n      \"b24\",\n      \"b33\",\n      \"b27\",\n      \"b11\"\n    ]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\n      \"b30\",\n      \"b26\",\n      \"b39\",\n      \"b31\"\n    ]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\n      \"b3\",\n      \"b14\"\n    ]\n  }\n}\n```"], "63eef09c90e50fcafda0cfa0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of unifying pre-training and downstream tasks for Graph Neural Networks (GNNs) and proposes a novel framework called GraphPrompt. The main objectives are to align the pre-training and downstream tasks using subgraph similarity and to introduce task-specific learnable prompts for effective knowledge transfer.\",\n  \"Direct Inspiration\": [\"b3\", \"b38\"],\n  \"Indirect Inspiration\": [\"b14\", \"b15\", \"b29\", \"b33\"],\n  \"Other Inspiration\": [\"b12\", \"b19\", \"b42\", \"b50\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of bridging the gap between graph pre-training and various downstream tasks by proposing a unified graph pre-training and prompting framework called GraphPrompt. This framework aims to align pre-training tasks with downstream tasks using a common template based on subgraph similarity, and introduces task-specific learnable prompts to guide downstream tasks effectively.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.9,\n    \"b15\": 0.9,\n    \"b29\": 0.8,\n    \"b53\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b42\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of unifying pre-training and downstream tasks for graph neural networks (GNNs) and proposes a novel framework called GraphPrompt. The framework uses a unified template based on subgraph similarity to align pre-training and downstream tasks. It introduces a task-specific learnable prompt to guide the ReadOut operation in downstream tasks, enhancing the transfer of knowledge from the pre-trained model.\",\n  \"Direct Inspiration\": [\"b3\", \"b38\"],\n  \"Indirect Inspiration\": [\"b0\", \"b6\", \"b14\", \"b15\", \"b29\", \"b53\"],\n  \"Other Inspiration\": [\"b12\", \"b19\", \"b42\", \"b50\", \"b54\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unifying pre-training and downstream tasks for Graph Neural Networks (GNNs) to handle different tasks like node and graph classification. It proposes a framework called GraphPrompt, which unifies these tasks using subgraph similarity. The framework also introduces task-specific learnable prompts to guide downstream tasks effectively.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b29\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unifying pre-training and downstream tasks for graph neural networks (GNNs) to ensure effective knowledge transfer and task-specific optimization. The proposed solution, GraphPrompt, leverages a unified template based on subgraph similarity for both pre-training and downstream tasks and introduces task-specific learnable prompts to guide the ReadOut operation for downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b38\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.75,\n    \"b15\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b19\": 0.6,\n    \"b50\": 0.6\n  }\n}\n```"], "62620f1c5aee126c0f686d0d": ["```json\n{\n  \"Summary\": \"The paper addresses primary challenges in training GNNs, such as over-fitting, over-smoothing, and non-robustness, and proposes a novel random dropping method called DropMessage. The method performs dropping operations on the message matrix, which enhances robustness and stabilizes the training process while preserving information diversity. Theoretical analysis and experiments demonstrate the efficacy of DropMessage over existing methods.\",\n  \"Direct Inspiration\": [\"b14\", \"b19\", \"b48\"],\n  \"Indirect Inspiration\": [\"b16\", \"b26\", \"b68\", \"b31\", \"b41\", \"b59\", \"b66\", \"b71\", \"b72\"],\n  \"Other Inspiration\": [\"b54\", \"b55\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in training GNNs on large-scale graphs, such as over-fitting, over-smoothing, and non-robustness. It introduces a novel random dropping method called DropMessage, which performs dropping operations on the message matrix to improve robustness and training stability.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b19\": 0.9,\n    \"b48\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b54\": 0.7,\n    \"b55\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of training Graph Neural Networks (GNNs) on large-scale graphs, particularly issues like over-fitting, over-smoothing, and non-robustness. It introduces a novel random dropping method called DropMessage, which performs dropping operations on the message matrix, making the training process more stable and the models more robust.\",\n  \"Direct Inspiration\": [\"b14\", \"b19\", \"b48\"],\n  \"Indirect Inspiration\": [\"b26\", \"b68\", \"b31\", \"b41\", \"b59\", \"b66\", \"b71\", \"b72\"],\n  \"Other Inspiration\": [\"b6\", \"b12\", \"b13\", \"b29\", \"b53\"]\n}\n```\n", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of over-fitting, over-smoothing, and non-robustness in training Graph Neural Networks (GNNs) on large-scale graphs. It proposes a novel random dropping method called DropMessage, which performs dropping operations on the message matrix. This method is shown to improve training stability, reduce sample variance, and maintain information diversity compared to existing methods like Dropout, DropEdge, and DropNode.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b19\": 0.9,\n    \"b48\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b26\": 0.8,\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.7,\n    \"b59\": 0.7,\n    \"b66\": 0.7,\n    \"b71\": 0.7,\n    \"b72\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in training Graph Neural Networks (GNNs) on large-scale graphs, specifically over-fitting, over-smoothing, and non-robustness. It proposes a novel random dropping method called DropMessage, which performs dropping operations directly on the message matrix to improve the stability and robustness of GNNs. The method is shown to unify existing random dropping methods and provides theoretical analysis and experimental validation to support its effectiveness.\",\n    \"Direct Inspiration\": [\"b14\", \"b19\", \"b48\"],\n    \"Indirect Inspiration\": [\"b16\", \"b26\", \"b29\"],\n    \"Other Inspiration\": [\"b31\", \"b41\", \"b54\"]\n}\n```"], "63a413f690e50fcafd6d1a7a": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficiently computing Graph Edit Distance (GED) for graph similarity computation (GSC) in graph databases. The proposed method, ERIC, aims to balance predictive accuracy and computational efficiency by introducing a novel Alignment Regularization (AReg) technique and a Multi-Scale GED Discriminator. The core motivation is to avoid the computationally expensive node-to-node matching in the inference stage by learning underlying alignment information during the training stage.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1,\n        \"b21\": 1,\n        \"b15\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b31\": 0.8,\n        \"b17\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b7\": 0.6,\n        \"b29\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently computing graph similarity, which is crucial for applications such as drug design and social group identification. The proposed method, ERIC, improves upon existing models by introducing a novel Alignment Regularization (AReg) technique and a multi-scale GED discriminator. These innovations aim to balance predictive accuracy and computational efficiency by learning cross-graph interactions during training and simplifying the inference process.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b17\": 0.8,\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b7\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently and accurately computing Graph Edit Distance (GED) in graph similarity computation by proposing a novel framework called ERIC. The framework incorporates a GNN encoder with Alignment Regularization (AReg) and a Multi-Scale GED Discriminator. The key innovation is the separation of the matching model from the end-to-end pipeline, which significantly reduces computational cost while maintaining high accuracy.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b17\": 1.0,\n    \"b21\": 1.0,\n    \"b0\": 1.0,\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.9,\n    \"b12\": 0.8,\n    \"b15\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b16\": 0.7,\n    \"b18\": 0.7,\n    \"b6\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the NP-hard problem of computing the exact Graph Edit Distance (GED) for graph similarity computation (GSC) in graph databases. The authors propose the ERIC framework, which includes a novel Alignment Regularization (AReg) method and a Multi-Scale GED Discriminator to improve efficiency and accuracy in GED approximation. The framework aims to overcome the computational and memory costs associated with traditional GNN-based GSC models by detaching the matching model during inference.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of efficiently computing Graph Edit Distance (GED) for graph similarity computation (GSC). It proposes a novel framework called ERIC, which includes an Alignment Regularization (AReg) module and a Multi-Scale GED Discriminator. The key innovations are the detachment of the matching model from the inference stage and the use of a multi-scale discriminator to improve prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b17\": 0.8,\n    \"b15\": 0.8,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b18\": 0.6,\n    \"b6\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```"], "637c3dcf90e50fcafd77c35b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing instruction cache (i-cache) misses in datacenter applications by proposing an Admission-Controlled Instruction Cache (ACIC). The ACIC leverages bursty access patterns and introduces a 16-entry i-Filter and a two-level predictor-based admission control mechanism. This novel approach aims to differentiate between spatial and temporal localities, providing significant speedup and energy savings.\",\n  \"Direct Inspiration\": [\"b60\"],\n  \"Indirect Inspiration\": [\"b28\", \"b48\", \"b36\"],\n  \"Other Inspiration\": [\"b46\", \"b63\", \"b88\", \"b31\", \"b32\", \"b22\", \"b57\", \"b43\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of instruction cache (i-cache) misses in datacenter applications, which have large code footprints leading to higher i-cache misses. The proposed solution, Admission-Controlled Instruction Cache (ACIC), introduces a 16-entry buffer (i-Filter) and a prediction mechanism to better manage i-cache space by determining the temporal locality of instruction blocks before bringing them into the i-cache. This approach aims to reduce i-cache misses by 18.14%, achieving a 1.0223 speedup on average. The paper draws inspiration from prior works on bursty accesses and cache management policies, particularly focusing on spatio-temporal separation and admission control mechanisms.\",\n  \"Direct Inspiration\": [\"b60\"],\n  \"Indirect Inspiration\": [\"b15\", \"b19\", \"b28\", \"b48\", \"b69\"],\n  \"Other Inspiration\": [\"b30\", \"b63\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge addressed in the paper is the high instruction cache (i-cache) miss rates in datacenter applications due to their complex and deep software stacks. Traditional LRU-based replacement algorithms are not effective in these scenarios due to the bursty access patterns observed in these workloads.\",\n    \"Inspirations\": \"The paper is inspired by prior works on dead block predictors and bursty access patterns, particularly for data caches. It proposes an Admission-Controlled Instruction Cache (ACIC) to address these challenges by incorporating a 16-entry buffer (i-Filter) and a sophisticated admission control mechanism.\"\n  },\n  \"Direct Inspiration\": [\"b60\"],\n  \"Indirect Inspiration\": [\"b28\", \"b48\", \"b46\", \"b63\"],\n  \"Other Inspiration\": [\"b30\", \"b36\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Reducing i-cache misses in datacenter applications\",\n    \"Inspirations\": \"Bursty accesses to data blocks, spatio-temporal separation, and improved cache management\"\n  },\n  \"Direct Inspiration\": [\"b60\"],\n  \"Indirect Inspiration\": [\"b28\", \"b48\", \"b36\"],\n  \"Other Inspiration\": [\"b63\", \"b46\", \"b33\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The paper addresses the issue of instruction cache (i-cache) misses in datacenter applications. These applications have large code footprints and complex control flows, leading to higher i-cache misses. The main challenge is to manage the limited i-cache space efficiently to reduce these misses.\",\n    \"Inspirations\": \"The proposed solution, Admission-Controlled Instruction Cache (ACIC), is inspired by prior work on dead block predictors and bursty access patterns in data blocks. The paper aims to provide spatio-temporal separation of instruction blocks to handle bursty accesses effectively.\"\n  },\n  \"Direct Inspiration\": [\"b60\"],\n  \"Indirect Inspiration\": [\"b28\", \"b48\", \"b36\"],\n  \"Other Inspiration\": [\"b63\", \"b46\", \"b31\", \"b32\"]\n}\n```"], "634d805b90e50fcafd4e1168": ["```json\n{\n  \"Summary\": \"The paper addresses the limitations of current dynamic loaders in handling the increasing number of dynamic libraries and diverse operations required by modern software. The proposed solution, iFed, is a new infrastructure that offers extensibility, modularity, and flexibility for dynamic library operations by organizing them into a pipeline of transformation passes.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b51\": 0.85,\n    \"b36\": 0.8,\n    \"b49\": 0.8,\n    \"b45\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b13\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of current dynamic loaders which are insufficient and inefficient for modern software requirements. It introduces iFed, a new pass-based infrastructure designed for extensibility, modularity, and flexibility in dynamic library operations. The proposed system includes features like a runnable in-memory format (RiMF) and a pass manager to orchestrate transformation passes.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b51\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b13\": 0.8,\n    \"b23\": 0.7,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges posed by the limitations of current dynamic loaders, which struggle with extensibility, modularity, and performance optimization. To tackle these challenges, the authors propose iFed, a new infrastructure that organizes dynamic library operations into a pass-based pipeline, enhancing extensibility, modularity, and performance. iFed includes two performance optimization passes for efficient hugepage usage and relocation branch elimination.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b13\": 0.9,\n    \"b31\": 0.9,\n    \"b36\": 0.9,\n    \"b49\": 0.9,\n    \"b51\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.7,\n    \"b45\": 0.7,\n    \"b52\": 0.7,\n    \"b56\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b15\": 0.5,\n    \"b33\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of current dynamic loaders in handling a large number of dynamic libraries and diverse manipulation requirements. It proposes iFed, a new infrastructure that offers extensibility, modularity, and flexibility for dynamic library operations. iFed organizes dynamic library transformations into distinct passes, enabling efficient memory management, security enhancements, and performance optimizations.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b51\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.8,\n    \"b49\": 0.8,\n    \"b52\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b13\": 0.7,\n    \"b38\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of dynamic loader inefficiencies and the need for extensibility, modularity, and flexibility in dynamic library operations. The proposed solution, iFed, introduces a pass-based infrastructure with a runnable in-memory format (RiMF) to decouple and manage different transformation passes. Key innovations include dynamic library concatenation for efficient memory usage and relocation branch elimination for performance optimization.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b51\": 1,\n    \"b13\": 1,\n    \"b38\": 1,\n    \"b54\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b23\": 0.8,\n    \"b45\": 0.8,\n    \"b49\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.6,\n    \"b16\": 0.6,\n    \"b15\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```"], "62d4cf305aee126c0fa9ab5a": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in neural architecture design for deep learning-based recommender systems, particularly in handling multi-modality features and optimizing heterogeneous architectures. The authors propose NASRec, a new paradigm utilizing Weight Sharing Neural Architecture Search (WS-NAS) to overcome these challenges. Key innovations include a supernet with diverse operators, single-operator any-connection sampling, and post-training fine-tuning to enhance the ranking and efficiency of discovered models.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b12\": 0.85,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b7\": 0.7,\n    \"b16\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper proposes NASRec, an advanced Neural Architecture Search (NAS) approach specifically designed for recommender systems. It addresses the challenges of multi-modality data and architecture heterogeneity by building a supernet with diverse operators and dense connectivity. The authors introduce novel techniques like single-operator any-connection sampling, operator-balancing interaction modules, and post-training fine-tuning to enhance the training and ranking efficiency of the supernet. The paper demonstrates NASRec's superiority over existing NAS methods in terms of performance and computational efficiency.\",\n    \"Direct Inspiration\": [\n        \"b7\",\n        \"b12\",\n        \"b16\",\n        \"b22\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b2\",\n        \"b29\"\n    ],\n    \"Other Inspiration\": [\n        \"b4\",\n        \"b9\",\n        \"b13\"\n    ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in designing deep learning-based recommender systems, particularly dealing with multi-modality features and architecture heterogeneity. The proposed solution, NASRec, introduces a weight-sharing neural architecture search (WS-NAS) paradigm that minimizes human priors, supports diverse data modalities, and incorporates heterogeneous operators. Key methods include the construction of a supernet, single-operator any-connection sampling, operator-balancing interaction modules, and post-training fine-tuning.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b7\": 1,\n    \"b12\": 1,\n    \"b16\": 1,\n    \"b22\": 1,\n    \"b29\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b9\": 0.9,\n    \"b13\": 0.9,\n    \"b14\": 0.9,\n    \"b28\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.8,\n    \"b6\": 0.8,\n    \"b24\": 0.8,\n    \"b25\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing neural architectures for recommender systems that handle multi-modality features. It introduces NASRec, a novel approach utilizing Weight Sharing Neural Architecture Search (WS-NAS) to overcome limitations in existing methods by incorporating diverse operators and minimizing human priors. The proposed method is shown to improve performance and efficiency compared to both manually crafted and other NAS-crafted models.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b7\": 0.9,\n    \"b12\": 0.9,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b27\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing recommender systems through Neural Architecture Search (NAS), specifically focusing on overcoming the limitations of manual efforts and narrow design spaces. The proposed NASRec framework introduces a flexible supernet with weight sharing to efficiently search and train models while addressing data modality and architecture heterogeneity.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b7\": 1,\n    \"b12\": 1,\n    \"b16\": 1,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b9\": 0.8,\n    \"b13\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b26\": 0.7\n  }\n}\n```"], "63dcdb422c26941cf00b6094": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing an automated graph Transformer (AutoGT) for graph data. It proposes a novel neural architecture search method that incorporates a unified search space combining Transformer architectures and graph encoding strategies. The main contributions include a unified search space, encoding-aware performance estimation, and extensive experiments demonstrating superior performance.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b30\": 0.8,\n    \"b26\": 0.7,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges in designing an automated graph Transformer, specifically addressing the need for a unified search space and an encoding-aware architecture search strategy. The proposed solution, AutoGT, introduces novel methods such as node attribution augmentation and attention map augmentation to optimize graph Transformers.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b18\", \"b30\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b5\", \"b26\", \"b3\", \"b23\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b12\", \"b1\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in designing an automated graph Transformer (AutoGT) that can handle non-Euclidean graph data by proposing a unified search space and an encoding-aware performance estimation strategy. This approach aims to optimize both Transformer architectures and graph encoding strategies simultaneously.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b30\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.7,\n    \"b3\": 0.65,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces Automated Graph Transformer (AutoGT), a novel neural architecture search method designed specifically for graph Transformers. The main challenges identified are the need for a unified search space for graph Transformers that considers node relations and the coupling between Transformer architectures and graph encoding strategies. The proposed AutoGT framework addresses these by introducing node attribution augmentation and attention map augmentation, and by implementing an encoding-aware performance estimation strategy.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b3\": 0.8,\n    \"b19\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in designing state-of-the-art graph Transformers due to the reliance on hand-crafted human design, which introduces bias and inefficiency. It proposes AutoGT, a novel neural architecture search method for graph Transformers, introducing a unified search space and an encoding-aware performance estimation strategy tailored for graphs.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b30\": 0.9,\n    \"b26\": 0.7,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.5,\n    \"b13\": 0.6,\n    \"b19\": 0.5\n  }\n}\n```"], "633cf5cf90e50fcafd772e24": ["```json\n{\n    \"Summary\": \"The paper introduces a new paradigm called RECITation-augmented generation (RECITE) to enhance the performance of large language models (LLMs) in knowledge-intensive NLP tasks. The method involves a two-step process: first, reciting relevant information from the model's internal memory, and then using that recitation to generate accurate outputs. This approach addresses the challenge that LLMs face when generating factual knowledge without access to an external corpus. The paper demonstrates the effectiveness of RECITE in few-shot Closed-Book Question Answering (CBQA) tasks and shows that fine-tuning on synthetic generated question-passage pairs can further improve performance.\",\n    \"Direct Inspiration\": {\n        \"b5\": 0.9,\n        \"b18\": 0.85,\n        \"b23\": 0.8,\n        \"b20\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.75,\n        \"b58\": 0.75,\n        \"b41\": 0.7,\n        \"b56\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.65,\n        \"b10\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of large language models (LLMs) on knowledge-intensive NLP tasks, specifically in the context of closed-book question answering (CBQA). The authors propose a new paradigm called RECITE, a recitation-augmented generation approach that decomposes the task into knowledge-recitation and task-execution steps. This method aims to enhance the LLMs' ability to recall information from their parameters, akin to a student reciting knowledge before answering a question.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b18\": 0.8,\n    \"b28\": 0.8,\n    \"b56\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b10\": 0.7,\n    \"b20\": 0.7,\n    \"b23\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.6,\n    \"b26\": 0.65,\n    \"b14\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the factual accuracy of large language models (LLMs) in knowledge-intensive tasks without retrieving from an external corpus. It proposes a new paradigm called RECITation-augmented gEneration (RECITE), where the task is decomposed into two sub-tasks: knowledge-recitation and task-execution. This approach is inspired by the observation that few-shot prompting alone is insufficient for effective knowledge recitation from LLMs' memory.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b7\": 0.7,\n    \"b23\": 0.6,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing large language models (LLMs) to generate accurate factual knowledge without relying on external corpus retrieval. It introduces a novel paradigm called RECITation-augmented gEneration (RECITE), which decomposes tasks into knowledge-recitation and task-execution steps, improving the performance of LLMs on knowledge-intensive tasks. The motivation is to mimic human recitation before answering questions, leveraging the LLMs' existing knowledge base in their model parameters.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b18\": 1,\n    \"b41\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b20\": 0.8,\n    \"b23\": 0.8,\n    \"b56\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b14\": 0.6,\n    \"b26\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces RECITation-augmented gEneration (RECITE), a new paradigm for large language models (LLMs) to improve few-shot performance on knowledge-intensive NLP tasks by first reciting relevant information and then generating the answers. The main challenge addressed is the difficulty LLMs face in effectively reciting knowledge from their memory due to the dissimilarity between the few-shot prompting tasks and the original pre-training objectives. The paper demonstrates the effectiveness of the recite-and-answer scheme on various CBQA tasks and proposes fine-tuning LLMs on synthetic question-passage pairs for further improvement.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b26\": 1,\n    \"b18\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b39\": 0.9,\n    \"b10\": 0.9,\n    \"b23\": 0.8,\n    \"b20\": 0.8,\n    \"b56\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```"], "635b486790e50fcafd32f8b1": ["```json\n{\n  \"Summary\": \"The paper introduces MABEL, a novel and lightweight method for attenuating gender bias in pre-trained language models. MABEL uses a contrastive learning framework with entailment pairs from NLI datasets and an alignment regularizer to minimize the distance between entailment pairs and their gender-swapped counterparts. It is task-agnostic and aims to produce fairer contextualized representations. The method is evaluated on both intrinsic and extrinsic measures, demonstrating its effectiveness in bias mitigation and preserving language understanding.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b62\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b34\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b64\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces MABEL, a task-agnostic method for attenuating gender bias in pre-trained language models using a contrastive learning framework with entailment pairs from NLI datasets. Major challenges include addressing inherent social biases in language models and ensuring fair representations across various tasks. The paper identifies limitations in current bias metrics like SEAT and proposes a comprehensive set of intrinsic and extrinsic evaluation metrics.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b62\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b64\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b34\": 0.7,\n    \"b38\": 0.7,\n    \"b17\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are gender biases in pre-trained language models, which can propagate and amplify discriminatory judgments. The paper introduces MABEL, a novel, lightweight, and task-agnostic method for attenuating gender bias using a contrastive learning framework with entailment pairs from supervised natural language inference datasets. This approach includes a contrastive loss, an alignment loss, and an optional masked language modeling loss to ensure debiased representations while preserving language understanding.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b62\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b64\": 0.8,\n    \"b38\": 0.7,\n    \"b10\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b28\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents MABEL, a novel task-agnostic method for attenuating gender bias in pre-trained language models. MABEL utilizes entailment pairs from supervised natural language inference (NLI) datasets and employs a contrastive learning framework with an alignment regularizer to produce fairer contextualized representations. The method is evaluated against multiple intrinsic and extrinsic measures, demonstrating its effectiveness in bias mitigation and language understanding.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b62\": 0.85,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b64\": 0.75,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.65,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents MABEL, a task-agnostic, lightweight method for mitigating gender bias in pre-trained language models using a contrastive learning framework and entailment pairs from NLI datasets. The main challenges addressed include inherent biases in pre-trained models and inconsistencies in bias evaluation metrics. MABEL leverages entailment pairs, counterfactual data augmentation, contrastive loss, and alignment loss to learn fairer contextualized representations while preserving language understanding.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b62\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b64\": 0.7,\n    \"b58\": 0.6,\n    \"b49\": 0.6\n  }\n}\n```"], "628749485aee126c0fff0290": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of mitigating biases in pretrained language models (PLMs) without relying on external corpora. The proposed method, Auto-Debias, uses cloze-style prompts to identify and correct biases in masked language models like BERT, ALBERT, and RoBERTa. The approach involves two stages: generating biased prompts to probe biases and fine-tuning the model to minimize these biases using an equalizing loss.\",\n    \"Direct Inspiration\": {\n        \"b29\": 0.9,\n        \"b35\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.7,\n        \"b20\": 0.7,\n        \"b25\": 0.7,\n        \"b30\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b34\": 0.5,\n        \"b32\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is mitigating biases in pretrained language models (PLMs) without relying on external corpora. The proposed algorithm, Auto-Debias, utilizes cloze-style prompts to probe and identify biases and then fine-tunes the language model to minimize these biases.\",\n  \"Direct Inspiration\": [\"b29\", \"b35\"],\n  \"Indirect Inspiration\": [\"b13\", \"b20\", \"b25\"],\n  \"Other Inspiration\": [\"b30\", \"b32\", \"b34\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of mitigating biases in pretrained language models (PLMs) without relying on external corpora. It introduces a novel method, Auto-Debias, which uses cloze-style prompts to probe and identify biases within PLMs and then fine-tunes the models to minimize these biases while maintaining performance on natural language understanding tasks.\",\n    \"Direct Inspiration\": {\n        \"b29\": 1.0,\n        \"b35\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b13\": 0.8,\n        \"b20\": 0.8,\n        \"b25\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.5,\n        \"b21\": 0.5,\n        \"b32\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is mitigating biases and undesired social stereotypes in pretrained language models (PLMs) without relying on external corpora. The proposed algorithm, Auto-Debias, automatically crafts biased prompts and uses these prompts to fine-tune the language models by minimizing the disagreement between the predictions of masked tokens.\",\n    \"Direct Inspiration\": {\n        \"b29\": 0.9,\n        \"b35\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.7,\n        \"b25\": 0.7,\n        \"b20\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b30\": 0.6,\n        \"b34\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is mitigating biases in pretrained language models (PLMs) without relying on external corpora. The proposed algorithm, Auto-Debias, uses cloze-style prompts to identify and minimize biases in PLMs by automatically generating biased prompts and using a distribution alignment loss.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b35\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b20\": 0.8,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b34\": 0.5,\n    \"b31\": 0.5\n  }\n}\n```"], "627cdc8e5aee126c0f50229e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing cache misses, which significantly impact application performance. It introduces CachePerf, a novel tool capable of identifying and classifying all types of fixable cache misses while imposing minimal overhead. The main challenges include choosing an appropriate profiling method, differentiating cache miss types, and identifying allocator-induced cache misses. The proposed hybrid hardware sampling technique and practical filtering mechanisms are key innovations.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b30\": 0.9,\n    \"b31\": 0.9,\n    \"b33\": 0.9,\n    \"b37\": 0.9,\n    \"b44\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b41\": 0.7,\n    \"b47\": 0.7,\n    \"b50\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b29\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of identifying and classifying all types of cache misses with minimal overhead. The proposed tool, CachePerf, is a unified profiler that addresses the shortcomings of existing tools by reporting both the type and origin of cache misses, thus providing useful information for bug fixes.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b33\", \"b44\", \"b5\", \"b31\", \"b37\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b41\", \"b20\", \"b30\", \"b24\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": []\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of identifying and reducing different types of cache misses (capacity, conflict, and coherence) with minimal performance overhead. It introduces CachePerf, a novel tool that uses a hybrid sampling approach combining PMU-based coarse-grained sampling and fine-grained breakpoint-based sampling to classify cache misses and differentiate those caused by allocators from those caused by applications.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b5\": 1.0,\n    \"b31\": 1.0,\n    \"b37\": 1.0,\n    \"b44\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b24\": 0.8,\n    \"b41\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b29\": 0.6,\n    \"b47\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is the identification and classification of different types of cache misses (capacity, conflict, and coherence misses) with minimal performance overhead. The proposed tool, CachePerf, aims to address these challenges by employing a novel hybrid sampling scheme that combines coarse-grained PMU-based sampling and fine-grained breakpoint-based sampling. The tool also differentiates cache misses caused by the allocator from those caused by applications, and prunes insignificant issues to avoid wasting programmer effort.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b44\": 0.85,\n    \"b5\": 0.8,\n    \"b31\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.7,\n    \"b24\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b29\": 0.6,\n    \"b18\": 0.5,\n    \"b36\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying and reducing various types of cache misses in applications. It proposes a novel tool, CachePerf, which uses a hybrid sampling approach combining coarse-grained PMU-based sampling and fine-grained breakpoint-based sampling to classify different types of cache misses. CachePerf also differentiates between application-induced and allocator-induced cache misses and proposes practical mechanisms to filter out minor issues.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b31\": 1.0,\n    \"b37\": 1.0,\n    \"b44\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.8,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b41\": 0.9\n  }\n}\n```"], "62d16e895aee126c0fd68310": ["```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is the unsupervised dynamic compilation for network code, which captures sufficient domain-specific knowledge to enable efficient dynamic performance optimization while being inexpensive enough to operate online within the data plane pipeline. The proposed system, Morpheus, employs a mix of static code analysis, adaptive instrumentation, and dynamic optimization techniques to achieve these goals.\",\n  \"Direct Inspiration\": {\n    \"b69\": 1.0,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b75\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b29\": 0.6,\n    \"b92\": 0.5,\n    \"b44\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised dynamic compilation for network code optimization, introducing Morpheus, a system that leverages domain-specific dynamic optimization techniques. The primary challenges include effective low-overhead runtime instrumentation, dynamic code generation, and ensuring consistency of optimized code.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.95,\n    \"b29\": 0.95,\n    \"b34\": 0.9,\n    \"b69\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.85,\n    \"b75\": 0.85,\n    \"b28\": 0.8,\n    \"b7\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.75,\n    \"b54\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in this paper is unsupervised dynamic compilation for network code, enabling efficient dynamic performance optimization based on domain-specific knowledge. The proposed system, Morpheus, dynamically optimizes network code at runtime without requiring prior knowledge of traffic patterns or configurations. The paper introduces several novel techniques, including adaptive instrumentation and the use of LLVM JIT compiler toolchain, to achieve these goals.\",\n  \"Direct Inspiration\": {\n    \"b69\": 0.9,\n    \"b34\": 0.85,\n    \"b92\": 0.8,\n    \"b29\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b75\": 0.65,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.55,\n    \"b54\": 0.5,\n    \"b26\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised dynamic compilation for network code, aiming to optimize performance without requiring prior knowledge of control plane configurations or traffic patterns. The proposed system, Morpheus, utilizes domain-specific dynamic optimization techniques and introduces several innovative methods such as adaptive instrumentation and dynamic optimization passes to achieve efficient runtime optimization.\",\n  \"Direct Inspiration\": {\n    \"b69\": 0.9,\n    \"b34\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b75\": 0.65,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.5,\n    \"b7\": 0.45,\n    \"b54\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised dynamic compilation for network code, focusing on capturing domain-specific knowledge to enable efficient dynamic performance optimization. It introduces Morpheus, a system that operates in unsupervised mode, leveraging static code analysis and adaptive instrumentation to minimize overhead and optimize the code at runtime.\",\n  \"Direct Inspiration\": {\n    \"b69\": 0.9,\n    \"b34\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b29\": 0.75,\n    \"b38\": 0.6,\n    \"b22\": 0.65,\n    \"b75\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.55,\n    \"b28\": 0.5\n  }\n}\n```"], "63dcdb422c26941cf00b604a": ["```json\n{\n  \"Summary\": \"The paper addresses the problem of neural architecture search (NAS) and proposes a novel Bayesian Optimization (BO) method with a deep-kernel surrogate to efficiently transfer architectures across datasets. The primary challenges identified include the inefficiency and lack of robustness in existing NAS methods, especially in transferring information across datasets. The proposed method leverages dataset-contextualized surrogates and combines the reliability of blackbox optimization with the computational efficiency of one-shot approaches.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b22\": 0.9,\n    \"b52\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.7,\n    \"b58\": 0.7,\n    \"b50\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.5,\n    \"b11\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the inefficiency and lack of robustness in existing NAS methods, and the need to transfer information across datasets for NAS. The proposed algorithm is an efficient Bayesian Optimization (BO) method with a novel deep-kernel surrogate that leverages dataset-contextualized surrogates for transfer learning. Key components include a graph encoder for neural architectures, an attention-based dataset encoder, and deep kernel learning to obtain meta-learned kernels for the joint space of architectures and datasets.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b22\": 0.9,\n    \"b52\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b50\": 0.85,\n    \"b34\": 0.8,\n    \"b45\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.7,\n    \"b47\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of neural architecture search (NAS) by proposing a novel Bayesian Optimization (BO) method that leverages dataset-contextualized surrogates for transfer learning. It combines deep-kernel Gaussian Processes with a graph neural network encoder and a transformer-based dataset encoder for efficient NAS.\",\n  \"Direct Inspiration\": [\"b21\", \"b22\", \"b52\"],\n  \"Indirect Inspiration\": [\"b13\", \"b45\", \"b50\"],\n  \"Other Inspiration\": [\"b31\", \"b34\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an efficient and robust neural architecture search (NAS) method, leveraging transfer learning and Bayesian optimization (BO) with deep kernel learning to encode both architecture and dataset information. The proposed method aims to combine the reliability of blackbox optimization with the computational efficiency of one-shot approaches.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b52\": 0.8,\n    \"b45\": 0.7,\n    \"b58\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b50\": 0.6,\n    \"b34\": 0.6,\n    \"b47\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the inefficiencies and lack of robustness in current Neural Architecture Search (NAS) methods. The proposed algorithm leverages Bayesian Optimization (BO) with novel deep-kernel surrogates, combining graph neural networks and transformer-based dataset encoders to meta-learn kernels for efficient NAS. This approach addresses key issues of previous methods, such as MetaD2A, by improving exploration vs. exploitation trade-offs and adapting to new function evaluations on a test task.\",\n  \"Direct Inspiration\": {\n    \"Inspired by manual architecture design, we treat NAS as a transfer or few-shot learning problem. We leverage ideas from transfer HPO to meta-learn a kernel for Bayesian Optimization, which encodes both architecture and dataset information.\": [\"b21\", \"b52\"],\n    \"Following [b21], we use a graph encoder [b58] to encode neural architectures and an attention-based dataset encoder [b22] to obtain context features.\": [\"b21\", \"b58\", \"b22\"]\n  },\n  \"Indirect Inspiration\": {\n    \"One approach to obtain efficient NAS methods that has been overlooked in the literature so far is to exploit the common formulation of NAS as a hyperparameter optimization (HPO) problem [b1] [b5] [b0] and draw on the extensive literature on transfer HPO [b53] [b13] [b29] [b35] [b52].\": [\"b1\", \"b5\", \"b0\", \"b53\", \"b13\", \"b29\", \"b35\", \"b52\"],\n    \"In contrast to standard transfer HPO methods that meta-learn parametric surrogates from a pool of source datasets [b53] [b13] [b52], in this work we explore the direction of meta learning surrogates by contextualizing them on the dataset characteristics (a.k.a. meta-features) [b45] Jomaa et al., 2021a; [b33].\": [\"b53\", \"b13\", \"b52\", \"b45\", \"b33\"]\n  },\n  \"Other Inspiration\": {\n    \"A promising direction for designing powerful and efficient kernel functions that adapt to a learning task is Deep Kernel Learning [b50], where kernels are represented as trainable neural networks.\": [\"b50\"]\n  }\n}\n```"], "640fe64790e50fcafd9e237f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning high-quality graph representations under heterophily using Graph Neural Networks (GNNs). The proposed HLCL framework leverages both high-pass and low-pass graph filters to capture dissimilarities and similarities in node features, respectively. This method contrasts smooth and non-smooth node representations to enhance the learning of rich node representations without the need for explicit augmentation.\",\n  \"Direct Inspiration\": [\"b29\"],\n  \"Indirect Inspiration\": [\"b9\", \"b16\", \"b22\", \"b32\"],\n  \"Other Inspiration\": [\"b4\", \"b13\", \"b12\", \"b31\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of learning high-quality graph neural network (GNN) representations under heterophily without access to labels. It proposes HLCL, a method leveraging high-pass and low-pass filters to create non-smooth and smooth node representations, respectively, which are then contrasted to learn rich representations.\",\n    \"Direct Inspiration\": {\n        \"b29\": 1.0,\n        \"b16\": 0.9,\n        \"b22\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b9\": 0.8,\n        \"b32\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.6,\n        \"b31\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of learning high-quality node representations in graphs with heterophily using Graph Neural Networks (GNNs). The proposed method, HLCL, leverages both high-pass and low-pass graph filters to capture the dissimilarities and similarities of nodes within their neighborhoods, respectively. This approach contrasts the smooth and non-smooth node representations to learn richer representations without the need for explicit graph augmentation.\",\n    \"Direct Inspiration\": {\n        \"b29\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.8,\n        \"b16\": 0.8,\n        \"b22\": 0.8,\n        \"b31\": 0.7,\n        \"b32\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.6,\n        \"b12\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning effective node representations in graphs with heterophily, an area where existing graph neural network (GNN) methods struggle. The proposed HLCL framework leverages both high-pass and low-pass graph filters to capture dissimilarities and similarities among nodes, respectively, enhancing the representation learning process.\",\n  \"Direct Inspiration\": [\"b13\", \"b29\"],\n  \"Indirect Inspiration\": [\"b4\", \"b16\", \"b22\", \"b32\"],\n  \"Other Inspiration\": [\"b9\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of learning high-quality graph representations under heterophily in Graph Neural Networks (GNNs). It proposes a novel contrastive learning method called HLCL that leverages both high-pass and low-pass filters to capture dissimilarities and similarities of node features, respectively. The paper demonstrates that HLCL achieves state-of-the-art performance on both heterophilic and homophilic graphs.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b31\": 0.9,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b16\": 0.8,\n    \"b18\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b9\": 0.6,\n    \"b13\": 0.6,\n    \"b12\": 0.5\n  }\n}\n```"], "628749355aee126c0ffec021": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of text categorization by comparing different models, including BoW-based, graph-based, and sequence-based models. It highlights that BoW-based models, like their proposed WideMLP, can be surprisingly effective and outperform many recent graph-based approaches. The paper also fine-tunes BERT and DistilBERT for text categorization, setting a new state of the art.\",\n  \"Direct Inspiration\": {\n    \"b62\": 1,\n    \"b28\": 0.9,\n    \"b41\": 0.9,\n    \"b9\": 1,\n    \"b43\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b45\": 0.8,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in text categorization by proposing a simple but effective BoW-based model, WideMLP, which outperforms many recent graph-based models. It also fine-tunes BERT and DistilBERT to set new state-of-the-art results. The study highlights the neglect of MLPs as baseline methods in the literature, emphasizing their importance in scientific advancement and industry applications due to lower operational and maintenance costs.\",\n  \"Direct Inspiration\": [\"b45\", \"b62\", \"b28\", \"b41\", \"b9\", \"b43\"],\n  \"Indirect Inspiration\": [\"b3\", \"b17\", \"b10\"],\n  \"Other Inspiration\": [\"b24\", \"b20\", \"b14\", \"b40\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include the effective categorization of text units and the exploration of different models for text categorization. The paper proposes a simple but effective BoW-based model called WideMLP, and compares it with other families of models including graph-based and sequence-based models. The paper is particularly focused on the performance and utility of MLPs in text categorization, highlighting their often-overlooked potential.\",\n    \"Direct Inspiration\": {\n        \"b17\": 0.9,\n        \"b45\": 0.9,\n        \"b9\": 0.85,\n        \"b43\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b62\": 0.7,\n        \"b28\": 0.7,\n        \"b41\": 0.7,\n        \"b10\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b24\": 0.5,\n        \"b20\": 0.5,\n        \"b40\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of text categorization, proposing a novel BoW-based model called WideMLP. The study aims to compare this model with existing BoW, graph-based, and sequence-based models on various benchmark datasets. The authors hypothesize that simple BoW-based models can be highly effective for text categorization and conduct extensive experiments to test this hypothesis.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1.0,\n        \"b62\": 1.0,\n        \"b9\": 0.9,\n        \"b43\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b45\": 0.8,\n        \"b41\": 0.7,\n        \"b10\": 0.7,\n        \"b28\": 0.7,\n        \"b40\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.6,\n        \"b24\": 0.6,\n        \"b26\": 0.6,\n        \"b0\": 0.6,\n        \"b67\": 0.6,\n        \"b14\": 0.6,\n        \"b64\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of text categorization, focusing on evaluating simple yet effective BoW-based models against more advanced methods like graph-based and sequence-based models. The key contribution is the development and evaluation of a BoW-based MLP model called WideMLP, which outperforms many recent graph-based models and sets a new state of the art when fine-tuned with BERT and DistilBERT.\",\n  \"Direct Inspiration\": {\n    \"b62\": 0.9,\n    \"b28\": 0.8,\n    \"b41\": 0.8,\n    \"b9\": 0.9,\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b45\": 0.7,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b16\": 0.6,\n    \"b48\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```"], "6287045b5aee126c0f5ca192": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of balancing workload and minimizing edge-cuts in distributed graph partitioning by proposing new sampling-based heuristics (RD-B-GRAP and HD-B-GRAP) as extensions of the B-GRAP algorithm. These heuristics aim to improve the performance of label propagation partitioning by using graph sampling methods for seed node selection.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9,\n    \"b24\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.75,\n    \"b5\": 0.8,\n    \"b20\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on improving graph partitioning in distributed systems using a label propagation approach. The primary challenge is balancing the workload while minimizing inter-node communication. The authors propose new sampling-based heuristics (RD-B-GRAP and HD-B-GRAP) to improve the performance of the partitioning and reduce computation time, building on the B-GRAP algorithm.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b8\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b18\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b20\", \"b24\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficiently partitioning large graphs in distributed systems while minimizing communication time. The authors propose new sampling-based heuristics, RD-B-GRAP and HD-B-GRAP, that improve the label propagation algorithm for graph partitioning. These heuristics aim to balance vertex and edge distributions and reduce computation time.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.9,\n        \"b24\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.7,\n        \"b5\": 0.6,\n        \"b20\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving label propagation-based graph partitioning by introducing new sampling-based heuristics, RD-B-GRAP and HD-B-GRAP, to enhance performance in terms of computation time and partitioning quality. The novel methods tackle the issues of seed node selection and label propagation by leveraging graph sampling techniques.\",\n  \"Direct Inspiration\": [\"b8\"],\n  \"Indirect Inspiration\": [\"b4\", \"b18\"],\n  \"Other Inspiration\": [\"b14\", \"b7\", \"b23\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of balancing workload and minimizing edge-cut in distributed graph partitioning, leveraging a label propagation approach. The authors propose new sampling-based heuristics, RD-B-GRAP and HD-B-GRAP, to improve the performance and stability of the label propagation process.\",\n  \"Direct Inspiration\": [\"b8\", \"b18\"],\n  \"Indirect Inspiration\": [\"b20\", \"b24\"],\n  \"Other Inspiration\": [\"b1\", \"b5\"]\n}\n```"], "62708f615aee126c0fa6920a": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of sampling bias in contrastive learning for unsupervised sentence representation learning. It introduces a novel framework, DCLR, which incorporates a noise-based negatives generation strategy and an instance weighting method to punish false negatives and ensure the uniformity of the representation space. The proposed approach aims to improve the alignment and uniformity of sentence representations, leveraging BERT and RoBERTa models with experimental validation on seven semantic textual similarity tasks.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b46\": 0.9,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b19\": 0.6,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the quality of unsupervised sentence representations, particularly focusing on mitigating the issues of anisotropy and sampling bias in contrastive learning methods. The authors propose DCLR, a debiased contrastive learning framework that incorporates noise-based negative sampling and an instance weighting method to enhance the uniformity and quality of sentence representations.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b48\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b46\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of sampling bias in contrastive learning for unsupervised sentence representation. The authors propose a new framework called DCLR, which includes a noise-based negatives generation strategy and an instance weighting method to punish false negatives. The main contributions are reducing sampling bias and improving the uniformity of sentence representations in the vector space.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b28\": 0.8,\n    \"b46\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the problem of sampling bias in contrastive learning for sentence representation due to the anisotropy of pre-trained language models (PLMs). It proposes a debiased contrastive learning framework (DCLR) that incorporates a noise-based negative sampling strategy and an instance weighting method to penalize false negatives, aiming to improve the uniformity and effectiveness of sentence representations.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b46\": 0.8,\n    \"b28\": 0.7,\n    \"b33\": 0.6,\n    \"b48\": 0.5\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.5,\n    \"b14\": 0.4,\n    \"b19\": 0.3\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.2,\n    \"b25\": 0.2,\n    \"b29\": 0.2,\n    \"b45\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the anisotropy problem of pre-trained language models (PLMs) and the sampling bias in contrastive learning for sentence representations. The paper proposes a novel debiased contrastive learning framework (DCLR) that incorporates noise-based negatives and an instance weighting method to address these challenges.\",\n  \"Direct Inspiration\": [\"b14\", \"b16\", \"b28\"],\n  \"Indirect Inspiration\": [\"b13\", \"b46\", \"b48\"],\n  \"Other Inspiration\": [\"b33\"]\n}\n```"], "63fd715990e50fcafd146bdb": ["```json\n{\n    \"Summary\": \"The primary challenge addressed in this paper is the insufficiency of training data in natural language processing (NLP) tasks, particularly in few-shot learning (FSL) scenarios. The authors propose a novel data augmentation method named ChatAug, which leverages ChatGPT to generate auxiliary samples for few-shot text classification. This method aims to improve the accuracy and diversity of generated text data while reducing the burden of human annotation.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b18\": 0.8,\n        \"b19\": 0.8,\n        \"b20\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b11\": 0.7,\n        \"b12\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.6,\n        \"b14\": 0.6,\n        \"b15\": 0.6,\n        \"b16\": 0.6,\n        \"b17\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the insufficiency of training data in NLP, particularly in few-shot learning (FSL) scenarios. The proposed solution, ChatAug, leverages ChatGPT to generate auxiliary samples for text classification, improving accuracy and diversity while maintaining semantic similarity. The performance of ChatAug is validated on both general and medical domain datasets.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b18\": 0.9,\n    \"b20\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b3\": 0.5,\n    \"b8\": 0.5,\n    \"b46\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the insufficiency of training data in NLP, particularly in few-shot learning (FSL) scenarios. The proposed solution is a new data augmentation method named ChatAug, which leverages ChatGPT to generate auxiliary samples for few-shot text classification. ChatAug shows significant improvements in sentence classification accuracy by generating more diversified and accurate augmented samples.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b20\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b46\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b3\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of training data insufficiency in NLP, especially in few-shot learning (FSL) scenarios. It proposes a new data augmentation method named ChatAug, which leverages ChatGPT to generate auxiliary samples for few-shot text classification, demonstrating significant improvements in sentence classification accuracy.\",\n    \"Direct Inspiration\": {\n        \"b7\": 1,\n        \"b20\": 0.95\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.85,\n        \"b3\": 0.8,\n        \"b5\": 0.75,\n        \"b46\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.65,\n        \"b19\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training data insufficiency in NLP, particularly in few-shot learning (FSL) scenarios. It proposes a novel data augmentation method named ChatAug, which leverages ChatGPT to generate auxiliary samples for few-shot text classification, showing significant improvements in performance compared to existing methods.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b5\": 0.8,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```"], "62b52c635aee126c0f459d22": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of optimizing large language models to perform user-specified tasks using reinforcement learning (RL). The primary challenges include simplifying the RL algorithm, ensuring it can optimize various user-defined rewards, making it practical for interactive settings, and leveraging existing data efficiently. The proposed method, ILQL, builds on the IQL algorithm and introduces several novel components to handle these challenges effectively.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0,\n        \"b5\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b10\": 0.8,\n        \"b11\": 0.8,\n        \"b12\": 0.7,\n        \"b13\": 0.7,\n        \"b14\": 0.7,\n        \"b16\": 0.7,\n        \"b17\": 0.7,\n        \"b29\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.6,\n        \"b42\": 0.6,\n        \"b43\": 0.6,\n        \"b44\": 0.6,\n        \"b45\": 0.6,\n        \"b46\": 0.6,\n        \"b47\": 0.6,\n        \"b48\": 0.6,\n        \"b49\": 0.6,\n        \"b50\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of steering large language models toward user-specified tasks using a novel offline reinforcement learning method, ILQL. The key challenges include system complexity, human interaction costs, and the need for stability, scalability, and flexibility in optimization processes. The proposed ILQL method builds on dynamic programming and implicit Q-learning to provide a stable and efficient training process for language models.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.6,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.75,\n    \"b15\": 0.5,\n    \"b14\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b42\": 0.5,\n    \"b43\": 0.5,\n    \"b44\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of steering large language models (LLMs) to optimize user-specified utility functions using reinforcement learning (RL). It proposes a novel offline RL method, Implicit Language Q-Learning (ILQL), that integrates dynamic programming with the benefits of supervised learning for NLP tasks. The ILQL method aims to achieve simplicity, stability, and efficiency while handling sub-optimal data and optimizing various reward functions.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of steering large language models towards user-specified tasks using reinforcement learning (RL). It proposes a novel offline RL method called ILQL which combines stability, ease of use, and effective policy learning from sub-optimal data. Key criteria include leveraging existing data, optimizing user-defined rewards, and handling interactive settings. The method is based on dynamic programming with an implicit dataset support constraint and uses a transformer model to predict and optimize the state-action Q function and state value function.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.6,\n    \"b14\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.4,\n    \"b19\": 0.4,\n    \"b20\": 0.4,\n    \"b21\": 0.4,\n    \"b22\": 0.4,\n    \"b23\": 0.4,\n    \"b24\": 0.4,\n    \"b25\": 0.4,\n    \"b26\": 0.4,\n    \"b27\": 0.4,\n    \"b28\": 0.4,\n    \"b30\": 0.4,\n    \"b31\": 0.3,\n    \"b32\": 0.3,\n    \"b33\": 0.3,\n    \"b34\": 0.3,\n    \"b35\": 0.3,\n    \"b36\": 0.3,\n    \"b37\": 0.3,\n    \"b38\": 0.3,\n    \"b39\": 0.3,\n    \"b40\": 0.3,\n    \"b41\": 0.3,\n    \"b42\": 0.3,\n    \"b43\": 0.3,\n    \"b44\": 0.3,\n    \"b45\": 0.3,\n    \"b46\": 0.3,\n    \"b47\": 0.3,\n    \"b48\": 0.3,\n    \"b49\": 0.3,\n    \"b50\": 0.3,\n    \"b51\": 0.3,\n    \"b52\": 0.3,\n    \"b53\": 0.3,\n    \"b54\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of steering large language models (LLMs) towards user-specified tasks by proposing a novel offline reinforcement learning (RL) method called Implicit Language Q-Learning (ILQL). The key challenges include optimizing user-defined rewards, handling interactive settings, leveraging existing data, and achieving temporal compositionality. The method builds on dynamic programming and incorporates a support constraint to improve stability and performance.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b14\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```"], "6417d04190e50fcafd83de21": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of noisy, incomplete, adversarial, and heterophily graphs in Graph Neural Networks (GNNs) and proposes SE-GSL, a graph structure learning framework utilizing structural entropy and encoding tree theory to enhance the robustness and interpretability of GNNs.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b21\": 0.95,\n    \"b57\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b17\": 0.85,\n    \"b45\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b35\": 0.65,\n    \"b59\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in Graph Neural Networks (GNNs) related to robustness, interpretability, and handling noisy and heterophily graphs. It proposes SE-GSL, a graph structure learning framework that optimizes graph topology using structural entropy and encoding tree theory to improve node representations and the robustness of GNNs.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b17\": 0.7,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b35\": 0.6,\n    \"b59\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in Graph Neural Networks (GNNs) posed by noisy, incomplete, adversarial, and heterophily graph structures. It introduces SE-GSL, a general graph structure learning framework that optimizes graph topology in a learning-free manner using structural entropy and encoding tree theory to enhance node representation and robustness.\",\n    \"Direct Inspiration\": {\n        \"b19\": 0.9,\n        \"b21\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b17\": 0.7,\n        \"b45\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.5,\n        \"b35\": 0.4,\n        \"b58\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges faced by Graph Neural Networks (GNNs) due to noisy, incomplete, and adversarial graph structures. The proposed algorithm, SE-GSL, introduces a structural entropy-based optimization and encoding tree theory to enhance the robustness and interpretability of GNN models. The core contributions include a method to enhance graph topology, an encoding tree for hierarchical abstraction, and a sampling-based graph reconstruction approach.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b21\": 0.9,\n    \"b57\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b17\": 0.7,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.65,\n    \"b58\": 0.65,\n    \"b63\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of robustness to noise and heterophily in Graph Neural Networks (GNNs) and introduces a novel Graph Structure Learning (GSL) framework called SE-GSL. The core contribution is the use of structural entropy and encoding tree theory to optimize graph topology in a learning-free manner, enhancing the resilience and interpretability of GNNs.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b21\": 0.9,\n    \"b64\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b17\": 0.7,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```"], "628d1ea25aee126c0f3e9734": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges inherent in representing and reasoning over complex heterogeneous networks, emphasizing the limitations of traditional methods in capturing the highly nonlinear and coupled relations within these networks. The authors propose a novel ontology information constrained knowledge representation learning model, TransO, which aims to seamlessly incorporate rich ontology information into knowledge graphs (KGs) to improve model performance and decision-making in complex network scenarios.\",\n  \"Direct Inspiration\": [\"b1\", \"b25\", \"b14\"],\n  \"Indirect Inspiration\": [\"b10\", \"b17\", \"b21\"],\n  \"Other Inspiration\": [\"b23\", \"b13\", \"b9\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of traditional heterogeneous network representation learning methods, which are insufficient for complex real-world tasks due to their inability to explicitly model relations and effectively embed rich semantic information. The proposed solution is the ontology information constrained knowledge representation learning model, TransO, which incorporates rich ontology information to improve model performance and maintain low complexity.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b14\": 0.8,\n    \"b25\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b12\": 0.7,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b29\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of effectively incorporating rich ontology information into knowledge representation learning (KRL) models to improve decision-making in complex network scenarios. The proposed solution, TransO, aims to seamlessly integrate ontology information, including entity types, relations, and hierarchical structures, into KRL to enhance performance and reduce computational complexity.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b23\": 0.8,\n        \"b13\": 0.8,\n        \"b9\": 0.8,\n        \"b14\": 0.7,\n        \"b7\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.6,\n        \"b25\": 0.6,\n        \"b32\": 0.6,\n        \"b31\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.5,\n        \"b30\": 0.5,\n        \"b15\": 0.5,\n        \"b20\": 0.5,\n        \"b11\": 0.5,\n        \"b10\": 0.4,\n        \"b18\": 0.4,\n        \"b26\": 0.4,\n        \"b27\": 0.4,\n        \"b3\": 0.4,\n        \"b12\": 0.4,\n        \"b2\": 0.4,\n        \"b29\": 0.4,\n        \"b8\": 0.4,\n        \"b24\": 0.4,\n        \"b28\": 0.4,\n        \"b33\": 0.4,\n        \"b17\": 0.4,\n        \"b21\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing knowledge representation learning (KRL) in heterogeneous networks by incorporating rich ontology information. The authors propose a novel model, TransO, which explicitly models relations and incorporates ontology information to improve model performance and decision-making in complex network applications.\",\n  \"Direct Inspiration\": [\"b1\", \"b14\", \"b7\", \"b25\"],\n  \"Indirect Inspiration\": [\"b9\", \"b13\", \"b23\", \"b20\"],\n  \"Other Inspiration\": [\"b2\", \"b3\", \"b12\", \"b29\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving decision-making in complex network scenarios through a novel knowledge representation learning model called TransO. This model explicitly incorporates ontology information to enhance the representation of relations and entities in knowledge graphs (KGs). The key contributions include the development of constraint strategies for entity types, relations, and hierarchical information, and the demonstration of the model's effectiveness through experimental tasks like link prediction and triple classification.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b14\": 0.9,\n    \"b25\": 0.9,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b13\": 0.7,\n    \"b23\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b9\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```"], "63dcdb422c26941cf00b6413": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in AutoML for graph learning, focusing on the computational efficiency and task-specific nature of GNNs. The proposed solution, AUTOTRANSFER, introduces a task-model bank and a task embedding space to transfer architectural design knowledge across tasks, significantly improving search efficiency and performance.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.9,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b45\": 0.7,\n    \"b19\": 0.6,\n    \"b27\": 0.6,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.5,\n    \"b0\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing deep neural network architectures and hyperparameters for new tasks with minimal computational overhead, particularly in the domain of graph neural networks (GNNs). The proposed solution, AUTOTRANSFER, introduces a task-model bank and a task embedding space to efficiently transfer architectural design knowledge across tasks. The key innovation is the use of the Fisher Information Matrix (FIM) for task embeddings, enabling fast and effective knowledge transfer.\",\n  \"Direct Inspiration\": {\n    \"b40\": 0.9,\n    \"b36\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b45\": 0.6,\n    \"b43\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in AutoML for graph neural networks (GNNs), focusing on the high computational cost of architecture search and the task-specific nature of optimal GNN designs. The proposed solution, AUTOTRANSFER, introduces a task-model bank and a task embedding space to transfer architectural design knowledge across tasks, significantly improving search efficiency.\",\n  \"Direct Inspiration\": {\n    \"b40\": 0.9,\n    \"b9\": 0.8,\n    \"b45\": 0.85,\n    \"b36\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.7,\n    \"b19\": 0.65,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.55,\n    \"b42\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing the design of deep neural networks, specifically graph neural networks (GNNs), by proposing an efficient AutoML solution named AUTOTRANSFER. The key innovation is the introduction of a task-model bank and task embeddings derived from the Fisher Information Matrix, which facilitate the transfer of design knowledge across tasks and significantly reduce computational overhead.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.9,\n    \"b40\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b27\": 0.7,\n    \"b42\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b45\": 0.5,\n    \"b43\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents AUTOTRANSFER, an AutoML solution designed to transfer previous architectural design knowledge to new tasks, particularly in the domain of graph neural networks (GNNs). The key innovation is the introduction of a task-model bank and a task embedding space that captures performance rankings of different architectures across tasks. This allows for efficient knowledge transfer and drastically improves the search efficiency compared to state-of-the-art methods.\",\n  \"Direct Inspiration\": [\"b40\", \"b45\"],\n  \"Indirect Inspiration\": [\"b9\", \"b36\", \"b0\"],\n  \"Other Inspiration\": [\"b27\", \"b19\"]\n}\n```"], "6310233d90e50fcafdc2f5b3": ["```json\n{\n  \"Summary\": \"The paper addresses the task of Named Entity Recognition (NER) by proposing a novel BI-encoder architecture called BINDER. The key contributions include employing two encoders to separately map text and entity types into the same vector space, using span-based contrastive learning to distinguish between entity and non-entity spans, and introducing a dynamic thresholding strategy to handle non-entity spans. The authors claim significant improvements in NER performance over existing methods in both supervised and distantly supervised settings.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b56\": 0.85,\n    \"b62\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b33\": 0.7,\n    \"b27\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b1\": 0.6,\n    \"b49\": 0.65,\n    \"b9\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is improving the efficiency and accuracy of Named Entity Recognition (NER) systems. The authors propose a novel BI-encoder architecture named BINDER, which utilizes span-based contrastive learning to differentiate entity spans from non-entity spans. This approach aims to overcome the limitations of existing NER methods that treat NER as a classification problem and label non-entity spans uniformly, potentially introducing false negatives.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b56\": 1.0,\n    \"b62\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b1\": 0.8,\n    \"b4\": 0.6,\n    \"b33\": 0.6,\n    \"b27\": 0.6,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Named Entity Recognition (NER) by proposing a novel BI-encoder model called BINDER. This model uses two encoders to map text and entity types into a shared vector space and introduces a dynamic thresholding strategy in contrastive learning to separate entity spans from non-entity spans. The paper claims significant improvements in NER performance over existing methods, including supervised and distantly supervised NER settings.\",\n  \"Direct Inspiration\": [\"b21\", \"b56\", \"b62\"],\n  \"Indirect Inspiration\": [\"b8\", \"b1\", \"b4\", \"b33\", \"b27\", \"b13\", \"b9\", \"b30\"],\n  \"Other Inspiration\": [\"b49\", \"b54\", \"b17\", \"b25\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Named Entity Recognition (NER) by proposing a novel BI-encoder architecture called BINDER, which uses span-based contrastive learning to improve both supervised and distantly supervised NER tasks. The model is inspired by recent successes in open-domain question answering and entity linking, and it introduces a dynamic thresholding strategy to distinguish entity spans from non-entity spans.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b56\": 0.9,\n    \"b62\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b1\": 0.8,\n    \"b4\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b49\": 0.6,\n    \"b27\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces BINDER, a novel bi-encoder model for Named Entity Recognition (NER) using contrastive learning. The primary challenges addressed include improving training and inference speed, handling partially annotated data, and improving the accuracy of entity recognition. The model leverages recent advances in bi-encoder architectures and contrastive learning, specifically optimizing the bi-encoder for NER tasks. The paper also introduces a dynamic thresholding strategy to better distinguish entity spans from non-entity spans.\",\n  \"Direct Inspiration\": [\n    \"b21\",\n    \"b56\",\n    \"b62\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b4\",\n    \"b33\",\n    \"b27\",\n    \"b13\",\n    \"b8\",\n    \"b1\"\n  ],\n  \"Other Inspiration\": []\n}\n```"], "6389d6fe90e50fcafdffc634": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of high storage consumption in prefetchers due to data redundancy in memory access patterns. It proposes a novel Pattern Merging Prefetcher (PMP) that reduces storage requirements by merging similar patterns based on trigger offsets and employs an extraction strategy to accurately prefetch data.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b6\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b11\": 0.5,\n    \"b12\": 0.5,\n    \"b13\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing an efficient prefetcher for processors dealing with complex memory access patterns, focusing on reducing storage overhead and maintaining high performance. The proposed solution, Pattern Merging Prefetcher (PMP), utilizes a pattern merging strategy based on trigger offsets to reduce redundant storage and accurately predict prefetch targets.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b6\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b9\": 0.8,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high storage consumption in prefetchers due to data redundancy in memory access patterns. It proposes a novel Pattern Merging Prefetcher (PMP) that merges similar patterns indexed by trigger offsets to reduce storage requirements while maintaining high performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high storage consumption and redundancy in memory access pattern prefetchers. It proposes a Pattern Merging Prefetcher (PMP) that merges similar patterns based on trigger offsets to reduce storage overhead and maintain high performance.\",\n  \"Direct Inspiration\": {\n    \"Inspired by\": [\"b1\", \"b6\"],\n    \"Motivated by\": [\"b1\", \"b6\"]\n  },\n  \"Indirect Inspiration\": {\n    \"Following\": [\"b8\", \"b10\"],\n    \"Adopt\": [\"b8\"]\n  },\n  \"Other Inspiration\": {\n    \"Important\": [\"b2\", \"b3\", \"b9\", \"b11\", \"b12\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing an efficient prefetcher that balances performance and storage overhead by reducing data redundancy in memory access patterns. It introduces a novel Pattern Merging Prefetcher (PMP) that uses a pattern merging strategy based on trigger offsets and an access frequency-based extraction strategy to improve prefetch accuracy and reduce storage consumption.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b6\": 0.8,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b9\": 0.65,\n    \"b11\": 0.6,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b19\": 0.5\n  }\n}\n```"], "63881b9290e50fcafd3db3f8": ["```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the high memory (SRAM) usage of neural network inference on microcontroller units (MCUs), which prevents the deployment of models with large activation tensors. The paper introduces a novel model compiler called PEX, which applies partial execution to reduce memory usage. PEX creates partial execution schedules and utilizes structured pruning to further optimize memory usage, achieving significant reductions in peak memory usage and establishing state-of-the-art models for low SRAM usage regimes.\",\n    \"Direct Inspiration\": {\n        \"b25\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.9,\n        \"b7\": 0.9,\n        \"b11\": 0.8,\n        \"b17\": 0.8,\n        \"b20\": 0.75,\n        \"b21\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.7,\n        \"b13\": 0.7,\n        \"b23\": 0.65,\n        \"b27\": 0.65\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of high memory (SRAM) usage in neural network inference on microcontroller units (MCUs). The authors propose a novel model compiler, PEX, which reduces memory usage through partial execution. The methodology is further enhanced with structured pruning to co-design the network architecture and execution schedule, achieving significant reductions in memory usage and improved model accuracy.\",\n    \"Direct Inspiration\": {\n        \"b25\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b17\": 0.8,\n        \"b3\": 0.7,\n        \"b20\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b16\": 0.6,\n        \"b23\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the significant challenge of high memory usage in neural network inference on microcontroller units (MCUs), which prevents the deployment of models with large activation tensors. The paper proposes a novel model compiler called PEX, which uses partial execution to reduce peak memory usage by executing operators partially. The compiler also integrates structured pruning to further optimize memory usage.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b17\": 0.8,\n    \"b21\": 0.7,\n    \"b13\": 0.7,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b4\": 0.6,\n    \"b27\": 0.6,\n    \"b11\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of high memory (SRAM) usage in neural network inference on microcontroller units (MCUs) and proposes a novel model compiler, PEX, which uses partial execution to reduce memory usage. The approach includes structured pruning to further optimize memory efficiency.\",\n    \"Direct Inspiration\": {\n        \"b25\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b17\": 0.7,\n        \"b3\": 0.7,\n        \"b5\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.5,\n        \"b20\": 0.5,\n        \"b23\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the high memory usage (SRAM) during neural network inference on microcontroller units (MCUs), which prevents the deployment of models with large activation tensors. The proposed solution is a novel model compiler called PEX, which reduces memory usage through partial execution of operators and structured pruning.\",\n    \"Direct Inspiration\": {\n        \"b25\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b5\": 0.7,\n        \"b17\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b20\": 0.6,\n        \"b23\": 0.5\n    }\n}\n```"], "640fe64790e50fcafd9e2811": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the presence of noisy labels in training data for deep neural networks, which can significantly degrade performance. The paper introduces TCL, a twin contrastive learning model that combines unsupervised representations and noisy annotations to detect and correct label noise. Key innovations include modeling data distribution via a Gaussian mixture model (GMM), out-of-distribution label noise detection, bootstrap cross-supervision with entropy regulation, and leveraging contrastive learning and Mixup techniques.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b19\": 1,\n    \"b20\": 1,\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b11\": 0.8,\n    \"b27\": 0.8,\n    \"b36\": 0.7,\n    \"b46\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b30\": 0.6,\n    \"b41\": 0.5,\n    \"b47\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the degradation of deep neural network performance due to noisy labels in datasets. The authors propose TCL, a novel twin contrastive learning model that leverages unsupervised representations and noisy annotations for learning from noisy labels. Key contributions include a novel out-of-distribution (OOD) label noise detection method, an effective cross-supervision with entropy regulation, and the use of contrastive learning and Mixup techniques to enhance robustness.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b19\": 1.0,\n    \"b20\": 1.0,\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b22\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.8,\n    \"b30\": 0.8,\n    \"b36\": 0.7,\n    \"b38\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"The primary challenge is training noise-robust classification networks that can handle datasets with mislabeled data, which significantly degrade the performance of deep neural networks.\",\n        \"inspirations\": \"The paper proposes a novel Twin Contrastive Learning (TCL) model that leverages unsupervised representations and label-noisy annotations for learning from noisy labels. It introduces a novel out-of-distribution label noise detection method and an effective cross-supervision with entropy regularization loss to handle noisy labels.\"\n    },\n    \"Direct Inspiration\": {\n        \"references\": [\"b0\", \"b19\", \"b20\", \"b28\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b2\", \"b11\", \"b27\", \"b37\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b24\", \"b30\", \"b36\", \"b46\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training deep neural networks with noisy labels. It introduces TCL, a twin contrastive learning model that combines unsupervised representation learning and noisy annotations to detect and correct label noise. The primary innovations include modeling data distribution with a Gaussian mixture model (GMM), formulating label noise detection as an out-of-distribution (OOD) problem, and implementing a cross-supervision mechanism with entropy regulation to enhance robustness against noisy labels.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b19\": 0.9,\n    \"b20\": 0.9,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b11\": 0.6,\n    \"b27\": 0.7,\n    \"b37\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.5,\n    \"b34\": 0.5,\n    \"b30\": 0.5,\n    \"b22\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the issue of noisy labels in training datasets, which can significantly degrade the performance of deep neural networks. The authors propose a novel twin contrastive learning model (TCL) that leverages label-free Gaussian mixture models (GMM) and noisy annotations to tackle this problem. Key contributions include a new out-of-distribution (OOD) label noise detection method, a cross-supervision mechanism with entropy regularization, and the integration of contrastive learning and Mixup techniques to enhance representation learning under noisy conditions.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1.0,\n        \"b19\": 1.0,\n        \"b28\": 1.0,\n        \"b20\": 0.9,\n        \"b22\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b11\": 0.8,\n        \"b37\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.7,\n        \"b18\": 0.7,\n        \"b31\": 0.7,\n        \"b39\": 0.7,\n        \"b41\": 0.7,\n        \"b44\": 0.7,\n        \"b47\": 0.7\n    }\n}\n```"], "63d9d87390e50fcafd57e29e": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of structural pruning of deep neural networks, focusing on the dependency between layers that makes pruning difficult. The proposed solution, Dependency Graph (DepGraph), aims to model these dependencies to enable automatic and generalizable structural pruning across various network architectures.\",\n    \"Direct Inspiration\": {\n        \"b27\": 1,\n        \"b62\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.9,\n        \"b36\": 0.9,\n        \"b19\": 0.8,\n        \"b22\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.7,\n        \"b40\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on structural pruning of neural networks, addressing the challenge of interdependencies among network layers. The proposed method introduces a Dependency Graph (DepGraph) for automatic parameter grouping, aiming to improve the generalizability and effectiveness of structural pruning across various network architectures.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b30\": 0.9,\n    \"b62\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b19\": 0.65,\n    \"b36\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.55,\n    \"b53\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of structural pruning in deep neural networks, which involves removing redundant parameters while preserving network integrity. The proposed method, Dependency Graph (DepGraph), models interdependencies between layers to enable automatic and generalizable pruning across various architectures, including CNNs, RNNs, GNNs, and Vision Transformers.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b36\": 0.8,\n    \"b62\": 0.9,\n    \"b19\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.6,\n    \"b30\": 0.6,\n    \"b53\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b9\": 0.5,\n    \"b11\": 0.5,\n    \"b49\": 0.5,\n    \"b51\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a novel Dependency Graph (DepGraph) method to address the challenges of structural pruning in deep neural networks. This method aims to model and utilize interdependencies between layers to facilitate automatic and generalizable structural pruning across various network architectures such as CNNs, RNNs, GNNs, and Vision Transformers.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b36\": 0.85,\n    \"b62\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b22\": 0.75,\n    \"b9\": 0.7,\n    \"b19\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.65,\n    \"b11\": 0.6,\n    \"b51\": 0.6,\n    \"b53\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of structural pruning in deep neural networks, which is complex due to interdependencies among network layers. The proposed method introduces a Dependency Graph (DepGraph) to model these dependencies and facilitate automatic parameter grouping for effective structural pruning across various neural network architectures.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b36\": 0.9,\n    \"b62\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b19\": 0.8,\n    \"b22\": 0.7,\n    \"b30\": 0.7,\n    \"b53\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b11\": 0.6,\n    \"b51\": 0.6\n  }\n}\n```"], "6424fe3390e50fcafd78b58e": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge outlined in the paper is determining when to pre-train Graph Neural Networks (GNNs) to avoid negative transfer and ensure that the pre-training data benefits downstream tasks. The proposed W2PGNN framework addresses this by evaluating the generative mechanism from pre-training data to downstream data to assess feasibility.\",\n    \"inspirations\": \"The framework is inspired by the need to avoid negative transfer in graph pre-training and takes insights from existing pre-training techniques in other domains like computer vision and natural language processing.\"\n  },\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b15\": 1,\n    \"b24\": 1,\n    \"b31\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b16\": 0.8,\n    \"b46\": 0.8,\n    \"b55\": 0.8,\n    \"b51\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b10\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b30\": 0.6,\n    \"b47\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of determining the feasibility of pre-training Graph Neural Networks (GNNs) to avoid negative transfer. It proposes the W2PGNN framework, which uses graphons to generate a space of possible downstream graphs that can benefit from pre-training. The method involves fitting pre-training graphs into a graphon basis to construct a generator space, from which the feasibility of pre-training can be measured.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b15\": 1,\n    \"b24\": 1,\n    \"b31\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b16\": 0.8,\n    \"b46\": 0.8,\n    \"b55\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.6,\n    \"b43\": 0.6,\n    \"b48\": 0.6,\n    \"b4\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of determining when to pre-train Graph Neural Networks (GNNs) to benefit downstream tasks. It proposes the W2PGNN framework, which uses a graph data generation perspective to assess the feasibility of pre-training. The key idea is to create a graph generator based on graphons that summarize the topological characteristics of pre-training data. This generator helps determine the likelihood that downstream data can benefit from pre-training, thus minimizing negative transfer and optimizing resources.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b15\": 1,\n    \"b24\": 1,\n    \"b31\": 1,\n    \"b51\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b16\": 0.8,\n    \"b46\": 0.8,\n    \"b55\": 0.8,\n    \"b5\": 0.7,\n    \"b7\": 0.7,\n    \"b18\": 0.7,\n    \"b29\": 0.7,\n    \"b38\": 0.7,\n    \"b40\": 0.7,\n    \"b54\": 0.7,\n    \"b58\": 0.7,\n    \"b8\": 0.7,\n    \"b11\": 0.7,\n    \"b20\": 0.7,\n    \"b27\": 0.7,\n    \"b37\": 0.7,\n    \"b39\": 0.7,\n    \"b52\": 0.7,\n    \"b56\": 0.7,\n    \"b60\": 0.7,\n    \"b19\": 0.6,\n    \"b35\": 0.6,\n    \"b59\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b10\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5,\n    \"b47\": 0.5,\n    \"b30\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of determining when pre-training graph neural networks (GNNs) is beneficial. It introduces the W2PGNN framework, which assesses the generative mechanisms of graph data to predict the feasibility of pre-training. The proposed method focuses on creating a graph generator based on graphons, which can summarize the topological characteristics of pre-training data. The study aims to provide practical applications, including guiding the use of pre-trained models, estimating the feasibility of pre-training, and selecting pre-training data to maximize downstream performance.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b14\": 0.95,\n    \"b15\": 0.9,\n    \"b24\": 0.9,\n    \"b31\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b16\": 0.7,\n    \"b46\": 0.7,\n    \"b51\": 0.8,\n    \"b55\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b10\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of determining when to pre-train Graph Neural Networks (GNNs) to avoid negative transfer and improve downstream performance. The proposed W2PGNN framework uses a novel approach to evaluate the feasibility of pre-training by studying the graph data generation mechanism. The key contributions include the definition of transferable graph patterns, the introduction of a graph generator based on graphons, and the approximation of pre-training feasibility using different graphon bases.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b31\": 0.9,\n    \"b24\": 0.9,\n    \"b51\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b46\": 0.7,\n    \"b55\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b12\": 0.6,\n    \"b42\": 0.6,\n    \"b47\": 0.6\n  }\n}\n```"], "62bab8f95aee126c0f6afca5": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of graph self-supervised learning (SSL) by proposing a new method that uses simple augmentations on latent graph representations to improve predictive SSL. The authors introduce the Wiener Graph Deconvolutional Network (WGDN) to better reconstruct graph attributes from augmented latent space, overcoming issues related to additive augmentations in existing methods.\",\n    \"Direct Inspiration\": [\"b2\", \"b3\"],\n    \"Indirect Inspiration\": [\"b10\", \"b22\"],\n    \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges addressed in the paper include the inefficiencies and semantic ambiguities caused by traditional augmentation techniques in predictive self-supervised learning (SSL) for graph data. The authors propose a novel approach by injecting simple augmentations (e.g., Gaussian noise) into latent graph representations, and introduce an adaptive decoder utilizing a Wiener filter for better reconstruction of graph attributes.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b3\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.7,\n        \"b22\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b16\": 0.65,\n        \"b23\": 0.55,\n        \"b41\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of semantic ambiguity and inefficiency in graph self-supervised learning (SSL) due to data augmentation. It proposes a novel approach by injecting simple augmentations like Gaussian noise on latent graph representations to improve generalization and efficiency. Additionally, it introduces the Wiener Graph Deconvolutional Network (WGDN) to better reconstruct graph attributes from augmented latent space.\",\n    \n    \"Direct Inspiration\": [\"b2\", \"b3\"],\n    \n    \"Indirect Inspiration\": [\"b16\", \"b23\", \"b10\"],\n    \n    \"Other Inspiration\": [\"b22\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in predictive self-supervised learning (SSL) for graphs, particularly the issues of high dependence on data augmentation schemes and the inefficiency of domain-specific knowledge augmentation. The authors propose a new method that uses simple augmentations like Gaussian noise on latent graph representations and introduces the Wiener Graph Deconvolutional Network (WGDN) to reconstruct graph attributes from these augmented representations.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b3\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b16\": 0.75,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.65,\n    \"b14\": 0.6,\n    \"b4\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the inefficiency and low generalization of existing data augmentation techniques in predictive self-supervised learning (SSL) for graphs. The proposed algorithm introduces simple augmentations (e.g., Gaussian noise) to latent graph representations and develops a Wiener Graph Deconvolutional Network (WGDN) to improve the reconstruction of graph attributes from augmented latent space.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b1\": 0.7,\n    \"b23\": 0.7,\n    \"b16\": 0.65,\n    \"b22\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b41\": 0.5\n  }\n}\n```"], "6419208e90e50fcafda928aa": ["```json\n{\n  \"Summary\": \"The paper tackles the primary challenge of over-smoothing in deep Graph Neural Networks (GNNs). Over-smoothing refers to the phenomenon where node features in a GNN converge to the same value as the network depth increases, impairing the model's performance. The authors propose a formal definition and measure for over-smoothing and empirically evaluate various methods to mitigate it. They emphasize the need for balancing the mitigation of over-smoothing with maintaining the expressive power of the GNN.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b47\": 1.0,\n    \"b50\": 1.0,\n    \"b49\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.8,\n    \"b36\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b21\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of over-smoothing in deep Graph Neural Networks (GNNs), which results in the convergence of node features to a constant value, impeding the performance of GNNs. The authors propose a formal definition of over-smoothing, review recent measures and methods to mitigate it, and conduct empirical evaluations to highlight the balance needed between mitigating over-smoothing and maintaining the expressive power of GNNs.\",\n  \"Direct Inspiration\": {\n    \"b35\": 0.9,\n    \"b47\": 0.8,\n    \"b50\": 0.85,\n    \"b21\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.7,\n    \"b10\": 0.65,\n    \"b49\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b52\": 0.55,\n    \"b36\": 0.5,\n    \"b13\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed in this paper is the over-smoothing phenomenon in deep Graph Neural Networks (GNNs), which leads to the exponential convergence of all node features towards the same constant value as the number of layers increases. This occurs due to issues such as graph bottlenecks, over-squashing, and over-smoothing, impairing the performance of deep GNNs in realistic settings.\",\n    \"inspirations\": \"The paper aims to unify several recent approaches and define over-smoothing in a formal and tractable manner through an axiomatic construction. It also reviews various methods to measure and mitigate over-smoothing, emphasizing the balance between the expressive power of deep GNNs and their ability to preserve the diversity of node features.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b50\", \"b35\", \"b47\", \"b49\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b34\", \"b21\", \"b14\", \"b43\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b52\", \"b13\", \"b36\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of over-smoothing in deep Graph Neural Networks (GNNs), proposing a unified definition of over-smoothing through axiomatic construction. It reviews existing approaches and measures for over-smoothing, evaluates them empirically, and explores methods to mitigate this phenomenon without sacrificing the expressive power of GNNs.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b47\": 1.0,\n    \"b50\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b36\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"The primary challenge addressed in the paper is the over-smoothing phenomenon in Graph Neural Networks (GNNs), where node features converge to the same constant value with increasing network depth, leading to loss of information and expressivity.\",\n        \"inspirations\": \"The paper seeks to unify existing definitions and measures of over-smoothing and proposes a formal, axiomatic definition. It also reviews and empirically evaluates various methods to mitigate over-smoothing.\"\n    },\n    \"Direct Inspiration\": {\n        \"references\": [\"b35\", \"b47\", \"b50\", \"b7\", \"b49\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b42\", \"b13\", \"b36\", \"b43\", \"b14\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b34\", \"b21\", \"b22\", \"b8\", \"b45\", \"b27\"]\n    }\n}\n```"], "63dc7b5290e50fcafdc836c3": ["```json\n{\n    \"Summary\": \"The paper discusses the challenges posed by the iterative message passing (MP) scheme in Graph Neural Networks (GNNs), particularly the neighbor explosion problem. It proposes a novel subgraph-wise sampling method called Local Message Compensation (LMC) to address these challenges by providing efficient and effective compensations for discarded messages, thereby accelerating convergence and ensuring accurate gradient estimation.\",\n    \"Direct Inspiration\": {\n        \"b4\": 0.9,\n        \"b28\": 0.9,\n        \"b9\": 0.9,\n        \"b5\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b12\": 0.7,\n        \"b30\": 0.7,\n        \"b15\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.6,\n        \"b29\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Graph Neural Networks (GNNs) on large-scale graphs, particularly focusing on the neighbor explosion problem in the iterative message passing (MP) scheme. The proposed Local Message Compensation (LMC) method offers a novel subgraph-wise sampling technique that provides efficient and effective compensations to correct the biases of mini-batch gradients, ensuring faster convergence and scalability.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b5\": 0.9,\n    \"b9\": 0.9,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b12\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.5,\n    \"b29\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of training Graph Neural Networks (GNNs) on large-scale graphs due to the neighbor explosion problem, particularly focusing on the inefficiencies and inaccuracies of subgraph-wise sampling methods. The authors propose a novel method called Local Message Compensation (LMC), which uses efficient and effective compensations to correct the biases of mini-batch gradients, thereby accelerating convergence.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b9\": 0.85,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b5\": 0.75,\n    \"b19\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the neighbor explosion problem in Graph Neural Networks (GNNs) when training on large-scale graphs. The proposed algorithm, Local Message Compensation (LMC), addresses this issue by using efficient and effective compensations to correct the biases of mini-batch gradients, accelerating convergence. LMC is the first subgraph-wise sampling method with a provable convergence guarantee.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b9\": 1,\n    \"b12\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b28\": 0.85\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the neighbor explosion problem in training Graph Neural Networks (GNNs) on large-scale graphs, proposing a novel subgraph-wise sampling method called Local Message Compensation (LMC) to ensure convergence and improve efficiency. The main challenges include the computational complexity and memory consumption due to the iterative message passing scheme in GNNs.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b5\": 0.9,\n    \"b9\": 0.85,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.65,\n    \"b27\": 0.6\n  }\n}\n```"], "64363413cf18aa11d8b55a06": ["```json\n{\n  \"Summary\": \"The paper presents GraphMAE2, an improved generative self-supervised learning framework for graph neural networks aiming to enhance feature reconstruction. The key contributions include multi-view random re-mask decoding and latent representation prediction. The challenges addressed are the reliance on the discriminability of input node features in existing methods, like GraphMAE, and extending the framework to large-scale graphs.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b44\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b29\": 0.6,\n    \"b51\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is improving feature reconstruction in self-supervised learning (SSL) for graph neural networks (GNNs), particularly when node features are less discriminative. The proposed solution, GraphMAE2, introduces two decoding strategies: multi-view random re-mask decoding and latent representation prediction. These innovations aim to reduce overfitting and enhance the informativeness of reconstruction targets.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b31\": 0.8,\n    \"b55\": 0.85,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.75,\n    \"b49\": 0.7,\n    \"b58\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving feature reconstruction in graph self-supervised learning (SSL) by introducing GraphMAE2. The key issues identified include the reliance on the discriminability of input node features, which can be noisy and less informative. The proposed solution, GraphMAE2, incorporates two novel decoding strategies: multi-view random re-mask decoding and latent representation prediction, which serve as regularization techniques to improve the robustness and effectiveness of the feature reconstruction process.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.9,\n        \"b6\": 0.8,\n        \"b12\": 0.85,\n        \"b24\": 0.8,\n        \"b40\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.75,\n        \"b19\": 0.7,\n        \"b29\": 0.7,\n        \"b38\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving feature reconstruction in self-supervised learning (SSL) for graph neural networks (GNNs), particularly when node features are less discriminative. The proposed GraphMAE2 framework introduces two decoding strategies: multi-view random re-mask decoding and latent representation prediction, aiming to enhance the robustness and performance of graph SSL.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b44\": 0.8,\n    \"b40\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b1\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving feature reconstruction in self-supervised learning for graph neural networks (GNNs). It introduces GraphMAE2, which incorporates regularization strategies through multi-view random re-mask decoding and latent representation prediction to enhance the robustness and effectiveness of feature reconstruction. The proposed framework is tested on various public graph datasets, showing significant improvements over existing methods, especially in large-scale graphs.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b29\": 0.7,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b51\": 0.5\n  }\n}\n```"], "642525e790e50fcafdfdd202": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of generating high-performance programs for DLAs by proposing an automatically constrained exploration-based approach called Heron. The main contributions include automatic generation of sophisticated constraints, a novel constraint-based genetic algorithm, and comprehensive evaluation on various DLAs.\",\n  \"Direct Inspiration\": {\n    \"b70\": 0.95,\n    \"b71\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b72\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b24\": 0.6,\n    \"b49\": 0.6,\n    \"b66\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of inefficient and manual development cycles for DLAs due to diverse and complex architectural constraints. It proposes Heron, an automatically constrained exploration-based approach to generate high-performance libraries by enforcing sophisticated constraints through static analysis and a novel constraint-based genetic algorithm (CGA).\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b70\": 0.9,\n    \"b71\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b72\": 0.7,\n    \"b66\": 0.6,\n    \"b49\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.4,\n    \"b16\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently generating high-performance deep learning operators for DLAs by proposing an automatically constrained exploration-based approach called Heron. Heron utilizes static analysis for constraint generation and a novel constraint-based genetic algorithm (CGA) for space exploration.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b70\": 1.0,\n    \"b71\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b66\": 0.8,\n    \"b72\": 0.8,\n    \"b49\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b69\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges in generating high-performance libraries for Deep Learning Accelerators (DLAs) by proposing Heron, an automatically constrained exploration-based approach. The key challenges include accurately constraining the search space and efficiently exploring this constrained space. Heron introduces automatic generation of sophisticated constraints and a novel constraint-based genetic algorithm (CGA) to find optimal programs efficiently.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b70\": 0.9,\n    \"b71\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.75,\n    \"b72\": 0.7,\n    \"b49\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b66\": 0.6,\n    \"b24\": 0.55\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of automatically generating high-performance software libraries for Deep Learning Accelerators (DLAs) by proposing an approach called Heron. The key innovation is the automatic enforcement of sophisticated constraints throughout the program generation process, leveraging static analysis and a novel constraint-based genetic algorithm to navigate irregular search spaces. This approach aims to overcome the inefficiencies of existing exploration-based methods which fail to find optimal programs due to low-quality search spaces.\",\n    \"Direct Inspiration\": {\n        \"b25\": 1,\n        \"b70\": 1,\n        \"b71\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b24\": 0.8,\n        \"b66\": 0.8,\n        \"b72\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.6,\n        \"b31\": 0.6\n    }\n}\n```"], "63aaa48a90e50fcafd27b0fa": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of deploying deep neural networks (DNNs) on edge devices with limited memory and computing resources. It introduces an automatic mapping framework named AutoMap, which features a new intermediate representation (EDWG), a groupwise mapping method, a dynamic programming partitioner (DPP), and a dynamic memory allocator (DMA). These innovations aim to optimize model execution latency and power efficiency under constrained hardware conditions.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b9\": 1.0,\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b10\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of deploying deep neural network (DNN) models on edge devices with limited hardware resources, particularly memory. It proposes an automatic mapping framework named AutoMap. Key contributions include a new intermediate representation (IR) called Extended Directed Weighted Graph (EDWG), groupwise mapping methods, and dynamic programming partitioner (DPP) and dynamic memory allocator (DMA) techniques to optimize memory usage and reduce data movement.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b9\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b5\": 0.8,\n    \"b14\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of deploying deep neural networks (DNNs) on edge devices with limited hardware resources, especially focusing on memory capacity and execution latency. It introduces an automatic mapping framework named AutoMap that uses a new intermediate representation called EDWG, a dynamic programming partitioner (DPP), and a dynamic memory allocator (DMA) to optimize the mapping of DNNs on domain-specific deep learning accelerators (DLAs).\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b9\": 0.9,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of deploying deep neural networks (DNNs) on edge devices with limited hardware resources, particularly focusing on memory capacity and execution latency. It proposes an automatic mapping framework named AutoMap, which includes a novel intermediate representation called EDWG, a dynamic programming partitioner (DPP), and a dynamic memory allocator (DMA). These methods aim to optimize the mapping of DNNs to domain-specific deep learning accelerators (DLAs) to improve performance under resource constraints.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b9\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of deploying deep neural networks (DNNs) on edge devices with limited memory and computational resources. The proposed framework, AutoMap, introduces a new intermediate representation (IR) called extended directed weighted graph (EDWG) and employs a groupwise mapping method with dynamic programming partitioner (DPP) and dynamic memory allocator (DMA) to optimize memory utilization and reduce latency.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b9\": 0.9,\n    \"b10\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.6,\n    \"b14\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.4,\n    \"b4\": 0.4,\n    \"b6\": 0.4,\n    \"b7\": 0.4\n  }\n}\n```"], "6427029c90e50fcafd5d6bd8": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficiently processing large numbers of small tasks in datacenter networks, focusing on reducing the 'datacenter tax' by offloading network and application processing from server CPUs to cache-coherent accelerators (cc-accelerators). RAMBDA is proposed as a solution to achieve efficient memory-intensive \u00b5s-scale datacenter applications with a modularized approach, leveraging standard RDMA NICs and cc-accelerators.\",\n    \"Direct Inspiration\": {\n        \"b26\": 0.9,\n        \"b38\": 0.9,\n        \"b110\": 0.9,\n        \"b111\": 0.9,\n        \"b117\": 0.9,\n        \"b159\": 0.9,\n        \"b160\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.8,\n        \"b83\": 0.8,\n        \"b93\": 0.8,\n        \"b115\": 0.8,\n        \"b140\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b29\": 0.75,\n        \"b68\": 0.75,\n        \"b32\": 0.75\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently processing memory-intensive microsecond-scale datacenter applications. It proposes RAMBDA, a system that utilizes a cache-coherent accelerator (cc-accelerator) and RDMA NIC to reduce CPU involvement in network and application processing. Key components include a unified abstraction for inter- and intra-machine communications, a fast cc-accelerator notification mechanism, a specialized cc-accelerator architecture, and an adaptive device-to-host data transfer mechanism.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b68\": 1.0,\n    \"b32\": 0.9,\n    \"b110\": 0.8,\n    \"b38\": 0.8,\n    \"b117\": 0.8,\n    \"b159\": 0.8,\n    \"b160\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b83\": 0.7,\n    \"b93\": 0.7,\n    \"b115\": 0.7,\n    \"b140\": 0.7,\n    \"b26\": 0.6,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b153\": 0.5,\n    \"b6\": 0.5,\n    \"b82\": 0.5,\n    \"b129\": 0.5,\n    \"b135\": 0.5,\n    \"b144\": 0.5,\n    \"b124\": 0.4,\n    \"b24\": 0.4,\n    \"b147\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies in server processing due to the stagnation of single-thread CPU performance and the limitations of current accelerators for handling datacenter applications. It proposes RAMBDA, a system that integrates a cache-coherent accelerator (cc-accelerator) with RDMA NICs to improve performance and efficiency for memory-intensive applications.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b68\": 1.0,\n    \"b32\": 1.0,\n    \"b38\": 0.9,\n    \"b153\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b76\": 0.8,\n    \"b83\": 0.8,\n    \"b115\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b93\": 0.7,\n    \"b110\": 0.7,\n    \"b111\": 0.7,\n    \"b159\": 0.7,\n    \"b160\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently processing memory-intensive datacenter applications with high network throughput demands, proposing RAMBDA, a system combining RDMA NICs and cache-coherent accelerators. The system aims to reduce CPU load, improve performance, and increase power efficiency by facilitating direct communication between the RNIC and the cc-accelerator.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b29\", \"b68\", \"b32\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b38\", \"b153\", \"b18\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b26\", \"b110\", \"b111\", \"b117\", \"b159\", \"b160\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently processing memory-intensive microsecond-scale datacenter applications. It proposes RAMBDA, a system that leverages a cache-coherent accelerator (cc-accelerator) working in tandem with a standard RDMA NIC (RNIC) to offload parts of application processing from the CPU, aiming to reduce latency, increase throughput, and improve power efficiency.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b68\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.8,\n    \"b153\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b83\": 0.7,\n    \"b115\": 0.7\n  }\n}\n```"], "63a910a390e50fcafd2a8a6e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of personalized subgraph selection for graph neural networks (GNNs) based link prediction (GNNLP). It introduces PS2, a novel framework that automatically and inductively selects the most informative subgraphs for different edges, aiming to improve the performance of existing GNNLP methods. The main contributions include the development of a subgraph selector formulated under bi-level optimization and the demonstration of its effectiveness through extensive experiments.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges of subgraph selection in graph neural network-based link prediction (GNNLP) methods. The main contribution is the development of a personalized subgraph selector (PS2) that automatically and inductively identifies the most informative subgraphs for different edges. This approach is inspired by differentiable architecture search and aims to improve the performance of existing GNNLP methods by making the subgraph selection process adaptive and scalable.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of personalized subgraph selection for graph neural networks based link prediction (GNNLP). Existing methods use a fixed subgraph structure for all edges, which leads to suboptimal results. The proposed approach, PS2, aims to automatically and inductively select the most informative subgraph structures for different edges, addressing issues of scalability, adaptability to unseen edges, and compatibility with existing GNNLP methods.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of link prediction in graphs by proposing a novel personalized subgraph selector (PS2) for Graph Neural Networks based link prediction models (GNNLP). The main challenges include the exponential subgraph selection space, the need for inductive subgraph selection for unseen edges, and the integration of the selector with existing GNNLP methods. The proposed PS2 aims to identify the most informative subgraphs for different edges automatically and inductively, leveraging bi-level optimization and alternating gradient-descent algorithms.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b18\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting missing links in graphs, particularly focusing on the limitations of existing GNN-based link prediction methods. These methods often use a fixed subgraph structure, which may not be optimal for all edges. To overcome this, the authors propose a personalized subgraph selector (PS2) that automatically identifies the most informative subgraphs for different edges. This method is designed to be scalable and inductive, making it applicable to unseen edges during training.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b14\": 1.0,\n    \"b16\": 1.0,\n    \"b17\": 1.0,\n    \"b18\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b21\": 0.8,\n    \"b22\": 0.8,\n    \"b23\": 0.8,\n    \"b24\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```"], "634f6ae490e50fcafdcb6525": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of reducing latency and maximizing throughput in DNN model execution by proposing a new paradigm called task-mapping-oriented programming. This approach simplifies the development of tensor programs and reduces tuning time, compared to existing loop-oriented scheduling paradigms. The proposed compiler, Hidet, shows improved performance over state-of-the-art frameworks and schedulers.\",\n  \"Direct Inspiration\": [\"b9\", \"b46\"],\n  \"Indirect Inspiration\": [\"b8\", \"b24\", \"b33\"],\n  \"Other Inspiration\": [\"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in optimizing deep neural network (DNN) inference on GPUs, focusing on reducing latency and maximizing throughput. The authors propose a task-mapping-oriented programming paradigm to simplify tensor program development and introduce a new DNN compiler called Hidet. This approach overcomes the limitations of existing loop-oriented scheduling primitives, enabling fine-grained program manipulation and reducing tuning time significantly.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b9\": 1.0,\n    \"b10\": 0.9,\n    \"b24\": 1.0,\n    \"b46\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b33\": 0.8,\n    \"b34\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b21\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a new paradigm for writing efficient tensor programs named task-mapping-oriented programming paradigm to address limitations in existing deep learning compilers' loop-oriented scheduling. The novel approach allows for fine-grained program manipulation, reducing tuning time and enhancing performance. Hidet, an implementation of this paradigm, outperforms state-of-the-art frameworks and schedulers while reducing tuning time significantly.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b9\": 0.85,\n    \"b24\": 0.8,\n    \"b46\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b33\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.65,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Limited expressiveness of loop-oriented scheduling in existing DNN compilers.\",\n      \"Difficulty in implementing certain key optimizations like double buffering.\",\n      \"Long tuning times for existing schedulers.\"\n    ],\n    \"Inspirations\": [\n      \"The need for a new programming paradigm to simplify tensor program development while maintaining optimization capability.\",\n      \"The aim to reduce tuning times by exploring a hardware-centric schedule space.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b9\": 1.0,\n    \"b24\": 1.0,\n    \"b46\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b19\": 0.8,\n    \"b33\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b23\": 0.6,\n    \"b38\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of optimizing the execution of Deep Neural Networks (DNNs) on GPUs, particularly focusing on reducing latency and improving throughput. The authors propose a new paradigm called task-mapping-oriented programming to overcome the limitations of loop-oriented scheduling primitives in existing deep learning compilers. They introduce a new DNN compiler named Hidet, which simplifies tensor program development and significantly reduces tuning time while improving performance.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b9\": 0.85,\n    \"b46\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b33\": 0.75,\n    \"b19\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b23\": 0.65\n  }\n}\n```"], "64264f7b90e50fcafd68e145": ["```json\n{\n  \"Summary\": \"The paper discusses the development of CodeGeeX, a multilingual code generation model with 13 billion parameters trained on a large corpus of code across 23 programming languages. The primary challenges addressed include improving code generation and translation, enhancing cross-platform inference, and developing a comprehensive multilingual benchmark (HumanEval-X) to evaluate code models based on functional correctness. The model and its open-source training processes aim to facilitate understanding and advancement in pre-trained code generation models.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b20\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.7,\n    \"b36\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces CodeGeeX, a multilingual code generation model with 13 billion parameters, pre-trained on a large code corpus of 23 programming languages. The primary challenges addressed include computational costs, efficient training, and multilingual code generation and translation. The model leverages transformer-based architectures and generative pre-training strategies to achieve significant performance improvements over existing models. Additionally, the paper presents the HumanEval-X benchmark for evaluating the functional correctness of multilingual code generation and translation.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b7\": 0.9,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b33\": 0.7,\n    \"b32\": 0.7,\n    \"b23\": 0.6,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b36\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents CodeGeeX, a multilingual code generation model with 13 billion parameters, pre-trained on a large corpus of 23 programming languages. The primary challenges addressed include the need for efficient code generation, translation, and explanation across multiple languages. The authors highlight the strengths of CodeGeeX in terms of open-source training, cross-platform inference, and superior performance compared to existing models.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b7\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.6,\n    \"b18\": 0.6,\n    \"b47\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents CodeGeeX, a multilingual code generation model with 13 billion parameters, designed to outperform existing models like Codex, CodeGen, and others. The main challenges addressed include the computational affordability of testing different architectural designs for large pre-trained models and the need for multilingual benchmarks to evaluate code generation and translation tasks. The model introduces several novel methods, including cross-platform support, additional functionalities like code explanation and translation, and a new multilingual benchmark called HumanEval-X.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b7\": 0.95,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.85,\n    \"b10\": 0.8,\n    \"b11\": 0.75,\n    \"b28\": 0.7,\n    \"b44\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b16\": 0.55,\n    \"b21\": 0.5,\n    \"b25\": 0.45,\n    \"b36\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents CodeGeeX, a multilingual code generation model with 13 billion parameters, trained on a large corpus of code data in 23 programming languages. The primary challenges addressed include achieving high performance in code generation, code translation, and code explanation. CodeGeeX offers improvements over existing models like Codex and provides additional functionalities.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b7\": 0.85,\n    \"b20\": 0.8,\n    \"b10\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b36\": 0.65,\n    \"b26\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.55,\n    \"b17\": 0.5,\n    \"b12\": 0.45\n  }\n}\n```"], "634d809c90e50fcafd4e77f0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of low coverage in taxonomies used for NLP tasks. It proposes a prompt-based taxonomy expansion framework called Taxo-Prompt, which leverages a random walk algorithm to capture global structure information and employs prompt tuning to enhance semantic knowledge in language models.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```\n", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of expanding taxonomies by inserting new concepts into an existing taxonomy. It proposes a novel framework named Taxo-Prompt, which leverages a prompt-based approach and a random walk algorithm to capture global structure information of the taxonomy. The method aims to enhance the learning of lexical-syntactic features and infuse global structure knowledge into the language model, improving the efficiency and effectiveness of taxonomy expansion.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b16\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.8,\n        \"b14\": 0.8,\n        \"b15\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b8\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the low coverage problem in taxonomy expansion, proposing a prompt-based taxonomy expansion framework called Taxo-Prompt. The framework utilizes a random walk algorithm to capture the global structure of the existing taxonomy and leverages prompt tuning to fully exploit the semantic knowledge in language models.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b15\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of low coverage in taxonomy expansion for NLP tasks by proposing a novel prompt-based framework called Taxo-Prompt. This framework leverages a random walk algorithm to infuse global structure knowledge into language models, enhancing the learning of lexical-syntactic features and improving hypernym generation for new concepts.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of expanding taxonomies by introducing a prompt-based taxonomy expansion framework called Taxo-Prompt. The proposed method leverages a prompt tuning paradigm and a random walk algorithm to capture global structure information and infuse it into the language model for better hypernym generation.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b14\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.6,\n    \"b6\": 0.5\n  }\n}\n```"], "63dcdb422c26941cf00b642d": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to explore the security implications of neural architecture search (NAS) and to determine if adversaries can exploit NAS to launch new types of backdoor attacks. The proposed algorithm, EVAS, leverages NAS to find neural architectures with inherent vulnerabilities that can be exploited by backdoor attacks. The novel methods include defining a metric based on neural tangent kernel (NTK) and integrating it into the NAS-without-training framework to efficiently identify candidate architectures without requiring model training or backdoor testing.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b31\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b18\": 0.6,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of vulnerabilities in neural architecture search (NAS) that can be exploited by adversaries to launch backdoor attacks. It introduces a novel backdoor attack method called EVAS, which leverages NAS to find neural architectures with inherent, exploitable vulnerabilities. The paper emphasizes the need for understanding the security implications of NAS, especially in security-sensitive domains.\",\n    \"Direct Inspiration\": {\n        \"b5\": 1.0,\n        \"b27\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b31\": 0.8,\n        \"b22\": 0.7,\n        \"b18\": 0.7,\n        \"b41\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.5,\n        \"b3\": 0.4,\n        \"b34\": 0.4,\n        \"b16\": 0.3\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the security implications of neural architecture search (NAS) in automated machine learning (AutoML). It proposes a novel backdoor attack, termed exploitable and vulnerable architecture search (EVAS), which leverages NAS to identify neural architectures with inherent vulnerabilities that can be exploited by adversaries. The approach is robust against traditional defenses and highlights the necessity for enhanced security measures in NAS, especially in security-sensitive domains.\",\n  \"Direct Inspiration\": [\"b5\", \"b27\", \"b22\"],\n  \"Indirect Inspiration\": [\"b31\", \"b41\", \"b15\", \"b18\"],\n  \"Other Inspiration\": [\"b14\", \"b22\", \"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the security implications of neural architecture search (NAS) within automated machine learning (AutoML), specifically proposing a novel backdoor attack called exploitable and vulnerable architecture search (EVAS). The main challenges include defining trigger patterns, identifying exploitable architectures, and efficiently searching for such architectures without training. The paper integrates a novel metric based on neural tangent kernel (NTK) into the NAS framework to achieve this.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b22\": 0.9,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.7,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.5,\n    \"b14\": 0.4,\n    \"b41\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper revolve around the security implications of neural architecture search (NAS), specifically the potential for adversaries to exploit NAS to launch backdoor attacks. The key contribution is the introduction of EVAS, a new backdoor attack method that leverages NAS to find neural architectures with inherent vulnerabilities. The proposed method uses a novel metric based on neural tangent kernel (NTK) to efficiently identify exploitable architectures without requiring model training or backdoor testing. The evaluation demonstrates the effectiveness and robustness of EVAS across various scenarios and benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.9,\n    \"b31\": 0.8,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b15\": 0.5\n  }\n}\n```"], "64250fee90e50fcafdb2d890": ["```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the lack of support for collecting and reporting automated coverage metrics in many open-source or innovative research simulators. The paper proposes a new approach that uses a compiler to lower common automated coverage metrics to a single cover primitive that can be easily implemented for a wide range of different simulators. The approach is implemented for the Chisel hardware construction language and the FIRRTL compiler, and it supports multiple simulators, including Verilator and Treadle.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0,\n        \"b11\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b12\": 0.8,\n        \"b17\": 0.8,\n        \"b24\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.6,\n        \"b19\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper presents a new approach for dynamic verification of digital circuits by leveraging a compiler to lower automated coverage metrics to a single cover primitive that can be implemented across various simulators. The main challenges include the lack of support for automated coverage metrics in open-source simulators, difficulties in merging coverage across different simulators, and the inability to map coverage results back to the original high-level code. Their method involves implementing compiler passes that generate cover primitives, collecting metadata, and using a simulator-independent report generator.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1,\n        \"b11\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b12\": 0.8,\n        \"b19\": 0.7,\n        \"b24\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.5,\n        \"b17\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Lack of support for collecting and reporting automated coverage metrics in open-source or innovative research simulators\",\n      \"Difficulty in merging coverage across various software or FPGA-accelerated simulators and formal tools\",\n      \"Lack of source-level coverage metrics in new hardware languages\"\n    ],\n    \"Approach\": \"The paper presents a new approach that relies on a compiler to lower common automated coverage metrics to a single cover primitive, which is implemented for a wide range of different simulators. This includes the FIRRTL compiler and Chisel hardware construction language.\"\n  },\n  \"Direct Inspiration\": {\n    \"b4\": 0.95,\n    \"b11\": 0.9,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b27\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.65,\n    \"b9\": 0.55,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Lack of support for collecting and reporting automated coverage metrics in open-source simulators.\",\n      \"Difficulty in merging coverage across various software or FPGA-accelerated simulators and formal tools.\",\n      \"Lack of support for source-level coverage metrics in new hardware languages.\"\n    ],\n    \"solutions\": [\n      \"A compiler-based approach to lower automated coverage metrics to a single cover primitive.\",\n      \"Implementing coverage metrics as a compiler pass and creating a simulator-independent report generator.\",\n      \"Creating a unified interface for different simulators to report coverage results.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b11\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in dynamic verification of digital circuit designs, particularly focusing on the lack of support for automated coverage metrics in open-source simulators, difficulties in merging coverage across different simulators, and the absence of source-level coverage metrics for new hardware languages. It proposes a novel approach using a compiler to lower automated coverage metrics to a single cover primitive that is simulator-independent, allowing for a unified and efficient coverage collection and reporting system.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b11\": 0.8,\n    \"b12\": 0.85,\n    \"b17\": 0.75,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.6,\n    \"b19\": 0.65,\n    \"b27\": 0.6,\n    \"b29\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b6\": 0.4,\n    \"b9\": 0.4,\n    \"b15\": 0.4,\n    \"b18\": 0.4,\n    \"b21\": 0.4,\n    \"b22\": 0.4,\n    \"b23\": 0.4\n  }\n}\n```"], "643e0acf0746dc40e3418ed8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning on graphs with mixed homophily and heterophily. It introduces a Multi-view Graph Encoder (MVGE) framework to effectively condense diverse types of signals into embeddings using multiple pretext tasks. The proposed solution focuses on capturing both high-frequency (disparity) and low-frequency (commonality) signals and integrating them into a unified semantic space.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b48\": 0.8,\n    \"b49\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.65,\n    \"b31\": 0.6,\n    \"b17\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in unsupervised Graph Representation Learning (GRL) for graphs with mixed homophily and heterophily patterns. It proposes a novel Multi-view Graph Encoder (MVGE) framework to condense different types of signals into embeddings using multiple pretext tasks, and a novel design to transform those embeddings into a uniform semantic space.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b21\": 0.95,\n    \"b48\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b18\": 0.8,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b15\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in unsupervised Graph Representation Learning (GRL) for graphs exhibiting both homophily and heterophily. It proposes a novel Multi-view Graph Encoder (MVGE) framework that captures both high-frequency and low-frequency signals in graph data using multiple pretext tasks. The method aims to provide a unified embedding space that integrates these diverse signals, enhancing performance across various types of graphs.\",\n  \"Direct Inspiration\": [\"b21\"],\n  \"Indirect Inspiration\": [\"b9\", \"b15\", \"b16\"],\n  \"Other Inspiration\": [\"b38\", \"b48\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the complexities of unsupervised Graph Representation Learning (GRL) in non-Euclidean data, particularly focusing on graphs with mixed homophily and heterophily. The proposed Multi-view Graph Encoder (MVGE) framework aims to capture diverse frequency signals (high and low) simultaneously using two novel pretext tasks and integrates them into a unified semantic space to improve embedding quality.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b2\": 0.9,\n    \"b48\": 0.9,\n    \"b38\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.7,\n    \"b6\": 0.7,\n    \"b18\": 0.7,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b9\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of graph representation learning (GRL) in the context of both homophily and heterophily. It highlights the limitations of existing GRL methods that primarily focus on homophily and proposes a novel Multi-view Graph Encoder (MVGE) framework. This framework captures both high-frequency and low-frequency signals through two pretext tasks: the ego-task and the agg-task. The paper also introduces a design to integrate embeddings from different tasks into a uniform semantic space.\",\n    \"Direct Inspiration\": {\n        \"b21\": 0.9,\n        \"b2\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b31\": 0.75,\n        \"b6\": 0.7,\n        \"b18\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b48\": 0.6,\n        \"b38\": 0.55\n    }\n}\n```"], "634cc7a390e50fcafd162fef": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of heterophily in Graph Neural Networks (GNNs), which occurs when connected nodes have different attributes, leading to poor performance. The authors propose a novel Adaptive Channel Mixing (ACM) framework that improves GNN performance by adaptively exploiting aggregation, diversification, and identity channels at a node-wise level.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1,\n    \"b27\": 0.9,\n    \"b30\": 0.8,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b43\": 0.6,\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b22\": 0.4,\n    \"b40\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges posed by the heterophily problem in Graph Neural Networks (GNNs) and proposes the Adaptive Channel Mixing (ACM) GNN framework, which augments baseline GNNs to better handle heterophily by using adaptive, node-wise, and local information from multiple channels.\",\n  \"Direct Inspiration\": [\"b44\", \"b27\", \"b7\", \"b15\"],\n  \"Indirect Inspiration\": [\"b18\", \"b40\"],\n  \"Other Inspiration\": [\"b2\", \"b38\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the heterophily problem in Graph Neural Networks (GNNs) by proposing a new metric for homophily from a post-aggregation node similarity perspective and introduces the Adaptive Channel Mixing (ACM) GNN framework. This framework enhances uni-channel GNNs to adaptively leverage aggregation, diversification, and identity channels, significantly improving performance on heterophilic graphs.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b44\", \"b27\", \"b7\", \"b15\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b18\", \"b40\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b13\", \"b3\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the heterophily problem in Graph Neural Networks (GNNs) by proposing new homophily metrics based on post-aggregation node similarity and introducing the Adaptive Channel Mixing (ACM) framework to enhance GNN performance on heterophilic graphs.\",\n  \"Direct Inspiration\": [\n    \"b18\",\n    \"b40\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b44\",\n    \"b27\",\n    \"b7\",\n    \"b15\"\n  ],\n  \"Other Inspiration\": [\n    \"b2\",\n    \"b13\",\n    \"b33\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of heterophily in Graph Neural Networks (GNNs) by proposing new homophily metrics derived from a post-aggregation node similarity matrix and introducing the Adaptive Channel Mixing (ACM) framework. The ACM framework boosts GNN performance by utilizing aggregation, diversification, and identity channels adaptively at the node level.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1,\n    \"b27\": 0.9,\n    \"b30\": 0.9,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b40\": 0.8,\n    \"b34\": 0.7,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b42\": 0.6,\n    \"b3\": 0.6,\n    \"b38\": 0.5\n  }\n}\n```"], "643e0ad10746dc40e3419478": ["```json\n{\n  \"Summary\": \"The paper introduces InstructUIE, a unified information extraction framework based on multi-task instruction tuning, which reformulates IE tasks as a natural language generation problem. Key contributions include the new IE INSTRUCTIONS benchmark and significant performance improvements in zero-shot settings.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.95,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b68\": 0.7,\n    \"b59\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b40\": 0.6,\n    \"b39\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving information extraction (IE) tasks using large language models (LLMs). The proposed solution, InstructUIE, reformulates IE tasks as natural language generation problems and uses descriptive instructions for better understanding and performance. It also introduces a new benchmark, IE INSTRUCTIONS, to evaluate the model's effectiveness across diverse datasets.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b36\", \"b34\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b68\", \"b59\", \"b72\", \"b40\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b31\", \"b12\", \"b6\", \"b2\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of large language models (LLMs) in information extraction (IE) tasks. It introduces a new framework, InstructUIE, that leverages natural language instructions to guide LLMs. The paper also presents a new benchmark called IE INSTRUCTIONS, consisting of 32 diverse IE datasets in a unified text-to-text format.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1.0,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b68\": 0.8,\n    \"b59\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7,\n    \"b38\": 0.7,\n    \"b72\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of information extraction (IE) with large language models (LLMs). It critiques existing methods like UIE and USM for their limitations in performance, especially in low-resource settings and new label schemas. The proposed solution, InstructUIE, leverages natural language instructions to guide LLMs for IE tasks, along with auxiliary tasks to improve the model's understanding of diverse semantics. A new benchmark, IE INSTRUCTIONS, consisting of 32 diverse IE datasets, is introduced for evaluation.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b38\": 0.7,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b68\": 0.5,\n    \"b59\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Significant performance gap in LLMs for information extraction tasks.\",\n      \"Poor performance of UIE in low-resource settings and new label schema.\",\n      \"USM's difficulty in integrating with generative language models and increased training/inference time.\"\n    ],\n    \"inspirations\": [\n      \"Reformulation of IE tasks as natural language generation problems.\",\n      \"Development of descriptive instructions and auxiliary tasks for better model understanding.\",\n      \"Construction of a new benchmark (IE INSTRUCTIONS) for standardized evaluation.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b36\": 1.0,\n    \"b34\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b68\": 0.8,\n    \"b59\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b38\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```"], "629041ac5aee126c0fb5da8a": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of long-term time series forecasting (LTSF) and questions the effectiveness of Transformer-based models for this task. The authors propose a simple yet effective baseline model, DLinear, which decomposes time series data into trend and remainder components and utilizes two one-layer linear networks for direct multi-step forecasting. Extensive experiments demonstrate that DLinear outperforms existing Transformer-based models in most cases.\",\n  \"Direct Inspiration\": {\n    \"Inspired by [b26]\": 1,\n    \"Inspired by [b27]\": 1,\n    \"Inspired by [b29]\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"inspired by [b18]\": 0.9,\n    \"inspired by [b16]\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"cites [b1]\": 0.7,\n    \"cites [b28]\": 0.85,\n    \"cites [b15]\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the complexities and inefficiencies of Transformer-based models for long-term time series forecasting (LTSF). The proposed algorithm, DLinear, addresses these challenges by decomposing the time series into trend and remainder series, employing two one-layer linear networks for direct multi-step (DMS) forecasting. The paper is inspired by previous Transformer-based models and aims to validate their effectiveness in LTSF.\",\n  \"Direct Inspiration\": [\"b27\", \"b29\"],\n  \"Indirect Inspiration\": [\"b16\", \"b18\", \"b28\"],\n  \"Other Inspiration\": [\"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed is the effectiveness of Transformer-based models for long-term time series forecasting (LTSF). The authors propose a simple baseline model, DLinear, which decomposes time series into trend and remainder series and uses two one-layer linear networks for direct multi-step (DMS) forecasting. The study shows that DLinear outperforms complex Transformer-based models in most cases, questioning the necessity of Transformers for this task.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b27\": 0.9,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of long-term time series forecasting (LTSF), questioning the effectiveness of Transformer-based solutions for such tasks. It proposes a simple baseline model, DLinear, which decomposes time series into trend and remainder components and uses two linear networks for direct multi-step forecasting. The paper highlights the limitations of Transformer models in temporal dynamics modeling and demonstrates that DLinear outperforms existing Transformer-based models in various benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b27\": 1.0,\n    \"b29\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.9,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b22\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of time series forecasting (TSF), particularly long-term TSF (LTSF), and evaluates the effectiveness of Transformer-based models for this task. The proposed method, DLinear, decomposes time series into trend and remainder components and uses two one-layer linear networks for direct multi-step forecasting, showing superior performance compared to existing Transformer-based models.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b26\", \"b27\", \"b29\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b18\", \"b28\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b16\"]\n  }\n}\n```"], "63e312ef90e50fcafdc191fd": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately tagging scientific literature across diverse fields using metadata such as venues, authors, and references. It introduces a new benchmark dataset called Maple, which covers 19 scientific fields and evaluates multiple classifier models including Parabel, Transformer, and OAG-BERT. The study finds that the effect of metadata varies significantly across fields and classifiers, with venues being consistently beneficial.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b32\": 0.8,\n    \"b43\": 0.8,\n    \"b58\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b44\": 0.7,\n    \"b46\": 0.75,\n    \"b50\": 0.75,\n    \"b59\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b16\": 0.65,\n    \"b20\": 0.65,\n    \"b40\": 0.7,\n    \"b51\": 0.65,\n    \"b52\": 0.6,\n    \"b55\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two major limitations in existing scientific literature tagging approaches: the restriction to one or two scientific fields and the unclear generalization of metadata usage across different classifier models. The authors propose a systematic cross-field cross-model study on the effect of metadata, constructing a large-scale benchmark, Maple, from the Microsoft Academic Graph. They explore the impact of venues, authors, and references on tagging using three classifiers: Parabel, Transformer, and OAG-BERT.\",\n  \"Direct Inspiration\": {\n    \"b32\": 0.9,\n    \"b43\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.7,\n    \"b58\": 0.7,\n    \"b59\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b37\": 0.6,\n    \"b11\": 0.6,\n    \"b41\": 0.6,\n    \"b46\": 0.6,\n    \"b56\": 0.6,\n    \"b20\": 0.6,\n    \"b47\": 0.6,\n    \"b51\": 0.6,\n    \"b55\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scientific literature tagging by constructing a large-scale benchmark Maple and performing a systematic cross-field cross-model study on the effect of metadata. The authors examine the impact of venues, authors, and references across 19 fields using three classifier types: bag-of-words (Parabel), sequence-based (Transformer), and pre-trained language models (OAG-BERT).\",\n  \"Direct Inspiration\": {\n    \"b37\": 1,\n    \"b43\": 0.9,\n    \"b58\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b44\": 0.8,\n    \"b46\": 0.75,\n    \"b59\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b20\": 0.7,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scientific literature tagging, particularly the limitations of existing methods that focus on specific fields and the need to generalize metadata usage across different classifiers. The authors introduce a large-scale benchmark, Maple, and evaluate the effect of metadata on scientific literature tagging using three types of classifiers: bag-of-words, sequence-based, and pre-trained language models.\",\n  \"Direct Inspiration\": [\"b32\", \"b43\", \"b23\"],\n  \"Indirect Inspiration\": [\"b40\", \"b58\"],\n  \"Other Inspiration\": [\"b44\", \"b46\", \"b59\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the limitations of existing scientific literature tagging methods, which often focus on a narrow set of fields and are typically trained from scratch using RNN or Transformer architectures. The paper proposes a systematic study across multiple fields and models to evaluate the effect of metadata on tagging accuracy. Key contributions include the creation of a large-scale benchmark dataset, Maple, and the evaluation of three major types of classifiers enhanced with metadata: Parabel, Transformer, and OAG-BERT. The study finds that the effect of metadata varies significantly across fields and classifiers, with some fields benefiting more from certain types of metadata than others.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1.0,\n    \"b43\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.8,\n    \"b58\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.6,\n    \"b50\": 0.6,\n    \"b59\": 0.5\n  }\n}\n```"], "6344dede90e50fcafd24d1af": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of robustly identifying near-duplicate texts in large and noisy corpora, particularly historical news articles. It highlights the limitations of N-gram methods and proposes neural methods, including a contrastively trained bi-encoder and a re-ranking style method, to significantly improve de-duplication performance. The paper introduces the NEWS-COPY dataset for unbiased evaluation and demonstrates the scalability of the proposed neural methods.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b3\": 0.65,\n    \"b18\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper are the robust identification of near-duplicate texts in large, noisy corpora, the lack of data for unbiased evaluation of de-duplication methods, and the need for scalable, high-performance de-duplication methods.\",\n    \"inspirations\": \"The paper is inspired by previous work on semantic textual similarity, retrieval, and neural text de-duplication methods. It develops new neural methods for text de-duplication using the NEWS-COPY dataset.\"\n  },\n  \"Direct Inspiration\": {\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.9,\n    \"b16\": 0.8,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b22\": 0.5,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of robust identification of near-duplicate texts in large, noisy corpora, particularly focusing on the inadequacy of traditional N-gram based methods and the advantages of neural methods. The authors propose two neural approaches for text de-duplication using a contrastively trained bi-encoder and a re-ranking method, and they introduce the NEWS-COPY dataset for evaluation.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b14\": 0.85,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.65,\n    \"b29\": 0.6,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The paper addresses the challenge of robustly identifying near-duplicate texts in large, noisy corpora, which is critical for maintaining the integrity of training data and avoiding privacy risks. It highlights the inadequacy of existing N-gram based de-duplication methods, particularly in the presence of noise, and proposes neural methods for more effective and scalable de-duplication.\",\n    \"Inspirations\": \"The authors draw inspiration from semantic textual similarity and retrieval literature to develop neural de-duplication methods. They introduce two approaches: a contrastively trained bi-encoder plus clustering method and a reranking style method using a transformer bi-encoder followed by a cross-encoder.\"\n  },\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b14\": 0.9,\n    \"b16\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b20\": 0.75,\n    \"b30\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b29\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of robust text de-duplication in large, noisy corpora, proposing neural methods that outperform traditional N-gram approaches. The study introduces the NEWS-COPY dataset and develops a contrastively trained bi-encoder and a reranking method for neural text de-duplication, highlighting their scalability and performance.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b16\": 0.7,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b4\": 0.4,\n    \"b29\": 0.4\n  }\n}\n```"], "633ba44790e50fcafdfe4b50": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Graph Neural Networks (GNNs) efficiently, which is particularly time-consuming and costly for large-scale graphs. The authors propose a novel method, MLPInit, which leverages the weight space equivalence between Multilayer Perceptrons (MLPs) and GNNs. By training an MLP (PeerMLP) and using its converged weights to initialize the GNN, they significantly accelerate GNN training while maintaining or even improving model performance.\",\n  \"Direct Inspiration\": {\n    \"b65\": 0.95,\n    \"b14\": 0.92,\n    \"b20\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.88,\n    \"b51\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b67\": 0.80,\n    \"b58\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge in this paper is the efficient training of Graph Neural Networks (GNNs) on large-scale graphs, which is time-consuming and costly. The proposed method, MLPInit, leverages the relationship between Multi-Layer Perceptrons (MLPs) and GNNs by transferring the weights of a converged PeerMLP to initialize GNNs, thereby significantly accelerating GNN training and improving performance.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.8,\n    \"b51\": 0.75,\n    \"b60\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b48\": 0.65,\n    \"b62\": 0.6,\n    \"b4\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency and time-consuming nature of training Graph Neural Networks (GNNs) on large-scale graphs. The proposed method, MLPInit, aims to accelerate GNN training by initializing GNNs with the weights of a converged Multi-Layer Perceptron (MLP), which can be trained much faster. The key insights include the observation that GNNs and MLPs share the same weight space, allowing for the transfer of weights between the two models. This approach not only speeds up the training process significantly but also often improves the performance of GNNs on node classification and link prediction tasks.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b65\": 0.8,\n    \"b14\": 0.7,\n    \"b20\": 0.7,\n    \"b67\": 0.7,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b63\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.5,\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accelerating the training of Graph Neural Networks (GNNs) by leveraging Multi-Layer Perceptrons (MLPs). The core idea is to use the weights of a converged MLP to initialize a GNN, termed as MLPInit. This method allows GNNs to train significantly faster while often improving model performance. The paper proposes and validates the MLPInit method through extensive experiments, demonstrating its effectiveness in reducing training time and enhancing performance on various tasks.\",\n  \"Direct Inspiration\": {\n    \"b65\": 0.9,\n    \"b14\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b51\": 0.6,\n    \"b29\": 0.6,\n    \"b67\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.4,\n    \"b63\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Graph Neural Networks (GNNs) efficiently on large-scale graphs. It proposes a novel method called MLPInit, which leverages the weight space similarity between Multi-Layer Perceptrons (MLPs) and GNNs to transfer weights from a converged PeerMLP to initialize the GNN. This approach significantly accelerates the training process while maintaining or improving the performance of the GNNs.\",\n  \"Direct Inspiration\": {\n    \"b29\": 0.9,\n    \"b14\": 0.8,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b51\": 0.7,\n    \"b34\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b25\": 0.65,\n    \"b10\": 0.5\n  }\n}\n```"], "628d9e805aee126c0f979841": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of pre-training language models for dense retrieval tasks, proposing a novel masked auto-encoding framework called RetroMAE. Key inspirations include the limitations of mainstream pre-trained models in sentence-level representation and the need for more effective auto-encoding pre-training algorithms.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b6\": 0.85,\n    \"b21\": 0.8,\n    \"b34\": 0.8,\n    \"b35\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b20\": 0.7,\n    \"b24\": 0.65,\n    \"b31\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b12\": 0.6,\n    \"b22\": 0.55,\n    \"b33\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in dense retrieval by proposing a novel masked auto-encoding (MAE) framework called RetroMAE. The primary challenges include the limitations of existing pre-trained models in sentence-level representation and the inefficiencies and restrictions of current self-contrastive learning (SCL) and auto-encoding (AE) methods. RetroMAE introduces novel components and strategies such as a unique masked auto-encoding process, an asymmetric model structure, and asymmetric masking ratios to achieve competitive performance in dense retrieval tasks.\",\n  \"Direct Inspiration\": {\n    \"b35\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b6\": 0.7,\n    \"b21\": 0.7,\n    \"b24\": 0.6,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b20\": 0.6,\n    \"b34\": 0.5,\n    \"b12\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving dense retrieval performance by proposing a novel masked autoencoding (MAE) framework named RetroMAE. The primary issues identified include the limitations of token-level pre-trained models like BERT and RoBERTa in sentence-level representation, and the inefficiencies in existing self-contrastive learning (SCL) and auto-encoding (AE) methods. RetroMAE introduces an innovative pre-training process with asymmetric masking and structure to enhance the quality of sentence embeddings for dense retrieval tasks.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b6\": 0.8,\n    \"b21\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b20\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving dense retrieval tasks by introducing a novel masked auto-encoding framework, RetroMAE. This method aims to enhance sentence-level representation capabilities, which are inadequately developed by mainstream pre-trained language models. RetroMAE features a novel masked auto-encoding process, asymmetric model structure, and asymmetric masking ratios to improve pre-training effectiveness and downstream retrieval performance.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b6\": 0.85,\n    \"b21\": 0.8,\n    \"b34\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b20\": 0.6,\n    \"b35\": 0.65,\n    \"b9\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b25\": 0.45,\n    \"b2\": 0.4,\n    \"b32\": 0.35\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving dense retrieval by proposing a novel masked auto-encoding (MAE) framework called RetroMAE. The key innovations include a dual-masking approach for encoding and decoding, an asymmetric model structure, and an aggressive masking ratio for the decoder. These innovations aim to enhance the quality of sentence embeddings and the efficiency of training.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b6\": 0.85,\n    \"b21\": 0.8,\n    \"b35\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b24\": 0.65,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.55,\n    \"b2\": 0.5\n  }\n}\n```"], "634d80a390e50fcafd4e7c23": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of overlapping and nested event extraction (EE) in information extraction. Traditional methods often fail in these scenarios due to their sequential nature, which causes error propagation. The authors propose a novel tagging scheme that transforms EE into a word-word relation recognition task and introduce a one-stage model, OneEE, that avoids error propagation by parallelly predicting span and role relations.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.8,\n    \"b16\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Event Extraction (EE) with a focus on overlapping and nested events, which are not well-handled by traditional sequence labeling methods. To overcome this, the authors propose a novel tagging scheme that transforms EE into word-word relation recognition. They introduce OneEE, a one-stage model that uses BERT for encoding and an adaptive event fusion layer to obtain contextual representations, effectively extracting event triggers, arguments, and their roles in parallel without error propagation.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.8,\n    \"b16\": 0.7,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b12\": 0.4,\n    \"b36\": 0.4,\n    \"b0\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of overlapping and nested Event Extraction (EE) by proposing a novel tagging scheme that transforms EE into word-word relation recognition. It introduces a one-stage model, OneEE, which includes an adaptive event fusion layer and parallel prediction to eliminate error propagation.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b38\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.75,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.6,\n    \"b13\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of overlapping and nested event extraction (EE) in information extraction research. Traditional methods fail to adequately handle these complex scenarios, often leading to error propagation due to their multi-stage nature. The proposed solution is a novel tagging scheme that transforms EE into a word-word relation recognition task, implemented in a one-stage model called OneEE. This model uses BERT for encoding, an adaptive event fusion layer for contextual representation, and a prediction layer for parallel processing of span and role relations.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b38\": 0.9,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.7,\n    \"b40\": 0.7,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b21\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of extracting overlapping and nested events in Event Extraction (EE). It proposes a novel tagging scheme that transforms EE into a word-word relation recognition task and introduces a one-stage model, OneEE, to effectively extract these relations without error propagation. The model uses BERT for contextualized word representations and an adaptive event fusion layer for event-aware contextual representations.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b29\": 0.85,\n    \"b38\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.75,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b21\": 0.65,\n    \"b25\": 0.6\n  }\n}\n```"], "62b3da1e5aee126c0fb1b3bc": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively integrating rich text features into GNN models, proposing a stage-wise fine-tuning framework, LM-GNN, to gradually infuse transformer models with graph structure information. It emphasizes the necessity of pre-fine-tuning transformers for graph-aware tasks and presents optimizations for training efficiency.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b17\": 1.0,\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b8\": 0.7,\n    \"b29\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving GNN models using rich text features and proposes a stage-wise fine-tuning framework called LM-GNN, which combines transformers and GNN models. The key contributions include pre-fine-tuning the transformer for graph-aware tasks and optimizing the training process to reduce time and improve performance.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b17\": 0.8,\n    \"b31\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b11\": 0.4,\n    \"b29\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating large-scale language models with graph neural networks (GNNs) for improved performance on graph-related tasks. It proposes a stage-wise fine-tuning framework called LM-GNN that gradually infuses graph structure information into a transformer model before fine-tuning it together with a GNN model. This approach aims to improve performance and reduce training time.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b17\": 0.9,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b16\": 0.7,\n    \"b20\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.75,\n    \"b29\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training GNN models with rich text features by introducing a stage-wise fine-tuning framework called LM-GNN. This framework uses transformers to encode text data and GNN models to incorporate graph structure information. Key contributions include a graph-aware pre-fine-tuning approach, optimization techniques to reduce training time, and a scalable distributed framework. The paper demonstrates improved performance over baseline models on multiple datasets.\",\n  \"Direct Inspiration\": [\"b7\"],\n  \"Indirect Inspiration\": [\"b17\", \"b31\"],\n  \"Other Inspiration\": [\"b12\", \"b20\", \"b16\", \"b26\", \"b8\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating text data into Graph Neural Networks (GNNs) to improve their performance. The proposed algorithm, LM-GNN, employs a stage-wise fine-tuning framework combining transformers and GNNs to encode text data. This method aims to overcome the effectiveness and efficiency challenges associated with directly using pre-trained language models in graph-based tasks. Key contributions include graph-aware pre-fine-tuning of transformers and several system optimizations to reduce computational overhead.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b20\": 0.8,\n    \"b17\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.6,\n    \"b28\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.4,\n    \"b11\": 0.4\n  }\n}\n```"], "62e0acfd5aee126c0f20a05e": ["```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in automatically finding bugs in deep-learning compilers: generating structurally diverse and valid models, and executing compiled models without floating point (FP) exceptional values. The proposed solution, NNSmith, uses a three-step approach: generating computation graphs, compiling models, and running these models with inputs that avoid FP exceptional values. NNSmith achieves model diversity and validity by using operator specifications and SMT solver for constraint solving, and ensures numeric validity with gradient-guided search.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b32\": 1,\n    \"b56\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.8,\n    \"b62\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b37\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges of generating structurally diverse and valid models for testing deep-learning compilers and generating computational inputs that avoid floating point exceptional values during model execution. NNSmith, the proposed solution, uses an SMT solver for constraint solving and a gradient-guided search for generating viable model inputs/weights.\",\n  \"Direct Inspiration\": [\n    \"b28\",\n    \"b32\",\n    \"b56\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b10\",\n    \"b43\",\n    \"b37\",\n    \"b39\"\n  ],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces NNSmith, a tool designed to find bugs in deep-learning compilers by generating structurally diverse and valid models and inputs that avoid floating point (FP) exceptional values during execution. The two primary challenges addressed are generating valid and diverse computation graphs and generating inputs that do not result in FP exceptional values. NNSmith uses a three-step approach: generating a valid computation graph, compiling the model, and generating inputs to compare outputs from the compiled model with a reference implementation.\",\n    \"Direct Inspiration\": {\n        \"b28\": 0.9,\n        \"b32\": 0.9,\n        \"b56\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b43\": 0.8,\n        \"b37\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b39\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of automatically finding bugs in deep-learning compilers by generating structurally diverse and valid models and selecting computation inputs that avoid floating point exceptional values. The authors propose NNSmith, a three-step approach leveraging fuzzing, differential testing, and a gradient-guided search to achieve these goals.\",\n  \"Direct Inspiration\": [\"b28\", \"b32\", \"b56\", \"b39\"],\n  \"Indirect Inspiration\": [\"b10\", \"b37\", \"b43\"],\n  \"Other Inspiration\": [\"b1\", \"b2\", \"b19\", \"b41\", \"b50\", \"b62\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in automatically finding bugs in deep-learning compilers: generating structurally diverse and valid models, and selecting computational inputs that avoid floating point (FP) exceptional values. The proposed solution, NNSmith, generates valid computation graphs and uses gradient-guided search to find viable inputs/weights for testing. The system outperforms state-of-the-art fuzzers in terms of bug detection and code coverage.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1.0,\n    \"b56\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.8,\n    \"b36\": 0.8,\n    \"b58\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b39\": 0.7,\n    \"b43\": 0.7\n  }\n}\n```"], "62c28ae45aee126c0f8a1660": ["```json\n{\n    \"Summary\": \"The primary challenge in the paper is addressing the issue of graph neural network (GNN) architecture overfitting to training data distributions, thereby failing under distribution shifts in real-world applications like drug discovery. The authors propose a novel method called GRACES, which customizes GNN architectures for each graph instance using a self-supervised disentangled graph encoder and architecture self-customization with prototype strategy, enhancing generalization under non-I.I.D. settings.\",\n    \"Direct Inspiration\": [\"b19\", \"b9\", \"b40\"],\n    \"Indirect Inspiration\": [\"b13\", \"b23\", \"b47\"],\n    \"Other Inspiration\": [\"b15\", \"b33\", \"b45\", \"b11\", \"b25\"]\n}\n```", "```json\n{\n    \"Summary\": {\n        \"Challenges\": \"The primary challenge addressed in the paper is the inability of existing Graph Neural Network (GNN) architectures to handle distribution shifts in graph classification tasks. This is particularly problematic in real-world applications like drug discovery where training data is limited and test data distributions can vary significantly.\",\n        \"Inspirations\": \"The paper proposes a novel approach called Graph Neural Architecture Customization with Disentangled Self-supervised learning (GRACES) to design tailored GNN architectures for each graph instance, enhancing their generalization ability under distribution shifts.\"\n    },\n    \"Direct Inspiration\": {\n        \"b19\": 1.0,\n        \"b9\": 1.0,\n        \"b40\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b23\": 0.8,\n        \"b47\": 0.8,\n        \"b31\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.7,\n        \"b33\": 0.7,\n        \"b45\": 0.7,\n        \"b11\": 0.7,\n        \"b25\": 0.7,\n        \"b6\": 0.7,\n        \"b46\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of graph neural network (GNN) architecture search under distribution shifts, which is common in real-world applications such as drug discovery. The proposed method, GRACES, customizes GNN architectures for each graph instance by using a self-supervised disentangled graph encoder and a prototype-based architecture self-customization mechanism. This approach aims to improve generalization under non-I.I.D. settings by capturing key information in a more controllable manner.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b33\": 0.9,\n    \"b45\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b9\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.7,\n    \"b23\": 0.7,\n    \"b47\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of graph classification under distribution shifts by proposing the GRACES model, which customizes GNN architectures for individual graph instances using self-supervised disentangled learning and a prototype-based customization strategy. This approach enhances the generalization ability of GNNs in non-I.I.D settings.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b33\": 0.9,\n    \"b45\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b9\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b34\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of graph classification under distribution shifts and proposes a novel Graph Neural Architecture Customization with Disentangled Self-supervised learning (GRACES) model. The model captures key information on graphs with varying distributions by tailoring a unique GNN architecture for each graph instance. The model integrates three cascaded modules: a self-supervised disentangled graph encoder, architecture self-customization with prototype strategy, and a customized super-network.\",\n    \"Direct Inspiration\": {\n        \"b19\": 0.9,\n        \"b9\": 0.85,\n        \"b40\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.7,\n        \"b33\": 0.7,\n        \"b45\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.6,\n        \"b25\": 0.6,\n        \"b23\": 0.65,\n        \"b47\": 0.65,\n        \"b34\": 0.6,\n        \"b31\": 0.6,\n        \"b6\": 0.55,\n        \"b46\": 0.55\n    }\n}\n```"], "63f5888490e50fcafd27c90c": ["```json\n{\n    \"Summary\": \"The paper addresses challenges in neural recommendation systems, specifically the memory and computational demands of embedding tables. It proposes a new hybrid embedding representation and a dynamic representation-hardware co-design technique called MP-Rec to improve performance and accuracy. The hybrid representation leverages both memory-based and compute-based approaches to increase learning capacity and produce higher quality embeddings, while MP-Rec dynamically maps representations to hardware to maximize throughput and minimize latency.\",\n    \"Direct Inspiration\": {\n        \"b28\": 0.9,\n        \"b53\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b24\": 0.7,\n        \"b25\": 0.7,\n        \"b26\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.6,\n        \"b21\": 0.6,\n        \"b41\": 0.6,\n        \"b45\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of memory capacity and computational demands in neural recommendation models, particularly focusing on embedding representations. It proposes a new hybrid embedding representation and a dynamic representation-hardware co-design technique, MP-Rec, to improve performance and accuracy. The work demonstrates significant improvements using custom AI accelerators like TPUs and IPUs.\",\n  \"Direct Inspiration\": {\n    \"b53\": 1.0,\n    \"b28\": 1.0,\n    \"b24\": 1.0,\n    \"b25\": 1.0,\n    \"b26\": 1.0,\n    \"b9\": 1.0,\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.9,\n    \"b45\": 0.9,\n    \"b41\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b50\": 0.8,\n    \"b29\": 0.8,\n    \"b33\": 0.8,\n    \"b48\": 0.8,\n    \"b23\": 0.8,\n    \"b35\": 0.8,\n    \"b2\": 0.8,\n    \"b6\": 0.8,\n    \"b32\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of memory capacity and bandwidth in neural recommendation models, particularly focusing on embedding tables. It proposes a new hybrid embedding representation that leverages both memory and compute resources to improve model quality. The paper also introduces MP-Rec, a dynamic representation-hardware co-design technique, and MP-Cache to enhance performance further.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b53\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b21\": 0.7,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b45\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of memory capacity and bandwidth in large-scale embedding tables used in deep learning recommendation models. It proposes a hybrid embedding representation and a dynamic representation-hardware co-design technique (MP-Rec) to improve model quality and performance. The key contributions include leveraging custom AI accelerators, dynamic mapping of heterogeneous accelerators, and introducing MP-Cache for efficient caching.\",\n    \"Direct Inspiration\": {\n        \"b27\": 0.9,\n        \"b45\": 0.9,\n        \"b53\": 0.8,\n        \"b28\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.7,\n        \"b16\": 0.7,\n        \"b21\": 0.7,\n        \"b24\": 0.7,\n        \"b25\": 0.7,\n        \"b26\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b41\": 0.6,\n        \"b9\": 0.6,\n        \"b35\": 0.6,\n        \"b36\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of large-scale neural recommendation models, particularly focusing on the memory and compute requirements of embedding tables. The authors propose a new hybrid embedding representation and a dynamic representation-hardware co-design technique called MP-Rec to optimize performance and accuracy. The paper highlights the use of custom AI accelerators and introduces MP-Cache to exploit caching opportunities.\",\n  \"Direct Inspiration\": {\n    \"b53\": 0.9,\n    \"b28\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.75,\n    \"b45\": 0.75,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```"], "626603225aee126c0f2338e4": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"Memory inefficiencies in point cloud algorithms due to irregular computation and memory behaviors, excessive and random DRAM accesses, and frequent SRAM bank conflicts.\",\n    \"algorithm\": \"Crescent, which includes an approximate neighbor search algorithm and selectively bank conflict elision, co-designed hardware, and an approximation-aware training procedure.\"\n  },\n  \"Direct Inspiration\": {\n    \"b43\": 0.9,\n    \"b65\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b13\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies in point cloud networks, specifically focusing on memory inefficiencies in neighbor search and feature computation. The proposed Crescent framework co-designs algorithm and hardware to perform approximate neighbor search and selective bank conflict elision, achieving streaming DRAM accesses, reduced DRAM traffic, and improved performance and energy efficiency without significant accuracy loss.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b43\": 0.85,\n    \"b65\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.7,\n    \"b59\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b38\": 0.6,\n    \"b54\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies in point cloud algorithms, particularly focusing on irregular memory accesses in both DRAM and SRAM. The proposed Crescent system co-designs algorithm and hardware to improve memory access patterns and efficiency. Key contributions include an approximate neighbor search algorithm, selective bank conflict elision, and a training procedure that integrates these techniques to maintain accuracy while optimizing performance.\",\n  \"Direct Inspiration\": {\n    \"b43\": 0.9,\n    \"b65\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b13\": 0.7,\n    \"b31\": 0.7,\n    \"b59\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies of point cloud algorithms on current hardware architectures, focusing on memory access irregularities. It proposes Crescent, which co-designs the algorithm and hardware to optimize memory accesses and reduce inefficiencies. Key contributions include an approximate neighbor search algorithm, selective bank conflict elision, and an approximation-aware training procedure.\",\n  \"Direct Inspiration\": {\n    \"b43\": 0.9,\n    \"b65\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b54\": 0.6,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.5,\n    \"b45\": 0.5,\n    \"b59\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies in point cloud networks related to memory access patterns, specifically in neighbor search and feature computation stages. It proposes Crescent, a co-designed algorithm and hardware solution that imposes structure on memory accesses to convert irregular DRAM accesses into streaming accesses, reduces SRAM bank conflicts, and integrates approximation-aware training to mitigate accuracy loss.\",\n  \"Direct Inspiration\": {\n    \"b43\": 0.9,\n    \"b65\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b59\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b45\": 0.6\n  }\n}\n```"], "63fec3cd90e50fcafdd70322": ["```json\n{\n  \"Summary\": \"The paper introduces GLM-Dialog, an open-source, knowledge-grounded dialogue model in Chinese. The primary challenges addressed include the limited scale of high-quality datasets and the diverse exploitation of external knowledge. The proposed solution involves augmenting datasets, utilizing a classification loss for knowledge usage decisions, and iterative training. Comprehensive evaluations and an online platform for human-bot interaction are also provided.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b8\": 0.85,\n    \"b6\": 0.95,\n    \"b16\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b31\": 0.75,\n    \"b4\": 0.7,\n    \"b1\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.65,\n    \"b36\": 0.65,\n    \"b37\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the development of a knowledge-grounded dialogue model in Chinese, named GLM-Dialog. The primary challenges mentioned include the limited scale of high-quality datasets and diverse exploitation of external knowledge. The model is designed by fine-tuning GLM10B using data augmentation, auxiliary classification loss, and iterative training strategies. It aims to provide an open platform for researchers to overcome these challenges and includes a novel evaluation platform for dialogue models.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.8,\n    \"b8\": 0.85,\n    \"b16\": 0.75,\n    \"b21\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b11\": 0.65,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b4\": 0.5,\n    \"b13\": 0.4,\n    \"b29\": 0.5,\n    \"b33\": 0.4,\n    \"b36\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in developing robust knowledge-grounded dialogue models, particularly for non-English languages, due to limited high-quality datasets and diverse exploitation of external knowledge. The authors propose GLM-Dialog, an open-source Chinese dialogue model, fine-tuned from GLM10B, incorporating data augmentation, auxiliary classification loss, and iterative training.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b6\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b16\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses primary challenges in developing knowledge-grounded dialogue systems, particularly in non-English languages like Chinese. The main challenges include the limited scale of high-quality datasets and the diverse exploitation of external knowledge. The proposed GLM-Dialog model aims to overcome these challenges by fine-tuning a pre-trained Chinese LLM with data augmentation and model training strategies to handle knowledge integration and noise discrimination.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b2\": 0.85,\n        \"b8\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.75,\n        \"b16\": 0.7,\n        \"b21\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.65,\n        \"b11\": 0.6,\n        \"b31\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper aims to address two primary challenges in developing knowledge-grounded dialogue models for non-English languages: the limited scale of high-quality datasets and the diverse exploitation of external knowledge. The proposed solution, GLM-Dialog, is a knowledge-grounded dialogue model in Chinese, enhanced by fine-tuning GLM10B with data augmentation and model training strategies. The paper highlights a series of techniques for dataset augmentation, an auxiliary classification loss for response generation, and an iterative training scheme to improve model performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b8\": 0.8,\n    \"b16\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.6,\n    \"b6\": 0.8,\n    \"b11\": 0.6,\n    \"b31\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b4\": 0.5,\n    \"b13\": 0.4,\n    \"b29\": 0.4\n  }\n}\n```"], "6466faedd68f896efaeb70be": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying Graph Neural Networks (GNNs) to directed graphs, which is often overlooked in favor of undirected graphs. The authors propose a Directed Graph Neural Network (Dir-GNN) framework that separately aggregates incoming and outgoing edges to improve learning on heterophilic graphs. The framework is shown to be as expressive as the Directed Weisfeiler-Lehman test and more expressive than traditional MPNNs. Empirical results demonstrate significant performance gains on heterophilic benchmarks without negatively impacting homophilic benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b21\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b11\": 0.7,\n    \"b49\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b52\": 0.6,\n    \"b25\": 0.55,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying Graph Neural Networks (GNNs) to directed graphs, which is often neglected in favor of undirected graphs. It proposes a novel Directed Graph Neural Network (Dir-GNN) framework that separately aggregates incoming and outgoing edges to handle directed graphs effectively, especially in heterophilic settings where neighbors tend to have different labels.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b21\": 0.8,\n    \"b41\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b13\": 0.75,\n    \"b16\": 0.7,\n    \"b46\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b11\": 0.6,\n    \"b40\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying Graph Neural Networks (GNNs) to directed graphs, particularly in heterophilic settings where neighbors tend to have different labels. The proposed solution is the Directed Graph Neural Network (Dir-GNN) framework, which performs separate aggregations for incoming and outgoing edges, improving performance on heterophilic datasets without negatively impacting homophilic ones.\",\n  \"Direct Inspiration\": [\"b25\", \"b52\"],\n  \"Indirect Inspiration\": [\"b16\", \"b21\", \"b46\"],\n  \"Other Inspiration\": [\"b14\", \"b37\", \"b41\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Applying Graph Neural Networks (GNNs) to directed graphs and improving their performance on heterophilic graphs without negatively impacting performance on homophilic graphs.\",\n    \"algorithm\": \"Proposing a Directed Graph Neural Network (Dir-GNN) framework, which performs separate aggregations of incoming and outgoing edges to enhance learning on heterophilic graphs.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b14\", \"b21\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b6\", \"b25\", \"b49\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b33\", \"b52\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying Graph Neural Networks (GNNs) to directed graphs, particularly in heterophilic settings where traditional methods based on undirected graphs or single-direction information propagation are inadequate. The authors propose a novel Directed Graph Neural Network (Dir-GNN) framework that separates the aggregation of incoming and outgoing edges to improve the learning performance on directed heterophilic graphs.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b21\": 1.0,\n    \"b46\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b25\": 0.8,\n    \"b52\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b11\": 0.6,\n    \"b41\": 0.6\n  }\n}\n```"], "6459ac6bd68f896efa659285": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the difficulty in combining Large Language Models (LLMs) with semantic technologies to enable reasoning and inference, specifically in the creation of Knowledge Graphs from unstructured text data on the Web. The paper is inspired by the need to extract information from raw text data for intelligent reasoning and to address the topic of sustainability. Key inspirations include state-of-the-art NLP techniques and models like Named Entity Recognition (NER), Relation Extraction (RE), Semantic Parsing, and specific models such as GPT-3, T5, BERT, REBEL, and Chat-GPT.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.85,\n    \"b3\": 0.85,\n    \"b4\": 0.85,\n    \"b5\": 0.95,\n    \"b6\": 0.95,\n    \"b7\": 0.95,\n    \"b10\": 0.9,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b12\": 0.75,\n    \"b13\": 0.75,\n    \"b14\": 0.75,\n    \"b15\": 0.75,\n    \"b16\": 0.75,\n    \"b17\": 0.75,\n    \"b18\": 0.75,\n    \"b19\": 0.75,\n    \"b20\": 0.75,\n    \"b21\": 0.75,\n    \"b22\": 0.75,\n    \"b23\": 0.75,\n    \"b24\": 0.75\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extracting useful information from raw text data to create Knowledge Graphs, particularly focusing on sustainability. It proposes leveraging Large Language Models (LLMs) such as GPT-3, T5, BERT, REBEL, and ChatGPT for tasks like Named Entity Recognition, Relation Extraction, and Semantic Parsing to build comprehensive Knowledge Graphs.\",\n  \"Direct Inspiration\": [\"b10\", \"b11\"],\n  \"Indirect Inspiration\": [\"b5\", \"b6\", \"b7\"],\n  \"Other Inspiration\": [\"b1\", \"b2\", \"b3\", \"b4\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of combining Large Language Models (LLMs) with semantic technologies to enable reasoning and inference, specifically through the creation of Knowledge Graphs from raw text data. The novel contribution lies in evaluating the integration of popular NLP models, such as REBEL and ChatGPT, for entity-relation extraction in the domain of sustainability.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b2\": 1.0,\n        \"b3\": 1.0,\n        \"b10\": 1.0,\n        \"b11\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.9,\n        \"b6\": 0.9,\n        \"b7\": 0.9,\n        \"b13\": 0.8,\n        \"b14\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.7,\n        \"b9\": 0.7,\n        \"b15\": 0.7,\n        \"b17\": 0.6,\n        \"b18\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of creating Knowledge Graphs from unstructured text data using state-of-the-art NLP techniques. The authors aim to connect Large Language Models (LLMs) with semantic reasoning to generate Knowledge Graphs automatically, particularly focusing on sustainability. The key NLP models used are REBEL and ChatGPT.\",\n    \"Direct Inspiration\": {\n        \"b10\": 1.0,\n        \"b11\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b3\": 0.8,\n        \"b5\": 0.7,\n        \"b6\": 0.7,\n        \"b7\": 0.7,\n        \"b14\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b4\": 0.6,\n        \"b13\": 0.6,\n        \"b15\": 0.6,\n        \"b18\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the integration of Large Language Models (LLMs) with semantic technologies to enable reasoning and inference. The proposed algorithm focuses on creating Knowledge Graphs from unstructured text data using advanced NLP techniques such as Named Entity Recognition, Relation Extraction, and Semantic Parsing. The novel contribution of the paper is the comparison of REBEL and ChatGPT for generating Knowledge Graphs specifically on the topic of sustainability.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b10\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b12\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```"], "635753cc90e50fcafdddd9f5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of optimizing tensor operations in deep learning compilation, specifically focusing on joint data layout and loop optimization. It proposes ALT, a deep compiler that integrates graph-level and operator-level optimizations to improve performance and efficiency. The key challenges include eliminating overhead from layout transformations, addressing search space reconstruction during joint tuning, and managing the large combined search space of layout and loop tuning.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b82\": 0.9,\n    \"b5\": 0.85,\n    \"b80\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b43\": 0.7,\n    \"b89\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b32\": 0.55,\n    \"b26\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of optimizing data layout and loop transformations in deep learning compilers to achieve high-performance tensor programs. The proposed solution, ALT, focuses on joint graph-level and operator-level optimizations by using a novel auto-tuning framework that incorporates layout propagation and advanced loop transformations.\",\n  \n  \"Direct Inspiration\": [],\n  \n  \"Indirect Inspiration\": [\"b5\", \"b9\", \"b69\", \"b80\", \"b82\"],\n  \n  \"Other Inspiration\": [\"b3\", \"b6\", \"b9\", \"b25\", \"b27\", \"b35\", \"b43\", \"b48\", \"b54\", \"b55\", \"b56\", \"b60\", \"b62\", \"b68\", \"b89\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are optimizing data layout and loop transformations in deep learning compilers to achieve significant performance improvements across various hardware platforms. The proposed algorithm, ALT, introduces a novel deep compiler that performs joint graph-level and operator-level optimizations, integrating data layout and loop tuning into a unified auto-tuning framework. The key innovations include a low-cost layout manipulation infrastructure, a layout propagation mechanism, and a judicious exploration design to manage the expanded search space efficiently.\",\n  \"Direct Inspiration\": [\"b9\", \"b69\", \"b80\", \"b82\", \"b89\"],\n  \"Indirect Inspiration\": [\"b3\", \"b5\", \"b6\", \"b10\", \"b43\"],\n  \"Other Inspiration\": [\"b0\", \"b11\", \"b25\", \"b27\", \"b32\", \"b35\", \"b55\", \"b60\", \"b62\", \"b68\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses key challenges in deep learning compilation, specifically focusing on data layout and loop optimization. The novel contributions include proposing a deep compiler called ALT that integrates these optimizations to improve performance significantly. The work is inspired by the limitations of existing deep compilers and auto-tuning techniques that treat data layout and loop optimizations separately, leading to suboptimal performance.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b5\": 0.9,\n    \"b69\": 0.85,\n    \"b80\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b43\": 0.75,\n    \"b82\": 0.8,\n    \"b89\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.7,\n    \"b32\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in optimizing data layout and loop transformations in deep learning compilers, proposing the ALT compiler for joint graph-level and operator-level optimizations. Key problems include the high overhead of layout transformations, inefficiency due to search space reconstruction, and the large combined search space of layout and loop tuning. ALT introduces a layout propagation mechanism, a cross-exploration architecture for joint tuning, and space pruning techniques to tackle these challenges.\",\n  \"Direct Inspiration\": [\"b5\", \"b9\", \"b69\", \"b80\", \"b82\"],\n  \"Indirect Inspiration\": [\"b3\", \"b13\", \"b35\", \"b55\", \"b60\", \"b62\", \"b68\", \"b89\"],\n  \"Other Inspiration\": [\"b6\", \"b25\", \"b27\", \"b54\"]\n}\n```"], "63a2794890e50fcafd29405f": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of enabling complex reasoning in small language models using a novel approach called Fine-tune-CoT. This approach leverages the zero-shot chain-of-thought (CoT) reasoning capabilities of very large teacher models to generate reasoning samples, which are then used to fine-tune smaller student models. The main contributions include the use of diverse reasoning to generate multiple reasoning solutions for each sample, leading to improved performance in small models without requiring human annotation or task-specific engineering.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b26\": 0.8,\n        \"b3\": 0.8,\n        \"b49\": 0.7,\n        \"b15\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b45\": 0.6,\n        \"b43\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling complex reasoning in smaller language models (LMs) by utilizing a method called Fine-tune-CoT. This method leverages the chain-of-thought (CoT) reasoning capabilities of very large teacher models to fine-tune smaller student models. The approach aims to overcome the computational and cost limitations of using extremely large LMs for reasoning tasks by generating reasoning samples from large models and using them to train smaller models. The proposed method also introduces diverse reasoning to improve performance and sample efficiency.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b17\", \"b45\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b43\", \"b26\", \"b3\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b13\", \"b2\", \"b15\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling small language models to perform complex reasoning tasks. The proposed method, Fine-tune-CoT, leverages the reasoning capabilities of large teacher models to generate reasoning samples, which are then used to fine-tune smaller student models. This approach aims to overcome the limitations of prompt-based CoT methods that require extremely large models by making complex reasoning feasible in smaller models.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b45\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b43\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.7,\n    \"b3\": 0.7,\n    \"b49\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling complex reasoning in smaller language models by leveraging the capabilities of large language models (LLMs) through a method called Fine-tune-CoT. This method involves using zero-shot chain-of-thought (CoT) reasoning from a large teacher model to generate reasoning samples, which are then used to fine-tune smaller student models. The approach aims to preserve the advantages of prompt-based CoT methods while overcoming their reliance on prohibitively large models. Additionally, the paper proposes an extension called diverse reasoning to maximize the teaching effects of Fine-tune-CoT by generating multiple reasoning solutions for each training sample.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b26\": 0.7,\n    \"b45\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling complex reasoning in small language models (LMs) by leveraging the reasoning capabilities of very large teacher models. The proposed method, Fine-tune-CoT, generates reasoning samples from zero-shot CoT prompting of large teacher models and uses these samples to fine-tune smaller student models. This approach aims to overcome the computational limitations of deploying large models at scale while still achieving high performance in reasoning tasks.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.8,\n    \"b30\": 0.7,\n    \"b5\": 0.7,\n    \"b43\": 0.9,\n    \"b45\": 0.8,\n    \"b26\": 0.7,\n    \"b3\": 0.7,\n    \"b49\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b2\": 0.6,\n    \"b12\": 0.5,\n    \"b33\": 0.5,\n    \"b1\": 0.5,\n    \"b10\": 0.4,\n    \"b46\": 0.5,\n    \"b24\": 0.4,\n    \"b25\": 0.4,\n    \"b37\": 0.4,\n    \"b16\": 0.4,\n    \"b15\": 0.6\n  }\n}\n```"], "63b3f1f890e50fcafdea0718": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of asymmetric multicore processors (AMPs), particularly in optimizing thread scheduling to improve system throughput and fairness on Intel Alder Lake processors. It evaluates the Intel Thread Director (TD) hardware support for predicting the Speedup Factor (SF) of threads and compares it with machine learning-based SF prediction models. The main contributions include implementing TD support in the Linux kernel, developing SF prediction models, and experimentally analyzing multiple asymmetry-aware scheduling algorithms.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b25\": 1.0,\n    \"b15\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.7,\n    \"b14\": 0.7,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b16\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of efficiently scheduling threads on asymmetric multicore processors (AMPs). The primary focus is on evaluating Intel Thread Director (TD) for its accuracy in predicting Speedup Factors (SFs) and its effectiveness in aiding OS scheduling decisions. The paper also builds and assesses performance-counter based prediction models using machine learning to address the limitations observed in TD.\",\n    \"Direct Inspiration\": {\n        \"b13\": 0.95,\n        \"b14\": 0.90,\n        \"b25\": 0.85,\n        \"b9\": 0.80\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.75,\n        \"b30\": 0.70,\n        \"b16\": 0.65,\n        \"b23\": 0.60\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.55,\n        \"b28\": 0.50,\n        \"b29\": 0.45\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving OS scheduling decisions on asymmetric multicore processors (AMPs) by utilizing Intel Thread Director (TD) for estimating Speedup Factors (SF) of threads. It proposes kernel-level implementations of asymmetry-aware scheduling algorithms and evaluates the effectiveness of TD-based and PMC-based SF prediction models.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b25\": 1,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b30\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in efficiently scheduling threads on asymmetric multicore processors (AMPs) with Intel Thread Director (TD) technology. It evaluates the effectiveness of TD in predicting the Speedup Factor (SF) and proposes machine learning-based PMC models for better SF prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b25\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b15\": 0.85,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.75,\n    \"b23\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing OS schedulers for Asymmetric Multicore Processors (AMPs) to better distribute workloads across high-performance big cores and power-efficient small cores. It evaluates Intel's Thread Director (TD) for predicting Speedup Factors (SFs) and proposes machine-learning-based PMC models as an alternative.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```"], "6466fafbd68f896efaeb77ac": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of determining whether large, pretrained language models (LMs) can capture semantically meaningful information from the text they process. The primary contributions include empirical evaluation of semantic representations in LMs through program synthesis tasks, development of a novel interventional technique to separate semantic content from syntactic content in model states, and demonstrating that LMs can generate outputs that differ from training distributions in semantically meaningful ways.\",\n  \"Direct Inspiration\": [\"b36\"],\n  \"Indirect Inspiration\": [\"b5\", \"b38\", \"b40\", \"b7\"],\n  \"Other Inspiration\": [\"b4\", \"b0\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper investigates whether large, pretrained language models (LMs) can capture semantically meaningful information about text when trained solely on form-based tasks like next token prediction. The authors test two hypotheses: (H1) LMs are limited to surface-level statistical correlations, and (H2) LMs cannot assign meaning to text. By applying language modeling to program synthesis and probing the LM's hidden states, they find evidence that LMs develop meaningful semantic representations during training. They also introduce a novel interventional experiment to distinguish whether semantics are learned by the LM or just the probe.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.8,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b31\": 0.7,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.65,\n    \"b42\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of whether large, pretrained language models (LMs) can capture semantically meaningful information by training them to perform next token prediction on text. The authors propose an approach using program synthesis as a benchmark, leveraging the semantics of programming languages. Key contributions include demonstrating the emergence of meaningful representations in LMs, designing a novel interventional technique to distinguish between semantic and syntactic representations, and showing that LM outputs differ from their training distribution in semantically meaningful ways.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b21\": 0.8,\n    \"b0\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7,\n    \"b31\": 0.7,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper investigates whether large, pretrained language models (LMs) are capable of understanding and generating semantically meaningful text solely through next token prediction. The authors test two main hypotheses: that LMs are limited to surface-level statistical correlations and cannot assign meaning to text. They explore these hypotheses by applying language modeling to program synthesis, using a trained LM to generate programs and probing its hidden states for semantic representations. The results indicate that meaningful representations do emerge in LMs during training and that these representations are aligned with the semantics of the programming language, suggesting that LMs can indeed learn meaning from text.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b42\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.75,\n    \"b21\": 0.7,\n    \"b0\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b24\": 0.6,\n    \"b34\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper investigates whether large, pretrained language models (LMs) can capture semantically meaningful information when trained solely on next token prediction. The study applies language modeling to program synthesis, probing LM's hidden states for semantic representations and proposing a novel interventional technique to disentangle LM and probe contributions. Key findings include evidence that LMs develop linear encodings of program semantics that correlate with generative accuracy, suggesting they do learn meaning beyond surface-level statistical correlations.\",\n  \"Direct Inspiration\": [\"b4\", \"b22\", \"b21\", \"b0\"],\n  \"Indirect Inspiration\": [\"b10\", \"b11\", \"b1\", \"b41\"],\n  \"Other Inspiration\": [\"b3\", \"b44\", \"b2\", \"b17\"]\n}\n```"], "646c3addd68f896efa5d165d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling in-context learning for graph machine learning tasks, particularly how to formulate a unified task representation and design model architecture and pretraining objectives that facilitate in-context learning across diverse tasks and graphs. It introduces a novel approach called prompt graph and a framework named PRODIGY, which includes a graph neural network-based architecture and self-supervised pretraining tasks for in-context learning.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b17\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.65,\n    \"b20\": 0.65,\n    \"b23\": 0.65,\n    \"b18\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges: (1) formulating a unified task representation for diverse graph machine learning tasks to enable in-context learning without retraining or parameter tuning, and (2) designing model architecture and pretraining objectives for achieving in-context learning across diverse tasks and graphs. The proposed solution introduces the 'prompt graph' for unified task representation and the 'PRODIGY' framework for pretraining an in-context learner over these prompt graphs.\",\n  \"Direct Inspiration\": [\"b1\"],\n  \"Indirect Inspiration\": [\"b6\", \"b7\", \"b12\", \"b18\"],\n  \"Other Inspiration\": [\"b4\", \"b10\", \"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of enabling in-context learning for diverse graph machine learning tasks. The primary challenges include formulating a unified task representation for graph tasks and designing model architecture and pretraining objectives to achieve in-context learning without retraining or parameter tuning. The authors introduce a general approach called PRODIGY, which includes a prompt graph for task representation and a pretraining framework. The framework uses graph neural networks and attention mechanisms to handle node and edge representations and proposes novel pretraining objectives like neighbor matching.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b23\": 0.8,\n    \"b7\": 0.8,\n    \"b12\": 0.8,\n    \"b18\": 0.8,\n    \"b8\": 0.8,\n    \"b2\": 0.8,\n    \"b16\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b21\": 0.7,\n    \"b5\": 0.7,\n    \"b15\": 0.7,\n    \"b22\": 0.7,\n    \"b10\": 0.6,\n    \"b17\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of enabling in-context learning for diverse graph machine learning tasks. The primary challenges include formulating a unified task representation for varied node, edge, and graph-level tasks, and designing pretraining objectives and model architecture to achieve in-context learning without retraining. The proposed solutions are the prompt graph and the PRODIGY framework for pretraining an in-context learner over prompt graphs.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b12\": 0.8,\n    \"b18\": 0.8,\n    \"b23\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b15\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges: (1) how to formulate and represent node-, edge-, and graph-level tasks over graphs with a unified task representation that allows the model to solve diverse tasks without the need for retraining or parameter tuning, and (2) how to design model architecture and pretraining objectives that enable models to achieve in-context learning capability across diverse tasks and diverse graphs in the unified task representation. The authors propose a general approach called PRODIGY, which includes the prompt graph in-context graph task representation and a pretraining framework for in-context learning over graphs. The approach is inspired by previous works and introduces novel methods for task formulation and pretraining.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b20\": 0.8,\n    \"b6\": 0.7,\n    \"b23\": 0.7,\n    \"b7\": 0.7,\n    \"b12\": 0.7,\n    \"b18\": 0.7,\n    \"b8\": 0.7,\n    \"b2\": 0.7,\n    \"b16\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b21\": 0.6,\n    \"b5\": 0.6,\n    \"b15\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```"], "64702deed68f896efa51ffa0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of knowledge graph (KG) completion, specifically focusing on open knowledge graph completion, where the KG can be constructed using new facts from external resources. The proposed framework, TAGREAL, leverages pre-trained language models (PLMs) for automatic prompt generation and information retrieval to facilitate KG completion without relying on manually crafted prompts or pre-defined sets of facts.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b27\": 0.9,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b24\": 0.7,\n    \"b34\": 0.6,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of incomplete large-scale knowledge graphs (KGs) and proposes an end-to-end framework named TAGREAL for open knowledge graph completion using pre-trained language models (PLMs) and textual information. The main issues identified include the poor performance of existing KG completion methods on sparse graphs and the limitations of relying on manually crafted prompts and a predefined set of facts. TAGREAL aims to automatically generate high-quality prompts and retrieve support information to improve KG completion without domain expert knowledge.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b19\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.7,\n    \"b11\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of incomplete knowledge graphs (KGs) by proposing an end-to-end framework TAGREAL for open knowledge graph completion. The framework leverages pre-trained language models (PLMs) and automatically generates high-quality prompts and retrieves support information from external text corpora.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b24\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b18\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of open knowledge graph (KG) completion by proposing a framework called TAGREAL, which leverages pre-trained language models (PLMs) to automatically generate high-quality prompts and retrieve support information from large corpora. This framework aims to overcome the limitations of existing KG completion methods, including their reliance on dense graphs, closed-world assumptions, and manually crafted prompts.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b29\": 0.75,\n    \"b18\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of open knowledge graph (KG) completion, focusing on overcoming the limitations of existing KG completion methods that rely heavily on dense graphs and closed-world assumptions. The paper proposes an end-to-end framework, TAGREAL, that uses pre-trained language models (PLMs) to automatically generate high-quality prompts and retrieve support information to complete KGs using external text information.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b24\": 1,\n    \"b27\": 1,\n    \"b12\": 0.9,\n    \"b3\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b18\": 0.75,\n    \"b21\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b43\": 0.5,\n    \"b29\": 0.4\n  }\n}\n```"], "632630ff90e50fcafdf67436": ["```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the inefficiency of existing cache replacement policies, which fail to closely mimic Belady's MIN policy due to inaccuracies in reuse distance predictions and reliance on age-based heuristics. The paper proposes Mockingjay, a novel ETA-based policy for cache replacement that aims to closely emulate Belady's MIN policy by using accurate reuse distance predictions and minimizing the use of age information.\",\n    \"Direct Inspiration\": {\n        \"b17\": 0.9,\n        \"b27\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.7,\n        \"b18\": 0.6,\n        \"b39\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.5,\n        \"b1\": 0.4,\n        \"b5\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents Mockingjay, an ETA-based cache replacement policy that aims to outperform existing state-of-the-art policies by mimicking Belady's MIN policy more closely. It addresses the challenges of inaccurate reuse distance predictions and over-reliance on age-based eviction policies in previous ETA-based solutions. Mockingjay improves performance by using a long history of past accesses and per-set reuse distances, and by considering age information only when reuse distance information is unavailable.\",\n  \"Direct Inspiration\": [\"b17\", \"b27\"],\n  \"Indirect Inspiration\": [\"b11\", \"b18\", \"b39\"],\n  \"Other Inspiration\": [\"b4\", \"b35\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the problem of cache replacement by proposing a new ETA-based policy called Mockingjay. This method aims to closely mimic Belady's MIN policy by improving reuse distance predictions and minimizing the reliance on age-based ordering. Mockingjay's key contributions include its accuracy in reuse distance predictions, effectiveness in multiclass prediction, and its ability to outperform existing state-of-the-art policies.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1.0,\n        \"b27\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.8,\n        \"b18\": 0.7,\n        \"b39\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b5\": 0.6,\n        \"b8\": 0.6,\n        \"b36\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is improving cache replacement policies by accurately predicting reuse distances and mimicking Belady's MIN policy. The proposed algorithm, Mockingjay, uses multiclass prediction and ETA-based eviction to achieve superior performance compared to existing binary classification-based policies.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b32\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper 'Mockingjay' addresses the shortcomings of previous ETA-based cache replacement policies by introducing a more accurate multiclass prediction approach. The key challenges include overcoming low accuracy in ETA predictions and avoiding reliance on age-based eviction policies. Mockingjay achieves its goals by using a long history of past accesses, per-set reuse distances, and considering age information only when necessary.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b11\": 0.7,\n    \"b18\": 0.7,\n    \"b39\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b5\": 0.5,\n    \"b8\": 0.5,\n    \"b36\": 0.5\n  }\n}\n```"], "6456389bd68f896efacf6b14": ["```json\n{\n  \"Summary\": \"The paper presents AutoML-GPT, an Automatic Machine Learning system utilizing Large Language Models (LLMs) to automate the training of models on datasets with user inputs and descriptions. The system addresses challenges such as incorporating multiple AI models, handling diverse data types, and providing optimal hyperparameter settings for unseen datasets. The core contributions include using language as a universal interface for model descriptions, integrating model and data cards for prompt generation, and establishing a training pipeline for general AI tasks.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b20\": 0.7,\n    \"b16\": 0.7,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.5,\n    \"b27\": 0.5,\n    \"b32\": 0.5,\n    \"b33\": 0.5,\n    \"b34\": 0.4,\n    \"b0\": 0.4,\n    \"b23\": 0.4,\n    \"b1\": 0.4,\n    \"b4\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating multiple AI models into LLMs to create an Automatic Machine Learning (AutoML) system called AutoML-GPT. This system utilizes LLMs to automatically train models on datasets with user inputs and descriptions. The primary challenges include data processing, model architecture design, hyperparameter tuning, and handling unseen datasets. The novel approach involves using model cards and data cards to create prompt paragraphs for LLMs, facilitating the connection of diverse models and enabling effective AI task solutions.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b13\": 0.85,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b30\": 0.7,\n    \"b16\": 0.75,\n    \"b27\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.65,\n    \"b32\": 0.65,\n    \"b33\": 0.65,\n    \"b34\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the development of AutoML-GPT, an AutoML system that leverages large language models (LLMs) to automate the training of machine learning models using user inputs and descriptions. The system integrates data and model descriptions into prompts for LLMs to handle tasks such as data processing, model architecture design, and hyperparameter tuning. The main challenges addressed include the integration of multiple AI models into LLMs and the need for high-quality model descriptions. The proposed solution involves using model cards and data cards to provide structured descriptions for models and datasets, facilitating the connection of diverse models through a language-based interface.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b16\": 0.75,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.65,\n    \"b32\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an Automatic Machine Learning (AutoML) system, AutoML-GPT, which utilizes large language models (LLMs) to automatically train models on datasets with user inputs and descriptions. It aims to integrate diverse AI models through a universal language-based interface, overcoming the difficulty of incorporating multiple AI models and requiring high-quality model descriptions.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b30\": 0.7,\n    \"b16\": 0.7,\n    \"b27\": 0.7,\n    \"b8\": 0.7,\n    \"b32\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.8,\n    \"b28\": 0.8,\n    \"b0\": 0.6,\n    \"b23\": 0.6,\n    \"b1\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper deals with the challenges of developing an Automatic Machine Learning (AutoML) system called AutoML-GPT, which leverages Large Language Models (LLMs) for automatic training on datasets with user inputs and descriptions. The paper proposes using language as a universal interface for LLMs to interact with users and manage AI models for data processing, model architecture design, hyperparameter tuning, and more.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.9,\n    \"b20\": 0.8,\n    \"b30\": 0.8,\n    \"b16\": 0.8,\n    \"b27\": 0.8,\n    \"b8\": 0.8,\n    \"b32\": 0.8,\n    \"b33\": 0.8,\n    \"b34\": 0.8,\n    \"b0\": 0.8,\n    \"b23\": 0.8,\n    \"b1\": 0.8,\n    \"b24\": 0.8,\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.7,\n    \"b29\": 0.7,\n    \"b12\": 0.7,\n    \"b17\": 0.7\n  }\n}\n```"], "648697e6d68f896efaa87966": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving the expressive power of Graph Neural Networks (GNNs) by proposing Path Neural Networks (PathNNs), which generate node representations based on paths emanating from the nodes of graphs. The primary focus is on subsets of paths computable in polynomial time to achieve better performance than the Weisfeiler-Leman (WL) algorithm.\",\n  \"Direct Inspiration\": {\n    \"b38\": 0.9,\n    \"b43\": 0.9,\n    \"b40\": 0.8,\n    \"b1\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b61\": 0.6,\n    \"b46\": 0.6,\n    \"b54\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.5,\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Develop a more expressive graph neural network (GNN) that can distinguish non-isomorphic graphs better than the Weisfeiler-Leman (WL) algorithm, which standard message passing architectures are limited by.\",\n    \"inspirations\": \"The need for a more powerful GNN inspired by the limitations of WL algorithm and the potential of paths (shortest, all shortest, and simple paths) to improve model expressiveness.\"\n  },\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b38\": 0.9,\n    \"b40\": 0.9,\n    \"b43\": 0.95,\n    \"b54\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b61\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.6,\n    \"b46\": 0.65,\n    \"b57\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is developing a more expressive Graph Neural Network (GNN) that can distinguish between non-isomorphic graphs better than traditional approaches such as the Weisfeiler-Leman (WL) algorithm. The proposed solution, Path Neural Networks (PathNNs), leverages paths of increasing length to update node representations, thereby improving the expressive power beyond the WL algorithm. The paper emphasizes the use of shortest paths and simple paths of bounded length, with distinct variants of the PathNN model tailored to these path types. The approach is validated through theoretical proofs and empirical evaluations on synthetic and real-world datasets.\",\n  \"Direct Inspiration\": {\n    \"b43\": 1.0,\n    \"b38\": 1.0,\n    \"b1\": 0.9,\n    \"b54\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b61\": 0.8,\n    \"b63\": 0.8,\n    \"b65\": 0.8,\n    \"b69\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b27\": 0.7,\n    \"b40\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenge of enhancing the expressive power of Graph Neural Networks (GNNs) to better distinguish non-isomorphic graphs by using path-based neural networks (PathNNs). The proposed model computes node representations by aggregating paths of increasing length, specifically focusing on shortest paths and simple paths of bounded length, which are computationally feasible.\",\n  \"Direct Inspiration\": {\n    \"b38\": 0.9,\n    \"b54\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.7,\n    \"b1\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.6,\n    \"b15\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is improving the expressive power of Graph Neural Networks (GNNs) beyond the Weisfeiler-Leman (WL) algorithm. The proposed solution is Path Neural Networks (PathNNs), which compute node representations by aggregating paths of increasing length. The paper introduces three variants of PathNNs focused on different subsets of paths (shortest paths, all shortest paths, and all simple paths). The authors demonstrate that two of these variants are strictly more powerful than the WL algorithm in distinguishing non-isomorphic graphs.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1.0,\n    \"b54\": 0.9,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.8,\n    \"b40\": 0.8,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b61\": 0.6,\n    \"b17\": 0.5\n  }\n}\n```"], "648697e6d68f896efaa8781e": ["```json\n{\n  \"Summary\": \"The paper addresses the issue of under-confidence in GNN-to-MLP knowledge distillation and proposes a Knowledge-inspired Reliable Distillation (KRD) framework. This framework quantifies reliability of knowledge points in GNNs based on their response to noise perturbations and uses this to provide more reliable supervision for training MLPs.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1.0,\n    \"b33\": 1.0,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b27\": 0.6,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.8,\n    \"b28\": 0.7,\n    \"b34\": 0.7,\n    \"b29\": 0.6,\n    \"b2\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of GNN-to-MLP knowledge distillation, specifically identifying and resolving the under-confidence problem in distilled MLPs. The proposed Knowledge-inspired Reliable Distillation (KRD) framework quantifies the reliability of knowledge points in GNNs to provide more reliable supervision for training student MLPs.\",\n  \"Direct Inspiration\": [\"b30\", \"b33\"],\n  \"Indirect Inspiration\": [\"b5\", \"b7\"],\n  \"Other Inspiration\": [\"b0\", \"b9\", \"b28\", \"b29\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of under-confidence in GNN-to-MLP knowledge distillation and proposes a Knowledge-inspired Reliable Distillation (KRD) framework. This framework quantifies the reliability of knowledge points in GNNs by measuring the invariance of their information entropy to noise perturbations and uses these reliable points as additional supervision for training student MLPs.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1.0,\n    \"b33\": 1.0,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b7\": 0.7,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b19\": 0.6,\n    \"b27\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the challenge of the under-confidence problem in GNN-to-MLP knowledge distillation. The authors propose a novel framework named Knowledge-inspired Reliable Distillation (KRD) to filter out unreliable knowledge points and effectively utilize reliable ones for training student MLPs. The key contributions include quantifying the reliability of knowledge points using a perturbation invariance-based metric and providing additional supervision through a dynamic sampling strategy.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.9,\n    \"b33\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b0\": 0.7,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of under-confidence in GNN-to-MLP knowledge distillation, proposing a Knowledge-inspired Reliable Distillation (KRD) framework to enhance the reliability and effectiveness of this process. The main contributions include identifying the under-confidence problem, introducing a perturbation invariance-based metric for quantifying knowledge reliability, and developing a dynamic sampling strategy to provide reliable supervision for training student MLPs.\",\n  \"Direct Inspiration\": [\"b30\", \"b33\"],\n  \"Indirect Inspiration\": [\"b5\", \"b28\"],\n  \"Other Inspiration\": [\"b1\", \"b7\", \"b15\", \"b18\"]\n}\n```"], "6389d70490e50fcafdffd95e": ["```json\n{\n  \"Summary\": \"The paper addresses high address translation overheads in large and/or irregular workloads under hardware memory compression. It proposes Translation-optimized Memory Compression for Capacity (TMCC) to mitigate these overheads by embedding Compression Translation Entries (CTEs) into Page Table Blocks (PTBs) and specializing ASIC Deflate for memory to improve decompression speed.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b28\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of high address translation overheads in hardware memory compression for large and irregular workloads. It proposes Translation-optimized Memory Compression for Capacity (TMCC) which builds on an OS-inspired approach to compress only cold pages and embeds Compression Translation Entries (CTEs) into Page Table Blocks (PTBs) to reduce translation latencies.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.6,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the high address translation overheads under hardware memory compression for large and/or irregular workloads. It introduces Translation-optimized Memory Compression for Capacity (TMCC) which builds on an OS-inspired approach and proposes embedding Compression Translation Entries (CTEs) into Page Table Blocks (PTBs) and a specialized ASIC Deflate for memory to achieve higher performance and effective memory capacity.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of high address translation overheads in large and/or irregular workloads under hardware memory compression. It proposes Translation-optimized Memory Compression for Capacity (TMCC) which builds on an OS-inspired approach but introduces novel techniques to tackle the challenges of CTE misses and high decompression latency.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.95,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.85,\n    \"b4\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.75,\n    \"b23\": 0.7,\n    \"b28\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge discussed in the paper is the high address translation overhead for large and/or irregular workloads under hardware memory compression. The proposed algorithm, Translation-optimized Memory Compression for Capacity (TMCC), addresses this by embedding Compression Translation Entries (CTEs) into Page Table Blocks (PTBs) and optimizing the Deflate algorithm for memory.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b22\": 0.65,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.55,\n    \"b29\": 0.5\n  }\n}\n```"], "64389992d6db87a146dd25d0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of evaluating and comparing the precise event sampling (PES) facilities of Intel and AMD processors, specifically Intel PEBS and AMD IBS. The motivation is to provide tool developers and hardware designers with insights into these systems to improve profiling tools and future processor designs. The paper presents both qualitative and quantitative comparisons, develops a suite of benchmarks for assessing these systems, and highlights key differences in accuracy, overhead, and stability.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b7\": 0.9,\n    \"b10\": 0.9,\n    \"b14\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b23\": 0.7,\n    \"b30\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"long latency remote memory accesses in NUMA multicore systems\",\n      \"data locality problems\",\n      \"performance degrading bandwidth consumption\",\n      \"conflict cache misses\"\n    ],\n    \"Inspirations\": [\n      \"Intel PEBS and AMD IBS adopt drastically different designs that affect the accuracy, stability, overhead and functionality of the sampling facility.\",\n      \"The qualitative and quantitative characteristics and comparisons of Intel PEBS and AMD IBS through extensive benchmarks.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b15\": 0.8,\n    \"b7\": 0.85,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.6,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of accurate, stable, and low-overhead event sampling in multicore systems, particularly comparing Intel PEBS and AMD IBS. It proposes synthetic benchmarks to evaluate these event sampling mechanisms and provides a comprehensive analysis of their differences.\",\n    \"Direct Inspiration\": {\n        \"b20\": 1.0,\n        \"b23\": 1.0,\n        \"b29\": 0.9,\n        \"b30\": 0.8,\n        \"b31\": 0.8,\n        \"b32\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.7,\n        \"b10\": 0.7,\n        \"b14\": 0.6,\n        \"b15\": 0.6,\n        \"b33\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b34\": 0.5,\n        \"b35\": 0.5,\n        \"b36\": 0.5,\n        \"b37\": 0.5,\n        \"b38\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenges of precise event sampling (PES) in Intel and AMD systems, comparing their designs, accuracy, stability, overhead, and functionality. It proposes benchmarks to evaluate these differences and provides insights for both tool developers and hardware designers.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b33\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b10\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.75,\n    \"b23\": 0.75,\n    \"b30\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges of precise event sampling (PES) in Intel and AMD processors, comparing their designs, accuracy, stability, overhead, and functionality. It introduces a suite of microbenchmarks for evaluating these characteristics and provides detailed qualitative and quantitative comparisons. The primary goal is to aid profiling tool developers, performance analysts, and hardware designers in better understanding and improving their designs.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b8\": 0.9,\n    \"b10\": 0.95,\n    \"b14\": 0.9,\n    \"b15\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b9\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b20\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```"], "6482a38fd68f896efa8db695": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting pre-trained language models (PLMs) to new domain knowledge while preserving old-domain knowledge. The proposed solution, MixDA, introduces domain-specific adapters in parallel with original pre-trained feed-forward networks (FFNs) to maintain old-domain knowledge. The approach involves two stages: domain-specific tuning and task-specific tuning, achieving reliability, scalability, and efficiency in domain adaptation.\",\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [\"b18\", \"b5\", \"b48\"],\n  \"Other Inspiration\": [\"b17\", \"b41\", \"b56\", \"b39\", \"b41\", \"b26\", \"b53\", \"b27\", \"b78\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to adapt pre-trained language models (PLMs) with new domain knowledge while keeping the old-domain knowledge unmodified. The proposed solution is Mixture-of-Domain-Adapters (MixDA), which decouples feed-forward networks (FFNs) into two parts: the original pre-trained FFNs and novel domain-specific adapters, trained in parallel to inject domain-specific knowledge without affecting the old-domain knowledge. The method involves two stages: domain-specific tuning of multiple knowledge adapters on unlabeled data and task-specific tuning of adapters on labeled data.\",\n  \"Direct Inspiration\": [\"b18\", \"b5\", \"b48\"],\n  \"Indirect Inspiration\": [\"b17\", \"b41\", \"b56\", \"b39\", \"b26\", \"b53\", \"b27\"],\n  \"Other Inspiration\": [\"b78\", \"b20\", \"b51\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is how to adapt pre-trained language models (PLMs) to new domain knowledge while retaining old-domain knowledge, without incurring high costs or catastrophic forgetting. The proposed solution is the Mixture-of-Domain-Adapters (MixDA), which uses domain-specific adapters paralleled with original pre-trained feed-forward networks (FFNs) to inject domain-specific knowledge efficiently.\",\n  \"Direct Inspiration\": [\"b18\", \"b5\", \"b48\"],\n  \"Indirect Inspiration\": [\"b21\", \"b17\", \"b41\", \"b56\", \"b39\", \"b26\", \"b53\", \"b27\"],\n  \"Other Inspiration\": [\"b12\", \"b78\", \"b20\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting pre-trained language models (PLMs) to new domains without incurring high costs or catastrophic forgetting. The proposed solution, Mixture-of-Domain-Adapters (MixDA), decouples feed-forward networks (FFNs) into pre-trained and domain-specific adapters to inject domain-specific knowledge while preserving old-domain knowledge.\",\n  \"Direct Inspiration\": [\"b18\", \"b5\", \"b48\"],\n  \"Indirect Inspiration\": [\"b17\", \"b41\", \"b56\", \"b39\", \"b41\", \"b26\", \"b53\", \"b27\", \"b51\", \"b20\"],\n  \"Other Inspiration\": [\"b21\", \"b23\", \"b12\", \"b78\", \"b67\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting pre-trained language models (PLMs) to new domain-specific tasks without losing the original domain knowledge. The authors propose a novel algorithm called Mixture-of-Domain-Adapters (MixDA) that decouples feed-forward networks (FFNs) into original and domain-specific adapters to maintain both old and new knowledge. The approach involves two stages: domain-specific tuning on unlabeled data and task-specific tuning on labeled data. The effectiveness of MixDA is demonstrated through experiments on various datasets.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b5\": 0.9,\n    \"b48\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b26\": 0.7,\n    \"b53\": 0.7,\n    \"b78\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b41\": 0.5,\n    \"b27\": 0.5\n  }\n}\n```"], "640e949e90e50fcafd114d4b": ["```json\n{\n  \"Summary\": \"The paper addresses the scalability and efficiency challenges of graph transformers (GTs) by proposing EXPHORMER, a sparse attention mechanism that combines global nodes, expander graphs, and local neighborhoods. The approach aims to reduce the computational complexity and memory usage of GTs while maintaining or surpassing the performance of full transformers.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b9\": 0.9,\n    \"b47\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b2\": 0.7,\n    \"b40\": 0.7,\n    \"b30\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.6,\n    \"b16\": 0.5,\n    \"b29\": 0.5,\n    \"b43\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of scalability and efficiency in graph transformers. It proposes EXPHORMER, which introduces sparse attention mechanisms based on global nodes and expander graphs to reduce computational complexity while maintaining high performance and expressivity. The key contributions include achieving state-of-the-art results on various datasets and improving scalability compared to existing methods.\",\n    \"Direct Inspiration\": {\n        \"b35\": 1,\n        \"b9\": 0.9,\n        \"b47\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b25\": 0.8,\n        \"b44\": 0.7,\n        \"b29\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5,\n        \"b40\": 0.5,\n        \"b33\": 0.5,\n        \"b30\": 0.5,\n        \"b43\": 0.5,\n        \"b16\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges related to the scalability and performance of graph transformers, particularly focusing on the quadratic complexity of the global attention mechanism and the lag in accuracy compared to message-passing networks. The proposed solution, EXPHORMER, introduces sparse attention mechanisms using global nodes, expander graphs, and local neighborhoods, achieving state-of-the-art performance while being more efficient and less memory-intensive.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1,\n    \"b9\": 1,\n    \"b47\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.9,\n    \"b44\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the scalability and efficiency challenges of graph transformers, particularly when dealing with large graphs. It introduces EXPHORMER, a novel sparse attention mechanism that combines global nodes, expander graphs, and local neighborhoods to reduce computational complexity while maintaining performance.\",\n  \"Direct Inspiration\": {\n    \"b35\": 0.9,\n    \"b47\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b44\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.6,\n    \"b29\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalability and efficiency in graph transformers, particularly for large graphs. It introduces EXPHORMER, a novel sparse attention mechanism that combines global nodes and expander graphs to achieve linear computational cost in the number of nodes and edges. The approach aims to overcome the quadratic complexity of standard transformers and the limitations of existing sparse attention mechanisms designed for sequences.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1,\n    \"b25\": 0.9,\n    \"b47\": 0.9,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.7,\n    \"b44\": 0.7,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```"], "63dcdb422c26941cf00b6136": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the limitations of current Sparse Neural Networks (SNN) evaluation protocols, which are narrowly evaluated and potentially ill-suited to identify new and unexpected capabilities. The paper introduces the 'Sparsity May Cry' (SMC-Bench) benchmark to address these limitations by providing a suite of large-scale, challenging, realistic, and diverse tasks and datasets. The main contribution is the comprehensive evaluation of state-of-the-art pruning and sparse training approaches on SMC-Bench, revealing that existing sparse algorithms fail on these complex tasks, which demands reconsideration of the benefits of SNNs.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b31\": 0.9,\n    \"b50\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b20\": 0.8,\n    \"b37\": 0.8,\n    \"b44\": 0.8,\n    \"b93\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b67\": 0.7,\n    \"b53\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of current Sparse Neural Networks (SNNs) evaluation protocols, which are often narrow and restricted to well-understood datasets. It introduces a new benchmark, 'Sparsity May Cry' (SMC-Bench), to provide a comprehensive and challenging evaluation of SNNs across various tasks. The paper's findings indicate that existing SNN algorithms fail to generalize well on SMC-Bench. Inspired by the 'lazy regime' dynamics in overparameterized models, the paper proposes the use of second-order pruning approaches, such as oBERT, for better performance.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b50\": 1,\n    \"b31\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.9,\n    \"b20\": 0.9,\n    \"b37\": 0.8,\n    \"b44\": 0.7,\n    \"b93\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"The paper addresses the limitations of current SNN evaluation protocols which are narrowly focused on specific tasks and datasets. It proposes a new benchmark, SMC-Bench, to evaluate the generalization and scalability of SNNs across diverse and challenging tasks.\",\n        \"inspirations\": \"The paper is inspired by the need to develop a comprehensive and realistic benchmark for evaluating SNNs, driven by the observation that existing evaluation methods are insufficient to capture the full potential and limitations of SNNs.\"\n    },\n    \"Direct Inspiration\": {\n        \"references\": [\"b7\", \"b50\", \"b31\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b16\", \"b20\", \"b93\", \"b37\", \"b14\", \"b44\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b94\", \"b67\", \"b4\", \"b53\", \"b60\"]\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the limitations of current Sparse Neural Network (SNN) evaluation protocols by introducing a new benchmark called 'Sparsity May Cry' (SMC-Bench). It aims to assess the capability of SNNs across a diverse set of challenging tasks including commonsense reasoning, arithmetic reasoning, multilingual translation, and protein prediction. The main contributions include the introduction of SMC-Bench, critical observations on the performance of state-of-the-art (SOTA) sparse algorithms, and an investigation into the causes of SNN failures.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b50\": 0.9,\n        \"b31\": 0.9,\n        \"b67\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.8,\n        \"b20\": 0.8,\n        \"b44\": 0.8,\n        \"b37\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b64\": 0.7,\n        \"b93\": 0.7,\n        \"b18\": 0.7,\n        \"b4\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of current sparse neural network (SNN) evaluation protocols, proposing the 'Sparsity May Cry' Benchmark (SMC-Bench) to provide a comprehensive assessment of SNNs across diverse, challenging tasks. Key observations indicate that state-of-the-art (SOTA) sparse algorithms fail significantly on SMC-Bench, prompting a need to reconsider the benefits of SNNs. The authors propose second-order pruning approaches inspired by the 'lazy regime' dynamics to improve performance.\",\n  \"Direct Inspiration\": [\"b7\", \"b50\", \"b31\"],\n  \"Indirect Inspiration\": [\"b87\", \"b79\", \"b70\", \"b67\", \"b4\"],\n  \"Other Inspiration\": [\"b16\", \"b20\", \"b44\"]\n}\n```"], "6464b048d68f896efa35a285": ["```json\n{\n  \"Summary\": \"The paper proposes a novel Generative Adversarial Networks-based pre-training framework (GAN-MAE) for self-supervised learning in computer vision. The main challenges addressed are the blurrier output images from reconstruction optimization with MSE loss, the lack of inner dependency between masked patches, and the substantial computation cost of mask-reconstruction methods. The proposed GAN-MAE framework introduces a generator-discriminator model to improve image patch modeling, shared parameters to promote memory reduction and training efficiency, and adversarial training for better representation learning.\",\n  \"Direct Inspiration\": [\n    \"b26\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b2\",\n    \"b68\",\n    \"b17\",\n    \"b75\"\n  ],\n  \"Other Inspiration\": [\n    \"b19\",\n    \"b24\",\n    \"b7\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in masked autoencoder (MAE) models used for self-supervised learning in vision transformers. It proposes a novel generative adversarial network (GAN) based pre-training framework called GAN-MAE, which integrates a discriminator to guide the generator for better image patch reconstruction, thus improving the representation learning and computational efficiency.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b26\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.85,\n    \"b60\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.70,\n    \"b24\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in masked autoencoder (MAE) models for self-supervised learning in computer vision. It proposes a GAN-MAE framework that introduces a discriminator to improve the quality and efficiency of masked patch reconstruction, leveraging a shared vision transformer backbone for both generator and discriminator.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b68\": 0.6,\n    \"b75\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.8,\n    \"b54\": 0.8,\n    \"b7\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three main issues with masked autoencoder (MAE) models in computer vision: (i) blurrier output images due to MSE loss, (ii) lack of inner dependency between masked patches, and (iii) substantial computation cost. It proposes a novel Generative Adversarial Networks-based pre-training framework called GAN-MAE, which integrates a generator and discriminator to improve image patch reconstruction and classification performance.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b68\": 0.7,\n    \"b75\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b36\": 0.6,\n    \"b49\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents GAN-MAE, a new self-supervised learning framework for computer vision tasks that integrates Generative Adversarial Networks (GAN) with Masked Autoencoders (MAE). The primary challenges addressed include improving reconstruction quality, handling inner dependencies between masked patches, and reducing computation costs. The core contributions are the introduction of a discriminator to guide better feature representation and sharing parameters between the generator and discriminator to enhance efficiency.\",\n  \"Direct Inspiration\": [\"b26\"],\n  \"Indirect Inspiration\": [\"b2\", \"b68\", \"b75\"],\n  \"Other Inspiration\": [\"b5\", \"b6\", \"b33\", \"b7\", \"b50\", \"b54\"]\n}\n```"], "63dcdb422c26941cf00b613b": ["```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include uncovering data dependencies from observations in real-world data with unknown underlying physical principles, dealing with incomplete/noisy observed relations, and creating a scalable method for learning expressive representations. The proposed algorithm, DIFFORMER, is inspired by diffusion processes on Riemannian manifolds and aims to leverage global information from other instances to obtain more informative representations through a diffusion model and energy function.\",\n    \"Direct Inspiration\": [\"b19\", \"b33\"],\n    \"Indirect Inspiration\": [\"b3\", \"b24\", \"b35\", \"b41\", \"b46\", \"b52\"],\n    \"Other Inspiration\": [\"b10\", \"b16\", \"b17\", \"b22\", \"b27\", \"b48\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of uncovering data dependencies in real-world datasets, which often violate the assumption of IID sampling in standard representation learning paradigms. The proposed solution, DIFFORMER, is a novel diffusion-based Transformer model inspired by physics, specifically heat diffusion on Riemannian manifolds. It aims to leverage global information from other instances to obtain more informative representations. The model features a flexible diffusivity function and a principled energy function to guide instance representations towards ideal constraints of internal consistency.\",\n  \"Direct Inspiration\": [\"b19\", \"b33\"],\n  \"Indirect Inspiration\": [\"b24\", \"b35\", \"b41\", \"b46\", \"b52\"],\n  \"Other Inspiration\": [\"b4\", \"b15\", \"b25\", \"b38\", \"b50\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of uncovering data dependencies from real-world data that are not IID sampled. It proposes a novel general-purpose encoder framework, DIFFORMER, inspired by physics, specifically heat diffusion on Riemannian manifolds. The model leverages a diffusion process for feature propagation among arbitrary instance pairs with optimal inter-connecting structures. The framework unifies and extends popular models like MLP, GCN, and GAT, and demonstrates success in various tasks such as semi-supervised node classification and spatial-temporal dynamics prediction.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1,\n    \"b19\": 1,\n    \"b61\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.8,\n    \"b24\": 0.8,\n    \"b41\": 0.8,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b52\": 0.6,\n    \"b16\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of uncovering data dependencies in real-world datasets where underlying physical principles are unknown. The authors propose a novel encoder framework, DIFFORMER, inspired by diffusion processes on Riemannian manifolds. This model leverages continuous dynamics and a principled energy function to propagate features among instances and achieve better representation learning.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.95,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.85,\n    \"b22\": 0.85,\n    \"b24\": 0.8,\n    \"b48\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.75,\n    \"b35\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of uncovering data dependencies in real-world datasets where underlying physical principles are unknown. The proposed solution, DIFFORMER, is a novel encoder framework inspired by diffusion processes on Riemannian manifolds, leveraging a principled energy function to guide instance representations.\",\n  \"Direct Inspiration\": [\"b19\", \"b33\"],\n  \"Indirect Inspiration\": [\"b41\", \"b24\", \"b48\"],\n  \"Other Inspiration\": [\"b52\", \"b16\"]\n}\n```"], "648000a9d68f896efaa12362": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the susceptibility of Graph Neural Networks (GNNs) to performance degradation due to data distribution shifts between training and inference phases. Inspired by this understanding, the authors propose a novel method called GCONDA to mitigate conditional shifts and improve GNN performance in unsupervised domain adaptation scenarios.\",\n  \"Direct Inspiration\": [\"b10\", \"b39\"],\n  \"Indirect Inspiration\": [\"b4\", \"b13\", \"b45\"],\n  \"Other Inspiration\": [\"b18\", \"b33\", \"b44\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performance degradation in Graph Neural Networks (GNNs) under distribution shift conditions. The authors propose a novel algorithm, GCONDA, to mitigate conditional shift in GNN applications, particularly in unsupervised domain adaptation (UDA) for node classification tasks.\",\n  \"Direct Inspiration\": [\"b18\", \"b45\"],\n  \"Indirect Inspiration\": [\"b20\", \"b38\", \"b33\", \"b4\", \"b13\"],\n  \"Other Inspiration\": [\"b10\", \"b39\", \"b40\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performance degradation in Graph Neural Networks (GNNs) under distribution shifts. The authors propose a novel graph conditional shift adaptation method (GCONDA) to mitigate conditional shifts in GNNs, which they demonstrate both theoretically and empirically.\",\n  \"Direct Inspiration\": [\"b4\", \"b13\", \"b45\"],\n  \"Indirect Inspiration\": [\"b10\", \"b39\"],\n  \"Other Inspiration\": [\"b18\", \"b20\", \"b33\", \"b40\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performance degradation of Graph Neural Networks (GNNs) under distribution shifts. It proposes a novel method, GCONDA, to mitigate conditional shifts in graph domain adaptation. The paper is inspired by previous works on domain invariant representation learning, conditional domain adaptation, and theoretical analysis of GNN performance under distribution shifts.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b39\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.85,\n    \"b13\": 0.9,\n    \"b24\": 0.8,\n    \"b40\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b45\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of distribution shift in Graph Neural Networks (GNNs), specifically focusing on the conditional shift that occurs in node classification tasks. The authors propose a novel graph conditional shift adaptation method called GCONDA to mitigate this shift using Wasserstein distance. The paper's contributions include theoretical derivations of conditional shift, empirical validation on synthetic and real datasets, and a demonstration of the effectiveness of GCONDA compared to existing methods.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1.0,\n    \"b18\": 0.9,\n    \"b10\": 0.85,\n    \"b39\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b13\": 0.7,\n    \"b40\": 0.65,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.55,\n    \"b34\": 0.5,\n    \"b41\": 0.45,\n    \"b24\": 0.4,\n    \"b43\": 0.35\n  }\n}\n```"], "6385788590e50fcafdf49aef": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a versatile Named Entity Recognition (NER) model capable of handling multiple domains and a large number of entity types simultaneously. The proposed model, PUnifiedNER, is built upon the T5 language model and uses prompting to guide the model in recognizing different entity types based on user interest. The model is trained on eight different datasets and shows improved performance over single dataset-trained models.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.9,\n    \"b6\": 0.8,\n    \"b21\": 0.8,\n    \"b13\": 0.8,\n    \"b10\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a versatile Named Entity Recognition (NER) model that can handle multiple domains and entity types simultaneously. The proposed solution is a prompting-based unified NER system (PUnifiedNER) built upon the T5 language model. It leverages prompt learning to train a single model on multiple datasets, allowing it to generate entity types of interest based on user prompts and achieve better performance compared to single-dataset models.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b41\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b13\": 0.75,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b30\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating a versatile NER model capable of handling multiple domains and a large number of entity types simultaneously. The proposed algorithm, PUnifiedNER, is based on the T5 language model and employs prompt learning to enhance the versatility and performance of the model across various NER datasets.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.95,\n    \"b8\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b41\": 0.85,\n    \"b13\": 0.75,\n    \"b6\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.65,\n    \"b34\": 0.65,\n    \"b30\": 0.6,\n    \"b19\": 0.6,\n    \"b24\": 0.6,\n    \"b18\": 0.6,\n    \"b9\": 0.55,\n    \"b7\": 0.55,\n    \"b39\": 0.55,\n    \"b29\": 0.55,\n    \"b3\": 0.5,\n    \"b38\": 0.5,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of building a versatile Named Entity Recognition (NER) model capable of handling multiple domains and entity types simultaneously. The proposed solution, PUnifiedNER, leverages prompt learning and the T5 language model to achieve better performance through joint training on multiple datasets. This approach aims to overcome the limitations of dataset-specific models by exploiting shared semantic information across different corpora.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0,\n    \"b41\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b13\": 0.6,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a versatile Named Entity Recognition (NER) model capable of handling multiple domains and a large number of entity types simultaneously. The proposed solution, PUnifiedNER, is based on the T5 language model and employs prompt learning to enable on-demand entity recognition.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b41\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.6,\n    \"b10\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```"], "6391890790e50fcafd2b45f2": ["```json\n{\n  \"Summary\": \"The paper discusses the challenges in distributed GNN training, particularly the inefficiencies in the sampling phase due to high network communication overhead and low CPU utilization. It proposes ByteGNN, a framework that uses a DAG-based task scheduling and an effective graph partitioning algorithm to improve sampling efficiency and overall training throughput.\",\n  \"Direct Inspiration\": {\n    \"b78\": 0.95,\n    \"b76\": 0.90,\n    \"b44\": 0.85,\n    \"b29\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b75\": 0.70,\n    \"b11\": 0.65,\n    \"b32\": 0.60\n  },\n  \"Other Inspiration\": {\n    \"b57\": 0.50,\n    \"b54\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces ByteGNN, a distributed GNN training framework designed to address inefficiencies in sampling and training large graph neural networks. It addresses challenges such as high network communication overhead, low CPU utilization, and insufficient GPU benefits by proposing a directed acyclic graph (DAG)-based task abstraction for sampling, two-level scheduling for balancing resource allocation, and a new graph partitioning algorithm tailored for GNN training.\",\n  \"Direct Inspiration\": [\n    \"b29\",\n    \"b44\",\n    \"b76\",\n    \"b78\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b11\",\n    \"b32\",\n    \"b57\",\n    \"b75\"\n  ],\n  \"Other Inspiration\": [\n    \"b30\",\n    \"b41\",\n    \"b54\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the inefficiencies in existing GNN training systems, particularly the imbalance in time taken for sampling versus training. It proposes ByteGNN, a distributed framework that uses a task-based DAG model for efficient sampling and dynamic resource allocation to minimize end-to-end training time. The paper also introduces a novel graph partitioning algorithm tailored for mini-batch sampling.\",\n    \"Direct Inspiration\": {\n        \"b0\": 0.9,\n        \"b29\": 0.85,\n        \"b76\": 0.8,\n        \"b78\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.7,\n        \"b44\": 0.7,\n        \"b57\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b32\": 0.6,\n        \"b34\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient distributed GNN training on large graphs, focusing on optimizing the sampling phase and balancing computation loads. The authors propose the ByteGNN framework, which includes a novel two-level scheduling strategy and a graph partitioning algorithm tailored for GNN training.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b76\": 0.9,\n    \"b78\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b44\": 0.7,\n    \"b57\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b32\": 0.6,\n    \"b41\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in distributed GNN training, particularly the inefficiency of the sampling phase, high network communication overhead, and low CPU utilization. It proposes ByteGNN, a novel distributed GNN training framework that models the sampling phase as a DAG of small tasks to enable parallel execution, and introduces two-level scheduling to balance resource utilization. ByteGNN also includes a graph partitioning algorithm tailored for mini-batch graph sampling to maintain data locality and balance computation loads.\",\n  \"Direct Inspiration\": {\n    \"b44\": 0.9,\n    \"b76\": 0.9,\n    \"b78\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.8,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b75\": 0.7\n  }\n}\n```"], "64893b17d68f896efa9826b7": ["```json\n{\n  \"Summary\": \"The paper presents WebGLM, a web-enhanced QA system that improves on WebGPT by being more efficient, cost-effective, and human preference-aware. Key components include an LLM-augmented retriever, a bootstrapped generator, and a human preference-aware scorer.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b7\": 0.8,\n    \"b6\": 0.7,\n    \"b15\": 0.6,\n    \"b11\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.4,\n    \"b14\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces WebGLM, a cost-effective and efficient web-enhanced QA system that addresses the limitations of WebGPT. It leverages LLMs for enhanced retrieval, generation, and scoring to deliver high-quality responses with human preference-awareness.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b15\": 0.7,\n    \"b13\": 0.7,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b4\": 0.5,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in creating an efficient and cost-effective web-enhanced question answering (QA) system, named WebGLM, that overcomes limitations of WebGPT and other existing systems. The system introduces a two-staged retriever for web search, a bootstrapped generator for high-quality citation-based data generation, and a human preference-aware scorer trained on user feedback from online QA forums.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b11\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b4\": 0.7,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include the high cost and inefficiency of current web-enhanced QA systems like WebGPT, which rely on expensive expert-level annotations and slow retrieval methods. The proposed WebGLM system aims to be efficient, cost-effective, and capable of human preference-aware scoring while maintaining comparable quality to WebGPT. Key innovations include an LLM-augmented retriever for efficient information retrieval, a bootstrapped generator for generating high-quality answers with proper citations, and a human preference-aware scorer trained on online QA forums.\",\n    \"Direct Inspiration\": {\n        \"references\": [\n            \"b23\"\n        ]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\n            \"b7\",\n            \"b9\",\n            \"b11\",\n            \"b15\"\n        ]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\n            \"b5\",\n            \"b6\"\n        ]\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include the limitations of existing LLMs in handling tasks requiring rare-knowledge and the inefficiency and high cost of current web-enhanced QA systems such as WebGPT. The algorithm proposed by the authors, WebGLM, introduces new strategies including an LLM-augmented Retriever, a Bootstrapped Generator, and a Human Preference-aware Scorer to create an efficient and cost-effective web-enhanced QA system.\",\n    \"Direct Inspiration\": {\n        \"b23\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.8,\n        \"b7\": 0.6,\n        \"b15\": 0.5,\n        \"b11\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.4,\n        \"b5\": 0.4\n    }\n}\n```"], "646aeca9d68f896efa05a572": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of implicit sentiment analysis (ISA), which involves detecting sentiment polarity from texts that do not contain explicit emotional expressions. The authors propose a Three-hop Reasoning CoT framework (THOR) based on large-scale language models (LLMs) to tackle ISA through multi-hop reasoning and common-sense understanding.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b33\", \"b36\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b19\", \"b16\", \"b30\", \"b35\", \"b34\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": []\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of implicit sentiment analysis (ISA) where inputs contain factual descriptions without explicit opinion expressions, making it difficult for traditional sentiment analysis methods to infer sentiment polarity. The authors propose the THOR framework, a three-hop reasoning chain-of-thought (CoT) method using large-scale language models (LLMs) to perform incremental reasoning steps for better sentiment prediction.\",\n    \"Direct Inspiration\": {\n        \"b33\": 1.0,\n        \"b36\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.8,\n        \"b16\": 0.8,\n        \"b30\": 0.8,\n        \"b35\": 0.8,\n        \"b34\": 0.8\n    },\n    \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of implicit sentiment analysis (ISA), where sentiment polarity towards a target is not explicitly stated in the text. The authors propose a novel three-hop reasoning framework (THOR) based on large-scale pretrained language models (LLMs) to handle ISA by utilizing common-sense and multi-hop reasoning. The THOR framework employs a chain-of-thought (CoT) prompting mechanism to perform incremental reasoning steps, aiming to infer fine-grained aspects, underlying opinions, and final sentiment polarity with improved accuracy over traditional methods.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b36\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b16\": 0.8,\n    \"b30\": 0.7,\n    \"b35\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of implicit sentiment analysis (ISA), which requires detecting sentiment polarity in texts where emotional expressions are not explicit. The authors propose a Three-hop Reasoning CoT framework (THOR) based on large-scale language models (LLMs) to infer sentiment through incremental reasoning steps. The framework leverages common-sense and multi-hop reasoning capabilities of LLMs to achieve accurate ISA, demonstrating significant improvements over traditional methods.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b36\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b16\": 0.7,\n    \"b30\": 0.6,\n    \"b35\": 0.6,\n    \"b34\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of implicit sentiment analysis (ISA), which involves detecting sentiment polarity in texts without explicit opinion expressions. The authors propose a Three-hop Reasoning CoT framework (THOR) that leverages large-scale language models (LLMs) for multi-hop and common-sense reasoning. The framework involves three reasoning steps to infer the fine-grained aspect, underlying opinion, and final polarity. This method aims to improve the accuracy of ISA by utilizing the world knowledge embedded in LLMs and the self-consistency mechanism inspired by previous works.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b16\": 0.8,\n    \"b30\": 0.8,\n    \"b35\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.7\n  }\n}\n```"], "64741c33d68f896efaa7b708": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of aligning pre-trained language models (LMs) with human social norms, termed 'social alignment.' The authors introduce SANDBOX, a simulated human society where LMs interact to learn social alignment through feedback and iterative refinements. They propose a novel algorithm, Stable Alignment, which uses contrastive learning to enhance alignment without the need for a separate reward model.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b6\": 0.85,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b53\": 0.7,\n    \"b52\": 0.65,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.55,\n    \"b45\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of aligning language models (LMs) with human social norms and values by introducing a novel alignment algorithm called Stable Alignment. The authors propose SANDBOX, a simulated human society, to facilitate social interactions among LM-based agents. This approach aims to overcome limitations of traditional methods like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), which are prone to adversarial attacks and reward gaming.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b12\": 0.65,\n    \"b21\": 0.7,\n    \"b23\": 0.65,\n    \"b27\": 0.7,\n    \"b52\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b4\": 0.55,\n    \"b6\": 0.55,\n    \"b19\": 0.6,\n    \"b33\": 0.6,\n    \"b36\": 0.6,\n    \"b43\": 0.55,\n    \"b44\": 0.55,\n    \"b45\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of aligning AI language models with social norms by simulating social interactions through a novel platform called SANDBOX. It introduces a new alignment algorithm called Stable Alignment, which leverages feedback from simulated interactions to improve the models' alignment and robustness against adversarial attacks.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b51\": 0.9,\n    \"b15\": 0.9,\n    \"b7\": 0.9,\n    \"b49\": 0.9,\n    \"b31\": 0.9,\n    \"b45\": 0.9,\n    \"b28\": 0.9,\n    \"b1\": 0.9,\n    \"b53\": 0.9,\n    \"b27\": 0.9,\n    \"b10\": 0.9,\n    \"b52\": 0.9,\n    \"b23\": 0.9,\n    \"b26\": 0.9,\n    \"b34\": 0.9,\n    \"b11\": 0.9,\n    \"b21\": 0.9,\n    \"b44\": 0.9,\n    \"b24\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b47\": 0.8,\n    \"b3\": 0.8,\n    \"b19\": 0.8,\n    \"b36\": 0.8,\n    \"b0\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b35\": 0.7,\n    \"b20\": 0.7,\n    \"b16\": 0.7,\n    \"b30\": 0.7,\n    \"b17\": 0.7,\n    \"b33\": 0.7,\n    \"b4\": 0.7,\n    \"b6\": 0.7,\n    \"b46\": 0.7,\n    \"b32\": 0.7,\n    \"b22\": 0.7,\n    \"b14\": 0.7,\n    \"b40\": 0.7,\n    \"b41\": 0.7,\n    \"b18\": 0.7,\n    \"b42\": 0.7,\n    \"b43\": 0.7,\n    \"b38\": 0.7,\n    \"b13\": 0.7,\n    \"b9\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of current pre-trained language models (LMs) in social alignment and proposes a novel alignment learning paradigm using simulated social interactions. The key contributions include the introduction of the SANDBOX platform for simulating human society and the Stable Alignment algorithm for learning from these interactions.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b8\", \"b15\", \"b27\", \"b10\", \"b52\", \"b21\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b12\", \"b47\", \"b1\", \"b24\", \"b25\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b7\", \"b49\", \"b31\", \"b45\", \"b28\", \"b53\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the limitations of current pre-trained language models (LMs) in social alignment, the susceptibility of models to adversarial attacks, and the inherent imperfection of reward models that can lead to reward gaming or tampering. The paper proposes a novel alignment learning paradigm called Stable Alignment, which enables LMs to learn from simulated social interactions within a platform named SANDBOX. This approach aims to address the shortcomings of existing methods such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF).\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b10\": 1.0,\n    \"b52\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b12\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```"], "629b0af15aee126c0fbc9a00": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in integrating fragmented biomedical knowledge across various organizational scales to support precision medicine. The proposed method, PrimeKG, is a comprehensive and multimodal knowledge graph that integrates 20 high-quality resources to improve disease coverage and support AI analyses for drug-disease interactions.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.95,\n    \"b43\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.85,\n    \"b29\": 0.85,\n    \"b44\": 0.80,\n    \"b45\": 0.80,\n    \"b47\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.75,\n    \"b30\": 0.75,\n    \"b32\": 0.75,\n    \"b33\": 0.75,\n    \"b34\": 0.75,\n    \"b48\": 0.75,\n    \"b54\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating a comprehensive, multimodal knowledge graph, PrimeKG, for precision medicine. It aims to unify diverse biomedical datasets to support disease research and drug repurposing. The PrimeKG integrates multiple high-quality resources, offers extensive disease coverage, and includes clinically meaningful descriptions for drugs and diseases.\",\n  \"Direct Inspiration\": [\"b32\", \"b33\", \"b34\", \"b37\"],\n  \"Indirect Inspiration\": [\"b36\", \"b38\", \"b41\", \"b59\", \"b60\", \"b61\"],\n  \"Other Inspiration\": [\"b28\", \"b29\", \"b30\", \"b39\", \"b62\", \"b63\", \"b64\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the fragmentation and heterogeneity of biomedical knowledge across different organizational scales and datasets, which hinders the development of comprehensive resources for precision medicine. The proposed solution, PrimeKG, integrates 20 high-quality resources to create a holistic and multimodal knowledge graph capturing extensive disease-related information. PrimeKG aims to improve coverage of diseases, support artificial intelligence analyses, and address disease entity resolution challenges.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9,\n    \"b33\": 0.8,\n    \"b64\": 0.8,\n    \"b43\": 0.85,\n    \"b44\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.7,\n    \"b29\": 0.7,\n    \"b42\": 0.7,\n    \"b47\": 0.75,\n    \"b48\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b55\": 0.6,\n    \"b56\": 0.6,\n    \"b100\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in integrating fragmented biomedical knowledge across different scales and datasets to support precision medicine. It proposes the development of a comprehensive knowledge graph, PrimeKG, which consolidates data from various high-quality resources to provide a holistic view of diseases and their relationships with drugs, phenotypes, and other biomedical entities. This knowledge graph aims to improve disease coverage, support AI analyses, and enable multimodal analyses by resolving disease entity conflicts and enhancing disease representation.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b29\": 0.9,\n    \"b37\": 0.95,\n    \"b33\": 0.85,\n    \"b64\": 0.9,\n    \"b43\": 0.9,\n    \"b44\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b63\": 0.8,\n    \"b41\": 0.75,\n    \"b36\": 0.75,\n    \"b27\": 0.75,\n    \"b30\": 0.75,\n    \"b39\": 0.75,\n    \"b62\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.7,\n    \"b47\": 0.7,\n    \"b45\": 0.7,\n    \"b50\": 0.7,\n    \"b52\": 0.7,\n    \"b54\": 0.7,\n    \"b55\": 0.7,\n    \"b56\": 0.7,\n    \"b58\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in precision medicine related to the fragmentation of biomedical knowledge across various organizational scales. The authors propose the development of a comprehensive and multimodal knowledge graph, PrimeKG, which integrates 20 high-quality resources to cover 129,375 nodes and 4,050,249 relationships across ten major biological scales. The key challenges include the need for expert curation in network analysis, lack of consistent disease representation, and the medical ambiguity of disease definitions. PrimeKG aims to unify biomedical knowledge and support AI analyses in precision medicine.\",\n  \"Direct Inspiration\": [\"b32\", \"b37\", \"b38\", \"b41\"],\n  \"Indirect Inspiration\": [\"b27\", \"b28\", \"b29\", \"b33\", \"b39\"],\n  \"Other Inspiration\": [\"b63\", \"b64\"]\n}\n```"], "63e1c14790e50fcafd2dd585": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of grounding Transformer-based Large Language Models (LLMs) in interactive environments using Reinforcement Learning (RL). It aims to achieve functional grounding through incremental online RL, enabling LLMs to adapt, generalize to new objects and tasks, and improve sample efficiency and robustness against distractors.\",\n  \"Direct Inspiration\": {\n    \"b45\": 0.9,\n    \"b36\": 0.8,\n    \"b39\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b49\": 0.7,\n    \"b43\": 0.65,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.5,\n    \"b1\": 0.55,\n    \"b52\": 0.4,\n    \"b14\": 0.35,\n    \"b41\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of functionally grounding large language models (LLMs) in interactive textual environments using reinforcement learning (RL). It aims to empirically answer questions about sample efficiency, generalization to new objects and tasks, and the impact of online interventions. The proposed solution involves using an LLM as an agent policy in a textual RL environment, specifically a text-only version of the BabyAI platform. The method leverages online RL finetuning to improve the LLM's functional grounding based on collected observations and rewards.\",\n  \n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b36\": 1,\n    \"b39\": 1,\n    \"b45\": 1\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b24\": 0.8,\n    \"b30\": 0.8,\n    \"b49\": 0.8\n  },\n  \n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b43\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper tackles the problem of functional grounding for Transformer-based Large Language Models (LLMs) in interactive textual environments using Reinforcement Learning (RL). The key challenges addressed include sample efficiency, generalization to new objects and tasks, and the impact of online interventions. The proposed algorithm uses PPO to finetune LLMs in the BabyAI-Text environment, focusing on grounding through incremental online RL.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1.0,\n    \"b36\": 1.0,\n    \"b39\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.8,\n    \"b24\": 0.8,\n    \"b49\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b43\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper include the lack of grounding in LLMs, which impacts their functional competence in interactive environments. The proposed algorithm focuses on functional grounding through incremental online RL, using the BabyAI-Text environment to study sample efficiency, generalization to new objects and tasks, and the impact of online interventions.\",\n  \"Direct Inspiration\": [\"b45\", \"b36\", \"b39\", \"b10\"],\n  \"Indirect Inspiration\": [\"b49\", \"b24\", \"b30\", \"b52\"],\n  \"Other Inspiration\": [\"b1\", \"b43\", \"b25\", \"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper aims to address the lack of functional grounding in Transformer-based Large Language Models (LLMs) by proposing a method to use LLMs in interactive environments through online Reinforcement Learning (RL). The main challenges include improving sample efficiency, generalization to new objects and tasks, and assessing the impact of online interventions compared to offline methods.\",\n  \"Direct Inspiration\": [\"b1\", \"b24\", \"b30\", \"b36\", \"b39\", \"b45\"],\n  \"Indirect Inspiration\": [\"b10\", \"b49\"],\n  \"Other Inspiration\": [\"b43\"]\n}\n```"], "64927546d68f896efa88a31b": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the inefficiency and overfitting of existing graph structure learning models under a closed-world hypothesis. The paper proposes 'GraphGLOW,' an open-world graph structure learning model that generalizes across multiple source graphs and can directly adapt to new target graphs without retraining. The approach involves a bi-level optimization framework, variational inference for learning objectives, and multi-head weighted similarity functions to capture diverse structural information.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b39\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b7\": 0.8,\n    \"b9\": 0.8,\n    \"b20\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b16\": 0.7,\n    \"b27\": 0.7,\n    \"b32\": 0.7,\n    \"b33\": 0.7,\n    \"b37\": 0.7,\n    \"b41\": 0.7,\n    \"b42\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing graph neural networks (GNNs) in the presence of noisy data and proposes a novel Open-World Graph Structure Learning (GLOW) model. The key contribution is a bi-level optimization framework that trains a generalizable graph structure learner to adapt to new, unseen graphs without re-training. The model relies on variational inference and multi-head weighted similarity functions to enhance the generalization capability.\",\n  \"Direct Inspiration\": [\"b5\", \"b7\", \"b9\", \"b26\", \"b39\"],\n  \"Indirect Inspiration\": [\"b8\", \"b40\", \"b41\"],\n  \"Other Inspiration\": [\"b12\", \"b17\", \"b33\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the inefficiencies and overfitting issues in existing Graph Structure Learning (GSL) models when applied to new, unseen graph datasets. The proposed model, GraphGLOW, aims to generalize GSL to an open-world setting by training a universal structure learner on multiple source graphs, which can then be applied to target graphs without retraining. The approach leverages variational inference and multi-head weighted similarity functions to reduce computational complexity while ensuring expressivity.\",\n    \"Direct Inspiration\": {\n        \"b26\": 0.9,\n        \"b39\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b7\": 0.8,\n        \"b9\": 0.8,\n        \"b20\": 0.7,\n        \"b40\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.6,\n        \"b37\": 0.6,\n        \"b41\": 0.6,\n        \"b42\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Open-World Graph Structure Learning by proposing GraphGLOW, a model that generalizes graph structure learning across multiple source graphs to new unseen target graphs without re-training. The main contributions include a bi-level optimization framework, a probabilistic model formulation using variational inference, and a multi-head weighted similarity function to enhance expressivity and reduce computational overhead.\",\n  \"Direct Inspiration\": [\"b26\", \"b39\"],\n  \"Indirect Inspiration\": [\"b5\", \"b7\", \"b9\", \"b8\"],\n  \"Other Inspiration\": [\"b12\", \"b17\", \"b33\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving Graph Neural Networks (GNNs) performance by mitigating the issue of spurious and unobserved edges in the input graph data. The novel contribution is the introduction of an Open-World Graph Structure Learning model named GraphGLOW, which generalizes graph structure learning to new unseen graphs without re-training or fine-tuning. The paper formulates the problem as a bi-level optimization task and employs variational inference to derive a feasible learning objective. Key methods include a multi-head weighted similarity function for structure learning and a pivot-based approach to reduce computational complexity.\",\n    \"Direct Inspiration\": {\n        \"b26\": 0.9,\n        \"b39\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b7\": 0.75,\n        \"b9\": 0.7,\n        \"b20\": 0.65,\n        \"b40\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.6,\n        \"b41\": 0.55\n    }\n}\n```"], "6482a38ed68f896efa8db612": ["```json\n{\n  \"Summary\": \"The paper presents MUSICGEN, a model for generating high-quality music from textual descriptions, addressing challenges such as modeling long sequences, harmonies, and diverse control methods. The proposed method leverages advancements in audio representation learning, sequential modeling, and audio synthesis, introducing a novel framework for modeling parallel streams of acoustic tokens and unsupervised melody conditioning.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b9\": 0.9,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces MUSICGEN, a model for generating high-quality music from textual descriptions. The key challenges addressed include modeling long-range sequences, harmonies, and melodies in music, and ensuring controllability of the generated music. The paper proposes a novel framework for modeling multiple parallel streams of acoustic tokens, and introduces unsupervised melody conditioning for better harmonic and melodic structure alignment.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b9\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces MUSICGEN, a model for text-to-music generation. It addresses challenges in generating high-quality music from text descriptions by proposing a framework for modeling multiple parallel streams of acoustic tokens and improving controllability through unsupervised melody conditioning. The model utilizes an autoregressive transformer-based decoder and is evaluated against various baselines, showing superior performance in both objective and human evaluations.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b7\": 1.0,\n    \"b9\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality music from textual descriptions. The proposed model, MUSICGEN, introduces a framework for modeling multiple parallel streams of acoustic tokens and introduces unsupervised melody conditioning to enhance controllability. The paper evaluates MUSICGEN extensively, showing superior performance compared to baselines and highlighting the importance of various components through ablation studies.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b9\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b10\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality music from text descriptions, emphasizing the complexity due to the need for high sampling rates, handling harmonies/melodies, and providing control over musical attributes. The proposed solution, MUSICGEN, introduces a novel framework for modeling multiple parallel streams of acoustic tokens, leveraging unsupervised melody conditioning to improve controllability and quality. The paper evaluates the model extensively against baselines, showing significant improvements.\",\n  \"Direct Inspiration\": [\"b6\", \"b7\", \"b9\", \"b11\"],\n  \"Indirect Inspiration\": [\"b8\"],\n  \"Other Inspiration\": [\"b12\", \"b13\"]\n}\n```"], "6433f67f90e50fcafd6db326": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of unifying various Information Extraction (IE) tasks, such as named entity recognition (NER), relation extraction (RE), and event extraction (EE), into a single model. The proposed solution is a novel paradigm called Unified Token-pair Classification (UTC-IE) which simplifies IE tasks into token-pair classification tasks. The key innovation is the Plusformer module, which leverages plus-shaped self-attention and convolutional neural networks (CNN) to model interactions between token pairs. This approach aims to enhance performance and efficiency compared to previous graph-based and generative methods.\",\n    \"Direct Inspiration\": {\n        \"b11\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b34\": 0.8,\n        \"b32\": 0.7,\n        \"b1\": 0.6,\n        \"b2\": 0.6,\n        \"b36\": 0.6,\n        \"b12\": 0.6\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unifying various Information Extraction (IE) tasks (NER, RE, and EE) into a single model to facilitate knowledge sharing and improve efficiency. The authors propose a novel paradigm that simplifies and unifies all IE tasks into token-pair classification tasks. They introduce a new architecture called Unified Token-pair Classification for Information Extraction (UTC-IE), which includes a Plusformer module to model interactions between token pairs, leveraging Biaffine model and CNN for local dependencies.\",\n  \"Direct Inspiration\": [\"b11\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b34\", \"b32\", \"b1\", \"b2\", \"b36\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unifying various Information Extraction (IE) tasks such as named entity recognition, relation extraction, and event extraction into a single model to facilitate knowledge sharing and improve efficiency. The proposed solution is a Unified Token-pair Classification architecture (UTC-IE), which decomposes IE tasks into token-pair classifications and utilizes a novel Plusformer structure for modeling interactions between token pairs.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b21\": 0.7,\n    \"b25\": 0.8,\n    \"b26\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.4,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the unification of Information Extraction (IE) tasks without the need for dedicated modules for each task. The proposed solution is a novel paradigm called Unified Token-pair Classification architecture for Information Extraction (UTC-IE), which simplifies all IE tasks into token-pair classification tasks. This architecture leverages a Biaffine model, a Plusformer module for interaction modeling, and a CNN layer for local dependency exploitation.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unifying various Information Extraction (IE) tasks into a single model to facilitate knowledge sharing between tasks. It introduces a novel paradigm that reformulates all IE tasks into token-pair classification tasks, simplifying the model structure. The proposed method, UTC-IE, leverages a Plusformer architecture to model interactions between token pairs using a combination of Biaffine mechanism, self-attention, and CNN.\",\n  \"Direct Inspiration\": [\"b11\"],\n  \"Indirect Inspiration\": [\"b13\", \"b16\", \"b18\", \"b34\"],\n  \"Other Inspiration\": [\"b24\", \"b10\", \"b46\", \"b43\"]\n}\n```"], "64a29612d68f896efa28bcf5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of performance tuning in heterogeneous computing environments by proposing Time-proportional Event Analysis (TEA). TEA creates time-proportional Per-Instruction Cycle Stacks (PICS) to answer both which instructions are performance-critical and why they are performance-critical. This is achieved by tracking key performance events across all in-flight instructions using Performance Signature Vectors (PSVs). TEA is implemented within the Berkeley Out-of-Order Machine (BOOM) core and demonstrates significant accuracy and low overhead compared to state-of-the-art approaches like AMD IBS, Arm SPE, and IBM RIS.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b3\": 0.8,\n    \"b28\": 0.8,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges posed by the end of Dennard scaling and Moore's law, highlighting the critical need for performance tuning of sequential CPU code in heterogeneous systems. The proposed Time-proportional Event Analysis (TEA) algorithm creates time-proportional Per-Instruction Cycle Stacks (PICS) to explain both which instructions are performance-critical (Q1) and why (Q2). TEA achieves this by tracking key performance events for all in-flight instructions using Performance Signature Vectors (PSVs). The implementation of TEA within the Berkeley Out-of-Order Machine (BOOM) core demonstrates its low overhead and high accuracy in performance analysis.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b3\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b57\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of performance tuning for sequential CPU code in heterogeneous systems. It introduces a new algorithm, Time-proportional Event Analysis (TEA), which creates time-proportional Per-Instruction Cycle Stacks (PICS) to explain both which instructions are performance-critical and why they are performance-critical.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b3\": 0.8,\n    \"b28\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b57\": 0.7,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing sequential CPU code performance in the context of heterogeneous systems where performance tuning is critical. The authors propose a novel Time-proportional Event Analysis (TEA) algorithm to create time-proportional Per-Instruction Cycle Stacks (PICS) that break down the execution time of each instruction into performance events. This approach aims to answer two fundamental questions: which instructions are performance-critical and why they are performance-critical.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b3\": 0.8,\n    \"b28\": 0.8,\n    \"b29\": 0.8,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b1\": 0.6,\n    \"b15\": 0.5,\n    \"b27\": 0.5,\n    \"b31\": 0.5,\n    \"b57\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in performance tuning for sequential CPU code in heterogeneous systems, specifically focusing on instruction-level performance analysis. It proposes Time-proportional Event Analysis (TEA) to accurately create Per-Instruction Cycle Stacks (PICS) that answer both which instructions are performance-critical (Q1) and why (Q2). TEA tracks key performance events with minimal overhead and demonstrates significant accuracy improvements over existing methods like AMD IBS, Arm SPE, and IBM RIS.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b3\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b57\": 0.7,\n    \"b29\": 0.6,\n    \"b45\": 0.5,\n    \"b37\": 0.5\n  }\n}\n```"], "646c3addd68f896efa5d1805": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying knowledge distillation when training datasets are unavailable due to security and privacy concerns. It proposes leveraging synthetic images generated by state-of-the-art diffusion models for knowledge distillation. The key findings include the counterintuitive observation that relatively weak classifiers serve as better teachers on synthetic datasets.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b14\": 1.0,\n    \"b33\": 1.0,\n    \"b62\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b2\": 0.8,\n    \"b45\": 0.8,\n    \"b39\": 0.8,\n    \"b49\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b7\": 0.7,\n    \"b37\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of knowledge distillation without access to real training data, proposing a method that utilizes synthetic images generated by state-of-the-art diffusion models. The main inspiration is derived from previous works on knowledge distillation and diffusion models, leveraging their capabilities to generate high-fidelity images for downstream tasks. The paper also finds that relatively weak classifiers serve as better teachers in synthetic datasets.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1,\n    \"b14\": 1,\n    \"b20\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b19\": 0.8,\n    \"b33\": 0.8,\n    \"b62\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b39\": 0.6,\n    \"b45\": 0.6,\n    \"b49\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of knowledge distillation without access to real training data due to privacy and security concerns. It proposes using synthetic images generated by state-of-the-art diffusion models (DiT, GLIDE, Stable Diffusion) for distillation. The key findings include the observation that relatively weak classifiers serve as better teachers on synthetic datasets.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b14\": 0.9,\n        \"b33\": 0.8,\n        \"b62\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b21\": 0.7,\n        \"b39\": 0.7,\n        \"b45\": 0.7,\n        \"b49\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b19\": 0.6,\n        \"b20\": 0.6,\n        \"b22\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenge of performing knowledge distillation when real training datasets are not available due to privacy concerns. The proposed method leverages state-of-the-art conditional diffusion models to generate synthetic images as proxy training datasets. The study identifies that relatively weak classifiers serve as better teacher models for knowledge distillation on synthetic datasets.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b14\": 0.9,\n    \"b33\": 0.8,\n    \"b62\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b20\": 0.7,\n    \"b45\": 0.8,\n    \"b49\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.65,\n    \"b39\": 0.7,\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b51\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of knowledge distillation when training datasets are unavailable due to security and privacy concerns. It proposes using synthetic images generated by state-of-the-art diffusion models instead of real images for knowledge distillation. The key findings include the observation that relatively weak classifiers are better teachers on synthetic datasets.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.7,\n        \"b14\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b39\": 0.65,\n        \"b45\": 0.65,\n        \"b49\": 0.65,\n        \"b62\": 0.6\n    }\n}\n```"], "635753d490e50fcafdddf4bd": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of lossy audio compression using neural networks. The primary challenges include representing a wide range of signals without overfitting and efficiently compressing audio in terms of both compute time and size. The proposed algorithm, EnCodec, combines an encoder-decoder architecture with residual vector quantization and adversarial perceptual losses to achieve state-of-the-art compression performance. Extensive human evaluations are conducted to validate the model's effectiveness.\",\n  \"Direct Inspiration\": [\"b74\"],\n  \"Indirect Inspiration\": [\"b16\", \"b43\", \"b67\"],\n  \"Other Inspiration\": [\"b31\", \"b32\", \"b40\", \"b45\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of efficient and high-quality lossy audio compression using neural networks. It proposes the EnCodec, a convolutional-based encoder-decoder architecture with residual vector quantization and perceptual loss mechanisms. The model ensures real-time performance and supports various bandwidths while maintaining high audio quality through extensive human and objective evaluations.\",\n  \"Direct Inspiration\": {\n    \"b74\": 1.0,\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b67\": 0.8,\n    \"b21\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b64\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper are (1) representing a wide range of audio signals without overfitting or producing artifacts and (2) compressing audio efficiently in terms of both compute time and size.\",\n    \"algorithm\": \"The proposed algorithm, EnCodec, employs a neural encoder-decoder architecture with residual vector quantization, adversarial losses, and optionally a small Transformer language model for entropy coding.\"\n  },\n  \"Direct Inspiration\": {\n    \"b74\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b49\": 0.8,\n    \"b58\": 0.7,\n    \"b67\": 0.7,\n    \"b64\": 0.6,\n    \"b43\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficient audio compression with minimal distortion, leveraging neural networks for encoding and decoding. It introduces EnCodec, a neural-based audio codec that utilizes a convolutional encoder-decoder architecture, residual vector quantization, and perceptual loss via discriminator networks. The paper highlights the importance of large, diverse datasets and efficient compression methods, aiming for state-of-the-art performance in both speech and music compression across various bit rates and sample rates.\",\n    \"Direct Inspiration\": [\"b74\"],\n    \"Indirect Inspiration\": [\"b64\", \"b43\", \"b21\", \"b31\", \"b32\"],\n    \"Other Inspiration\": [\"b16\", \"b17\", \"b40\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of lossy audio compression using neural networks. It proposes the EnCodec model, an encoder-decoder architecture with residual vector quantization and discriminative perceptual losses, aimed at minimizing bitrate while maintaining audio quality. Key contributions include the use of a large and diverse training set, discriminator networks for perceptual losses, real-time processing on a single CPU core, and variable bandwidth training.\",\n  \"Direct Inspiration\": {\n    \"b74\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b67\": 0.8,\n    \"b64\": 0.7,\n    \"b43\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b21\": 0.4,\n    \"b49\": 0.3,\n    \"b58\": 0.2\n  }\n}\n```"], "649a5e2ad68f896efad8460f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of weakly supervised multi-label text classification of full-text scientific papers, focusing on large and fine-grained label spaces and the use of full texts instead of just abstracts. The proposed FuTex framework leverages cross-paper network structures and in-paper hierarchical structures to improve classification performance.\",\n  \"Direct Inspiration\": {\n    \"b68\": 1.0,\n    \"b54\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.5,\n    \"b46\": 0.45\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of weakly supervised multi-label text classification in large and fine-grained label spaces using full-text scientific papers. It proposes a new framework, FuTex, which leverages cross-paper network structures and in-paper hierarchy structures to enhance classification performance.\",\n    \"Direct Inspiration\": {\n        \"b68\": 1,\n        \"b54\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8,\n        \"b2\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b39\": 0.6,\n        \"b10\": 0.5,\n        \"b53\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are (1) handling a large and fine-grained label space for scientific paper classification, which requires multi-label capabilities, and (2) effectively utilizing the full texts of papers, which are often lengthy and contain important information scattered throughout different sections. The paper proposes the FuTex framework, which leverages the cross-paper network structure and the in-paper hierarchy structure to improve classification performance. FuTex includes three main modules: network-aware contrastive fine-tuning, hierarchy-aware aggregation, and self-training to enhance classification results.\",\n  \"Direct Inspiration\": [\"b68\", \"b10\"],\n  \"Indirect Inspiration\": [\"b54\", \"b39\"],\n  \"Other Inspiration\": [\"b1\", \"b2\", \"b28\", \"b32\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges: (1) classifying scientific papers in a large and fine-grained label space, and (2) effectively utilizing the full text of papers for classification. The proposed solution, FuTex, leverages both cross-paper network structure and in-paper hierarchy structure to enhance weakly supervised multi-label text classification. FuTex includes three modules: network-aware contrastive fine-tuning, hierarchy-aware aggregation, and self-training.\",\n  \"Direct Inspiration\": {\n    \"b68\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b54\": 0.7,\n    \"b39\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b53\": 0.5,\n    \"b56\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Weakly supervised multi-label text classification of full-text scientific papers\",\n      \"Large and fine-grained label space\",\n      \"Dealing with long text and structural information in scientific papers\"\n    ],\n    \"proposed_algorithm\": \"FuTex framework using cross-paper network structure and in-paper hierarchy structure\"\n  },\n  \"Direct Inspiration\": [\"b68\", \"b10\", \"b54\"],\n  \"Indirect Inspiration\": [\"b1\", \"b2\", \"b39\"],\n  \"Other Inspiration\": []\n}\n```"], "6225978c5aee126c0f2d4a7c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of data locality optimization in shared-memory multicore architectures. It introduces ReuseTracker, a low-overhead reuse distance analysis tool for multi-threaded applications, leveraging hardware counters of performance monitoring units (PMUs) and debug registers. The tool improves accuracy in profiling reuse distances in threads and shared caches, with significantly lower overhead compared to existing tools.\",\n  \"Direct Inspiration\": {\n    \"b41\": 0.95,\n    \"b7\": 0.9,\n    \"b60\": 0.85,\n    \"b64\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b26\": 0.65,\n    \"b33\": 0.6,\n    \"b46\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b47\": 0.5,\n    \"b48\": 0.45,\n    \"b0\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is developing a fast and low-overhead reuse distance analysis tool for multi-threaded applications to optimize data locality in shared-memory multicore architectures. The proposed solution, ReuseTracker, leverages hardware counters (PMUs) and debug registers to sample and trap memory accesses, providing accurate profiling of reuse distances while minimizing performance and memory overheads. The tool aims to address the limitations of existing methods, such as high overheads and distortion of parallel schedules, and to offer an open-source solution that can handle multi-threaded codes and interactions across threads.\",\n  \"Direct Inspiration\": {\n    \"b41\": 0.95,\n    \"b7\": 0.90,\n    \"b60\": 0.85,\n    \"b64\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b20\": 0.70,\n    \"b26\": 0.70,\n    \"b33\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.65,\n    \"b29\": 0.65,\n    \"b36\": 0.65,\n    \"b47\": 0.60,\n    \"b48\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reuse distance analysis in multi-threaded applications. It proposes ReuseTracker, an open-source, low-overhead tool that leverages hardware counters and debug registers to profile reuse distance more accurately and efficiently compared to existing tools. ReuseTracker can detect reuses across different threads and shared caches, making it suitable for multi-threaded applications.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b0\", \"b41\", \"b60\", \"b64\"],\n    \"phrases\": [\"inspired by\", \"motivated by\", \"following.. we adopt ... to solve the challenge/problem\", \"we use... based on to achieve...\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b7\", \"b14\", \"b29\", \"b36\"],\n    \"phrases\": [\"the pioneering/previous work\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b20\", \"b26\", \"b33\", \"b46\", \"b47\", \"b48\", \"b59\"],\n    \"phrases\": [\"employ analytical models\", \"high overhead exists\", \"significant overheads and distortion\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately measuring reuse distances in multi-threaded applications with low overhead. It introduces ReuseTracker, an open-source tool that leverages hardware counters of performance monitoring units (PMUs) and debug registers to achieve its goals. The tool focuses on profiling reuse distances in shared caches and individual threads, aiming to optimize cache sizes and improve performance.\",\n  \"Direct Inspiration\": {\n    \"b41\": 1,\n    \"b60\": 1,\n    \"b64\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b47\": 0.7,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is improving data locality in shared-memory multicore architectures, focusing on reuse distance analysis for multi-threaded applications to reduce energy consumption and enhance performance. The proposed solution, ReuseTracker, leverages hardware counters of performance monitoring units (PMUs) and debug registers to create a low-overhead, accurate reuse distance analysis tool for multi-threaded applications, addressing shortcomings of existing tools such as high overhead and limited thread interaction analysis.\",\n  \"Direct Inspiration\": [\"b41\"],\n  \"Indirect Inspiration\": [\"b0\", \"b60\", \"b64\"],\n  \"Other Inspiration\": [\"b7\", \"b23\"]\n}\n```"], "6427029c90e50fcafd5d6c03": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is improving cache performance by considering both data locality and concurrency, rather than just locality. The paper introduces the concept of Pure Miss Contribution (PMC) as a new cost metric for cache misses and presents CARE, a concurrency-aware cache management framework. CARE uses PMC values to guide cache replacement decisions, aiming to improve performance by reserving performance-critical blocks and evicting less critical ones.\",\n  \"Direct Inspiration\": {\n    \"b43\": 1.0,\n    \"b33\": 0.9,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b47\": 0.6,\n    \"b52\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the memory wall problem by introducing Pure Miss Contribution (PMC), a novel metric to quantify the cost and performance impact of cache misses. The proposed algorithm, CARE, is a concurrency-aware cache management framework that considers both data locality and concurrency, using PMC to guide cache optimization. CARE aims to improve cache performance by learning re-reference behaviors and implementing a Dynamic Threshold Reconfiguration Mechanism (DTRM) for adaptability.\",\n  \"Direct Inspiration\": {\n    \"b43\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.8,\n    \"b33\": 0.8,\n    \"b47\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b51\": 0.5,\n    \"b52\": 0.5,\n    \"b41\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the memory wall problem by proposing a new metric, Pure Miss Contribution (PMC), to quantify the cost and performance impact of outstanding cache misses. It introduces CARE, a concurrency-aware cache management framework that considers both data locality and concurrency, and shows that CARE outperforms state-of-the-art cache management schemes.\",\n  \"Direct Inspiration\": {\n    \"b43\": 0.95,\n    \"b29\": 0.9,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b47\": 0.8,\n    \"b18\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b16\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the memory wall problem by introducing a novel metric called Pure Miss Contribution (PMC) to better quantify the cost and performance impact of cache misses, especially in the context of data concurrency and overlapping. It proposes the CARE framework to optimize cache management considering both data locality and concurrency, outperforming existing schemes with low overhead.\",\n  \"Direct Inspiration\": {\n    \"b43\": 1.0,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b47\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b44\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the memory wall problem by introducing Pure Miss Contribution (PMC) and presenting CARE, a concurrency-aware cache management framework. CARE improves cache management by considering both data locality and concurrency, using PMC to guide replacement decisions and incorporating a Dynamic Threshold Reconfiguration Mechanism (DTRM) for adaptability.\",\n  \"Direct Inspiration\": {\n    \"b43\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.9,\n    \"b47\": 0.85,\n    \"b29\": 0.8,\n    \"b52\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.75,\n    \"b40\": 0.7\n  }\n}\n```"], "629435a25aee126c0f2fecb3": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper are the runtime and memory complexity of Transformer models for long sequences due to the quadratic time and memory complexity of the self-attention module.\",\n    \"Algorithm\": \"The paper proposes FlashAttention, a new attention algorithm designed to be IO-aware, reducing reads and writes to different levels of memory. The key techniques applied include tiling and recomputation to avoid materializing the large attention matrix and achieve sub-quadratic HBM accesses.\"\n  },\n  \"Direct Inspiration\": {\n    \"b0\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b48\": 0.8,\n    \"b62\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b67\": 0.6,\n    \"b66\": 0.6,\n    \"b3\": 0.6,\n    \"b37\": 0.6,\n    \"b81\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is the inefficiency of self-attention mechanisms in Transformer models due to high time and memory complexity. The authors propose FlashAttention, an algorithm designed to minimize memory access overheads by being IO-aware. The key techniques include tiling and recomputation to avoid storing large intermediate matrices, leading to significant speed and memory improvements over standard attention implementations.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b76\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b7\": 0.8,\n    \"b48\": 0.85,\n    \"b70\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.75,\n    \"b31\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of making transformer models more memory-efficient and faster by proposing FlashAttention, a new attention algorithm that focuses on reducing memory accesses. The algorithm avoids reading and writing the large attention matrix to and from high bandwidth memory (HBM) by using techniques like tiling and recomputation, implemented in CUDA for fine-grained memory control.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b0\", \"b48\", \"b62\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b40\", \"b55\", \"b57\", \"b58\", \"b59\", \"b81\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b2\", \"b7\", \"b10\", \"b47\", \"b70\", \"b76\", \"b80\", \"b88\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces FlashAttention, an algorithm designed to make attention mechanisms in Transformer models more memory-efficient and faster for long sequences. The primary challenge addressed is the quadratic time and memory complexity of the self-attention module in Transformers, which becomes a bottleneck for long sequences. The proposed solution involves IO-aware algorithms that reduce memory access overheads through techniques like tiling and recomputation.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b76\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b48\": 0.7,\n    \"b10\": 0.6,\n    \"b47\": 0.6,\n    \"b80\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b7\": 0.5,\n    \"b88\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of making Transformer models more efficient in handling long sequences by proposing FlashAttention, an IO-aware attention algorithm designed to reduce memory accesses and speed up computation. The core contributions include the use of tiling and recomputation techniques to optimize memory usage and computation speed.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b48\": 0.9,\n    \"b76\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b7\": 0.8,\n    \"b10\": 0.8,\n    \"b47\": 0.8,\n    \"b80\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b31\": 0.7,\n    \"b37\": 0.7,\n    \"b81\": 0.7\n  }\n}\n```"], "64a78f10d68f896efa01dee3": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the high capital expenditure and energy cost of running large language models (LLMs) on commodity hardware, such as GPUs and TPUs. The proposed solution is the Chiplet Cloud, a chiplet-based ASIC AI-supercomputer architecture aimed at significantly reducing the total-cost-of-ownership (TCO) per token served. Key methods include storing model parameters and KV values in on-chip SRAM memory, and exploring hardware-software co-design to optimize performance and TCO.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b25\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.75,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.65,\n    \"b15\": 0.6,\n    \"b5\": 0.55,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of running large language models (LLMs) like GPT-3 on commodity hardware, which is inefficient and costly. The authors propose a chiplet-based ASIC AI-supercomputer architecture called Chiplet Cloud to reduce the total cost of ownership (TCO) per token served. The main contributions include a detailed study of current hardware limitations, a two-phase design-search methodology for hardware and software co-design, and specific architectural solutions to optimize TCO and performance.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1,\n        \"b22\": 0.9,\n        \"b25\": 0.9,\n        \"b31\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.85,\n        \"b4\": 0.8,\n        \"b10\": 0.75,\n        \"b13\": 0.7,\n        \"b15\": 0.7,\n        \"b37\": 0.7\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the high costs and energy consumption of running large language models (LLMs) on commodity hardware by proposing a specialized ASIC-based architecture called Chiplet Cloud. The challenges include scalability issues, high total-cost-of-ownership (TCO) per token, and inefficiencies of current hardware. The proposed solution aims to optimize TCO per performance through hardware-software co-design, on-chip SRAM memory, and specialized ASIC supercomputers.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b22\": 0.8,\n    \"b25\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b15\": 0.6,\n    \"b31\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the high capital expenditure and energy cost of running large language models (LLMs) like GPT-3 on current hardware platforms such as GPUs and TPUs. The proposed solution is a chiplet-based ASIC supercomputer architecture called Chiplet Cloud, which aims to significantly reduce the total-cost-of-ownership (TCO) per token served. The paper presents a detailed design and evaluation of this architecture, including considerations for on-chip SRAM memory, inter-chiplet communication, and a two-phase design-search methodology for hardware-software co-design.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.95,\n    \"b22\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.85,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.75,\n    \"b31\": 0.7,\n    \"b10\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the scalability and profitability challenges of running large generative language models (LLMs) on commodity hardware, such as GPUs, which are hitting a scalability wall due to high capital and operational expenditures. The authors propose 'Chiplet Cloud,' a chiplet-based ASIC AI-supercomputer architecture designed to significantly reduce the total-cost-of-ownership (TCO) per generated token. The novel contributions include the aggressive customization of Chiplet Cloud architecture for targeted LLMs, using on-chip SRAM memory, and implementing a two-phase design-search methodology for hardware and software co-design.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b3\": 0.95,\n    \"b4\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.85,\n    \"b15\": 0.8,\n    \"b22\": 0.8,\n    \"b31\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b8\": 0.7,\n    \"b10\": 0.7,\n    \"b20\": 0.7,\n    \"b24\": 0.7,\n    \"b37\": 0.65\n  }\n}\n```"], "6346305e90e50fcafda07ab8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of continually post-training language models (LMs) with new domains without suffering from catastrophic forgetting (CF) and proposes a system called CPT. This system uses a novel hard masking mechanism to handle CF and the catastrophic butterfly effect (CBE) during fine-tuning.\",\n  \"Direct Inspiration\": {\n    \"b49\": 1.0,\n    \"b20\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b33\": 0.8,\n    \"b61\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b44\": 0.7,\n    \"b59\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of continually post-training language models (LMs) to handle new and emerging domains without suffering from catastrophic forgetting (CF) or catastrophic butterfly effect (CBE). The proposed method, CPT, incrementally post-trains a pre-trained LM (like RoBERTa) with a sequence of domains using parameter isolation and hard masking mechanisms to prevent CF and CBE.\",\n  \"Direct Inspiration\": {\n    \"b49\": 1.0,\n    \"b20\": 0.9,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b61\": 0.7,\n    \"b59\": 0.6,\n    \"b37\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving a language model's (LM) ability to handle new and emerging domains without suffering from catastrophic forgetting (CF) and catastrophic butterfly effect (CBE). The proposed system, CPT, incrementally post-trains a pre-trained LM like RoBERTa with a sequence of domains using unlabeled corpora. A novel hard masking mechanism is introduced to prevent CF and CBE, allowing the model to be fine-tuned for end-tasks in the trained domains.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b21\": 0.9,\n    \"b49\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b44\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7,\n    \"b61\": 0.7,\n    \"b59\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of continually post-training language models (LMs) with new domains to improve their ability to handle emerging domains without suffering from catastrophic forgetting (CF) and catastrophic butterfly effect (CBE). The proposed approach, CPT, uses a hard masking mechanism in a continual learning (CL) system to prevent CF and ensure effective end-task fine-tuning.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b49\": 0.8,\n    \"b20\": 0.75,\n    \"b17\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b61\": 0.6,\n    \"b14\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b59\": 0.5,\n    \"b37\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving a language model's (LM) ability to handle new and emerging domains without suffering from catastrophic forgetting (CF) or catastrophic butterfly effect (CBE). The proposed system, CPT, continually post-trains a pre-trained LM like RoBERTa using a sequence of domains. It introduces a novel hard masking mechanism in CL-plugins to prevent CF and CBE.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b49\": 0.85,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b33\": 0.7,\n    \"b61\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b59\": 0.6,\n    \"b50\": 0.55,\n    \"b44\": 0.55\n  }\n}\n```"], "6327dda690e50fcafd67dea3": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the lack of language models specifically pre-trained for user-generated text on Twitter, which contains informal diction, abbreviations, emojis, and hashtags. The paper introduces TwHIN-BERT, a multilingual language model for Twitter, leveraging socially similar Tweets for pre-training. The model constructs a Twitter Heterogeneous Information Network (TwHIN) to unify user engagement logs and introduces a contrastive social objective alongside masked language modeling. The model demonstrates state-of-the-art performance in social recommendation and semantic understanding tasks.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b41\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b10\": 0.7,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of pre-training a language model specifically for Twitter, leveraging social engagements to enhance Tweet content understanding. The proposed TwHIN-BERT model constructs a Twitter Heterogeneous Information Network (TwHIN) to mine socially similar Tweets and integrates a contrastive social objective with masked language modeling for pre-training.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b15\": 0.8,\n    \"b41\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b18\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b8\": 0.5,\n    \"b16\": 0.55,\n    \"b17\": 0.55,\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the pre-training of language models for user-generated text on Twitter, which includes informal diction, abbreviations, emojis, and hashtags. The paper introduces TwHIN-BERT, a language model pre-trained with social engagements to leverage socially similar Tweets. The approach involves constructing a Twitter Heterogeneous Information Network (TwHIN), mining socially similar Tweet pairs, and pre-training with a contrastive social objective in addition to masked language modeling.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b15\": 0.9,\n    \"b10\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b31\": 0.7,\n    \"b41\": 0.7,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of pre-training language models for user-generated text on Twitter, which features informal diction and social interactions. The authors propose TwHIN-BERT, a multilingual language model pre-trained with social engagements, leveraging socially similar Tweets to improve performance on social recommendation and semantic understanding tasks.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b18\": 0.9,\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b4\": 0.7\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b41\": 0.6,\n    \"b1\": 0.5,\n    \"b7\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b31\": 0.4,\n    \"b26\": 0.4,\n    \"b38\": 0.4,\n    \"b8\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the difficulty of understanding Tweet semantics due to their informal diction, abbreviations, emojis, and topical tokens such as hashtags. The proposed solution is TwHIN-BERT, a multilingual language model for Twitter pre-trained with social engagements. The model leverages socially similar Tweets for pre-training, constructs a Twitter Heterogeneous Information Network (TwHIN) to unify multi-typed user engagement logs, and introduces a contrastive social objective along with masked language modeling.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b10\": 0.9,\n    \"b1\": 0.85,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b8\": 0.55\n  }\n}\n```"], "6260bd7f5aee126c0fc6bba4": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include capturing the 3D geometric structure and chemical features of the binding site, handling the enormous chemical space and continuous 3D space, and ensuring the equivariance property in molecule generation. The proposed GraphBP algorithm addresses these challenges by using a 3D graph neural network to capture contextual information, constructing a local coordinate system to ensure equivariance, and employing a flow model for sequential atom placement.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b27\": 0.9,\n    \"b44\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in structure-based drug design using machine learning methods, specifically focusing on generating molecules that bind to specific protein binding sites. The main challenges include capturing 3D geometric and chemical context, dealing with the vast chemical space, and ensuring equivariance to transformations of the binding site. The proposed GraphBP framework uses a 3D graph neural network to encode context, selects local reference atoms, and generates new atoms in a way that preserves equivariance and flexibility in atom placement.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b27\": 1.0,\n    \"b44\": 1.0,\n    \"b36\": 1.0,\n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of structure-based drug design, particularly focusing on capturing 3D geometric and chemical features, handling the vast chemical space, and maintaining equivariance in generated molecules. The proposed GraphBP algorithm generates 3D molecules by placing atoms sequentially in a continuous space, ensuring equivariance by using a local coordinate system based on a selected reference atom.\",\n  \"Direct Inspiration\": [\"b6\", \"b27\", \"b44\"],\n  \"Indirect Inspiration\": [\"b25\", \"b36\"],\n  \"Other Inspiration\": [\"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in structure-based drug design, specifically focusing on generating molecules that can bind to specific protein binding sites. The main challenges include handling complicated conditional information, navigating the enormous chemical and continuous 3D space, and maintaining the equivariance property. The proposed solution, GraphBP, uses a novel generative framework that places atoms one by one in the 3D binding site, employs a 3D graph neural network for context encoding, constructs a local coordinate system for equivariance, and utilizes a flow model for continuous atom placement.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1.0,\n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b44\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in structure-based drug design, specifically focusing on generating 3D molecules that bind to specific protein binding sites. It highlights three main challenges: capturing complicated conditional information, managing the enormous chemical and 3D space, and maintaining the equivariance property. The proposed solution, GraphBP, uses a novel generative framework that incorporates a 3D graph neural network, local coordinate systems, and flow models to place atoms sequentially while ensuring the equivariance property and capturing underlying dependencies.\",\n  \"Direct Inspiration\": [\"b6\", \"b44\", \"b27\"],\n  \"Indirect Inspiration\": [\"b36\", \"b25\"],\n  \"Other Inspiration\": []\n}\n```"], "62b2888c5aee126c0fbc731c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of accelerating drug discovery and reducing costs by proposing a novel generative modeling framework called Latent Inceptionism on Molecules (LIMO). LIMO builds on the variational autoencoder (VAE) framework, introduces a novel property predictor network architecture, and employs an inceptionism-like reverse optimization technique. It is faster and more efficient than existing methods, particularly in generating drug-like molecules with high binding affinities to target proteins.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b32\": 0.85,\n    \"b60\": 0.8,\n    \"b63\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.7,\n    \"b38\": 0.65,\n    \"b61\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b58\": 0.55,\n    \"b27\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces Latent Inceptionism on Molecules (LIMO), a generative modeling framework for drug discovery. It uses a variational autoencoder (VAE) and a novel property predictor network to generate drug-like molecules with desirable properties much faster than existing methods. The primary challenges addressed include the slow optimization of molecular properties and the difficulty of generating valid molecules with high binding affinity.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b32\": 1.0,\n    \"b61\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b60\": 0.8,\n    \"b63\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b30\": 0.6,\n    \"b10\": 0.6,\n    \"b50\": 0.6,\n    \"b58\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accelerating the drug discovery process by proposing a novel generative modeling framework called Latent Inceptionism on Molecules (LIMO). LIMO is designed to generate drug-like molecules with desirable properties more efficiently than existing methods. It builds on the variational autoencoder (VAE) framework, incorporates a novel property predictor network, and utilizes an inceptionism-like reverse optimization technique. The key contributions include improved speed and performance in generating molecules with high binding affinities while maintaining desirable properties.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b32\": 0.8,\n    \"b61\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b60\": 0.75,\n    \"b63\": 0.7,\n    \"b37\": 0.7,\n    \"b58\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b30\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the high cost and time-consuming nature of the drug discovery process, and the difficulty in identifying compounds with high binding affinity to designated protein targets. The proposed algorithm, Latent Inceptionism on Molecules (LIMO), aims to address these challenges by using a variational autoencoder (VAE) framework combined with a novel property predictor network architecture. It employs an inceptionism-like reverse optimization technique on a latent space to generate drug-like molecules with desirable properties faster than existing methods.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b37\": 0.8,\n    \"b60\": 0.9,\n    \"b63\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b29\": 0.6,\n    \"b61\": 0.7,\n    \"b58\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.5,\n    \"b10\": 0.4,\n    \"b3\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents Latent Inceptionism on Molecules (LIMO), a novel generative modeling framework for fast de novo molecule design. The primary challenges addressed include optimizing molecular properties that are computationally expensive to evaluate, such as binding affinity, and generating molecules with desired properties while keeping a molecular substructure fixed. The proposed approach builds on the VAE framework and employs an inceptionism-like reverse optimization technique on a latent space to achieve significant performance improvements over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.95,\n    \"b32\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.85,\n    \"b29\": 0.80,\n    \"b61\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b60\": 0.70,\n    \"b63\": 0.65,\n    \"b37\": 0.60\n  }\n}\n```"], "6385788690e50fcafdf4a0f3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of finding well-performing untrained sparse networks within Graph Neural Networks (GNNs). It introduces the Untrained GNNs Tickets (UGTs) algorithm, which leverages gradual sparsification techniques to identify these subnetworks without any model weight training. The primary motivations include extending the concept of untrained subnetworks from CNNs to GNNs and mitigating common issues in deep GNNs, such as over-smoothing.\",\n  \"Direct Inspiration\": [\"b10\", \"b11\"],\n  \"Indirect Inspiration\": [\"b12\", \"b13\", \"b19\"],\n  \"Other Inspiration\": [\"b14\", \"b17\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper explores the possibility of finding untrained sparse subnetworks within Graph Neural Networks (GNNs) that can perform well without any training of the model weights. The primary challenges addressed include the over-smoothing problem in deep GNNs and the difficulty in training deep GNNs. The proposed algorithm, Untrained GNNs Tickets (UGTs), leverages gradual sparsification techniques to discover such subnetworks and mitigate over-smoothing.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b10\", \"b11\"],\n    \"confidence\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b12\", \"b19\"],\n    \"confidence\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b17\", \"b18\", \"b13\"],\n    \"confidence\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to explore the existence of untrained sparse networks within GNNs that can perform as well as fully-trained dense GNNs. The proposed algorithm, Untrained GNNs Tickets (UGTs), aims to discover these matching untrained subnetworks at extremely high sparsities, mitigating issues such as over-smoothing and improving robustness and out-of-distribution performance.\",\n  \"Direct Inspiration\": [\"b10\", \"b11\", \"b12\"],\n  \"Indirect Inspiration\": [\"b19\", \"b13\"],\n  \"Other Inspiration\": [\"b17\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of finding well-performing graph neural networks (GNNs) without training model weights. It proposes the Untrained GNNs Tickets (UGTs) method, which discovers high-performing untrained subnetworks within dense GNNs. The core contributions include demonstrating the existence of such subnetworks, improving performance with gradual sparsification, and mitigating the over-smoothing problem.\",\n  \"Direct Inspiration\": [\"b10\", \"b11\"],\n  \"Indirect Inspiration\": [\"b12\", \"b19\"],\n  \"Other Inspiration\": [\"b13\", \"b17\", \"b18\", \"b45\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of discovering well-performing untrained sparse networks within Graph Neural Networks (GNNs). It explores the possibility of finding graph neural subnetworks that match the performance of fully trained networks without any weight updates, thus potentially mitigating issues like over-smoothing in deep GNNs. The proposed method, Untrained GNNs Tickets (UGTs), leverages gradual sparsification to identify these subnetworks across various GNN architectures and datasets.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b13\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b45\": 0.5\n  }\n}\n```"], "6466fafbd68f896efaeb7607": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"How to extend a 2D image encoder to extract features from 3D medical images?\",\n      \"How to align image and text features and learn multi-modal representations?\",\n      \"How to obtain a lightweight language model for CAD purposes?\"\n    ],\n    \"Inspirations\": [\n      \"Inspired by BLIP-2 [b5]\",\n      \"We propose MedBLIP\",\n      \"We adopt a learnable patch embedding\",\n      \"We propose MedQFormer\",\n      \"We choose BioMedLM [b9] as our basic language model\",\n      \"Fine-tune using LoRA Hu et al. [b2020]\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b5\",\n    \"b9\",\n    \"b2020\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b3\",\n    \"b35\",\n    \"b34\"\n  ],\n  \"Other Inspiration\": [\n    \"b1\",\n    \"b2\",\n    \"b11\",\n    \"b12\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of making clinical diagnoses using electronic health records (EHRs) that include different types of medical data, such as radiology images and text. The proposed solution, MedBLIP, aims to fuse 3D medical images and texts for computer-aided diagnosis (CAD), specifically for Alzheimer's Disease (AD). The main contributions include a lightweight CAD system, a MedQFormer module for aligning 3D medical image features with textural features, and the collection of a large public dataset for AD research.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b9\": 1.0,\n    \"b2020\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of fusing different types of medical data (3D medical scans and texts) from EHRs for computer-aided diagnosis (CAD). It proposes a novel lightweight CAD system called MedBLIP, which utilizes a bootstrapping language-image pre-training model inspired by BLIP-2 to align and merge 3D medical images and textual information. The main contributions include a MedQFormer module for extracting and aligning 3D medical image features with textual ones, and a pre-training approach on a large dataset for Alzheimer's Disease (AD) diagnosis.\",\n  \"Direct Inspiration\": [\"b5\"],\n  \"Indirect Inspiration\": [\"b8\", \"b9\", \"b2020\"],\n  \"Other Inspiration\": [\"b3\", \"b6\", \"b7\", \"b35\"]\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges of fusing different types of medical data (e.g., 3D medical scans and texts) for computer-aided diagnosis (CAD) of Alzheimer's Disease (AD). The proposed model, MedBLIP, is inspired by existing vision-language pre-training models and aims to extend these technologies to handle 3D medical images and textual information from electronic health records (EHRs). The main contributions are the development of a query mechanism to align visual and textual features, and the creation of a lightweight, trainable CAD system.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b8\": 0.9,\n    \"b9\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of fusing different types of medical data from EHRs (such as 3D medical scans and text) for CAD, specifically for Alzheimer's Disease diagnosis. It introduces MedBLIP, a lightweight CAD system inspired by BLIP-2, that employs a novel MedQFormer module to align visual and textual features.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b9\": 0.7,\n    \"b2020\": 0.7,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b6\": 0.4,\n    \"b7\": 0.4\n  }\n}\n```"], "622577a75aee126c0f008d4a": ["```json\n{\n  \"Summary\": \"The paper proposes a novel method called DIAD to address challenges in anomaly detection (AD) for tabular data, including noisy and irrelevant features, heterogeneous features, limited labeled data, and the need for interpretability. The method is inspired by Generalized Additive Models (GAMs) and incorporates Partial Identification (PID) as an AD objective, using differentiable GAM architecture. The approach combines unsupervised anomaly detection with semi-supervised fine-tuning using labeled data.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b5\": 0.9,\n    \"b13\": 0.9,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b29\": 0.7,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes DIAD, a novel Data-efficient Interpretable Anomaly Detection method targeting challenges in tabular AD, including noisy/irrelevant features, heterogeneous features, limited labeled data, and interpretability. The method integrates Generalized Additive Models (GAMs) with Partial Identification (PID) as AD objectives, and employs differentiable AUC loss for fine-tuning with labeled data.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b5\": 0.9,\n    \"b13\": 0.9,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b18\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in anomaly detection (AD) for tabular data, such as noisy and irrelevant features, heterogeneous features, limited labeled data, and interpretability. It proposes a novel method, DIAD, inspired by Generalized Additive Models (GAMs) and Partial Identification (PID) to handle these challenges. DIAD employs a differentiable GAM architecture fine-tuned with a small amount of labeled data to improve AD performance and provide interpretable outputs.\",\n    \"Direct Inspiration\": [\"b3\", \"b5\", \"b13\"],\n    \"Indirect Inspiration\": [\"b29\", \"b8\"],\n    \"Other Inspiration\": [\"b4\", \"b14\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses key challenges in anomaly detection (AD) for tabular data, including noisy and irrelevant features, heterogeneous features, small labeled data sets, and interpretability. The authors propose a novel Data-efficient Interpretable AD (DIAD) approach inspired by Generalized Additive Models (GAMs) to handle these challenges. The DIAD model leverages Partial Identification (PID) as the AD objective and incorporates differentiable GAMs, enabling it to scale to high-dimensional features, handle heterogeneous features, and utilize labeled data effectively while maintaining interpretability.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b5\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b10\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in anomaly detection (AD) for tabular data, specifically focusing on noisy and irrelevant features, heterogeneous features, limited labeled data, and interpretability. The proposed method, DIAD, is inspired by Generalized Additive Models (GAMs) and introduces Partial Identification (PID) as the AD objective. DIAD leverages differentiable GAM architecture to handle high-dimensional and heterogeneous features, incorporate labeled data, and provide interpretability.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9,\n        \"b5\": 0.9,\n        \"b13\": 0.9,\n        \"b8\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.7,\n        \"b4\": 0.7,\n        \"b18\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b29\": 0.5,\n        \"b31\": 0.5\n    }\n}\n```"], "64741c33d68f896efaa7b7a1": ["```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the high complexity and dimensionality of biomedical data, and the inefficiencies of developing specialized models for each disease or task. The authors propose BiomedGPT, a unified and generalist AI model capable of handling various data modalities and tasks through a sequence-to-sequence abstraction. The model is designed to transfer knowledge across tasks and outperform specialist models in vision-language tasks.\",\n  \"Direct Inspiration\": [\"b126\", \"b30\"],\n  \"Indirect Inspiration\": [\"b101\", \"b69\", \"b12\", \"b34\"],\n  \"Other Inspiration\": [\"b7\", \"b62\", \"b27\", \"b115\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in this paper is the development of a unified and generalist AI model, BiomedGPT, for biomedicine that can handle various data modalities and perform diverse tasks efficiently. The inspiration for BiomedGPT came from existing transformer-based models and the need for a comprehensive approach in biomedical AI to overcome the limitations of task/domain-specific and modality-specific applications. The proposed model leverages a sequence-to-sequence architecture with integrated task-oriented prompts and demonstrates state-of-the-art performance across multiple vision-language tasks.\",\n    \"Direct Inspiration\": [\"b126\"],\n    \"Indirect Inspiration\": [\"b30\", \"b56\", \"b101\"],\n    \"Other Inspiration\": [\"b122\", \"b12\", \"b68\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents BiomedGPT, a unified and generalist AI model designed for handling various types of biomedical data through straightforward serialization integrated with task-oriented prompts. The main challenges addressed include the complexity and high dimensionality of biomedical data, the limitations of previous task/domain-specific models, and the need for a model capable of performing diverse tasks with a unified architecture.\",\n  \"Direct Inspiration\": [\n    \"b126\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b30\",\n    \"b56\",\n    \"b101\",\n    \"b13\",\n    \"b27\"\n  ],\n  \"Other Inspiration\": [\n    \"b104\",\n    \"b60\",\n    \"b7\",\n    \"b44\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces BiomedGPT, a unified AI model designed for the biomedical field that can handle various modalities and tasks. BiomedGPT employs a sequence-to-sequence architecture, leveraging both self-supervised pretraining and fine-tuning to achieve state-of-the-art performance in vision-language tasks such as image captioning and visual question answering. The model aims to address the limitations of task-specific and modality-specific applications by using a generalist approach inspired by existing models and techniques.\",\n  \"Direct Inspiration\": [\"b126\"],\n  \"Indirect Inspiration\": [\"b30\", \"b102\", \"b34\", \"b12\"],\n  \"Other Inspiration\": [\"b27\", \"b56\", \"b131\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces BiomedGPT, a unified and generalist model designed for handling various types of biomedical data through straightforward serialization integrated with task-oriented prompts. The main challenges addressed are the need for a comprehensive model that can handle multiple modalities and tasks, as well as the limitations of existing models that are usually task/domain-specific or modality-specific. The authors propose an encoder-decoder architecture inspired by OFA to improve the efficiency and flexibility of the model in pretraining and fine-tuning stages across various biomedical domains and tasks.\",\n  \"Direct Inspiration\": {\n    \"b126\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.9,\n    \"b34\": 0.8,\n    \"b10\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b144\": 0.7,\n    \"b131\": 0.7\n  }\n}\n```"], "63e312f590e50fcafdc1a1bb": ["```json\n{\n  \"Summary\": \"The paper addresses the problem of multiway spectral graph partitioning, aiming to partition a graph into k subgraphs that are internally well connected but loosely connected to each other. The novel approach revolves around the subspace spanned by eigenvectors and introduces new cut functions and Cheeger inequalities to establish the relation between the eigenvalue problem and the partitioning problem. The proposed method includes an alternating algorithm for solving the partitioning problem, starting approximations, and a semi-sparse orthogonal approximation technique.\",\n  \"Direct Inspiration\": [\"b2\", \"b6\"],\n  \"Indirect Inspiration\": [\"b12\", \"b14\", \"b15\", \"b22\"],\n  \"Other Inspiration\": [\"b1\", \"b7\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the problem of multiway spectral partitioning of graphs and the development of novel, fast, and reliable algorithms for solving this problem. The authors propose an algorithm that reinterprets the spectral partitioning problem by focusing on the subspace spanned by the eigenvectors rather than the eigenvectors themselves. The key innovation is the use of an alternating method to solve an optimization problem that leads directly to the desired partitioning.\",\n  \"Direct Inspiration\": [\"b6\", \"b2\", \"b12\", \"b14\", \"b15\"],\n  \"Indirect Inspiration\": [\"b3\", \"b22\", \"b10\"],\n  \"Other Inspiration\": [\"b1\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multiway spectral partitioning of graphs into k subgraphs, improving on the spectral 2-partitioning method. The authors propose a novel algorithm that leverages the subspace spanned by eigenvectors rather than the eigenvectors themselves. They introduce cut functions and Cheeger inequalities to establish a relation between eigenvalues and the partitioning problem, and propose an alternating method for solving the formulated optimization problem.\",\n  \"Direct Inspiration\": [\"b2\", \"b6\"],\n  \"Indirect Inspiration\": [\"b1\", \"b22\", \"b14\", \"b15\"],\n  \"Other Inspiration\": [\"b3\", \"b12\", \"b25\", \"b0\", \"b11\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on the problem of multiway spectral partitioning of graphs, aiming to partition a graph into k subgraphs that are internally well connected and loosely connected to each other. The main contributions include defining cut functions, proving Cheeger inequalities, and proposing novel algorithms for multiway spectral partitioning.\",\n  \"Direct Inspiration\": [\"b2\", \"b6\"],\n  \"Indirect Inspiration\": [\"b7\", \"b12\", \"b14\", \"b15\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multiway spectral partitioning of graphs, which involves partitioning a graph into k subgraphs that are internally well-connected but loosely connected to each other. It proposes a novel approach based on the subspace spanned by eigenvectors and the use of indicator matrices to achieve this partitioning efficiently.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b22\": 0.7,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```"], "6487e9fad68f896efa482c3a": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of localizing Adaptive Spatial-Temporal Graph Neural Networks (ASTGNNs) by sparsifying adjacency matrices. This reduces computational overhead, enables efficient distributed deployment, and can potentially reveal overlapping information between spatial and temporal dependencies. The authors propose an Adaptive Graph Sparsification (AGS) algorithm for this purpose, achieving significant sparsification without accuracy loss during inference but noting a decrease in accuracy when retraining from scratch.\",\n    \"Direct Inspiration\": {\n        \"b24\": 0.9,\n        \"b2\": 0.8,\n        \"b5\": 0.8,\n        \"b6\": 0.8,\n        \"b7\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b11\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.6,\n        \"b15\": 0.6,\n        \"b26\": 0.6,\n        \"b28\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Understanding the extent to which spatial dependencies can be localized in ASTGNNs without accuracy loss.\",\n      \"Designing resource-efficient ASTGNN models for large-scale and distributed applications.\"\n    ],\n    \"inspirations\": [\n      \"Adaptive spatial-temporal graph neural networks (ASTGNNs) which capture spatial and temporal dependencies.\",\n      \"Graph sparsification techniques to reduce computational overhead.\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b2\",\n    \"b24\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b7\",\n    \"b5\",\n    \"b4\"\n  ],\n  \"Other Inspiration\": [\n    \"b6\",\n    \"b26\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of localizing spatial-temporal graph models, specifically adaptive spatial-temporal graph neural networks (ASTGNNs), to reduce computation costs and enable efficient distributed deployment. It introduces the Adaptive Graph Sparsification (AGS) algorithm to sparsify the adjacency matrices of ASTGNNs, significantly reducing resource requirements while maintaining accuracy.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b2\": 1.0,\n    \"b5\": 1.0,\n    \"b6\": 1.0,\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b11\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b15\": 0.7,\n    \"b28\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of localising Adaptive Spatial-Temporal Graph Neural Networks (ASTGNNs) to make them more resource-efficient, particularly by sparsifying the adjacency matrices. It introduces the Adaptive Graph Sparsification (AGS) algorithm, which uses a differentiable approximation of the l0-regularization for progressive sparsification during training. The paper demonstrates that spatial dependencies can be largely ignored during inference without significant accuracy loss, but they are crucial during training for effective model performance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b24\": 0.85,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b28\": 0.7,\n    \"b26\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of localizing Adaptive Spatial-Temporal Graph Neural Networks (ASTGNNs) to improve computational efficiency and enable distributed deployment. The authors propose a novel algorithm, Adaptive Graph Sparsification (AGS), to sparsify the adjacency matrices in ASTGNNs, thus reducing the resource requirements without significantly impacting accuracy. The paper's contributions include the introduction of AGS, extensive experiments demonstrating the feasibility of localization, and insights into the overlapping information provided by spatial and temporal dependencies.\",\n    \"Direct Inspiration\": [\"b2\", \"b24\"],\n    \"Indirect Inspiration\": [\"b10\", \"b11\", \"b6\"],\n    \"Other Inspiration\": [\"b4\", \"b15\", \"b28\"]\n}\n```"], "63fec3ce90e50fcafdd70610": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in Architecture-centric Software Engineering (ACSE), specifically focusing on leveraging AI, particularly ChatGPT, to enhance the architecting process through human-bot collaboration. It aims to investigate the potential of ChatGPT in automating architectural analysis, synthesis, and evaluation, thereby relieving architects of tedious tasks and enabling more efficient collaborative architecting.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b1\", \"b2\", \"b12\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b6\", \"b7\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b3\", \"b10\", \"b13\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Architecture-centric Software Engineering (ACSE), particularly focusing on the integration of AI, specifically ChatGPT, to support human-bot collaborative architecting. The key contributions include investigating the potential of ChatGPT in automating ACSE processes, and analyzing its impact on architects' productivity and the ethical, governance, and socio-technical constraints involved.\",\n  \"Direct Inspiration\": [\"b1\", \"b12\"],\n  \"Indirect Inspiration\": [\"b6\", \"b7\", \"b13\"],\n  \"Other Inspiration\": [\"b8\", \"b9\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in Architecture-centric Software Engineering (ACSE), focusing on human-bot collaboration using ChatGPT to automate and enrich the architecting process. Key contributions include investigation of ChatGPT's role in collaborative architecting, identification of ethical and socio-technical issues, and empirical validation of ChatGPT's capabilities in ACSE.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 0.9,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.5,\n    \"b7\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.4,\n    \"b9\": 0.4,\n    \"b10\": 0.3,\n    \"b11\": 0.3,\n    \"b13\": 0.3,\n    \"b14\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges of Architecture-centric Software Engineering (ACSE), particularly focusing on the integration of artificial intelligence (AI) to enhance collaborative architecting. The study aims to investigate the potential of ChatGPT as a DevBot to assist in human-bot collaborative architecting by automating architectural analysis, synthesis, and evaluation tasks.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.8,\n    \"b3\": 0.85,\n    \"b6\": 0.75,\n    \"b7\": 0.7,\n    \"b12\": 0.88\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.65,\n    \"b5\": 0.6,\n    \"b10\": 0.55,\n    \"b11\": 0.5,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b9\": 0.65,\n    \"b14\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the integration of AI, specifically ChatGPT, into the Architecture-centric Software Engineering (ACSE) process to enable collaborative architecting. The main challenges include automating architectural tasks, managing architectural drift, and incorporating intelligence in the architecting process. The proposed method involves using ChatGPT for architectural analysis, synthesis, and evaluation through human-bot collaboration.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 1.0,\n    \"b3\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```"], "63aa623e90e50fcafd978bc2": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of aligning large language models (LLMs) to the medical domain to improve their performance in medical question answering. Key contributions include the introduction of instruction prompt tuning to create Med-PaLM, evaluations using newly curated and existing datasets, and development of a human evaluation framework to assess model performance in terms of scientific consensus, potential harm, and bias.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b49\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b11\", \"b13\", \"b28\", \"b78\", \"b90\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b32\", \"b63\", \"b33\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the key challenges of adapting large language models (LLMs) to the medical domain, highlighting the limitations of current single-task AI systems in healthcare. It proposes the novel method of instruction prompt tuning to further adapt Flan-PaLM into a medical-specific model named Med-PaLM. The paper also introduces new datasets and a human evaluation framework to assess the performance of LLMs in medical question answering, revealing that Med-PaLM outperforms Flan-PaLM and is comparable to clinician-generated answers in accuracy and safety.\",\n    \"Direct Inspiration\": {\n        \"b9\": 0.9,\n        \"b32\": 0.85,\n        \"b33\": 0.85,\n        \"b63\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b78\": 0.75,\n        \"b49\": 0.7,\n        \"b90\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.6,\n        \"b13\": 0.6,\n        \"b4\": 0.55,\n        \"b24\": 0.55,\n        \"b40\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges include the lack of expressivity and interactive capabilities in current single-task AI models for medical applications, the need for alignment of LLMs to the medical domain, and addressing issues related to fairness, equity, and bias in these models.\",\n    \"inspirations\": \"The paper proposes instruction prompt tuning to adapt Flan-PaLM to the medical domain, resulting in Med-PaLM, and introduces a human evaluation framework to assess LLM performance in medical question answering.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b49\", \"b78\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b13\", \"b11\", \"b62\", \"b28\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b32\", \"b33\", \"b63\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of using AI models in medicine, particularly their lack of expressivity and interactive capabilities. It proposes instruction prompt tuning to adapt the Flan-PaLM model to the medical domain, resulting in Med-PaLM, which shows improved alignment with scientific consensus and reduced harmful outcomes. The paper also introduces new datasets and a human evaluation framework to assess LLM performance in medical question answering.\",\n  \"Direct Inspiration\": {\n    \"b78\": 1,\n    \"b49\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b13\": 0.8,\n    \"b32\": 0.7,\n    \"b33\": 0.7,\n    \"b63\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b62\": 0.6,\n    \"b90\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of adapting large language models (LLMs) to the medical domain, proposing instruction prompt tuning to create Med-PaLM. Key contributions include the curation of HealthSearchQA and MultiMedQA datasets, a pilot framework for human evaluation, state-of-the-art results on medical question answering benchmarks, and revealing key limitations of LLMs via human evaluation.\",\n    \"Direct Inspiration\": {\n        \"b49\": 1.0,\n        \"b78\": 0.9,\n        \"b90\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b32\": 0.8,\n        \"b33\": 0.8,\n        \"b63\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.7,\n        \"b13\": 0.7,\n        \"b28\": 0.7\n    }\n}\n```"], "63f2e4ae90e50fcafd283025": ["```json\n{\n  \"Summary\": \"The paper presents a novel model called Mixed Graph+3D Denoising Diffusion (MiDi) for generating molecular structures by simultaneously considering both 2D graph and 3D conformation. The main challenges addressed include the limitation of existing models which only focus on either 2D or 3D structures, thereby lacking end-to-end differentiability and optimizing potential for complex drug discovery tasks. MiDi overcomes this by using a unique noise schedule and a novel rEGNN layer within a transformer architecture to ensure SE(3)-equivariance and achieve high performance in molecule generation tasks.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1,\n    \"b46\": 0.9,\n    \"b15\": 0.9,\n    \"b11\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.7,\n    \"b13\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b36\": 0.6,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing generative models that can simultaneously generate a molecular graph and its corresponding 3D conformation. The novel MiDi model overcomes this limitation by using a mixed Graph+3D denoising diffusion approach that incorporates both Gaussian and discrete noise models. The model also introduces a novel rEGNN layer for improved translation and rotation equivariance.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b15\": 0.9,\n    \"b36\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b41\": 0.7,\n    \"b11\": 0.6,\n    \"b46\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.5,\n    \"b34\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating molecular graphs and their corresponding 3D coordinates simultaneously, which is crucial for drug discovery tasks. Existing models either focus on 2D structures or 3D conformers but not both. The proposed MiDi model uses a novel Mixed Graph+3D Denoising Diffusion approach, leveraging both Gaussian and discrete diffusion processes. Key contributions include an adaptive noise schedule and the introduction of a novel rEGNN layer for denoising, which maintains SE(3) equivariance.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b38\": 1,\n    \"b46\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b51\": 0.8,\n    \"b34\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the simultaneous generation of molecular graphs and their 3D coordinates to better capture chemical properties for drug discovery. This is achieved through the proposed MiDi model, which utilizes a combination of Gaussian and discrete noise models in a denoising diffusion framework. Key contributions include the introduction of an adaptive noise schedule and a novel relaxed Equivariant Graph Neural Network (rEGNN) layer.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b15\": 1.0,\n    \"b38\": 1.0,\n    \"b46\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b41\": 0.8,\n    \"b51\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge addressed in this paper is the simultaneous generation of molecular graphs and their corresponding 3D coordinates to accurately capture the chemical space for drug discovery. Existing models either generate molecular graphs or 3D conformers but not both, which limits their effectiveness in complex tasks.\",\n    \"Proposed Solution\": \"The authors propose a new model called Mixed Graph+3D Denoising Diffusion (MiDi) that generates both a molecular graph and its 3D coordinates simultaneously. The model uses both Gaussian and discrete noise models and introduces an adaptive noise schedule and a novel Transformer architecture incorporating a relaxed Equivariant Graph Neural Network (rEGNN) layer.\"\n  },\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b46\": 0.7,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b41\": 0.6,\n    \"b34\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```"], "64be53113fda6d7f06326238": ["```json\n{\n  \"Summary\": \"The paper presents the Question Answering on Scientific Articles (QASA) benchmark and method, addressing the challenge of full-stack cognitive reasoning in QA systems. It introduces a two-stage cognitive reasoning process inspired by human reasoning, to answer advanced, multifaceted questions from scientific articles. The key contributions include a novel schema for collecting diverse question types, a detailed annotation process, and a multi-step approach for generating comprehensive answers using associative selection, evidential rationale-generation, and systematic composition.\",\n  \"Direct Inspiration\": {\n    \"b39\": 0.9,\n    \"b36\": 0.9,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.7,\n    \"b35\": 0.7,\n    \"b6\": 0.7,\n    \"b17\": 0.7,\n    \"b15\": 0.7,\n    \"b11\": 0.7,\n    \"b21\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b41\": 0.7,\n    \"b26\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b8\": 0.5,\n    \"b32\": 0.5,\n    \"b44\": 0.5,\n    \"b37\": 0.5,\n    \"b16\": 0.5,\n    \"b1\": 0.5,\n    \"b42\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of advanced question answering on scientific articles, emphasizing the need for full-stack cognitive reasoning. It proposes the QASA benchmark, which involves a two-stage reasoning process similar to human intellectual reasoning. The approach decomposes the task into associative selection, evidential rationale-generation, and systematic composition, using large language models.\",\n  \"Direct Inspiration\": {\n    \"b39\": 0.9,\n    \"b36\": 0.9,\n    \"b27\": 0.85,\n    \"b35\": 0.85,\n    \"b6\": 0.85,\n    \"b17\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.75,\n    \"b11\": 0.75,\n    \"b21\": 0.75,\n    \"b13\": 0.75,\n    \"b14\": 0.75,\n    \"b41\": 0.75,\n    \"b26\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b5\": 0.7,\n    \"b8\": 0.7,\n    \"b32\": 0.7,\n    \"b44\": 0.65,\n    \"b37\": 0.65,\n    \"b16\": 0.65,\n    \"b1\": 0.65,\n    \"b42\": 0.65,\n    \"b2\": 0.65,\n    \"b28\": 0.65,\n    \"b22\": 0.65,\n    \"b31\": 0.65,\n    \"b23\": 0.65,\n    \"b24\": 0.65,\n    \"b18\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are associated with achieving advanced cognitive reasoning in machine learning Question Answering systems, which involve both associative and logical reasoning stages. The proposed algorithm, Question Answering on Scientific Articles (QASA), aims to address these challenges by modeling the full-stack reasoning process through state-of-the-art large language models. The QASA benchmark and approach differentiate from existing datasets by requiring more elaborated efforts and deeper reasoning to answer advanced questions posed by readers and authors of scientific papers.\",\n    \"Direct Inspiration\": {\n        \"b27\": 0.9,\n        \"b35\": 0.85,\n        \"b5\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.75,\n        \"b11\": 0.75,\n        \"b21\": 0.75,\n        \"b13\": 0.75,\n        \"b14\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.7,\n        \"b17\": 0.7,\n        \"b23\": 0.6,\n        \"b24\": 0.6,\n        \"b31\": 0.6,\n        \"b8\": 0.65,\n        \"b32\": 0.65,\n        \"b16\": 0.6,\n        \"b44\": 0.6,\n        \"b42\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of advanced question answering (QA) on scientific articles, particularly focusing on questions that require full-stack cognitive reasoning from associative selection to systematic composition. It introduces the Question Answering on Scientific Articles (QASA) benchmark and proposes a novel approach to model the full-stack reasoning process using state-of-the-art large language models. The QASA dataset is designed to handle diverse and balanced questions, including surface, testing, and deep questions, with answers composed by AI/ML experts.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b42\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b32\": 0.7,\n    \"b28\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b3\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces QASA (Question Answering on Scientific Articles), a novel QA benchmark that addresses the challenge of full-stack cognitive reasoning from associative knowledge extraction to logical reasoning. It differentiates from existing datasets by focusing on advanced questions that require both surface and deep reasoning, and by involving expert readers and authors in creating multifaceted, long-form answers based on comprehensive evidence from scientific articles.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b8\": 0.9,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b42\": 0.7,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5\n  }\n}\n```"], "64a63bddd68f896efaec64af": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inability of existing KGC models to effectively fuse KG structural information with the linguistic context provided by PLMs. The proposed CSProm-KG model introduces Conditional Soft Prompts to address this challenge, leveraging both structural and textual knowledge to improve KGC performance.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b16\": 1,\n    \"b35\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.8,\n    \"b33\": 0.8,\n    \"b38\": 0.8,\n    \"b7\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b17\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the effective fusion of Knowledge Graph (KG) structural information into Pretrained Language Models (PLMs) for Knowledge Graph Completion (KGC). The proposed algorithm, CSProm-KG, introduces the concept of Conditional Soft Prompts to address the over-fitting issue toward textual information in KG by integrating structural knowledge. It also proposes Local Adversarial Regularization to improve the model's robustness in distinguishing textually similar entities.\",\n  \"Direct Inspiration\": [\"b18\", \"b16\", \"b35\"],\n  \"Indirect Inspiration\": [\"b42\", \"b33\", \"b38\"],\n  \"Other Inspiration\": [\"b41\", \"b7\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of Knowledge Graph Completion (KGC) by introducing the CSProm-KG model which combines structural knowledge from graph embeddings with textual knowledge from frozen Pretrained Language Models (PLMs) using Conditional Soft Prompts. The primary challenge is to effectively fuse the KG structural information into PLM-based KGC models, which typically overly focus on textual information and may confuse entities with similar text. The proposed model aims to achieve better completion tasks and distinguish textually similar entities by integrating both structural and textual data.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.6,\n    \"b42\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively fusing structural information of Knowledge Graphs (KGs) with pretrained language models (PLMs) to improve Knowledge Graph Completion (KGC). The proposed solution, CSProm-KG, introduces Conditional Soft Prompts to integrate structural and textual information, and Local Adversarial Regularization to distinguish textually similar entities.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b16\": 0.85,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.7,\n    \"b33\": 0.65,\n    \"b38\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b41\": 0.55,\n    \"b7\": 0.45,\n    \"b20\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively fusing Knowledge Graph (KG) structural information into Pretrained Language Models (PLMs) for Knowledge Graph Completion (KGC). The authors propose a novel CSProm-KG model which uses Conditional Soft Prompts to incorporate structural knowledge into frozen PLMs, thereby overcoming the over-reliance on textual information. The method includes Local Adversarial Regularization to distinguish textually similar entities.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b18\": 1,\n    \"b35\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.6,\n    \"b38\": 0.6,\n    \"b42\": 0.6\n  }\n}\n```"], "64cc77b33fda6d7f06aebd0d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of text-to-music generation, focusing on the limited availability of text-music parallel training data and the risk of plagiarism. The authors propose a new model called MusicLDM, which adapts the architectures of Stable Diffusion and AudioLDM to the music domain. They introduce two novel mixup strategies, Beat-Synchronous Audio Mixup (BAM) and Beat-Synchronous Latent Mixup (BLM), to enhance the training process and mitigate these challenges.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b30\": 1,\n    \"b40\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b32\": 0.5,\n    \"b2\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of limited text-music parallel training data and the risk of plagiarism in text-to-music generation. It proposes a new model, MusicLDM, which adapts the Stable Diffusion and AudioLDM architectures to the music domain. To enhance the model's performance and mitigate data limitations, the authors introduce two novel mixup strategies, Beat-Synchronous Audio Mixup (BAM) and Beat-Synchronous Latent Mixup (BLM), which ensure beat alignment before interpolating between samples.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b30\": 1.0,\n    \"b40\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of limited training data and plagiarism in text-to-music generation. It proposes a novel model, MusicLDM, which adapts Stable Diffusion and AudioLDM architectures to the music domain. It also introduces two beat-synchronous mixup strategies (BAM and BLM) to enhance data augmentation and mitigate plagiarism risks.\",\n    \"Direct Inspiration\": [\n        \"b23\",\n        \"b30\",\n        \"b40\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b21\",\n        \"b38\",\n        \"b41\"\n    ],\n    \"Other Inspiration\": [\n        \"b0\",\n        \"b32\"\n    ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in text-to-music generation, specifically the limited availability of text-music parallel training data and the risk of plagiarism in generated outputs. The authors propose a new model, MusicLDM, which adapts the Stable Diffusion and AudioLDM architectures to the music domain. They introduce two novel mixup strategies, Beat-Synchronous Audio Mixup (BAM) and Beat-Synchronous Latent Mixup (BLM), to augment training data and encourage novel music generation while mitigating plagiarism risks.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b30\": 0.9,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b32\": 0.7,\n    \"b2\": 0.7,\n    \"b41\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b15\": 0.5,\n    \"b7\": 0.5,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of limited text-music parallel training data and the risk of plagiarism in text-to-music generation. The authors propose MusicLDM, a state-of-the-art text-to-music generation model adapted from Stable Diffusion and AudioLDM architectures. They introduce two novel mixup strategies, beat-synchronous audio mixup (BAM) and beat-synchronous latent mixup (BLM), designed specifically for music generation to encourage novel and diverse outputs while mitigating data limitations and plagiarism risks.\",\n    \"Direct Inspiration\": {\n        \"b30\": 1.0,\n        \"b23\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b40\": 0.9,\n        \"b38\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.75,\n        \"b15\": 0.75,\n        \"b41\": 0.7\n    }\n}\n```"], "64a29621d68f896efa28fd65": ["```json\n{\n  \"Summary\": \"The paper tackles the challenge of efficiently hiding the latency of medium-duration events (10s to 100s of ns) such as L2/L3 cache misses and memory accesses, which cause significant CPU stalls. The proposed solution combines light-weight coroutines for low-overhead context switching and sample-based profiling for event visibility, aiming to create a deployable software mechanism that is transparent, generally applicable, and allows fine-grained latency control.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b9\": 0.8,\n    \"b26\": 0.75,\n    \"b34\": 0.8,\n    \"b50\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b43\": 0.7,\n    \"b62\": 0.7,\n    \"b65\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b45\": 0.6,\n    \"b49\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of hiding the latency of medium-duration events (10s to 100s of ns), such as L2/L3 cache misses, using software mechanisms. The proposed solution leverages light-weight coroutines and sample-based profiling to reduce switching overhead and improve event visibility without requiring hardware changes.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b43\": 0.9,\n    \"b62\": 0.9,\n    \"b9\": 0.9,\n    \"b34\": 0.9,\n    \"b65\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b35\": 0.8,\n    \"b22\": 0.7,\n    \"b27\": 0.7,\n    \"b52\": 0.7,\n    \"b26\": 0.7,\n    \"b50\": 0.7,\n    \"b55\": 0.7,\n    \"b71\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b45\": 0.6,\n    \"b24\": 0.6,\n    \"b66\": 0.6,\n    \"b47\": 0.6,\n    \"b49\": 0.6,\n    \"b60\": 0.6,\n    \"b32\": 0.6,\n    \"b0\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of hiding event latencies in the range of 10s to 100s of nanoseconds. The authors propose a novel software mechanism using light-weight coroutines and sample-based profiling to mitigate the inefficiencies of traditional hardware and software solutions. The main contributions include reducing context-switching overhead and improving event visibility without significant hardware changes.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b34\": 1.0,\n    \"b65\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b22\": 0.7,\n    \"b27\": 0.7,\n    \"b32\": 0.6,\n    \"b50\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b15\": 0.5,\n    \"b45\": 0.5,\n    \"b49\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of hiding the latency of medium-duration events (10s to 100s of ns) to reduce CPU stalls. The authors propose a novel combination of light-weight coroutines and sample-based profiling to achieve this. The primary contributions are leveraging coroutines for low-overhead context switching and using profiling to gather event visibility.\",\n    \"Direct Inspiration\": {\n        \"b16\": 1,\n        \"b43\": 1,\n        \"b62\": 1,\n        \"b9\": 1,\n        \"b34\": 1,\n        \"b65\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.9,\n        \"b35\": 0.9,\n        \"b26\": 0.85,\n        \"b50\": 0.85,\n        \"b55\": 0.85,\n        \"b71\": 0.85\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.8,\n        \"b27\": 0.8,\n        \"b52\": 0.8,\n        \"b9\": 0.8,\n        \"b47\": 0.75,\n        \"b49\": 0.75,\n        \"b21\": 0.75\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of hiding mid-range event latencies (10s to 100s of ns) in software, which is typically inefficient. It proposes using a novel combination of light-weight coroutines and sample-based profiling to mitigate inefficiencies in software mechanisms. The proposed design targets L2/L3 cache misses and aims for a transparent interface, general applicability, and controllable latency.\",\n  \"Direct Inspiration\": [\"b16\", \"b43\", \"b62\", \"b9\", \"b34\", \"b65\"],\n  \"Indirect Inspiration\": [\"b5\", \"b35\", \"b26\", \"b50\", \"b55\", \"b71\"],\n  \"Other Inspiration\": [\"b24\", \"b66\", \"b27\", \"b30\", \"b52\"]\n}\n```"], "63969ba790e50fcafdcf1d53": ["```json\n{\n    \"Summary\": \"The paper tackles the high computational cost of training large neural networks by proposing a method called model upcycling. This method upgrades an existing dense model into a sparsely activated Mixture-of-Experts (MoE) model with a relatively small additional computational budget. The primary challenges addressed include overcoming the initial performance drop when changing a trained network's structure and ensuring the upgraded model outperforms the original model at a smaller computational cost.\",\n    \"Direct Inspiration\": {\n        \"b36\": 0.95,\n        \"b37\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b42\": 0.85,\n        \"b12\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.75,\n        \"b6\": 0.7,\n        \"b31\": 0.65\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the high computational cost of training large neural networks from scratch. The proposed solution is 'model upcycling', which involves upgrading an existing dense model to a sparsely activated Mixture-of-Experts (MoE) model with a small additional computational budget. The novel approach aims to improve model performance by leveraging the additional capacity of MoE layers without the need for new data or significantly higher computation costs.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b36\", \"b12\", \"b37\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b42\", \"b25\", \"b31\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b26\", \"b7\", \"b39\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing computational costs in training large neural networks by proposing a method called model upcycling. This method upgrades existing dense models into Mixture-of-Experts (MoE) models with a smaller additional computational budget. The approach involves using pretrained dense Transformer checkpoints to warm-start the training of MoE models. Key techniques include Expert Choice routing and modifications to MLP layers. The effectiveness of the method is demonstrated in vision and language tasks, showing significant performance improvements with less computational overhead compared to training dense models from scratch.\",\n  \"Direct Inspiration\": {\n    \"b42\": 0.9,\n    \"b12\": 0.85,\n    \"b36\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.75,\n    \"b31\": 0.7,\n    \"b7\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing the computational costs associated with training large neural networks by proposing a method called 'model upcycling.' This method involves transforming dense models into larger, sparsely activated Mixture-of-Experts (MoEs) to improve performance with a relatively small additional computational budget. The key inspiration comes from the need to make the training of large models more efficient and accessible, leveraging existing pretrained models and enhancing them with sparse structures.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9,\n    \"b36\": 0.85,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.75,\n    \"b31\": 0.7,\n    \"b7\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.6,\n    \"b25\": 0.55,\n    \"b44\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high computational costs in training large language and vision models from scratch. The authors propose an upcycling approach to convert dense models into sparsely activated Mixture-of-Experts (MoEs) with a relatively small additional computational budget. This approach leverages pretrained dense Transformer checkpoints to warm-start the training of MoEs, thus enhancing performance at lower costs. Key challenges include overcoming the initial performance drop when transitioning to a new model structure and optimizing the configuration of the MoE layers.\",\n  \"Direct Inspiration\": {\n    \"b42\": 0.9,\n    \"b7\": 0.85,\n    \"b55\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.75,\n    \"b12\": 0.7,\n    \"b37\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b6\": 0.55\n  }\n}\n```"], "64893b17d68f896efa982789": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of integrating vision-language models for medical applications, particularly in analyzing chest radiographs. The main contributions include fine-tuning a medical LLM (Vicuna) on medical data, generating high-quality interactive summaries, and aligning a specialized medical visual encoder (MedClip) with the fine-tuned LLM to generate insightful conversations about chest radiographs. The approach leverages pre-existing resources and focuses on optimizing performance while minimizing the need for extensive data.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b2\": 0.9,\n    \"b6\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b9\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b7\": 0.5,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying large-scale vision-language models to the medical domain, particularly for chest radiographs, by fine-tuning a pre-trained medical vision encoder and a medical large language model. The main contributions include fine-tuning the Vicuna LLM on medical data, generating high-quality interactive summaries from radiology reports, and aligning a specialized medical visual encoder with the fine-tuned LLM to enhance performance with limited data.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b6\": 0.95,\n    \"b3\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.85,\n    \"b16\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b19\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in creating a vision-language model tailored for medical domains, specifically chest radiographs. It aims to overcome the lack of large medical datasets and the nuanced distinctions required in medical imaging. The authors propose a method that fine-tunes a large language model (Vicuna) on medical data and aligns it with a medical visual encoder (MedClip) using a linear transformation layer. Key contributions include generating high-quality interactive summaries from radiology reports and making the model and datasets publicly available.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b2\": 0.95,\n    \"b3\": 0.9,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b9\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.85,\n    \"b4\": 0.75,\n    \"b7\": 0.75,\n    \"b17\": 0.75,\n    \"b18\": 0.75\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of applying large-scale vision-language models to the medical domain, particularly for analyzing chest radiographs. It proposes an innovative model, XrayGPT, which aligns medical visual and textual representations to generate meaningful conversations about radiographs. The approach involves fine-tuning a medical vision encoder and a large language model on high-quality image-summary pairs to overcome the lack of data and domain-specific nuances in medical contexts.\",\n    \"Direct Inspiration\": {\n        \"b22\": 1.0,\n        \"b2\": 0.9,\n        \"b6\": 0.8,\n        \"b3\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.6,\n        \"b9\": 0.6,\n        \"b19\": 0.6,\n        \"b16\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.5,\n        \"b15\": 0.5,\n        \"b4\": 0.5,\n        \"b17\": 0.5,\n        \"b18\": 0.5,\n        \"b21\": 0.5,\n        \"b1\": 0.4,\n        \"b11\": 0.4,\n        \"b20\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\"Lack of medical image-text pair data\", \"Subtle distinctions in medical terms\", \"Adapting general vision-language models to medical domain\"],\n    \"Inspirations\": [\"The success of large-scale vision-language models\", \"The capabilities of models like Mini-GPT and Vicuna in general contexts\"]\n  },\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b7\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```"], "64a29612d68f896efa28bca5": ["```json\n{\n    \"Summary\": \"The paper addresses the primary challenge of improving processor performance by enhancing cache replacement policies, specifically focusing on instruction caching for modern architectures that tolerate many L1I misses. The proposed algorithm, EMISSARY, improves performance by prioritizing instruction lines whose misses cause decode starvation, preserving these lines in L2 upon eviction from L1I, and treating them bimodally.\",\n    \"Direct Inspiration\": {\n        \"b31\": 0.9,\n        \"b40\": 0.9,\n        \"b46\": 0.9,\n        \"b51\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b21\": 0.6,\n        \"b45\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b27\": 0.5,\n        \"b35\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the lack of cost-aware cache replacement policies for instruction caches in modern processors. The proposed EMISSARY algorithm prioritizes instruction lines whose misses cause decode starvation and preserves these high-priority lines in L2 upon eviction from L1I, resulting in performance improvements and energy savings with minimal hardware footprint.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b22\": 0.85,\n    \"b45\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.75,\n    \"b40\": 0.75,\n    \"b46\": 0.75,\n    \"b51\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b10\": 0.7,\n    \"b23\": 0.7,\n    \"b25\": 0.7,\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b37\": 0.7,\n    \"b41\": 0.7,\n    \"b48\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of cost-aware cache replacement policies for instruction caches, specifically focusing on performance improvement by reducing decode starvation. The proposed EMISSARY algorithm prioritizes instruction lines whose misses cause decode starvation, ensuring they are preserved in L2 cache upon eviction from L1I. This approach results in improved performance and energy savings while maintaining a minimal hardware footprint.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b45\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.7,\n    \"b40\": 0.7,\n    \"b46\": 0.7,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed by the paper is the lack of cost-aware cache replacement policies specifically for instruction caches, which can lead to suboptimal processor performance due to decode starvation. The paper introduces EMISSARY, a bimodal cache replacement policy that prioritizes instruction lines causing decode starvation, preserving them in the L2 cache to improve performance and energy efficiency.\",\n    \"Direct Inspiration\": {\n        \"b21\": 0.9,\n        \"b22\": 0.9,\n        \"b45\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b27\": 0.8,\n        \"b31\": 0.75,\n        \"b40\": 0.75,\n        \"b46\": 0.75,\n        \"b51\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b23\": 0.6,\n        \"b25\": 0.6,\n        \"b37\": 0.6,\n        \"b35\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces EMISSARY, a cost-aware cache replacement policy addressing decode starvation in modern processor architectures. The primary challenge is the lack of effective cost-aware replacement policies for instruction caching. EMISSARY prioritizes instruction lines causing decode starvation and preserves them in the L2 cache upon eviction from the L1I cache, leading to improved performance and energy savings.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b22\": 0.85,\n    \"b45\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.75,\n    \"b31\": 0.7,\n    \"b40\": 0.7,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.65,\n    \"b35\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```"], "621454435aee126c0f1d702f": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of load imbalance and sub-optimal training efficiency in sparsely gated mixture-of-experts (MoE) models. It proposes a novel routing method called 'expert choice' that ensures perfect load balancing and allows a variable number of experts for each token, leading to improved training efficiency and downstream performance.\",\n    \"Direct Inspiration\": {\n        \"b9\": 1.0,\n        \"b20\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b21\": 0.8,\n        \"b27\": 0.8,\n        \"b7\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b30\": 0.7,\n        \"b11\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of load imbalance, under specialization, and fixed compute per token in conventional mixture-of-experts (MoE) models. It proposes a novel 'expert choice' routing method that allows each expert to select the top-k tokens, thus ensuring perfect load balancing, flexible compute allocation, and substantial gains in training efficiency and downstream performance.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b9\", \"b20\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b21\", \"b27\", \"b7\", \"b30\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b11\"]\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of load imbalance and suboptimal training in conventional Mixture-of-Experts (MoE) models by proposing a novel routing method called 'expert choice'. This method ensures perfect load balancing, allows a variable number of experts for each token, and achieves significant gains in training efficiency and downstream performance.\",\n    \"Direct Inspiration\": {\n        \"b9\": 0.9,\n        \"b20\": 0.9,\n        \"b30\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b21\": 0.7,\n        \"b27\": 0.6,\n        \"b7\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.5,\n        \"b25\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of inefficiency and sub-optimal performance in conventional sparsely gated mixture-of-experts (MoE) models, primarily focusing on load imbalance, under specialization, and uniform compute allocation per token. The proposed 'expert choice' routing method allows each expert to select top-k tokens, ensuring perfect load balancing and variable expert allocation per token, which significantly improves training efficiency and downstream performance.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b27\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.8,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include load imbalance and under specialization in conventional mixture-of-experts (MoE) models. The algorithm proposed by the authors, called 'expert choice,' addresses these issues by allowing each expert to pick the top-k tokens, ensuring perfect load balancing and enabling variable compute allocation for each token. This method guarantees better training efficiency and downstream performance.\",\n  \"Direct Inspiration\": [\"b9\", \"b20\", \"b30\"],\n  \"Indirect Inspiration\": [\"b21\", \"b27\", \"b7\"],\n  \"Other Inspiration\": [\"b11\"]\n}\n```"], "64ae66f63fda6d7f0684abc0": ["```json\n{\n    \"Summary\": \"The paper focuses on evaluating Pretrained Language Models' (PLMs) ability to memorize and understand ontological knowledge. It constructs datasets to probe PLMs' memorization of ontological relationships and their reasoning capabilities based on ontological entailment rules. The study finds that while PLMs can memorize certain ontological knowledge, their understanding and reasoning abilities are limited.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b38\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.75,\n        \"b26\": 0.75,\n        \"b7\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.65,\n        \"b29\": 0.65,\n        \"b23\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are to assess whether Pretrained Language Models (PLMs) can encode and understand ontological knowledge rather than just rote memorization. The paper proposes a methodology for constructing datasets and tasks to probe PLMs' memorization and reasoning abilities regarding ontological knowledge. Key contributions include constructing a dataset for evaluating PLMs' ability to memorize and infer ontological knowledge, and demonstrating that while PLMs can memorize some ontological knowledge, their understanding is limited.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9,\n    \"b6\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of determining whether Pretrained Language Models (PLMs) can encode and understand ontological knowledge, and if they can perform logical reasoning based on this knowledge. The authors propose a methodology that involves constructing a dataset to evaluate PLMs' ability to memorize and reason with ontological knowledge.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9,\n    \"b6\": 0.85,\n    \"b26\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b38\": 0.75,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of determining if Pretrained Language Models (PLMs) can encode and understand ontological knowledge, rather than just memorizing surface forms. The authors propose a methodology to probe PLMs' memorization and reasoning abilities regarding ontological knowledge by constructing specific datasets and tasks. The main contributions include creating a dataset to evaluate PLMs' ontological knowledge memorization and inference capabilities, and systematically probing their reasoning abilities.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.95,\n        \"b38\": 0.9,\n        \"b7\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b37\": 0.8,\n        \"b34\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.7,\n        \"b9\": 0.7,\n        \"b6\": 0.65,\n        \"b26\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of probing Pretrained Language Models (PLMs) for ontological knowledge, including their ability to memorize and reason with such knowledge. The authors propose a methodology for constructing evaluation datasets and tasks to test PLMs' memorization and reasoning capabilities. They introduce novel probing methods and templates to assess the models' performance accurately.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b38\": 0.7,\n    \"b7\": 0.7,\n    \"b34\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b26\": 0.5,\n    \"b25\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```"], "62a7fc635aee126c0ff5e394": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in this paper is the identification of protein-ligand binding sites using computational methods based on the geometrical structure of the protein. The algorithm proposed leverages geometric properties and machine learning techniques to predict these binding sites. It uses the Solvent Excluded Surface (SES) generated by NanoShaper and focuses on geometric methods combined with machine learning approaches, particularly a Transformer-based neural network model for segmenting 3D protein shapes.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b15\": 0.85,\n    \"b16\": 0.85,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b18\": 0.8,\n    \"b35\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b7\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper tackles the challenge of recognizing protein-ligand binding sites based on the geometrical structure of proteins. The proposed method leverages geometric properties of molecular surfaces and machine learning (ML) techniques, particularly focusing on geometric and ML-driven approaches. The primary inspiration derives from state-of-the-art biophysical pocket detection methods and the adoption of transformer-based neural networks for segmenting 3D protein shapes.\",\n    \"Direct Inspiration\": {\n        \"b11\": 0.9,\n        \"b35\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.75,\n        \"b16\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.6,\n        \"b18\": 0.55,\n        \"b37\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Summary of the challenges and inspirations of the paper\": \"The primary challenge addressed in the paper is the identification of protein-ligand binding sites based on the geometrical structure of proteins. The authors propose a method to evaluate computational techniques for predicting such binding sites, emphasizing the importance of these predictions for drug design. The paper uses a geometric approach combined with machine learning (ML) techniques to identify and rank binding sites, leveraging data from existing protein-ligand complexes.\"\n  },\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b11\": 1.0,\n    \"b7\": 1.0,\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b18\": 0.8,\n    \"b17\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily focuses on evaluating computational methods for recognizing protein-ligand binding sites based on the geometrical structure of proteins. It utilizes NanoShaper to build the proteins' Solvent Excluded Surface (SES) and applies machine learning techniques to predict binding regions. The main challenges include accurately identifying these binding sites for drug design purposes and dealing with the inherent difficulties in learning the features of 'druggable' pockets due to limitations in available training data.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b7\": 0.85,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.75,\n    \"b18\": 0.7,\n    \"b16\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b20\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying protein-ligand binding sites using computational methods based on the geometrical structure of proteins. The key motivation is to leverage geometrical properties and machine learning techniques to improve the accuracy of binding site predictions, which is crucial for drug design and biophysics research.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b16\": 0.7,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b3\": 0.5,\n    \"b4\": 0.5\n  }\n}\n```"], "63d9d87390e50fcafd57d878": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is generating high-quality text-to-audio (TTA) outputs efficiently, especially without relying on large-scale high-quality audio-text data pairs. The paper proposes AudioLDM, a generative model using latent diffusion models (LDMs) conditioned on audio embeddings learned by a variational auto-encoder (VAE) and Contrastive Language-Audio Pretraining (CLAP). Key contributions include demonstrating the use of continuous LDMs for TTA, enabling TTA generation without text supervision, and achieving state-of-the-art performance on the AudioCaps dataset.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b47\": 0.9,\n    \"b60\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b61\": 0.7,\n    \"b58\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes AudioLDM, a system for text-to-audio (TTA) generation leveraging Latent Diffusion Models (LDMs) conditioned on audio embeddings. The main contributions include achieving state-of-the-art TTA generation quality, enabling text-guided audio manipulations like style transfer and inpainting, and demonstrating that training LDMs with only audio data can outperform models trained with audio-text pairs.\",\n  \"Direct Inspiration\": [\"b47\", \"b60\", \"b24\"],\n  \"Indirect Inspiration\": [\"b12\", \"b54\", \"b61\"],\n  \"Other Inspiration\": [\"b8\", \"b58\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of generating high-quality audio from text descriptions (Text-to-Audio, TTA) by utilizing Latent Diffusion Models (LDMs) in a continuous space. The proposed method, AudioLDM, leverages Contrastive Language-Audio Pretraining (CLAP) to enable TTA without relying on paired text-audio data during training. This approach aims to improve computational efficiency and generation quality, and also enables various zero-shot text-guided audio manipulations.\",\n  \"Direct Inspiration\": {\n    \"b47\": 0.9,\n    \"b60\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b61\": 0.7,\n    \"b58\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b41\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating audio prompted by natural language descriptions. Inspired by recent advancements in generative models and diffusion models, the authors propose AudioLDM, a text-to-audio (TTA) system that leverages continuous latent diffusion models (LDMs). The novel contributions include utilizing CLAP latents to enable TTA generation without audio-text paired data, achieving state-of-the-art TTA performance, and enabling zero-shot audio manipulations.\",\n  \"Direct Inspiration\": [\"b47\", \"b60\", \"b24\", \"b61\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b12\", \"b54\", \"b58\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality audio from natural language descriptions (text-to-audio, TTA) by leveraging continuous latent diffusion models (LDMs) and Contrastive Language-Audio Pretraining (CLAP). Key contributions include the development of a novel TTA system, AudioLDM, which outperforms existing methods in both subjective and objective evaluations, and the implementation of text-guided audio manipulations without requiring audio-text paired data for training.\",\n  \"Direct Inspiration\": [\"b47\"],\n  \"Indirect Inspiration\": [\"b24\", \"b60\"],\n  \"Other Inspiration\": [\"b7\", \"b8\", \"b12\", \"b40\", \"b61\"]\n}\n```"], "643e0ad10746dc40e34197a9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of using large language models (LLMs) in the medical field due to privacy concerns with non-transparent models. The authors propose fine-tuning open-source LLMs specifically for biomedical tasks using a variety of datasets. They evaluate the effectiveness of these models on the USMLE examination and make the models and datasets publicly available for medical and AI researchers.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b9\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of leveraging large language models (LLMs) for medical applications while maintaining privacy. It proposes the use of open-source models fine-tuned on biomedical tasks to avoid unauthorized data transfers and privacy concerns. The models are evaluated on the USMLE to assess their effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of using large language models (LLMs) for medical applications while maintaining data privacy. It proposes fine-tuning open-source LLMs with biomedical datasets, specifically to suit medical tasks and ensure on-site implementation to mitigate privacy concerns. The models are evaluated on the USMLE exams to validate their performance.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.9,\n    \"b12\": 0.9\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying large language models (LLMs) in the medical field, particularly focusing on privacy concerns and the need for open-source models. The authors present fine-tuned models for biomedical tasks using a blend of new and established open-source biomedical datasets. The models are evaluated on the United States Medical Licensing Examination (USMLE) to assess their effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b1\": 0.9,\n    \"b2\": 0.9,\n    \"b3\": 0.9,\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of using large language models (LLMs) in the medical field while ensuring data privacy. The authors propose using open-source models fine-tuned with biomedical datasets to mitigate privacy concerns and improve the performance of LLMs in medical tasks. They evaluate these models using the United States Medical Licensing Examination (USMLE) to demonstrate their effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b1\": 0.9,\n    \"b2\": 0.9,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```"], "6334268390e50fcafd6a5a91": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in text mining and knowledge discovery from biomedical literature, specifically focusing on the generative capabilities of pre-trained language models. The proposed BioGPT model is tailored for the biomedical domain by pre-training on 15M PubMed abstracts, and it is applied to various biomedical NLP tasks including relation extraction, question answering, and document classification. The novel aspect of this work is the creation of a domain-specific generative model, which aims to overcome the limitations of existing BERT and GPT models in the biomedical context.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b9\": 0.8,\n    \"b13\": 0.75,\n    \"b23\": 0.65\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.55,\n    \"b5\": 0.5,\n    \"b6\": 0.45,\n    \"b10\": 0.4\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.35,\n    \"b12\": 0.3,\n    \"b14\": 0.3,\n    \"b15\": 0.3,\n    \"b16\": 0.25,\n    \"b24\": 0.25,\n    \"b25\": 0.25,\n    \"b27\": 0.25,\n    \"b28\": 0.25\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting pre-training models to the biomedical domain, specifically focusing on generative tasks. The proposed solution is BioGPT, a domain-specific generative pre-trained Transformer language model for biomedical text generation and mining. BioGPT is pre-trained on 15M PubMed abstracts and applied to various biomedical NLP tasks. The authors highlight the shortcomings of previous biomedical pre-training models, mainly BERT-like models, and the limited success of prior GPT applications in the biomedical domain.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b13\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the domain shift problem when applying general pre-training models to the biomedical domain, especially for text generation tasks. The paper proposes BioGPT, a domain-specific generative pre-trained Transformer language model for biomedical text generation and mining. BioGPT is pre-trained on 15M PubMed abstracts and is evaluated on six biomedical NLP tasks, demonstrating superior performance compared to baseline methods.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b9\": 0.9,\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b4\": 0.5,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the adaptation of pre-training models to the biomedical domain, particularly for text generation tasks. The proposed algorithm, BioGPT, is a domain-specific generative pre-trained transformer language model designed for biomedical text generation and mining. It is pre-trained on 15M PubMed abstracts and applied to six biomedical NLP tasks, demonstrating superior performance compared to baseline methods.\",\n  \"Direct Inspiration\": {\n    \"BioBERT\": \"b9\",\n    \"PubMedBERT\": \"b8\",\n    \"GPT\": \"b4\",\n    \"GPT-2\": \"b5\",\n    \"GPT-3\": \"b6\"\n  },\n  \"Indirect Inspiration\": {\n    \"DARE\": \"b20\",\n    \"BioELECTRA\": \"b27\",\n    \"LinkBERT\": \"b28\"\n  },\n  \"Other Inspiration\": {\n    \"BC5CDR\": \"b12\",\n    \"KD-DTI\": \"b13\",\n    \"DDI\": \"b14\",\n    \"PubMedQA\": \"b15\",\n    \"HoC\": \"b16\"\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating an effective generative pre-trained Transformer language model tailored for biomedical text generation and mining, named BioGPT. The main inspiration is the success of pre-training models in general NLP and the need to adapt these models to the biomedical domain due to domain-specific challenges.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b9\": 0.9,\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b13\": 0.8,\n    \"b23\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5,\n    \"b29\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.4,\n    \"b33\": 0.4,\n    \"b34\": 0.4,\n    \"b35\": 0.4,\n    \"b36\": 0.4,\n    \"b37\": 0.4,\n    \"b38\": 0.4,\n    \"b39\": 0.4,\n    \"b40\": 0.4,\n    \"b41\": 0.4,\n    \"b42\": 0.4,\n    \"b43\": 0.4,\n    \"b44\": 0.4,\n    \"b45\": 0.4,\n    \"b46\": 0.4\n  }\n}\n```"], "64a29621d68f896efa28fd4a": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of enabling programmability for developers in processors, which is currently restricted and complex due to the use of microcode by processor vendors. The proposed solution, Metal, is a processor hardware extension that allows developers to create new architectural extensions through a vertical microcode-like programming interface. Metal aims to simplify the development of complex architectural features and improve security by providing greater transparency and flexibility.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b3\": 1.0,\n        \"b11\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.8,\n        \"b22\": 0.8,\n        \"b30\": 0.8,\n        \"b31\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b5\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling developers to modify and extend processor architecture through software rather than hardware. It proposes 'Metal', a processor hardware extension that provides a microcode-like programming interface for developers to add new architectural features and applications. The solution involves a dedicated RAM for storing Metal code, new privileged operation modes, and specific instructions for accessing architectural features.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b1\": 0.9,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b13\": 0.6,\n    \"b22\": 0.7,\n    \"b17\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b6\": 0.5,\n    \"b30\": 0.4,\n    \"b31\": 0.4,\n    \"b38\": 0.45,\n    \"b43\": 0.4,\n    \"b16\": 0.5,\n    \"b18\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents Metal, a processor hardware extension that allows developers to evolve processor architecture through software, enhancing capabilities like user-defined privilege levels, custom page tables, and transactional memory. The main challenges addressed include the complexity of microcode, the limitations of current processor programmability, and the need for secure and efficient implementation of new architectural features.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b1\": 0.85,\n    \"b11\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b17\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.75,\n    \"b16\": 0.7,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling developers to define new processor instructions and architectural features through an open architecture that simplifies the complexity of microcode. The proposed solution, Metal, is a processor hardware extension providing a vertical microcode-like interface that allows developers to create new features and applications. Metal aims to overcome the limitations of traditional microcode by offering a more accessible programming model.\",\n  \"Direct Inspiration\": [\"b0\", \"b22\"],\n  \"Indirect Inspiration\": [\"b1\", \"b3\", \"b11\", \"b13\", \"b30\", \"b31\"],\n  \"Other Inspiration\": [\"b16\", \"b38\", \"b43\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling programmability for system developers at the processor microcode level. The authors propose Metal, a processor hardware extension that allows developers to create new architectural features and applications via a microcode-like interface. This solution aims to simplify the development of complex architectural features and improve security by providing more transparency and control over microcode.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b3\": 1.0,\n    \"b11\": 1.0,\n    \"b22\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.8,\n    \"b31\": 0.8,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```"], "640a9ffc90e50fcafd03ca47": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include disambiguating pairs of company names using a machine learning approach and making human labeling more efficient through active learning. The paper proposes a Siamese Recurrent Neural Network for disambiguation and an active learning setting to prioritize labeling samples.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.95,\n    \"b17\": 0.9,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b20\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of disambiguating pairs of company names using a Siamese Recurrent Neural Network (RNN) approach. It claims the proposed method outperforms other baseline models and is effective in generalizing to other domains. Additionally, the paper integrates the model into an Active Learning framework to enhance the efficiency of human labeling by prioritizing samples for labeling.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b15\": 0.85,\n    \"b2\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.75,\n    \"b22\": 0.7,\n    \"b25\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b28\": 0.55\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of disambiguating company names using a Siamese Recurrent Neural Network (RNN) and improving human labelling efficiency through Active Learning. It proposes novel methodologies in both these areas, demonstrating superior performance over baseline models.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b17\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.75,\n        \"b2\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b24\": 0.65,\n        \"b3\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of disambiguating company names using a Siamese Recurrent Neural Network and improving human labeling efficiency through Active Learning. It demonstrates the effectiveness of the proposed approach over baseline models and validates the Active Learning setting.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b2\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the disambiguation of company names and efficient human labeling through active learning. The paper proposes a Siamese Recurrent Neural Network (SRNN) for disambiguating pairs of company names, and uses active learning to prioritize samples for human labeling.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b17\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b19\": 0.7,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.5,\n    \"b3\": 0.5,\n    \"b15\": 0.5\n  }\n}\n```"], "628304515aee126c0f6f0c83": ["```json\n{\n  \"Summary\": \"The paper addresses the high failure rates in clinical trials, particularly in oncology, due to inadequate preclinical model systems. It introduces SystemMatch, an end-to-end machine learning pipeline that optimizes preclinical models using single-cell genomic data to better approximate human clinical behavior. The process involves using a Compositional Perturbational Autoencoder (CPA) to predict cellular responses to experimental conditions and iteratively refining the model systems to enhance their predictive power.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.65,\n    \"b11\": 0.6,\n    \"b30\": 0.55\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the high failure rates in clinical trials, particularly in oncology, by proposing SystemMatch, an end-to-end machine learning pipeline. SystemMatch aims to optimize preclinical models to better predict clinical outcomes using single-cell genomic data. The pipeline involves evaluating preclinical models against a multi-study atlas of single-cell genomics data from target tissues, and employs a Compositional Perturbational Autoencoder (CPA) to predict the behavior of single cells under various experimental conditions.\",\n    \"Direct Inspiration\": {\n        \"b16\": 0.95,\n        \"b9\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b10\": 0.75,\n        \"b34\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b13\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the high failure rates in clinical trials, particularly in oncology, by proposing an end-to-end machine learning pipeline, SystemMatch, which optimizes preclinical models to closely approximate the behavior of a target population in humans. SystemMatch leverages single-cell genomic data and a Compositional Perturbational Autoencoder (CPA) to predict and recommend experimental changes to enhance the similarity of preclinical models to the target population.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b7\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.7,\n    \"b6\": 0.6,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.5,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the high failure rates in clinical trials, particularly in oncology, due to poorly predictive preclinical model systems. The authors propose SystemMatch, an end-to-end machine learning pipeline that uses single-cell genomic data to optimize preclinical models for better clinical predictivity. SystemMatch integrates a Compositional Perturbational Autoencoder (CPA) to predict cellular responses under various experimental conditions and recommend modifications to preclinical models to enhance their predictive power.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b6\": 0.95,\n    \"b14\": 0.95,\n    \"b13\": 0.9,\n    \"b5\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.85,\n    \"b9\": 0.85,\n    \"b25\": 0.8,\n    \"b15\": 0.8,\n    \"b11\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.75,\n    \"b4\": 0.75,\n    \"b19\": 0.75,\n    \"b30\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the high failure rates of clinical trials in oncology and proposes an end-to-end machine learning pipeline, SystemMatch, to optimize preclinical models. This system leverages single-cell genomic data and a Compositional Perturbational Autoencoder (CPA) to predict and enhance preclinical models' similarity to target human populations. The ultimate goal is to improve the predictive power of these models for therapeutic development.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b25\": 0.5\n  }\n}\n```"], "646c3addd68f896efa5d1972": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in de novo design of drug molecules in 3D space, proposing a coarse-to-fine approach using a Hierarchical Diffusion-based model (HierDiff). The model first generates coarse-grained structures and then decodes these into fine-grained fragments to assemble atomic structures, addressing issues of atom-level generation and geometric constraints.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b21\": 0.7,\n    \"b33\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the critical challenge of generating realistic 3D drug molecules while maintaining chemical validity, which is a complex task due to the high-dimensional nature of molecular structures. The proposed solution is a Hierarchical Diffusion-based model (HierDiff) that uses a coarse-to-fine approach to generate molecule structures more accurately by first generating coarse-grained structures and then refining them.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b33\": 0.7,\n    \"b41\": 0.7,\n    \"b20\": 0.6,\n    \"b37\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b43\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Designing drug molecules in 3D space.\",\n      \"Integrating 3D information in molecule design.\",\n      \"Autoregressive models suffering from scale and error accumulation.\",\n      \"Generating reliable molecule structures and avoiding atom-bonds conflicts.\"\n    ],\n    \"Algorithm\": [\n      \"Proposing a coarse-to-fine approach for 3D molecule generation.\",\n      \"Hierarchical Diffusion-based model (HierDiff).\",\n      \"Generating coarse-grained structure, then decoding to fine-grained fragments.\",\n      \"Using geometric diffusion model and equivariant message-passing network.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"References\": [\n      \"b16\"\n    ],\n    \"Reasoning\": [\n      \"'Inspired by the successful diffusion model in text and image generation, [b16] proposed the first diffusion model for molecule generation and significantly improves the validity of generated molecules.'\"\n    ],\n    \"Confidence\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"References\": [\n      \"b11\",\n      \"b33\",\n      \"b41\"\n    ],\n    \"Reasoning\": [\n      \"Early studies on 3D molecule generation using autoregressive approach.\",\n      \"Hierarchical graph generation and hierarchical structure derivation.\",\n      \"Continuous time normalizing flows and non-autoregressive modeling.\"\n    ],\n    \"Confidence\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"References\": [\n      \"b6\",\n      \"b26\",\n      \"b39\"\n    ],\n    \"Reasoning\": [\n      \"Challenges and advantages of non-autoregressive methods.\",\n      \"Issues with autoregressive models and error accumulation.\"\n    ],\n    \"Confidence\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating 3D drug molecules by proposing a novel Hierarchical Diffusion-based model (HierDiff) that integrates a coarse-to-fine approach. The model first generates coarse-grained structures representing clusters of fragments and then refines these into fine-grained atomic structures, ensuring chemical validity and reducing computational complexity.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b16\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b20\", \"b41\", \"b6\", \"b26\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b11\", \"b33\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of de novo design of drug molecules in 3D space by proposing a coarse-to-fine approach called HierDiff. This approach first generates a coarse-grained structure of the molecule, where each node represents a cluster of fragments, and then decodes this into a fine-grained atomic structure. Key inspirations include the diffusion model and the need for chemically interpretable features and geometric constraints.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b30\": 0.75,\n    \"b33\": 0.8,\n    \"b41\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7,\n    \"b26\": 0.65,\n    \"b37\": 0.6\n  }\n}\n```"], "6423ac7790e50fcafd55eacf": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of knowledge graph (KG) completion by proposing a Transformer-based inductive KG representation model called iHT. The model aims to learn transferable knowledge representations from both unstructured texts and structured KGs. The proposed method leverages pre-trained language models and introduces a unified Transformer architecture for better scalability and adaptability across different KGs and downstream tasks.\",\n  \"Direct Inspiration\": [\"b37\", \"b6\", \"b4\"],\n  \"Indirect Inspiration\": [\"b32\", \"b38\"],\n  \"Other Inspiration\": [\"b9\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is the incomplete nature of Knowledge Graphs (KGs) and the need for models that can generalize well across different KGs and accommodate new entities. The proposed solution is a Transformer-based inductive KG representation model, iHT, which learns transferable knowledge representations from both unstructured texts and structured KGs. The model is pre-trained on the Wikidata5M dataset and shows significant performance improvements on standard KG completion benchmarks.\",\n  \"Direct Inspiration\": [\"b37\", \"b6\", \"b4\"],\n  \"Indirect Inspiration\": [\"b32\", \"b38\", \"b36\"],\n  \"Other Inspiration\": [\"b9\", \"b2\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a Transformer-based inductive knowledge graph (KG) representation model called iHT, aimed at improving KG completion. The primary challenge addressed is the incomplete nature of KGs and the need for models that can generalize to unseen entities and facts. The paper leverages pre-trained language models and a Transformer-based relational scoring module to capture richer semantics from both unstructured texts and structured KGs.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b32\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of knowledge graph (KG) completion by proposing a Transformer-based inductive KG representation model called iHT. The model leverages both structured KGs and unstructured texts to learn transferable knowledge representations. The primary challenges include handling large-scale KGs, adapting to new entities, and improving generalization from one KG to another.\",\n  \"Direct Inspiration\": [\"b4\", \"b6\", \"b37\"],\n  \"Indirect Inspiration\": [\"b9\", \"b32\", \"b38\"],\n  \"Other Inspiration\": [\"b2\", \"b36\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of incomplete knowledge graphs (KGs) by proposing a Transformer-based inductive KG representation model, iHT. This model aims to learn transferable knowledge representations from both unstructured texts and structured KGs. Key components include an entity encoder and a context encoder, both utilizing Transformer architecture. The model is pre-trained on the Wikidata5M dataset and shows significant improvements in KG completion tasks.\",\n    \"Direct Inspiration\": {\n        \"b37\": 0.9,\n        \"b6\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b32\": 0.8,\n        \"b38\": 0.7,\n        \"b33\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b36\": 0.6\n    }\n}\n```"], "63b3f1fc90e50fcafdea3697": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently auto-scheduling dynamic-shape workloads in deep neural networks (DNNs). The proposed solution, DietCode, creates a shape-generic search space using micro-kernels and a joint learning cost model to optimize performance across various shapes. This approach significantly reduces the auto-scheduling time and improves runtime performance compared to existing solutions.\",\n  \"Direct Inspiration\": [\"b45\", \"b9\", \"b1\", \"b37\"],\n  \"Indirect Inspiration\": [\"b12\", \"b24\", \"b11\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently auto-scheduling dynamic-shape workloads in deep neural networks. The proposed solution, DietCode, introduces a shape-generic search space and a micro-kernel-based cost model to reduce the auto-scheduling time and improve runtime performance.\",\n  \"Direct Inspiration\": {\n    \"b45\": 0.9,\n    \"b9\": 0.8,\n    \"b1\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b24\": 0.6,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b48\": 0.5,\n    \"b38\": 0.5,\n    \"b4\": 0.5,\n    \"b41\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inefficiency of existing auto-schedulers in handling dynamic-shape workloads, which require static-shape workloads to be known at compile-time. The proposed solution, DietCode, introduces a shape-generic search space and a micro-kernel-based cost model to efficiently support dynamic-shape workloads by reusing the same micro-kernel across different shapes, thus reducing auto-scheduling time and improving runtime performance.\",\n  \"Direct Inspiration\": {\n    \"b45\": 0.9,\n    \"b9\": 0.9,\n    \"b1\": 0.9,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b48\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently auto-scheduling dynamic-shape workloads in deep neural networks (DNNs). It proposes DietCode, a new auto-scheduler framework that constructs a shape-generic search space using micro-kernels and a cost model with shape-generic and shape-dependent components to reduce auto-scheduling time and improve runtime performance.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b45\": 1,\n    \"b1\": 1,\n    \"b37\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b24\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b48\": 0.6,\n    \"b36\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently auto-scheduling dynamic-shape workloads in deep neural networks (DNNs). It proposes DietCode, a new auto-scheduler framework that constructs a shape-generic search space using micro-kernels and a cost model that separates shape-generic and shape-dependent components. This approach significantly reduces auto-scheduling time and improves runtime performance for dynamic-shape workloads.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1.0,\n    \"b9\": 1.0,\n    \"b1\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b24\": 0.8,\n    \"b11\": 0.8,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b38\": 0.7,\n    \"b41\": 0.7,\n    \"b17\": 0.6\n  }\n}\n```"], "648fd298d68f896efa163bfb": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the application of large language models to the medical field, addressing issues like factual errors, logic inconsistencies, and coherence problems in generative models. The proposed solution, ClinicalGPT, is trained on diverse medical datasets using parameter-efficient fine-tuning and reinforcement learning to enhance performance in medical tasks.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b6\": 1,\n    \"b3\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.8,\n    \"b4\": 0.8,\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper presents ClinicalGPT, a large language model tailored for medical applications. The primary challenges addressed include applying generative language models to the medical field, overcoming issues like factual inaccuracies, logic inconsistencies, coherence problems, and limited reasoning abilities. The model leverages extensive medical datasets and employs parameter-efficient fine-tuning methods, reinforcement learning, and knowledge graphs to enhance performance in medical tasks such as question-answering, medical examinations, patient consultations, and medical record analysis.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1,\n        \"b6\": 1,\n        \"b3\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.8,\n        \"b4\": 0.7,\n        \"b5\": 0.7,\n        \"b7\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.5,\n        \"b12\": 0.5,\n        \"b14\": 0.5,\n        \"b15\": 0.5,\n        \"b16\": 0.5,\n        \"b17\": 0.5,\n        \"b18\": 0.5,\n        \"b19\": 0.5,\n        \"b20\": 0.5,\n        \"b21\": 0.5,\n        \"b22\": 0.5,\n        \"b23\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of applying generative language models to the medical field, emphasizing the need for high accuracy, interpretability, and secure handling of sensitive health data. The proposed solution is ClinicalGPT, a large language model tailored for medical tasks, trained on extensive and diverse medical datasets, and fine-tuned using parameter-efficient methods to enhance performance in medical knowledge question-answering, patient consultations, and medical record analysis.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b6\": 0.9,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents ClinicalGPT, a large language model specifically designed for medical applications. The model leverages diverse datasets of real-world medical records and employs parameter-efficient fine-tuning methods to improve performance. Key challenges include factual errors, logic inconsistencies, and lack of domain-specific expertise in existing language models.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b3\": 1,\n    \"b6\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b10\": 0.7,\n    \"b14\": 0.8,\n    \"b17\": 0.8,\n    \"b18\": 0.8,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b9\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b16\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper presents ClinicalGPT, a large language model specifically designed for medical applications. The primary challenges addressed include the need for high accuracy, interpretability, and secure handling of sensitive health data in medical AI. The model leverages extensive medical datasets and parameter-efficient fine-tuning methods. Key inspirations include the use of T5's text generation strategy, reinforcement learning with human feedback as seen in InstructGPT, and efficient training methods like LoRA.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b6\": 0.9,\n        \"b14\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.7,\n        \"b17\": 0.7,\n        \"b18\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.6,\n        \"b19\": 0.6,\n        \"b20\": 0.6\n    }\n}\n```"], "6456385ad68f896efacf20ac": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting large language models (LLMs) for domain-specific applications, particularly in the medical field. The authors propose PMC-LLaMA, a fine-tuned version of the LLaMA model, tailored specifically for medical QA tasks. The main contributions include full fine-tuning, parameter-efficient fine-tuning, and data-efficient fine-tuning on medical datasets such as PubMedQA, MedMCQA, and USMLE.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b3\": 0.9,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b6\": 0.6,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b12\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of adapting foundational language models, specifically LLaMA, to medical-specific tasks to improve performance on medical QA datasets. The authors propose further fine-tuning LLaMA on a large-scale medical corpus (PMC) and evaluate the model using various fine-tuning techniques such as full fine-tuning, parameter-efficient fine-tuning (LoRA), and data-efficient fine-tuning. The results show that the proposed PMC-LLaMA performs better than the original LLaMA on medical QA benchmarks.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b15\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b3\": 0.8,\n        \"b8\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.6,\n        \"b6\": 0.6,\n        \"b5\": 0.6,\n        \"b12\": 0.6,\n        \"b13\": 0.6,\n        \"b7\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting foundational language models like LLaMA to perform better in medical-specific tasks. It proposes PMC-LLaMA, a fine-tuned version of LLaMA on a medical corpus, and demonstrates enhanced performance on medical QA datasets. The primary inspirations come from past efforts to improve LLMs for specific domains, particularly through fine-tuning and parameter-efficient techniques.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b15\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b3\": 0.80,\n    \"b8\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.65,\n    \"b12\": 0.60,\n    \"b16\": 0.68\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting foundational language models, specifically LLaMA, to medical-specific corpora to improve performance on medical QA tasks. The proposed solution involves fine-tuning LLaMA with domain-specific data from PMC papers and evaluating the model using various fine-tuning techniques and medical QA benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b3\": 0.8,\n    \"b8\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.6,\n    \"b5\": 0.6,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b7\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting large language models (LLMs) like LLaMA to medical-specific tasks by injecting domain knowledge from medical corpora. This is achieved through fine-tuning on medical QA datasets (PubMedQA, MedMCQA, USMLE) using methods like full fine-tuning, parameter-efficient fine-tuning (LoRA), and data-efficient fine-tuning.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.8,\n    \"b3\": 0.85,\n    \"b8\": 0.75,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.65,\n    \"b12\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b13\": 0.6,\n    \"b16\": 0.55\n  }\n}\n```"], "64c78ba13fda6d7f06dba840": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling semi-dynamic variability in compiled programs, allowing for runtime flexibility in choosing between different compiled variants of code. The proposed approach, MELF (Multivariant ELF), provides a solution by incorporating multiple compile-time variants within the same binary and enabling flexible switching between them at runtime on a function/section granularity.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b14\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b1\": 0.6,\n    \"b7\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling dynamic flexibility in static binaries, specifically in cloud and heterogeneous environments. The proposed solution, MELF, allows for the inclusion and run-time switching of multiple compile-time variants within the same binary, providing semi-dynamic variability.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of providing flexible, semi-dynamic variability for compiled languages. It introduces the MELF approach, which allows multiple compile-time variants within the same binary and enables run-time switching between them. This is achieved through modifications to the linker and loader, facilitating whole-program optimization while maintaining variant flexibility.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.9,\n    \"b15\": 0.8,\n    \"b17\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.75,\n    \"b11\": 0.8,\n    \"b16\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces the MELF approach to provide semi-dynamic variability for compiled functions, enabling runtime flexibility between multiple compile-time variants within the same binary. The challenges addressed include improving performance, safety, and security while managing heterogeneous hardware and use-case environments.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b1\": 0.6,\n    \"b7\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of incorporating semi-dynamic variability for performance, debugging, and safety/security-critical features at run time, which are traditionally bound at compile time. The proposed MELF approach allows for multiple compile-time variants within the same binary and enables flexible switching between them at run time, leveraging the MMViews kernel extension.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b11\": 0.8,\n        \"b12\": 0.7,\n        \"b16\": 0.7,\n        \"b17\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b14\": 0.6,\n        \"b15\": 0.6\n    }\n}\n```"], "64c78b9f3fda6d7f06db9a87": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving robust control flow isolation in modern CPUs, particularly focusing on the Conditional Branch Predictor (CBP). The authors propose a novel software-based defense mechanism, called Half&Half, to partition the CBP and prevent side-channel attacks with minimal performance overhead.\",\n  \"Direct Inspiration\": {\n    \"b60\": 1.0,\n    \"b46\": 0.9,\n    \"b45\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b50\": 0.7,\n    \"b14\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b24\": 0.6,\n    \"b26\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper tackles the challenge of control flow isolation in modern CPUs, specifically focusing on Conditional Branch Predictor (CBP) isolation. The authors propose a novel software-based defense mechanism called Half&Half, which partitions the CBP to prevent side-channel attacks and speculation-based attacks like Spectre. This approach is implemented using LLVM and the Spectre-hardened WebAssembly compiler Swivel, demonstrating significant performance improvements over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b46\": 0.9,\n    \"b60\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b50\": 0.7,\n    \"b45\": 0.7,\n    \"b16\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b20\": 0.6,\n    \"b87\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of achieving robust control flow isolation in modern CPUs, specifically focusing on the Conditional Branch Predictor (CBP). The proposed algorithm, Half&Half, partitions the CBP into two isolated domains using a novel software-based approach, which incurs minimal performance overhead. This approach is implemented on top of LLVM and a Spectre-hardened WebAssembly compiler, demonstrating significant performance improvements over existing defenses.\",\n    \"Direct Inspiration\": {\n        \"b45\": 0.95,\n        \"b46\": 0.95,\n        \"b60\": 0.90\n    },\n    \"Indirect Inspiration\": {\n        \"b20\": 0.85,\n        \"b27\": 0.80,\n        \"b50\": 0.80,\n        \"b25\": 0.75,\n        \"b9\": 0.70,\n        \"b11\": 0.70\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.65,\n        \"b24\": 0.60,\n        \"b26\": 0.60,\n        \"b35\": 0.60,\n        \"b82\": 0.55,\n        \"b105\": 0.55\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of control flow isolation in modern CPUs, specifically focusing on isolating the Conditional Branch Predictor (CBP) to prevent side-channel and transient execution attacks. The proposed solution, named Half&Half, is a novel software-based defense that partitions the CBP to mitigate these attacks with minimal performance impact. This work involves comprehensive reverse engineering of Intel's branch predictors and implements the defense mechanism on top of the LLVM compiler and Swivel, a WebAssembly compiler.\",\n    \"Direct Inspiration\": [\"b46\", \"b60\"],\n    \"Indirect Inspiration\": [\"b24\", \"b26\", \"b27\", \"b45\"],\n    \"Other Inspiration\": [\"b50\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of ensuring control flow isolation on modern CPUs to prevent attacks like Spectre. It introduces a novel software-based defense called Half&Half that partitions the Conditional Branch Predictor (CBP) with minimal performance overhead. The approach is implemented on top of LLVM and Swivel compilers, showing significant improvements in efficiency over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b27\": 1.0,\n    \"b46\": 1.0,\n    \"b60\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b20\": 0.8,\n    \"b45\": 0.8,\n    \"b50\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b19\": 0.7,\n    \"b31\": 0.7,\n    \"b48\": 0.7,\n    \"b69\": 0.7,\n    \"b87\": 0.7\n  }\n}\n```"], "6389d6fb90e50fcafdffbdc3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of the Memory Wall and proposes the Page-size Propagation Module (PPM) to enable safe prefetching beyond 4KB physical page boundaries by leveraging the presence of large pages. This approach aims to improve the effectiveness of spatial cache prefetchers operating in the physical address space without requiring modifications to the prefetcher's design.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b14\": 1,\n    \"b15\": 1,\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b20\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b32\": 0.6,\n    \"b59\": 0.7,\n    \"b72\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the limitations of existing cache prefetchers, specifically their inability to prefetch beyond 4KB physical page boundaries due to issues with physical address contiguity and security vulnerabilities. The proposed solution is the Page-size Propagation Module (PPM), which propagates page size information to lower-level cache prefetchers, enabling them to safely prefetch beyond 4KB boundaries when the accessed block resides in a large page. This approach leverages the prevalence of large pages in modern systems to improve prefetching effectiveness and overall system performance without modifying the prefetcher's design.\",\n  \"Direct Inspiration\": [\"b13\", \"b14\", \"b15\", \"b16\"],\n  \"Indirect Inspiration\": [\"b19\", \"b20\", \"b28\", \"b29\"],\n  \"Other Inspiration\": [\"b32\", \"b59\", \"b66\", \"b67\", \"b68\", \"b69\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing cache prefetchers, particularly the inability to prefetch across 4KB physical page boundaries due to security and performance concerns. It proposes the Page-size Propagation Module (PPM), an architectural scheme that enables safe prefetching beyond 4KB boundaries by making cache prefetchers aware of page sizes. The paper demonstrates that leveraging large pages can significantly improve prefetching performance and introduces a composite prefetcher design called Pref-PSA-SD that dynamically selects between 4KB and large page prefetching.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b14\": 1,\n    \"b15\": 1,\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.9,\n    \"b20\": 0.9,\n    \"b29\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.8,\n    \"b66\": 0.7,\n    \"b67\": 0.7,\n    \"b68\": 0.7,\n    \"b69\": 0.7,\n    \"b72\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the Memory Wall challenge by proposing a spatial prefetching technique that allows safe prefetching beyond 4KB physical page boundaries using Page-size Propagation Module (PPM). This technique leverages modern systems' support for large pages to improve prefetching efficiency and overall system performance.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b14\": 1.0,\n    \"b15\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b20\": 0.8,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b32\": 0.6,\n    \"b59\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the limitations imposed by the 'Memory Wall' and the constraints of existing cache prefetchers that cannot efficiently prefetch beyond 4KB physical page boundaries due to security and performance issues. The proposed solution, the Page-size Propagation Module (PPM), enables safe prefetching beyond these boundaries by propagating page size information to lower-level cache prefetchers, thus improving performance without modifying the prefetcher's design.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b14\": 0.9,\n    \"b15\": 0.9,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.7,\n    \"b30\": 0.7,\n    \"b31\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b59\": 0.6,\n    \"b66\": 0.6,\n    \"b67\": 0.6,\n    \"b68\": 0.6\n  }\n}\n```"], "6426ed4590e50fcafd444b1b": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the long-latency off-chip memory access bottleneck in modern computer systems, which leads to severe performance loss when the processor stalls during off-chip misses. The proposed solution, Dual Stream Data Prefetcher (DSDP), aims to improve prefetching accuracy by using PC localization and ensure coverage by preserving cross-PC relationships through spatial localization. Key contributions include treating localization as an event, proposing a new classification method for component prefetchers, and developing DSDP to learn from both PC and spatial localized streams, achieving significant performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b16\": 1,\n    \"b20\": 1,\n    \"b29\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b14\": 0.9,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the long-latency off-chip memory access bottleneck in modern computer systems, proposing the Dual Stream Data Prefetcher (DSDP) to improve prefetching accuracy and coverage. DSDP leverages both PC and spatial localization streams to enhance prefetching by treating localization as an event rather than a filtering method. Key contributions include a new prefetching component classification method, and a dual-stream prefetcher that improves system performance by 41.9% over a baseline with no prefetcher.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b16\": 0.85,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.75,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.65,\n    \"b7\": 0.65,\n    \"b23\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of long-latency off-chip memory access in modern computer systems. It introduces the Dual Stream Data Prefetcher (DSDP), a new spatial prefetcher that tracks high probability events on both the PC localized stream and the spatial localized stream. The primary challenges include balancing coverage and accuracy in prefetching. The proposed DSDP improves system performance by ensuring both high coverage and accuracy through cooperative prefetching among different streams.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b16\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b7\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the long-latency off-chip memory access, which causes performance bottlenecks in modern computer systems. The proposed solution, Dual Stream Data Prefetcher (DSDP), aims to improve both coverage and accuracy of prefetching by learning from both PC localized streams and spatial localized streams. This approach mitigates the loss of prefetch opportunities and ensures high performance gains by tracking high-probability events on different streams.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b16\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b7\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of long-latency off-chip memory access in modern computer systems, specifically focusing on the balance between coverage and accuracy in hardware prefetching. The proposed Dual Stream Data Prefetcher (DSDP) improves both coverage and accuracy by learning from both PC localized and spatial localized streams, addressing the limitations of previous methods that treated localization as a filtering method.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b16\": 0.9,\n    \"b14\": 0.8,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b23\": 0.7,\n    \"b29\": 0.75\n  },\n  \"Other Inspiration\": {}\n}\n```"], "64e432bf3fda6d7f0600af81": ["```json\n{\n  \"Summary\": \"The paper addresses the primary challenges in Graph Neural Networks (GNNs) such as over-fitting, over-smoothing, and vanishing gradients, particularly when GNNs go deeper. The proposed solution, referred to as the Snowflake Hypothesis (SnoH), involves element-level adjacency matrix pruning to create unique receptive fields for each node, thus mitigating the over-smoothing issue and enabling the training of deeper GNNs. The paper also details the implementation of two versions of SnoH, extensive experimental evaluations, and comparisons with existing methods.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b6\": 1.0,\n    \"b27\": 1.0,\n    \"b29\": 1.0,\n    \"b39\": 1.0,\n    \"b48\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b15\": 0.8,\n    \"b18\": 0.8,\n    \"b45\": 0.8,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b22\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b41\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the long-standing problems of over-fitting, over-smoothing, and vanishing gradients in Graph Neural Networks (GNNs), especially when GNNs go deeper. The proposed solution, the Snowflake Hypothesis (SnoH), involves node-specific receptive field pruning. This approach is intended to mitigate over-smoothing and improve the interpretability and performance of deep GNNs.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.9,\n        \"b27\": 0.85,\n        \"b29\": 0.85,\n        \"b48\": 0.9,\n        \"b5\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.8,\n        \"b39\": 0.8,\n        \"b41\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.7,\n        \"b23\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of over-fitting, over-smoothing, and vanishing gradients in Graph Neural Networks (GNNs) as they go deeper. The proposed solution, termed the Snowflake Hypothesis (SnoH), hypothesizes that each node within a graph should possess its unique receptive field, actualized through element-level adjacency matrix pruning. This approach aims to mitigate the over-smoothing issue and enable the training of deeper GNNs by using gradient and cosine distance-based pruning methods. The paper integrates SnoH with various deep GNN models and compares it with existing methods like DropEdge and graph lottery ticket algorithms, showing improved performance and interpretability.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.95,\n    \"b6\": 0.90,\n    \"b27\": 0.85,\n    \"b48\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.75,\n    \"b25\": 0.70,\n    \"b39\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.60,\n    \"b29\": 0.55,\n    \"b41\": 0.50,\n    \"b32\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in training deep Graph Neural Networks (GNNs), particularly over-fitting, over-smoothing, and vanishing gradients. It proposes the Snowflake Hypothesis (SnoH), which involves node-level adjacency matrix pruning to give each node a unique receptive field, aiding in mitigating over-smoothing. The paper evaluates this hypothesis using various training schemes and datasets and compares it with existing approaches.\",\n  \"Direct Inspiration\": [\"b27\", \"b3\", \"b30\", \"b6\", \"b39\"],\n  \"Indirect Inspiration\": [\"b24\", \"b20\", \"b42\", \"b0\", \"b53\", \"b51\", \"b17\"],\n  \"Other Inspiration\": [\"b48\", \"b5\", \"b41\", \"b18\", \"b45\", \"b10\", \"b31\", \"b57\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of over-fitting, over-smoothing, and vanishing gradients in deep Graph Neural Networks (GNNs). It introduces the Snowflake Hypothesis (SnoH), proposing unique receptive fields for each node through element-level adjacency matrix pruning to mitigate these issues. The proposed methods, SnoHv1 and SnoHv2, are tested across various training algorithms, GNN models, and datasets, demonstrating improved performance and scalability.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b6\": 1.0,\n    \"b27\": 1.0,\n    \"b29\": 1.0,\n    \"b39\": 1.0,\n    \"b48\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b22\": 0.8,\n    \"b30\": 0.8,\n    \"b41\": 0.8,\n    \"b45\": 0.8,\n    \"b56\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b15\": 0.5,\n    \"b32\": 0.5,\n    \"b42\": 0.5\n  }\n}\n```"], "64c78b993fda6d7f06db5b90": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the scalability issue of knowledge graph completion (KGC) methods when applied to large-scale knowledge graphs (KGs). The proposed algorithm, ReSKGC, enhances the seq2seq KGC model by incorporating a retrieval mechanism to fetch relevant triplets from the KG to aid in generating the missing entity, thus addressing the limitations of implicit reasoning and catastrophic forgetting in existing methods.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b17\": 0.85,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b24\": 0.65,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the scalability issue in Knowledge Graph Completion (KGC) methods due to the large size of real-world Knowledge Graphs (KGs). The proposed algorithm, ReSKGC, enhances seq2seq KGC by integrating a retrieval module that leverages relevant triplets from the KG to improve the generation of missing entities. This approach aims to overcome the limitation of implicit reasoning in existing methods by directly utilizing retrieved triplets for inference, thus improving performance on large-scale datasets.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b17\": 0.9,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the scalability issue of Knowledge Graph Completion (KGC) methods when dealing with large-scale knowledge graphs. The proposed solution, ReSKGC, enhances sequence-to-sequence (seq2seq) models by incorporating a retrieval module to utilize relevant triplets from the knowledge graph, thus overcoming the limitations of existing methods that rely solely on model parameters for inference.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b16\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.75,\n    \"b15\": 0.7,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scalability in Knowledge Graph Completion (KGC) by proposing a Retrieval-enhanced Seq2seq KG Completion model (ReSKGC). The model combines free-text retrieval with seq2seq generation to enhance the prediction of missing entities in large-scale knowledge graphs.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b17\": 0.85,\n    \"b21\": 0.8,\n    \"b16\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b28\": 0.65,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b26\": 0.5,\n    \"b22\": 0.5,\n    \"b0\": 0.5,\n    \"b19\": 0.5,\n    \"b7\": 0.5,\n    \"b15\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the scalability challenge in Knowledge Graph Completion (KGC) for large-scale knowledge graphs (KGs) by proposing a Retrieval-enhanced Seq2seq KG Completion model (ReSKGC). The proposed model combines retrieval mechanisms with sequence-to-sequence (seq2seq) models to enhance the generation of missing entities by utilizing relevant triplets from the KG.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b17\": 1.0,\n    \"b21\": 1.0,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```"], "6323e96890e50fcafd8a40e6": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Limited chemical space of existing compounds for virtual screening.\",\n      \"Dependence on experimental structures for structure-based drug design.\",\n      \"Computational intensity of molecular generation steps.\"\n    ],\n    \"inspirations\": [\n      \"Using deep learning-based generative models to explore large chemical spaces.\",\n      \"Incorporating both ligand and target structures into generative algorithms.\",\n      \"Employing protein sequence information and DL architectures for efficient drug-target affinity prediction.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b8\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b6\", \"b7\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b5\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in de novo drug design, particularly the dependency on experimental structures of ligand-target complexes and the computational expense of 3D descriptors in molecular generation. The authors propose a reinforcement learning model using 1D protein sequences and molecular SMILES data to predict drug-target affinity efficiently. The model is trained with datasets such as Davis, Kiba, and BindingDB, and its effectiveness is demonstrated through the generation of potential inhibitors for various targets.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b7\": 1,\n    \"b8\": 1\n  }, \n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in de novo molecular design for drug discovery, specifically the limitations of current structure-based methods which require experimental structures of ligand-target complexes and are computationally intensive. The authors propose a reinforcement learning (RL) model using 1D protein sequences and molecular SMILES data to predict drug-target affinity (DTA) efficiently and generate potential inhibitors.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b7\": 1,\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of current structure-based de novo drug design methods, which require experimental structures of ligand-target complexes and involve computationally intensive processes. The authors propose a protein sequence-based reinforcement learning (RL) model that utilizes convolutional neural networks (CNNs) trained on 1D protein sequences and molecular SMILES data to predict drug-target affinity (DTA) efficiently. This approach aims to generate potential inhibitors for various targets with high computational efficiency and explore new chemical spaces.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b7\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include limited chemical space in virtual screening, dependency on experimental structures for structure-based drug design, and computational intensity of molecular generation steps. The proposed algorithm leverages a reinforcement learning (RL) model using a protein sequence-based drug-target affinity (DTA) predictive function, significantly accelerating the interaction prediction and generating potential active molecules efficiently.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b8\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b2\": 0.75,\n    \"b3\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7\n  }\n}\n```"], "64a407dcd68f896efaf1ba6d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately simulating wrong-path execution in functional-first processor simulators. The authors propose three techniques for modeling the impact of wrong-path execution, evaluate their accuracy and speed, and conclude that the convergence exploitation technique provides the best balance between accuracy and simulation speed.\",\n  \"Direct Inspiration\": [\"b1\", \"b21\", \"b11\"],\n  \"Indirect Inspiration\": [\"b23\", \"b9\", \"b7\"],\n  \"Other Inspiration\": [\"b16\", \"b10\", \"b25\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of accurately modeling wrong-path execution in functional-first simulators, which is essential for simulating modern processors. It proposes three novel techniques to improve wrong-path execution modeling, focusing on instruction reconstruction, full emulation, and memory address reconstruction using convergence detection. The goal is to enhance simulation accuracy while maintaining reasonable simulation speed.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b23\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b16\": 0.8,\n        \"b32\": 0.8,\n        \"b31\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.7,\n        \"b11\": 0.7,\n        \"b21\": 0.7,\n        \"b9\": 0.7,\n        \"b7\": 0.7,\n        \"b22\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the challenges in accurately and efficiently simulating processor performance, particularly focusing on wrong-path execution in functional-first simulators. It proposes three novel techniques to model wrong-path execution and evaluates their accuracy and simulation speed.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 0.9,\n    \"b5\": 0.85,\n    \"b10\": 0.9,\n    \"b16\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b22\": 0.75,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b9\": 0.7,\n    \"b11\": 0.75,\n    \"b25\": 0.7,\n    \"b26\": 0.7,\n    \"b31\": 0.7,\n    \"b32\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of accurately and efficiently simulating processor performance, especially under the context of wrong-path execution. It proposes three novel techniques to model the impact of wrong-path execution in functional-first simulators: instruction reconstruction, functional emulation of the wrong path, and memory address reconstruction using convergence detection. The paper evaluates these techniques, showing that the convergence detection method significantly improves accuracy while maintaining reasonable simulation speed.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b23\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.65,\n    \"b16\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately modeling wrong-path execution in functional-first simulators, which is crucial for emerging workloads that have irregular branching patterns and higher cache miss rates. It proposes three novel techniques to model wrong-path execution, evaluates their accuracy and speed, and shows that the convergence exploitation technique significantly improves accuracy with limited simulation speed overhead.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b23\": 0.7,\n    \"b10\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```"], "63ed9f3290e50fcafd0f10bd": ["```json\n{\n  \"Summary\": \"The paper proposes DrugGEN, a new de novo drug design system leveraging GANs, transformers, and graph representation learning to generate target-specific small molecules. The main challenges addressed are the limitations of conventional drug design, such as high costs, long development times, and the complexity of molecular-target interactions.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b10\": 0.9,\n    \"b11\": 0.8,\n    \"b43\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b19\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.6,\n    \"b33\": 0.6,\n    \"b44\": 0.6,\n    \"b45\": 0.6,\n    \"b46\": 0.6,\n    \"b47\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in conventional drug design, such as long development times, high costs, and high failure rates due to toxicity or inefficacy. It proposes an AI-driven method called DrugGEN for de novo drug design, which is an end-to-end framework that generates target-specific small molecules using GANs, transformers, and graph representation learning. The main contributions include using a two-stage GAN architecture to generate novel structures for a given target and incorporating protein features to guide the generation of target-specific molecules.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b10\": 0.9,\n    \"b11\": 0.8,\n    \"b32\": 0.8,\n    \"b43\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b44\": 0.6,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b14\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5,\n    \"b29\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.5,\n    \"b33\": 0.5,\n    \"b34\": 0.5,\n    \"b35\": 0.5,\n    \"b36\": 0.5,\n    \"b37\": 0.5,\n    \"b38\": 0.5,\n    \"b39\": 0.5,\n    \"b40\": 0.5,\n    \"b41\": 0.5,\n    \"b42\": 0.5,\n    \"b46\": 0.5,\n    \"b47\": 0.5,\n    \"b48\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of discovering novel, target-specific small molecule drugs. It proposes DrugGEN, a GAN-based system enhanced with transformer and graph representation learning, for generating target-specific drug candidates efficiently.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b9\": 1,\n    \"b10\": 1,\n    \"b11\": 1,\n    \"b32\": 1,\n    \"b43\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b45\": 0.8,\n    \"b47\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b46\": 0.6,\n    \"b44\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Developing new drugs is lengthy and expensive, with high failure rates due to toxicity or low efficacy. Structural diversity in small-molecule drugs is low, limiting their ability to target diverse biomolecules. AI-driven methods like GANs and graph representation learning are proposed to design novel, target-specific small molecules.\",\n    \"algorithm\": \"The proposed DrugGEN system is an end-to-end framework that uses two serially connected GANs along with graph transformer encoders and decoders to generate target-specific small molecules.\"\n  },\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b9\": 0.9,\n    \"b11\": 0.9,\n    \"b32\": 0.9,\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.8,\n    \"b46\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b44\": 0.7,\n    \"b47\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in drug discovery, particularly the need for novel, structurally diverse small molecules to target understudied proteins. It proposes DrugGEN, a GAN-based framework incorporating transformers and graph representation learning to generate target-specific small molecules.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.95,\n    \"b32\": 0.90,\n    \"b33\": 0.85,\n    \"b43\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.75,\n    \"b44\": 0.70,\n    \"b45\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.65\n  }\n}\n```"], "642702aa90e50fcafd5de13d": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in reducing app launch latency by using a novel app prefetcher named Paralfetch. It tackles issues such as accurate tracking of launch-related blocks, increasing prefetch throughput, and minimizing I/O contention. The main contributions include a file-system-level block dependency check, a new I/O reordering method called metadata shift, and tailored overlapping of application execution with prefetching.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b8\": 0.7,\n    \"b10\": 0.7,\n    \"b27\": 0.7,\n    \"b35\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b7\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b14\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b22\": 0.6,\n    \"b24\": 0.6,\n    \"b26\": 0.6,\n    \"b31\": 0.6,\n    \"b33\": 0.6,\n    \"b34\": 0.6,\n    \"b38\": 0.6,\n    \"b40\": 0.6,\n    \"b42\": 0.6,\n    \"b45\": 0.6,\n    \"b46\": 0.6,\n    \"b47\": 0.6,\n    \"b48\": 0.6,\n    \"b49\": 0.6,\n    \"b52\": 0.6,\n    \"b54\": 0.6,\n    \"b56\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the high latency issues during app launches due to cold starts and suboptimal prefetching. Paralfetch, a novel algorithm, introduces accurate tracking of launch-related blocks, pre-scheduling to increase prefetch throughput, and overlapping application execution with prefetching to mitigate I/O contention.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b23\": 0.85,\n    \"b48\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.7,\n    \"b11\": 0.6,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.5,\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of reducing app launch latency, focusing on the performance limitations of flash storage, the increasing complexity of apps, and the underutilization of parallelism in modern hardware. Paralfetch, the proposed solution, introduces accurate tracking of launch-related blocks, pre-scheduling to increase prefetch throughput, and tailored overlapping of application execution with prefetching to mitigate I/O contention.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b5\": 0.85,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.75,\n    \"b28\": 0.7,\n    \"b35\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the latency of launching apps due to the performance limitations of flash storage, the complexity of apps, and the underutilization of parallelism during app launch. The proposed algorithm, Paralfetch, addresses these issues by accurately tracking launch-related blocks, pre-scheduling these blocks to increase prefetch throughput, and tailoring the overlapping of application execution with prefetching.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b48\": 0.9,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b28\": 0.6,\n    \"b36\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b31\": 0.5,\n    \"b38\": 0.5,\n    \"b7\": 0.5,\n    \"b18\": 0.5,\n    \"b42\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the latency of launching large apps due to the performance limitations of flash storage, the complexity of apps requiring higher-level programming languages, and the inability to exploit existing sources of parallelism during app launches. The paper proposes Paralfetch, an algorithm that addresses these issues by accurately tracking launch-related blocks, pre-scheduling blocks to increase prefetch throughput, and tailoring the overlapping of application execution with prefetching. Paralfetch is evaluated on various devices and shows significant improvements in launch performance.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b40\": 0.9,\n    \"b48\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b11\": 0.6,\n    \"b26\": 0.6,\n    \"b31\": 0.6,\n    \"b42\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```"], "63f2e4aa90e50fcafd2820a9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting drug-drug interactions (DDIs) and drug-target interactions using a multimodal graph approach. The authors propose using variational graph autoencoders (VGAE) to learn representations of drugs and proteins on a multimodal graph and employ these learned representations to predict interactions. The novel method combines drug molecular representations with latent representations to enhance the model's performance. The study demonstrates that VGAE can outperform traditional graph autoencoders and handcrafted feature-based methods in predicting DDIs.\",\n  \"Direct Inspiration\": {\n    \"b41\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.8,\n    \"b23\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b17\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of drug repurposing and predicting drug-drug interactions (DDIs) using machine learning models, specifically focusing on multimodal graphs and variational graph autoencoders (VGAE). The primary challenge is the complexity and infeasibility of clinical testing for all possible drug combinations. The proposed solution leverages VGAE to learn representations of drugs and proteins on multimodal graphs, aiming for superior performance in predicting DDIs compared to traditional methods.\",\n    \"Direct Inspiration\": {\n        \"b41\": 0.9,\n        \"b16\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b23\": 0.7,\n        \"b34\": 0.6,\n        \"b39\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.5,\n        \"b17\": 0.5,\n        \"b18\": 0.45\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting drug-drug and drug-target interactions, especially in the context of polypharmacy, using a multimodal graph neural network approach. It proposes using a variational graph autoencoder (VGAE) to learn drug and protein representations from multimodal graphs, aiming to improve prediction accuracy over traditional methods. The approach leverages drug molecular representations concatenated with latent representations to enhance performance.\",\n  \"Direct Inspiration\": {\n    \"b41\": 1.0,\n    \"b40\": 0.9,\n    \"b42\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.75,\n    \"b34\": 0.7,\n    \"b39\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b16\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the high cost and time required to develop new drugs, and the need to predict drug-drug interactions (DDIs) for polypharmacy. The proposed algorithm leverages variational graph autoencoders (VGAE) to learn drug and protein representations on multimodal graphs, predicting node-pair interactions such as drug-pair polypharmacy side effects. The core contribution is using VGAE to attain superior performance in predicting DDIs on multimodal networks, outperforming traditional methods.\",\n  \"Direct Inspiration\": {\n    \"b41\": 1.0,\n    \"b40\": 0.9,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.75,\n    \"b34\": 0.7,\n    \"b31\": 0.65,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the high cost and time-consuming nature of drug development by focusing on drug repurposing and polypharmacy, particularly through the prediction of drug-drug interactions (DDIs) using multimodal graph neural networks (GNNs) and deep generative models. The proposed method utilizes variational graph autoencoders (VGAEs) for learning drug and protein representations, enhancing prediction capabilities over traditional methods.\",\n  \"Direct Inspiration\": {\n    \"b41\": 1.0,\n    \"b16\": 0.9,\n    \"b39\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b31\": 0.7,\n    \"b34\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```"], "62466dd35aee126c0f8b7b62": ["```json\n{\n  \"Summary\": \"The paper introduces E(3) Equivariant Diffusion Models (EDMs) for molecular generation in 3D space, addressing challenges of scalability and efficiency compared to existing methods. The proposed model leverages equivariant layers to handle molecular symmetries and performs better than previous models like E-NF and autoregressive models.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.95,\n    \"b30\": 0.9,\n    \"b42\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.75,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces E(3) Equivariant Diffusion Models (EDMs) that generate molecules in 3D space. The primary challenge addressed is the efficient generation of stable molecules without requiring specific atom ordering, unlike autoregressive models. EDMs leverage geometric symmetries (E(3)) and are rotation equivariant, which enhances their efficiency and scalability relative to previous models such as continuous-time normalizing flows.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b18\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b20\": 0.9,\n    \"b42\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b31\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces E(3) Equivariant Diffusion Models (EDMs) designed to generate molecules in 3D space. The primary challenges addressed include leveraging geometric symmetries (translations, rotations, reflections) for better generalization, overcoming inefficiencies in existing autoregressive models and continuous-time normalizing flows, and ensuring efficient training and scalability. The proposed method eliminates the need for atom ordering and achieves superior performance in molecule stability and log-likelihood compared to previous models.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1.0,\n    \"b42\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b18\": 0.8,\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces E(3) Equivariant Diffusion Models (EDMs) for generating molecules in 3D space. The primary challenges addressed include leveraging geometric symmetries in molecular data and improving efficiency over autoregressive models and normalizing flows. The key contributions are the introduction of an equivariant denoising diffusion model for atom coordinates and categorical features, probabilistic analysis for likelihood computation, and demonstrating superior performance in log-likelihood and molecule stability.\",\n  \"Direct Inspiration\": [\"b10\", \"b30\", \"b42\"],\n  \"Indirect Inspiration\": [\"b15\", \"b18\"],\n  \"Other Inspiration\": [\"b3\", \"b31\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces the E(3) Equivariant Diffusion Models (EDMs) to generate molecules in 3D space. The primary challenges addressed include leveraging geometric symmetries for better generalization and efficient training compared to existing methods like autoregressive models and E-NFs. The proposed EDMs are novel in that they do not require specific atom ordering and significantly outperform previous models in terms of log-likelihood and molecule stability.\",\n    \"Direct Inspiration\": [\"b30\", \"b42\"],\n    \"Indirect Inspiration\": [\"b20\", \"b31\"],\n    \"Other Inspiration\": [\"b15\", \"b18\"]\n}\n```"], "6459ac57d68f896efa657eec": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of low-resource text classification by proposing a novel approach called Graph-Grounded Pre-training and Prompting (G2P2). The approach aims to capture fine-grained textual semantics while leveraging graph structure information. It involves graph-grounded contrastive pre-training and using prompting instead of fine-tuning for low-resource classification. The paper highlights the limitations of current methods, such as the large model size of PLMs and the rudimentary treatment of text features in GNNs, and seeks to overcome these with its novel dual-modal embedding space and prompting strategies.\",\n  \n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b17\": 0.85,\n    \"b16\": 0.8,\n    \"b49\": 0.75,\n    \"b35\": 0.7\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b13\": 0.65,\n    \"b14\": 0.65,\n    \"b23\": 0.6\n  },\n  \n  \"Other Inspiration\": {\n    \"b18\": 0.55,\n    \"b36\": 0.5,\n    \"b47\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of low-resource text classification by proposing a novel model called Graph-Grounded Pre-training and Prompting (G2P2), which jointly pre-trains text and graph encoders and uses prompting for low-resource classification. The main contributions include graph-grounded contrastive pre-training, zero-shot and few-shot classification with handcrafted discrete prompts and continuous prompt tuning, and context-based initialization for prompt tuning.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b17\": 1.0,\n    \"b23\": 1.0,\n    \"b35\": 0.9,\n    \"b47\": 1.0,\n    \"b56\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b49\": 0.8,\n    \"b13\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b16\": 0.6,\n    \"b57\": 0.6,\n    \"b20\": 0.5,\n    \"b42\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of low-resource text classification by proposing a novel approach called Graph-Grounded Pre-training and Prompting (G2P2). The key challenges include capturing fine-grained textual semantics while leveraging graph structure information and augmenting low-resource text classification with a jointly pre-trained graph-text model. The proposed method involves graph-grounded contrastive pre-training and prompt-tuning.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b17\": 1,\n    \"b23\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b36\": 0.8,\n    \"b47\": 0.7,\n    \"b57\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b18\": 0.6,\n    \"b49\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of low-resource text classification by integrating graph structures with text data. The authors propose a novel algorithm called Graph-Grounded Pre-training and Prompting (G2P2), which employs graph interaction-based contrastive strategies for pre-training and uses prompting for downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b17\": 1,\n    \"b23\": 1,\n    \"b35\": 1,\n    \"b57\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b36\": 0.8,\n    \"b49\": 0.8,\n    \"b56\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b29\": 0.6,\n    \"b47\": 0.6,\n    \"b50\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of low-resource text classification by proposing a novel model called Graph-Grounded Pre-training and Prompting (G2P2). The key challenges include capturing fine-grained textual semantics while leveraging graph structure information jointly, and augmenting low-resource text classification using a jointly pre-trained graph-text model. The proposed solution involves graph-grounded contrastive pre-training and prompt-based adaptation for zero-shot and few-shot classification scenarios.\",\n  \"Direct Inspiration\": [\"b2\", \"b17\", \"b18\", \"b23\", \"b35\"],\n  \"Indirect Inspiration\": [\"b13\", \"b14\", \"b36\", \"b47\", \"b49\", \"b50\"],\n  \"Other Inspiration\": [\"b16\", \"b29\", \"b57\"]\n}\n```"], "64a407dcd68f896efaf1ba4e": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the front-end bottleneck in modern server workloads due to increased instruction memory footprints and the disparity between processor and memory performance. The proposed solution involves characterizing and optimizing software instruction prefetching techniques in the context of a realistic decoupled front-end with Fetch-Directed Prefetching (FDP). The authors evaluate the effectiveness of the AsmDB software instruction prefetcher and discuss the impact of different front-end states on its performance.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b7\": 0.8,\n    \"b12\": 0.85,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of the front-end bottleneck in modern server workloads due to instruction memory performance issues. It evaluates the performance of a state-of-the-art software instruction prefetcher, AsmDB, in both conservative and industry-standard Fetch-Directed Prefetching (FDP) implementations. The work characterizes the stalling behaviors and proposes potential solutions to optimize software prefetching mechanisms.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.9,\n        \"b7\": 0.8,\n        \"b17\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.75,\n        \"b18\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.65,\n        \"b19\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of front-end bottlenecks in modern server workloads due to increased instruction memory footprints and proposes improved techniques for software instruction prefetching in decoupled front-ends. The main contributions include characterizing front-end stalling behavior, evaluating state-of-the-art software prefetchers like AsmDB, and discussing future research directions.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b7\": 1,\n    \"b12\": 1,\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the front-end bottleneck in modern processors caused by the memory wall and the large instruction memory footprints of server workloads. It evaluates the effectiveness of software instruction prefetching techniques, particularly AsmDB, in a realistic decoupled front-end environment. The paper highlights the limitations of software prefetching when combined with Fetch-Directed Prefetching (FDP) and proposes optimizations to reduce the overhead of inserting software prefetches.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b7\": 0.8,\n    \"b12\": 0.9,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the front-end bottleneck caused by large instruction memory footprints in server workloads and the disparity between processor and memory performance. The algorithm proposed involves characterizing the side-effects of decoupled front-ends in the context of software instruction prefetching, evaluating state-of-the-art software prefetching techniques in a realistic simulation environment, and exploring potential solutions to improve performance.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b12\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```"], "64be63403fda6d7f063e57a7": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of solving s-rectangular Robust Markov Decision Processes (RMDPs) by proposing a novel Double-Loop Robust Policy Gradient (DRPG) method. The primary contributions include a new policy gradient method inspired by double-loop algorithms for saddle point problems, and proving the global convergence of this method for s-rectangular RMDPs.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b35\": 1.0,\n    \"b51\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b7\": 0.8,\n    \"b22\": 0.7,\n    \"b71\": 0.7,\n    \"b40\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.5,\n    \"b66\": 0.5,\n    \"b43\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of solving robust Markov decision processes (RMDPs) with unknown transition parameters by proposing a novel Double-Loop Robust Policy Gradient (DRPG) method. This method is designed to find globally optimal policies for s-rectangular RMDPs by iterating over nested loops to update policies and compute the worst-case transition probabilities. The proposed method aims to overcome the computational difficulties and non-differentiability issues inherent in RMDPs.\",\n    \"Direct Inspiration\": [\"b24\", \"b35\", \"b51\"],\n    \"Indirect Inspiration\": [\"b50\", \"b64\", \"b71\", \"b23\", \"b40\", \"b22\"],\n    \"Other Inspiration\": [\"b0\", \"b7\", \"b72\", \"b45\", \"b47\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of model parameter uncertainty in Markov Decision Processes (MDPs) by proposing a new policy gradient method, the Double-Loop Robust Policy Gradient (DRPG), for solving s-rectangular Robust MDPs (RMDPs). The DRPG method is inspired by double-loop algorithms for saddle point problems and involves an outer loop for policy updates and an inner loop for optimizing the worst-case transition probabilities. The paper highlights the convergence guarantees and scalability of the proposed method.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b35\": 0.8,\n    \"b51\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b40\": 0.7,\n    \"b71\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the uncertainty in model parameters, specifically the transition function, in Markov decision processes (MDPs). The paper proposes a new policy gradient method named Double-Loop Robust Policy Gradient (DRPG) for solving s-rectangular Robust Markov Decision Processes (RMDPs). The contributions include an algorithm that guarantees global convergence to an optimal policy and a method to handle the non-differentiability and non-convexity of the RMDP return.\",\n    \"Direct Inspiration\": [\"b24\", \"b35\", \"b51\"],\n    \"Indirect Inspiration\": [\"b0\", \"b7\", \"b22\", \"b71\", \"b23\", \"b40\"],\n    \"Other Inspiration\": [\"b41\", \"b66\", \"b13\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of model ambiguity in Markov decision processes (MDPs) and proposes a new Double-Loop Robust Policy Gradient (DRPG) method for solving s-rectangular Robust MDPs (RMDPs). This method aims to optimize policies by addressing worst-case transition probabilities and ensuring global convergence.\",\n  \"Direct Inspiration\": [\"b24\", \"b35\", \"b51\"],\n  \"Indirect Inspiration\": [\"b0\", \"b7\", \"b22\", \"b71\"],\n  \"Other Inspiration\": [\"b23\", \"b40\", \"b66\"]\n}\n```"], "6326f71890e50fcafdd04899": ["```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper involve predicting drug-drug interactions (DDI) and their types using an attributed heterogeneous graph embedding and deep learning approach. The proposed algorithm integrates various drug features and utilizes a GNN model to generate drug embedding vectors, which are then used in a fully connected neural network to predict DDI types.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.9,\n        \"b4\": 0.9,\n        \"b5\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.7,\n        \"b19\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.6,\n        \"b40\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is the prediction of DDI-associated events using an attributed heterogeneous graph embedding and deep learning approach. The authors propose a novel framework that integrates different drug properties to create drug embedding vectors, which are then used in a fully connected neural network for predicting drug interaction types. Key inspirations include previous works on drug-drug similarity matrices, matrix factorization methods, network-based methods, and deep learning techniques, particularly Graph Neural Networks (GNNs).\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.95,\n    \"b4\": 0.90,\n    \"b39\": 0.88,\n    \"b40\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.80,\n    \"b16\": 0.75,\n    \"b17\": 0.73,\n    \"b27\": 0.70,\n    \"b28\": 0.68\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.65,\n    \"b8\": 0.60,\n    \"b18\": 0.55,\n    \"b19\": 0.50,\n    \"b21\": 0.48\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper revolve around predicting drug-drug interactions (DDIs) and their types using an attributed heterogeneous graph embedding combined with a deep learning approach. The proposed method integrates various drug features to create a comprehensive representation of drugs and their interactions, employing a new GNN model for embedding generation and a fully connected neural network for prediction.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b19\": 0.9,\n    \"b46\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b39\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b21\": 0.7,\n    \"b23\": 0.7,\n    \"b27\": 0.7,\n    \"b28\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting drug-drug interactions (DDI) and their types using an attributed heterogeneous graph embedding and a deep learning approach. The proposed method involves integrating various drug features, constructing a heterogeneous network, extracting drug embedding vectors, and using a fully connected neural network for prediction.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b11\": 0.9,\n    \"b12\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b19\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b26\": 0.7,\n    \"b27\": 0.7,\n    \"b28\": 0.7,\n    \"b29\": 0.7,\n    \"b30\": 0.7,\n    \"b31\": 0.7,\n    \"b32\": 0.7,\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7,\n    \"b36\": 0.7,\n    \"b37\": 0.7,\n    \"b38\": 0.7,\n    \"b39\": 0.7,\n    \"b40\": 0.7,\n    \"b41\": 0.7,\n    \"b42\": 0.7,\n    \"b43\": 0.7,\n    \"b44\": 0.7,\n    \"b45\": 0.7,\n    \"b46\": 0.7,\n    \"b47\": 0.7,\n    \"b48\": 0.7,\n    \"b49\": 0.7,\n    \"b50\": 0.7,\n    \"b51\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a method for predicting Drug-Drug Interactions (DDI) and their types using attributed heterogeneous graph embedding and a deep learning approach. The primary challenges include integrating various drug features effectively and maintaining the intrinsic characteristics of drugs. The authors create a two-stage framework that first generates drug embeddings from attributed heterogeneous networks and then uses these embeddings in a deep neural network to predict DDI types.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b4\": 1.0,\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b14\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b27\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6,\n    \"b36\": 0.6,\n    \"b37\": 0.6,\n    \"b38\": 0.6,\n    \"b39\": 0.6,\n    \"b40\": 0.6,\n    \"b41\": 0.6,\n    \"b42\": 0.6,\n    \"b43\": 0.6,\n    \"b44\": 0.6,\n    \"b45\": 0.6,\n    \"b46\": 0.6,\n    \"b47\": 0.6,\n    \"b48\": 0.6,\n    \"b49\": 0.6,\n    \"b50\": 0.6,\n    \"b51\": 0.6\n  }\n}\n```"], "64f59fc23fda6d7f0648f1fb": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of scaling Reinforcement Learning from Human Feedback (RLHF) due to the need for high-quality human labels. It proposes and evaluates Reinforcement Learning from AI Feedback (RLAIF) as an alternative, by comparing it directly to RLHF in summarization tasks. Key techniques include preference labeling with LLMs, chain-of-thought reasoning, and self-consistency. The results suggest that RLAIF can achieve comparable performance to RLHF without relying on human annotations.\",\n    \"Direct Inspiration\": {\n        \"b26\": 1,\n        \"b21\": 1,\n        \"b7\": 0.9,\n        \"b22\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.8,\n        \"b16\": 0.8,\n        \"b31\": 0.7,\n        \"b29\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.6,\n        \"b24\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing language models using reinforcement learning from AI feedback (RLAIF) as a viable alternative to reinforcement learning from human feedback (RLHF). The main contributions include demonstrating that RLAIF achieves comparable performance to RLHF, comparing techniques for generating AI labels, and identifying optimal settings for practitioners.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b16\": 1.0,\n    \"b21\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b18\": 0.7,\n    \"b22\": 0.7,\n    \"b24\": 0.7,\n    \"b29\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the need for high-quality human labels for Reinforcement Learning from Human Feedback (RLHF) and whether artificially generated labels can achieve comparable results. The authors propose an approach called Reinforcement Learning from AI Feedback (RLAIF), where AI-generated preferences are used to train a reward model for reinforcement learning fine-tuning, and they compare its performance directly to RLHF.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b14\": 0.8,\n        \"b16\": 0.8,\n        \"b21\": 0.9,\n        \"b26\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.7,\n        \"b24\": 0.7,\n        \"b29\": 0.6,\n        \"b31\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"This paper tackles the challenge of aligning language models to human preferences using Reinforcement Learning from AI Feedback (RLAIF) as an alternative to Reinforcement Learning from Human Feedback (RLHF). The authors propose a methodology that uses an off-the-shelf language model to generate preference labels, which are then used to train a reward model and fine-tune a policy model through reinforcement learning. The main contributions include demonstrating the effectiveness of RLAIF compared to RLHF, identifying optimal techniques for generating AI labels, and studying the alignment of AI-generated preferences with human preferences.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b26\": 0.9,\n    \"b14\": 0.8,\n    \"b16\": 0.8,\n    \"b18\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b22\": 0.6,\n    \"b24\": 0.6,\n    \"b29\": 0.6,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing dependence on high-quality human labels in Reinforcement Learning from Human Feedback (RLHF) by exploring the use of AI-generated labels. The primary contributions are a direct comparison of Reinforcement Learning from AI Feedback (RLAIF) and RLHF in summarization tasks, and the identification of optimal techniques for generating AI labels. The results show that RLAIF can be a viable alternative to RLHF, achieving comparable performance without reliance on human annotation.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b21\": 1,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b16\": 0.8,\n    \"b22\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b29\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```"], "6459ac63d68f896efa658a19": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in Named Entity Recognition (NER) by leveraging machine reading comprehension (MRC) strategies. The main challenges include low-transferability, hand-crafted question templates, and insufficient information in input data. The proposed solution involves converting NER to a multiple-choice MRC problem, using high-quality data reconstruction, and integrating MRC reasoning strategies to improve NER performance without relying on external data.\",\n  \"Direct Inspiration\": {\n    \"b51\": 0.9,\n    \"b20\": 0.8,\n    \"b3\": 0.8,\n    \"b52\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.7,\n    \"b50\": 0.7,\n    \"b26\": 0.6,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b8\": 0.5,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in named entity recognition (NER) by proposing a new framework that converts NER tasks into machine reading comprehension (MRC) problems. It focuses on reducing manual effort in data reconstruction and integrating MRC reasoning strategies to enhance NER performance without relying on additional data.\",\n    \"Direct Inspiration\": {\n        \"b51\": 0.9,\n        \"b20\": 0.85,\n        \"b42\": 0.8,\n        \"b3\": 0.75,\n        \"b52\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b26\": 0.7,\n        \"b28\": 0.7,\n        \"b10\": 0.65,\n        \"b23\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b1\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Low-transferability due to lack of a unified data reconstruction paradigm\",\n      \"High reproduction difficulty and unstable performance due to hand-crafted question templates\",\n      \"Insufficient information as the input ignores label semantics\"\n    ],\n    \"Inspirations\": [\n      \"Utilizing MRC (Machine Reading Comprehension) strategies to solve NER (Named Entity Recognition) tasks\",\n      \"Incorporating label information into MRC reasoning strategies\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b51\",\n    \"b52\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b20\",\n    \"b3\",\n    \"b26\",\n    \"b28\",\n    \"b42\",\n    \"b50\"\n  ],\n  \"Other Inspiration\": [\n    \"b6\",\n    \"b35\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in named entity recognition (NER) by leveraging machine reading comprehension (MRC) strategies. It focuses on converting NER tasks into MRC format to improve performance without relying on additional external data. The proposed solution includes high-quality data reconstruction, MRC reasoning strategies, and integration of label information.\",\n  \"Direct Inspiration\": {\n    \"b51\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.9,\n    \"b3\": 0.85,\n    \"b42\": 0.8,\n    \"b52\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.75,\n    \"b26\": 0.7,\n    \"b28\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper tackles challenges in Named Entity Recognition (NER) by reformulating it as a Machine Reading Comprehension (MRC) problem. It introduces a data reconstruction method to convert NER tasks into MRC formats using multiple-choice questions, and integrates MRC reasoning strategies to enhance the NER task performance. The main contributions include a stable data reconstruction rule, employing MRC strategies without additional data, and achieving state-of-the-art performance on multiple NER benchmarks.\",\n    \"Direct Inspiration\": {\n        \"b51\": 1.0,\n        \"b3\": 0.9,\n        \"b20\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b42\": 0.7,\n        \"b50\": 0.6,\n        \"b52\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b26\": 0.4,\n        \"b28\": 0.3\n    }\n}\n```"], "63d9d87390e50fcafd57d920": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Handling multiple tasks across domains without incurring high computational expense.\",\n      \"Bridging domain shifts between language and chemistry.\",\n      \"Avoiding costly pre-training on large mono-domain datasets and task-specific fine-tuning.\"\n    ],\n    \"inspirations\": [\n      \"Multi-tasking and cross-domain capabilities of T5 [b28].\",\n      \"Improvements in generative language models and transformer architectures [b39].\",\n      \"Existing specialized models in chemistry such as Molecular Transformers [b34] and RXN family [b33].\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\n      \"b28\",\n      \"b34\",\n      \"b10\"\n    ]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\n      \"b19\",\n      \"b39\",\n      \"b31\"\n    ]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\n      \"b23\",\n      \"b40\"\n    ]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency and high computational cost associated with current language models for chemical tasks, which require separate models for each task and extensive pre-training and fine-tuning. The proposed solution is Text+Chem T5, a multi-task, multi-domain transformer model that can handle both natural language and chemical tasks without task-specific heads or separate pre-training, thus improving cross-domain translation and generalization.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0,\n    \"b10\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.8,\n    \"b35\": 0.8,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the limitations of current models in handling cross-domain tasks between natural language and chemistry without incurring high computational costs and inefficiencies. The proposed solution, Text+Chem T5, is a multi-task, multi-domain model designed to translate between natural and chemical languages, eliminating the need for costly pre-training and task-specific fine-tuning.\",\n  \"Direct Inspiration\": {\n    \"Inspired by [b2]\": 0.9,\n    \"The transformer architecture [b39]\": 1.0,\n    \"T5 [b28]\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"Molecular Transformers [b34]\": 0.8,\n    \"MolT5 [b10]\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"T5Chem [b19]\": 0.7,\n    \"RXN family [b33]\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a novel multi-task, multi-domain model called Text+Chem T5 for natural and chemical language translation. The primary challenges addressed include the high computational cost and inefficiencies of current specialized models for each task. The proposed solution leverages the T5 transformer architecture to handle tasks in both domains without the need for costly pre-training or task-specific fine-tuning.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.95,\n    \"b10\": 0.9,\n    \"b34\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b35\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b39\": 0.65,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a multi-task, multi-domain language model that can effectively translate between natural and chemical languages. The proposed model, Text+Chem T5, eliminates the need for costly pre-training and fine-tuning on large mono-domain datasets, while improving cross-domain translation by sharing information between tasks and domains.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0,\n    \"b34\": 1.0,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b39\": 0.7,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.5,\n    \"b23\": 0.4\n  }\n}\n```"], "64659ad1d68f896efa87539f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a Medical Visual Question Answering (MedVQA) system that can handle open-ended questions in clinical practice by leveraging medical visual content. It introduces MedVInT, a novel generative learning-based model, and constructs a large-scale MedVQA dataset called PMC-VQA to facilitate training and evaluation. The study highlights the limitations of existing datasets and proposes a scalable pipeline for creating a more diverse and extensive dataset.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b18\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Medical Visual Question Answering (MedVQA) by introducing a generative model framework, MedVInT, which aligns a pre-trained vision encoder and large language model through visual instruction tuning. Additionally, the authors present a new large-scale MedVQA dataset, PMC-VQA, to overcome the limitations of existing datasets in size and diversity.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b18\": 0.8,\n    \"b19\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b22\": 0.75,\n    \"b34\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.5,\n    \"b21\": 0.5,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an open-ended Medical Visual Question Answering (MedVQA) system that can generate free-form answers leveraging medical visual content. The authors propose a generative model called MedVInT, which aligns a pre-trained vision encoder with a large language model through visual instruction tuning. They also introduce a new large-scale dataset, PMC-VQA, to overcome the limitations of existing datasets.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b18\": 0.85,\n    \"b19\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b24\": 0.75,\n    \"b21\": 0.7,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.65,\n    \"b16\": 0.65,\n    \"b25\": 0.65,\n    \"b31\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an open-ended Medical Visual Question Answering (MedVQA) system that can generate diverse and informative answers in natural language, rather than being constrained by a predefined set of outcomes. The proposed approach, MedVInT, aligns a pre-trained vision encoder with a large language model through visual instruction tuning. A new large-scale dataset, PMC-VQA, is created to facilitate this task, significantly surpassing existing datasets in size and diversity. The paper's contributions include reframing MedVQA as a generative learning task, constructing the PMC-VQA dataset, pre-training and fine-tuning the MedVInT model, and proposing a new challenging benchmark for MedVQA.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b18\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.9,\n    \"b24\": 0.8,\n    \"b21\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b22\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating visual modalities into large language models (LLMs) for Medical Visual Question Answering (MedVQA). The authors propose MedVInT, a model that aligns a pre-trained vision encoder with a large language model via visual instruction tuning. They also introduce PMC-VQA, a large-scale MedVQA dataset, and demonstrate significant performance improvements over existing models.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b18\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.8,\n    \"b12\": 0.7,\n    \"b16\": 0.7,\n    \"b25\": 0.7,\n    \"b31\": 0.7,\n    \"b24\": 0.7,\n    \"b21\": 0.7,\n    \"b5\": 0.7,\n    \"b17\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b20\": 0.6,\n    \"b11\": 0.6,\n    \"b2\": 0.6,\n    \"b35\": 0.6,\n    \"b14\": 0.6,\n    \"b4\": 0.6,\n    \"b30\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```"], "6427029c90e50fcafd5d6cf3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of quantifying and reducing interference in datacenters where latency-critical (LC) and best-effort (BE) applications coexist. The proposed solution involves the concept of system entropy (E_S), which quantifies interference using a three-step paradigm inspired by Shannon's information entropy. The paper introduces E_S, decomposes it into LC entropy (E_LC) and BE entropy (E_BE), and proposes a new scheduling strategy (ARQ) to mitigate interference.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.8,\n    \"b47\": 0.8,\n    \"b35\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b6\": 0.6,\n    \"b9\": 0.6,\n    \"b19\": 0.6,\n    \"b29\": 0.6,\n    \"b40\": 0.6,\n    \"b50\": 0.6,\n    \"b51\": 0.6,\n    \"b52\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of interference and contention in shared hardware resources in datacenters that negatively affect application performance. It introduces 'system entropy (ES)' to quantify interference in a datacenter and proposes the 'ARQ' scheduling strategy to mitigate this interference. The concept and methodology are inspired by Shannon's information entropy.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b43\": 0.7,\n    \"b46\": 0.7,\n    \"b47\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of quantifying and reducing interference among latency-critical (LC) and best-effort (BE) applications in datacenters. It introduces 'system entropy (E S )' as a novel metric to quantify interference, inspired by Shannon's information entropy. The proposed ARQ scheduling strategy dynamically adjusts resource isolation and sharing to minimize interference and improve overall system performance.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b23\": 0.8,\n    \"b2\": 0.8,\n    \"b4\": 0.7,\n    \"b7\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b11\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6,\n    \"b34\": 0.6,\n    \"b36\": 0.6,\n    \"b46\": 0.6,\n    \"b47\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of quantifying and reducing interference in datacenters where latency-critical (LC) and best-effort (BE) applications are collocated. The authors propose a novel metric called system entropy (E_S) inspired by Shannon's information entropy to measure interference. They also introduce the Ah-Q toolkit and an associative scheduling strategy (ARQ) to manage resources effectively, enhancing overall system performance.\",\n  \"Direct Inspiration\": {\n    \"b39\": 0.9,\n    \"b35\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b6\": 0.7,\n    \"b13\": 0.7,\n    \"b36\": 0.7,\n    \"b41\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.5,\n    \"b46\": 0.5,\n    \"b47\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the interference and contention in shared hardware resources in datacenters, which affects the performance of both latency-critical (LC) and best-effort (BE) applications. The proposed solution is to quantify this interference using a novel metric called system entropy (ES), inspired by Shannon\u2019s information entropy. The paper also introduces a resource scheduling strategy called ARQ to mitigate interference.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b6\": 0.7,\n    \"b9\": 0.7,\n    \"b19\": 0.7,\n    \"b29\": 0.7,\n    \"b40\": 0.7,\n    \"b50\": 0.7,\n    \"b51\": 0.7,\n    \"b52\": 0.7,\n    \"b43\": 0.6,\n    \"b46\": 0.6,\n    \"b47\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b11\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b32\": 0.5,\n    \"b33\": 0.5,\n    \"b34\": 0.5,\n    \"b35\": 0.5,\n    \"b13\": 0.4,\n    \"b36\": 0.4\n  }\n}\n```"], "634d805590e50fcafd4e05f7": ["```json\n{\n  \"Summary\": \"The paper addresses the complexity of processor design and the difficulty in ensuring correctness when using traditional HDL at the RTL level. The authors propose a Pipeline Description Language (PDL) that raises the level of abstraction, allowing for easier specification and control of processor pipeline functionality while still providing fine-grained microarchitectural control. Key challenges include data hazard prevention, speculation, and out-of-order execution.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in designing processor pipelines at a high level of abstraction using a Pipeline Description Language (PDL). The proposed PDL simplifies the construction of processor pipelines by allowing designers to specify functionality while providing fine-grained microarchitectural control. The PDL compiler generates necessary control logic, supports hazard locks for data hazard prevention, and enables speculative execution.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b19\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b27\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of designing processor pipelines using RTL, which is complex and difficult to reason about. It proposes a new Pipeline Description Language (PDL) that abstracts pipeline construction, supports various microarchitectural optimizations, and simplifies correctness assurance.\",\n    \"Direct Inspiration\": {\n        \"b17\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.7,\n        \"b19\": 0.7,\n        \"b26\": 0.8,\n        \"b27\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b25\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in designing processor pipelines using traditional hardware description languages (HDLs) at the register transfer level (RTL), which are complex and hard to validate. It proposes a novel Pipeline Description Language (PDL) that raises the level of abstraction, allowing for easier specification of processor pipelines while ensuring correctness through one-instruction-at-a-time semantics. Key innovations include hazard locks for data hazard prevention, a speculation API, and modular abstractions for pipeline stages.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b27\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies the challenges in designing and validating processor pipelines using RTL, and proposes a Pipeline Description Language (PDL) to raise the level of abstraction. The key innovations include hazard locks to prevent data hazards, a speculation API to manage speculative thread execution, and an easy-to-understand one-instruction-at-a-time semantics for pipeline design.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b19\": 0.85,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.75,\n    \"b26\": 0.7,\n    \"b27\": 0.65\n  },\n  \"Other Inspiration\": {}\n}\n```"], "64a63bbad68f896efaec478f": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of insufficient graph labels, over-fitting, and the difficulty in transferring pre-trained graph models to various downstream tasks. It proposes a novel 'pre-training, prompting, and fine-tuning' approach inspired by language prompts in NLP to improve the generalization of graph neural networks (GNNs). The paper introduces a unified format for graph prompts and employs meta-learning to enhance multi-task performance.\",\n    \"Direct Inspiration\": {\n        \"b19\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.9,\n        \"b26\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.7,\n        \"b10\": 0.6,\n        \"b34\": 0.6,\n        \"b35\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of training graph neural networks (GNNs) for varied downstream tasks, proposing the introduction of prompt learning from natural language processing (NLP) to graphs. The key challenges include the gap between pre-training and downstream tasks, negative transfer when tuning pre-trained models, and difficulty in designing graph prompts. The authors propose a novel multi-task prompting framework, unifying graph tasks into a graph-level format, and employing meta-learning to improve prompt learning across tasks.\",\n    \"Direct Inspiration\": {\n        \"b19\": 1,\n        \"b26\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.8,\n        \"b10\": 0.7,\n        \"b34\": 0.7,\n        \"b35\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b2\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of graph neural networks (GNNs) in traditional supervised learning due to insufficient graph labels and over-fitting on out-of-distribution data. It proposes a novel approach of 'pre-training, prompting, and fine-tuning' inspired by natural language processing (NLP) to better generalize pre-trained models to various downstream tasks in the graph domain. The proposed method involves unifying language and graph prompts, reformulating node-level and edge-level tasks to graph-level tasks, and applying meta-learning techniques to improve multi-task performance.\",\n  \"Direct Inspiration\": [\n    \"b19\",\n    \"b26\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b12\",\n    \"b34\",\n    \"b35\"\n  ],\n  \"Other Inspiration\": [\n    \"b6\",\n    \"b10\",\n    \"b31\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in this paper include the limitations of traditional supervised learning on graphs due to insufficient graph labels and the overfitting problem with out-of-distribution testing data. The authors propose a novel approach, 'pre-training, prompting, and fine-tuning,' inspired by prompt learning in NLP, which aims to bridge the gap between graph pre-training and multiple downstream tasks through effective graph prompt designs, task reformulations, and meta-learning techniques.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b26\": 0.85,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b21\": 0.7,\n    \"b34\": 0.75,\n    \"b35\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b10\": 0.65,\n    \"b13\": 0.6,\n    \"b17\": 0.65,\n    \"b20\": 0.6,\n    \"b25\": 0.6,\n    \"b31\": 0.55,\n    \"b33\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of pre-training and fine-tuning in graph neural networks (GNNs) and introduces a novel method of 'pre-training, prompting, and fine-tuning' inspired by prompt learning from natural language processing (NLP). The primary contributions include unifying the prompt format for graphs and languages, reformulating graph tasks to graph-level tasks, and leveraging meta-learning to improve prompt reliability.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b26\": 0.9,\n    \"b12\": 0.9,\n    \"b21\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b10\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b25\": 0.5,\n    \"b15\": 0.5,\n    \"b18\": 0.5,\n    \"b13\": 0.4,\n    \"b37\": 0.4\n  }\n}\n```"], "63a1751790e50fcafd1f49ce": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating robust text embeddings that perform well across multiple tasks and domains without needing extensive finetuning. The authors propose INSTRUCTOR, a multitask model that generates task- and domain-aware embeddings using task instructions. This approach significantly improves performance on diverse downstream tasks by embedding inputs together with their end task and domain instructions. INSTRUCTOR leverages a new dataset, MEDI, consisting of 330 text embedding datasets annotated with task instructions.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1,\n    \"b58\": 1,\n    \"b67\": 1,\n    \"b50\": 0.9,\n    \"b49\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b27\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b32\": 0.7,\n    \"b8\": 0.6,\n    \"b43\": 0.6,\n    \"b55\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge highlighted in the paper is the degraded performance of existing text embeddings when applied to new tasks or domains. The proposed solution, INSTRUCTOR, addresses this issue by generating task- and domain-aware embeddings based on task instructions without further finetuning. This model incorporates task-specific instructions into the embedding process, achieving state-of-the-art performance across various domains and tasks by leveraging a new dataset, MEDI, which includes 330 annotated text embedding datasets.\",\n  \"Direct Inspiration\": {\n    \"b50\": 1,\n    \"b49\": 0.9,\n    \"b45\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b17\": 0.8,\n    \"b58\": 0.7,\n    \"b67\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.6,\n    \"b8\": 0.6,\n    \"b43\": 0.6,\n    \"b55\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting text embeddings to different tasks and domains without extensive finetuning. It introduces INSTRUCTOR, a multitask model that generates task- and domain-aware embeddings using task instructions. The model is trained on a diverse set of tasks and demonstrates significant performance improvements over existing embedding models.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1.0,\n    \"b58\": 1.0,\n    \"b67\": 1.0,\n    \"b50\": 0.9,\n    \"b49\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b25\": 0.7,\n    \"b32\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.5,\n    \"b27\": 0.5,\n    \"b63\": 0.5,\n    \"b47\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting text embeddings to various downstream tasks and domains without the need for further task- or domain-specific finetuning. The authors propose a novel model named INSTRUCTOR, which generates task- and domain-aware embeddings based on task instructions. This model, trained with a new dataset called MEDI, significantly outperforms previous state-of-the-art models by embedding text inputs together with their task instructions. The key contributions include the introduction of instruction-based finetuning and the creation of the MEDI dataset.\",\n  \"Direct Inspiration\": {\n    \"b45\": 0.95,\n    \"b58\": 0.92,\n    \"b67\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b50\": 0.85,\n    \"b49\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.75,\n    \"b27\": 0.70,\n    \"b19\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of text embeddings' performance degradation when applied to new tasks or domains. The proposed solution, INSTRUCTOR, is a multitask model that generates task- and domain-aware embeddings using task instructions without requiring further finetuning. The model is trained on a new dataset, MEDI, and outperforms prior state-of-the-art models.\",\n  \"Direct Inspiration\": {\n    \"b50\": 1.0,\n    \"b49\": 0.9,\n    \"b45\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.7,\n    \"b17\": 0.65,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b58\": 0.55,\n    \"b67\": 0.5\n  }\n}\n```"], "64337e3190e50fcafd76ef32": ["```json\n{\n  \"Summary\": \"The paper introduces generative agents, which use a novel architecture to simulate believable human behavior by remembering, retrieving, reflecting, and planning based on their experiences and environment. The primary challenges addressed include ensuring long-term coherence of agent behavior and managing dynamically evolving memories. The proposed solution leverages large language models to create agents that interact naturally and exhibit emergent social dynamics.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b76\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b77\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b57\": 0.6,\n    \"b82\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces generative agents that use a large language model to simulate believable human behavior. The agents are designed to remember, reflect, plan, and interact with their environment and other agents in a coherent and dynamic manner. The paper outlines the architecture and components of these agents, evaluates their performance, and discusses potential applications and ethical considerations.\",\n  \"Direct Inspiration\": [\"b5\", \"b57\", \"b77\"],\n  \"Indirect Inspiration\": [\"b37\", \"b16\", \"b82\"],\n  \"Other Inspiration\": [\"b8\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Crafting an interactive artificial society that reflects believable human behavior, ensuring long-term coherence in agent behavior, and managing dynamically-evolving memory.\",\n    \"inspirations\": \"Leveraging large language models to simulate believable human behavior, and using an agent architecture that stores, synthesizes, and applies relevant memories.\"\n  },\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b57\": 0.9,\n    \"b82\": 0.8,\n    \"b77\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b16\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b76\": 0.6,\n    \"b89\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating interactive artificial societies that exhibit believable human behavior over long-term interactions. It introduces 'generative agents' that leverage large language models to simulate realistic individual and group behaviors. The architecture includes memory management, reflection, and planning components to ensure agents' behaviors are coherent over time.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b77\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b57\": 0.75,\n    \"b82\": 0.7,\n    \"b107\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b37\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating believable artificial agents capable of long-term coherent behavior by utilizing generative models. The proposed architecture includes components for memory storage, retrieval, reflection, and planning, which enable agents to simulate realistic human interactions and social dynamics in a virtual environment. Evaluations demonstrate the efficacy of these generative agents in producing believable individual and group behaviors.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b76\": 0.95,\n    \"b77\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b44\": 0.75,\n    \"b105\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b57\": 0.6,\n    \"b82\": 0.6\n  }\n}\n```"], "63bfce0790e50fcafd0a10d0": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper are: 1) The inability of sophisticated prefetchers to predict all long-latency off-chip load requests. 2) Significant latency of off-chip load requests due to the increasing size and complexity of the on-chip cache hierarchy.\",\n    \"Inspirations\": \"The paper introduces Hermes, a technique to accelerate long-latency off-chip load requests by predicting which loads might go off-chip and fetching their data directly from main memory while concurrently accessing the cache hierarchy. This aims to hide on-chip cache access latency under the shadow of main memory access latency.\"\n  },\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b125\": 0.9,\n    \"b105\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.7,\n    \"b74\": 0.7,\n    \"b109\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b107\": 0.6,\n    \"b20\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Long-latency load requests limiting out-of-order (OOO) processor performance.\",\n      \"Ineffectiveness of state-of-the-art prefetchers in predicting irregular access patterns.\",\n      \"High on-chip cache access latency due to increasing cache sizes.\"\n    ],\n    \"algorithm\": \"Hermes: Predicts which load requests might go off-chip and starts fetching their corresponding data directly from main memory, while concurrently accessing the cache hierarchy.\"\n  },\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b34\": 0.8,\n    \"b74\": 0.7,\n    \"b125\": 0.6\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.5,\n    \"b27\": 0.5,\n    \"b32\": 0.4,\n    \"b105\": 0.4\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.3,\n    \"b17\": 0.3,\n    \"b23\": 0.3,\n    \"b24\": 0.3,\n    \"b59\": 0.3,\n    \"b83\": 0.3,\n    \"b103\": 0.3,\n    \"b109\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of long-latency off-chip load requests in high-performance out-of-order processors. To tackle this, it proposes a technique called Hermes that predicts which load requests might go off-chip and concurrently fetches data directly from the main memory and the on-chip cache hierarchy. This approach aims to hide the on-chip cache access latency under the shadow of the main memory access latency, significantly reducing the overall latency of an off-chip load request. The key challenges include accurately identifying off-chip load requests and managing the performance impact of false-positive predictions. The proposed solution, Hermes, uses a perceptron-based off-chip predictor called POPET to achieve high accuracy and coverage in predicting off-chip load requests.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b125\": 0.9,\n    \"b74\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.7,\n    \"b34\": 0.7,\n    \"b105\": 0.6,\n    \"b109\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b59\": 0.5,\n    \"b83\": 0.5,\n    \"b103\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the limitations of current prefetchers in predicting long-latency off-chip load requests and the increasing latency due to larger on-chip caches. The proposed algorithm, Hermes, aims to predict off-chip loads and fetch their data directly from main memory, thereby reducing overall latency. The main issues are ensuring the accuracy of these predictions and minimizing unnecessary memory requests.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b125\": 0.85,\n    \"b34\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b74\": 0.75,\n    \"b59\": 0.7,\n    \"b83\": 0.7,\n    \"b103\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b107\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of long-latency off-chip load requests that significantly hinder the performance of high-performance out-of-order processors. The proposed solution, Hermes, predicts which load requests might go off-chip and fetches their corresponding data directly from the main memory while concurrently accessing the cache hierarchy. This aims to hide the on-chip cache access latency under the shadow of the main memory access latency. The key challenge is accurately predicting off-chip load requests to avoid unnecessary main memory requests and their associated overheads.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b125\": 0.9,\n    \"b74\": 0.8,\n    \"b27\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b59\": 0.7,\n    \"b83\": 0.7,\n    \"b103\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b87\": 0.6,\n    \"b90\": 0.6,\n    \"b91\": 0.6,\n    \"b10\": 0.6,\n    \"b15\": 0.6,\n    \"b9\": 0.6,\n    \"b14\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.6,\n    \"b29\": 0.6,\n    \"b49\": 0.6,\n    \"b115\": 0.6,\n    \"b58\": 0.6,\n    \"b98\": 0.6,\n    \"b121\": 0.6,\n    \"b17\": 0.6,\n    \"b105\": 0.6,\n    \"b109\": 0.6,\n    \"b107\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b50\": 0.6,\n    \"b3\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6,\n    \"b73\": 0.6\n  }\n}\n```"], "637aec2590e50fcafd92962b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving performance and energy efficiency for irregular workload patterns in data-driven applications by proposing AXI-PACK, an extension to the AXI4 on-chip protocol. AXI-PACK supports tightly-packed strided and indirect memory streams while maintaining compatibility with existing AXI4 features. The proposed solution demonstrates significant improvements in bus utilization, speedups, and energy efficiency through extensive evaluations on a RISC-V vector processor system.\",\n  \"Direct Inspiration\": [\"b10\", \"b11\", \"b12\", \"b13\", \"b14\"],\n  \"Indirect Inspiration\": [\"b4\", \"b5\", \"b7\", \"b8\", \"b9\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency of general-purpose CPUs and GPUs in handling irregular data access patterns common in domains like machine learning, graph analytics, and fluid dynamics. The authors propose AXI-PACK, an extension to Arm's AXI4 on-chip protocol, to support end-to-end bus-packed strided and indirect memory streams. This solution aims to improve performance and energy efficiency, achieving significant speedups and high bus utilizations on irregular workloads.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b11\": 0.9,\n    \"b12\": 0.9,\n    \"b13\": 0.9,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving performance and energy efficiency in data-driven applications with irregular data access patterns, which pose difficulties for traditional CPU and GPU architectures. The proposed solution, AXI-PACK, extends the AXI4 protocol to support end-to-end, tightly-packed strided and indirect memory streams. This extension is implemented on an open-source RISC-V vector processor and evaluated for various irregular workloads, demonstrating significant improvements in bus utilization, speedup, and energy efficiency.\",\n  \"Direct Inspiration\": [\"b4\", \"b5\", \"b10\", \"b11\", \"b13\"],\n  \"Indirect Inspiration\": [\"b7\", \"b8\", \"b9\", \"b12\"],\n  \"Other Inspiration\": [\"b0\", \"b1\", \"b2\", \"b3\", \"b6\", \"b14\", \"b15\", \"b17\", \"b18\", \"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving performance and energy efficiency for irregular memory access patterns in data-driven applications, which general-purpose CPUs and GPUs struggle to handle. The proposed solution, AXI-PACK, extends Arm's AXI4 on-chip protocol to support tightly-packed strided and indirect memory streams, ensuring backward compatibility and efficient handling of irregular bursts. The solution is demonstrated using an extended RISC-V vector processor and a banked memory controller, achieving significant improvements in bus utilization, speedups, and energy efficiency.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b11\": 0.9,\n    \"b12\": 0.9,\n    \"b13\": 0.9,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges posed by irregular data access patterns in modern applications, which affect the performance and efficiency of general-purpose CPUs and GPUs. The authors propose AXI-PACK, an extension to Arm's AXI4 protocol, to support tightly-packed strided and indirect memory streams, aiming to improve bus utilization, speedup, and energy efficiency. AXI-PACK is demonstrated on a RISC-V vector processor and a banked memory controller, achieving significant performance improvements in various benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b11\": 1,\n    \"b12\": 1,\n    \"b13\": 1,\n    \"b14\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b18\": 0.8,\n    \"b19\": 0.8\n  }\n}\n```"], "64b60eaf3fda6d7f06eaf557": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling up the context length of Transformers, specifically focusing on improving the efficiency of the attention mechanism. The authors propose FlashAttention-2, which builds on FlashAttention by optimizing parallelism and work partitioning to achieve better performance on GPUs. The key contributions include reducing non-matmul FLOPs, increasing GPU resource utilization, and improving memory access patterns.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b2\": 0.5,\n    \"b3\": 0.5,\n    \"b7\": 0.5,\n    \"b8\": 0.5,\n    \"b13\": 0.5,\n    \"b18\": 0.5,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling up the context length of Transformers due to the quadratic memory and runtime requirements of the attention layer. It proposes an improved algorithm called FlashAttention-2, which enhances parallelism and work partitioning to achieve better performance on GPUs, specifically the A100. The key improvements include reducing non-matmul FLOPs, increasing GPU resource utilization, and optimizing work partitioning within thread blocks, leading to significant speedup and memory efficiency over the previous FlashAttention algorithm.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b13\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inefficiency in scaling up the context length of Transformers due to the quadratic runtime and memory requirements of the attention layer. The authors propose FlashAttention-2 to improve parallelism and work partitioning, yielding significant speedup and memory savings compared to existing methods, including FlashAttention.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b13\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of scaling up the context length of Transformers, which requires high runtime and memory due to the attention layer having quadratic complexity in input sequence length. The authors propose FlashAttention-2, an improvement over FlashAttention, to achieve better parallelism and work partitioning, reducing non-matmul FLOPs and enhancing GPU utilization.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b12\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.6,\n        \"b5\": 0.6,\n        \"b6\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling up the context length of Transformers efficiently by proposing FlashAttention-2, which builds upon FlashAttention to improve parallelism and work partitioning, thereby reducing non-matmul FLOPs and increasing GPU utilization.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": [\n    {\n      \"b11\": 0.7,\n      \"b1\": 0.6,\n      \"b2\": 0.6,\n      \"b3\": 0.6,\n      \"b7\": 0.6,\n      \"b8\": 0.6,\n      \"b13\": 0.6,\n      \"b18\": 0.6,\n      \"b19\": 0.6\n    },\n    {\n      \"b5\": 0.5,\n      \"b6\": 0.5\n    }\n  ]\n}\n```"], "64a407dcd68f896efaf1ba64": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Identifying bottlenecks in gem5 architectural simulator on different platforms\",\n      \"Improving simulation speed without complex changes to the simulator\",\n      \"Understanding the impact of different hardware platforms on simulation performance\"\n    ],\n    \"inspirations\": [\n      \"Existing architectural simulators like gem5, Sniper, MARSSx86, and ZSim\",\n      \"Techniques to improve simulation performance like parallelizing, hardware virtualization, and sampling\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b16\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b5\",\n    \"b7\",\n    \"b8\",\n    \"b9\",\n    \"b10\",\n    \"b11\",\n    \"b12\",\n    \"b13\"\n  ],\n  \"Other Inspiration\": [\n    \"b1\",\n    \"b2\",\n    \"b3\",\n    \"b4\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of the gem5 architectural simulator, which is widely used in computer architecture research. The primary contributions include profiling gem5 on different platforms, identifying execution bottlenecks, and proposing system and architectural optimizations to improve simulation speed.\",\n  \"Direct Inspiration\": [\"b5\", \"b16\", \"b19\"],\n  \"Indirect Inspiration\": [\"b7\", \"b8\", \"b10\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The paper primarily addresses the challenge of improving the simulation speed of architectural simulators, specifically gem5, by profiling its execution on different platforms and proposing simple system tuning and architectural recommendations. The authors are motivated by the significant performance differences observed when running simulations on different hardware platforms and aim to provide insights for future hardware and software optimizations.\",\n    \"Direct Inspiration\": {\n        \"b16\": 0.9,\n        \"b19\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b2\": 0.75,\n        \"b3\": 0.7,\n        \"b5\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b20\": 0.6,\n        \"b26\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving the simulation performance of the gem5 architectural simulator, identifying bottlenecks on different platforms (Intel Xeon and Apple M1), and proposing architectural recommendations for speed enhancement.\",\n  \"Direct Inspiration\": [\"b2\", \"b16\"],\n  \"Indirect Inspiration\": [\"b5\", \"b19\"],\n  \"Other Inspiration\": [\"b1\", \"b3\", \"b6\", \"b7\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving the simulation speed of the gem5 architectural simulator by identifying and optimizing bottlenecks. The study evaluates the performance of gem5 on different hardware platforms, including Intel Xeon and Apple M1 chips, and proposes system tuning and architectural recommendations to enhance simulation performance.\",\n    \"Direct Inspiration\": [\"b16\", \"b19\"],\n    \"Indirect Inspiration\": [\"b5\", \"b26\"],\n    \"Other Inspiration\": [\"b2\", \"b3\", \"b4\"]\n}\n```"], "6423ac7790e50fcafd55eaa0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently fine-tuning large language models on downstream tasks due to the exponential growth in model size compared to limited computational resources. It proposes Parameter-Efficient Fine-Tuning (PEFT) methods as a solution, aiming to optimize a small subset of parameters or introduce new parameters to improve memory and computational efficiency. The paper categorizes and compares various PEFT methods, including Adapters, BitFit, LoRa, and Soft Prompts, and suggests future research directions.\",\n  \"Direct Inspiration\": [\"b27\", \"b28\", \"b4\", \"b36\"],\n  \"Indirect Inspiration\": [\"b13\", \"b58\", \"b67\", \"b46\"],\n  \"Other Inspiration\": [\"b14\", \"b43\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges of scaling language models, particularly focusing on the limitations of in-context learning and the need for parameter-efficient fine-tuning (PEFT) methods. It introduces a taxonomy of PEFT methods and provides a detailed overview of 30 approaches, highlighting current unresolved challenges and suggesting avenues for future research.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b58\": 1,\n    \"b46\": 1,\n    \"b27\": 1,\n    \"b4\": 1,\n    \"b28\": 1,\n    \"b67\": 1,\n    \"b36\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.9,\n    \"b43\": 0.8,\n    \"b38\": 0.85,\n    \"b33\": 0.85,\n    \"b59\": 0.8,\n    \"b22\": 0.8,\n    \"b54\": 0.8,\n    \"b45\": 0.8,\n    \"b23\": 0.8,\n    \"b16\": 0.8,\n    \"b29\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b3\": 0.7,\n    \"b19\": 0.7,\n    \"b60\": 0.7,\n    \"b56\": 0.7,\n    \"b1\": 0.7,\n    \"b20\": 0.7,\n    \"b31\": 0.7,\n    \"b11\": 0.7,\n    \"b15\": 0.7,\n    \"b18\": 0.7,\n    \"b55\": 0.7,\n    \"b39\": 0.7,\n    \"b35\": 0.7,\n    \"b2\": 0.7,\n    \"b40\": 0.7,\n    \"b0\": 0.7,\n    \"b32\": 0.7,\n    \"b41\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of fine-tuning large-scale Transformer models given the constraints of limited GPU memory and computational resources. It focuses on parameter-efficient fine-tuning (PEFT) methods to make this feasible, providing a systematic overview, comparison, and taxonomy of these methods, and suggesting several avenues for improvement.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.95,\n    \"b28\": 0.90,\n    \"b36\": 0.85,\n    \"b4\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.75,\n    \"b46\": 0.70,\n    \"b67\": 0.65,\n    \"b16\": 0.60\n  },\n  \"Other Inspiration\": {\n    \"b58\": 0.55,\n    \"b24\": 0.50,\n    \"b3\": 0.45,\n    \"b39\": 0.40\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of fine-tuning large language models, particularly with limited computational resources and memory. It proposes parameter-efficient fine-tuning (PEFT) methods, which involve training only a small set of parameters, either by adding new parameters or fine-tuning a subset of existing ones. The paper systematically reviews various PEFT methods, including Adapters, Soft Prompts, and others, and suggests avenues for improvement such as developing standardized benchmarks and novel reparameterization techniques.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b28\": 0.9,\n    \"b36\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b67\": 0.7,\n    \"b29\": 0.6,\n    \"b16\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.5,\n    \"b38\": 0.5,\n    \"b22\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of parameter-efficient fine-tuning (PEFT) of large language models to handle downstream tasks efficiently. It highlights the rapid growth in model sizes and the need for efficient fine-tuning methods to circumvent the limitations of hardware memory. The paper proposes a taxonomy of PEFT methods and evaluates various approaches, including adapters, soft prompts, and reparametrization techniques, to improve training efficiency, memory usage, and model performance.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b28\": 1.0,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b43\": 0.8,\n    \"b46\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b67\": 0.7\n  }\n}\n```"], "6459ac63d68f896efa6588b4": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in fine-grained multilingual named entity recognition (NER), focusing on the limitations of existing systems in terms of insufficient knowledge, limited context length, and single retrieval strategies. It proposes a unified retrieval-augmented system (U-RaNER) that utilizes multiple retrieval strategies and combines knowledge from both Wikipedia and Wikidata to improve performance.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b4\": 0.7,\n    \"b17\": 0.6,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b22\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in identifying fine-grained complex named entities (NE) in multilingual contexts with low context and semantic ambiguity. The proposed algorithm, U-RaNER, incorporates diverse knowledge bases (Wikipedia and Wikidata) and multiple retrieval strategies to augment the context available for NER models, aiming to overcome limitations in existing methods such as insufficient knowledge, limited context length, and single retrieval strategies.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b11\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying complex named entities (NE) in multilingual contexts, particularly focusing on fine-grained classification and handling simulated errors such as spelling mistakes. The authors propose a unified retrieval-augmented system (U-RaNER) that uses diverse knowledge bases from Wikipedia and Wikidata and employs multiple retrieval strategies (TEXT2TEXT, TEXT2ENT, ENT2ENT) and an infusion approach to enhance the contextual information available to the model.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1,\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b22\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b15\": 0.6,\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the identification of complex named entities (NE) with low context and semantic ambiguity, the incorporation of a fine-grained entity taxonomy, and dealing with simulated errors like spelling mistakes. The proposed algorithm, U-RaNER, addresses these challenges by using both Wikipedia and Wikidata knowledge bases, employing multiple retrieval strategies and the infusion approach to enhance the context visibility for the model.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1.0,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.8,\n    \"b14\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b25\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of identifying complex named entities (NEs) in short contexts, particularly within the MultiCoNER II task, which includes a fine-grained entity taxonomy and simulated errors. The proposed solution is a unified retrieval-augmented system (U-RaNER) that leverages both Wikipedia and Wikidata to enhance knowledge retrieval and uses multiple retrieval strategies to improve model performance. The system aims to overcome issues related to insufficient knowledge, limited context length, and single retrieval strategies seen in previous top systems like RaNER and GAIN.\",\n    \"Direct Inspiration\": {\n        \"b32\": 1,\n        \"b5\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.7,\n        \"b4\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b25\": 0.5,\n        \"b17\": 0.5\n    }\n}\n```"], "64741a3ad68f896efaa62202": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of class collapse in supervised contrastive learning (CL) and feature suppression in unsupervised CL. It proposes a unified framework to understand which semantically relevant features are learned and leverages this to characterize and suggest solutions for these issues.\",\n    \"Direct Inspiration\": [\"b9\", \"b3\", \"b19\"],\n    \"Indirect Inspiration\": [\"b5\", \"b38\", \"b30\"],\n    \"Other Inspiration\": [\"b15\", \"b40\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning high-quality representations in contrastive learning (CL) that generalize well to downstream tasks. It proposes a unified framework to characterize class collapse in supervised CL and feature suppression in unsupervised CL. The paper also provides theoretical justifications for combining supervised and unsupervised CL to avoid these issues.\",\n  \"Direct Inspiration\": [\"b9\", \"b3\", \"b19\"],\n  \"Indirect Inspiration\": [\"b5\", \"b38\", \"b48\", \"b26\", \"b44\", \"b2\", \"b46\"],\n  \"Other Inspiration\": [\"b15\", \"b20\", \"b49\", \"b40\", \"b14\", \"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of class collapse and feature suppression in contrastive learning (CL). It proposes a unified framework for supervised and unsupervised CL to understand which semantically relevant features are learned. The paper provides the first theoretical characterizations of class collapse and feature suppression, identifies the simplicity bias of SGD as a key factor, and suggests practical solutions.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1, \n    \"b3\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.9, \n    \"b27\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.8, \n    \"b40\": 0.8, \n    \"b14\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of class collapse in supervised contrastive learning (CL) and feature suppression in unsupervised CL. It proposes a unified framework for both supervised and unsupervised CL to understand and mitigate these issues. The paper suggests that the simplicity bias of Stochastic Gradient Descent (SGD) is a key factor in both phenomena and provides theoretical and empirical evidence to support this claim.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b3\": 1,\n    \"b19\": 1,\n    \"b38\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b44\": 0.7,\n    \"b48\": 0.75,\n    \"b2\": 0.7,\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.65,\n    \"b40\": 0.6,\n    \"b14\": 0.6,\n    \"b49\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of class collapse and feature suppression in both supervised and unsupervised contrastive learning (CL). The authors propose a unified framework to understand which semantically relevant features are learned, and leverage this framework to characterize and provide solutions for class collapse and feature suppression. Their findings indicate that the simplicity bias of (S)GD is a key factor in these phenomena.\",\n    \"Direct Inspiration\": {\n        \"b9\": 1,\n        \"b3\": 1,\n        \"b19\": 1\n    },\n    \"Indirect Inspiration\": {},\n    \"Other Inspiration\": {\n        \"b5\": 0.9,\n        \"b38\": 0.9,\n        \"b48\": 0.8,\n        \"b26\": 0.8,\n        \"b44\": 0.8,\n        \"b2\": 0.8,\n        \"b46\": 0.8,\n        \"b15\": 0.8\n    }\n}\n```"], "64a29621d68f896efa28fd54": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of cache eviction algorithms, focusing on the inefficiencies of LRU (Least Recently Used) and proposing FIFO-based (First-In-First-Out) algorithms enhanced with Lazy Promotion (LP) and Quick Demotion (QD) techniques. The main contributions include demonstrating that FIFO with Lazy Promotion can achieve lower miss ratios than LRU and that Quick Demotion is critical for cache efficiency.\",\n  \"Direct Inspiration\": [\"b61\", \"b68\", \"b40\", \"b51\"],\n  \"Indirect Inspiration\": [\"b7\", \"b42\", \"b78\"],\n  \"Other Inspiration\": [\"b10\", \"b14\", \"b33\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenges of improving cache eviction algorithms by proposing Lazy Promotion (LP) and Quick Demotion (QD) techniques. These techniques aim to enhance the efficiency and throughput of eviction algorithms, particularly those based on FIFO, by retaining popular objects and quickly evicting unpopular ones. The study presents a comprehensive evaluation, demonstrating that LP-FIFO can achieve lower miss ratios than LRU and that QD significantly improves the performance of state-of-the-art eviction algorithms.\",\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [\"b6\", \"b33\", \"b38\", \"b42\", \"b55\"],\n  \"Other Inspiration\": [\"b7\", \"b10\", \"b40\", \"b51\", \"b61\", \"b68\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving cache efficiency and throughput by proposing new eviction algorithms based on FIFO rather than LRU. The key contributions include the introduction of Lazy Promotion (LP) and Quick Demotion (QD) techniques that enhance FIFO's performance, making it more efficient than traditional LRU-based algorithms.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b33\": 0.9,\n    \"b38\": 0.9,\n    \"b42\": 0.9,\n    \"b55\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b10\": 0.8,\n    \"b40\": 0.8,\n    \"b49\": 0.8,\n    \"b51\": 0.8,\n    \"b61\": 0.8,\n    \"b68\": 0.8,\n    \"b78\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b3\": 0.7,\n    \"b9\": 0.7,\n    \"b17\": 0.7,\n    \"b19\": 0.7,\n    \"b22\": 0.7,\n    \"b32\": 0.7,\n    \"b50\": 0.7,\n    \"b79\": 0.7,\n    \"b80\": 0.7,\n    \"b88\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of cache eviction algorithms, focusing on improving both efficiency (miss ratio) and throughput (requests served per second). It proposes novel techniques: Lazy Promotion (LP) and Quick Demotion (QD) on top of FIFO to improve caching efficiency. The study demonstrates that these techniques can outperform traditional LRU-based algorithms and even state-of-the-art algorithms in certain scenarios.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b33\": 0.85,\n    \"b38\": 0.8,\n    \"b42\": 0.8,\n    \"b55\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.75,\n    \"b79\": 0.75,\n    \"b17\": 0.7,\n    \"b50\": 0.7,\n    \"b71\": 0.65,\n    \"b80\": 0.7,\n    \"b61\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.7,\n    \"b68\": 0.75,\n    \"b51\": 0.75,\n    \"b7\": 0.7,\n    \"b78\": 0.65,\n    \"b10\": 0.7,\n    \"b3\": 0.65,\n    \"b15\": 0.65,\n    \"b88\": 0.7,\n    \"b14\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges associated with cache eviction algorithms, particularly focusing on efficiency (miss ratio) and throughput. The authors propose breaking away from traditional LRU-based algorithms and instead utilize FIFO-based algorithms enhanced with Lazy Promotion (LP) and Quick Demotion (QD) techniques. The paper demonstrates that these techniques can lead to lower miss ratios and better scalability.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b33\": 1.0,\n    \"b38\": 1.0,\n    \"b42\": 1.0,\n    \"b55\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b61\": 0.8,\n    \"b79\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b10\": 0.7,\n    \"b17\": 0.7,\n    \"b50\": 0.7,\n    \"b68\": 0.7,\n    \"b78\": 0.7,\n    \"b80\": 0.7\n  }\n}\n```"], "64a29621d68f896efa28fd67": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of developing efficient, accurate, and online learning memory prefetchers that avoid catastrophic interference, inspired by the cognitive theory of Complementary Learning Systems (CLS) and Hebbian networks.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b14\": 0.8,\n    \"b43\": 0.8,\n    \"b48\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing efficient and accurate online memory prefetchers. It critiques the high resource usage and offline training limitations of prior DNN-based prefetching approaches. Inspired by the Complementary Learning Systems (CLS) theory from cognitive science, the authors propose a new prefetching method that incorporates biologically-inspired Hebbian networks, which are more resource-efficient and can mitigate catastrophic interference through interleaved learning.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b34\": 0.9,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b14\": 0.7,\n    \"b43\": 0.7,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b16\": 0.6,\n    \"b39\": 0.6,\n    \"b26\": 0.6,\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing efficient and adaptive memory prefetchers that can operate in real-world systems. It critiques existing deep learning (DL) based prefetchers for their impractical resource requirements and inability to learn online, introducing catastrophic interference. The authors propose a novel approach inspired by the Complementary Learning Systems (CLS) theory from cognitive science, which combines interleaved learning and resource-efficient Hebbian networks to overcome these challenges.\",\n  \"Direct Inspiration\": [\"b31\"],\n  \"Indirect Inspiration\": [\"b9\", \"b16\", \"b39\"],\n  \"Other Inspiration\": [\"b26\", \"b2\", \"b22\", \"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of developing efficient, online memory prefetchers that adapt to dynamic execution patterns. Traditional deep learning (DL) methods face high compute and storage overheads, offline training limitations, and catastrophic interference. The proposed solution draws inspiration from Complementary Learning Systems (CLS) theory and Hebbian networks to create a resource-efficient, online learning framework that mitigates interference and adapts to changing access patterns.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b13\": 0.9,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b16\": 0.8,\n    \"b39\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b22\": 0.6,\n    \"b27\": 0.6,\n    \"b11\": 0.5,\n    \"b14\": 0.5,\n    \"b43\": 0.5,\n    \"b48\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of developing efficient, accurate, and online memory prefetching systems. Traditional deep learning (DL) based prefetchers have high resource requirements, are trained offline, and suffer from catastrophic interference. The authors propose a novel approach inspired by the Complementary Learning Systems (CLS) theory and Hebbian learning principles from cognitive neuroscience to create a more efficient and adaptive prefetching system. This method aims to mitigate catastrophic interference and reduce resource consumption while maintaining high accuracy.\",\n    \"Direct Inspiration\": {\n        \"b31\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.8,\n        \"b14\": 0.8,\n        \"b43\": 0.8,\n        \"b48\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.7,\n        \"b16\": 0.7,\n        \"b39\": 0.7,\n        \"b26\": 0.7,\n        \"b2\": 0.7\n    }\n}\n```"], "64be5e653fda6d7f063a95ce": ["```json\n{\n  \"Summary\": \"The paper addresses the integration of privacy and robustness in distributed machine learning (ML), specifically focusing on the privacy-robustness-utility trilemma. The main contributions include characterizing the added error due to the interplay of privacy and robustness, introducing a new distributed ML algorithm SAFE-DSHB, and a robust aggregation rule SMEA. These contributions are aimed at ensuring differential privacy and robustness against adversarial workers while maintaining acceptable learning accuracy.\",\n  \"Direct Inspiration\": {\n    \"b32\": 0.9,\n    \"b38\": 0.85,\n    \"b73\": 0.85,\n    \"b52\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.75,\n    \"b74\": 0.7,\n    \"b76\": 0.7,\n    \"b34\": 0.7,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.65,\n    \"b1\": 0.65,\n    \"b43\": 0.6,\n    \"b23\": 0.55,\n    \"b80\": 0.55,\n    \"b36\": 0.55,\n    \"b49\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The paper addresses the challenges of privacy and robustness in distributed machine learning, particularly when the server may be curious. The key contributions include a tight analysis of the privacy-robustness-utility trilemma, a new distributed ML algorithm (SAFE-DSHB), and the introduction of a robust high-dimension aggregation rule (SMEA).\",\n    \"Inspirations\": \"The paper is inspired by previous works on differential privacy, robust distributed learning, and the interplay between privacy and robustness.\"\n  },\n  \"Direct Inspiration\": {\n    \"b38\": 0.95,\n    \"b32\": 0.9,\n    \"b52\": 0.85,\n    \"b73\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.75,\n    \"b1\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.7,\n    \"b80\": 0.7,\n    \"b36\": 0.7,\n    \"b49\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of ensuring both privacy and robustness in distributed machine learning (ML) systems, especially under conditions where the server may be curious and a subset of workers may be adversarial. The authors propose a novel algorithm, SAFE-DSHB, and introduce a robust high-dimension aggregation rule, SMEA, to achieve differential privacy and robustness simultaneously.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b38\", \"b31\", \"b37\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b32\", \"b73\", \"b52\", \"b15\", \"b1\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b50\", \"b43\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of integrating privacy and robustness in distributed machine learning (ML), specifically in the context of stochastic gradient descent (SGD). The authors present a novel algorithm, SAFE-DSHB, which ensures robustness against adversarial workers while maintaining differential privacy (DP) for each worker's data. The key contributions include a tight analysis of the error incurred by any distributed ML algorithm that ensures both privacy and robustness, and the introduction of a robust high-dimension aggregation rule, SMEA.\",\n    \"Direct Inspiration\": {\n        \"b32\": 0.9,\n        \"b38\": 0.85,\n        \"b73\": 0.8,\n        \"b52\": 0.75\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.7,\n        \"b1\": 0.65,\n        \"b43\": 0.6,\n        \"b31\": 0.55\n    },\n    \"Other Inspiration\": {\n        \"b17\": 0.5,\n        \"b56\": 0.45,\n        \"b74\": 0.4,\n        \"b76\": 0.35\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the privacy and robustness challenges in distributed machine learning (ML). It proposes an algorithm, SAFE-DSHB, which integrates differential privacy (DP) and robustness against adversarial workers. The key novelty is the precise characterization of the privacy-robustness-utility trilemma and the introduction of a new robust aggregation rule, SMEA. The paper also provides tight bounds for the error incurred by any distributed ML algorithm meeting these criteria.\",\n  \"Direct Inspiration\": {\n    \"b32\": 0.95,\n    \"b38\": 0.92,\n    \"b73\": 0.88,\n    \"b52\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.80,\n    \"b1\": 0.78\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.75,\n    \"b74\": 0.72,\n    \"b76\": 0.70,\n    \"b34\": 0.68,\n    \"b27\": 0.65\n  }\n}\n```"], "64ba03413fda6d7f062732f5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of reinforcement learning (RL) with high-dimensional continuous action spaces. It proposes a novel framework, Reparameterized Policy Gradient (RPG), which models continuous RL policy as a multimodal density function using a generative model for multimodal action parameterization. The framework aims to improve exploration efficiency and escape local optima by combining multimodal policy parameterization with a learned world model and a novel density estimator.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b18\": 0.85,\n    \"b44\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b70\": 0.75,\n    \"b71\": 0.75,\n    \"b34\": 0.75,\n    \"b57\": 0.7,\n    \"b62\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.65,\n    \"b80\": 0.65,\n    \"b81\": 0.6,\n    \"b60\": 0.6,\n    \"b79\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of reinforcement learning (RL) with high-dimensional continuous action spaces, specifically focusing on the limitations of single-modal policy parameterization. The authors propose a novel framework that models the continuous RL policy as a multimodal density function using a generative model. This approach enhances exploration efficiency and helps the policy escape local optima. The proposed Reparameterized Policy Gradient (RPG) method leverages a learned world model and a novel density estimator to solve challenging sparse-reward tasks more effectively.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b18\": 1,\n    \"b44\": 1,\n    \"b70\": 1,\n    \"b71\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.9,\n    \"b62\": 0.8,\n    \"b57\": 0.8,\n    \"b80\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b41\": 0.7,\n    \"b2\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of reinforcement learning (RL) in high-dimensional continuous action spaces, particularly the limitations of single-modal policy parameterization. The authors propose a novel framework, the Reparameterized Policy Gradient (RPG), which models the continuous RL policy as a multimodal density function through multimodal action parameterization. This approach allows for better exploration and optimization in RL by leveraging a generative model to parameterize multimodal policies and using variational methods to model the posterior of optimal trajectories.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b18\": 0.8,\n    \"b44\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b70\": 0.7,\n    \"b71\": 0.7,\n    \"b81\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b13\": 0.6,\n    \"b80\": 0.6,\n    \"b62\": 0.6,\n    \"b57\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of reinforcement learning (RL) in high-dimensional continuous action spaces, particularly the limitations of single-modal policies and their impact on optimization and exploration. The authors propose a novel framework, Reparameterized Policy Gradient (RPG), which uses multimodal policy parameterization and a learned world model to improve exploration efficiency and escape local optima. This is achieved through a sequence modeling perspective and variational inference, enabling multimodal trajectory sampling and optimization.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b7\": 0.95,\n        \"b8\": 0.85,\n        \"b18\": 0.9,\n        \"b31\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b34\": 0.8,\n        \"b70\": 0.8,\n        \"b81\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.75,\n        \"b22\": 0.75,\n        \"b44\": 0.75,\n        \"b62\": 0.75,\n        \"b80\": 0.75\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of reinforcement learning (RL) in high-dimensional continuous action spaces, specifically focusing on the limitations of single-modal policies. The authors propose a novel multimodal policy parameterization approach using variational methods and a sequence modeling perspective. The Reparameterized Policy Gradient (RPG) framework is introduced to optimize multimodal trajectories and improve exploration efficiency, particularly in sparse reward tasks.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b31\", \"b18\", \"b44\", \"b80\", \"b62\", \"b57\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b2\", \"b53\", \"b10\", \"b13\", \"b8\", \"b41\", \"b70\", \"b71\", \"b72\", \"b81\", \"b29\", \"b34\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b60\", \"b79\", \"b33\", \"b51\", \"b32\", \"b47\", \"b35\", \"b50\", \"b1\", \"b42\", \"b16\", \"b54\", \"b66\", \"b28\", \"b39\", \"b11\", \"b36\", \"b25\", \"b14\", \"b48\"]\n    }\n}\n```"], "6426ed4490e50fcafd443eef": ["```json\n{\n  \"Summary\": \"The paper tackles the performance and energy bottleneck in modern computing systems posed by the main memory. The proposed solution, PIMFlow, is a compiler and runtime solution that accelerates CNN models on a PIM-enabled GPU memory by optimizing compiler and runtime support, extending DRAM-PIM memory architecture, and enabling mixed-parallel GPU-PIM execution.\",\n  \"Direct Inspiration\": [\n    \"b25\",\n    \"b36\",\n    \"b37\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b2\",\n    \"b21\",\n    \"b46\",\n    \"b47\",\n    \"b57\"\n  ],\n  \"Other Inspiration\": [\n    \"b16\",\n    \"b17\",\n    \"b20\",\n    \"b26\",\n    \"b32\",\n    \"b50\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the performance and energy bottleneck caused by the memory wall in modern computing systems. The paper proposes PIMFlow, a compiler and runtime solution designed to accelerate CNN models on DRAM-PIM hardware by leveraging mixed-parallel execution across GPUs and DRAM-PIM. The key innovation is the transformation of model graphs to exploit GPU-PIM parallel execution, optimizing memory layout, and extending DRAM-PIM architecture with new commands for efficient convolution operations.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b37\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b36\": 0.8,\n    \"b47\": 0.8,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b32\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include addressing the memory wall in modern computing systems and optimizing the acceleration of convolutional neural network (CNN) models using processing-in-memory (PIM) technology. The authors propose PIMFlow, a compiler, and runtime solution that leverages both GPU and PIM for accelerating CNN models. PIMFlow utilizes PIM-aware graph transformations and mixed-parallel execution models to improve PIM utilization and reduce data movement overheads.\",\n    \"Direct Inspiration\": {\n        \"b25\": 1,\n        \"b37\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.7,\n        \"b26\": 0.7,\n        \"b36\": 0.8,\n        \"b47\": 0.6,\n        \"b57\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.5,\n        \"b21\": 0.5,\n        \"b32\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": [\n    \"The main challenge addressed in the paper is the performance and energy bottleneck caused by the main memory in modern computing systems, particularly due to the 'memory wall'.\",\n    \"The authors propose PIMFlow, a compiler and runtime solution that accelerates CNN models on a PIM-enabled GPU memory with PIM-aware transformations and command generation support.\",\n    \"The key insights include leveraging DRAM-PIM for mixed-parallel execution and extending DRAM-PIM memory architecture for efficient convolution operations.\"\n  ],\n  \"Direct Inspiration\": {\n    \"b25\": 0.95,\n    \"b37\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b21\": 0.70,\n    \"b26\": 0.70,\n    \"b47\": 0.65,\n    \"b32\": 0.60\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.55,\n    \"b50\": 0.50,\n    \"b56\": 0.50\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accelerating convolutional layers in CNN models using DRAM-PIM hardware. The proposed solution, PIMFlow, is a compiler and runtime system designed to optimize mixed-parallel execution across GPU and DRAM-PIM. The key contributions include extending the DRAM-PIM architecture, introducing new PIM commands, and developing PIM-aware graph transformations to improve PIM utilization and performance.\",\n  \"Direct Inspiration\": {\n    \"reference\": [\"b25\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b36\", \"b37\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b2\", \"b14\", \"b26\", \"b32\", \"b47\"]\n  }\n}\n```"], "638eb2ef90e50fcafd58b2af": ["```json\n{\n  \"Summary\": \"The paper focuses on the challenge of integrating retrieval and reading within a single Transformer model for open-domain question answering (QA). The proposed method, Retrieval as Attention (ReAtt), leverages self-attention for retrieval and aims for fully end-to-end training to improve domain adaptation and simplify the training process. The ReAtt model is tested on the Natural Questions dataset and BEIR benchmark, demonstrating competitive performance.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b15\": 0.85,\n    \"b17\": 0.9,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.75,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b19\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating retrieval and reading processes in knowledge-intensive tasks such as question answering within a single Transformer model to enable end-to-end training. It proposes the Retrieval as Attention (ReAtt) model that uses self-attention mechanisms for retrieval, thus unifying retriever and reader components.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b31\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b13\": 0.5,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performing both retrieval and reading within a single Transformer model to achieve competitive performance in open-domain QA. It proposes the Retrieval as Attention (ReAtt) method, which uses the T5 model as both retriever and reader, and optimizes retrieval and QA performance through end-to-end training. The method leverages self-attention for retrieval and cross-document adjustment to improve relevance scoring.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b17\": 0.8,\n    \"b11\": 0.7,\n    \"b15\": 0.6,\n    \"b44\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b35\": 0.5,\n    \"b43\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating retrieval and reading in a single end-to-end model for knowledge-intensive tasks like open-domain QA. The authors propose an innovative approach using the Transformer model's self-attention mechanism to perform both retrieval and reading, eliminating the need for separate retriever warm-up and simplifying adaptation to new domains.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b10\": 1.0,\n    \"b11\": 1.0,\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b17\": 0.8,\n    \"b20\": 0.8,\n    \"b31\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b19\": 0.5,\n    \"b35\": 0.5,\n    \"b37\": 0.5,\n    \"b43\": 0.5,\n    \"b44\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating retrieval and reading in a single end-to-end trainable model for knowledge-intensive tasks like open-domain question answering. The proposed solution, Retrieval as Attention (ReAtt), leverages self-attention within a Transformer model to perform both retrieval and reading, aiming to eliminate the need for retrieval-specific annotation and warm-up, thus simplifying retrieval-augmented training and making adaptation to new domains easier.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b19\": 0.6,\n    \"b31\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.4,\n    \"b22\": 0.4\n  }\n}\n```"], "63c8b59590e50fcafd90b721": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of using Shape-from-Polarization (SfP) methods for estimating high-quality surface normals. It proposes novel methods leveraging event cameras to balance the trade-off between speed and resolution. The paper introduces a geometry-based method and a learning-based framework for surface normal estimation using event cameras, outperforming previous state-of-the-art approaches.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b51\": 1.0,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b24\": 0.6,\n    \"b47\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of estimating high-quality surface normals from polarization images using event cameras. It proposes a novel Shape-from-Polarization (SfP) method that leverages the high-speed and high-resolution capabilities of event cameras to improve surface normal estimation. The paper introduces a geometry-based method and a learning-based framework, achieving significant improvements over traditional methods.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b51\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of estimating high-quality surface normals from polarization images, specifically focusing on the speed-resolution trade-off in Shape-from-Polarization (SfP) methods. It proposes a novel approach using event cameras to improve acquisition speed and resolution. The authors introduce both geometry-based and learning-based algorithms to estimate surface normals from events, achieving significant improvements over existing methods.\",\n    \"Direct Inspiration\": {\n        \"b18\": 1,\n        \"b22\": 1,\n        \"b51\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b35\": 0.8,\n        \"b14\": 0.7,\n        \"b45\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.5,\n        \"b7\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of estimating high-quality surface normals from polarization images, leveraging event cameras to overcome the speed-resolution trade-off inherent in traditional methods. The authors propose a novel Shape-from-Polarization (SfP) approach using event cameras, resulting in higher acquisition speeds and full resolution. They introduce two algorithms: a geometry-based method and a deep learning framework using a U-Net network for improved accuracy. The approach is validated on both synthetic and real-world datasets, showcasing significant improvements over previous state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b51\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.8,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b34\": 0.6,\n    \"b50\": 0.6,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Estimating high-quality surface normals from polarization images remains challenging due to constraints in spatial resolution and acquisition time. Traditional Division of Focal Plane (DoFP) and Division of Time (DoT) methods have limitations in resolution and speed respectively.\",\n    \"proposed_algorithm\": \"The paper proposes using event cameras to capture polarization information at high speeds and resolutions, introducing two algorithms for surface normal estimation: one geometry-based and one learning-based using a U-Net architecture.\"\n  },\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b51\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b52\": 0.6,\n    \"b32\": 0.5\n  }\n}\n```"], "64bb03bb3fda6d7f06002e9f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of performance degradation in tiered NUMA memory systems due to the imbalance in access latency between near and far memory nodes. The authors propose a dynamic cache allocation technique called T-CAT, which partitions and dynamically resizes the last-level cache (LLC) to balance memory traffic and access latency between near and far memory nodes.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b9\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b8\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b7\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the imbalance between processor and memory performance in data-intensive applications by proposing a dynamic cache allocation technique called T-CAT. This technique aims to improve the performance of tiered memory systems by dynamically partitioning the last-level cache (LLC) for near and far memory, thus balancing the access latency between different memory nodes.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b3\": 0.7,\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of imbalanced performance between processor and memory in tiered memory systems, particularly focusing on NUMA systems with near and far memory. It proposes a dynamic cache allocation technique, T-CAT, which partitions the last-level cache for near and far memory and adjusts the partitions based on average access latency to balance memory traffic and improve performance.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b8\": 0.9,\n    \"b9\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b10\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b3\": 0.65,\n    \"b11\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performance degradation in memory-intensive applications due to the imbalance between processor and memory performance in tiered memory systems. It proposes a dynamic cache allocation technique called T-CAT to partition and resize the last-level cache (LLC) for near and far memory, balancing the average memory access latency between them.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.85,\n    \"b6\": 0.85,\n    \"b7\": 0.85,\n    \"b8\": 0.85,\n    \"b9\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.75,\n    \"b2\": 0.75,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance imbalance between processor and memory in modern computing systems, particularly in tiered memory systems with near and far memory. It proposes a dynamic cache allocation technique called T-CAT to improve performance by balancing memory access latency between near and far memory nodes.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.95,\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```"], "64cc25d83fda6d7f063be43e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of transitioning to a 128-bit virtual address space and proposes a novel microarchitecture that uses clustering to manage hardware complexity and mitigate performance penalties. The authors propose a 128-bit cluster dedicated to address computations and a separate 64-bit cluster for general-purpose integer computations, leveraging region-based compression to reduce hardware overhead.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b5\": 1.0,\n    \"b6\": 1.0,\n    \"b7\": 1.0,\n    \"b8\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b1\": 0.6,\n    \"b3\": 0.7,\n    \"b9\": 0.5,\n    \"b10\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of transitioning to a 128-bit virtual address (VA) space without incurring significant performance penalties. It proposes a clustered microarchitecture where a 128-bit cluster is dedicated to address computations while general-purpose integer computations are handled by a 64-bit cluster. The paper leverages region-based compression to mitigate the increased hardware overhead.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1.0,\n        \"b4\": 0.9,\n        \"b5\": 0.9,\n        \"b8\": 0.9,\n        \"b12\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b7\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.6,\n        \"b10\": 0.6,\n        \"b11\": 0.6,\n        \"b13\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of transitioning to a 128-bit virtual address (VA) microarchitecture, focusing on mitigating the area, power, and latency costs associated with naive scaling. The proposed solution involves using a clustered architecture, where a 128-bit cluster handles address computations while a 64-bit cluster handles general-purpose integer computations. Additionally, the paper suggests leveraging region-based compression techniques to reduce the footprint of common structures.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b5\": 1,\n    \"b6\": 1,\n    \"b7\": 1,\n    \"b8\": 1,\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b0\": 0.8,\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of transitioning to a 128-bit virtual address space, focusing on the microarchitectural implications and proposing a novel clustered microarchitecture. It aims to mitigate the increased area, latency, and power consumption associated with naive scaling by dividing the hardware complexity into clusters dedicated to different types of computations.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b4\": 1.0,\n    \"b5\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b8\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b1\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of transitioning to a 128-bit processor microarchitecture, emphasizing the need to manage increased area, latency, and power consumption. The authors propose a novel clustered microarchitecture that separates 128-bit address computations from general-purpose integer computations, leveraging region-based compression to mitigate hardware overhead.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6\n  }\n}\n```"], "6449e7fc582c1376bbfc600e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently comparing dense event representations for event cameras using the Gromov-Wasserstein Discrepancy (GWD). The authors propose a novel method to measure distortion between raw events and their representations without needing to train neural networks, thus speeding up the comparison process significantly. This method enables the discovery of a new, powerful event representation called ERGO-12, which outperforms existing representations in object detection tasks.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.8,\n    \"b48\": 0.8,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.6,\n    \"b51\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently comparing dense event representations from event cameras, which are sparse and asynchronous in nature. Traditional methods involve training deep-learning models for each representation, which is time-consuming. The authors propose a novel approach using the Gromov-Wasserstein Discrepancy (GWD) to compare event representations without the need for network training. This method allows for the optimization of event representations, unveiling a new powerful representation, ERGO-12, that improves performance on object detection tasks.\",\n  \"Direct Inspiration\": [\"b13\"],\n  \"Indirect Inspiration\": [\"b34\", \"b48\", \"b0\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the difficulty in comparing event representations for event cameras, which are sparse and asynchronous, using traditional methods. The paper proposes a novel and efficient method to compare these representations without the need for extensive neural network training. This is achieved through the introduction of the Gromov-Wasserstein Discrepancy (GWD) metric, which measures the distortion introduced when converting raw events to their representations. The paper demonstrates that the GWD preserves task performance rankings across various datasets and neural network backbones and uses it to optimize event representations, resulting in a new, powerful 12-channel Event Representation (ERGO-12) that outperforms current state-of-the-art methods in object detection.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b34\": 0.85,\n    \"b48\": 0.8,\n    \"b0\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.7,\n    \"b23\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b47\": 0.6,\n    \"b51\": 0.6,\n    \"b38\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of comparing dense event representations from event cameras, proposing a novel method using the Gromov-Wasserstein Discrepancy (GWD) to measure distortion without training neural networks. This method allows for an efficient hyperparameter search, leading to the development of a new event representation, ERGO-12, which outperforms state-of-the-art methods in object detection tasks.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1,\n    \"b13\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.8,\n    \"b47\": 0.8,\n    \"b51\": 0.8,\n    \"b48\": 0.8,\n    \"b0\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently comparing event representations from event cameras without the need for extensive neural network training. It proposes the Gromov-Wasserstein Discrepancy (GWD) as a metric to measure the distortion introduced by converting raw events to representations. Using this metric, the paper introduces a new event representation, ERGO-12, which outperforms existing representations in object detection tasks.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b34\": 0.8,\n    \"b48\": 0.8,\n    \"b0\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.6,\n    \"b38\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b44\": 0.5,\n    \"b36\": 0.5\n  }\n}\n```"], "6523793e939a5f4082e182a2": ["```json\n{\n  \"Summary\": \"The paper addresses the significant performance bottleneck of address translation in modern data-intensive workloads. It identifies the high miss rate at the last-level TLB (L2 TLB) and the resulting high-latency page table walks (PTWs) as primary challenges. The proposed algorithm aims to increase the translation reach by leveraging underutilized resources in the cache hierarchy, rather than expanding hardware TLBs or introducing large software-managed TLBs.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b12\": 1,\n    \"b26\": 0.9,\n    \"b27\": 0.9,\n    \"b28\": 0.9,\n    \"b29\": 0.9,\n    \"b30\": 0.9,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b16\": 0.8,\n    \"b21\": 0.8,\n    \"b32\": 0.8,\n    \"b33\": 0.8,\n    \"b34\": 0.8,\n    \"b35\": 0.8,\n    \"b36\": 0.8,\n    \"b37\": 0.8,\n    \"b38\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b20\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.7,\n    \"b25\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the high latency in address translation caused by high miss rates in the last-level TLB, particularly exacerbated in virtualized environments. The proposed solution leverages underutilized cache hierarchy to store TLB entries, aiming to increase the translation reach without incurring the drawbacks of larger hardware or software-managed TLBs.\",\n  \"Direct Inspiration\": [\"b11\", \"b12\", \"b26\", \"b27\", \"b28\", \"b29\", \"b30\", \"b31\", \"b32\"],\n  \"Indirect Inspiration\": [\"b13\", \"b14\", \"b15\", \"b16\", \"b17\", \"b18\", \"b19\", \"b20\", \"b21\", \"b22\", \"b23\", \"b24\", \"b25\", \"b33\", \"b34\", \"b35\", \"b36\", \"b37\", \"b38\", \"b39\"],\n  \"Other Inspiration\": [\"b0\", \"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b6\", \"b7\", \"b8\", \"b9\", \"b10\", \"b72\", \"b73\", \"b74\", \"b75\", \"b76\", \"b77\", \"b78\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the high miss rate and latency of last-level TLBs (L2 TLB) in modern data-intensive workloads, which is exacerbated in virtualized environments due to two-level address translation. The proposed solution leverages the underutilized cache hierarchy to store TLB entries, thereby increasing translation reach without requiring additional hardware or complex software modifications.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b27\": 0.9,\n    \"b28\": 0.9,\n    \"b29\": 0.9,\n    \"b30\": 0.9,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the significant performance bottleneck caused by address translation in modern data-intensive workloads. The proposed solution leverages the underutilized cache hierarchy to store TLB entries, thereby increasing the translation reach and reducing the high latency of page table walks (PTWs). This method is designed to be effective in both native and virtualized environments, without requiring contiguous physical allocations and remaining transparent to both applications and the OS.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b27\": 1,\n    \"b28\": 1,\n    \"b29\": 1,\n    \"b30\": 1,\n    \"b31\": 1,\n    \"b32\": 1,\n    \"b33\": 1,\n    \"b34\": 1,\n    \"b35\": 1,\n    \"b36\": 1,\n    \"b37\": 1,\n    \"b38\": 1,\n    \"b39\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b20\": 0.8,\n    \"b21\": 0.8,\n    \"b22\": 0.8,\n    \"b23\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b25\": 0.7,\n    \"b72\": 0.7,\n    \"b73\": 0.7,\n    \"b74\": 0.7,\n    \"b75\": 0.7,\n    \"b76\": 0.7,\n    \"b77\": 0.7,\n    \"b78\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high miss rates and latencies in address translation for modern data-intensive and virtualized workloads. It proposes leveraging underutilized resources in the existing cache hierarchy to store TLB entries, thus increasing the translation reach of the processor's TLB hierarchy while avoiding the drawbacks of large hardware and software-managed TLBs.\",\n  \"Direct Inspiration\": [\"b16\", \"b17\", \"b18\", \"b19\", \"b20\", \"b21\", \"b22\", \"b23\", \"b24\"],\n  \"Indirect Inspiration\": [\"b13\", \"b14\", \"b15\", \"b26\", \"b27\", \"b28\", \"b29\", \"b30\", \"b31\", \"b32\", \"b33\", \"b34\", \"b35\", \"b36\", \"b37\", \"b38\", \"b39\"],\n  \"Other Inspiration\": [\"b11\", \"b12\", \"b25\"]\n}\n```"], "64c78ba33fda6d7f06dbcb16": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of efficiently executing graph neural network (GNN) computations on multi-GPU systems. The authors propose a novel approach to formalize GNN computation as a fine-grained dynamic software pipeline to overlap communication and computation, thus hiding the communication costs. This involves pipeline construction, mapping to GPU units, and dynamic adaptation with runtime feedback.\",\n  \"Direct Inspiration\": {\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b4\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b17\": 0.5,\n    \"b27\": 0.5,\n    \"b44\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of efficiently executing Graph Neural Networks (GNNs) on multi-GPU systems. The primary challenges include crafting an efficient pipeline structure, mapping the pipeline to GPU processing units, and dynamically adapting the pipeline configuration to various GNN inputs and hardware setups. The authors propose MGG, a novel GNN-tailored pipeline construction technique with workload management, hybrid data placement, and intelligent runtime optimization to overcome these challenges.\",\n  \"Direct Inspiration\": {\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.8,\n    \"b49\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7,\n    \"b42\": 0.7,\n    \"b48\": 0.7,\n    \"b17\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper revolve around optimizing the execution of graph neural networks (GNNs) on multi-GPU platforms. The key challenges include designing an efficient pipeline structure, mapping the pipeline to GPU processing units, and dynamically adapting the pipeline configuration to different GNN inputs and hardware setups. The paper introduces MGG, a holistic system for multi-GPU GNNs, employing a fine-grained dynamic software pipeline to overlap computation and communication, thus improving performance.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b11\": 0.7,\n    \"b17\": 0.7,\n    \"b25\": 0.6 \n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b37\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of graph neural networks (GNNs) on multi-GPU systems, particularly focusing on overcoming the bottlenecks caused by irregular and sparse local memory access during neighbor aggregation. The proposed solution, MGG, introduces a fine-grained dynamic software pipeline to overlap communication and computation, thereby hiding communication costs. The paper outlines three major contributions: a GNN-tailored pipeline construction technique, a GPU-aware pipeline mapping strategy, and an intelligent runtime for dynamic performance improvement.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b48\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b21\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b27\": 0.6,\n    \"b45\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the inefficiencies in scaling GNN computation on multi-GPU platforms due to irregular and sparse local memory access and the high cost of inter-GPU communication. The proposed algorithm addresses these challenges by abstracting GNN computation as a fine-grained dynamic software pipeline to encourage communication and computation overlapping, thus hiding communication costs and improving performance. This involves crafting the pipeline structure, mapping it to GPU processing units, and dynamically adapting the pipeline configuration based on runtime feedback.\",\n  \"Direct Inspiration\": {\n    \"b35\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b48\": 0.7,\n    \"b45\": 0.6,\n    \"b49\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```"], "65260ee8cd549670787e1513": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of training a visual language model (VLM) from an off-the-shelf pretrained language model while retaining its natural language processing (NLP) capabilities. The main inspiration is drawn from the comparison between p-tuning and LoRA in efficient finetuning, and the paper proposes a novel approach by adding a trainable visual expert module to each layer of the language model to enable deep visual-language feature alignment.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b18\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b3\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.7,\n    \"b45\": 0.7,\n    \"b60\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training a Visual Language Model (VLM) from an off-the-shelf pretrained language model while retaining its NLP capabilities and adding strong visual understanding abilities. The proposed method, CogVLM, adds a trainable visual expert to each layer of the language model to enable deep visual-language feature alignment. This approach is inspired by efficient finetuning methods in NLP and aims to overcome the limitations of shallow alignment methods, which lead to weaker performance and hallucination issues in chat-style VLMs.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b3\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.7,\n    \"b10\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is to train a Visual Language Model (VLM) from an off-the-shelf pretrained language model while retaining the language model's NLP capabilities and adding superior visual understanding abilities. The novel approach proposed by the authors involves adding a trainable visual expert module to the language model, enabling deep visual-language feature alignment without affecting the original language model's behavior when no image input is present.\",\n    \"Direct Inspiration\": {\n        \"b18\": 0.9,\n        \"b26\": 0.9,\n        \"b3\": 0.8,\n        \"b8\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.7,\n        \"b45\": 0.7,\n        \"b10\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b27\": 0.5,\n        \"b46\": 0.5,\n        \"b38\": 0.4,\n        \"b56\": 0.4\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of training a Visual Language Model (VLM) from a pretrained large language model while retaining strong Natural Language Processing (NLP) capabilities. The proposed CogVLM model introduces a visual expert module to enable deep fusion of vision and language features, overcoming the shortcomings of shallow alignment methods.\",\n    \"Direct Inspiration\": {\n        \"b26\": 0.9,\n        \"b18\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.8,\n        \"b3\": 0.8,\n        \"b14\": 0.8,\n        \"b27\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.7,\n        \"b44\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating visual understanding into large language models (LLMs) without compromising their natural language processing (NLP) capabilities. It critiques existing shallow alignment methods for their poor performance and introduces a new approach called CogVLM, which incorporates a visual expert module to enable deep fusion of visual and language features. The proposed model achieves state-of-the-art performance on multiple benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b3\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b27\": 0.7,\n    \"b60\": 0.7\n  }\n}\n```"], "6503bec83fda6d7f067c7787": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving prompt engineering for in-context learning (ICL) in large language models (LLMs) by leveraging the model's existing knowledge to select the most effective demonstrations. The novel method proposed involves selecting demonstrations based on semantic similarity, label ambiguity, and mis-classified examples' model predictions, which significantly enhances performance in multi-class classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b18\": 1.0,\n    \"b23\": 1.0,\n    \"b29\": 0.9,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b20\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b21\": 0.7,\n    \"b24\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in in-context learning (ICL) using large language models (LLMs), particularly focusing on the sensitivity of LLMs to prompt choices and the difficulty of prompt engineering. The proposed method leverages the model's existing knowledge to select more effective demonstrations by considering semantic similarity, the model's ambiguity around output labels, and mis-classified examples.\",\n    \"Direct Inspiration\": [\"b16\", \"b18\", \"b20\", \"b23\", \"b29\", \"b31\"],\n    \"Indirect Inspiration\": [\"b10\", \"b21\", \"b22\", \"b24\"],\n    \"Other Inspiration\": [\"b3\", \"b9\", \"b11\", \"b14\", \"b28\", \"b35\", \"b36\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of prompt engineering for large language models (LLMs) in in-context learning (ICL). The proposed method aims to improve the selection of ICL demonstrations by leveraging the model's existing knowledge and its ambiguity in label prediction. The approach involves selecting demonstrations based on semantic similarity, label ambiguity, and misclassification by the model to enhance performance in text classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b10\": 0.9,\n    \"b18\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b29\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.6,\n    \"b14\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of selecting effective in-context learning (ICL) demonstrations for large language models (LLMs) to enhance performance on multi-class classification tasks. The key contribution is a method that selects demonstrations based on both semantic similarity and the model's existing knowledge about label ambiguity and misclassifications. The proposed method shows significant improvements over baseline models.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b18\": 0.9,\n    \"b23\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of prompt engineering in in-context learning (ICL) for large language models (LLMs). It proposes a method to select more effective demonstrations by leveraging the model's existing knowledge and focusing on ambiguous label sets. The method involves using semantic similarity to rank training data, identifying ambiguous labels the model struggles with, and selecting misclassified examples that lie within this ambiguous label set.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b18\": 0.95,\n    \"b23\": 0.9,\n    \"b29\": 0.85,\n    \"b31\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b26\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b22\": 0.7\n  }\n}\n```"], "645dad16d68f896efad9df53": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the efficiency of Vision Transformers (ViTs) in terms of memory access, computation redundancy, and parameter usage. The proposed solution, EfficientViT, introduces a novel transformer architecture with a sandwich layout, cascaded group attention module, and parameter reallocation strategy to enhance memory, computation, and parameter efficiency.\",\n  \"Direct Inspiration\": [\"b42\", \"b43\", \"b67\"],\n  \"Indirect Inspiration\": [\"b9\", \"b18\", \"b24\", \"b48\", \"b69\"],\n  \"Other Inspiration\": [\"b2\", \"b16\", \"b19\", \"b29\", \"b30\", \"b31\", \"b50\", \"b71\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of large model sizes and high computation overhead in Vision Transformers (ViTs). It proposes a new family of memory-efficient transformer models named EfficientViT. The key innovations include a sandwich layout to reduce memory access time, a cascaded group attention module to improve computation efficiency, and parameter reallocation to enhance parameter usage efficiency.\",\n    \"Direct Inspiration\": {\n        \"b42\": 1,\n        \"b67\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b29\": 0.8,\n        \"b43\": 0.9,\n        \"b70\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b24\": 0.6,\n        \"b48\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high model sizes and computation overhead in Vision Transformers (ViTs) for real-time applications. It proposes a new family of memory efficient transformer models called EfficientViT, which focuses on memory access, computation redundancy, and parameter usage. The novel methods include a sandwich layout block, cascaded group attention (CGA) module, and parameter reallocation strategies to improve memory and computation efficiency without compromising performance.\",\n  \"Direct Inspiration\": {\n    \"b42\": 1.0,\n    \"b67\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.8,\n    \"b48\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b85\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the efficiency of Vision Transformers (ViTs) by focusing on memory access, computation redundancy, and parameter usage. The authors propose EfficientViT, which incorporates a sandwich layout, cascaded group attention, and parameter reallocation strategies to enhance efficiency without compromising performance.\",\n  \"Direct Inspiration\": {\n    \"b42\": 1,\n    \"b67\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b29\": 0.8,\n    \"b43\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b85\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the large model sizes and computational costs of Vision Transformers (ViTs), which make them unsuitable for real-time applications. The paper proposes new memory-efficient transformer models named EfficientViT, focusing on improving memory access, reducing computation redundancy, and optimizing parameter usage to enhance inference speed without compromising performance.\",\n  \"Direct Inspiration\": {\n    \"b42\": 0.9,\n    \"b67\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.8,\n    \"b48\": 0.7,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b29\": 0.5,\n    \"b70\": 0.5\n  }\n}\n```"], "64dbf5883fda6d7f060c36fd": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of managing latency and power consumption in serverless computing, specifically within the Function-as-a-Service (FaaS) paradigm. The authors propose DVFaaS, a framework that employs Dynamic Voltage and Frequency Scaling (DVFS) controlled by a PID controller to dynamically adjust core frequencies, ensuring QoS requirements are met while minimizing power consumption.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.8,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b12\": 0.5,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge discussed in the paper is the regulation of end-to-end execution latency in serverless computing environments while minimizing power consumption. The authors propose DVFaaS, a QoS-aware serverless framework extension that employs Dynamic Voltage and Frequency Scaling (DVFS) to regulate execution latency through a PID controller and fine-tuned frequency scaling.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b4\", \"b8\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b5\", \"b6\", \"b10\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b1\", \"b2\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces DVFaaS, a QoS-aware and function-agnostic serverless framework extension that employs Dynamic Voltage and Frequency Scaling (DVFS) to regulate the end-to-end execution latency of serverless function chains, aiming to minimize power consumption while maintaining QoS requirements. The primary challenges addressed include the insufficiency of Linux frequency governors for QoS regulation and the need for a real-system runtime mechanism to manage serverless chains' latency and power consumption efficiently.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0,\n        \"b5\": 1.0,\n        \"b8\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8,\n        \"b2\": 0.8,\n        \"b6\": 0.8,\n        \"b10\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.7,\n        \"b9\": 0.7,\n        \"b11\": 0.7,\n        \"b12\": 0.7,\n        \"b13\": 0.7,\n        \"b14\": 0.7,\n        \"b15\": 0.7,\n        \"b16\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents DVFaaS, a QoS-aware and function-agnostic serverless framework extension that applies Dynamic Voltage and Frequency Scaling (DVFS) to regulate end-to-end execution latency in serverless workflows, aiming to minimize power consumption while meeting QoS requirements.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.8,\n    \"b8\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b6\": 0.6,\n    \"b10\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b9\": 0.55,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.55,\n    \"b14\": 0.4,\n    \"b15\": 0.45,\n    \"b16\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces DVFaaS, a QoS-aware and function-agnostic serverless framework extension that leverages Dynamic Voltage and Frequency Scaling (DVFS) to regulate end-to-end execution latency while minimizing power consumption. The primary challenges addressed include the insufficiency of existing Linux frequency governors to satisfy QoS requirements and the need for a real-system runtime mechanism to regulate serverless chains' latency with precision.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b6\": 0.8\n  }\n}\n```"], "62ea18d35aee126c0fca1369": ["```json\n{\n  \"Summary\": \"The paper introduces SLED, a method to efficiently apply transformer-based pretrained language models to long texts by partitioning them into overlapping chunks and using a pretrained decoder to handle global cross-chunk reasoning. The main challenges addressed include the quadratic complexity of transformers on long texts and the difficulty in fusing information across chunks. The paper is motivated by the fusion-in-decoder (FiD) approach and shows competitive results on the SCROLLS benchmark.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b23\": 0.9,\n    \"b34\": 0.9,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b37\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying transformer-based pretrained language models (LMs) to long texts, overcoming their quadratic complexity in input length. The authors introduce SLED (SLiding-Encoder and Decoder), a method that partitions long documents into overlapping chunks processed by a pretrained encoder and then uses a pretrained decoder for global contextualization. SLED shows significant improvements in performance on long-range language understanding tasks, such as those in the SCROLLS benchmark, and can be used as a diagnostic tool for analyzing long-range benchmarks.\",\n  \"Direct Inspiration\": {\n    \"Fusion in Decoder approach by [b17]\": \"b17\"\n  },\n  \"Indirect Inspiration\": {\n    \"Efficient transformer variants\": \"b19\",\n    \"Sparse attention methods\": \"b51\",\n    \"Local sparse attention in LED\": \"b2\",\n    \"Long-range language understanding tasks\": \"b37\",\n    \"Pretrained models like BART and T5\": [\"b23\", \"b34\"]\n  },\n  \"Other Inspiration\": {\n    \"SCROLLS benchmark\": \"b37\",\n    \"Evaluation datasets like SQuAD 1.1 and HotpotQA\": [\"b35\", \"b49\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the inefficiency of transformer-based pretrained language models when applied to long texts due to their quadratic complexity. The authors propose SLED, a method that partitions long documents into overlapping chunks, processes them independently with a pretrained encoder, and uses a decoder to handle global reasoning. The novelty lies in its use of off-the-shelf pretrained models without the need for specialized pretraining or custom implementations, achieving competitive performance on long-range language understanding tasks.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b12\": 0.7,\n    \"b23\": 0.7,\n    \"b34\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.6,\n    \"b49\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying transformer-based pretrained language models to long texts due to their quadratic complexity. The authors propose SLED, a method that partitions long documents into overlapping chunks, processes them with a pretrained encoder, and uses a decoder to handle global context. The method leverages short-text LMs and avoids specialized pretraining, making it efficient and scalable for long-range language understanding tasks.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b23\": 0.8,\n    \"b34\": 0.8,\n    \"b38\": 0.85,\n    \"b37\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b49\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents SLED, a method to apply off-the-shelf pretrained encoder-decoder models to long text problems with linear time and space complexity. Inspired by fusion-in-decoder, it partitions documents into overlapping chunks, encodes each independently, and uses a pretrained decoder for global cross-chunk reasoning. The method is evaluated on QA, summarization, and NLI tasks, showing competitive results with specialized models and demonstrating the ability to find and fuse information across long texts.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b34\": 0.8,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b37\": 0.6,\n    \"b49\": 0.5\n  }\n}\n```"], "64ec1b763fda6d7f0626f480": ["```json\n{\n  \"Summary\": \"The paper addresses the computational inefficiency of transformer models in long-sequence processing tasks by proposing a framework called SimCAS, which involves chunking long texts into sub-sequences, aligning global semantic information among chunks, and selecting representative tokens for decoding. The framework incorporates a sequential batch alignment to ensure semantic coherence across chunks and utilizes Proximal Policy Optimization for training the token selector.\",\n  \"Direct Inspiration\": {\n    \"b54\": 0.9,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b69\": 0.7,\n    \"b5\": 0.6,\n    \"b21\": 0.7,\n    \"b33\": 0.6,\n    \"b49\": 0.6,\n    \"b68\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b27\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of high computational costs in transformer models for long-sequence processing by introducing a novel framework called SimCAS. SimCAS involves chunking long texts, aligning semantic information between chunks, and selecting representative tokens for decoding. The method leverages Proximal Policy Optimization (PPO) for reinforcement learning to train the token selector.\",\n  \"Direct Inspiration\": {\n    \"b54\": 1,\n    \"b40\": 0.9,\n    \"b49\": 0.85,\n    \"b68\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b69\": 0.7,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.65,\n    \"b38\": 0.65,\n    \"b33\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of long-sequence processing in transformer-based models, which face computational inefficiencies due to the quadratic complexity of self-attention mechanisms. The proposed solution is a framework called SimCAS (Simple learning framework with Chunk, Align, and Select operations) that splits long texts into manageable chunks, aligns global semantic information, and selects representative tokens for efficient processing. The framework leverages a sequential batch alignment (SBA) mechanism and a reinforcement learning-based token selector trained using Proximal Policy Optimization (PPO).\",\n  \"Direct Inspiration\": {\n    \"b40\": 1,\n    \"b54\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b5\": 0.8,\n    \"b21\": 0.85,\n    \"b68\": 0.8,\n    \"b49\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b69\": 0.7,\n    \"b33\": 0.7,\n    \"b36\": 0.7,\n    \"b38\": 0.7,\n    \"b3\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high computational consumption in transformer-based models for long text processing tasks. It introduces SimCAS, a framework combining chunking, aligning, and selecting operations to enhance transformers' capability in long-sequence processing without sacrificing performance.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1,\n    \"b54\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b5\": 0.9,\n    \"b69\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.7,\n    \"b72\": 0.7,\n    \"b36\": 0.7,\n    \"b33\": 0.7,\n    \"b49\": 0.7,\n    \"b68\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of processing long text sequences in transformer models, which suffer from high computational complexity due to the quadratic increase in self-attention operations with sequence length. The proposed solution, SimCAS, involves chunking the input text into sub-sequences, aligning these chunks to capture global semantic information, and selecting the most representative tokens for decoding using reinforcement learning techniques. This method aims to combine the benefits of efficient attention mechanisms and sub-sequence processing while mitigating their respective drawbacks.\",\n  \"Direct Inspiration\": {\n    \"b54\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.85,\n    \"b49\": 0.80,\n    \"b68\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.70,\n    \"b5\": 0.70,\n    \"b21\": 0.65,\n    \"b33\": 0.65\n  }\n}\n```"], "636482d790e50fcafdccab10": ["```json\n{\n  \"Summary\": \"The paper addresses the novel problem of defending black box models against adversarial attacks without access to network weights and original training samples. The proposed method, DBMA, utilizes wavelet transforms for noise removal and a U-net-based regenerator network to restore high-frequency image content. The main contributions are the introduction of a wavelet-based noise remover and a regenerator network to achieve robustness against data-free black box attacks.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b23\": 0.9,\n    \"b41\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b25\": 0.7,\n    \"b37\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of defending deep neural networks against black-box adversarial attacks without access to the original training data or model weights. The proposed solution, DBMA, leverages wavelet transforms to remove corrupted high-frequency coefficients and employs a U-net-based regenerator network to restore image quality, enhancing both clean and adversarial performance.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b23\": 0.85,\n    \"b41\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.75,\n    \"b28\": 0.75,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b24\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Adversarial attacks on black-box models without access to network weights or original training samples.\",\n    \"Inspirations\": \"Utilizing wavelet transforms and a U-net-based regenerator network to defend against adversarial attacks in a data-free setup.\"\n  },\n  \"Direct Inspiration\": [\"b41\", \"b45\"],\n  \"Indirect Inspiration\": [\"b22\", \"b23\", \"b40\"],\n  \"Other Inspiration\": [\"b24\", \"b37\", \"b39\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of defending deep neural networks against black box adversarial attacks in the absence of training data. The proposed algorithm, DBMA, uses a wavelet-based noise remover (WNR) and a U-net-based regenerator network to filter and reconstruct images, enhancing their robustness against adversarial perturbations.\",\n    \"Direct Inspiration\": {\n        \"b45\": 0.9,\n        \"b41\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b22\": 0.7,\n        \"b23\": 0.7,\n        \"b40\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.5,\n        \"b19\": 0.5,\n        \"b37\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of defending deep neural network models against black-box adversarial attacks in a setup where there is no access to the original training data. The proposed method, DBMA, leverages wavelet transforms to remove noise introduced by adversarial attacks and uses a U-net-based regenerator network to restore the lost high-frequency content in images. The paper aims to enhance robustness while maintaining clean accuracy by selecting optimal wavelet coefficients and regenerating images for better feature preservation.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b23\": 1,\n    \"b41\": 1,\n    \"b45\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b20\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b37\": 0.7,\n    \"b39\": 0.7,\n    \"b40\": 0.7\n  }\n}\n```"], "62c4fd9a5aee126c0fad6f58": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges in multi-label recognition, specifically focusing on unseen labels in images. A novel algorithm called Multi-modal Knowledge Transfer (MKT) is proposed, combining image and text embeddings from Vision and Language Pre-training (VLP) models, utilizing knowledge distillation and prompt tuning to enhance image-text consistency and feature extraction.\",\n    \"Direct Inspiration\": [\n        \"b33\",\n        \"b14\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b46\",\n        \"b0\",\n        \"b12\",\n        \"b30\"\n    ],\n    \"Other Inspiration\": [\n        \"b7\"\n    ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multi-label classification, particularly the recognition of unseen labels in images, which is critical for real-world applications like scene understanding and self-driving cars. The authors propose a novel open-vocabulary framework called multi-modal knowledge transfer (MKT) that leverages multi-modal knowledge from image-text pairs using vision and language pre-training (VLP) models. The main contributions include the combination of knowledge distillation and prompt tuning to ensure consistency in image and label embeddings, and the introduction of a two-stream feature extraction module to enhance feature expression.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1,\n    \"b14\": 0.9,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b34\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b45\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multi-label recognition in images, particularly focusing on unseen labels. It proposes a novel open-vocabulary framework named Multi-Modal Knowledge Transfer (MKT), leveraging vision and language pre-training (VLP) models to enhance multi-label classification through knowledge distillation and prompt tuning. The MKT framework aims to improve the alignment of image and label embeddings to handle both seen and unseen labels effectively.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b30\": 0.75,\n    \"b46\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b34\": 0.55,\n    \"b26\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the multi-label recognition in images, which includes handling unseen labels. The authors propose a novel open-vocabulary framework named multi-modal knowledge transfer (MKT) that leverages vision and language pre-training (VLP) models for multi-label classification. The key contributions include the use of knowledge distillation and prompt tuning to align image and label embeddings and a two-stream feature extraction module to capture both local and global features.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b30\": 0.8,\n    \"b46\": 0.8,\n    \"b0\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b14\": 0.7,\n    \"b27\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multi-label recognition in images, specifically focusing on the identification of unseen labels using a novel open-vocabulary framework named multi-modal knowledge transfer (MKT). The MKT framework leverages multi-modal knowledge from vision and language pre-training (VLP) models, incorporating knowledge distillation and prompt tuning to ensure consistency between image and label embeddings. The framework also introduces a two-stream feature extraction module to capture local and global features, significantly improving performance on open-vocabulary multi-label classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.8,\n    \"b0\": 0.8,\n    \"b12\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```"], "65123f453fda6d7f06e54a4b": ["```json\n{\n  \"Summary\": \"The paper proposes a novel adapter-style tuning strategy called GraphAdapter for efficient transfer learning (ETL) of vision-language models (VLMs) in low-data regimes. The main challenges addressed include modeling task-specific knowledge with dual-modality (textual and visual) structure knowledge and leveraging this knowledge for better performance on downstream tasks. Key inspirations come from existing methods in prompt tuning, adapter-style tuning, and graph learning.\",\n  \"Direct Inspiration\": {\n    \"b48\": 0.9,\n    \"b16\": 0.9,\n    \"b70\": 0.9,\n    \"b74\": 0.8,\n    \"b77\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b76\": 0.7,\n    \"b73\": 0.7,\n    \"b42\": 0.6,\n    \"b6\": 0.6,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b78\": 0.5,\n    \"b4\": 0.5,\n    \"b3\": 0.5,\n    \"b8\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Modeling structure knowledge (inter-class relationship) for downstream tasks.\",\n      \"Learning task-specific knowledge by introducing dual-modality (visual and textual) structure knowledge.\"\n    ],\n    \"Inspirations\": [\n      \"Exploiting graph learning to model structure knowledge.\",\n      \"Introducing dual knowledge graph composed of textual and visual sub-graphs.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"References\": [\"b42\", \"b6\", \"b33\"]\n  },\n  \"Indirect Inspiration\": {\n    \"References\": [\"b48\", \"b24\", \"b77\", \"b16\", \"b70\", \"b74\"]\n  },\n  \"Other Inspiration\": {\n    \"References\": [\"b4\", \"b3\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is adapting large-scale vision-language models (VLMs) to downstream tasks efficiently in resource-constrained scenarios while avoiding overfitting and leveraging multi-modality structure knowledge. The proposed algorithm, GraphAdapter, aims to address these challenges by exploiting a dual knowledge graph (textual and visual) with graph learning to enhance the model's ability to adapt to downstream tasks with few-shot learning.\",\n  \"Direct Inspiration\": {\n    \"b42\": 0.9,\n    \"b6\": 0.8,\n    \"b33\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b48\": 0.7,\n    \"b24\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b74\": 0.6,\n    \"b70\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies two primary challenges: modeling structure knowledge for downstream tasks and learning task-specific knowledge from dual modalities (visual and textual). The proposed solution, GraphAdapter, introduces a dual knowledge graph to leverage both textual and visual structure knowledge using graph learning.\",\n  \"Direct Inspiration\": [\"b42\", \"b6\", \"b33\"],\n  \"Indirect Inspiration\": [\"b16\", \"b70\", \"b74\", \"b48\", \"b77\"],\n  \"Other Inspiration\": [\"b23\", \"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently transferring knowledge from large vision-language models (VLMs) to downstream tasks in resource-constrained scenarios. It introduces a novel adapter-style tuning strategy named GraphAdapter, which leverages dual-modality structure knowledge (textual and visual) using graph learning to improve task-specific adaptation. The primary challenges include modeling structure knowledge for downstream tasks and learning task-specific knowledge through fused visual and textual modalities.\",\n  \"Direct Inspiration\": {\n    \"b42\": 0.9,\n    \"b6\": 0.85,\n    \"b33\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b48\": 0.8,\n    \"b16\": 0.75,\n    \"b70\": 0.75,\n    \"b74\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b77\": 0.65,\n    \"b76\": 0.65,\n    \"b73\": 0.6\n  }\n}\n```"], "6531e2ca939a5f4082f5d5d2": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of inter-application interference in SMT (Simultaneous Multithreading) cores, particularly for ARM processors. It proposes SYNPA, a thread allocation policy that uses performance counters to estimate and synergistically pair applications to minimize interference, thereby improving overall processor performance.\",\n  \"Direct Inspiration\": [\"b2\", \"b3\", \"b4\"],\n  \"Indirect Inspiration\": [\"b5\", \"b6\", \"b7\", \"b10\"],\n  \"Other Inspiration\": [\"b8\", \"b9\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of inter-application interference in SMT cores, particularly on ARM processors, and proposes SYNPA, a thread allocation policy that selects synergistic applications to minimize interference and improve performance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.9,\n    \"b4\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces SYNPA, a thread allocation policy specifically for ARM processors with SMT cores. The primary challenges addressed are characterizing application synergy using performance counters and estimating the most synergistic application pairs. SYNPA uses a simple three-variable model at the dispatch stage and a linear regression model to predict performance. Experimental results show significant improvements over the default Linux scheduler.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.95,\n    \"b2\": 0.85,\n    \"b4\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.60,\n    \"b9\": 0.70,\n    \"b10\": 0.65,\n    \"b11\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.55,\n    \"b7\": 0.50,\n    \"b8\": 0.45,\n    \"b12\": 0.55,\n    \"b13\": 0.50,\n    \"b14\": 0.45,\n    \"b17\": 0.60,\n    \"b18\": 0.50,\n    \"b19\": 0.40,\n    \"b20\": 0.55,\n    \"b21\": 0.45,\n    \"b22\": 0.40\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of intra-core interference in SMT (Simultaneous Multithreading) environments, specifically for ARM processors. The proposed solution, SYNPA, is a thread allocation policy that selects synergistic application pairs to minimize inter-application interference, thereby improving performance. The paper introduces a simple three-variable model to characterize performance at the dispatch stage and a linear regression model to predict performance when applications run together in an SMT core.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b3\": 1.0,\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b9\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of reducing inter-application interference in SMT cores, particularly for ARM processors. It introduces SYNPA, a thread allocation policy that selects synergistic applications to minimize interference and improve performance. The method relies on a simple three-variable model at the dispatch stage and a linear regression model to predict performance in SMT execution.\",\n  \"Direct Inspiration\": [\"b2\", \"b3\", \"b4\"],\n  \"Indirect Inspiration\": [\"b1\", \"b5\", \"b6\", \"b7\", \"b8\", \"b9\", \"b10\", \"b11\", \"b12\", \"b13\", \"b14\", \"b15\", \"b16\", \"b17\", \"b18\"],\n  \"Other Inspiration\": [\"b19\", \"b20\", \"b21\", \"b22\"]\n}\n```"], "6514e2043fda6d7f062dc9f8": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Optimizing the VQ-VAE formulation\",\n      \"Underutilized codebooks when the size of C is increased\",\n      \"Simplifying the algorithm while maintaining performance\"\n    ],\n    \"inspirations\": [\n      \"VQ-VAE [b39]\", \n      \"Neural compression literature [b5] [b37]\", \n      \"Bounded scalar quantization [b30] [b2]\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b39\", \n    \"b5\", \n    \"b37\", \n    \"b30\", \n    \"b2\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b18\", \n    \"b40\", \n    \"b25\", \n    \"b36\", \n    \"b16\"\n  ],\n  \"Other Inspiration\": [\n    \"b10\", \n    \"b23\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving vector quantization (VQ) techniques used in neural network-based models, specifically aiming to simplify the VQ-VAE formulation and achieve higher codebook utilization without auxiliary losses. The proposed solution, Finite Scalar Quantization (FSQ), offers a drop-in replacement for VQ, leveraging bounded scalar quantization inspired by neural compression techniques. The paper's contributions include demonstrating FSQ's effectiveness across various tasks, analyzing VQ vs. FSQ trade-offs, and establishing FSQ's advantages in terms of codebook usage and parameter efficiency.\",\n  \"Direct Inspiration\": [\"b5\", \"b37\"],\n  \"Indirect Inspiration\": [\"b18\", \"b39\", \"b40\"],\n  \"Other Inspiration\": [\"b6\", \"b27\", \"b30\", \"b2\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of optimizing Vector Quantization (VQ) in VQ-VAEs, particularly focusing on underutilized codebooks and auxiliary losses. The authors propose Finite Scalar Quantization (FSQ) as an alternative to VQ, aiming to improve codebook utilization and simplify the quantization process. The key contributions include demonstrating FSQ as a drop-in replacement for VQ in various models, analyzing trade-offs between VQ and FSQ, and showing that FSQ can leverage large codebooks more effectively without auxiliary losses.\",\n  \"Direct Inspiration\": {\n    \"b39\": 0.9,\n    \"b5\": 0.8,\n    \"b37\": 0.7,\n    \"b30\": 0.7,\n    \"b2\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.6,\n    \"b18\": 0.6,\n    \"b40\": 0.6,\n    \"b25\": 0.6,\n    \"b36\": 0.6,\n    \"b16\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b27\": 0.5,\n    \"b4\": 0.5,\n    \"b0\": 0.5,\n    \"b22\": 0.5,\n    \"b1\": 0.5,\n    \"b10\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Optimizing the VQ-VAE formulation to address underutilized codebooks\",\n      \"Removing auxiliary losses while maintaining high codebook utilization\",\n      \"Introducing a drop-in replacement for VQ that simplifies the original formulation\"\n    ],\n    \"inspirations\": [\n      \"Neural compression literature using scalar quantization\",\n      \"Existing VQ-VAE methodologies and their limitations\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b5\": 0.9,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b25\": 0.7,\n    \"b36\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in vector quantization (VQ) for variational autoencoders (VAEs), particularly the issues of non-differentiability, underutilized codebooks, and optimization difficulties. It proposes Finite Scalar Quantization (FSQ) as a simpler alternative to VQ, aiming to improve codebook utilization without auxiliary losses while maintaining the functional setup. The paper demonstrates the application of FSQ in various architectures and tasks, analyzing trade-offs and showing that FSQ can effectively replace VQ with fewer parameters and better performance for larger codebooks.\",\n  \"Direct Inspiration\": {\n    \"b39\": 0.9,\n    \"b5\": 0.85,\n    \"b37\": 0.8,\n    \"b30\": 0.75,\n    \"b2\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.6,\n    \"b10\": 0.55,\n    \"b23\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.4,\n    \"b4\": 0.35,\n    \"b6\": 0.3,\n    \"b22\": 0.25,\n    \"b1\": 0.2\n  }\n}\n```"], "6531e2ca939a5f4082f5d4fe": ["```json\n{\n    \"Summary\": \"The paper outlines the challenges of integrating both topological and heterogeneous information in Text-Attributed Heterogeneous Graphs (TAHGs) into Pretrained Language Models (PLMs). The main contributions include a topology-aware pretraining task and a text augmentation strategy to address these challenges.\",\n    \"Direct Inspiration\": {\n        \"b39\": 1,\n        \"b3\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b28\": 0.7,\n        \"b5\": 0.8,\n        \"b41\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b19\": 0.6,\n        \"b32\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of abundant topological information and imbalanced textual descriptions in Text-Attributed Heterogeneous Graphs (TAHGs). It proposes a new pretraining framework, THLM, which includes a topology-aware pretraining task and a text augmentation strategy to enable language models to capture topological connections and mitigate text imbalance.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b39\": 0.85,\n    \"b5\": 0.8,\n    \"b41\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.6,\n    \"b22\": 0.55,\n    \"b31\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b8\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses two primary challenges in pretraining language models on Text-Attributed Heterogeneous Graphs (TAHGs): abundant topological information and imbalanced textual descriptions of nodes. The proposed algorithm, THLM, integrates topological and heterogeneous information using a topology-aware pretraining task and a text augmentation strategy.\",\n    \"Direct Inspiration\": [\"b39\", \"b3\"],\n    \"Indirect Inspiration\": [\"b5\", \"b41\"],\n    \"Other Inspiration\": [\"b8\", \"b4\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of pretraining Language Models (LMs) on Text-Attributed Heterogeneous Graphs (TAHGs), which incorporate both abundant topological information and imbalanced textual descriptions. The proposed method, THLM, integrates a topology-aware pretraining task and a text augmentation strategy to improve the performance of LMs on tasks such as link prediction and node classification.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b39\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b4\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b18\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b41\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses two primary challenges in Text-Attributed Heterogeneous Graphs (TAHGs): Abundant Topological Information (both first and higher-order connections) and Imbalanced Textual Descriptions of Nodes (rich-text nodes and textless nodes). To tackle these, the authors propose a new pretraining framework called THLM which integrates both topological and heterogeneous information into language models. The framework includes a topology-aware pretraining task (context graph prediction) and a text augmentation strategy to enrich the semantics of textless nodes.\",\n    \"Direct Inspiration\": {\n        \"b39\": 0.9,\n        \"b3\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b41\": 0.75,\n        \"b28\": 0.7,\n        \"b2\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.6,\n        \"b9\": 0.6\n    }\n}\n```"], "65252d90939a5f40827eabe7": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the instruction-following capabilities of Large Language Models (LLMs) through a novel technique called Noisy Embedding Instruction Fine Tuning (NEFTune). The primary inspiration comes from the need to mitigate overfitting in LLMs during instruction fine-tuning, with the aim of enhancing conversational quality without additional computational overhead.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b27\": 0.7,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b22\": 0.6,\n    \"b12\": 0.6,\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving the performance of large language models (LLMs) during instruction fine-tuning by introducing a novel method, NEFTune, which adds random noise to embedding vectors during the fine-tuning process. This method aims to reduce overfitting and improve conversational quality without additional computational overhead.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.9,\n    \"b18\": 0.85,\n    \"b6\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b27\": 0.75,\n    \"b12\": 0.7,\n    \"b2\": 0.65,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b4\": 0.55,\n    \"b29\": 0.5,\n    \"b9\": 0.55,\n    \"b39\": 0.5,\n    \"b5\": 0.5,\n    \"b14\": 0.45,\n    \"b19\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a novel algorithm named NEFTune, which incorporates random noise into the embedding vectors during the fine-tuning of large language models (LLMs). This approach aims to reduce overfitting and improve the conversational quality of LLMs, particularly when fine-tuning on small instruction datasets. The key challenges addressed include improving instruction-following abilities and minimizing overfitting in LLMs.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b24\": 0.75,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b18\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the fine-tuning process of large language models (LLMs) on small instruction datasets. The authors propose the Noisy Embedding Instruction Fine-Tuning (NEFTune) technique, which involves adding random noise to the embedding vectors during the training process. This technique is shown to improve the performance of LLMs significantly, particularly in conversational tasks, without additional computational or data overhead.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b27\": 0.7,\n    \"b12\": 0.7,\n    \"b2\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b14\": 0.4,\n    \"b19\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the instructional fine-tuning of large language models (LLMs) to enhance their conversational abilities without additional computational overhead. The authors propose a novel method called Noisy Embedding Instruction Fine Tuning (NEFTune), which adds random noise to the embedding vectors during the fine-tuning process. This method shows significant improvements in conversational quality and reduces overfitting to the instruction dataset.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b25\": 0.7,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b2\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```"], "626b49625aee126c0fffd149": ["```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are the difficulties of Automatic Music Transcription (AMT) due to partial frequency sharing, polyphony, echo effects, and multi-instrument performances, which complicate the task. The paper introduces the Note EM framework, which intertwines the alignment process with transcriber training using the Expectation Maximization (EM) framework. This method allows for unaligned supervision based on easily obtainable musical scores to supervise in-the-wild recordings, thereby mitigating the need for extensive manually annotated datasets.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b5\": 0.8,\n        \"b7\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.7,\n        \"b4\": 0.75,\n        \"b6\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.65,\n        \"b10\": 0.6,\n        \"b15\": 0.6,\n        \"b16\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Automatic Music Transcription (AMT) by introducing a novel framework called Note EM. This framework allows for polyphonic transcribers to be trained using unaligned supervision, which is based on easily obtainable musical scores. The approach intertwines the alignment process with the training of the transcriber using the Expectation Maximization (EM) framework. The method improves transcription accuracy, generalizes across different instruments and genres, and provides more accurate annotations compared to previous datasets.\",\n  \"Direct Inspiration\": [\n    \"b2\",\n    \"b5\",\n    \"b7\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b3\",\n    \"b4\",\n    \"b6\",\n    \"b8\"\n  ],\n  \"Other Inspiration\": [\n    \"b10\",\n    \"b16\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Automatic Music Transcription (AMT) by introducing a novel framework called Note EM, which uses unaligned supervision based on easy-to-obtain musical scores. The main contribution is an Expectation Maximization (EM) based approach that intertwines the alignment process with the training of the transcription model, improving transcription accuracy and generalization to various instruments and genres.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b5\": 0.9,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.65,\n    \"b10\": 0.6,\n    \"b11\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Automatic Music Transcription (AMT), particularly the difficulty in obtaining accurate annotated data for training deep neural networks. It proposes a novel framework, Note EM, which uses unaligned supervision based on musical scores and an Expectation Maximization (EM) framework to improve transcription accuracy. The method involves bootstrapping with synthetic data, predicting transcriptions for unlabeled recordings, and iteratively refining the transcriber using dynamic time warping (DTW) for alignment.\",\n  \"Direct Inspiration\": {\n    \"References\": [\"b2\", \"b5\", \"b7\"]\n  },\n  \"Indirect Inspiration\": {\n    \"References\": [\"b3\", \"b6\"]\n  },\n  \"Other Inspiration\": {\n    \"References\": [\"b4\", \"b10\", \"b15\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges in Automatic Music Transcription (AMT) due to polyphony, echo effects, and multi-instrument performances. It proposes a novel framework, Note EM, to support unaligned supervision using easy-to-obtain musical scores. The method leverages Expectation Maximization (EM) to intertwine the alignment process with the training of the transcriber, achieving superior transcription accuracy and generalization to unseen instruments and genres.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b5\": 0.9,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b6\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b9\": 0.5,\n    \"b10\": 0.5,\n    \"b11\": 0.5,\n    \"b12\": 0.5,\n    \"b13\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```"], "62a6aabf5aee126c0ff36991": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Over-smoothing and over-squashing in message passing-based GNNs with increased model depth.\",\n      \"High computational complexity of existing Graph Transformers for large-scale networks.\",\n      \"Difficulty in encoding structural topology and edge features into Transformers.\"\n    ],\n    \"inspirations\": [\n      \"Motivated by the need to handle large-scale graphs and to effectively encode neighborhood information.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b32\": 1,\n    \"b11\": 0.9,\n    \"b23\": 0.9,\n    \"b36\": 0.8,\n    \"b18\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b22\": 0.7,\n    \"b41\": 0.7,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.5,\n    \"b9\": 0.5,\n    \"b24\": 0.5,\n    \"b10\": 0.5,\n    \"b25\": 0.5,\n    \"b33\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inherent limitations of message passing-based Graph Neural Networks (GNNs) for graph representation learning, such as over-smoothing and over-squashing. It proposes a new model, NAGphormer, that incorporates a novel neighborhood aggregation module (Hop2Token) to construct sequences for each node based on tokens from different hops of neighbors. This model leverages the Transformer architecture to handle large graphs efficiently for node classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b32\": 0.9,\n    \"b21\": 0.8,\n    \"b8\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.6,\n    \"b36\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b23\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges identified in the paper include the limitations of message passing-based GNNs, such as over-smoothing and over-squashing, and the high computational complexity of existing Graph Transformers for node classification tasks. The proposed algorithm, NAGphormer, addresses these challenges by introducing the Hop2Token module for neighborhood aggregation and an attention-based readout function for adaptive learning of node representations, enabling efficient handling of large-scale graphs.\",\n  \"Direct Inspiration\": [\"b11\", \"b23\", \"b36\", \"b32\", \"b22\", \"b8\", \"b21\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of over-smoothing and over-squashing in Graph Neural Networks (GNNs) and the high computational complexity of existing Graph Transformers for large-scale node classification tasks. The novel contributions include the development of NAGphormer with the Hop2Token neighborhood aggregation module and an attention-based readout function, aimed at improving performance and scalability.\",\n  \"Direct Inspiration\": [\n    \"b41\",\n    \"b32\",\n    \"b36\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b21\",\n    \"b18\",\n    \"b23\",\n    \"b11\"\n  ],\n  \"Other Inspiration\": [\n    \"b8\",\n    \"b22\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of message passing-based GNNs, specifically over-smoothing and over-squashing, and the computational complexity of existing Graph Transformers for large-scale node classification tasks. It proposes NAGphormer, a novel Graph Transformer model with a neighborhood aggregation module, Hop2Token, to better handle large graphs and adaptively learn the importance of different hops.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b23\": 0.9,\n    \"b36\": 0.9,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b29\": 0.5,\n    \"b41\": 0.4\n  }\n}\n```"], "6535d747939a5f408295c3c4": ["```json\n{\n  \"Summary\": \"The paper aims to enhance the generalization capabilities of graph neural networks (GNNs) in zero-shot learning scenarios by integrating large language models (LLMs) with graph learning. The primary challenges addressed include aligning the structural information of graphs with language space, guiding LLMs to comprehend graphs, and endowing LLMs with step-by-step reasoning abilities. The proposed GraphGPT framework incorporates a text-graph grounding paradigm, a dual-stage graph instruction tuning process, and Chain-of-Thought (COT) distillation to achieve these goals.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b5\": 1,\n    \"b29\": 0.9,\n    \"b47\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.8,\n    \"b66\": 0.8,\n    \"b11\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b59\": 0.7,\n    \"b60\": 0.6,\n    \"b37\": 0.6,\n    \"b44\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces the GraphGPT framework, aiming to enhance the generalization capabilities of graph models in zero-shot learning scenarios. The framework integrates Large Language Models (LLMs) with graph learning by aligning graph structures with the language space through a dual-stage graph instruction tuning paradigm. This paradigm includes self-supervised instruction tuning using graph matching tasks and task-specific instruction tuning to improve model adaptability and reasoning abilities.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b47\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b5\": 0.9,\n    \"b44\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.8,\n    \"b21\": 0.8,\n    \"b37\": 0.8,\n    \"b60\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper aims to address the challenges of aligning graph structural information with large language models (LLMs) for enhanced generalization across diverse graph learning tasks. The proposed GraphGPT framework introduces a novel graph instruction tuning paradigm, incorporating self-supervised instruction tuning and task-specific instruction tuning to improve the LLM's understanding and reasoning capabilities in graph learning scenarios.\",\n  \"Direct Inspiration\": {\n    \"b29\": 0.9,\n    \"b47\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b21\": 0.8,\n    \"b38\": 0.8,\n    \"b42\": 0.8,\n    \"b59\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.7,\n    \"b66\": 0.7,\n    \"b11\": 0.7,\n    \"b34\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of enhancing the generalization capabilities of graph models in zero-shot learning scenarios by developing a novel framework called GraphGPT. This framework aligns Large Language Models (LLMs) with graph structures through a dual-stage graph instruction tuning paradigm that incorporates self-supervised signals and task-specific instructions to improve the model's comprehension of graph structures and reasoning abilities.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b21\": 0.8,\n    \"b59\": 0.8,\n    \"b60\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7,\n    \"b44\": 0.7,\n    \"b47\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes GraphGPT, a framework to enhance the generalization capabilities of graph neural networks (GNNs) using large language models (LLMs). The main challenges addressed include aligning graph structures with the language space, guiding LLMs to understand graph structures, and improving step-by-step reasoning for GNNs. The novel approach involves text-graph grounding and a dual-stage graph instruction tuning paradigm, leveraging self-supervised signals and task-specific instructions.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b5\": 1.0,\n    \"b29\": 0.9,\n    \"b47\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b21\": 0.7,\n    \"b37\": 0.7,\n    \"b44\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b26\": 0.6,\n    \"b38\": 0.6,\n    \"b59\": 0.6,\n    \"b60\": 0.6,\n    \"b66\": 0.6\n  }\n}\n```"], "6503bec83fda6d7f067c7717": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating a universal neural vocoder that can generate high-quality audio across various scenarios without fine-tuning, particularly focusing on unseen data. The proposed SnakeGAN vocoder improves robustness and audio quality by incorporating DDSP-based prior knowledge and the Snake activation function.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b12\": 0.8,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b16\": 0.7,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces SnakeGAN, a universal neural vocoder, which addresses the challenge of generating high-quality audio in various scenarios without fine-tuning. The key contributions include incorporating DDSP-based prior knowledge and the Snake activation function to improve robustness and audio quality.\",\n    \"Direct Inspiration\": {\n        \"b26\": 1.0,\n        \"b12\": 0.9,\n        \"b8\": 0.9,\n        \"b7\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.7,\n        \"b11\": 0.7,\n        \"b3\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.6,\n        \"b24\": 0.6,\n        \"b13\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving high-quality audio generation in various scenarios without fine-tuning, particularly focusing on improving the robustness and effectiveness of neural vocoders. The proposed solution, SnakeGAN, integrates DDSP-based prior knowledge of waveform composition and periodic nonlinearities using the Snake activation function to enhance the generator's performance.\",\n  \"Direct Inspiration\": [\n    \"b26\",\n    \"b12\",\n    \"b8\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b3\",\n    \"b16\",\n    \"b11\"\n  ],\n  \"Other Inspiration\": [\n    \"b7\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving flexible generation of high-quality audio under various scenarios without fine-tuning. It proposes a universal neural vocoder named SnakeGAN, which introduces DDSP-based prior knowledge of waveform composition and incorporates the Snake activation function to improve robustness and audio quality in unseen scenarios.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b12\": 0.9,\n    \"b8\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b16\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b7\": 0.65,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the robustness and quality of neural vocoders, particularly under out-of-domain (OOD) scenarios, by introducing SnakeGAN. This novel vocoder incorporates DDSP-based prior knowledge and the Snake activation function to enhance periodic inductive bias and robustness.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b8\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b13\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```"], "6303504190e50fcafd769fe6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of leveraging unlabeled time-series data for enhanced task-specific representation learning. The proposed TARNet model integrates a data-driven masking strategy for reconstructing key timestamps, optimizing performance for end tasks by sharing parameters between reconstruction and task learning phases.\",\n  \"Direct Inspiration\": [\"b36\"],\n  \"Indirect Inspiration\": [\"b28\", \"b20\"],\n  \"Other Inspiration\": [\"b0\", \"b9\", \"b14\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing end-task performance in time-series data by customizing the learned representation for the specific end task. The proposed solution, TARNet, employs a data-driven masking strategy to determine important timestamps for each end task and reconstructs them to learn task-specific representations, leading to improved performance.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b38\": 0.7,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing time-series data models by customizing data reconstruction tasks to be end-task specific. The proposed TARNet model alternates between training for an end task and a data reconstruction task, using a data-driven masking strategy to focus on important timestamps determined by the end task. This results in task-aware representations that improve end-task performance compared to generic representations.\",\n  \"Direct Inspiration\": [\"b36\"],\n  \"Indirect Inspiration\": [\"b28\"],\n  \"Other Inspiration\": [\"b1\", \"b38\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing time-series representations for specific end tasks by integrating a task-aware reconstruction approach. The authors propose TARNet, which alternates between end-task training and task-specific data reconstruction using a novel data-driven masking strategy to identify and mask important timestamps. This approach aims to improve performance on end tasks by learning more relevant representations.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.7,\n    \"b1\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the optimization of time-series data representation for end-task performance by leveraging task-aware reconstruction. The proposed algorithm, TARNet, alternates between an end-task and a data reconstruction task, using a data-driven masking strategy to identify and mask important timestamps based on self-attention scores from a transformer encoder. This approach aims to improve the learned representation's relevance to the end task, thereby enhancing performance on various classification and regression datasets.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.9,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b38\": 0.7,\n    \"b10\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```"], "6364c0ba90e50fcafdbb4aad": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the inefficiency of current L1D prefetchers, which suffer from storage overhead, bandwidth limitations, L1D pollution, and limited prefetching aggressiveness. The novel contribution is the Berti prefetcher that learns timely and accurate local deltas for efficient data prefetching, improving coverage and reducing dynamic energy consumption.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1.0,\n    \"b45\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.8,\n    \"b47\": 0.8,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b16\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper include storage overhead, L1D bandwidth starvation, L1D pollution due to inaccurate prefetching, and the limited size of the prefetch queue (PQ) and miss status holding registers (MSHR).\",\n    \"inspirations\": \"The proposed Berti prefetcher is inspired by the Best Offset Prefetcher (BOP) [b37] and Berti from DPC-3 [b45]. The authors propose a cost-effective, per-IP best request time delta L1D prefetcher to achieve high accuracy and timeliness in prefetching.\"\n  },\n  \"Direct Inspiration\": {\n    \"b37\": 1.0,\n    \"b45\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.8,\n    \"b39\": 0.8,\n    \"b47\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b16\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is improving data prefetching accuracy and timeliness at the L1D cache level while minimizing storage overhead, bandwidth consumption, and energy consumption. The authors propose a new local-delta prefetcher called Berti, which uses per-instruction pointer (IP) deltas to achieve high prefetch accuracy and timeliness.\",\n  \"Direct Inspiration\": [\"b37\", \"b45\"],\n  \"Indirect Inspiration\": [\"b12\", \"b16\", \"b34\", \"b39\", \"b47\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of designing a high-performance L1D prefetcher, specifically focusing on storage overhead, L1D bandwidth, L1D pollution due to inaccurate prefetching, and limited scope for aggressive prefetching. The proposed solution, Berti, aims to achieve high prefetch accuracy by learning and using timely and accurate local deltas for each instruction pointer (IP). The paper is inspired by several existing prefetching techniques, particularly focusing on the limitations of global delta prefetchers and proposing a local delta approach to enhance performance and energy efficiency.\",\n    \"Direct Inspiration\": {\n        \"b37\": 1.0,\n        \"b45\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b28\": 0.8,\n        \"b47\": 0.8,\n        \"b39\": 0.7,\n        \"b12\": 0.6,\n        \"b16\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b34\": 0.5,\n        \"b49\": 0.5,\n        \"b33\": 0.4,\n        \"b23\": 0.4,\n        \"b35\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include designing a high-performance L1D prefetcher with minimal storage overhead, avoiding L1D bandwidth starvation, mitigating L1D pollution due to inaccurate prefetching, and overcoming the limited size of the prefetch queue and miss status holding registers. The proposed algorithm, Berti, seeks to address these challenges by learning and utilizing timely and accurate local deltas for prefetching, which are dependent on local contextual information such as the instruction pointer (IP).\",\n  \"Direct Inspiration\": [\"b37\", \"b45\"],\n  \"Indirect Inspiration\": [\"b28\", \"b47\"],\n  \"Other Inspiration\": [\"b12\", \"b16\", \"b39\"]\n}\n```"], "63dcdb422c26941cf00b6339": ["```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the traditional neural language model's reliance on next-token predictions from a fixed vocabulary, which can limit accuracy and adaptability. The proposed model, COG (COPY-GENERATOR), addresses this by copying text segments (phrases) from existing text collections to generate text. This method allows for context-sensitive phrase selection, training-free adaptation to new knowledge sources, and the ability to handle multi-token sequences more effectively.\",\n    \"Direct Inspiration\": {\n        \"b20\": 0.9,\n        \"b30\": 0.9,\n        \"b40\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b26\": 0.7,\n        \"b7\": 0.7,\n        \"b12\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b14\": 0.5,\n        \"b13\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving neural language models (LMs) by introducing a novel approach called COPY-GENERATOR (COG). This approach reformulates text generation by copying text segments from existing text collections, rather than predicting the next token in an autoregressive manner. The key advantages include context-sensitive phrase selection, training-free adaptation to new knowledge sources, and the ability to handle multi-token sequences. The paper's innovations are inspired by various previous works, including retrieval-augmented generation models and methods for representing phrases in context.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b30\": 0.9,\n    \"b40\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b14\": 0.8,\n    \"b26\": 0.8,\n    \"b32\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a novel text generation model called COPY-GENERATOR (COG) that generates text by copying phrases from an existing text collection instead of predicting the next token from a fixed vocabulary. This approach aims to improve contextual accuracy, facilitate training-free adaptation to new knowledge sources, and enable the generation of multi-word phrases.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b30\": 1,\n    \"b40\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b26\": 0.8,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is improving neural language models' (LMs) text generation by moving away from traditional next-token predictions to a copy-and-paste mechanism using text segments from existing text collections. The proposed algorithm, COG (COPY-GENERATOR), benefits from more accurate candidate representation and selection, training-free adaptation to new knowledge sources, and the ability to handle sequences of multiple tokens.\",\n  \"Direct Inspiration\": [\"b20\", \"b30\", \"b40\"],\n  \"Indirect Inspiration\": [\"b7\", \"b12\", \"b26\"],\n  \"Other Inspiration\": [\"b11\", \"b14\", \"b32\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is to reformulate text generation by copying text segments from existing text collections rather than predicting next tokens from a fixed vocabulary. The proposed algorithm, COG (COPY-GENERATOR), leverages a context-dependent phrase encoder and a prefix encoder using Transformer architecture to select appropriate phrases from an offline index and append them to the current prefix. This method aims to improve accuracy in candidate representation and selection, allow training-free adaptation to new knowledge sources, and enhance domain adaptation and data expansion/filtering.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b30\": 0.9,\n    \"b40\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b26\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b14\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```"], "65364bdf939a5f40822568b2": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of user modeling from unlabeled user behavior data, emphasizing the unpredictability and noise in user behavior sequences compared to text sequences. The proposed Multi-scale Stochastic Distribution Prediction (MSDP) algorithm aims to predict behavior distributions over multiple time periods, leveraging a contrastive regularization term to improve robustness and generalization for various downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b6\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b11\": 0.7,\n    \"b14\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of modeling user behavior from unlabeled data, which is often noisy and random compared to textual data. It proposes a new paradigm called Multi-scale Stochastic Distribution Prediction (MSDP) to learn robust user behavior representations by predicting behavior distributions over various time periods rather than specific behaviors. The approach aims to improve the generalization ability of user representation for various downstream tasks. The paper introduces a contrastive regularization term to avoid overfitting on future behavior prediction.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b10\": 0.6,\n    \"b14\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of user modeling from unlabeled user behavior data, which is complicated by noise and randomness in user behavior sequences. It proposes a novel algorithm, Multi-scale Stochastic Distribution Prediction (MSDP), to predict user behavior distribution over various time periods rather than specific behaviors. This method aims to improve the robustness and generalization ability of user representations for various downstream tasks. Additionally, the paper introduces a contrastive regularization term to avoid over-fitting on future behavior predictions.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b6\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.85,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.75,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in user modeling from unlabeled user behavior data, emphasizing the noise and randomness in user behavior sequences compared to text sequences. The proposed solution, Multi-scale Stochastic Distribution Prediction (MSDP), predicts user behavior distribution over multiple time periods instead of specific behaviors to improve robustness and generalization. The approach includes a multi-scale stochastic sampling method and a contrastive regularization term to enhance representation learning.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper involve handling noise and randomness in user behavior sequences, which makes traditional masked behavior prediction and next behavior prediction tasks less effective.\",\n    \"algorithm\": \"The authors propose the Multi-scale Stochastic Distribution Prediction (MSDP) algorithm, which predicts users' behavior distribution over different time periods to achieve robust representation from user behavior sequences. They also introduce a contrastive regularization term to reinforce representation on historical sequences and avoid overfitting on future predictions.\"\n  },\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b9\": 0.7,\n    \"b13\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.5\n  }\n}\n```"], "623184035aee126c0f4848ed": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding the computations and representations in the auditory system during speech perception. It proposes the use of DNNs trained with different architectures and objectives to model and evaluate neural coding in the auditory cortex. Key findings include correlations between DNN layers and the auditory pathway, the capability of unsupervised models to learn human-like features, and the demonstration of language-specific properties in cross-language speech perception.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b14\": 1.0,\n    \"b33\": 0.9,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b16\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b29\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding the computations and representations in the human auditory system during speech perception. It proposes using deep neural networks (DNNs) to model these processes, comparing the DNN hierarchy with the ascending auditory pathway, and demonstrates that DNN models can reveal language-specific properties in cross-language speech perception.\",\n  \"Direct Inspiration\": {\n    \"reference\": [\"b13\", \"b14\", \"b37\"]\n  },\n  \"Indirect Inspiration\": {\n    \"reference\": [\"b9\", \"b10\", \"b11\", \"b12\"]\n  },\n  \"Other Inspiration\": {\n    \"reference\": [\"b15\", \"b16\", \"b18\", \"b23\", \"b28\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper aims to understand the computations and representations in the auditory system during speech perception using state-of-the-art DNN models. It demonstrates that DNNs can reveal language-invariant and language-specific aspects during speech perception, correlating their hierarchical structure with the ascending auditory pathway in humans. The paper highlights the importance of model architecture and training objectives in predicting neural responses and offers new data-driven approaches to modeling neural coding in the auditory cortex.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b16\": 0.8,\n    \"b25\": 0.7,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b19\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of understanding how computations and representations in the human auditory pathway can be modeled using deep neural networks (DNNs). The authors propose using various DNN architectures trained with supervised and unsupervised objectives to simulate the auditory pathway and analyze neural responses to speech. The paper finds that DNN layers correlate with different stages of the auditory pathway and can reveal both language-invariant and language-specific aspects during speech perception.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b14\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b25\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding the computations and representations in the human auditory system during speech perception using DNN models. It highlights the hierarchy in DNNs correlating with the auditory pathway, the effectiveness of unsupervised models, and the ability of DNNs to reveal language-specific properties.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b13\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b16\": 0.8,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```"], "656d3a42939a5f4082629226": ["```json\n{\n    \"Summary\": \"The primary challenge addressed by the paper is the lack of foresight capabilities in existing Multimodal Large Language Models (MLLMs). The paper introduces two novel methods, Foresight Pre-Training (FPT) and Foresight Instruction-Tuning (FIT), to integrate 'future modeling' into MLLMs, enabling them to predict future events based on present observations.\",\n    \"Direct Inspiration\": {\n        \"b8\": 0.9,\n        \"b40\": 0.8,\n        \"b41\": 0.8,\n        \"b84\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.6,\n        \"b26\": 0.6,\n        \"b31\": 0.6,\n        \"b59\": 0.6,\n        \"b64\": 0.6,\n        \"b77\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.5,\n        \"b50\": 0.5,\n        \"b70\": 0.5,\n        \"b10\": 0.4,\n        \"b73\": 0.4,\n        \"b25\": 0.4,\n        \"b42\": 0.4,\n        \"b76\": 0.4,\n        \"b38\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling Multimodal Large Language Models (MLLMs) to predict future events based on current observations, a skill termed as 'foresight minds'. The authors introduce two methods, Foresight Pre-Training (FPT) and Foresight Instruction-Tuning (FIT), to enhance MLLMs' ability to learn from trajectories and predict future outcomes. These methods are inspired by the modern learning paradigm of LLMs and aim to bridge the gap between past and future contexts in image sequences.\",\n  \"Direct Inspiration\": [\"b4\", \"b54\", \"b30\"],\n  \"Indirect Inspiration\": [\"b18\", \"b26\", \"b31\", \"b59\", \"b64\", \"b77\"],\n  \"Other Inspiration\": [\"b8\", \"b40\", \"b41\", \"b74\"]\n}\n```", "```json\n{\n  \"Summary\": \"This paper introduces the concept of 'foresight minds' in Multimodal Large Language Models (MLLMs), aiming to equip them with the ability to predict future events based on current observations. The primary challenges include the existing MLLMs' inability to foresee future events from current image observations and the difficulty in extracting dynamic clues from visual information. The authors propose two methods: Foresight Pre-Training (FPT) and Foresight Instruction-Tuning (FIT), which are inspired by the modern learning paradigm of LLMs to address these challenges.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b54\": 0.9,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b50\": 0.7,\n    \"b70\": 0.7,\n    \"b10\": 0.6,\n    \"b73\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b40\": 0.5,\n    \"b41\": 0.5,\n    \"b74\": 0.5,\n    \"b18\": 0.4,\n    \"b26\": 0.4,\n    \"b31\": 0.4,\n    \"b59\": 0.4,\n    \"b64\": 0.4,\n    \"b77\": 0.4,\n    \"b23\": 0.3,\n    \"b28\": 0.3,\n    \"b42\": 0.3,\n    \"b76\": 0.3,\n    \"b38\": 0.3,\n    \"b52\": 0.2,\n    \"b17\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling Multimodal Large Language Models (MLLMs) to foresee future events based on present observations, termed as 'foresight minds.' The proposed solution integrates 'future modeling' into existing MLLMs using subjects' trajectories as a learning objective. Two key methods, Foresight Pre-Training (FPT) and Foresight Instruction-Tuning (FIT), are introduced to empower MLLMs with this predictive capability.\",\n  \"Direct Inspiration\": {\n    \"b48\": 0.9,\n    \"b1\": 0.85,\n    \"b50\": 0.8,\n    \"b70\": 0.75,\n    \"b10\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.65,\n    \"b30\": 0.6,\n    \"b54\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.55,\n    \"b40\": 0.55,\n    \"b41\": 0.55,\n    \"b74\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling Multimodal Large Language Models (MLLMs) to foresee future events based on current observations, a capability termed 'foresight minds'. The authors propose integrating 'future modeling' into MLLM learning frameworks through two innovative methods: Foresight Pre-Training (FPT) and Foresight Instruction-Tuning (FIT). These methods aim to bridge the gap between past and future by using trajectory modeling as a learning objective.\",\n  \"Direct Inspiration\": {\n    \"b48\": 0.9,\n    \"b1\": 0.85,\n    \"b41\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.75,\n    \"b8\": 0.7,\n    \"b14\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b50\": 0.6,\n    \"b25\": 0.55,\n    \"b4\": 0.5\n  }\n}\n```"], "6427029c90e50fcafd5d6bdb": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of managing large datasets in datacenters by proposing AstriFlash, a hardware-software co-designed system that tightly integrates flash and DRAM. The goal is to achieve DRAM-like performance with the cost benefits of flash while maintaining the abstraction of virtual memory. Key insights include using lightweight user-level thread switches to hide flash accesses and eliminating traditional OS paging overheads.\",\n  \"Direct Inspiration\": {\n    \"b43\": 0.95,\n    \"b48\": 0.9,\n    \"b49\": 0.9,\n    \"b63\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b17\": 0.8,\n    \"b21\": 0.8,\n    \"b22\": 0.8,\n    \"b40\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b60\": 0.7,\n    \"b75\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7,\n    \"b50\": 0.7,\n    \"b61\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of managing large datasets in datacenters with high performance and cost-effectiveness. It proposes AstriFlash, a hardware-software co-designed system that integrates flash and DRAM to achieve DRAM-like performance while maintaining the abstraction of virtual memory. The key contributions include the use of DRAM as a hardware-managed cache and the use of lightweight user-level thread switches to hide flash access latency.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b8\": 1,\n    \"b43\": 1,\n    \"b48\": 1,\n    \"b49\": 1,\n    \"b63\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b40\": 0.8,\n    \"b60\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.7,\n    \"b35\": 0.7,\n    \"b50\": 0.7,\n    \"b61\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing datacenters that can manage increasing datasets with high performance and low cost. It proposes AstriFlash, a hardware-software co-designed system that integrates flash and DRAM to achieve DRAM-like performance with the cost benefits of flash. The key insights include using lightweight user-level thread switches to hide flash accesses and eliminating traditional OS paging overheads using an accelerated DRAM miss handler.\",\n  \"Direct Inspiration\": {\n    \"b43\": 0.9,\n    \"b48\": 0.8,\n    \"b49\": 0.8,\n    \"b63\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b17\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b40\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b60\": 0.5,\n    \"b75\": 0.5,\n    \"b34\": 0.5,\n    \"b35\": 0.5,\n    \"b50\": 0.5,\n    \"b61\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of integrating NAND flash with DRAM to create a cost-effective, high-performance memory hierarchy for datacenters. It proposes AstriFlash, a hardware-software co-designed system that combines the benefits of both DRAM and flash, while overcoming the performance overheads associated with traditional OS demand paging mechanisms.\",\n    \"Direct Inspiration\": {\n        \"b43\": 1.0,\n        \"b48\": 0.9,\n        \"b49\": 0.9,\n        \"b63\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.8,\n        \"b8\": 0.8,\n        \"b11\": 0.7,\n        \"b40\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.6,\n        \"b17\": 0.6,\n        \"b21\": 0.6,\n        \"b22\": 0.6,\n        \"b41\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high server costs and limited scalability of DRAM in datacenters by proposing AstriFlash, a hardware-software co-designed system that integrates flash and DRAM. AstriFlash aims to provide DRAM-like performance with the cost benefits of flash while maintaining virtual memory abstraction. Key methods include using DRAM as a hardware-managed cache, employing fast user-level thread switches to hide flash access latency, and providing microarchitectural support for efficient memory management.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b8\": 1.0,\n    \"b43\": 0.9,\n    \"b48\": 0.8,\n    \"b49\": 0.8,\n    \"b63\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b21\": 0.7,\n    \"b35\": 0.6,\n    \"b40\": 0.7,\n    \"b60\": 0.6,\n    \"b75\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b4\": 0.5,\n    \"b26\": 0.5,\n    \"b34\": 0.5,\n    \"b41\": 0.5,\n    \"b50\": 0.5,\n    \"b61\": 0.5,\n    \"b66\": 0.5,\n    \"b73\": 0.5\n  }\n}\n```"], "64a29612d68f896efa28bc9c": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in scheduling for tiled accelerators used in deep neural networks (DNNs). It introduces a novel Resource Allocation Tree notation to systematically define the space of inter-layer scheduling, which has been previously underexplored. The paper's contributions include a definition of the RA Tree, an analysis of its impact on hardware behavior, and the development of the SET framework for efficient scheduling exploration.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.85,\n    \"b47\": 0.8,\n    \"b53\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.7,\n    \"b33\": 0.65,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently translating massive computing and storage resources into actual performance on tiled accelerators for deep neural networks (DNNs). It introduces a systematic Resource Allocation (RA) Tree notation to define inter-layer scheduling space, analyzes its impact on hardware behavior, and develops the SET framework to explore this space, featuring a simulated-annealing-based algorithm.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b16\": 0.8,\n    \"b47\": 0.8,\n    \"b53\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b26\": 0.6,\n    \"b51\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of efficiently translating massive computing and storage resources into actual performance for tiled accelerators, particularly through inter-layer scheduling. The authors propose a novel Resource Allocation Tree (RA Tree) notation to define inter-layer scheduling systematically and develop an end-to-end scheduling framework, SET, to explore this newly defined space.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b17\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.7,\n        \"b16\": 0.7,\n        \"b47\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.6,\n        \"b33\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently translating massive computing and storage resources into actual performance on tiled accelerators for DNNs. The proposed solution includes a uniform and systematic Resource Allocation Tree notation for inter-layer scheduling, detailed analysis of hardware behaviors, and the development of an end-to-end scheduling framework named SET.\",\n  \"Direct Inspiration\": [\"b2\", \"b17\", \"b62\", \"b63\"],\n  \"Indirect Inspiration\": [\"b16\", \"b20\", \"b23\", \"b29\", \"b33\", \"b38\", \"b43\", \"b61\"],\n  \"Other Inspiration\": [\"b47\", \"b56\", \"b57\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high utilization and energy efficiency in tiled accelerators for deep neural networks (DNNs). It introduces the Resource Allocation (RA) Tree notation to depict inter-layer scheduling schemes and develops the SET framework to explore this scheduling space efficiently.\",\n  \"Direct Inspiration\": [\"b17\", \"b2\"],\n  \"Indirect Inspiration\": [\"b16\", \"b47\"],\n  \"Other Inspiration\": [\"b3\", \"b53\", \"b57\"]\n}\n```"], "654f510b939a5f408289af51": ["```json\n{\n  \"Summary\": \"The paper tackles the challenge of comparing the performance between AArch64 and RISC-V ISAs, focusing on path length and critical path analysis using The Simulation Engine (SimEng). The primary aim is to determine if RISC-V can match the performance of AArch64 in high-performance use cases.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b7\": 0.9,\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b1\": 0.6,\n    \"b3\": 0.5,\n    \"b4\": 0.4\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.3,\n    \"b11\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of comparing the performance of RISC-V and AArch64 ISAs in high-performance computing environments. It introduces a novel approach to path length analysis and critical path analysis using the SimEng microarchitectural simulator, which the authors extended with RISC-V support.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.95,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b9\": 0.75,\n    \"b10\": 0.7,\n    \"b12\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on comparing the performance of RISC-V and AArch64 ISAs by analyzing path lengths and critical paths using the SimEng microarchitectural simulator. The primary challenges include measuring and comparing execution times, path lengths, and theoretical minimum CPI for various workloads. The authors aim to identify performance bottlenecks and potential optimizations for high-performance computing applications.\",\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [\"b6\"],\n  \"Other Inspiration\": [\"b8\", \"b7\", \"b9\", \"b12\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper compares the performance of AArch64 and RISC-V ISAs, focusing on path length and critical path analysis using SimEng. The main goal is to determine if RISC-V can match AArch64 in high performance use cases.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b7\": 0.85,\n    \"b8\": 0.85,\n    \"b9\": 0.85,\n    \"b10\": 0.85,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b3\": 0.7,\n    \"b2\": 0.7,\n    \"b11\": 0.7,\n    \"b4\": 0.65\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n    \"Summary\": \"The paper focuses on comparing the performance of RISC-V and AArch64 ISAs by analyzing path lengths and critical paths using the SimEng microarchitectural simulator. It aims to determine if RISC-V can match the performance of AArch64 in high-performance applications. The study involves adding RISC-V support to SimEng, performing path length analysis, and conducting critical path analysis to estimate theoretical minimum runtimes.\",\n    \"Direct Inspiration\": {\n        \"b8\": 0.9,\n        \"b7\": 0.9,\n        \"b9\": 0.9,\n        \"b12\": 0.9,\n        \"b10\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.7,\n        \"b1\": 0.7,\n        \"b3\": 0.7,\n        \"b4\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5,\n        \"b11\": 0.5\n    }\n}\n```"], "646aecaad68f896efa05a6c5": ["```json\n{\n  \"Summary\": \"The paper introduces ToolkenGPT, an alternative solution enabling large language models (LLMs) to master massive tools without any LLM fine-tuning, by representing each tool as a new token ('toolken') to augment the vocabulary. The primary challenges addressed include the limitations of in-context learning and fine-tuning methods in handling numerous and unfamiliar tools. ToolkenGPT combines the strengths of fine-tuning and in-context learning while avoiding their limitations, demonstrating flexibility and effectiveness in numerical reasoning, knowledge-based question answering, and embodied plan generation.\",\n  \"Direct Inspiration\": {\n    \"b52\": 0.95,\n    \"b49\": 0.9,\n    \"b64\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.75,\n    \"b16\": 0.7,\n    \"b61\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b42\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces ToolkenGPT, an innovative approach for enabling Large Language Models (LLMs) to master and use a wide array of external tools efficiently. The main challenges addressed include the integration of multiple tools, minimizing the need for fine-tuning, and overcoming limitations of in-context learning such as context length constraints. The key idea is to represent each tool as a new token ('toolken') which is integrated into the LLM's vocabulary, allowing the model to generate inputs for tools dynamically during text generation. This method combines the strengths of fine-tuning and in-context learning while avoiding their respective drawbacks.\",\n  \"Direct Inspiration\": {\n    \"b52\": 1.0,\n    \"b46\": 0.9,\n    \"b64\": 0.9,\n    \"b49\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.7,\n    \"b61\": 0.7,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b16\": 0.5,\n    \"b32\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces ToolkenGPT, a framework that enables Large Language Models (LLMs) to master and utilize a wide array of tools efficiently without heavy fine-tuning. Key challenges include the rapid emergence of new tools, the limitations of in-context learning, and the high cost and inflexibility of fine-tuning. ToolkenGPT addresses these by representing each tool as a new token (toolken) and learning lightweight toolken embeddings, combining the strengths of both fine-tuning and in-context learning while avoiding their limitations.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.95,\n    \"b49\": 0.9,\n    \"b52\": 0.85,\n    \"b64\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.75,\n    \"b61\": 0.7,\n    \"b32\": 0.65,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.55,\n    \"b58\": 0.5,\n    \"b43\": 0.45,\n    \"b35\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces ToolkenGPT, a framework designed to enable large language models (LLMs) to efficiently master and use a wide array of external tools without the need for extensive fine-tuning. The key innovation is the representation of each tool as a token ('toolken') with its own embedding, facilitating quick adaptation and integration of new tools. This approach combines the strengths of fine-tuning and in-context learning while overcoming their limitations, allowing LLMs to handle a large number of tools and complex tasks efficiently.\",\n  \"Direct Inspiration\": {\n    \"b42\": 0.9,\n    \"b46\": 0.8,\n    \"b49\": 0.8,\n    \"b52\": 0.9,\n    \"b64\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b28\": 0.6,\n    \"b61\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b32\": 0.4,\n    \"b17\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents ToolkenGPT, an approach to enable LLMs to master and adapt to new tools efficiently by representing each tool as a new token ('toolken'). The primary challenges addressed include the complexity of tool learning due to the rapid emergence of new tools, the limitations of fine-tuning and in-context learning, and the need for efficient adaptation without extensive fine-tuning. ToolkenGPT introduces a novel method of embedding tools as tokens in the LLM vocabulary, allowing for efficient and scalable tool learning and usage.\",\n  \"Direct Inspiration\": {\n    \"b46\": 1,\n    \"b52\": 1,\n    \"b64\": 1,\n    \"b49\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b61\": 0.8,\n    \"b58\": 0.7,\n    \"b32\": 0.6,\n    \"b5\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.5,\n    \"b17\": 0.5,\n    \"b2\": 0.5,\n    \"b42\": 0.5\n  }\n}\n```"], "62c64f2e5aee126c0f6cf0f9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving dense retrieval methods for passage ranking by proposing a novel pre-training model, SimLM. The model leverages replaced language modeling to enhance the representation bottleneck, improving the performance of biencoder-based retrievers.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b12\": 1,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b9\": 0.8,\n    \"b16\": 0.7,\n    \"b23\": 0.75,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b13\": 0.6,\n    \"b17\": 0.65,\n    \"b24\": 0.55,\n    \"b25\": 0.55,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in dense retrieval methods for passage retrieval tasks by proposing a novel pre-training method called SimLM. The primary motivation is to improve the encoding of semantic information into a representation bottleneck, which is hypothesized to enhance the performance of biencoder-based dense retrievers.\",\n  \"Direct Inspiration\": [\"b11\", \"b12\", \"b22\"],\n  \"Indirect Inspiration\": [\"b5\", \"b9\"],\n  \"Other Inspiration\": [\"b3\", \"b16\", \"b21\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving dense passage retrieval for tasks like ad-hoc information retrieval and question answering. It introduces SimLM, a novel pre-training method using a representation bottleneck and a replaced language modeling objective. The proposed method aims to compress essential semantic information into a [CLS] vector, improving the performance of bi-encoder-based dense retrievers.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b12\": 0.9,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b9\": 0.7,\n    \"b16\": 0.6,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving dense retrieval methods for passage retrieval tasks by proposing a novel pre-training method called SimLM. This method uses a representation bottleneck with a replaced language modeling objective, aiming to improve the encoding of semantic information in the [CLS] vector. The paper compares SimLM with existing methods such as Condenser and coCondenser and demonstrates substantial performance gains on various datasets.\",\n  \"Direct Inspiration\": [\"b11\", \"b12\", \"b22\"],\n  \"Indirect Inspiration\": [\"b9\", \"b23\"],\n  \"Other Inspiration\": [\"b3\", \"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving passage retrieval performance using a novel pre-training method called SimLM. The primary focus is on adapting text encoders for retrieval tasks by employing a representation bottleneck with a replaced language modeling objective. The proposed method aims to enhance the semantic information encoded in the [CLS] vector, which is crucial for dense retrieval models.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b9\": 0.9,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b3\": 0.7,\n    \"b23\": 0.7,\n    \"b30\": 0.7,\n    \"b33\": 0.7,\n    \"b41\": 0.7\n  }\n}\n```"], "641137fd90e50fcafd17b84e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving information retrieval (IR) by leveraging large language models (LLMs) for query expansion. The proposed method, query2doc, generates pseudo-documents using few-shot prompts from LLMs, which are then concatenated with the original query to enhance retrieval performance. This method is simple, does not require model fine-tuning or architectural changes, and shows significant improvements in both sparse and dense retrieval systems, particularly for challenging queries and out-of-domain settings.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b3\": 0.9,\n    \"b11\": 0.9,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b15\": 0.7,\n    \"b21\": 0.7,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b19\": 0.5,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in information retrieval (IR) by proposing a novel query expansion method using large language models (LLMs). The method, called query2doc, generates pseudo-documents through few-shot prompting of LLMs, which are then concatenated with the original query to form a new query. This approach aims to improve retrieval performance without requiring changes in training pipelines or model architectures.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b18\": 0.9,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.75,\n    \"b27\": 0.7,\n    \"b15\": 0.65,\n    \"b3\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b23\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving information retrieval (IR) by leveraging large language models (LLMs) for query expansion. The proposed method, query2doc, involves generating pseudo-documents through few-shot prompting and concatenating them with the original query to enhance retrieval performance. This approach is evaluated on both in-domain and out-of-domain datasets, demonstrating significant improvements in retrieval metrics, particularly for sparse retrieval systems like BM25.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b11\": 0.9,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b15\": 0.8,\n    \"b18\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.75,\n    \"b29\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving information retrieval (IR) by utilizing Large Language Models (LLMs) for query expansion. The proposed method, query2doc, generates pseudo-documents through few-shot prompting of LLMs and concatenates them with the original query. This approach aims to enhance both sparse and dense retrieval systems without requiring changes to training pipelines or model architectures, ensuring ease of integration with existing methods.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b11\": 0.9,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b12\": 0.6,\n    \"b15\": 0.6,\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in information retrieval (IR), particularly the lexical gap in sparse retrieval systems and the need for effective query expansion. It proposes a novel method called query2doc that leverages large language models (LLMs) to generate pseudo-documents for query expansion. This method aims to improve retrieval performance without requiring changes to training pipelines or model architectures.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1.0,\n        \"b11\": 0.9,\n        \"b27\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.7,\n        \"b15\": 0.6,\n        \"b18\": 0.6,\n        \"b19\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.5,\n        \"b21\": 0.4,\n        \"b23\": 0.4\n    }\n}\n```"], "657c181a939a5f4082ab98e2": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of microarchitecture design space exploration (DSE) by proposing a novel approach that combines automated bottleneck analysis with a new graph model formulation based on critical path analysis. The method aims to circumvent the limitations of mechanistic models requiring immense domain knowledge and black-box methodologies with high computing demands.\",\n  \"Direct Inspiration\": [\"b20\", \"b22\", \"b34\", \"b42\", \"b43\", \"b54\"],\n  \"Indirect Inspiration\": [\"b6\", \"b10\", \"b26\", \"b33\", \"b35\"],\n  \"Other Inspiration\": [\"b3\", \"b8\", \"b13\", \"b14\", \"b30\", \"b32\", \"b58\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in microarchitecture Design Space Exploration (DSE) related to the extremely large design space and high simulation overheads. The proposed solution, ArchExplorer, uses a new graph model formulation based on critical path analysis to accurately identify and eliminate bottlenecks in microarchitecture, aiming to achieve better performance-power-area (PPA) trade-offs with fewer simulations.\",\n    \"Direct Inspiration\": [\"b20\", \"b22\", \"b34\", \"b42\", \"b43\", \"b54\"],\n    \"Indirect Inspiration\": [\"b3\", \"b6\", \"b10\", \"b26\", \"b33\", \"b35\"],\n    \"Other Inspiration\": [\"b24\", \"b25\", \"b46\", \"b53\", \"b17\", \"b29\", \"b41\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in microarchitecture design space exploration (DSE) by proposing an approach that combines automated bottleneck analysis with a new dynamic event-dependence graph (DEG) formulation. This method reduces the need for extensive domain knowledge and computational resources compared to traditional mechanistic and black-box models.\",\n  \"Direct Inspiration\": [\"b20\", \"b22\", \"b34\", \"b42\", \"b43\", \"b54\"],\n  \"Indirect Inspiration\": [\"b6\", \"b10\", \"b26\", \"b33\", \"b35\"],\n  \"Other Inspiration\": [\"b5\", \"b8\", \"b10\", \"b13\", \"b14\", \"b26\", \"b30\", \"b32\", \"b33\", \"b35\", \"b58\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in microarchitecture design space exploration (DSE), specifically targeting the performance-power-area (PPA) trade-offs. It proposes an algorithm called ArchExplorer that circumvents the limitations of mechanistic models and black-box methodologies by using automated bottleneck analysis. The method reduces the demand for expert knowledge and computing resources through a new dynamic event-dependence graph (DEG) formulation and critical path analysis.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b22\": 0.9,\n    \"b34\": 0.9,\n    \"b42\": 0.9,\n    \"b43\": 0.9,\n    \"b54\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b10\": 0.7,\n    \"b26\": 0.7,\n    \"b33\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b8\": 0.5,\n    \"b13\": 0.5,\n    \"b14\": 0.5,\n    \"b30\": 0.5,\n    \"b32\": 0.5,\n    \"b35\": 0.5,\n    \"b58\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of microarchitecture exploration to achieve optimal performance-power-area (PPA) trade-offs without extensive simulations or expert knowledge. The authors propose a novel design space exploration (DSE) method leveraging automated bottleneck analysis and a new dynamic event-dependence graph (DEG) formulation to efficiently identify and mitigate hardware resource bottlenecks.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b22\": 0.9,\n    \"b34\": 0.9,\n    \"b42\": 0.9,\n    \"b43\": 0.9,\n    \"b54\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b10\": 0.7,\n    \"b26\": 0.7,\n    \"b33\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b17\": 0.6,\n    \"b29\": 0.6,\n    \"b41\": 0.6,\n    \"b27\": 0.6,\n    \"b47\": 0.6,\n    \"b55\": 0.6\n  }\n}\n```"], "6326303790e50fcafdf36ca9": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inefficiency of existing prefetchers, which are usually hardware-implemented with fixed-function operations that cannot adapt to different applications. The paper proposes a novel analytical framework to develop optimized prefetching strategies based on dynamic application behavior, allowing for significant performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b4\": 0.9,\n    \"b31\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b13\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of optimizing prefetching strategies to reduce memory access latency in modern processors. It proposes a novel analytical framework to determine the best prefetching strategy based on application execution measurements, aiming to enhance application performance by providing accurate and timely prefetch schedules.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b4\": 0.9,\n        \"b31\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b20\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b7\": 0.6,\n        \"b37\": 0.6,\n        \"b45\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the increasing speed gap between processors and memory systems, and the inefficiencies of fixed-function hardware prefetchers. The paper proposes a novel analytical framework to dynamically optimize prefetcher strategies based on application execution metrics, aiming to improve prefetching accuracy and timeliness. The framework outputs an optimized prefetch schedule and estimates performance, providing developers with tools to better understand and utilize prefetching capabilities.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b4\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.8,\n    \"b20\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the bottleneck of memory accessing in modern processors, the limitations of existing hardware prefetchers, and the need for configurable and dynamic prefetching strategies. The paper proposes a novel analytical framework to optimize prefetching strategies based on application execution measurements, aiming to enhance performance predictability and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b4\": 1,\n    \"b31\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b20\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b6\": 0.6,\n    \"b37\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inefficiency of existing prefetchers due to their fixed-function operation, which limits their adaptability to application-specific memory access patterns. The authors propose a novel analytical framework that optimizes prefetcher strategies based on measurements of application execution. This framework aims to provide both an optimized prefetch schedule and a performance estimate, improving the transparency and effectiveness of prefetching.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```"], "64b60eaa3fda6d7f06eaea30": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling non-professional users to solve geospatial tasks autonomously using GIS tools, leveraging the semantic understanding capabilities of large language models (LLMs). The proposed solution, GeoGPT, integrates LLMs with mature GIS tools to interpret user demands expressed in natural language and execute the appropriate tools step-by-step.\",\n  \"Direct Inspiration\": [\"b16\", \"b17\", \"b20\"],\n  \"Indirect Inspiration\": [\"b12\", \"b13\", \"b14\", \"b15\", \"b18\", \"b19\"],\n  \"Other Inspiration\": [\"b36\", \"b37\", \"b47\", \"b48\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling non-professional users to solve geospatial tasks autonomously by leveraging GIS tools and large language models (LLMs). The proposed framework, GeoGPT, integrates LLMs with GIS tools to interpret user demands expressed in natural language and execute the appropriate GIS operations to achieve the desired results. The core inspiration comes from the capabilities of LLMs, particularly AutoGPT, in understanding and reasoning about user demands and calling external tools to solve complex tasks.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b17\": 1.0,\n    \"b19\": 1.0,\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b13\": 0.9,\n    \"b14\": 0.9,\n    \"b15\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b48\": 0.8,\n    \"b49\": 0.8\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of enabling non-professional users to autonomously and adaptively plan and execute GIS tools to solve geospatial tasks by leveraging the semantic understanding and reasoning capabilities of large language models (LLMs). The authors propose a framework called GeoGPT that integrates LLMs with mature GIS tools to perform geospatial data collection, processing, and analysis based on natural language instructions.\",\n    \"Direct Inspiration\": {\n        \"b16\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.8,\n        \"b13\": 0.8,\n        \"b14\": 0.8,\n        \"b15\": 0.8,\n        \"b17\": 0.8,\n        \"b18\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.6,\n        \"b20\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include lowering the threshold for non-professional users to solve geospatial tasks, accurately interpreting user demands, and autonomously organizing and executing GIS tools in the right sequence. The proposed algorithm, GeoGPT, integrates the semantic understanding ability of Large Language Models (LLMs) with mature GIS tools to automatically interpret and solve geospatial tasks based on natural language input.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.95,\n    \"b17\": 0.9,\n    \"b19\": 0.85,\n    \"b20\": 0.85,\n    \"b48\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b37\": 0.75,\n    \"b47\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b26\": 0.7,\n    \"b27\": 0.7,\n    \"b28\": 0.7,\n    \"b29\": 0.7,\n    \"b30\": 0.7,\n    \"b31\": 0.7,\n    \"b32\": 0.7,\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7,\n    \"b36\": 0.7,\n    \"b38\": 0.7,\n    \"b39\": 0.7,\n    \"b40\": 0.7,\n    \"b41\": 0.7,\n    \"b42\": 0.7,\n    \"b43\": 0.7,\n    \"b44\": 0.7,\n    \"b45\": 0.7,\n    \"b46\": 0.7,\n    \"b49\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of making GIS tools accessible to non-professional users through the development of an autonomous framework called GeoGPT. This framework integrates the semantic understanding capabilities of large language models (LLMs) with mature GIS tools to interpret user demands expressed in natural language and execute the necessary geospatial operations.\",\n    \"Direct Inspiration\": {\n        \"b16\": 0.95,\n        \"b17\": 0.85,\n        \"b19\": 0.80,\n        \"b20\": 0.80\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.75,\n        \"b11\": 0.75,\n        \"b12\": 0.70,\n        \"b13\": 0.70\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.65,\n        \"b22\": 0.65,\n        \"b23\": 0.65,\n        \"b24\": 0.65,\n        \"b25\": 0.60,\n        \"b26\": 0.60,\n        \"b27\": 0.60,\n        \"b36\": 0.55,\n        \"b37\": 0.55,\n        \"b38\": 0.55,\n        \"b39\": 0.55,\n        \"b47\": 0.50,\n        \"b48\": 0.50\n    }\n}\n```"], "6584f922939a5f408236fc36": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of temporal misalignment in language models, where differences in training and testing data over time lead to performance degradation. The proposed solution is 'time vectors', which are derived from the weight differences between pretrained and finetuned models over specific time periods. These vectors help customize model behavior for new time periods through interpolation.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b14\": 0.9,\n    \"b10\": 0.8,\n    \"b15\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.6,\n    \"b16\": 0.6,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of temporal misalignment in language models, where variations in train and test data over time lead to performance degradation. The authors propose using 'time vectors' to adapt language models to specific time periods by interpolating between model weights finetuned on different time periods.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b14\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b12\": 0.7,\n    \"b21\": 0.7,\n    \"b16\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b7\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of temporal misalignment in language models, where deviations in train and test data across different time periods lead to performance degradation. The proposed solution involves 'time vectors', which are vectors in weight space derived from the difference between pretrained and finetuned model weights. These vectors allow customization of model behavior to new time periods through interpolation and analogy arithmetic.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b14\": 0.8,\n    \"b15\": 0.7,\n    \"b12\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b11\": 0.6,\n    \"b16\": 0.6,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the temporal misalignment in language models, where deviations in train and test data across different time periods cause significant performance degradation. The proposed algorithm introduces 'time vectors', an extension of task vectors, to customize language models to new time periods through weight-space interpolation. These time vectors improve model performance on text from specific time periods and can generalize to future periods using analogy arithmetic.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b12\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b10\": 0.5,\n    \"b13\": 0.5,\n    \"b7\": 0.5,\n    \"b11\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the temporal misalignment in language models, resulting in performance degradation across different time periods. The proposed algorithm introduces 'time vectors' to customize language models to specific time periods through weight-space interpolation. This method allows models to generalize better to new or unseen time periods by interpolating between time vectors derived from models finetuned on specific time periods.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b15\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b14\": 0.7\n  }\n}\n```"], "642e38e090e50fcafd6abcf8": ["```json\n{\n  \"Summary\": \"The paper aims to build a foundation model for segmentation by introducing three interconnected components: a promptable segmentation task, a segmentation model (SAM) for data annotation and zero-shot transfer, and a data engine for collecting a large dataset (SA-1B). The primary challenges addressed include defining a task that enables zero-shot generalization, developing a model architecture that supports flexible prompts and real-time mask prediction, and creating a large and diverse dataset for training. The approach is inspired by the success of foundation models in NLP and computer vision, particularly in terms of zero-shot and few-shot learning capabilities enabled by prompt engineering.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b81\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b107\": 0.8,\n    \"b69\": 0.8,\n    \"b56\": 0.7,\n    \"b2\": 0.7,\n    \"b82\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b65\": 0.5,\n    \"b43\": 0.5,\n    \"b115\": 0.5,\n    \"b59\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of building a foundation model for segmentation through three components: a promptable segmentation task, a segmentation model (SAM), and a data engine for collecting a large dataset (SA-1B). It aims to develop a model that can generalize to new segmentation tasks using prompt engineering, trained on a broad dataset.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b81\": 0.9,\n    \"b56\": 0.8,\n    \"b43\": 0.8,\n    \"b115\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b69\": 0.7,\n    \"b13\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.6,\n    \"b2\": 0.6,\n    \"b1\": 0.6,\n    \"b58\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a foundation model for image segmentation, focusing on a promptable segmentation task, a model named SAM (Segment Anything Model), and a data engine for creating a large-scale dataset (SA-1B). The primary challenges addressed are enabling zero-shot generalization, designing an appropriate model architecture for flexible prompting, and collecting a diverse and large-scale dataset. The authors draw inspiration from foundation models in NLP and vision, such as CLIP and interactive segmentation methods.\",\n  \"Direct Inspiration\": [\"b9\", \"b81\"],\n  \"Indirect Inspiration\": [\"b65\", \"b59\", \"b43\", \"b115\", \"b107\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Defining a task that enables zero-shot generalization for image segmentation.\",\n      \"Developing a model architecture that supports flexible prompting and real-time mask computation.\",\n      \"Creating a large-scale and diverse dataset for training the model.\"\n    ],\n    \"inspirations\": [\n      \"Adapting prompt engineering techniques from NLP to image segmentation.\",\n      \"Building a data engine for generating a large dataset of segmentation masks.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\n      \"b9\",\n      \"b81\",\n      \"b107\",\n      \"b69\"\n    ]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\n      \"b13\",\n      \"b19\",\n      \"b61\",\n      \"b46\",\n      \"b32\",\n      \"b64\",\n      \"b72\",\n      \"b70\"\n    ]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\n      \"b56\",\n      \"b2\",\n      \"b1\",\n      \"b93\",\n      \"b89\",\n      \"b65\",\n      \"b58\",\n      \"b25\",\n      \"b112\",\n      \"b18\",\n      \"b53\"\n    ]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a foundation model for image segmentation, incorporating a promptable segmentation task, the Segment Anything Model (SAM), and a data engine to collect a large-scale dataset (SA-1B). The main challenges addressed include enabling zero-shot generalization, developing a model architecture that supports flexible prompting, and creating a large and diverse dataset for training.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b81\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b107\": 0.75,\n    \"b13\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.6,\n    \"b69\": 0.55,\n    \"b32\": 0.5\n  }\n}\n```"], "64f7fc6a3fda6d7f06f43fbe": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a lightweight and generic pitch estimation model that does not require labeled data for training. It proposes a self-supervised learning (SSL) paradigm based on Siamese networks and equivariance to pitch transpositions, inspired by previous works on equivariant pitch and tempo estimation.\",\n  \"Direct Inspiration\": [\"b19\", \"b20\"],\n  \"Indirect Inspiration\": [\"b22\", \"b24\", \"b26\", \"b28\", \"b29\"],\n  \"Other Inspiration\": [\"b17\", \"b35\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the need for a lightweight and generic model for pitch estimation that does not require labeled data. The proposed algorithm draws from the concepts of equivariant pitch and tempo estimation, using a self-supervised learning (SSL) paradigm based on Siamese networks and equivariance to pitch transpositions. The novel contributions include the formulation of pitch estimation as a multi-class problem, a novel class-based equivariance loss, and a lightweight, transposition-equivariant model architecture featuring Toeplitz fully-connected layers.\",\n    \"Direct Inspiration\": [\"b19\", \"b20\"],\n    \"Indirect Inspiration\": [\"b22\", \"b24\", \"b28\"],\n    \"Other Inspiration\": [\"b17\", \"b35\", \"b53\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of monophonic pitch estimation, proposing a lightweight and generic model that doesn't require labeled data for training. The authors introduce a novel equivariance loss, a class-based equivariance loss which prevents collapse, and Toeplitz fully-connected layers to achieve transposition-equivariance. The method is evaluated on several datasets, showing robustness to domain-shift and background music, and requiring minimal computational resources.\",\n  \"Direct Inspiration\": [\"b19\", \"b20\"],\n  \"Indirect Inspiration\": [\"b22\", \"b24\", \"b26\", \"b28\", \"b29\", \"b35\"],\n  \"Other Inspiration\": [\"b17\", \"b51\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of pitch estimation using deep learning techniques, specifically focusing on creating a lightweight, generic model that does not require labeled data for training. The proposed method is inspired by equivariant pitch estimation and tempo estimation algorithms, and it introduces a new equivariance loss to capture pitch information. The paper's contributions include formulating pitch estimation as a multi-class problem, proposing a novel class-based equivariance loss, and introducing Toeplitz fully-connected layers for a lightweight architecture.\",\n  \"Direct Inspiration\": [\"b19\", \"b20\"],\n  \"Indirect Inspiration\": [\"b22\", \"b24\", \"b26\", \"b28\", \"b29\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a lightweight and generic model for pitch estimation that does not require labeled data and is computationally efficient. The authors propose a self-supervised learning (SSL) approach using Siamese networks and equivariance to pitch transpositions. They introduce a novel class-based equivariance loss and a transposition-equivariant architecture with Toeplitz fully-connected layers, demonstrating superior performance and robustness to domain-shift and background music.\",\n  \"Direct Inspiration\": [\"b19\", \"b20\"],\n  \"Indirect Inspiration\": [\"b22\", \"b24\", \"b26\", \"b28\", \"b29\"],\n  \"Other Inspiration\": []\n}\n```"], "6257c63c5aee126c0f47280f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of solving nonconvex-nonconcave minimax problems, which are difficult for traditional scalable first-order methods. The authors propose an adaptive extragradient-type algorithm called (CurvatureEG+) that converges for a larger range of parameters in the weak Minty variational inequality (MVI) assumption. This method improves upon previous algorithms by allowing larger stepsizes, ensuring convergence for a broader class of problems.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b17\": 0.9,\n    \"b41\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.7,\n    \"b5\": 0.7,\n    \"b12\": 0.6,\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.5,\n    \"b4\": 0.5,\n    \"b39\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve the notorious difficulties posed by nonconvex-nonconcave constrained minimax problems for scalable first order methods. The novel algorithm proposed, (CurvatureEG+), adapts the extragradient method to handle a broader range of nonconvexity parameters and ensures convergence by adaptively choosing the extrapolation stepsize. The paper builds upon previous work on extragradient methods and variational inequalities, particularly focusing on weak Minty variational inequalities.\",\n  \"Direct Inspiration\": [\"b9\", \"b17\"],\n  \"Indirect Inspiration\": [\"b26\", \"b12\", \"b5\", \"b41\"],\n  \"Other Inspiration\": [\"b30\", \"b7\", \"b15\", \"b22\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of nonconvex-nonconcave minimax problems, proposing an adaptive extragradient-type algorithm called CurvatureEG+ to ensure convergence. The approach extends the parameter range in weak Minty variational inequality (MVI) and shows tightness of results through counterexamples.\",\n    \"Direct Inspiration\": [\"b9\", \"b17\", \"b41\"],\n    \"Indirect Inspiration\": [\"b26\", \"b12\", \"b5\"],\n    \"Other Inspiration\": [\"b30\", \"b7\", \"b22\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of nonconvex-nonconcave minimax problems by proposing an adaptive extragradient-type algorithm (CurvatureEG+), which extends the range of the parameter in the weak Minty variational inequality (MVI) and guarantees convergence for larger stepsizes. The proposed algorithm improves upon previous methods by incorporating adaptive stepsizes and by generalizing the EG+ method to include more general settings involving a maximally monotone operator.\",\n  \"Direct Inspiration\": [\"b9\", \"b17\"],\n  \"Indirect Inspiration\": [\"b13\", \"b21\", \"b41\"],\n  \"Other Inspiration\": [\"b26\", \"b27\", \"b5\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of nonconvex-nonconcave constrained minimax problems in machine learning, which are difficult for scalable first-order methods. It proposes a new adaptive extragradient-type algorithm (CurvatureEG+) that converges for a larger range of the weak Minty variational inequality (MVI) parameter and avoids limit cycles by adaptively choosing the extrapolation stepsize.\",\n  \"Direct Inspiration\": [\"b9\", \"b17\"],\n  \"Indirect Inspiration\": [\"b41\"],\n  \"Other Inspiration\": [\"b26\", \"b12\", \"b5\"]\n}\n```"], "63dcdb422c26941cf00b61c5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of solving stochastic weak Minty variational inequalities (MVIs) without increasing the batch size, which has been a significant hurdle in the field. The authors propose a bias-corrected stochastic extragradient (BC-SEG+) algorithm that maintains a fixed stepsize and introduces a bias-correction term. The paper makes notable contributions by showing convergence for weak MVIs without batch size increase, generalizing the result to a family of schemes, and proving almost sure convergence under specific conditions.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b9\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of solving stochastic weak Minty variational inequalities (MVIs) without increasing batch size, which is crucial for machine learning applications. The authors propose a bias-corrected stochastic extragradient (BC-SEG+) algorithm, which maintains a fixed stepsize and introduces a bias-correction term. The paper also extends the results to constrained and regularized settings, and proves almost sure convergence under the Robbins-Monro stepsize schedule.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.8,\n    \"b32\": 0.7,\n    \"b9\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of solving stochastic weak Minty variational inequalities (MVIs) without increasing the batch size, which is crucial for practical machine learning applications. It proposes a novel algorithm called bias-corrected stochastic extragradient (BC-SEG+) that includes a bias-correction term to ensure convergence while maintaining a fixed stepsize. The paper builds on previous work in both deterministic and stochastic settings, focusing on weak MVIs, and extends the results to constrained and regularized settings.\",\n  \"Direct Inspiration\": [\"b13\", \"b29\"],\n  \"Indirect Inspiration\": [\"b33\", \"b32\", \"b9\", \"b19\"],\n  \"Other Inspiration\": [\"b23\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is solving stochastic weak Minty variational inequalities (MVIs) without increasing the batch size. The authors propose a bias-corrected stochastic extragradient (BC-SEG+) algorithm that maintains a fixed stepsize and introduces a bias-correction term to ensure convergence. They generalize their results to constrained and regularized settings and provide almost sure convergence guarantees.\",\n  \"Direct Inspiration\": [\"b13\", \"b29\", \"b33\"],\n  \"Indirect Inspiration\": [\"b12\", \"b17\", \"b3\", \"b23\", \"b20\"],\n  \"Other Inspiration\": [\"b9\", \"b32\", \"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of solving stochastic weak Minty variational inequalities (MVIs) without increasing the batch size, which is crucial for many machine learning applications. The authors propose a bias-corrected stochastic extragradient (BC-SEG+) algorithm that maintains a fixed stepsize and introduces a bias-correction term to address the noise in the stochastic setting. The paper also generalizes this approach to constrained and regularized settings and provides almost sure convergence guarantees.\",\n  \"Direct Inspiration\": [\"b13\", \"b29\", \"b33\"],\n  \"Indirect Inspiration\": [\"b32\", \"b9\", \"b19\", \"b20\"],\n  \"Other Inspiration\": [\"b3\", \"b23\", \"b6\"]\n}\n```"], "640fe64790e50fcafd9e276e": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of high-throughput generative inference with large language models (LLMs) on limited GPU memory. The proposed solution, FlexGen, offers an efficient offloading strategy aggregating GPU, CPU, and disk memory, using compression methods and pipeline parallelism to achieve higher throughput. The key contributions are the formal definition of a search space for offloading strategies, development of a linear programming-based optimization algorithm, and demonstrating significant throughput improvements over existing systems.\",\n    \"Direct Inspiration\": {\n        \"b0\": 0.9,\n        \"b2\": 0.8,\n        \"b34\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.7,\n        \"b38\": 0.7,\n        \"b39\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.6,\n        \"b30\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high computational and memory requirements for Large Language Models (LLMs) during inference. The proposed solution, FlexGen, optimizes offloading strategies to improve throughput on a single commodity GPU. The main contributions include the development of a linear programming-based search algorithm for optimizing tensor placement and computational delegation, and a novel compression technique that reduces the memory and I/O costs.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b2\": 0.9,\n    \"b34\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b12\": 0.7,\n    \"b38\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b10\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the high computational and memory requirements for running large language models (LLMs) and the inefficiency of current offloading strategies for high-throughput generative inference on single commodity GPUs. The algorithm proposed by the authors, FlexGen, addresses these challenges by introducing an efficient offloading framework that aggregates memory from GPU, CPU, and disk, and optimizes I/O operations through a linear programming-based search algorithm. Key contributions include defining a search space for offloading strategies, compressing both weights and KV cache to 4 bits, and demonstrating FlexGen's superior throughput compared to existing systems.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b2\": 0.9,\n    \"b34\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b12\": 0.7,\n    \"b39\": 0.6,\n    \"b38\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high-throughput generative inference with limited GPU memory by designing efficient offloading strategies. FlexGen, the proposed framework, aggregates memory from GPU, CPU, and disk and schedules I/O operations efficiently. It also employs compression techniques to reduce memory usage. The main contributions include defining a search space for offloading strategies, developing a linear programming-based search algorithm, and demonstrating the efficacy of FlexGen through empirical evaluations.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b2\": 1.0,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b12\": 0.8,\n    \"b38\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of lowering resource requirements for large language model (LLM) inference, particularly focusing on throughput-oriented generative inference on a single commodity GPU. It introduces FlexGen, a framework that efficiently schedules I/O operations and incorporates compression methods and distributed pipeline parallelism to achieve high-throughput generative inference with limited GPU memory.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b2\": 0.9,\n    \"b34\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b38\": 0.75,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.65,\n    \"b31\": 0.6\n  }\n}\n```"], "6493c733d68f896efad19c1d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating a multimodal medical dialogue system focused on ophthalmology due to the limitations of current general-domain LLMs in biomedical scenarios. The proposed solution, OphGLM, integrates fundus images with large language models to enhance the accuracy and usability of medical Q&A systems in ophthalmology.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b7\": 0.95,\n    \"b10\": 0.8,\n    \"b14\": 0.85,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b11\": 0.6,\n    \"b18\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a multimodal large language-and-vision assistant (OphGLM) specifically for ophthalmology, leveraging fundus images and real-world medical dialogues to improve medical question-answering. The paper proposes a novel method combining visual and language models, constructing a fine-tuning dataset using knowledge graphs and real doctor-patient dialogues.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b14\": 0.9,\n    \"b23\": 0.8,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a multimodal medical dialogue system for ophthalmology, combining visual and language models to improve medical question-answering. The proposed system, OphGLM, is built using fundus images and leverages knowledge graphs and real-world medical dialogues to enhance the performance and authenticity of the model in medical applications.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b10\": 0.9,\n    \"b23\": 0.9,\n    \"b3\": 0.8,\n    \"b21\": 0.8,\n    \"b22\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b13\": 0.7,\n    \"b16\": 0.7,\n    \"b7\": 0.7,\n    \"b2\": 0.6,\n    \"b4\": 0.6,\n    \"b8\": 0.6,\n    \"b25\": 0.6,\n    \"b0\": 0.6,\n    \"b11\": 0.6,\n    \"b18\": 0.6,\n    \"b15\": 0.6,\n    \"b19\": 0.6,\n    \"b12\": 0.6,\n    \"b17\": 0.6,\n    \"b24\": 0.6,\n    \"b1\": 0.5\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing large language models (LLMs) for biomedical visual question answering (VQA) in ophthalmology. The authors propose OphGLM, a novel large language-and-vision assistant that combines visual models with LLMs to improve medical question-answering, especially for ophthalmic diseases. The paper focuses on constructing a specialized dataset using knowledge graphs, real-world medical dialogues, and public fundus image datasets for fine-tuning the model.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b10\": 0.8,\n    \"b7\": 0.85,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.7,\n    \"b3\": 0.65,\n    \"b21\": 0.65,\n    \"b22\": 0.65,\n    \"b20\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b13\": 0.6,\n    \"b1\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of applying large language models (LLMs) to biomedical scenarios, particularly ophthalmology, where existing models trained on general web data perform poorly. It proposes an ophthalmology large language-and-vision assistant (OphGLM) that combines visual models with large language models to assist in diagnosing fundus images. The methodology involves using knowledge graphs and real-world medical dialogues to create a fine-tuning dataset, and developing various disease diagnosis models.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b23\": 0.85,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b13\": 0.7,\n    \"b3\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b16\": 0.6,\n    \"b18\": 0.6,\n    \"b24\": 0.6,\n    \"b1\": 0.5\n  }\n}\n```"], "62283c435aee126c0fd5de60": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in generating realistic talking heads, focusing on accurate lip synchronization, articulator synergy (including the jaw), and temporal coherence. The proposed method includes the Multiple Synergy Network (MSN) for landmark prediction using multimodal inputs and the Video Consistency Network (VCN) for temporally coherent facial animation generation.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b17\": 1,\n    \"b19\": 0.9,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b3\": 0.7,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b20\": 0.6,\n    \"b23\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of generating realistic talking heads, focusing on multimodal inputs for accurate lip-sync, articulator synergy, and temporal coherence in facial animations. The proposed method involves two main steps: predicting mouth landmarks using a Multiple Synergy Network (MSN) and generating temporally coherent facial animations with a Video Consistency Network (VCN). Key contributions include the integration of multimodal inputs, exploration of articulator synergy, and ensuring inter-frame and intra-frame consistencies.\",\n    \"Direct Inspiration\": {\n        \"b0\": 0.9,\n        \"b14\": 0.9,\n        \"b17\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.8,\n        \"b27\": 0.8,\n        \"b29\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.7,\n        \"b20\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in generating realistic talking heads, emphasizing the importance of multimodal inputs (audio and text) for accurate mouth landmark prediction and temporal coherence in facial animations. The proposed method introduces a Multiple Synergy Network (MSN) for landmark prediction and a Video Consistency Network (VCN) for generating photo-realistic, temporally coherent videos.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1,\n        \"b14\": 1,\n        \"b17\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.9,\n        \"b27\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.8,\n        \"b29\": 0.8\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses significant challenges in generating realistic talking heads, including accurate lip-sync, articulator synergy, and temporal continuity. The proposed solution involves two key steps: predicting mouth landmarks using a Multiple Synergy Network (MSN) with multimodal inputs, and generating temporally coherent facial animations using a Video Consistency Network (VCN) that incorporates optical flow and mouth generation techniques.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b14\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b27\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in generating realistic talking head animations, focusing on accurate lip-sync, articulator synergy (lip and jaw movement), and temporal coherence between frames. The proposed method involves two main steps: predicting mouth landmarks using a Multiple Synergy Network (MSN) with multimodal inputs (audio and text) and generating temporally coherent facial animations using a Video Consistency Network (VCN).\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b14\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b27\": 0.8,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```"], "639a90a690e50fcafdf0a150": ["```json\n{\n    \"Summary\": \"The paper aims to improve video generation using GANs by addressing the challenge of modeling temporal dynamics across frames. Key contributions include the adoption of alias-free techniques, the use of a temporal shift module in the discriminator, a novel B-Spline based motion representation, and a low-rank strategy for motion embeddings.\",\n    \"Direct Inspiration\": {\n        \"b23\": 1,\n        \"b29\": 1,\n        \"b44\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b52\": 0.8,\n        \"b20\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b61\": 0.6,\n        \"b64\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of video synthesis using GANs, focusing on maintaining smooth transitions across frames, handling long durations, and generating new objects over time. The proposed algorithm improves upon StyleGAN-V by incorporating alias-free techniques, temporal shift modules, B-Spline motion representations, and low-rank temporal modulation.\",\n    \n    \"Direct Inspiration\": {\n        \"b29\": 0.9,\n        \"b44\": 0.9,\n        \"b23\": 0.8\n    },\n    \n    \"Indirect Inspiration\": {\n        \"b20\": 0.7,\n        \"b61\": 0.7\n    },\n    \n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b52\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of synthesizing videos using GANs by modeling multi-scale temporal relations. It proposes a novel approach incorporating alias-free techniques, temporal shift modules, B-Spline based motion representations, and a low-rank strategy to address issues of texture sticking, motion jittering, and repeated content in long videos.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b29\": 1.0,\n    \"b44\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b52\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b61\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in video generation using GANs, focusing on modeling multi-scale temporal relations to ensure smooth content transitions across frames. Key improvements include the adoption of alias-free techniques, temporal shift modules, B-Spline based motion representations, and a low-rank strategy.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b29\": 0.8,\n    \"b44\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b61\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b52\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in video generation using GANs, specifically focusing on modeling multi-scale temporal relations across frames to ensure smooth transitions, realistic dynamics, and consistent long-term generation. The proposed solutions involve leveraging alias-free techniques, temporal shift modules, B-Spline based motion representations, and low-rank strategy to mitigate issues such as texture sticking, content jittering, and cyclic repetition.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b29\": 1.0,\n    \"b44\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b61\": 0.8,\n    \"b2\": 0.7,\n    \"b52\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b35\": 0.6,\n    \"b17\": 0.5,\n    \"b41\": 0.5\n  }\n}\n```"], "6233f88c5aee126c0f94b3c4": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of identifying and differentiating various types of cache misses (capacity, conflict, coherency) with minimal overhead. It proposes CachePerf, a novel tool that utilizes a hybrid sampling scheme (PMU-based coarse-grained sampling and fine-grained breakpoint-based sampling) to classify cache misses and identify their origins, including allocator-induced misses.\",\n  \"Direct Inspiration\": [\"b30\", \"b31\", \"b33\", \"b44\"],\n  \"Indirect Inspiration\": [\"b5\", \"b24\", \"b37\"],\n  \"Other Inspiration\": [\"b18\", \"b38\", \"b50\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\"Identifying all types of cache misses with reasonable overhead\", \"Differentiating types of cache misses\", \"Distinguishing allocator-induced cache misses from application-induced ones\"],\n    \"algorithm\": \"CachePerf combines coarse-grained PMU-based sampling with fine-grained breakpoint-based sampling to classify cache misses and differentiate their origins.\"\n  },\n  \"Direct Inspiration\": {\n    \"b30\": 0.9,\n    \"b33\": 0.85,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.75,\n    \"b44\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.65,\n    \"b41\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents CachePerf, a novel profiling tool designed to identify and classify different types of cache misses with low overhead, and differentiate those caused by memory allocators from those caused by the application. Key challenges include choosing appropriate profiling methods, differentiating all types of cache misses, and identifying allocator-induced cache misses.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b31\": 0.9,\n    \"b37\": 0.9,\n    \"b44\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b33\": 0.75,\n    \"b53\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.6,\n    \"b30\": 0.65,\n    \"b24\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper 'CachePerf' addresses challenges related to identifying and reducing different types of cache misses, including capacity, conflict, and coherence misses. It proposes a novel hybrid sampling scheme combining PMU-based coarse-grained sampling and fine-grained breakpoint-based sampling to classify cache misses with reasonable overhead. CachePerf also distinguishes between cache misses caused by applications and those induced by memory allocators, providing detailed reporting to aid in bug fixes.\",\n  \"Direct Inspiration\": [\"b33\", \"b44\", \"b5\", \"b31\", \"b41\"],\n  \"Indirect Inspiration\": [\"b18\", \"b36\", \"b38\", \"b9\", \"b29\"],\n  \"Other Inspiration\": [\"b30\", \"b20\", \"b37\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined are identifying all types of cache misses with low overhead and determining the origin of these misses. The proposed algorithm is a hybrid sampling scheme that combines PMU-based coarse-grained sampling and breakpoint-based fine-grained sampling to classify and attribute cache misses accurately.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b5\": 0.8,\n    \"b31\": 0.8,\n    \"b44\": 0.8,\n    \"b37\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.6,\n    \"b41\": 0.6,\n    \"b20\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.4,\n    \"b29\": 0.4\n  }\n}\n```"], "634967f890e50fcafdb5120a": ["```json\n{\n  \"Summary\": \"The paper aims to improve single-image super-resolution (SR) for real-world applications by extending the ESRGAN framework to handle more complex and practical degradations. The authors propose a high-order degradation model, employ sinc filters for artifact simulation, and enhance the discriminator with a U-Net design and spectral normalization to achieve better visual performance and training stability.\",\n  \"Direct Inspiration\": [\"b51\", \"b42\", \"b56\"],\n  \"Indirect Inspiration\": [\"b8\", \"b13\", \"b27\", \"b3\"],\n  \"Other Inspiration\": [\"b57\", \"b15\", \"b35\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of modeling practical real-world degradations in single image super-resolution (SR). It extends the ESR-GAN framework by introducing a high-order degradation process and incorporating sinc filters to handle complex degradation artifacts. The paper also improves the discriminator design with U-Net architecture and spectral normalization to enhance training stability and local detail enhancement.\",\n  \"Direct Inspiration\": {\n    \"b51\": 1.0,\n    \"b56\": 0.9,\n    \"b42\": 0.85,\n    \"b38\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b57\": 0.7,\n    \"b55\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b53\": 0.55,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of single image super-resolution (SR) with a focus on real-world, complex degradations. The authors propose a high-order degradation model to better simulate real-world degradation processes, incorporating sinc filters to model common artifacts. They also enhance the ESRGAN framework by using a U-Net discriminator with spectral normalization to improve training stability and detail enhancement.\",\n    \"Direct Inspiration\": {\n        \"b51\": 0.9,\n        \"b42\": 0.85,\n        \"b53\": 0.85,\n        \"b38\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b56\": 0.75,\n        \"b13\": 0.7,\n        \"b27\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b57\": 0.65,\n        \"b55\": 0.6,\n        \"b11\": 0.6,\n        \"b47\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the restoration of low-resolution images suffering from unknown and complex degradations, which are not well handled by existing super-resolution methods that typically assume ideal bicubic downsampling. The authors propose a high-order degradation process model for practical degradations, incorporate sinc filters to simulate common artifacts, and make several network improvements including the use of a U-Net discriminator with spectral normalization to enhance discriminator capability and stabilize training dynamics.\",\n  \"Direct Inspiration\": {\n    \"b51\": 1.0,\n    \"b56\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.8,\n    \"b53\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b27\": 0.7,\n    \"b3\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper include handling real-world degradations in single image super-resolution (SR), which are complex and cannot be well-modeled by simple classical degradation models. The paper focuses on extending the ESR-GAN to handle these complex degradations by proposing a high-order degradation model and incorporating sinc filters for better artifact simulation.\",\n    \"inspirations\": \"The paper is inspired by previous work on ESR-GAN and aims to improve upon it by addressing its limitations in handling complex real-world degradations.\"\n  },\n  \"Direct Inspiration\": {\n    \"b51\": 1,\n    \"b13\": 0.9,\n    \"b42\": 0.8,\n    \"b53\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b56\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```"], "6287493f5aee126c0ffedf11": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of Entity Disambiguation (ED) in Natural Language Processing (NLP), specifically focusing on the limitations of existing multi-label classification and auto-regressive approaches. The authors propose a new method called Extractive Entity Disambiguation (EXTEND) that frames ED as a text extraction task, leveraging Transformer-based architectures to improve data efficiency and performance, especially on out-of-domain datasets.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b22\": 0.6,\n    \"b33\": 0.6,\n    \"b36\": 0.5,\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is improving Entity Disambiguation (ED) by framing it as a text extraction task, which aims to overcome the limitations of previous multi-label classification and auto-regressive approaches. The proposed method, EXTEND, leverages two Transformer-based architectures to achieve better performance and data efficiency.\",\n  \"Direct Inspiration\": [\"b40\"],\n  \"Indirect Inspiration\": [\"b20\", \"b36\", \"b26\", \"b2\"],\n  \"Other Inspiration\": [\"b19\", \"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency and limitations of current Entity Disambiguation (ED) methods, particularly those that frame ED as a multi-label classification problem. The proposed algorithm, EXTEND, reimagines ED as a text extraction task to better model mention-entity interactions and reduce computational overhead. The paper introduces two Transformer-based architectures to implement this approach and demonstrates their effectiveness on standard benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1.0,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b22\": 0.7,\n    \"b39\": 0.6,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b26\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Entity Disambiguation (ED) in Natural Language Processing (NLP), highlighting the limitations of current approaches such as high memory costs and lack of complex mention-entity interactions. The proposed method, EXTEND, frames ED as a text extraction task and implements two Transformer-based architectures to achieve better data efficiency and performance across various benchmarks.\",\n  \"Direct Inspiration\": [\n    \"b40\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b2\",\n    \"b19\",\n    \"b22\",\n    \"b24\",\n    \"b25\",\n    \"b28\",\n    \"b33\",\n    \"b35\"\n  ],\n  \"Other Inspiration\": [\n    \"b6\",\n    \"b7\",\n    \"b20\",\n    \"b26\",\n    \"b32\",\n    \"b36\",\n    \"b39\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the task of Entity Disambiguation (ED) in Natural Language Processing (NLP), which involves identifying the correct entity for a given mention from a set of candidates. Key challenges include capturing complex mention-entity interactions and reducing computational costs associated with high-dimensional entity representations. The authors propose a novel approach called Extractive Entity Disambiguation (EXTEND) that frames ED as a text extraction task. They evaluate their approach using Transformer-based architectures and demonstrate superior performance over existing methods, particularly when scaling out of domain.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1,\n    \"b22\": 0.9,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b39\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.85,\n    \"b24\": 0.8\n  }\n}\n```"], "62286c865aee126c0fa71627": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of mixed-cell-height standard cell legalization, focusing on minimizing both average and maximum displacement while adhering to complex design rules and constraints. The proposed negotiation-based legalizer (NBLG) is designed to efficiently handle large-scale resource allocation tasks and improve solution quality in mixed-cell-height circuits.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0,\n        \"b5\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b7\": 0.8,\n        \"b8\": 0.8,\n        \"b22\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.7,\n        \"b11\": 0.7,\n        \"b12\": 0.7,\n        \"b13\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the complex problem of mixed-cell-height standard cell legalization in advanced technology nodes. It proposes a novel negotiation-based legalizer (NBLG) that minimizes both average and maximum cell displacement while considering various constraints such as fence region and technology constraints. The algorithm is designed to efficiently handle large-scale resource allocation tasks by utilizing adaptive penalty functions, multithreading techniques, and other optimization strategies.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b22\": 0.7,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b14\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the mixed-cell-height standard cell legalization problem at 14-nm and smaller technology nodes. The challenges include handling complicated design rules, minimizing displacement, and dealing with overlaps caused by multirow-height cells. The proposed solution is a robust negotiation-based legalizer (NBLG) that reformulates the problem as a resource allocation task and uses adaptive penalty functions, isolation points, and multithreading to achieve high-quality solutions.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b22\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the mixed-cell-height standard cell legalization problem, which involves minimizing cell displacement while respecting complicated design rules and constraints at advanced technology nodes. It proposes a robust negotiation-based legalizer (NBLG) that reformulates the problem as a resource allocation task and uses a negotiation-based method to solve it efficiently.\",\n  \"Direct Inspiration\": [\"b4\"],\n  \"Indirect Inspiration\": [\"b5\", \"b6\", \"b8\", \"b9\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of mixed-cell-height standard cell legalization in advanced technology nodes. The primary contributions include a robust negotiation-based legalizer (NBLG) to minimize cell displacement considering fence region and technology constraints, which is a novel approach reformulating the problem as a resource allocation task.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0,\n        \"b5\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b8\": 0.8,\n        \"b22\": 0.7,\n        \"b9\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.6,\n        \"b11\": 0.6,\n        \"b12\": 0.6,\n        \"b13\": 0.6,\n        \"b15\": 0.6,\n        \"b18\": 0.5,\n        \"b20\": 0.5\n    }\n}\n```"]}