{"61dbf1dcd18a2b6e00d9f311": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in graph representation learning, particularly in improving the expressive power of graph embeddings through unsupervised or self-supervised methods. It proposes the AutoProNE framework, which automatically selects graph filters for smoothing graph representations using AutoML. The framework is inspired by recent advancements in self-supervised learning and novel graph filters, aiming to enhance graph embeddings without manual filter design.\",\n  \"Direct Inspiration\": [\"b14\", \"b15\"],\n  \"Indirect Inspiration\": [\"b3\", \"b10\"],\n  \"Other Inspiration\": [\"b9\", \"b12\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in graph representation learning, focusing on enhancing the expressive power of embeddings through optimized propagation techniques. Specifically, it proposes an automated framework, AutoProNE, to select the best graph filters for smoothing embeddings in an unsupervised manner, leveraging advancements in self-supervised learning and AutoML.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving graph representation learning through unsupervised/self-supervised methods, focusing on enhancing the propagation step with automated graph filters. The AutoProNE framework is proposed to automatically select the best graph filters using AutoML, thereby improving the expressive power of learned graph representations.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing graph representation learning through automatic selection of graph filters in a self-supervised manner. Inspired by simplified graph convolutional models (SGC) and the ProNE model, the proposed AutoProNE framework aims to improve the expressive power of graph embeddings by focusing on the propagation and smoothing steps. The framework uses AutoML to automatically select the best graph filters for each specific graph dataset.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of improving graph representation learning by focusing on the propagation step, aiming to enhance the expressive power of embeddings in an unsupervised or self-supervised manner. The proposed AutoProNE framework leverages AutoML to automatically select appropriate graph filters for different datasets, avoiding manual design. The method is inspired by simplifying GCNs and the use of Gaussian filters in ProNE, and seeks to provide a general approach to representation smoothing.\",\n    \"Direct Inspiration\": {\n        \"b14\": 1,\n        \"b15\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.8,\n        \"b9\": 0.7,\n        \"b10\": 0.7,\n        \"b11\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.6,\n        \"b13\": 0.6\n    }\n}\n```"], "61dbf06b6750f87b50ecd224": ["```json\n{\n  \"Summary\": \"The paper addresses the incompleteness challenge in Knowledge Graphs (KGs) and proposes a novel framework 'CogKR' inspired by the dual-process theory from cognitive science. This framework aims to improve multi-hop KG reasoning by leveraging a cognitive graph that combines iterative processes for information retrieval and reasoning.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b24\": 0.9,\n    \"b25\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incompleteness in Knowledge Graphs (KGs) and proposes a novel framework called CogKR for multi-hop KG reasoning. This framework is inspired by the dual-process theory in cognitive science and combines two iterative processes: retrieving relevant information and reasoning over it. The core contribution is the cognitive graph, a subgraph of the KG that expands iteratively during the reasoning process.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b24\": 1.0,\n    \"b25\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b15\": 0.8,\n    \"b20\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b14\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the incompleteness of Knowledge Graphs (KGs), which limits the accuracy of downstream tasks. The authors propose a novel framework called CogKR, inspired by the dual-process theory from cognitive science, to efficiently conduct multi-hop KG reasoning over subgraphs. This framework combines two iterative processes, one to expand the subgraph with relevant entities and edges (System 1), and another to conduct relational reasoning based on the subgraph (System 2).\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.95,\n    \"b24\": 0.95,\n    \"b25\": 0.95,\n    \"b26\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.85,\n    \"b15\": 0.85,\n    \"b16\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.70,\n    \"b22\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incompleteness in knowledge graphs (KGs) and proposes a novel framework, CogKR, inspired by the dual-process theory in cognitive science. This framework aims to improve multi-hop KG reasoning by efficiently conducting reasoning over subgraphs, which are iteratively expanded and reasoned over using two processes resembling human cognitive systems.\",\n  \"Direct Inspiration\": [\"b23\", \"b24\", \"b25\", \"b26\"],\n  \"Indirect Inspiration\": [\"b9\", \"b15\", \"b16\", \"b20\", \"b21\"],\n  \"Other Inspiration\": [\"b28\", \"b62\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incompleteness in Knowledge Graphs (KGs) and proposes a novel framework, CogKR, inspired by the dual-process theory of human cognition. This framework aims to improve multi-hop KG reasoning by using a cognitive graph structure, which iteratively expands and reasons over subgraphs of the KG.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b24\": 0.9,\n    \"b25\": 0.9,\n    \"b26\": 0.9,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b17\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```"], "6042187291e0115d09aff2a7": ["```json\n{\n    \"Summary\": \"The paper aims to understand user behavior in the WeChat Top Stories service, focusing on 'wow' and click actions. It identifies key challenges, such as the influence of user attributes, correlations between users, and local network structures on user behavior. The proposed solution, DiffuseGNN, integrates user attributes, dyadic correlations, and ego network structures to predict user behavior.\",\n    \"Direct Inspiration\": {\n        \"b30\": 1,\n        \"b43\": 1,\n        \"b49\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b22\": 0.8,\n        \"b37\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.7,\n        \"b25\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in understanding user behavior in the context of WeChat's Top Stories service, specifically focusing on user demographics, dyadic and triadic correlations, and ego network properties. The authors propose a novel algorithm, DiffuseGNN, which models user attributes, cleans ego networks, and incorporates graph attention mechanisms to improve prediction of user behavior.\",\n    \"Direct Inspiration\": [\"b30\", \"b43\"],\n    \"Indirect Inspiration\": [\"b6\", \"b22\", \"b37\", \"b49\"],\n    \"Other Inspiration\": [\"b25\", \"b33\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in understanding user behavior in online information diffusion, particularly on WeChat's 'Top Stories' feature. It proposes the DiffuseGNN model, which incorporates various factors such as user demographics, dyadic and triadic correlations, and ego network properties to predict user behaviors like 'wow' and click actions.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.9,\n    \"b43\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b49\": 0.8,\n    \"b37\": 0.75,\n    \"b25\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.65,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of understanding user behavior in online information diffusion, specifically focusing on the 'wow' and click behaviors in WeChat's Top Stories service. It explores how user demographics, dyadic and triadic correlations, and ego network properties influence these behaviors. The authors propose a hierarchical graph representation learning model called DiffuseGNN to predict user behavior based on these factors.\",\n    \"Direct Inspiration\": {\n        \"b30\": 0.9,\n        \"b43\": 0.85,\n        \"b6\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b49\": 0.75,\n        \"b37\": 0.7,\n        \"b25\": 0.65,\n        \"b22\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b41\": 0.55,\n        \"b40\": 0.5,\n        \"b27\": 0.45,\n        \"b8\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting user behavior in social networks, specifically in the context of WeChat's 'Top Stories' service. It explores how user demographics, dyadic and triadic correlations, and ego network properties influence user behavior. Inspired by previous work, the authors propose a hierarchical graph representation learning model named DiffuseGNN, which integrates user attributes, dyadic correlations, and ego network structures to predict user actions such as 'wow' and click behavior.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.9,\n    \"b43\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b49\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b12\": 0.6,\n    \"b37\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```"], "60c31feb6750f85387887e7c": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is improving the quality of negative samples in GNN-based recommender systems. The authors propose the MixGCF framework, which synthesizes negative samples through positive mixing and hop mixing, leveraging data augmentation and metric learning principles.\",\n  \"Direct Inspiration\": [\"b16\", \"b47\", \"b48\"],\n  \"Indirect Inspiration\": [\"b30\", \"b51\"],\n  \"Other Inspiration\": [\"b12\", \"b42\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving negative sampling for GNN-based recommender systems by proposing the MixGCF framework. MixGCF synthesizes negative samples through positive mixing and hop mixing techniques to enhance training efficacy.\",\n    \"Direct Inspiration\": {\n        \"b16\": 0.9,\n        \"b48\": 0.85,\n        \"b47\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.75,\n        \"b42\": 0.75,\n        \"b30\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b27\": 0.6,\n        \"b51\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to improve negative sampling strategies for training GNN-based recommender systems. The proposed algorithm, MixGCF, synthesizes negative samples using a combination of data augmentation and metric learning techniques. The paper introduces two strategies: positive mixing and hop mixing, to generate hard negative samples that enhance the performance of GNN-based recommenders.\",\n  \"Direct Inspiration\": {\n    \"b48\": 1.0,\n    \"b47\": 0.9,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b42\": 0.7,\n    \"b30\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving negative sampling in GNN-based recommender systems to enhance their performance. It introduces the MixGCF framework, which synthesizes hard negative samples using positive mixing and hop mixing strategies, inspired by data augmentation and metric learning.\",\n    \"Direct Inspiration\": {\n        \"b16\": 0.9,\n        \"b48\": 0.95,\n        \"b47\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.8,\n        \"b42\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b30\": 0.7,\n        \"b49\": 0.65,\n        \"b50\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of negative sampling in GNN-based recommender systems, proposing the MixGCF framework that generates synthetic hard negative samples through positive mixing and hop mixing strategies. This approach aims to improve the performance of GNN-based recommender systems by better distinguishing positive and negative samples.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b48\": 1,\n    \"b47\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b42\": 0.8,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b10\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```"], "5ff8844791e011c832676679": ["```json\n{\n  \"Summary\": \"The paper addresses limitations of Chernoff bounds in real-world machine learning problems involving matrix-valued random variables and Markov dependence. It proposes a Markov Chain Matrix Chernoff Bound and establishes large deviation bounds for the tail probabilities of the extreme eigenvalues of sums of random matrices sampled via a regular Markov chain. The paper also studies the sample efficiency of co-occurrence matrix estimation for regular finite Markov chains.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b9\": 0.95,\n    \"b52\": 0.9,\n    \"b37\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.8,\n    \"b48\": 0.75,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b10\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of Chernoff bounds in the context of matrix-valued random variables and Markov dependence. It proposes a new Markov Chain Matrix Chernoff Bound and provides convergence rate analysis for co-occurrence matrix estimation. The authors are motivated by the limitations of existing Chernoff bounds and the need for improved sample complexity analysis in machine learning applications.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.8,\n    \"b38\": 0.8,\n    \"b32\": 0.7,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges: the dependence of random variables, particularly Markov dependence, and the need to extend Chernoff bounds to matrix-valued random variables. The paper proposes large deviation bounds for the tail probabilities of extreme eigenvalues of sums of random matrices sampled via a regular Markov chain. This work significantly improves previous results by Garg et al. [b9] and extends the study of co-occurrence statistics in machine learning.\",\n  \"Direct Inspiration\": [\"b9\", \"b5\"],\n  \"Indirect Inspiration\": [\"b37\", \"b38\", \"b32\", \"b48\"],\n  \"Other Inspiration\": [\"b11\", \"b12\", \"b3\", \"b25\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges: the limitations of applying Chernoff bounds to dependent random variables and matrix-valued random variables in real-world machine learning problems. The authors propose large deviation bounds for the extreme eigenvalues of sums of random matrices sampled via a regular Markov chain, improving on previous results.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b5\": 0.9,\n    \"b37\": 0.8,\n    \"b38\": 0.8,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.6,\n    \"b48\": 0.6,\n    \"b6\": 0.5,\n    \"b21\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.4,\n    \"b29\": 0.4,\n    \"b3\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in extending Chernoff bounds: 1) dealing with dependence in random variables, particularly in Markov chains, and 2) extending the bounds to matrix-valued random variables. The proposed solution establishes large deviation bounds for the tail probabilities of the extreme eigenvalues of sums of random matrices sampled via a regular Markov chain. The paper also provides a sample complexity analysis for estimating co-occurrence matrices from random walks on graphs.\",\n  \"Direct Inspiration\": [\"b9\", \"b5\"],\n  \"Indirect Inspiration\": [\"b37\", \"b38\", \"b48\"],\n  \"Other Inspiration\": [\"b32\", \"b3\"]\n}\n```"], "60b9a4ebe4510cd7c8fc6b77": ["```json\n{\n  \"Summary\": \"The paper focuses on anomaly detection using graph neural networks (GNNs) and explores decoupled training versus joint training. The authors address the inconsistency between behavior patterns and label semantics, introducing Deep Cluster Infomax (DCI) as a novel self-supervised learning scheme to improve representation learning in highly inconsistent data.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b28\": 1,\n    \"b45\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b19\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b53\": 0.7,\n    \"b35\": 0.7,\n    \"b27\": 0.6,\n    \"b46\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of anomaly detection in graph-structured data, specifically focusing on the inconsistency between behavior patterns and label semantics in GNN-based models. Inspired by recent advances in self-supervised learning (SSL), it proposes a novel approach called Deep Cluster Infomax (DCI) to decouple representation learning and classification, aiming to reduce the impact of inconsistency.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b19\": 0.9,\n    \"b32\": 0.9,\n    \"b45\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b28\": 0.7,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b7\": 0.5,\n    \"b20\": 0.5,\n    \"b44\": 0.5,\n    \"b53\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of inconsistency between behavior patterns and label semantics in graph neural network (GNN) based anomaly detection, particularly focusing on detecting abnormal users in graph-structured data. The authors propose a novel method, Deep Cluster Infomax (DCI), which decouples representation learning and classification to better handle this inconsistency by clustering normal patterns within clusters and encoding these patterns in a self-supervised manner.\",\n    \"Direct Inspiration\": {\n        \"b45\": 1.0,\n        \"b28\": 0.9,\n        \"b8\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b9\": 0.8,\n        \"b32\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b53\": 0.7,\n        \"b35\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inconsistency between behavior patterns and label semantics in GNN-based anomaly detection. It proposes decoupling representation learning and classification, inspired by self-supervised learning techniques, to alleviate the impact of this inconsistency. The proposed method, Deep Cluster Infomax (DCI), aims to improve anomaly detection by incorporating a clustering step to better represent diverse behavior patterns.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b45\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b28\": 0.7,\n    \"b19\": 0.6,\n    \"b32\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of anomaly detection in graph-structured data, specifically focusing on the inconsistency between behavior patterns and label semantics in GNN-based models. The proposed solution is a novel graph SSL scheme, Deep Cluster Infomax (DCI), which improves upon Deep Graph Infomax (DGI) by incorporating a clustering step to handle diverse behavior patterns.\",\n  \"Direct Inspiration\": [\"b9\", \"b10\", \"b19\", \"b32\", \"b45\"],\n  \"Indirect Inspiration\": [\"b8\", \"b28\", \"b35\"],\n  \"Other Inspiration\": [\"b20\", \"b53\", \"b44\", \"b27\", \"b46\"]\n}\n```"], "6180ac435244ab9dcb793a8f": ["```json\n{\n  \"Summary\": \"The paper addresses the issue of graph-agnostic feature extraction in standard GNN pipelines and proposes the GIANT self-supervised framework to resolve this problem. It introduces a new approach called neighborhood prediction for numerical feature extraction, which leverages graph information. The paper establishes the connection between neighborhood prediction and the XMC problem, and demonstrates the effectiveness of GIANT through extensive experiments, showing significant improvements in GNN performance on downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b64\": 1.0,\n    \"b5\": 0.9,\n    \"b4\": 0.9,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b57\": 0.7,\n    \"b17\": 0.7,\n    \"b49\": 0.7,\n    \"b23\": 0.7,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.6,\n    \"b60\": 0.6,\n    \"b15\": 0.6,\n    \"b65\": 0.6,\n    \"b51\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the issue of graph-agnostic numerical feature extraction in standard GNN pipelines, particularly for raw text data. The proposed solution, GIANT, is a self-supervised learning (SSL) framework that integrates graph topology information into language models like BERT. A novel SSL task termed neighborhood prediction is introduced, which aligns with the eXtreme Multi-label Classification (XMC) problem. The method is validated through extensive experiments, demonstrating significant performance improvements in node classification tasks.\",\n    \"Direct Inspiration\": {\n        \"b64\": 1.0,\n        \"b8\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.8,\n        \"b57\": 0.8,\n        \"b17\": 0.8,\n        \"b49\": 0.8,\n        \"b23\": 0.7,\n        \"b6\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.6,\n        \"b45\": 0.6,\n        \"b60\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of graph-agnostic feature extraction in standard Graph Neural Network (GNN) pipelines. It proposes a self-supervised learning (SSL) framework called GIANT, which integrates graph topology information into language models like BERT through a novel SSL task termed neighborhood prediction. This method is scalable and improves the performance of GNNs on downstream tasks significantly.\",\n  \"Direct Inspiration\": {\n    \"b49\": 0.9,\n    \"b64\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b6\": 0.85,\n    \"b57\": 0.75,\n    \"b45\": 0.7,\n    \"b60\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.65,\n    \"b5\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies the issue of graph-agnostic feature extraction in standard GNN pipelines and proposes the GIANT framework. GIANT uses self-supervised learning (SSL) to generate numerical node features with graph-structured self-supervision and introduces neighborhood prediction to integrate graph topology information into language models like BERT.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b64\": 1,\n    \"b65\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b57\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b49\": 0.7,\n    \"b6\": 0.7,\n    \"b4\": 0.6,\n    \"b23\": 0.6,\n    \"b45\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of graph-agnostic numerical feature extraction in standard Graph Neural Network (GNN) pipelines. It proposes GIANT, a self-supervised learning (SSL) framework that integrates graph topology information into language models to generate numerical node features using a novel SSL task called neighborhood prediction. The framework leverages XR-Transformers to solve the Extreme Multi-label Classification (XMC) problem, resulting in improved performance on downstream tasks and state-of-the-art results on the Open Graph Benchmark (OGB) leaderboard.\",\n  \"Direct Inspiration\": {\n    \"b64\": 0.9,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b57\": 0.7,\n    \"b6\": 0.6,\n    \"b45\": 0.6,\n    \"b60\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b65\": 0.5,\n    \"b51\": 0.5,\n    \"b15\": 0.5,\n    \"b5\": 0.5,\n    \"b2\": 0.5\n  }\n}\n```"], "60c33c6a91e01104fa0ef733": ["```json\n{\n    \"Summary\": \"The paper addresses performance limitations in Forwardflow architecture, particularly focusing on token throughput and dependency chains. It proposes Omegaflow, which introduces three techniques: parallel DQ bank to improve token consumption, new interconnection networks for better token communication, and Writeback on Completion to shorten dependency chains.\",\n    \"Direct Inspiration\": {\n        \"b11\": 0.95,\n        \"b26\": 0.90\n    },\n    \"Indirect Inspiration\": {\n        \"b25\": 0.85,\n        \"b28\": 0.80\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.75,\n        \"b35\": 0.70,\n        \"b7\": 0.65\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the performance limitations of the Forwardflow architecture, specifically the token throughput and the total number of tokens, which throttle application performance. It introduces the Omegaflow architecture with three key techniques: parallel DQ banks, new interconnection networks, and Writeback on Completion mechanism to enhance performance and energy efficiency.\",\n    \"Direct Inspiration\": {\n        \"b29\": 1,\n        \"b35\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.9,\n        \"b26\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.8,\n        \"b25\": 0.8,\n        \"b36\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the performance limitations of the Forwardflow architecture, specifically focusing on the token throughput and the total number of tokens as key factors throttling application performance. To address these challenges, the paper proposes the Omegaflow architecture, which introduces three new techniques: the parallel DQ bank to improve token consumption rate, new interconnection networks to enhance bandwidth for token communication, and the Writeback on Completion mechanism to reduce the total number of tokens.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.95,\n    \"b25\": 0.8,\n    \"b28\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.6,\n    \"b11\": 0.65,\n    \"b26\": 0.65,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.55,\n    \"b16\": 0.55,\n    \"b27\": 0.55,\n    \"b29\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on improving the performance of Forwardflow architecture by addressing key performance throttles, namely token throughput and the total number of tokens. The proposed Omegaflow architecture introduces three techniques: parallel DQ bank to improve token consumption rate, new interconnection networks for better token communication, and Writeback on Completion mechanism to reduce the total number of tokens.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b25\": 0.7,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b16\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies performance bottlenecks in the Forwardflow architecture, specifically focusing on token throughput and the total number of tokens as primary limiting factors. It introduces Omegaflow, which includes three new techniques: parallel DQ banks, improved interconnection networks, and a Writeback on Completion mechanism to address these issues.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b26\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b25\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b0\": 0.5\n  }\n}\n```"], "60d996c80abde95dc965f5c0": ["```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in pre-training Graph Neural Networks (GNNs) on large-scale heterogeneous graphs: capturing semantic and structural properties and achieving efficient pre-training. The authors propose the PT-HGNN framework, which includes a contrastive pre-training strategy inspired by contrastive learning and a relation-based personalized PageRank strategy for edge sparsification.\",\n  \"Direct Inspiration\": [\"b3\", \"b5\", \"b9\", \"b10\", \"b11\", \"b24\", \"b33\"],\n  \"Indirect Inspiration\": [\"b16\", \"b28\"],\n  \"Other Inspiration\": [\"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in pre-training graph neural networks (GNNs) on large-scale heterogeneous graphs: capturing semantic and structural properties during pre-training, and efficiently pre-training on large-scale graphs. The proposed PT-HGNN framework introduces a contrastive pre-training strategy inspired by contrastive learning and a relation-based personalized PageRank for edge sparsification.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b5\": 0.9,\n    \"b9\": 0.9,\n    \"b10\": 0.95,\n    \"b11\": 0.95,\n    \"b24\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b12\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b14\": 0.7,\n    \"b26\": 0.7,\n    \"b16\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper first outlines the challenges of pre-training Graph Neural Networks (GNNs) on large-scale heterogeneous graphs, specifically capturing semantic and structural properties and ensuring scalability. The proposed PT-HGNN framework addresses these challenges by leveraging contrastive learning and relation-based personalized PageRank for edge sparsification.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b5\": 1,\n    \"b9\": 1,\n    \"b11\": 1,\n    \"b24\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b15\": 0.8,\n    \"b26\": 0.8,\n    \"b28\": 0.8,\n    \"b33\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b6\": 0.6,\n    \"b8\": 0.6,\n    \"b12\": 0.6,\n    \"b16\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of pre-training Graph Neural Networks (GNNs) on large-scale heterogeneous graphs. The main challenges are capturing the semantic and structural properties of heterogeneous graphs and efficiently pre-training GNNs on large graphs. The proposed PT-HGNN framework introduces a contrastive pre-training strategy and a relation-based personalized PageRank for edge sparsification.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b16\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b24\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b5\": 0.6,\n    \"b9\": 0.6,\n    \"b13\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges: capturing semantic and structural properties in heterogeneous graphs during GNN pre-training, and efficiently pre-training GNNs on large-scale heterogeneous graphs. The authors propose a novel PT-HGNN framework that includes a contrastive pre-training strategy at both node and schema levels, and a relation-based personalized PageRank for edge sparsification.\",\n  \"Direct Inspiration\": [\"b10\", \"b11\", \"b24\", \"b33\", \"b16\", \"b28\"],\n  \"Indirect Inspiration\": [\"b3\", \"b5\", \"b9\", \"b15\"],\n  \"Other Inspiration\": [\"b8\", \"b14\", \"b25\", \"b26\"]\n}\n```"], "60c402f491e011d44febefa7": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting the Transformer architecture, originally designed for sequence modeling, to graph representation learning. To achieve this, the authors develop Graphormer, incorporating three key structural encoding methods: Centrality Encoding, Spatial Encoding, and Edge Encoding. The aim is to accurately capture the structural information of graphs and achieve state-of-the-art performance on various graph-level prediction tasks.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1,\n    \"b21\": 0.9,\n    \"b22\": 0.85,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.75,\n    \"b49\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.65,\n    \"b43\": 0.65,\n    \"b47\": 0.6,\n    \"b34\": 0.6,\n    \"b30\": 0.6,\n    \"b15\": 0.6,\n    \"b18\": 0.55,\n    \"b38\": 0.55,\n    \"b37\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the effective incorporation of structural information of graphs into the Transformer model to achieve state-of-the-art performance in graph representation learning tasks. The paper proposes Graphormer, which introduces novel methods like Centrality Encoding, Spatial Encoding, and Edge Encoding to address these challenges.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1,\n    \"b21\": 0.9,\n    \"b22\": 0.9,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.8,\n    \"b47\": 0.8,\n    \"b49\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b35\": 0.7,\n    \"b6\": 0.7,\n    \"b17\": 0.7,\n    \"b12\": 0.7,\n    \"b36\": 0.7,\n    \"b19\": 0.7,\n    \"b56\": 0.7,\n    \"b40\": 0.7,\n    \"b7\": 0.7,\n    \"b23\": 0.7,\n    \"b55\": 0.7,\n    \"b42\": 0.7,\n    \"b13\": 0.7,\n    \"b26\": 0.7,\n    \"b18\": 0.7,\n    \"b41\": 0.7,\n    \"b43\": 0.7,\n    \"b15\": 0.7,\n    \"b30\": 0.7,\n    \"b34\": 0.7,\n    \"b38\": 0.7,\n    \"b37\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is how to effectively adapt the Transformer architecture, originally designed for sequence modeling, to graph representation learning. The paper introduces Graphormer, a model incorporating several structural encoding methods (centrality, spatial, and edge encodings) to leverage structural information of graphs. Graphormer achieves state-of-the-art performance on various graph-level prediction tasks.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b45\", \"b21\", \"b22\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b46\", \"b47\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b38\", \"b37\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting the Transformer architecture, originally designed for sequence modeling, to graph representation learning. The authors propose Graphormer, which incorporates several structural encoding methods, including Centrality Encoding, Spatial Encoding, and Edge Encoding, to capture the structural information of graphs. This approach demonstrates state-of-the-art performance on various graph-level prediction tasks.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b14\": 0.8,\n    \"b46\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b47\": 0.6,\n    \"b49\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively applying the Transformer architecture to graph representation learning. The main contributions include the development of Graphormer, which incorporates novel Centrality Encoding, Spatial Encoding, and Edge Encoding methods to leverage the structural information of graphs. These innovations enable the Transformer to achieve state-of-the-art performance on various graph-level prediction tasks.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1,\n    \"b46\": 0.9,\n    \"b47\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.75,\n    \"b17\": 0.7,\n    \"b41\": 0.65,\n    \"b43\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.95,\n    \"b22\": 0.85,\n    \"b14\": 0.8\n  }\n}\n```"], "61850e9691e01121084ca0d6": ["```json\n{\n  \"Summary\": \"The paper addresses major challenges in adaptive learning, including insufficient data coverage, coarse concept granularity, and limited data curation. It proposes MOOCCubeX, a large open-access repository for adaptive learning, integrating a vast amount of educational resources and student behavior data. The framework includes data processing, fine-grained concept acquisition, and data curation to create a comprehensive knowledge repository.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b40\": 0.85,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b20\": 0.7,\n    \"b7\": 0.65,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.55,\n    \"b38\": 0.5,\n    \"b21\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of insufficient data coverage, coarse concept granularity, and limited data curation in educational datasets used for adaptive learning. It proposes MOOCCubeX, a large open-access repository that integrates diverse educational resources and student behaviors, aiming to support systematic research on adaptive learning by constructing a fine-grained concept graph using weak supervision.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b28\": 0.9,\n    \"b40\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b20\": 0.8,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in current educational datasets, including insufficient data coverage, coarse concept granularity, and limited data curation. It proposes MOOCCubeX, a large open-access repository for adaptive learning, integrating extensive resources from XuetangX. The framework includes a general method for weakly supervised fine-grained knowledge concept acquisition and organization, aiming to enhance adaptive learning research.\",\n    \"Direct Inspiration\": [\"b1\", \"b40\"],\n    \"Indirect Inspiration\": [\"b7\", \"b28\", \"b30\"],\n    \"Other Inspiration\": [\"b0\", \"b20\", \"b38\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include insufficient data coverage, coarse concept granularity, and limited data curation in existing educational datasets. The proposed algorithm involves constructing a large open-access repository for adaptive learning called MOOCCubeX, which integrates a vast amount of educational resources and student behavior data from the XuetangX platform. The framework employs weakly supervised methods to build a fine-grained concept graph for effective data organization and retrieval.\",\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b1\", \"b6\", \"b7\", \"b17\", \"b22\", \"b23\", \"b28\", \"b40\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include insufficient data coverage, coarse concept granularity, and limited data curation in existing educational datasets. The paper proposes MOOCCubeX, a large open-access repository for adaptive learning, which integrates a vast amount of diverse educational resources and student behaviors. The novel methods include a framework for constructing a fine-grained concept graph via weak supervision and a series of toolkits for the refinement and usage of the repository.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b40\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b20\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b7\": 0.7,\n    \"b30\": 0.6\n  }\n}\n```"], "618ddb455244ab9dcbda8f5f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting masked autoencoding methods from NLP to computer vision, highlighting the architectural differences, information density, and the role of the decoder between the two domains. The proposed method, MAE (Masked Autoencoder), leverages an asymmetric encoder-decoder design to efficiently handle high masking ratios, enabling better generalization and scalability for large models in visual representation learning.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b13\", \"b40\", \"b41\", \"b3\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b15\", \"b49\", \"b39\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b48\", \"b5\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively applying masked autoencoding to computer vision, inspired by the success of similar techniques in NLP. The proposed method, a masked autoencoder (MAE) with an asymmetric encoder-decoder design, aims to overcome differences in information density and semantic representation between images and text. The MAE achieves high performance in image recognition tasks by efficiently reconstructing missing patches and reducing computational load.\",\n  \"Direct Inspiration\": [\"b13\", \"b15\", \"b1\"],\n  \"Indirect Inspiration\": [\"b3\", \"b40\", \"b41\"],\n  \"Other Inspiration\": [\"b49\", \"b39\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in applying masked autoencoding methods, successful in NLP, to computer vision. The authors propose a masked autoencoder (MAE) for visual representation learning, optimized by a high masking ratio and an asymmetric encoder-decoder design. This method aims to reduce redundancy and learn holistic image features, achieving significant improvements in tasks like image recognition and segmentation.\",\n    \"Direct Inspiration\": {\n        \"b13\": 0.95,\n        \"b15\": 0.90\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.80,\n        \"b5\": 0.75,\n        \"b40\": 0.70,\n        \"b41\": 0.70,\n        \"b3\": 0.70\n    },\n    \"Other Inspiration\": {\n        \"b39\": 0.65,\n        \"b49\": 0.65\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of adapting masked autoencoding techniques from NLP to computer vision, proposing a novel Masked Autoencoder (MAE) that incorporates an asymmetric encoder-decoder design. The key contributions include masking a high portion of image patches to reduce redundancy and the efficient handling of these masked patches to improve training efficiency and performance.\",\n    \"Direct Inspiration\": {\n        \"b15\": 0.9,\n        \"b13\": 0.8,\n        \"b1\": 0.7,\n        \"b5\": 0.7,\n        \"b40\": 0.6,\n        \"b41\": 0.6,\n        \"b3\": 0.6\n    },\n    \"Indirect Inspiration\": {\n        \"b49\": 0.5,\n        \"b39\": 0.5,\n        \"b23\": 0.4,\n        \"b12\": 0.4\n    },\n    \"Other Inspiration\": {\n        \"b47\": 0.3,\n        \"b48\": 0.3,\n        \"b28\": 0.3\n    }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"The primary challenges addressed in the paper are the differences in information density between language and vision, and the role of the autoencoder's decoder in reconstructing text versus images. The algorithm proposed by the authors is a masked autoencoder (MAE) for visual representation learning that uses an asymmetric encoder-decoder design to reduce computation and improve efficiency.\",\n        \"inspirations\": \"The paper takes inspiration from masked autoencoders in NLP, denoising autoencoders, and Vision Transformers (ViT).\"\n    },\n    \"Direct Inspiration\": [\n        \"b13\",\n        \"b15\",\n        \"b3\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b40\",\n        \"b41\",\n        \"b49\"\n    ],\n    \"Other Inspiration\": [\n        \"b1\",\n        \"b39\",\n        \"b5\"\n    ]\n}\n```"], "6164fcc15244ab9dcb24cf0b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of handling noisy labels in distantly supervised named entity recognition (DS-NER). It proposes a robust learning framework named Self-Collaborative Denoising Learning (SCDL), which uses two interactive teacher-student networks to address both incomplete and inaccurate annotations. The method aims to fully utilize mislabeled data through inner and outer loops of self and collaborative denoising procedures.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.95,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.85,\n    \"b37\": 0.8,\n    \"b5\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b22\": 0.65,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of noisy labels in Distantly Supervised Named Entity Recognition (DS-NER), specifically incomplete and inaccurate annotations. The proposed algorithm, Self-Collaborative Denoising Learning (SCDL), utilizes two teacher-student networks in a co-training framework to effectively handle label noise and improve data utilization.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.95,\n    \"b27\": 0.9,\n    \"b18\": 0.85,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b37\": 0.7,\n    \"b29\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b34\": 0.55,\n    \"b38\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of noisy labels in distantly supervised named entity recognition (DS-NER), proposing a Self-Collaborative Denoising Learning (SCDL) framework. This framework utilizes two teacher-student networks in inner and outer loops for self and collaborative denoising to rectify unreliable annotations and make full use of the training set.\",\n  \"Direct Inspiration\": [\"b30\", \"b27\", \"b18\", \"b19\"],\n  \"Indirect Inspiration\": [\"b22\", \"b31\", \"b8\"],\n  \"Other Inspiration\": [\"b15\", \"b6\", \"b10\", \"b36\", \"b33\", \"b4\", \"b25\", \"b14\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of noisy annotations in distantly supervised named entity recognition (DS-NER). The proposed solution is a robust learning framework named SCDL (Self-Collaborative Denoising Learning), which co-trains two teacher-student networks to handle both incomplete and inaccurate annotations, and fully utilize the training set.\",\n    \"Direct Inspiration\": [\"b19\", \"b30\"],\n    \"Indirect Inspiration\": [\"b27\", \"b18\", \"b10\"],\n    \"Other Inspiration\": [\"b4\", \"b25\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in Named Entity Recognition (NER), specifically the issues of incomplete and inaccurate annotations in distantly supervised NER (DS-NER). The proposed solution, Self-Collaborative Denoising Learning (SCDL), involves a robust learning framework using inner and outer loops of teacher-student networks to rectify noisy labels and fully utilize the training data.\",\n  \"Direct Inspiration\": [\"b18\", \"b27\", \"b30\"],\n  \"Indirect Inspiration\": [\"b19\", \"b37\"],\n  \"Other Inspiration\": [\"b4\", \"b9\", \"b25\"]\n}\n```"], "60cbeaaf91e011eef576dac1": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying mixup methods in NLP, particularly input-level mixup, due to the discrete nature of text data and variable sequence lengths. The proposed algorithm, SSMix, introduces a spanwise mixup method considering the saliency of spans to synthesize new text samples by replacing spans based on saliency information.\",\n  \"Direct Inspiration\": [\"b26\"],\n  \"Indirect Inspiration\": [\"b27\", \"b9\", \"b20\", \"b4\"],\n  \"Other Inspiration\": [\"b16\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying mixup methods in natural language processing (NLP) given the discrete nature of text data and variable sequence lengths. The authors propose SSMix, a novel input-level mixup method based on saliency information, which replaces spans of text between samples to create new training data. This method is designed to preserve the locality of the original texts and enhance the model's generalization ability.\",\n  \"Direct Inspiration\": [\"b26\", \"b27\"],\n  \"Indirect Inspiration\": [\"b5\", \"b1\", \"b20\"],\n  \"Other Inspiration\": [\"b24\", \"b3\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying mixup for data augmentation in natural language processing (NLP), specifically focusing on input-level mixup which is more complex due to the discrete nature of text data. The authors propose SSMix, a novel input-level spanwise mixup method that considers the saliency of spans, inspired by methods like CutMix. This method has been empirically validated as effective through extensive experiments on multiple text classification benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b20\": 0.7,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b24\": 0.5,\n    \"b3\": 0.5,\n    \"b12\": 0.5,\n    \"b28\": 0.5,\n    \"b14\": 0.5,\n    \"b22\": 0.5,\n    \"b21\": 0.5,\n    \"b16\": 0.5,\n    \"b10\": 0.5,\n    \"b23\": 0.5,\n    \"b11\": 0.5,\n    \"b7\": 0.5,\n    \"b13\": 0.5,\n    \"b17\": 0.5,\n    \"b25\": 0.5,\n    \"b15\": 0.5,\n    \"b0\": 0.5,\n    \"b2\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying mixup methods in natural language processing (NLP) due to the discrete nature of text data and variable sequence lengths. The authors propose SSMix, a novel input-level spanwise mixup method that utilizes saliency information to replace spans of tokens, to achieve better generalization in text classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b1\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b16\": 0.5,\n    \"b10\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying mixup methods in NLP, particularly at the input level, due to the discrete nature and variable sequence lengths of text data. The authors propose SSMix, a novel input-level spanwise mixup method that selects spans based on saliency information to create augmented text data and improve generalization in text classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b1\": 0.5\n  }\n}\n```"], "612c4c285244ab9dcbca22ce": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in collaborative filtering-based recommender systems, particularly the sparsity of user-item rating matrices and the lack of side information. The proposed solution, GLocal-K, utilizes a novel combination of local and global kernel-based autoencoder models to improve feature extraction and prediction accuracy without relying on side information.\",\n  \"Direct Inspiration\": [\"b7\"],\n  \"Indirect Inspiration\": [\"b0\", \"b5\"],\n  \"Other Inspiration\": [\"b1\", \"b2\", \"b6\", \"b8\", \"b9\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving feature extraction performance in collaborative filtering-based recommender systems without relying on side information. It proposes a Global-Local Kernel-based matrix completion framework (GLocal-K) that uses a two-stage process involving pre-training with a local kernelised weight matrix and fine-tuning with a global kernel-based rating matrix.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b0\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b6\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of matrix completion in collaborative filtering-based recommender systems, specifically under the condition of no side information available. It proposes a novel Global-Local Kernel-based autoencoder framework (GLocal-K) that combines local kernelised weight matrices for pre-training and global kernel-based rating matrices for fine-tuning to improve feature extraction and prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b5\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b4\": 0.5,\n    \"b6\": 0.4,\n    \"b8\": 0.3,\n    \"b9\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents a novel Global-Local Kernel-based matrix completion framework (GLocal-K) for recommender systems. It addresses the challenge of sparse user-item rating matrices without relying on side information. The research integrates local and global kernels to enhance feature extraction, pre-training with a local kernel, and fine-tuning with a global kernel to achieve superior performance in matrix completion tasks.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b8\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of matrix completion in collaborative filtering-based recommender systems, focusing on improving feature extraction performance without using side information. The proposed solution, GLocal-K, integrates local and global kernels in an auto-encoder framework to achieve this.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7\n  }\n}\n```"], "618c94bb6750f806be6689f3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving cross-lingual transferability by leveraging data augmentation and consistency regularization. It introduces XTUNE, a method that enhances cross-lingual fine-tuning by enforcing example and model consistency regularization. The method aims to make model predictions more consistent across semantically equivalent augmentations and different augmentation strategies.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1,\n    \"b31\": 0.9,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b0\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b39\": 0.45,\n    \"b25\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving cross-lingual fine-tuning by leveraging data augmentation strategies combined with consistency regularization. The proposed method, XTUNE, introduces two types of consistency regularization: example consistency and model consistency, to enhance the transferability of pre-trained cross-lingual language models.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1,\n    \"b7\": 0.9,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b18\": 0.75,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.65,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces XTUNE, a cross-lingual fine-tuning method enhanced by consistency regularization and data augmentation. The main challenges addressed include improving cross-lingual transferability and effectively leveraging data augmentation. The proposed method introduces example consistency regularization and model consistency regularization to ensure predictions are consistent across semantically equivalent augmentations and different models trained on augmented data. Four data augmentation strategies are studied: subword sampling, code-switch substitution, Gaussian noise, and machine translation. The method is evaluated on the XTREME benchmark and shows improved performance in various tasks such as classification, span extraction, and sequence labeling.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.95,\n    \"b30\": 0.90,\n    \"b0\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.80,\n    \"b31\": 0.75,\n    \"b6\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.65,\n    \"b25\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes XTUNE, a cross-lingual fine-tuning method enhanced by consistency regularization and data augmentation, to improve cross-lingual transferability. The main challenges addressed are the effective utilization of data augmentations and ensuring consistency in model predictions for semantically equivalent examples. The method introduces two types of consistency regularization: example consistency and model consistency, and evaluates the approach using four data augmentation strategies.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b6\": 0.9,\n    \"b30\": 0.85,\n    \"b0\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.75,\n    \"b18\": 0.7,\n    \"b22\": 0.65,\n    \"b17\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving cross-lingual fine-tuning by leveraging data augmentation and consistency regularization. The proposed method, XTUNE, aims to enhance model performance by enforcing consistency at both the example and corpus levels, using various data augmentation strategies.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b30\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b31\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b4\": 0.65,\n    \"b0\": 0.6\n  }\n}\n```"], "607ffb8b91e011772654f6ef": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the issue of noisy labels in supervised learning for entity-centric information extraction (IE) tasks. The proposed solution is a co-regularization framework that uses multiple neural classifiers with identical structures but different initializations. These classifiers are jointly optimized with task-specific losses and an agreement loss that regularizes the models to generate similar predictions to the aggregated soft target. This framework aims to prevent overfitting to noisy labels and improve the performance of IE models without the need for additional clean datasets.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b19\": 0.8,\n    \"b33\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b14\": 0.7,\n    \"b17\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of noisy labels in supervised learning for entity-centric information extraction (IE) tasks. The authors propose a co-regularization framework consisting of multiple neural classifiers with identical structures but different initialization. The framework incorporates an agreement loss to regularize the models to generate similar predictions, thereby preventing overfitting to noisy labels.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1,\n        \"b28\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b33\": 0.8,\n        \"b42\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b27\": 0.6,\n        \"b0\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of developing noise-robust learning methods for entity-centric information extraction (IE) tasks. The proposed co-regularization framework aims to prevent models from overfitting noisy labels by using multiple classifiers with identical structures but different initializations. The framework employs an agreement loss to regularize model predictions, enhancing noise-tolerant training without the need for extra learning resources.\",\n    \"Direct Inspiration\": [\"b1\", \"b28\"],\n    \"Indirect Inspiration\": [\"b33\", \"b17\"],\n    \"Other Inspiration\": [\"b40\", \"b27\", \"b42\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training deep neural models for information extraction (IE) tasks with noisy labels, which can cause significant performance degradation. The authors propose a co-regularization framework that incorporates multiple neural classifiers with different initializations and uses an agreement loss to regularize predictions. This method aims to prevent overfitting to noisy labels by encouraging similar predictions among classifiers.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b28\": 1,\n    \"b33\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b21\": 0.8,\n    \"b27\": 0.8,\n    \"b42\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b19\": 0.6,\n    \"b7\": 0.6,\n    \"b14\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of noisy labels in supervised learning for entity-centric information extraction (IE) tasks. It proposes a co-regularization framework that involves multiple neural classifiers with identical structures but different initializations to handle noisy training labels effectively. The framework is inspired by studies showing that noisy labels have delayed learning curves and are frequently forgotten in later epochs. The proposed method aims to prevent overfitting on noisy labels by using an agreement loss that regularizes the model predictions.\",\n  \"Direct Inspiration\": [\"b1\", \"b28\"],\n  \"Indirect Inspiration\": [\"b33\", \"b27\", \"b42\"],\n  \"Other Inspiration\": [\"b40\", \"b20\", \"b23\"]\n}\n```"], "60757d6d91e0110f6fe6843e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately recognizing facial expressions by considering the subtle differences in expressions, which are often characterized by similar facial actions. The proposed solution is the Feature Decomposition and Reconstruction Learning (FDRL) method, which includes a Feature Decomposition Network (FDN) and a Feature Reconstruction Network (FRN). These networks work together to model expression similarities and expression-specific variations, enabling the extraction of fine-grained expression features.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b27\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the difficulties in distinguishing facial expressions due to high similarities across different expressions and the need for fine-grained expression features. The authors propose a novel Feature Decomposition and Reconstruction Learning (FDRL) method, which includes a Feature Decomposition Network (FDN) and a Feature Reconstruction Network (FRN). These networks are designed to model expression similarities and expression-specific variations, thus enabling the extraction of fine-grained expression features.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b19\": 0.7,\n    \"b23\": 0.7,\n    \"b27\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of distinguishing subtle differences in facial expressions for Facial Expression Recognition (FER) by proposing a novel Feature Decomposition and Reconstruction Learning (FDRL) method. The method decomposes basic features into facial action-aware latent features and models their intra-feature and inter-feature relationships to extract fine-grained and discriminative expression features.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b12\": 0.7,\n    \"b19\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b14\": 0.6,\n    \"b17\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the identification of subtle differences in facial expressions, which is complicated by high similarities across different expressions. The proposed algorithm, Feature Decomposition and Reconstruction Learning (FDRL), addresses this by decomposing basic features into facial action-aware latent features and then reconstructing these features to model expression-specific variations. This method aims to achieve fine-grained expression feature extraction through the use of two networks: Feature Decomposition Network (FDN) and Feature Reconstruction Network (FRN).\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b28\": 0.7,\n    \"b2\": 0.6,\n    \"b12\": 0.6,\n    \"b19\": 0.6,\n    \"b25\": 0.6,\n    \"b14\": 0.5,\n    \"b17\": 0.5,\n    \"b30\": 0.5,\n    \"b4\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.4,\n    \"b23\": 0.4,\n    \"b27\": 0.4,\n    \"b22\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of accurately recognizing facial expressions by learning fine-grained expression features that identify subtle differences between expressions with similar facial actions. The proposed Feature Decomposition and Reconstruction Learning (FDRL) method includes a Feature Decomposition Network (FDN) and a Feature Reconstruction Network (FRN), which work together to model expression similarities and expression-specific variations.\",\n    \"Direct Inspiration\": {\n        \"References\": [\"b24\"]\n    },\n    \"Indirect Inspiration\": {\n        \"References\": [\"b21\", \"b19\", \"b23\", \"b27\", \"b22\"]\n    },\n    \"Other Inspiration\": {\n        \"References\": [\"b12\", \"b25\"]\n    }\n}\n```"], "6103d7ba91e01159791b21df": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of detecting fake news on social media by proposing a novel Multimodal Co-Attention Networks (MCAN) model. The key contributions include the development of an end-to-end approach using text and images without extra information, stacking multiple co-attention layers for inter-modality fusion, and demonstrating the model's effectiveness on large-scale datasets.\",\n    \"Direct Inspiration\": {\n        \"b8\": 0.9,\n        \"b28\": 0.85,\n        \"b10\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b20\": 0.75,\n        \"b29\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.65,\n        \"b23\": 0.6,\n        \"b5\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting fake news on social media by proposing a novel Multimodal Co-Attention Network (MCAN) that integrates spatial-domain, frequency-domain, and textual features to improve the accuracy of fake news detection. This approach is motivated by the limitations of existing methods that do not adequately fuse multimodal features or consider physical characteristics of images.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b10\": 0.9,\n    \"b28\": 0.9,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b5\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting fake news on social media by combining textual and visual features. The proposed MCAN model uses a novel fusion approach with multiple co-attention layers to learn inter-modality relations, aiming to overcome limitations in feature extraction and fusion of existing methods.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b20\": 0.9,\n    \"b28\": 0.85,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.7,\n    \"b23\": 0.65,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting fake news on social media, particularly focusing on multimodal approaches that combine textual and visual information. The proposed algorithm, Multimodal Co-Attention Networks (MCAN), extracts spatial-domain and frequency-domain features from images and textual features from text, and then employs a novel fusion approach with multiple co-attention layers to learn inter-modality relations.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b28\": 0.85,\n    \"b10\": 0.8,\n    \"b20\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b9\": 0.65,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b24\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on addressing the challenge of detecting fake news on social media using both textual and image data. The proposed solution, MCAN (Multimodal Co-Attention Networks), extracts spatial-domain and frequency-domain features from images and textual features from text, and fuses them through multiple co-attention layers to learn inter-modality relations. This approach aims to overcome the limitations of previous methods that required additional information and were not fine-grained in feature extraction and fusion.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b28\": 1.0,\n    \"b10\": 1.0,\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```"], "618c94976750f806be6689d2": ["```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include the effective detection of fake news through textual content analysis, avoiding feature engineering, and fully leveraging external knowledge bases (KB). The paper proposes a novel end-to-end graph neural model named CompareNet, which directly compares news entities to KB entities for fake news detection. The model incorporates topics to enrich news representations and uses a heterogeneous document graph to capture interactions among sentences, topics, and entities. A heterogeneous graph attention network is developed to learn topic-enriched news representations and contextual entity representations, which are then compared with KB-based entity representations for fake news detection.\",\n    \"Direct Inspiration\": {\n        \"b13\": 0.9,\n        \"b24\": 0.85,\n        \"b29\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.75,\n        \"b26\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.65,\n        \"b17\": 0.6,\n        \"b7\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting fake news by leveraging external knowledge bases (KB) and incorporating topic information to improve accuracy. The proposed model, CompareNet, constructs a directed heterogeneous document graph and employs a heterogeneous graph attention network to learn representations that are compared with KB-based entity representations. The goal is to capture semantic consistency between news content and external KB for effective fake news classification.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b13\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.75,\n    \"b12\": 0.7,\n    \"b8\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b3\": 0.55,\n    \"b1\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of early fake news detection by leveraging external knowledge bases (KB) and proposes a novel end-to-end graph neural model called CompareNet. CompareNet constructs a directed heterogeneous document graph incorporating sentences, topics, and entities, and uses heterogeneous graph attention networks to learn enriched news representations. It then compares these representations with KB-based entity representations to capture semantic consistency for fake news classification.\",\n  \"Direct Inspiration\": [\"b13\", \"b24\"],\n  \"Indirect Inspiration\": [\"b29\", \"b8\"],\n  \"Other Inspiration\": [\"b12\", \"b7\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of early fake news detection by leveraging external knowledge bases (KB) such as Wikipedia. The proposed model, CompareNet, constructs a directed heterogeneous document graph to incorporate sentences, topics, and entities. It uses a heterogeneous graph attention network to learn topic-enriched news representations and contextual entity representations, which are then compared to KB-based entity representations using an entity comparison network. The model aims to improve fake news detection by capturing semantic consistency between the news content and external knowledge.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.8,\n    \"b8\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of detecting fake news by proposing a novel end-to-end graph neural model called CompareNet. This model leverages external knowledge bases (KB) and incorporates topic information to enrich news representations. The primary contributions include the construction of a directed heterogeneous document graph, the development of a heterogeneous graph attention network, and the design of an entity comparison network to compare news content with KB-based entity representations.\",\n    \"Direct Inspiration\": {\n        \"b24\": 0.9,\n        \"b13\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b29\": 0.8,\n        \"b8\": 0.75,\n        \"b12\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.65,\n        \"b3\": 0.6\n    }\n}\n```"], "6098feeb91e011aa8bcb6dbf": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of forecasting human motion in crowds, combining the interpretability of discrete choice models (DCM) with the high accuracy of neural network (NN) models. The proposed model outputs a probability distribution over future intents, augmented by scene-specific residuals generated by NNs. This hybrid approach aims to capture complex social interactions and long-term dependencies while maintaining interpretability.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b4\": 0.9,\n    \"b47\": 0.9,\n    \"b52\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.75,\n    \"b29\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b30\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately forecasting human trajectories in public places while maintaining interpretability. It proposes a hybrid framework combining discrete choice models (DCMs) for their interpretability with neural networks (NNs) for their accuracy in capturing complex social interactions and long-term dependencies. The model outputs a probability distribution over a discrete set of future intents, refined with scene-specific residuals generated by an NN.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b4\": 0.9,\n    \"b47\": 0.9,\n    \"b52\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b25\": 0.7,\n    \"b29\": 0.7,\n    \"b63\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately forecasting human trajectories in crowds while maintaining interpretability. It proposes a hybrid model that combines the interpretability of discrete choice models (DCM) with the accuracy of neural network-based predictions. The model outputs a probability distribution over possible future intents, refined by neural network-generated scene-specific residuals.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b4\": 0.9,\n    \"b47\": 0.9,\n    \"b52\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.7,\n    \"b18\": 0.6,\n    \"b25\": 0.6,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately forecasting human trajectories in crowds while maintaining interpretability. It proposes a novel hybrid model that combines discrete choice models (DCM) with neural networks to leverage both interpretability and high prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b4\": 1,\n    \"b47\": 0.9,\n    \"b52\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.7,\n    \"b18\": 0.6,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of forecasting human motion in public spaces by combining the interpretability of discrete choice models (DCM) with the high accuracy of neural network-based models. The proposed model outputs a probability distribution over a discrete set of possible future intents, refined by scene-specific residuals generated by a neural network.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b4\": 1,\n    \"b47\": 1,\n    \"b52\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b25\": 0.7,\n    \"b29\": 0.9,\n    \"b61\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b63\": 0.4\n  }\n}\n```"], "60f2b1d05244ab9dcbbbdfe7": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting protein structures with atomic accuracy when no homologous structure is available. It introduces an advanced neural network model, AlphaFold, which leverages physical, biological, and geometric knowledge of protein structures to achieve near-experimental accuracy. The approach incorporates multi-sequence alignments and novel neural network architectures, validated through CASP14 assessment.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b22\": 0.9,\n    \"b23\": 0.9,\n    \"b9\": 0.8,\n    \"b24\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b26\": 0.6,\n    \"b27\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6,\n    \"b36\": 0.6,\n    \"b37\": 0.6,\n    \"b38\": 0.6,\n    \"b39\": 0.6,\n    \"b40\": 0.6,\n    \"b41\": 0.6,\n    \"b42\": 0.6,\n    \"b43\": 0.6,\n    \"b44\": 0.6,\n    \"b45\": 0.6,\n    \"b46\": 0.6,\n    \"b47\": 0.6,\n    \"b48\": 0.6,\n    \"b49\": 0.6,\n    \"b50\": 0.6,\n    \"b51\": 0.6,\n    \"b52\": 0.6,\n    \"b53\": 0.6,\n    \"b54\": 0.6,\n    \"b55\": 0.6,\n    \"b56\": 0.6,\n    \"b57\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the significant challenge of predicting the three-dimensional structure of proteins based solely on amino acid sequences, a problem that has persisted for over 50 years. The novel method introduced, AlphaFold, leverages a neural network-based model incorporating both physical and biological knowledge about protein structures, significantly improving prediction accuracy to atomic levels, even when no homologous structures are available.\",\n    \"Direct Inspiration\": {\n        \"b9\": 0.9,\n        \"b21\": 0.8,\n        \"b22\": 0.8,\n        \"b23\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.7,\n        \"b20\": 0.7,\n        \"b24\": 0.7,\n        \"b25\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.6,\n        \"b16\": 0.6,\n        \"b26\": 0.6,\n        \"b27\": 0.6,\n        \"b28\": 0.6,\n        \"b30\": 0.6,\n        \"b31\": 0.6,\n        \"b32\": 0.6,\n        \"b34\": 0.6,\n        \"b35\": 0.6,\n        \"b36\": 0.6,\n        \"b37\": 0.6,\n        \"b38\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of accurately predicting the 3D structure of proteins based solely on their amino acid sequence. The authors introduce a novel machine learning approach, AlphaFold, which incorporates physical and biological knowledge to achieve atomic accuracy in protein structure prediction. AlphaFold's innovations include a new neural network architecture, the Evoformer block, and a structure module that iteratively refines predictions.\",\n    \"Direct Inspiration\": {\n        \"b9\": 1.0,\n        \"b21\": 0.9,\n        \"b22\": 0.9,\n        \"b23\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b13\": 0.8,\n        \"b25\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.7,\n        \"b19\": 0.7,\n        \"b20\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately predicting protein structures computationally, especially in cases where no homologous structure is available. The authors propose a novel machine learning approach, AlphaFold, which incorporates physical and biological knowledge about protein structures and leverages multi-sequence alignments. This method has demonstrated accuracy competitive with experimental structures and greatly outperforms other existing methods.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b10\": 0.9,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b19\": 0.5,\n    \"b27\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting protein structures with atomic accuracy, especially when no homologous structure is available. It introduces a redesigned neural network-based model, AlphaFold, that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b32\": 0.85,\n    \"b34\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b10\": 0.7,\n    \"b21\": 0.65,\n    \"b38\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.55,\n    \"b19\": 0.5,\n    \"b22\": 0.45\n  }\n}\n```"], "607d4e9e91e011bf62020909": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of out-of-distribution (OOD) generalization in machine learning models, which fail to make trustworthy predictions due to distribution shifts. The proposed algorithm, StableNet, tackles the problem by decorrelating relevant and irrelevant features using Random Fourier Features (RFF) and an efficient optimization mechanism, thus improving the generalization ability of models under various challenging settings.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b49\": 1,\n    \"b43\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b48\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7,\n    \"b37\": 0.7,\n    \"b34\": 0.7,\n    \"b1\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of Out-of-Distribution (OOD) generalization by proposing a novel method called StableNet. The main challenges include the difficulty of measuring and eliminating non-linear dependencies among features in deep models and the infeasibility of global sample weighting strategies due to excessive storage and computational costs. The proposed method uses Random Fourier Features (RFF) to decorrelate features and an efficient optimization mechanism to globally remove correlations while minimizing computational costs.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0,\n    \"b49\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.8,\n    \"b2\": 0.7,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.5,\n    \"b16\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on addressing issues associated with distribution shifts in machine learning models, particularly out-of-distribution (OOD) generalization. The authors propose a novel method called StableNet, which employs a nonlinear feature decorrelation approach based on Random Fourier Features (RFF) and an efficient optimization mechanism to improve generalization ability under distribution shifts.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b49\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.7,\n    \"b51\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b24\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of out-of-distribution (OOD) generalization by proposing a method called StableNet, which aims to decorrelate relevant and irrelevant features to improve model generalization under distribution shifts. The method leverages Random Fourier Features (RFF) and introduces a novel sample weighting mechanism to handle nonlinear dependencies and reduce computational cost.\",\n  \"Direct Inspiration\": [\"b28\", \"b49\"],\n  \"Indirect Inspiration\": [\"b24\", \"b33\", \"b39\"],\n  \"Other Inspiration\": [\"b43\", \"b51\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of out-of-distribution (OOD) generalization in machine learning models, particularly under conditions where training and testing data distributions are not identical. The authors propose a novel method called StableNet, which incorporates nonlinear feature decorrelation using Random Fourier Features (RFF) and an efficient optimization mechanism to improve model performance in non-stationary environments.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b49\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.7,\n    \"b48\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```"], "60782d0091e011f5ecc9dc04": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Convolutional Neural Networks (CNNs) with noisy data, proposing a novel method named Joint Negative and Positive Learning (JNPL) which combines two newly developed loss functions, NL+ and PL+, in a unified single-stage pipeline. This method aims to solve the underfitting problem of the Negative Learning (NL) loss function and improve training efficiency compared to the previously proposed NLNL method.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.8,\n    \"b5\": 0.7,\n    \"b29\": 0.7,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b2\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges addressed in the paper are the robust training of Convolutional Neural Networks (CNNs) with noisy data, and the inefficiencies in existing methods like NLNL. The paper proposes a novel method called Joint Negative Learning and Positive Learning (JNPL), which features a unified single-stage pipeline for filtering noisy data. This includes two new loss functions, NL+ and PL+, designed to resolve issues of underfitting and enable faster training on clean data, respectively.\",\n    \"Direct Inspiration\": {\n        \"b11\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b2\": 0.8,\n        \"b28\": 0.7,\n        \"b17\": 0.7,\n        \"b5\": 0.6,\n        \"b32\": 0.6,\n        \"b29\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.5,\n        \"b30\": 0.5,\n        \"b12\": 0.4,\n        \"b13\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Convolutional Neural Networks (CNNs) with noisy data. The proposed algorithm, Joint Negative and Positive Learning (JNPL), introduces a unified single-stage pipeline with two novel loss functions, NL+ and PL+, to filter noisy data and improve convergence, leading to state-of-the-art accuracy across various datasets.\",\n  \"Direct Inspiration\": [\"b11\"],\n  \"Indirect Inspiration\": [\"b3\", \"b2\", \"b17\", \"b28\", \"b5\", \"b29\"],\n  \"Other Inspiration\": [\"b30\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Convolutional Neural Networks (CNNs) with noisy data, proposing a novel approach named Joint Negative Learning and Positive Learning (JNPL). This method introduces two new loss functions, NL+ and PL+, which aim to improve convergence and efficiency over existing methods like NLNL. The key inspirations and motivations are drawn from the limitations of previous noise-robust learning methods, particularly the shortcomings of the NLNL approach.\",\n  \"Direct Inspiration\": [\"b11\"],\n  \"Indirect Inspiration\": [\"b5\", \"b29\", \"b30\", \"b17\"],\n  \"Other Inspiration\": [\"b21\", \"b32\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Convolutional Neural Networks (CNNs) with noisy data, proposing a novel method called Joint Negative and Positive Learning (JNPL). The JNPL method features a unified single-stage pipeline with two newly designed loss functions, NL+ and PL+, which aim to improve convergence and training efficiency by filtering noisy data.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b29\": 0.8,\n    \"b17\": 0.7,\n    \"b30\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b2\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```"], "60a640a491e0115d932bfd75": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the generalization of person re-identification (ReID) models across different domains, proposing a novel Relevance-aware Mixture of Experts (RaMoE) approach. The approach exploits complementary information from multiple source domains and their relevance to unseen target domains. Key contributions include a decorrelation loss to maintain source domains' diversity, an adaptive voting network for feature integration, and a novel learning-to-learn algorithm for training the voting network.\",\n  \"Direct Inspiration\": {\n    \"b43\": 0.95,\n    \"b31\": 0.90,\n    \"b11\": 0.90,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.80,\n    \"b12\": 0.75,\n    \"b40\": 0.75,\n    \"b66\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.65,\n    \"b29\": 0.65,\n    \"b44\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of domain generalization (DG) in person re-identification (ReID) by proposing a novel approach called Relevance-aware Mixture of Experts (RaMoE). This method aims to improve model generalization on unseen target domains by leveraging the complementary information and relevance of multiple source domain experts. The key contributions include the introduction of a decorrelation loss to maintain diversity among domain experts, a voting network to adaptively integrate features, and a meta-learning algorithm to update the voting network for better generalization.\",\n    \"Direct Inspiration\": {\n        \"b43\": 0.9,\n        \"b20\": 0.85,\n        \"b1\": 0.8,\n        \"b31\": 0.75,\n        \"b11\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.6,\n        \"b40\": 0.55,\n        \"b27\": 0.5,\n        \"b22\": 0.45,\n        \"b66\": 0.4\n    },\n    \"Other Inspiration\": {\n        \"b44\": 0.35,\n        \"b28\": 0.3,\n        \"b29\": 0.25\n    }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"Domain biases in person re-identification (ReID) lead to performance drops when applying models to unseen domains. Existing approaches like unsupervised domain adaptation (UDA) and domain generalizable (DG) methods have limitations in practical scenarios.\",\n        \"inspirations\": \"Inspired by the mixture of experts (MoE) approach and meta-learning techniques to improve generalization for unseen domains.\"\n    },\n    \"Direct Inspiration\": [\"b43\", \"b20\", \"b27\", \"b31\", \"b11\", \"b22\"],\n    \"Indirect Inspiration\": [\"b44\", \"b28\", \"b29\", \"b12\", \"b40\"],\n    \"Other Inspiration\": [\"b24\", \"b37\", \"b53\", \"b55\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of domain generalization in person re-identification (ReID) by proposing a Relevance-aware Mixture of Experts (RaMoE) approach. The key contributions include the introduction of a decorrelation loss to maintain source domain diversity, an adaptive voting network for feature integration based on domain relevance, and a learning-to-learn algorithm for improving generalization on unseen target domains.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b20\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.9,\n    \"b40\": 0.85,\n    \"b31\": 0.85,\n    \"b11\": 0.85,\n    \"b22\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.8,\n    \"b66\": 0.8,\n    \"b27\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of domain generalization in person re-identification (DG ReID) by proposing a novel Relevance-aware Mixture of Experts (RaMoE) method. The primary contributions include leveraging diverse domain-specific characteristics through domain experts, an adaptive voting network to calculate domain relevance, and a novel learning-to-learn algorithm to improve model generalizability on unseen target domains.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b20\": 0.9,\n    \"b66\": 0.9,\n    \"b1\": 0.85,\n    \"b40\": 0.85,\n    \"b31\": 0.8,\n    \"b11\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b27\": 0.75,\n    \"b43\": 0.75\n  }\n}\n```"], "60c2fb6191e0117e30ca2951": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of combining convolutional layers and self-attention mechanisms to create a more efficient and effective neural network architecture for computer vision tasks. The proposed CoAtNet architecture leverages the generalization properties of convolutional layers and the model capacity of attention layers to achieve state-of-the-art performance under various data regimes.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b21\": 0.9,\n    \"b20\": 0.9,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b30\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b4\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the integration of convolutional networks (ConvNets) and transformer models to leverage their respective strengths for improved generalization and model capacity. The proposed solution, CoAtNet, systematically combines convolution and attention layers to achieve better performance across various data regimes. The paper is particularly inspired by the limitations of Vision Transformers (ViT) in low-data regimes and the strengths of ConvNets in such scenarios.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b4\": 1.0,\n    \"b19\": 0.9,\n    \"b21\": 0.85,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b23\": 0.8,\n    \"b22\": 0.8,\n    \"b8\": 0.75,\n    \"b9\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b26\": 0.65,\n    \"b27\": 0.6,\n    \"b30\": 0.6,\n    \"b11\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating the generalization capabilities of Convolutional Neural Networks (ConvNets) with the model capacity of Transformer models. It proposes a novel architecture, CoAtNet, which combines convolution and attention layers to achieve better performance across different data sizes. The key insights include merging depthwise convolution into attention layers and stacking convolutional and attention layers effectively.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b19\": 0.9,\n    \"b21\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively combining convolution and attention layers in neural network models to balance generalization and model capacity. The authors propose a novel architecture named CoAtNet, which integrates depthwise convolution and self-attention layers to leverage the strengths of both ConvNets and Transformers. CoAtNet demonstrates superior performance across different data scales, achieving state-of-the-art results under comparable resource constraints.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b21\": 0.85,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b19\": 0.75,\n    \"b20\": 0.7,\n    \"b22\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b27\": 0.55,\n    \"b29\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to effectively combine convolutional layers and self-attention layers to achieve better generalization and model capacity, particularly in low-data regimes. The authors propose a hybrid architecture named CoAtNet, which merges depthwise convolution into attention layers and stacks convolutional and attention layers in a specific manner to leverage their respective strengths. The paper aims to achieve state-of-the-art performance on image classification tasks with efficient use of computational resources.\",\n  \"Direct Inspiration\": [\"b12\", \"b19\", \"b21\", \"b25\"],\n  \"Indirect Inspiration\": [\"b20\", \"b22\", \"b23\", \"b24\"],\n  \"Other Inspiration\": [\"b26\", \"b27\", \"b28\", \"b29\", \"b30\", \"b33\"]\n}\n```"], "6034f66f91e01122c046f9dc": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Network anomaly detection with few labeled anomalies.\",\n      \"Transferring knowledge across multiple networks to improve detection performance.\"\n    ],\n    \"Inspirations\": [\n      \"Leverage limited labeled anomalies to learn anomaly-informed models.\",\n      \"Transfer valuable knowledge from source networks to target networks.\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b22\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b32\",\n    \"b50\",\n    \"b51\"\n  ],\n  \"Other Inspiration\": [\n    \"b5\",\n    \"b36\",\n    \"b28\",\n    \"b40\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are network anomaly detection with limited labeled data and the difficulty of transferring knowledge across multiple networks with varying characteristics of anomalies. The proposed algorithm, Meta-GDN, integrates a new family of graph neural networks (GDN) and a cross-network meta-learning algorithm to address these challenges.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b32\": 1,\n    \"b36\": 1,\n    \"b50\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b11\": 0.8,\n    \"b13\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot network anomaly detection under a cross-network setting. The primary challenges include limited knowledge of anomalies within a single network and the risk of negative transfer when using knowledge from a single auxiliary network. To tackle these challenges, the authors propose Graph Deviation Networks (GDN) and a cross-network meta-learning algorithm integrated into Meta-GDN, which leverages knowledge from multiple networks to improve anomaly detection on a target network.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b32\": 1.0,\n    \"b50\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b36\": 0.6,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of few-shot network anomaly detection under a cross-network setting. It introduces the Graph Deviation Networks (GDN) for anomaly detection with limited labeled data and proposes a cross-network meta-learning algorithm (Meta-GDN) for transferring this ability across multiple networks.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1,\n    \"b50\": 1,\n    \"b22\": 1,\n    \"b9\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b36\": 0.8,\n    \"b40\": 0.7,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b11\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of few-shot network anomaly detection under cross-network settings, proposing the Graph Deviation Networks (GDN) and a cross-network meta-learning algorithm called Meta-GDN. The key challenges include limited knowledge of anomalies and the need for transferring knowledge from multiple auxiliary networks to a target network.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b32\": 0.8,\n    \"b50\": 0.8,\n    \"b51\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b36\": 0.7,\n    \"b38\": 0.7\n  }\n}\n```"], "60641c5c9e795e72406b65cd": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"Detecting group anomalies in multi-aspect data streams, specifically in the context of intrusion detection in networks. Handling both categorical and numerical variables in a streaming manner to provide real-time anomaly detection.\",\n    \"inspiration\": \"Existing methods for point anomalies and dense subgraph/subtensor detection, with limitations in scaling and handling mixed-type variables.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b3\", \"b62\", \"b64\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b30\", \"b46\", \"b65\", \"b21\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b8\", \"b53\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting anomalous behavior in multi-aspect data streams, particularly for network intrusion detection. The proposed algorithm, MSTREAM, detects group anomalies by leveraging locality-sensitive hash functions and operates in a streaming manner. It is further extended with MSTREAM-PCA, MSTREAM-IB, and MSTREAM-AE to incorporate feature correlations and dimensionality reduction.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b62\": 0.8,\n    \"b64\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.6,\n    \"b10\": 0.6,\n    \"b21\": 0.6,\n    \"b25\": 0.6,\n    \"b46\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.5,\n    \"b47\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenge of detecting group anomalies in multi-aspect data streams, specifically for applications like intrusion detection in networks. The proposed algorithm, MSTREAM, utilizes locality-sensitive hash functions to process data in a streaming manner, allowing the detection of group anomalies based on both categorical and numerical attributes. MSTREAM is extended with dimensionality reduction techniques (PCA, IB, AE) to improve performance and speed.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b62\": 0.8,\n    \"b64\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b21\": 0.7,\n    \"b25\": 0.6,\n    \"b46\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the detection of group anomalies in multi-aspect data streams in an unsupervised manner, specifically for applications in network intrusion detection. The authors propose the MSTREAM algorithm, which leverages locality-sensitive hash functions (LSH) to process data in a streaming manner and detect group anomalies. Additionally, MSTREAM-PCA, MSTREAM-IB, and MSTREAM-AE are introduced to incorporate feature correlation and reduce dimensionality, enhancing the detection performance.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b62\": 0.9,\n    \"b64\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b46\": 0.6,\n    \"b65\": 0.6,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b11\": 0.5,\n    \"b58\": 0.5,\n    \"b38\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting group anomalies in multi-aspect data streams, particularly for intrusion detection in networks. The proposed algorithm, MSTREAM, processes data in a streaming manner, handles both categorical and numerical attributes, and improves scalability and performance compared to existing methods by using locality-sensitive hash functions and temporal scoring.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b62\": 0.85,\n    \"b64\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.65,\n    \"b46\": 0.6,\n    \"b21\": 0.55\n  }\n}\n```"], "6076c80791e0113d72574489": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is to effectively combine large language models (LMs) and knowledge graphs (KGs) for question answering (QA) tasks. The proposed solution, QA-GNN, introduces a joint graph representation and relevance scoring mechanism to enable joint reasoning over both LMs and KGs. The paper emphasizes the need to capture the nuance of QA contexts and perform structured reasoning, particularly handling negation and entity substitution.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b22\": 0.6,\n    \"b40\": 0.6,\n    \"b46\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.4,\n    \"b37\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Identify informative knowledge from a large knowledge graph (KG)\",\n      \"Capture the nuance of the QA context and the structure of the KGs to perform joint reasoning over these two sources of information\"\n    ],\n    \"Inspirations\": [\n      \"Combining large language models (LMs) and structured KGs for reasoning\",\n      \"Previous works on retrieving subgraphs from KGs\",\n      \"Limitations of existing LM+KG methods for reasoning\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b46\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.6,\n    \"b41\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in combining language models (LMs) and knowledge graphs (KGs) for question answering: (i) identifying informative knowledge from a large KG, and (ii) performing joint reasoning over the QA context and the KG structure. The novel QA-GNN model proposed introduces two key methods to tackle these challenges: KG node relevance scoring and a joint graph representation for the QA context and KG, followed by an attention-based GNN module for reasoning.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.8,\n    \"b2\": 0.7,\n    \"b40\": 0.7,\n    \"b46\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of combining language models (LMs) and knowledge graphs (KGs) for question answering. The proposed QA-GNN aims to identify informative knowledge from a large KG and perform joint reasoning over the QA context and KG. The novel contributions include a relevance scoring mechanism for KG nodes using pre-trained LMs and a joint reasoning algorithm via an attention-based GNN module.\",\n    \"Direct Inspiration\": {\n        \"b19\": 1.0,\n        \"b12\": 0.9,\n        \"b37\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.7,\n        \"b46\": 0.6,\n        \"b22\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.4,\n        \"b30\": 0.3\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reasoning over both large language models (LMs) and structured knowledge graphs (KGs) for question answering (QA). It proposes QA-GNN, an end-to-end model that combines LMs and KGs by creating a joint graph for reasoning. Key contributions include KG node relevance scoring and an attention-based GNN module for joint reasoning.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.8,\n    \"b41\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```"], "600fe888d4150a363c24b1e4": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of automated fact checking by not only determining the veracity of claims but also providing concise, coherent, and human-comprehensible explanations for its decisions. The proposed framework, FACE-KEG, integrates structured knowledge graphs and unstructured textual context, employing a graph transformer network and a bidirectional recurrent neural network for encoding. This approach is novel in generating abstractive explanations directly from knowledge without relying on pre-existing explanatory text.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b26\": 0.85,\n    \"b27\": 0.9,\n    \"b35\": 0.85,\n    \"b56\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b32\": 0.7,\n    \"b43\": 0.7,\n    \"b67\": 0.7,\n    \"b81\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.65,\n    \"b57\": 0.6,\n    \"b71\": 0.6,\n    \"b72\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automating the process of explaining fact-checking decisions by generating concise, coherent, and easily comprehensible explanations using a framework called FACE-KEG. The proposed algorithm constructs a knowledge graph, retrieves unstructured textual context, and employs a graph transformer network and bidirectional recurrent neural network to jointly predict the veracity of claims and generate textual explanations.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b26\": 0.85,\n    \"b27\": 0.85,\n    \"b56\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b23\": 0.75,\n    \"b32\": 0.75,\n    \"b43\": 0.75,\n    \"b55\": 0.75,\n    \"b67\": 0.75,\n    \"b81\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.65,\n    \"b35\": 0.65,\n    \"b57\": 0.65,\n    \"b66\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes the FACE-KEG framework to address the challenge of not only detecting the veracity of claims but also generating concise and coherent explanations for these veracity decisions. The primary inspiration for this work is to combine structured knowledge graphs with unstructured textual context to create a more comprehensive and explainable fact-checking system.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b27\": 0.8,\n    \"b35\": 0.8,\n    \"b56\": 0.85,\n    \"b81\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.75,\n    \"b57\": 0.7,\n    \"b43\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b32\": 0.6,\n    \"b49\": 0.55,\n    \"b67\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of explainable fact checking, proposing a novel framework FACE-KEG that jointly detects the veracity of input claims and generates concise, coherent explanations for their veracity. The framework utilizes both structured knowledge graphs and unstructured textual context to enhance the understanding and explanation of claims.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b35\": 0.85,\n    \"b26\": 0.8,\n    \"b27\": 0.8,\n    \"b56\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b32\": 0.7,\n    \"b43\": 0.7,\n    \"b55\": 0.7,\n    \"b81\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.65,\n    \"b17\": 0.65,\n    \"b31\": 0.65,\n    \"b38\": 0.65,\n    \"b49\": 0.65,\n    \"b57\": 0.65,\n    \"b74\": 0.65,\n    \"b79\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of automated fact-checking by not only determining the veracity of claims but also generating concise, human-readable explanations for these claims. The proposed algorithm, FACE-KEG, builds a knowledge graph and retrieves unstructured textual context related to each claim. It employs a graph transformer network and a bidirectional recurrent neural network to encode data, followed by a classifier to predict claim veracity and a decoder to generate textual explanations.\",\n    \"Direct Inspiration\": {\n        \"b26\": 0.9,\n        \"b27\": 0.9,\n        \"b56\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.7,\n        \"b43\": 0.7,\n        \"b57\": 0.7,\n        \"b81\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.6,\n        \"b35\": 0.6,\n        \"b32\": 0.6\n    }\n}\n```"], "60c312ac9e795e9243fd165e": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of tracing the evolution of scientific publications by proposing a framework called Master Reading Tree (MRT). The primary challenges include restricted access to academic resources, difficulties in unsupervised deep comprehension, problems with importance identification, and the lack of ground truth for evaluating evolution roadmaps. The MRT framework combines document embedding and graph embedding techniques in unsupervised ways to generate evolution roadmaps, which help readers understand the development of ideas in scientific literature.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b4\": 0.8,\n        \"b12\": 0.7,\n        \"b13\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.6,\n        \"b8\": 0.6,\n        \"b20\": 0.5,\n        \"b21\": 0.4\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.3,\n        \"b11\": 0.3,\n        \"b16\": 0.2,\n        \"b17\": 0.2\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in generating evolution roadmaps for scientific publications due to restricted access to academic resources, difficulties in unsupervised deep comprehension, problems in importance identification, and the lack of ground truth. It proposes the Master Reading Tree (MRT) framework, which combines textual and structural information to create embeddings and construct roadmaps, aiming to help readers understand the evolution of scientific ideas.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b4\": 0.8,\n    \"b8\": 0.8,\n    \"b12\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b9\": 0.5,\n    \"b10\": 0.5,\n    \"b11\": 0.5,\n    \"b14\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5,\n    \"b29\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5,\n    \"b33\": 0.5,\n    \"b34\": 0.5,\n    \"b35\": 0.5,\n    \"b36\": 0.5,\n    \"b37\": 0.5,\n    \"b38\": 0.5\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of tracing the evolution of scientific publications by proposing a novel framework named Master Reading Tree (MRT). The main challenges include restricted access to academic resources, difficulties in unsupervised deep comprehension, problems in importance identification, and lack of ground truth. The framework incorporates document and graph embeddings to generate evolution roadmaps, providing deep comprehension of relationships between publications.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b4\": 0.7,\n    \"b8\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Restricted access to academic resources\",\n      \"Difficulties for unsupervised deep comprehension\",\n      \"Problems for importance identification\",\n      \"Lack of ground truth\"\n    ],\n    \"Inspirations\": [\n      \"Concept extraction and taxonomy construction\",\n      \"Algorithm Roadmap\",\n      \"Deep representation of natural language\",\n      \"Graph embedding techniques\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"Inspired by\": [\"b1\"],\n    \"Motivated by\": []\n  },\n  \"Indirect Inspiration\": {\n    \"Inspired by\": [\"b2\", \"b4\", \"b12\"],\n    \"Motivated by\": []\n  },\n  \"Other Inspiration\": {\n    \"Inspired by\": [\"b8\", \"b13\", \"b20\"],\n    \"Motivated by\": []\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of restricted access to academic resources, difficulties in unsupervised deep comprehension, problems in importance identification, and lack of ground truth in tracing the evolution of scientific publications. The proposed solution, Master Reading Tree (MRT), generates evolution roadmaps by combining document embedding and graph embedding in unsupervised ways, and then constructs roadmaps through clustering and automatic labeling techniques.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b2\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.7,\n        \"b8\": 0.6,\n        \"b12\": 0.7,\n        \"b13\": 0.7,\n        \"b17\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.5,\n        \"b11\": 0.5,\n        \"b16\": 0.5\n    }\n}\n```"], "61fc99465aee126c0fcdcbf7": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of creating, refining, and sharing natural language prompts for zero-shot and few-shot learning in NLP. The proposed system, PromptSource, is designed to facilitate prompt engineering through a flexible templating language, a suite of tools for prompt management, and community-driven quality standards. The system supports the creation of a large collection of prompts (P3) and has been instrumental in recent research on zero-shot inference, multilingual prompting, and few-shot learning.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b18\": 1,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.8,\n    \"b16\": 0.8,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is enabling users to create, refine, and share prompts for NLP tasks in zero-shot and few-shot learning settings. The paper proposes PromptSource, an integrated development environment and repository for natural language prompts. The system supports a flexible templating language, prompt documentation, iterative refinement, and a global review process to ensure quality standards.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1,\n        \"b26\": 1,\n        \"b16\": 0.9,\n        \"b18\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b22\": 0.7,\n        \"b33\": 0.7,\n        \"b37\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.5,\n        \"b9\": 0.5,\n        \"b28\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper discusses the challenges and innovations in prompt engineering for NLP, specifically focusing on zero-shot and few-shot learning settings. It introduces PromptSource, a repository and development environment for natural language prompts, and highlights its features such as a flexible templating language, integration with HuggingFace Datasets, and guidelines for creating high-quality prompts.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.95,\n        \"b26\": 0.95,\n        \"b18\": 0.95,\n        \"b16\": 0.90\n    },\n    \"Indirect Inspiration\": {\n        \"b33\": 0.85,\n        \"b6\": 0.80,\n        \"b22\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.70,\n        \"b37\": 0.65,\n        \"b32\": 0.65,\n        \"b28\": 0.60\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating, refining, and sharing prompts for NLP tasks, particularly in zero-shot and few-shot learning settings. The authors propose PromptSource, an integrated development environment and repository for natural language prompts. PromptSource is designed to support prompt creation with a flexible templating language, a suite of tools for prompt management, and community-driven quality standards. The system is integrated with the HuggingFace Datasets library and includes a large collection of open-source prompts.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.95,\n    \"b33\": 0.9,\n    \"b2\": 0.85,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.75,\n    \"b22\": 0.7,\n    \"b32\": 0.65,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.55,\n    \"b28\": 0.5,\n    \"b9\": 0.45,\n    \"b37\": 0.4,\n    \"b19\": 0.35,\n    \"b29\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling users to create, refine, and share prompts for zero- and few-shot learning in NLP. It introduces PromptSource, an integrated development environment and repository for natural language prompts, which provides a web-based GUI, flexible templating language, and a large collection of reviewed prompts (P3). The paper emphasizes the importance of prompt engineering for adapting language models to various tasks and highlights the need for standardized tools and guidelines in this emerging field.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b26\": 1,\n    \"b33\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b16\": 0.8,\n    \"b18\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b19\": 0.6,\n    \"b22\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```"], "619e189e6750f82b1e8c7102": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised representation learning on graphs, proposing a novel method called Bootstrapped Graph Latents (BGRL). BGRL leverages self-supervised learning without the need for negative examples, making it scalable for large graphs. The main contributions include introducing BGRL, demonstrating its performance and memory efficiency over contrastive methods, and showing its effectiveness in utilizing unlabeled data for semi-supervised learning.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b52\": 0.8,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b47\": 0.6,\n    \"b36\": 0.6,\n    \"b20\": 0.6,\n    \"b50\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised representation learning for graph-structured data, introducing a scalable method called Bootstrapped Graph Latents (BGRL) which avoids the need for negative examples, thereby reducing memory and computational costs. Inspired by self-supervised learning advances in vision, BGRL uses two graph encoders (an online and a target encoder) to generate node embeddings without the need for contrasting negative examples, making it efficient for large graphs.\",\n  \"Direct Inspiration\": [\"b16\"],\n  \"Indirect Inspiration\": [\"b52\", \"b51\", \"b50\", \"b47\"],\n  \"Other Inspiration\": [\"b40\", \"b25\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is overfitting and the inability to generalize when training graph neural networks (GNNs) with supervised data alone. The authors propose Bootstrapped Graph Latents (BGRL), a scalable self-supervised learning method for graph representation that avoids the need for contrasting negative examples, making it computationally efficient and effective for large graphs.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b50\": 0.8,\n    \"b52\": 0.8,\n    \"b51\": 0.6,\n    \"b47\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.5,\n    \"b31\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed is forming simplified representations of graph-structured data without labels, avoiding issues like overfitting and computational inefficiency of negative sampling in contrastive methods.\",\n    \"inspirations\": \"The proposed method, Bootstrapped Graph Latents (BGRL), is inspired by recent advances in self-supervised learning in vision, particularly BYOL.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b16\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b47\", \"b50\", \"b52\", \"b51\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b31\", \"b14\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces Bootstrapped Graph Latents (BGRL), a scalable self-supervised representation learning method on graphs. BGRL addresses the challenge of overfitting in graph neural networks by eliminating the need for contrastive negative examples, which are computationally expensive for large graphs. Instead, BGRL uses two graph encoders (online and target) and updates the target encoder as an exponential moving average of the online encoder, optimizing the cosine similarity between their representations. The method shows superior scalability and performance on large datasets, including achieving state-of-the-art results in various benchmark tasks.\",\n    \"Direct Inspiration\": {\n        \"b16\": 1,\n        \"b52\": 0.9,\n        \"b51\": 0.8,\n        \"b50\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b40\": 0.7,\n        \"b25\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b36\": 0.5,\n        \"b47\": 0.5,\n        \"b14\": 0.5\n    }\n}\n```"], "606c685691e0114248cd042d": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the instability of training Vision Transformers (ViT) in self-supervised learning settings. The authors propose a novel approach using the MoCo v3 framework, which is inspired by contrastive learning and Siamese networks. They introduce a simple trick to stabilize training by freezing the patch projection layer, resulting in improved accuracy.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b9\": 0.8,\n    \"b16\": 0.8,\n    \"b6\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.6,\n    \"b11\": 0.6,\n    \"b10\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.5,\n    \"b40\": 0.5,\n    \"b14\": 0.5,\n    \"b34\": 0.4,\n    \"b8\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of instability in training Vision Transformers (ViT) using self-supervised learning frameworks, particularly contrastive/Siamese paradigms. They propose freezing the patch projection layer as a novel method to improve stability and accuracy.\",\n  \"Direct Inspiration\": [\"b16\", \"b9\", \"b18\", \"b6\"],\n  \"Indirect Inspiration\": [\"b15\", \"b19\"],\n  \"Other Inspiration\": [\"b42\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on training Vision Transformers (ViT) with self-supervised learning frameworks, particularly those based on Siamese networks. It compares the performance of contrastive learning approaches like MoCo v3 against masked auto-encoding methods and highlights the instability issues in training ViTs. The study finds that freezing the patch projection layer can improve stability and accuracy.\",\n  \"Direct Inspiration\": [\"b18\", \"b9\", \"b16\", \"b6\"],\n  \"Indirect Inspiration\": [\"b15\", \"b8\"],\n  \"Other Inspiration\": [\"b19\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Vision Transformers (ViT) with self-supervised learning frameworks, focusing on instability issues during training. The authors propose the MoCo v3 framework, inspired by earlier MoCo frameworks, and introduce a trick to improve stability by freezing the patch projection layer. The paper aims to close the gap between NLP and vision pre-training methodologies and demonstrates that larger self-supervised ViT models can achieve better accuracy compared to their supervised counterparts.\",\n  \"Direct Inspiration\": [\"b18\", \"b16\", \"b9\", \"b6\"],\n  \"Indirect Inspiration\": [\"b15\", \"b8\"],\n  \"Other Inspiration\": [\"b10\", \"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Vision Transformers (ViT) using self-supervised learning frameworks, particularly focusing on the instability issues during training. The proposed method, MoCo v3, is an incremental improvement over prior MoCo frameworks, aiming to provide a balance between simplicity, accuracy, and scalability. The study highlights the importance of batch size, learning rate, and optimizer choice in training stability and proposes a trick of freezing the patch projection layer to improve stability.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b9\": 1,\n    \"b16\": 1,\n    \"b6\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.7,\n    \"b42\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.6,\n    \"b19\": 0.6,\n    \"b3\": 0.5,\n    \"b34\": 0.5\n  }\n}\n```"], "605dbaf191e0113c28655a7f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of adapting Transformer models from NLP to computer vision, specifically focusing on scale variance and high-resolution images. The proposed Swin Transformer introduces hierarchical feature maps and local self-attention to achieve linear computational complexity relative to image size, overcoming limitations of existing Transformer-based vision models.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b18\", \"b60\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b40\", \"b49\", \"b59\", \"b77\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b24\", \"b44\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of adapting Transformer-based models, originally successful in NLP, to the domain of computer vision. Specifically, it notes difficulties related to the variability in visual element scale and high computational complexity due to high-resolution images. The proposed Swin Transformer introduces hierarchical feature maps and linear computational complexity by using non-overlapping windows and a shifted window approach, enhancing efficiency and performance across vision tasks like image classification, object detection, and semantic segmentation.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b60\": 1,\n    \"b31\": 0.9,\n    \"b59\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.8,\n    \"b49\": 0.8,\n    \"b28\": 0.7,\n    \"b66\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b77\": 0.6,\n    \"b44\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces Swin Transformer, a general-purpose Transformer backbone for computer vision tasks that addresses challenges in transferring the success of Transformers in NLP to vision domains. It constructs hierarchical feature maps and achieves linear computational complexity to image size by using shifted window-based self-attention. The proposed architecture aims to provide a unified model for both computer vision and NLP tasks.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b40\": 0.8,\n    \"b49\": 0.7,\n    \"b60\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.6,\n    \"b31\": 0.65,\n    \"b59\": 0.75,\n    \"b77\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b44\": 0.55,\n    \"b62\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper proposes the Swin Transformer, a new Transformer-based architecture designed to serve as a general-purpose backbone for computer vision tasks. The challenges addressed include the differences in scale and resolution between visual elements and word tokens, which impact the performance of traditional Transformers in vision applications. The Swin Transformer introduces hierarchical feature maps and linear computational complexity through a novel shifted window partitioning approach for self-attention.\",\n    \"Direct Inspiration\": {\n        \"b18\": 0.9,\n        \"b59\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b40\": 0.8,\n        \"b49\": 0.8,\n        \"b60\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b28\": 0.6,\n        \"b31\": 0.6,\n        \"b48\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Adapting the Transformer architecture for vision tasks, addressing issues of scale variance in visual elements, and high computational complexity due to pixel resolution.\",\n    \"Inspirations\": \"To create a general-purpose Transformer backbone for computer vision, analogous to the role of CNNs in vision and Transformers in NLP, while addressing specific challenges in visual data.\"\n  },\n  \"Direct Inspiration\": {\n    \"b18\": 0.95,\n    \"b40\": 0.85,\n    \"b49\": 0.80,\n    \"b59\": 0.90,\n    \"b60\": 0.92\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.75,\n    \"b48\": 0.70,\n    \"b66\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.60,\n    \"b77\": 0.55\n  }\n}\n```"], "604b4c7891e0110eed64c4e2": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is designing scaling strategies for neural networks that optimize both accuracy and model runtime, especially in high-compute regimes. The proposed algorithm introduces a new family of scaling strategies, parameterized by a single parameter \u03b1, referred to as fast compound model scaling or fast scaling. This approach aims to control the asymptotic rate at which model activations scale, resulting in models that are both fast and accurate.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b33\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.7,\n    \"b13\": 0.7,\n    \"b11\": 0.7,\n    \"b28\": 0.6,\n    \"b30\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.5,\n    \"b18\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing neural network scaling strategies that optimize both accuracy and runtime in high-compute regimes. It introduces a novel fast compound scaling strategy, parameterized by \u03b1, which emphasizes scaling primarily along width to achieve faster and more accurate models.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.8,\n    \"b30\": 0.8,\n    \"b11\": 0.7,\n    \"b13\": 0.7,\n    \"b36\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing neural network models in the high-compute regime by proposing a new scaling strategy called fast compound scaling or fast scaling. The main goal is to achieve both high accuracy and reduced runtime for large models. The authors develop a framework for analyzing the complexity of various scaling strategies, focusing on flops, parameters, and activations. They show that activations are a strong predictor of runtime and introduce a new family of scaling strategies parameterized by a single parameter \u03b1. This strategy emphasizes scaling primarily along model width to control activations and achieve faster models without sacrificing accuracy.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b33\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.7,\n    \"b30\": 0.7,\n    \"b36\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b13\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling convolutional neural networks in the high-compute regime, aiming to optimize both accuracy and runtime. Inspired by the limitations of existing scaling strategies, the authors propose a new family of scaling strategies called fast compound scaling, which emphasizes scaling primarily along model width to control the asymptotic rate of model activations. This results in models that are both fast and accurate, outperforming state-of-the-art models in terms of speed and memory efficiency.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.9,\n    \"b30\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b28\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of designing scaling strategies for neural networks optimized for both accuracy and runtime, particularly in high-compute regimes. It proposes a new scaling strategy, called fast compound scaling, which primarily scales model width to control the asymptotic rate of activation scaling, resulting in models that are both fast and accurate. The paper uses activations as a proxy for predicting model runtime on modern accelerators.\",\n    \"Direct Inspiration\": {\n        \"b24\": 0.9,\n        \"b33\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.7,\n        \"b28\": 0.7,\n        \"b36\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.6,\n        \"b30\": 0.6,\n        \"b13\": 0.6\n    }\n}\n```"], "6065b2ef91e011d10ad615a8": ["```json\n{\n  \"Summary\": \"This paper addresses the challenge of training deeper image transformers for image classification. The authors propose two key contributions: LayerScale, a learnable diagonal matrix added to the output of each residual block to improve training dynamics, and CaiT, an architecture that separates self-attention between patches from class-attention layers to better process class embeddings. The paper aims to stabilize and improve the performance of deep transformers on image classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b33\": 1,\n    \"b74\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b18\": 0.8,\n    \"b62\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.7,\n    \"b65\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in training deeper image transformers for image classification by proposing two main contributions: LayerScale and Class-Attention in Image Transformers (CaiT). LayerScale involves adding a learnable diagonal matrix on the output of each residual block, improving training dynamics and stability for deeper architectures. CaiT separates the self-attention layers for patch embeddings from class-attention layers, enhancing the processing of the class embedding for classification.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b62\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b15\": 0.8,\n    \"b33\": 0.8,\n    \"b65\": 0.8,\n    \"b74\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training deeper vision transformers for image classification by proposing two novel methods: LayerScale and Class-Attention in Image Transformers (CaiT). LayerScale introduces a learnable diagonal matrix after each residual block to improve training dynamics, while CaiT separates self-attention and class-attention stages to optimize processing of class embeddings.\",\n  \"Direct Inspiration\": [\"b1\", \"b15\", \"b74\", \"b33\"],\n  \"Indirect Inspiration\": [\"b18\", \"b27\", \"b65\"],\n  \"Other Inspiration\": [\"b12\", \"b62\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of training deeper image transformers for image classification by introducing two novel contributions: LayerScale and Class-Attention in Image Transformers (CaiT). LayerScale is a method to stabilize and improve the training of deep transformer architectures by adding a learnable diagonal matrix to each residual block. CaiT introduces a class-attention mechanism to separate patch attention from class attention, improving the processing of class embeddings.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.95,\n    \"b18\": 0.90,\n    \"b62\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b65\": 0.75,\n    \"b74\": 0.70,\n    \"b1\": 0.65,\n    \"b33\": 0.60,\n    \"b15\": 0.60\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.55,\n    \"b49\": 0.50,\n    \"b61\": 0.50,\n    \"b71\": 0.50\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of training deeper transformers for image classification, proposing two main contributions: LayerScale, a learnable diagonal matrix added to each residual block, and CaiT (Class-Attention in Image Transformers), which separates self-attention and class-attention stages. These methods improve training stability and performance for deep networks.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b15\": 1.0,\n        \"b33\": 1.0,\n        \"b74\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.9,\n        \"b62\": 0.9,\n        \"b65\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b26\": 0.8,\n        \"b27\": 0.8\n    }\n}\n```"], "60bdde338585e32c38af510f": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the difficulty in scaling Graph Neural Networks (GNNs) to large graphs due to GPU memory limitations and the problem of neighbor explosion. The proposed algorithm, GNNAutoScale (GAS), addresses these challenges by using historical embeddings to approximate node embeddings from previous iterations, allowing for constant memory consumption and scalability without sacrificing expressiveness. Key inspirations include various sampling techniques, decoupling propagation from predictions, and training GNN layers in isolation.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b14\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b44\": 0.8,\n    \"b40\": 0.7,\n    \"b7\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b22\": 0.6,\n    \"b42\": 0.6,\n    \"b46\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the difficulty of scaling Graph Neural Networks (GNNs) to large graphs due to GPU memory limitations and the neighbor explosion problem. The proposed solution, the GNNAutoScale (GAS) framework, addresses these challenges by using historical embeddings to maintain constant memory consumption and enable scalable training while preserving the expressiveness of GNN models.\",\n  \"Direct Inspiration\": [\"b3\"],\n  \"Indirect Inspiration\": [\"b14\", \"b25\", \"b44\"],\n  \"Other Inspiration\": [\"b2\", \"b5\", \"b40\", \"b41\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the scalability of Graph Neural Networks (GNNs) to large graphs due to GPU memory limitations and neighbor explosion. The proposed solution, GNNAutoScale (GAS), uses historical embeddings to approximate node dependencies and reduce memory consumption, enabling the application of deep and expressive GNN models on large graphs.\",\n  \n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b14\": 0.95,\n    \"b5\": 0.9,\n    \"b44\": 0.85\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b40\": 0.75,\n    \"b7\": 0.7,\n    \"b42\": 0.65,\n    \"b11\": 0.6,\n    \"b46\": 0.55\n  },\n  \n  \"Other Inspiration\": {\n    \"b32\": 0.5,\n    \"b2\": 0.45,\n    \"b53\": 0.4,\n    \"b19\": 0.35,\n    \"b49\": 0.3,\n    \"b26\": 0.25,\n    \"b18\": 0.2,\n    \"b45\": 0.15,\n    \"b29\": 0.1,\n    \"b4\": 0.05\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"0\": \"The paper addresses the challenge of scaling Graph Neural Networks (GNNs) to large graphs due to GPU memory limitations and neighbor explosion. The proposed GNNAutoScale (GAS) framework uses historical embeddings to make memory consumption constant regarding input node size, allowing scalable GNN training on large graphs while maintaining expressiveness and accuracy.\"\n    },\n    \"Direct Inspiration\": {\n        \"1\": 0.95,\n        \"2\": 0.9,\n        \"3\": 0.9,\n        \"4\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"5\": 0.85,\n        \"6\": 0.85\n    },\n    \"Other Inspiration\": {\n        \"7\": 0.8,\n        \"8\": 0.8\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the difficulty of scaling Graph Neural Networks (GNNs) to large graphs due to GPU memory limitations and the phenomenon of neighbor explosion. To tackle this, the authors propose the GNNAutoScale (GAS) framework, which uses historical embeddings to maintain constant memory consumption by pruning the GNN computation graph. GAS allows for scalable training of GNNs on large graphs while preserving expressiveness and achieving full-batch performance.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b14\": 0.7,\n    \"b5\": 0.7,\n    \"b44\": 0.7,\n    \"b40\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b22\": 0.6,\n    \"b29\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```"], "60cae12b91e011b32937419e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of training deep and wide Graph Neural Networks (GNNs) on large graph datasets while maintaining memory efficiency. The proposed methods include deep reversible GNNs, weight-tied GNNs, and deep equilibrium GNNs, all aimed at reducing memory consumption while allowing for deeper and more parameterized models.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b20\", \"b65\", \"b0\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b27\", \"b40\", \"b23\", \"b41\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b1\", \"b56\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the memory efficiency of Graph Neural Networks (GNNs) when training on large graphs. The authors propose several novel methods including deep reversible GNNs, weight-tied GNNs, and deep graph equilibrium GNNs to achieve state-of-the-art performance with reduced memory consumption.\",\n  \"Direct Inspiration\": {\n    \"1\": \"b20\",\n    \"2\": \"b65\",\n    \"3\": \"b0\"\n  },\n  \"Indirect Inspiration\": {\n    \"1\": \"b23\",\n    \"2\": \"b10\",\n    \"3\": \"b12\",\n    \"4\": \"b71\",\n    \"5\": \"b64\",\n    \"6\": \"b35\",\n    \"7\": \"b4\",\n    \"8\": \"b46\",\n    \"9\": \"b17\",\n    \"10\": \"b56\",\n    \"11\": \"b41\"\n  },\n  \"Other Inspiration\": {\n    \"1\": \"b27\",\n    \"2\": \"b40\"\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the memory bottleneck in training deep Graph Neural Networks (GNNs) on large graphs. It proposes several memory-efficient architectures, namely deep reversible GNNs, weight-tied GNNs, and deep graph equilibrium GNNs, to enable training very deep GNNs with constant memory consumption.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b45\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b65\": 0.7,\n    \"b0\": 0.6,\n    \"b56\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenge of training Graph Neural Networks (GNNs) on large graphs with limited GPU memory. The authors propose several novel techniques, including reversible GNNs, weight-tied GNNs, and deep equilibrium GNNs, to address memory efficiency while maintaining or improving performance. These techniques enable the training of very deep and over-parameterized models without increasing memory consumption.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b65\": 1,\n    \"b0\": 1,\n    \"b45\": 0.9,\n    \"b34\": 0.9,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.7,\n    \"b40\": 0.7,\n    \"b56\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training deep and wide Graph Neural Networks (GNNs) on large graphs, which is limited by GPU memory consumption. The authors propose several methods to enhance memory efficiency: reversible connections, weight-tied architectures, and deep equilibrium models. These methods allow training deeper and wider GNNs with constant memory consumption, leading to state-of-the-art performance on benchmark datasets.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b65\": 1,\n    \"b0\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.9,\n    \"b23\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.7,\n    \"b10\": 0.6\n  }\n}\n```"], "614012c15244ab9dcb8166d0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of fine-tuning large-scale pretrained language models (PLMs) with limited training data, which often leads to overfitting and poor generalization. The authors propose a novel fine-tuning technique called CHILD-TUNING, which updates only a subset of model parameters (the child network) while masking gradients of non-child parameters. This approach is explored in two variants: task-free (CHILD-TUNING F) and task-driven (CHILD-TUNING D). The technique aims to mitigate aggressive fine-tuning issues and improve model performance across various tasks and domains.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b3\": 1,\n    \"b19\": 1,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b31\": 0.8,\n    \"b8\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is how to effectively fine-tune large-scale pretrained language models (PLMs) with millions to billions of parameters, especially when training data is limited. The authors propose a novel technique called CHILD-TUNING that updates a subset of parameters, termed the child network, while strategically masking out gradients of the non-child network during the backward process. This approach aims to mitigate aggressive fine-tuning problems, prevent overfitting, and improve generalization ability. The paper introduces two variants: CHILD-TUNING F (task-free) and CHILD-TUNING D (task-driven).\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b3\": 1,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b31\": 0.7,\n    \"b8\": 0.7,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b29\": 0.6,\n    \"b4\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the adaptation of large-scale pretrained language models (PLMs) to various scenarios, especially when training data is limited. The proposed algorithm, CHILD-TUNING, mitigates the aggressive fine-tuning problem by updating only a subset of parameters via gradient masking. This technique is shown to improve generalization and performance across various NLP tasks.\",\n  \"Direct Inspiration\": [\"b18\"],\n  \"Indirect Inspiration\": [\"b3\", \"b10\", \"b19\"],\n  \"Other Inspiration\": [\"b15\", \"b31\", \"b8\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of fine-tuning large pretrained language models (PLMs) on limited data while preventing overfitting and maintaining generalization. It proposes a novel technique called CHILD-TUNING, which updates only a subset of parameters (child network) by masking gradients of the non-child network during backpropagation. Two variants, CHILD-TUNING F (task-free) and CHILD-TUNING D (task-driven), are explored to improve the fine-tuning process.\",\n    \"Direct Inspiration\": {\n        \"b18\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.9,\n        \"b19\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.8,\n        \"b15\": 0.8,\n        \"b31\": 0.8,\n        \"b8\": 0.8,\n        \"b32\": 0.8\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting large-scale pretrained language models (PLMs) with millions to billions of parameters to various scenarios, especially with limited labeled data. The authors propose a fine-tuning technique, CHILD-TUNING, which updates only a subset of parameters, called the child network, while still utilizing the knowledge of the whole model in the forward process. Two variants, task-free (CHILD-TUNING F) and task-driven (CHILD-TUNING D), are introduced to determine the child network. Extensive experiments show that CHILD-TUNING outperforms vanilla fine-tuning and other state-of-the-art methods in terms of generalization and adaptation to different tasks.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b3\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b31\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```"], "603c766f9e795ea1fb8225d9": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in protein structure prediction by proposing the MSA Transformer, a model that unifies Potts models and protein language models to operate on sets of aligned sequences. The model uses a novel variant of axial attention and is trained with a masked language modeling objective. It significantly outperforms current state-of-the-art methods in unsupervised contact prediction and other related tasks.\",\n  \"Direct Inspiration\": [\"b40\", \"b52\", \"b16\"],\n  \"Indirect Inspiration\": [\"b6\", \"b11\", \"b51\"],\n  \"Other Inspiration\": [\"b1\", \"b15\", \"b43\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the efficient and accurate learning of protein structures from sequences using unsupervised models. The authors propose the MSA Transformer, a model that unifies the paradigms of Potts models and protein language models by taking sets of aligned sequences as input and sharing parameters across diverse sequence families. This approach aims to improve parameter efficiency and surpasses current state-of-the-art unsupervised structure learning methods.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b40\": 0.9,\n    \"b52\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b51\": 0.7,\n    \"b47\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b43\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the parameter inefficiency and limited inference capabilities of current protein language models, which take a single sequence as input. The proposed algorithm, the MSA Transformer, addresses these challenges by unifying the paradigms of Potts models and protein language models. It takes sets of aligned sequences as input and shares parameters across diverse sequence families, improving parameter efficiency and enabling the extraction of covariation signals during inference.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b40\": 1.0,\n    \"b52\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b6\": 0.8,\n    \"b15\": 0.8,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b10\": 0.5,\n    \"b37\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the development of the MSA Transformer, a protein language model that takes sets of aligned sequences as input. This model aims to unify the paradigms of Potts models and protein language models, improving parameter efficiency and outperforming state-of-the-art unsupervised structure learning methods. Key challenges include leveraging sequence patterns and covariation signals, and addressing the computational complexity associated with multiple sequence alignments.\",\n  \"Direct Inspiration\": [\"b16\", \"b40\", \"b43\", \"b6\"],\n  \"Indirect Inspiration\": [\"b52\", \"b11\"],\n  \"Other Inspiration\": [\"b51\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised protein structure prediction by developing a model that unifies the strengths of Potts models and protein language models. The proposed model, MSA Transformer, processes multiple sequence alignments (MSAs) using a variant of axial attention and improves parameter efficiency and performance over existing methods.\",\n  \"Direct Inspiration\": [\"b40\", \"b52\", \"b16\"],\n  \"Indirect Inspiration\": [\"b1\", \"b15\", \"b43\"],\n  \"Other Inspiration\": [\"b5\", \"b6\", \"b51\"]\n}\n```"], "61c5694d5244ab9dcb0b3871": ["```json\n{\n  \"Summary\": \"This paper addresses the challenge of predicting protein complex structures computationally, emphasizing the inefficiencies of existing methods and introducing a novel approach that uses a simplified phylogeny-based MSA generation method combined with AlphaFold2 (AF2) to improve prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b15\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting protein complex structures, which is critical for understanding protein functions and developing therapeutics. The proposed method employs a simplified phylogeny-based approach to generate multiple sequence alignments (MSAs) of interologs. These MSAs are then input into AlphaFold2 (AF2) to predict protein complex structures. The protocol demonstrates improved prediction accuracy compared to existing methods, particularly by restricting interologs to specific taxonomic ranks and using predicted TM-scores (pTM) for selecting optimal models.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting protein complex structures due to the limited availability of experimental structures and the inefficiencies of existing computational methods. It proposes a novel approach by combining the simplified phylogeny-based method to generate MSAs of interologs and utilizing AlphaFold2 for structure prediction.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting protein complex structures computationally due to the limitations of experimental methods and existing computational approaches. The authors propose a novel method using a simplified phylogeny-based approach combined with AlphaFold2 to generate MSAs for predicting protein complex structures, achieving high prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b17\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b10\": 0.7,\n    \"b18\": 0.65,\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b21\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in predicting protein complex structures, specifically the inefficiencies and limitations of existing protein-protein docking and template-based approaches. The authors propose a simplified phylogeny-based approach to generate multiple sequence alignments (MSAs) of interologs, which are then inputted into the deep learning model AlphaFold2 (AF2) to predict protein complex structures.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.9,\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7\n  }\n}\n```"], "615d7dd35244ab9dcb92df24": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in this paper involve improving the prediction accuracy of gene expression and regulatory element interactions from DNA sequences. The proposed algorithm, Enformer, utilizes a transformer architecture with attention layers to effectively capture long-range interactions in the genome, outperforming previous models like Basenji2 and ExPecto in various metrics including gene expression prediction and enhancer prioritization.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1,\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.9,\n    \"b20\": 0.8,\n    \"b6\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b21\": 0.6,\n    \"b11\": 0.5,\n    \"b23\": 0.5,\n    \"b14\": 0.5,\n    \"b8\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include improving gene expression prediction accuracy, addressing tissue- and cell-type specificity, and enhancing the prediction of regulatory element interactions from DNA sequences. The Enformer model introduced by the authors leverages a transformer architecture with attention layers to expand the receptive field and improve information flow between distal elements. This approach is inspired by recent advances in NLP and aims to outperform existing models like Basenji2.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 0.9,\n    \"b6\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.8,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b14\": 0.6,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5,\n    \"b29\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5,\n    \"b38\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting gene expression solely from DNA sequence using a novel transformer architecture called Enformer. The key inspirations include the improvement over previous models like Basenji2, the use of attention layers for better handling long-range interactions, and the emphasis on distinguishing between proximal and distal regulatory elements. The main contributions are the significant performance improvements in gene expression prediction, enhancer prioritization, and variant effect prediction.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b8\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b11\": 0.6,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the prediction of gene expression from DNA sequences, with a specific focus on improving tissue- and cell-type-specific predictions. The authors propose the Enformer model, which employs transformer modules to achieve a larger receptive field, enhancing the model's ability to capture long-range interactions between regulatory elements. This architecture significantly improves the predictive accuracy over previous models like Basenji2 and ExPecto.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b8\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.6,\n    \"b12\": 0.8,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5,\n    \"b29\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5,\n    \"b38\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents Enformer, a novel transformer-based architecture for predicting gene expression and regulatory activities from DNA sequences. The primary challenges addressed include improving the prediction accuracy of gene expression and enhancer-promoter interactions, and predicting the effects of genetic variants on gene expression. Enformer offers substantial improvements over previous models like Basenji2, particularly through the use of attention layers and a larger receptive field.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 0.9,\n    \"b10\": 0.85,\n    \"b11\": 0.85,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b13\": 0.75,\n    \"b14\": 0.75,\n    \"b15\": 0.75,\n    \"b16\": 0.75,\n    \"b17\": 0.75,\n    \"b18\": 0.75,\n    \"b19\": 0.75,\n    \"b20\": 0.75,\n    \"b21\": 0.75,\n    \"b22\": 0.75,\n    \"b23\": 0.75,\n    \"b24\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5,\n    \"b29\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5,\n    \"b38\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```"], "618cf4845244ab9dcb6ffa1f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting interfacial contacts in protein-protein interactions using a novel deep learning method called GLINTER. The method integrates representations learned from monomer structures and attentions generated by MSA Transformer to predict contacts for both heterodimers and homodimers. This approach aims to overcome limitations in existing methods, such as the need for a large number of sequence homologs and slow computation times.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b45\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b47\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.6,\n    \"b10\": 0.5,\n    \"b11\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting interfacial contacts in protein complexes, leveraging deep learning approaches to improve accuracy and scalability. It proposes a novel method, GLINTER, which integrates learned representations from monomer structures and attentions generated by the MSA Transformer.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b45\": 1.0,\n    \"b47\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b34\": 0.7,\n    \"b44\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the longstanding challenge of in silico structural characterization of protein complexes, focusing on interfacial contact prediction. The proposed method, GLINTER, integrates representations learned from monomer structures and attentions generated by the MSA Transformer to improve accuracy and scalability in proteome-scale interfacial contact prediction.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b45\": 1.0,\n    \"b47\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b3\": 0.8,\n    \"b26\": 0.7,\n    \"b44\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.5,\n    \"b6\": 0.5,\n    \"b22\": 0.5,\n    \"b28\": 0.5,\n    \"b43\": 0.6,\n    \"b13\": 0.5,\n    \"b15\": 0.5,\n    \"b38\": 0.5,\n    \"b41\": 0.5,\n    \"b42\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes GLINTER, a supervised deep learning method for interfacial contact prediction in protein-protein interactions. The main challenges addressed include the need for accurate quaternary protein structure prediction and effective extraction of coevolution signals from a small number of interlogs. The authors emphasize integrating learned representations from monomer structures and attentions generated by the MSA Transformer.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1,\n    \"b25\": 1,\n    \"b47\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b44\": 0.8,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately predicting interfacial contacts in protein-protein interactions, which is crucial for understanding protein functions at the atomic level. The proposed method, GLINTER, integrates representations from monomer structures and attentions from the MSA Transformer to outperform existing models like ComplexContact and DeepHomo.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b45\": 1,\n    \"b47\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b44\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```"], "615e65735244ab9dcbf21774": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the accurate prediction of protein-protein interface contacts using computational methods. The authors propose a novel algorithm, DeepInteract, which employs a Geometric Transformer and a dilated convolution-based interaction module to capture geometric shapes and long-range interactions between amino acids. The work aims to improve the prediction of inter-chain residue-residue contacts in protein complexes, leveraging recent advancements in geometric deep learning and graph-based self-attention.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b20\": 1,\n    \"b31\": 1,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b29\": 0.8,\n    \"b7\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b12\": 0.7,\n    \"b8\": 0.7,\n    \"b21\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces DeepInteract, a novel end-to-end deep learning pipeline for protein interface prediction. The primary challenges addressed are predicting inter-chain residue interactions accurately by leveraging geometric deep learning and graph-based self-attention mechanisms. The authors propose the Geometric Transformer, a new graph transformer designed to exploit protein structure-specific geometric properties, and a dilated convolution-based interaction module. The paper builds upon previous works and datasets to achieve state-of-the-art results in predicting protein-protein interactions.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b12\": 0.85,\n    \"b20\": 0.95,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b7\": 0.75,\n    \"b22\": 0.8,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.65,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting protein-protein interface contacts. The authors propose DeepInteract, an end-to-end deep learning pipeline that leverages geometric deep learning and graph-based self-attention to predict inter-chain residue interactions. The Geometric Transformer within DeepInteract is a new graph neural network designed to capture and evolve protein geometric features, providing state-of-the-art results for interface contact prediction.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b2\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b12\": 0.75,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.65,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting protein-protein interface contacts, proposing DeepInteract, an end-to-end deep learning pipeline. It leverages geometric deep learning and a new graph transformer called the Geometric Transformer, as well as a dilated convolution-based interaction module to improve prediction accuracy. The method is trained on the DIPS-Plus dataset, the largest feature-rich dataset of protein complexes.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b20\": 0.92,\n    \"b31\": 0.88\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.85,\n    \"b7\": 0.80,\n    \"b4\": 0.78\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.72,\n    \"b6\": 0.70\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting protein-protein interface contacts using computational methods. The proposed solution, DeepInteract, utilizes geometric deep learning and graph-based self-attention to enhance the accuracy of these predictions. The primary contributions include the Geometric Transformer for evolving protein geometric representations and a dilated convolution-based interaction module.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b20\": 1,\n    \"b22\": 1,\n    \"b31\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b21\": 0.8,\n    \"b8\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b10\": 0.6,\n    \"b33\": 0.5\n  }\n}\n```"], "61e154295244ab9dcb120cb5": ["```json\n{\n  \"Summary\": \"The paper presents ProteinBERT, a new deep-learning model designed for protein sequences, addressing the challenge of understanding protein functions from sequences. It improves upon the classic Transformer/BERT architecture and introduces a novel pretraining task of predicting protein functions. The model was pretrained on a massive dataset of ~106M proteins and evaluated on nine benchmarks, demonstrating state-of-the-art performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.9,\n    \"b21\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.8,\n    \"b25\": 0.8,\n    \"b38\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents ProteinBERT, a novel deep-learning model designed for protein sequences, improving upon the classic Transformer/BERT architecture and introducing a novel pretraining task of predicting protein functions. The primary challenges addressed include adapting architectures and pretraining tasks from the natural language domain to the unique characteristics of proteins. ProteinBERT incorporates innovations such as separating local (character level) and global (whole protein level) representations, supporting multitasking, and maintaining sequence length flexibility.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Understanding protein functions from sequences remains a significant scientific challenge, with direct implications for human health. Existing methods often use architectures designed for natural language processing, which may not be optimal for proteins due to their unique properties.\",\n    \"contributions\": \"The paper introduces ProteinBERT, a deep-learning model tailored for protein sequences, with innovations including a novel pretraining task of predicting protein functions and the separation of local and global representations.\"\n  },\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b9\": 0.7,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding protein functions from sequences and proposes ProteinBERT, a deep-learning model specifically designed for protein sequences. The model improves upon the classic Transformer/BERT architecture by introducing a novel pretraining task for predicting protein functions and separating local and global representations to support multitasking. ProteinBERT was pretrained on a massive dataset of protein sequences and evaluated on diverse benchmarks, achieving state-of-the-art performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b19\": 0.85,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b21\": 0.75,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b4\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding protein functions from sequences using machine learning. The authors propose ProteinBERT, a deep-learning model specifically designed for protein sequences, incorporating architectural improvements and a novel pretraining task of predicting protein functions. ProteinBERT is tested on diverse benchmarks, showing state-of-the-art performance while being smaller and faster than existing models.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b19\": 0.95,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.85,\n    \"b20\": 0.8,\n    \"b10\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.65\n  }\n}\n```"], "6191faf46750f83ab86fb921": ["```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is the prediction of multimeric protein structures using an adapted version of the AlphaFold system. The authors propose modifications to the AlphaFold system to train it specifically for protein complexes, including changes to the loss function, model architecture, and training regimen. The paper is inspired by the success of AlphaFold in predicting single protein chains and aims to extend this capability to multimeric proteins.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b12\": 0.5,\n    \"b13\": 0.5,\n    \"b18\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the adaptation of the AlphaFold system for predicting the structures of protein complexes, addressing challenges such as permutation symmetry in homomeric components, cross-chain genetic pairings, and memory limitations for multi-chain structures. Key modifications include changes to the loss function, training regimen, and inference process to better handle multimeric inputs and improve accuracy at protein interfaces.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b12\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting the structure of protein complexes. Inspired by the success of AlphaFold in single-chain protein prediction, the authors propose modifications to adapt AlphaFold for multimeric proteins, including changes to loss functions, training regimens, and input handling to account for permutation symmetry, cross-chain genetics, and memory limitations.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b11\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.85,\n    \"b4\": 0.85,\n    \"b5\": 0.85,\n    \"b6\": 0.85,\n    \"b7\": 0.85,\n    \"b8\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.75,\n    \"b12\": 0.75,\n    \"b13\": 0.75,\n    \"b14\": 0.75,\n    \"b15\": 0.75,\n    \"b16\": 0.75,\n    \"b17\": 0.75,\n    \"b18\": 0.75,\n    \"b19\": 0.75,\n    \"b20\": 0.75,\n    \"b21\": 0.75,\n    \"b22\": 0.75,\n    \"b23\": 0.75,\n    \"b24\": 0.75,\n    \"b25\": 0.75,\n    \"b26\": 0.75,\n    \"b27\": 0.75,\n    \"b28\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting the AlphaFold system for the prediction of multimeric protein structures. The key innovations include multi-chain permutation alignment, cross-chain genetics, multi-chain cropping, modified architecture and losses, and a specific training regimen for multimeric protein complexes.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b18\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting the structure of protein complexes using the AlphaFold system, which was originally designed for single-chain proteins. The main modifications include changes to loss functions, alignment techniques, and training regimens to handle multimeric inputs and improve prediction accuracy. The paper is inspired by the success of AlphaFold and aims to adapt it for multimeric structures.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b1\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```"], "61a98b065244ab9dcb9585bc": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of applying Vision Transformers (ViT) to high-resolution object detection and space-time video understanding. It proposes improvements to the Multiscale Vision Transformers (MViT) architecture by enhancing pooling attention and integrating it with object detection frameworks like Mask R-CNN. The key contributions include decomposed relative position embedding, residual pooling connection, and Hybrid window attention for better performance across various visual recognition tasks.\",\n  \"Direct Inspiration\": [\"b54\", \"b21\"],\n  \"Indirect Inspiration\": [\"b17\", \"b35\", \"b52\"],\n  \"Other Inspiration\": [\"b22\", \"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in applying Vision Transformers (ViT) to high-resolution object detection and space-time video understanding tasks. The authors propose improvements to the Multiscale Vision Transformers (MViT) architecture, focusing on pooling attention, shift-invariant positional embeddings, and residual pooling connections. They aim to create a general vision backbone for image classification, object detection, and video classification.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b17\": 0.9,\n    \"b54\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.7,\n    \"b52\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b72\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in using Vision Transformers (ViT) for high-resolution object detection and space-time video understanding due to computational and memory complexity. It proposes improvements to Multiscale Vision Transformers (MViT) including shift-invariant positional embeddings and a residual pooling connection. The enhanced MViT is evaluated across image classification, object detection, and video classification tasks, showing significant performance improvements.\",\n  \"Direct Inspiration\": [\"b17\", \"b21\", \"b54\"],\n  \"Indirect Inspiration\": [\"b35\", \"b52\", \"b5\", \"b22\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in using Vision Transformers (ViT) for high-resolution object detection and space-time video understanding tasks due to computational and memory requirements. It proposes improvements to the Multiscale Vision Transformers (MViT), including shift-invariant positional embeddings and residual pooling connections. The paper studies MViT as a general vision backbone for image classification, object detection, and video classification, achieving state-of-the-art performance.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1,\n        \"b21\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b54\": 0.9,\n        \"b35\": 0.8,\n        \"b52\": 0.8,\n        \"b5\": 0.8\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of using Vision Transformers (ViT) for high-resolution object detection and space-time video understanding tasks due to their high compute and memory requirements. It proposes improvements to the Multiscale Vision Transformers (MViT) by introducing techniques such as decomposed relative position embeddings and residual pooling connections. The paper aims to enhance MViT's performance across image classification, object detection, and video classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b17\": 0.9,\n    \"b54\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.7,\n    \"b52\": 0.7,\n    \"b76\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b64\": 0.5\n  }\n}\n```"], "60bdde338585e32c38af4f97": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatic and adaptive selection of data augmentations for self-supervised learning on graph-structured data. The proposed JOAO framework automates the augmentation selection, optimizing a bi-level min-max objective inspired by adversarial training methods.\",\n  \"Direct Inspiration\": {\n    \"b59\": 1,\n    \"b53\": 0.9,\n    \"b55\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.7,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b26\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the diversity and heterogeneity of graph datasets, which previous self-supervised learning methods have not fully addressed. The main contribution is the introduction of the JOAO framework, which automates and optimizes the selection of data augmentations for graph contrastive learning, making the process more adaptive and dynamic. JOAO is designed to alleviate the need for manual tuning and empirical rules in augmentation selection, thus scaling up graph contrastive learning to a broader range of graph data types and scales.\",\n  \"Direct Inspiration\": {\n    \"b59\": 1.0,\n    \"b55\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b53\": 0.7,\n    \"b43\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b49\": 0.5,\n    \"b29\": 0.4,\n    \"b13\": 0.3\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the automatic selection of data augmentations for graph contrastive learning to avoid ad-hoc choices and tedious tuning. The proposed algorithm, JOAO, is a bi-level optimization framework that dynamically and automatically selects data augmentations, improving flexibility and ease of use without relying on labeled data.\",\n    \"Direct Inspiration\": {\n        \"b59\": 1,\n        \"b55\": 0.9,\n        \"b43\": 0.9,\n        \"b53\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.7,\n        \"b7\": 0.7,\n        \"b51\": 0.7,\n        \"b50\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.6,\n        \"b3\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatic data augmentation selection for graph contrastive learning, proposing a novel framework called JOAO. The JOAO framework aims to overcome the limitations of manual and ad-hoc augmentation selection by using a bi-level optimization approach to dynamically and adaptively select augmentations. This improves the flexibility, versatility, and robustness of graph contrastive learning methods without the need for labeled validation data.\",\n  \"Direct Inspiration\": {\n    \"b59\": 1.0,\n    \"b55\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b53\": 0.8,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatically selecting data augmentations for graph contrastive learning to overcome the limitations of ad-hoc choices and manual tuning. It proposes a joint augmentation optimization (JOAO) framework that is automatic, adaptive, and dynamic, integrating it with GraphCL for enhanced flexibility and applicability across diverse graph datasets.\",\n  \"Direct Inspiration\": {\n    \"b59\": 1.0,\n    \"b55\": 0.9,\n    \"b53\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b7\": 0.6,\n    \"b51\": 0.6\n  }\n}\n```"], "5ff68170d4150a363cb6cb0b": ["```json\n{\n  \"Summary\": \"The paper addresses the problem of course recommendation by proposing a model that integrates multi-modal heterogeneous data, including student profiles, interpersonal relationships, and evaluation texts, into a network structure. This model uses node2vec for embedding and DBSCAN for clustering, followed by Bayesian Probabilistic Tensor Factorization (BPTF) for rating prediction.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b11\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b5\": 0.7,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.5,\n    \"b13\": 0.45,\n    \"b14\": 0.4,\n    \"b16\": 0.35,\n    \"b17\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of course recommendation by leveraging a multi-modal heterogeneous network of student profiles, including academic grades, interpersonal relationships, and comment styles. It proposes a model based on Bayesian Probabilistic Tensor Factorization (BPTF) and embedding techniques such as node2vec and DBSCAN for personalized course recommendations.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b67\": 0.85,\n    \"b16\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b5\": 0.75,\n    \"b22\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b11\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the problem of information overload in course recommendation systems by proposing a novel model that incorporates a teaching evaluation network and Bayesian Probabilistic Tensor Factorization (BPTF). This model integrates heterogeneous data from multiple sources, such as student profiles, interpersonal relationships, and evaluation text, to improve recommendation accuracy.\",\n    \"Direct Inspiration\": [\"b12\", \"b67\", \"b13\", \"b14\", \"b15\"],\n    \"Indirect Inspiration\": [\"b16\", \"b17\", \"b18\"],\n    \"Other Inspiration\": [\"b11\", \"b4\", \"b5\", \"b6\"]\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses challenges in course recommendation systems, particularly information overload, and the inability to utilize the multi-modal heterogeneous data effectively. The authors propose a novel method combining embedding teaching evaluation networks and Bayesian Probabilistic Tensor Factorization (BPTF) to improve the accuracy of course recommendations.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b15\": 0.85,\n    \"b16\": 0.9,\n    \"b17\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.75,\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of information overload in course recommendations by proposing a course recommendation model based on embedding teaching evaluation networks and Bayesian Probabilistic Tensor Factorization (BPTF). The paper highlights the challenge of integrating multi-modal heterogeneous network features in traditional matrix factorization models and proposes a novel method that constructs a teaching evaluation network, utilizes node2vec and DBSCAN algorithms, and extends the rating matrix into a three-dimensional rating tensor for improved prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b6\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b4\": 0.6,\n    \"b10\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```"], "60377a1fd3485cfff1f0aa07": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of recommending scientific papers amidst information overload by proposing an attentive recurrent neural network (RNN) based recommendation model. The model integrates heterogeneous data from scientific publications to construct a unified bibliographic network, leveraging attention mechanisms to improve recommendation accuracy by considering authors' historical behaviors and preferences.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b12\": 1.0,\n    \"b13\": 1.0,\n    \"b14\": 1.0,\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.7,\n    \"b46\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b7\": 0.5,\n    \"b8\": 0.5,\n    \"b45\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of recommending scientific papers by leveraging heterogeneous and multi-modal data, proposing an attentive recurrent neural network (RNN) for personalized paper citation recommendations. Inspired by deep learning-based embedding representations and attention mechanisms, the authors construct a unified bibliographic network to model various entities and relations in research papers, enhancing recommendation accuracy by considering wider and deeper underlying features.\",\n  \"Direct Inspiration\": {\n    \"Inspired by the current development of the knowledge graph, we select the TransD [b46] method to vectorize the entities and relationships stored in a graph structure.\": \"b46\"\n  },\n  \"Indirect Inspiration\": {\n    \"The introduction of the attention mechanism has shown an outstanding utility to address challenging implicit feedback in recommendations [b13] [b14] [b15].\": [\"b13\", \"b14\", \"b15\"],\n    \"In recent years, deep learning-based embedding representation provides a new perspective on unifying heterogeneous features as well as an opportunity for exploiting a powerful deep recommending model to establish more accurate recommendation by considering wider and deeper underlying features including semantic, preferred and academic relations [b11] [b12].\": [\"b11\", \"b12\"]\n  },\n  \"Other Inspiration\": {\n    \"Existing recommendation methods demonstrate the feasibility of using different kinds of attributes and relationships from scientific papers as the basis for the recommendation, such as historical behavior and records, research content, citation network, etc. [b7] [b8].\": [\"b7\", \"b8\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of recommending scientific papers to researchers and students in the context of information overload due to the increasing volume of scholarly data. It proposes an attentive recurrent neural networks (RNN) model for paper citation recommendations, leveraging a heterogeneous bibliographic network and various features such as historical behaviors, citation networks, and semantic relations.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b13\": 0.9,\n    \"b46\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b12\": 0.7,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of recommending scientific papers to researchers by leveraging multi-modal, multi-sourced data. It introduces an attentive recurrent neural network (RNN) model that uses a heterogeneous bibliographic network and deep learning-based embedding to unify heterogeneous features and improve recommendation accuracy.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b12\": 0.9,\n    \"b13\": 0.85,\n    \"b14\": 0.85,\n    \"b15\": 0.85,\n    \"b46\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.75,\n    \"b10\": 0.75,\n    \"b45\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of recommending scientific papers to researchers to mitigate information overload caused by the vast amount of academic data. The proposed solution involves constructing a heterogeneous bibliographic network and using an attentive recurrent neural network (RNN) to make personalized recommendations based on a unified knowledge representation of papers, scholars, and other entities.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b12\": 0.9,\n    \"b13\": 0.85,\n    \"b14\": 0.85,\n    \"b15\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b45\": 0.75,\n    \"b46\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7\n  }\n}\n```"], "618c94d86750f806be668a0f": ["```json\n{\n  \"Summary\": \"The paper addresses the problem of retrieving relevant passages given a query in open-domain question answering systems. The primary challenge is the selection of negative examples in contrastive learning for training dense passage retrieval models. The proposed solution is Cross Momentum Contrastive Learning (xMoCo), which employs two sets of fast/slow encoders for questions and passages to improve training efficiency and effectiveness.\",\n  \"Direct Inspiration\": [\"b9\", \"b12\"],\n  \"Indirect Inspiration\": [\"b18\", \"b19\", \"b20\"],\n  \"Other Inspiration\": [\"b7\", \"b14\", \"b21\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of retrieving relevant passages for question answering (QA) systems, proposing a new contrastive learning method called Cross Momentum Contrastive Learning (xMoCo) to improve dense passage retrieval. This method employs two sets of fast/slow encoders and jointly optimizes the question-passage and passage-question matching tasks, overcoming limitations of traditional momentum contrastive learning in this context.\",\n    \"Direct Inspiration\": {\n        \"b9\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b12\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.7,\n        \"b19\": 0.75\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of retrieving relevant passages for open-domain question answering (QA) systems, which traditionally rely on sparse keyword matching. The authors propose a new contrastive learning method called Cross Momentum Contrastive Learning (xMoCo) to improve dense passage retrieval by efficiently maintaining a large number of negative examples, thereby enhancing the learning consistency during the training and inference phases.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving dense passage retrieval in open-domain QA systems. It proposes a novel contrastive learning method called Cross Momentum Contrastive Learning (xMoCo) to handle the non-interchangeability of questions and passages, which require different encoders. This method aims to maintain a large number of negative examples efficiently and jointly optimizes question-passage and passage-question matching tasks. The approach is tested on several QA tasks, showing its effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b19\": 0.8,\n    \"b20\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b6\": 0.6,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of retrieving relevant passages for open-domain question answering by introducing a novel contrastive learning method called Cross Momentum Contrastive Learning (xMoCo). The key innovation is the use of separate fast/slow encoders for both questions and passages, which allows for maintaining a large number of negative examples efficiently and optimizes both question-to-passage and passage-to-question matching tasks.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b14\": 0.6\n  }\n}\n```"], "6080029491e011772654f794": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning universal sentence embeddings, proposing a novel method called SimCSE. This method utilizes a simple contrastive framework with dropout noise for unsupervised learning and leverages natural language inference (NLI) datasets for supervised learning. The primary contributions include demonstrating the effectiveness of dropout as minimal data augmentation and proving that the contrastive learning objective improves uniformity and alignment in sentence embeddings.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b42\": 0.9,\n    \"b49\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b29\": 0.7,\n    \"b28\": 0.6,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.5,\n    \"b30\": 0.5,\n    \"b44\": 0.5,\n    \"b16\": 0.4,\n    \"b17\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents SimCSE, a contrastive learning framework for sentence embeddings that leverages both unsupervised and supervised approaches. The main challenges addressed include improving the quality of sentence embeddings using a contrastive objective and dropout noise for unsupervised learning, and utilizing Natural Language Inference (NLI) datasets for supervised learning.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b42\": 1.0,\n    \"b49\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b29\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.6,\n    \"b45\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning universal sentence embeddings by proposing a novel contrastive learning framework called SimCSE, which uses dropout noise to create positive pairs for unsupervised learning and leverages natural language inference (NLI) datasets for supervised learning.\",\n  \"Direct Inspiration\": [\"b49\", \"b12\", \"b42\", \"b30\"],\n  \"Indirect Inspiration\": [\"b13\", \"b29\", \"b28\", \"b57\"],\n  \"Other Inspiration\": [\"b26\", \"b45\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning universal sentence embeddings, proposing a novel method called SimCSE. SimCSE leverages a contrastive objective with pre-trained language models and uses dropout noise as a form of data augmentation. The primary contribution is the demonstration that both unsupervised and supervised SimCSE largely outperform existing methods, with the latter utilizing Natural Language Inference (NLI) datasets for improved performance.\",\n  \"Direct Inspiration\": [\"b12\", \"b42\", \"b49\"],\n  \"Indirect Inspiration\": [\"b26\", \"b30\", \"b28\", \"b45\"],\n  \"Other Inspiration\": [\"b13\", \"b29\", \"b44\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents SimCSE, a simple contrastive sentence embedding framework using dropout noise for unsupervised learning and NLI datasets for supervised learning. It addresses the challenges of sentence embedding by leveraging pre-trained language models and contrastive learning objectives to improve alignment and uniformity, thus overcoming representation degeneration and achieving superior performance in semantic textual similarity and transfer tasks.\",\n  \"Direct Inspiration\": [\"b12\", \"b42\", \"b49\"],\n  \"Indirect Inspiration\": [\"b13\", \"b29\", \"b16\", \"b28\"],\n  \"Other Inspiration\": [\"b30\", \"b45\", \"b26\"]\n}\n```"], "602f902591e01191ea516581": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the need to handle complex and dynamic nature of thunderstorms for long-term prediction, inherent biases in NWP-based predictions, and the difficulty in extracting valuable knowledge from massive heterogeneous spatiotemporal data. The proposed algorithm, Heterogeneous Spatiotemporal Network (HSTN), aims to mine knowledge from several heterogeneous data sources using a combination of a Gaussian diffusion module, ST encoder, and ST decoder.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of predicting lightning by proposing a Heterogeneous Spatiotemporal Network (HSTN). The primary challenges include the complex and dynamic nature of thunderstorms, deviations in NWP simulations, and the difficulty of extracting valuable knowledge from heterogeneous spatiotemporal data. The proposed HSTN model aims to mine knowledge from various data sources to improve prediction accuracy.\",\n  \"Direct Inspiration\": [\"b11\", \"b12\"],\n  \"Indirect Inspiration\": [\"b9\", \"b10\", \"b23\"],\n  \"Other Inspiration\": [\"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of lightning prediction by proposing a Heterogeneous Spatiotemporal Network (HSTN). The main difficulties include the complex nature of thunderstorms, deviations in NWP simulations, and the scattered valuable knowledge in massive heterogeneous spatiotemporal data. The HSTN model extracts and fuses information from various heterogeneous data sources to improve prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in lightning prediction due to the dynamic and complex nature of thunderstorms, biases in NWP simulations, and the difficulty in extracting valuable knowledge from heterogeneous spatiotemporal (ST) data. The authors propose a Heterogeneous Spatiotemporal Network (HSTN) designed to mine knowledge from several heterogeneous ST data sources, utilizing modules like Gaussian diffusion module, ST encoder, and ST decoder to handle these complexities.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b19\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of predicting lightning by proposing a heterogeneous spatiotemporal network (HSTN) that mines knowledge from several heterogeneous spatiotemporal (ST) data sources. The main contributions include a Gaussian diffusion module to convert sparse ST tensors into dense form, an ST encoder to extract task-related information, and an ST decoder to fuse all information for the final prediction. The approach aims to improve prediction accuracy by leveraging multi-source data and addressing the limitations of traditional nowcasting and NWP-based methods.\",\n    \"Direct Inspiration\": {\n        \"b11\": 0.9,\n        \"b12\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.7,\n        \"b10\": 0.7,\n        \"b22\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.5,\n        \"b1\": 0.5,\n        \"b2\": 0.5\n    }\n}\n```"], "6065b38291e011d10ad615b2": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of developing intuitive text-based interfaces for manipulating images generated by StyleGAN. It proposes leveraging the power of CLIP models to create text-guided image manipulation methods that do not require manual effort or extensive annotated data. The three main contributions include a text-guided latent optimization scheme, a latent mapper for local manipulations, and a method for mapping text prompts to global directions in StyleGAN's style space.\",\n    \"Direct Inspiration\": {\n        \"b33\": 1.0,\n        \"b17\": 0.9,\n        \"b49\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b44\": 0.8,\n        \"b45\": 0.8,\n        \"b50\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.7,\n        \"b40\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling intuitive text-based semantic image manipulation using StyleGAN and the recently introduced CLIP models. It proposes three primary techniques: text-guided latent optimization, a latent residual mapper, and a method for mapping text prompts to input-agnostic directions in StyleGAN's style space.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b17\": 0.9,\n    \"b45\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b49\": 0.7,\n    \"b50\": 0.6,\n    \"b40\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b37\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper aims to leverage the power of CLIP models with StyleGAN to develop a text-based interface for image manipulation without manual effort. The primary challenges include discovering semantically meaningful latent manipulations and enabling intuitive text-based image manipulation. The authors propose three methods: text-guided latent optimization, a latent residual mapper, and a method for mapping text prompts to input-agnostic directions in StyleGAN's style space.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1,\n    \"b50\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b45\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b49\": 0.7,\n    \"b12\": 0.65,\n    \"b0\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of manipulating images in StyleGAN's latent space using text inputs without manual effort. The authors introduce three techniques: text-guided latent optimization, a latent residual mapper, and a method for mapping text prompts to input-agnostic directions in StyleGAN's style space. These methods leverage the capabilities of CLIP models to enable intuitive and flexible image manipulations.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b50\": 0.8,\n    \"b45\": 0.7,\n    \"b17\": 0.7,\n    \"b49\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of text-based image manipulation using StyleGAN, which usually requires significant manual effort or annotated data. It proposes leveraging CLIP models to develop a more intuitive, text-based interface for manipulating images generated by StyleGAN. The paper introduces three main methods: text-guided latent optimization, a latent residual mapper, and global direction mapping in StyleGAN's style space.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b49\": 0.9,\n    \"b17\": 0.8,\n    \"b45\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b50\": 0.7,\n    \"b40\": 0.6,\n    \"b0\": 0.6\n  }\n}\n```"], "60829fe091e0118612e3f5b6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Transformers using large-scale, unlabeled visual data for computer vision tasks. The proposed solution, VATT, involves self-supervised, multimodal pre-training of Transformers on raw RGB frames, audio waveforms, and text transcripts from videos. Key inspirations include the success of Transformers in NLP and previous work in vision Transformers.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b22\": 1.0,\n    \"b24\": 1.0,\n    \"b87\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b69\": 0.8,\n    \"b70\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b105\": 0.7,\n    \"b106\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of leveraging large-scale, unlabeled visual data to pre-train Transformers for multimodal tasks, including image classification, video action recognition, and audio event classification. The proposed algorithm, VATT, utilizes a self-supervised multimodal learning strategy inspired by NLP models like BERT and GPT, employing contrastive learning for alignment across video, audio, and text modalities.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b24\": 1,\n    \"b69\": 1,\n    \"b70\": 1,\n    \"b9\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b87\": 0.9,\n    \"b8\": 0.8,\n    \"b4\": 0.8,\n    \"b107\": 0.7,\n    \"b106\": 0.7,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b63\": 0.5,\n    \"b101\": 0.5,\n    \"b102\": 0.5,\n    \"b94\": 0.5,\n    \"b39\": 0.5,\n    \"b16\": 0.5,\n    \"b99\": 0.5,\n    \"b36\": 0.5,\n    \"b40\": 0.5,\n    \"b84\": 0.5,\n    \"b11\": 0.5,\n    \"b17\": 0.5,\n    \"b81\": 0.5,\n    \"b89\": 0.5,\n    \"b7\": 0.5,\n    \"b90\": 0.5,\n    \"b55\": 0.5,\n    \"b37\": 0.5,\n    \"b38\": 0.5,\n    \"b53\": 0.5,\n    \"b96\": 0.5,\n    \"b44\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is empowering Transformers with large-scale, unlabeled visual data to address the limitations of supervised training. The proposed algorithm, VATT (Video, Audio, Text Transformer), leverages self-supervised multimodal pre-training inspired by techniques from NLP, particularly BERT and GPT, to handle raw signals from videos, audio, and text. The VATT architecture borrows from BERT and ViT, incorporating a minimal-change approach to facilitate transfer learning across various tasks. The paper also introduces DropToken to reduce training complexity.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b24\": 1.0,\n    \"b87\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b69\": 0.8,\n    \"b70\": 0.8,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b4\": 0.6,\n    \"b21\": 0.5,\n    \"b107\": 0.4,\n    \"b106\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of leveraging large-scale, unlabeled visual data to train Transformers for computer vision tasks without relying on extensive supervised training. The proposed method, VATT, utilizes a self-supervised, multimodal pre-training approach using raw RGB frames, audio waveforms, and text transcripts from internet videos. Key contributions include the adaptation of BERT and ViT architectures, the development of DropToken to reduce training complexity, and achieving state-of-the-art results on various downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b22\": 1.0,\n    \"b0\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b87\": 0.7,\n    \"b9\": 0.7,\n    \"b69\": 0.7,\n    \"b70\": 0.7,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include overcoming the limitations of large-scale supervised training of Transformers due to the reliance on labeled data and computational complexity. The proposed algorithm, VATT, addresses these challenges by leveraging self-supervised, multimodal pre-training using raw video, audio, and text data. The paper introduces a modality-agnostic Transformer and a technique called DropToken to reduce training complexity without significantly impacting performance.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b24\": 1,\n    \"b87\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b9\": 0.8,\n    \"b69\": 0.8,\n    \"b70\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b2\": 0.7,\n    \"b58\": 0.7\n  }\n}\n```"], "60da8fc20abde95dc965f701": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of negative knowledge infusion in pre-trained language models (LMs) by proposing selective injection and spectral regularization methods to mitigate the detrimental effects of redundant and irrelevant knowledge. The study is motivated by observations that excessive and indiscriminate knowledge infusion can lead to performance degradation in downstream tasks.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b5\": 1.0,\n        \"b6\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.8,\n        \"b12\": 0.8,\n        \"b13\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.5,\n        \"b10\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of negative knowledge infusion in pre-trained language models (LMs) by proposing selective injection and spectral regularization methods. The authors argue that redundant and irrelevant knowledge can deteriorate downstream task performance and propose techniques to mitigate this issue. The study is inspired by previous works demonstrating the existence of relational knowledge in pre-trained LMs and the negative effects of excessive knowledge incorporation.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b5\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b9\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of negative knowledge infusion in pre-trained language models (LMs) when incorporating external knowledge, which can result in performance degradation due to redundant or irrelevant knowledge. The authors propose two main approaches to mitigate this issue: selective injection of informative knowledge and spectral regularization to suppress irrelevant knowledge components. The work is motivated by previous findings that indicate the existence of redundant and irrelevant knowledge in pre-trained LMs and aims to enhance the effectiveness of knowledge infusion.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b1\": 0.9,\n    \"b5\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The main challenges outlined in the paper are the issues of redundant and irrelevant knowledge infusion into pre-trained language models (LMs) leading to negative knowledge infusion, which deteriorates the performance of downstream tasks.\",\n    \"inspirations\": \"The paper proposes a selective injection mechanism and spectral regularization to effectively manage the infusion of external knowledge into pre-trained LMs.\"\n  },\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b1\": 0.9,\n    \"b5\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b10\": 0.6,\n    \"b9\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of negative knowledge infusion in pre-trained language models (LMs) and proposes methods to mitigate this problem by selective injection of informative knowledge and spectral regularization. The main challenges include understanding the detrimental impact of redundant and irrelevant knowledge on downstream tasks. The proposed algorithm involves selective knowledge injection based on frequency and mutual reachability, and spectral regularization to suppress irrelevant knowledge.\",\n  \"Direct Inspiration\": [\"b6\", \"b1\", \"b5\", \"b13\"],\n  \"Indirect Inspiration\": [\"b12\", \"b10\", \"b9\"],\n  \"Other Inspiration\": []\n}\n```"], "600fe5e7d4150a363c1fa9b7": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of automatic building extraction from high-resolution remote sensing images by proposing a novel deep learning-based method called DABE-Net. This method incorporates squeeze-and-excitation operations, recurrent convolutional neural networks, and attention mechanisms to improve segmentation accuracy, particularly for small buildings. The approach aims to balance samples and focus on buildings of different scales, outperforming existing methods in complex scenes.\",\n    \"Direct Inspiration\": {\n        \"b29\": 1.0,\n        \"b30\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.7,\n        \"b24\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5,\n        \"b27\": 0.5,\n        \"b28\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in automatic building extraction from high-resolution remote sensing images, especially in complex scenes with varied building structures and low contrast with surroundings. The proposed method, DABE-Net, incorporates squeeze-and-excitation (SE) operations, a residual recurrent convolutional neural network (RRCNN), and attention mechanisms to enhance segmentation accuracy and handle buildings of different scales.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b29\": 1.0,\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatic building extraction from high-resolution remote sensing images, proposing the novel Deep Automatic Building Extraction Network (DABE-Net). This network incorporates squeeze-and-excitation (SE) operations, residual recurrent convolutional neural network (RRCNN), and attention mechanisms to enhance segmentation accuracy, particularly for small buildings.\",\n  \"Direct Inspiration\": [\"b14\", \"b29\", \"b30\"],\n  \"Indirect Inspiration\": [\"b24\", \"b27\", \"b28\"],\n  \"Other Inspiration\": [\"b35\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatic building extraction from remote sensing images, particularly in complex scenes with diverse building structures and surrounding obstacles. The proposed method, DABE-Net, utilizes squeeze-and-excitation (SE) operations, residual recurrent convolutional neural network (RRCNN) blocks, and attention mechanisms to improve segmentation accuracy, especially for small buildings.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b29\": 0.95,\n    \"b30\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.85,\n    \"b28\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.75,\n    \"b27\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of automatic building extraction from remote sensing images, particularly in complex urban environments. The proposed method, DABE-Net, introduces SE and RRCNN blocks along with attention mechanisms to enhance feature discrimination and segmentation accuracy. The novelty lies in the incorporation of these blocks and a multi-scale segmentation loss function to balance sample scales.\",\n  \"Direct Inspiration\": {\n    \"1\": \"b29\",\n    \"2\": \"b30\"\n  },\n  \"Indirect Inspiration\": {\n    \"1\": \"b14\",\n    \"2\": \"b17\",\n    \"3\": \"b24\"\n  },\n  \"Other Inspiration\": {\n    \"1\": \"b2\",\n    \"2\": \"b35\"\n  }\n}\n```"], "604b3d8e91e0110eed64c3a6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing a large-scale Chinese multi-modal pre-training model, 'WenLan,' inspired by the success of pre-training models in NLP and the advanced multi-modal models like OpenAI CLIP. The proposed model adopts a two-tower architecture and introduces an advanced cross-modal contrastive learning algorithm (CMCL) to improve image-text retrieval performance on noisy web-crawled data.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b5\": 0.9,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b3\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the development of a large-scale Chinese multi-modal pre-training model, CMCL, which adopts a two-tower architecture with contrastive learning to improve image-text retrieval and image captioning tasks. The primary challenges addressed include the weak semantic correlation between image-text pairs in real-world scenarios and the need for improved representation ability in multi-modal pre-training models.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b34\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.6,\n    \"b20\": 0.5,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effective image-text retrieval in multimodal pre-training models. It proposes a novel cross-modal contrastive learning (CMCL) model with a two-tower architecture, inspired by OpenAI CLIP, but introducing more advanced techniques like an expanded MoCo framework to enhance representation ability and handle weakly correlated image-text pairs.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b20\": 0.6,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include bridging the gap between vision and language through multi-modal pre-training models, and improving the representation ability of models in noisy real-world scenarios. The proposed CMCL model adopts a two-tower architecture with advanced cross-modal contrastive learning to address these challenges. The inspiration for the work primarily comes from recent advances in self-supervised learning and contrastive learning methods, as well as prior work on multi-modal models like OpenAI CLIP.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b22\": 0.8,\n    \"b3\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b20\": 0.7,\n    \"b31\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the development of a multi-modal pre-training model that can effectively handle image-text retrieval tasks, particularly when dealing with noisy, web-crawled data. The authors propose a new model called CMCL, which employs a two-tower architecture and introduces advanced cross-modal contrastive learning techniques to improve representation ability. The model leverages a large queue-based dictionary for incorporating more negative samples and utilizes self-supervised learning and contrastive learning strategies to achieve better performance.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b15\": 0.9,\n    \"b5\": 0.9,\n    \"b2\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b12\": 0.7,\n    \"b4\": 0.7,\n    \"b22\": 0.6,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.5,\n    \"b29\": 0.5,\n    \"b31\": 0.5\n  }\n}\n```"], "619eff455244ab9dcbddef84": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"High-dimensional and redundant modality of images compared to text.\",\n      \"Difference in semantic meaning between discrete text tokens and visual tokens.\",\n      \"Vulnerability of low-level vision objectives like per-pixel loss.\"\n    ],\n    \"inspirations\": [\n      \"Inspired by the success of Transformer-based models in NLP and their adaptation in computer vision.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b1\", \"b49\", \"b15\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b8\", \"b25\", \"b34\", \"b54\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b21\", \"b29\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning perceptual discrete visual tokens for image pre-training, introducing a perceptual codebook (PeCo) using VQ-VAE. This method aims to improve subsequent pre-training tasks by enforcing perceptual similarity between the original and reconstructed images using deep visual features from a self-supervised trained network.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b49\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b29\": 0.8,\n    \"b54\": 0.8,\n    \"b21\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b25\": 0.6,\n    \"b34\": 0.6,\n    \"b11\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the development of a perceptual codebook (PeCo) to facilitate image pre-training tasks with perceptual similarity in a VQ-VAE framework. The key challenges addressed include the difficulty of designing objectives for vision similar to those in NLP due to the high-dimensional and redundant nature of images. The proposed method aims to learn perceptual discrete visual tokens that capture semantic meanings and improve pre-training effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b49\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.9,\n    \"b21\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b54\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of designing self-supervised learning objectives for images, which differ from texts due to their high-dimensional and redundant nature. Inspired by the success of self-attention in NLP, the authors propose learning a perceptual codebook (PeCo) for images using a vector quantized variational autoencoder (VQ-VAE) with perceptual loss. This method aims to facilitate image pre-training tasks and improve performance on downstream tasks such as image classification, object detection, and semantic segmentation.\",\n    \"Direct Inspiration\": [\"b1\", \"b49\"],\n    \"Indirect Inspiration\": [\"b10\", \"b21\", \"b29\"],\n    \"Other Inspiration\": [\"b15\", \"b54\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is designing effective pre-training tasks for vision transformers, given that images, unlike texts, are high-dimensional and redundant. The proposed algorithm, PeCo, introduces a perceptual codebook for images to facilitate pre-training by enforcing perceptual similarity between the original and reconstructed images using a vector quantized variational autoencoder (VQ-VAE). The approach leverages self-supervised learning for deep visual features, showing improved performance in various downstream tasks compared to existing methods like BEiT and MAE.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b49\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.7,\n        \"b29\": 0.7,\n        \"b21\": 0.6,\n        \"b15\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.5,\n        \"b25\": 0.5\n    }\n}\n```"], "618516a96750f82bde97530f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating a unified framework for various information extraction tasks in a task-agnostic setting, overcoming limitations of task-specific models and data scarcity. The proposed DEEPEX framework treats information extraction as a text-to-triple translation problem, leveraging pre-trained language models for zero-shot performance.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b30\": 0.8,\n    \"b37\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of task-specific pipelines in information extraction, proposing DEEPEX, a unified framework that treats all information extraction tasks as text-to-triple problems. The framework leverages pre-trained language models for a zero-shot translation process, achieving competitive performance across multiple datasets without requiring task-specific labeled data.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b30\": 1,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b23\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of task-specific pipelines in information extraction and the scarcity of labeled data for such tasks. It proposes a unified framework, DEEPEX, for solving information extraction tasks in a zero-shot setting by treating them as text-to-triple problems. The framework leverages pre-trained language models and a task-agnostic corpus to generate and rank candidate triples. The key contributions include the application of the framework to various tasks, achieving competitive zero-shot performance, and demonstrating the interpretability and transparency of the approach.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b30\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b26\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b36\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of task-specific pipelines and the scarcity of labeled data in information extraction (IE). It proposes a unified, task-agnostic framework called DEEPEX that treats every IE problem as a 'text-to-triple' problem. This framework leverages pre-trained language models (LMs) for zero-shot performance across multiple IE tasks, achieving competitive results without task-specific training.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b30\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b23\": 0.75,\n        \"b37\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b26\": 0.6,\n        \"b4\": 0.6,\n        \"b0\": 0.65\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in information extraction, particularly the difficulty of leveraging a single pipeline for multiple tasks and the scarcity of task-specific labeled data. It proposes a unified framework called DEEPEX, which translates input text to output triples, enabling zero-shot performance on various information extraction tasks, including open information extraction, relation classification, and factual probing.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b30\": 0.85,\n        \"b37\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b23\": 0.75,\n        \"b36\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.65,\n        \"b4\": 0.6,\n        \"b26\": 0.6\n    }\n}\n```"], "60b1a60491e0115374595823": ["```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper revolve around improving text-to-image generation. The authors propose a novel algorithm, CogView, which utilizes a large-scale generative joint pretraining approach for both text and image tokens. The paper outlines significant improvements over previous methods, such as DALL-E, by introducing the Precision Bottleneck Relaxation and Sandwich LayerNorm techniques to stabilize training and enhance the quality of generated images.\",\n  \"Direct Inspiration\": {\n    \"b38\": 0.95,\n    \"b45\": 0.9,\n    \"b14\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.75,\n    \"b19\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b54\": 0.65,\n    \"b44\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of text-to-image generation by leveraging a novel large-scale generative joint pretraining approach using Transformers. The proposed model, CogView, utilizes techniques such as Precision Bottleneck Relaxation and Sandwich Layernorm to stabilize training and improve the quality of generated images. CogView is compared with DALL-E and demonstrates superior performance in various aspects, including zero-shot generation, finetuning for downstream tasks, and self-reranking.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1,\n    \"b45\": 0.9,\n    \"b19\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b28\": 0.7,\n    \"b3\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.6,\n    \"b32\": 0.6,\n    \"b46\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of text-to-image generation by proposing the CogView model, which leverages a large-scale generative joint pretraining for both text and image tokens. The main contributions include improvements in model stability, tokenization techniques, and finetuning capabilities, along with the introduction of Precision Bottleneck Relaxation and Sandwich Layernorm to stabilize the training of large Transformers.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.9,\n    \"b14\": 0.85,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b41\": 0.65,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in text-to-image generation, particularly focusing on disentangling features from pixels, understanding input text, aligning objects with words, and generating complex distributions. The proposed CogView model leverages a large-scale generative joint pretraining approach using VQ-VAE and Transformer architectures, introducing Precision Bottleneck Relaxation and Sandwich Layernorm techniques to stabilize training on heterogeneous data.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.9,\n    \"b46\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.7,\n    \"b3\": 0.7,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving text-to-image generation by proposing the CogView model. The model leverages a large-scale generative joint pretraining approach for both text and image tokens using a Transformer with 4 billion parameters. Key innovations include the Precision Bottleneck Relaxation and Sandwich Layernorm techniques to stabilize training on complex datasets, leading to significant improvements in generation quality and adaptability to diverse downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1.0,\n    \"b45\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b14\": 0.8,\n    \"b36\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.5,\n    \"b47\": 0.5\n  }\n}\n```"], "6189e6d15244ab9dcb76e8da": ["```json\n{\n  \"Summary\": \"The paper addresses the limitations in existing Graph Machine Learning (GML) models' robustness against adversarial attacks and proposes the Graph Robustness Benchmark (GRB) to provide a unified, scalable, and reproducible framework for evaluating the adversarial robustness of GML models. The primary challenges include unrealistic attack/defense scenarios, lack of a unified evaluation protocol, and lack of scalability in current solutions.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.9,\n    \"b26\": 0.8,\n    \"b27\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.75,\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes the Graph Robustness Benchmark (GRB) to address the limitations in evaluating the adversarial robustness of Graph Machine Learning (GML) models. The key challenges identified include unrealistic attack/defense scenarios, lack of a unified evaluation protocol, and scalability issues. The GRB aims to provide a reproducible framework for fair evaluation of adversarial attacks and defenses on GML models under unified settings.\",\n  \"Direct Inspiration\": [\"b20\", \"b21\", \"b22\", \"b23\", \"b24\"],\n  \"Indirect Inspiration\": [\"b10\", \"b11\", \"b12\", \"b15\", \"b16\"],\n  \"Other Inspiration\": [\"b25\", \"b26\", \"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the adversarial robustness of Graph Machine Learning (GML) models, highlighting challenges such as unrealistic attack/defense scenarios, lack of a unified evaluation protocol, and scalability issues. It proposes the Graph Robustness Benchmark (GRB) to provide a reproducible framework for fair evaluation of adversarial attacks and defenses on GML models under unified settings.\",\n  \"Direct Inspiration\": [\"b20\", \"b21\", \"b22\", \"b23\", \"b24\"],\n  \"Indirect Inspiration\": [\"b25\", \"b26\", \"b27\"],\n  \"Other Inspiration\": [\"b10\", \"b11\", \"b12\", \"b15\", \"b16\", \"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of Graph Machine Learning (GML) models to adversarial attacks by proposing the Graph Robustness Benchmark (GRB). It identifies key challenges such as unrealistic attack/defense scenarios, lack of a unified evaluation protocol, and lack of scalability in existing works. GRB provides a reproducible framework for evaluating adversarial robustness with refined attack/defense scenarios, scalable and unified evaluations, and public leaderboards.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b21\": 1.0,\n    \"b22\": 1.0,\n    \"b23\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b27\": 0.7,\n    \"b31\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in evaluating the adversarial robustness of Graph Machine Learning (GML) models, highlighting the limitations such as unrealistic attack/defense scenarios, lack of a unified evaluation protocol, and lack of scalability. The authors propose the Graph Robustness Benchmark (GRB) to provide a reproducible framework for fair evaluation of adversarial attacks and defenses on GML models under unified settings.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b23\": 0.85,\n    \"b24\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b11\": 0.75,\n    \"b12\": 0.75,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b28\": 0.7,\n    \"b30\": 0.7,\n    \"b33\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.65,\n    \"b27\": 0.65,\n    \"b36\": 0.6,\n    \"b37\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```"], "6001719f91e01163e836a4b6": ["```json\n{\n  \"Summary\": \"The paper introduces TANL, a framework for structured prediction tasks in NLP using a unified approach with augmented natural languages. The main challenge addressed is the variability and complexity of structured prediction tasks, traditionally handled by task-specific models. The proposed method leverages the generative power of transformer models to perform these tasks without the need for specialized modules, showing state-of-the-art performance across multiple datasets.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b45\": 0.7,\n    \"b63\": 0.7,\n    \"b68\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b24\": 0.5,\n    \"b55\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces TANL, a framework for structured prediction tasks in NLP, using a unified approach that casts these tasks as translation problems with augmented natural languages. The key contributions include achieving state-of-the-art results on multiple datasets, demonstrating the framework's robustness in low-resource settings, and enabling multi-task learning without task-specific modules.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b45\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b10\": 0.8,\n    \"b63\": 0.8,\n    \"b68\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b9\": 0.6,\n    \"b26\": 0.6,\n    \"b30\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are the complexity of structured prediction tasks in NLP, such as entity and relation extraction, semantic role labeling, and coreference resolution. The novel method proposed is the TANL framework, which translates structured prediction tasks into text-to-text translation problems using augmented natural languages. The key contributions include the use of a unified architecture for multiple tasks, robust alignment algorithms, and effective multi-task learning without task-specific modules.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1.0,\n        \"b45\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.8,\n        \"b10\": 0.7,\n        \"b32\": 0.7,\n        \"b68\": 0.7,\n        \"b63\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b24\": 0.5,\n        \"b6\": 0.5,\n        \"b55\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is solving various structured prediction tasks in a unified manner using a generative approach. The authors propose the TANL model, which frames these tasks as text-to-text translation problems using augmented natural languages. This approach allows the same model architecture and hyperparameters to be used across different tasks, improving performance, especially in low-data regimes, and handling nested entities and multiple relations robustly.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b36\": 0.9,\n    \"b45\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.85,\n    \"b68\": 0.8,\n    \"b63\": 0.85,\n    \"b57\": 0.8,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.75,\n    \"b10\": 0.7,\n    \"b26\": 0.65,\n    \"b30\": 0.6,\n    \"b37\": 0.55,\n    \"b41\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces TANL, a novel framework for structured prediction tasks in NLP. It frames these tasks as text-to-text translation problems using augmented natural languages. The primary challenges addressed include handling nested entities, multiple relations, and performing well in low-resource settings. The framework leverages out-of-the-box transformer models to generate state-of-the-art results across various datasets without task-specific modules.\",\n  \"Direct Inspiration\": [\"b0\", \"b45\", \"b32\", \"b68\", \"b63\"],\n  \"Indirect Inspiration\": [\"b8\", \"b30\", \"b10\", \"b26\"],\n  \"Other Inspiration\": [\"b4\", \"b57\", \"b38\"]\n}\n```"], "60d996c80abde95dc965f65d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in verifying medical entity relations within medical knowledge graphs. It highlights three key challenges: handling medical synonyms, utilizing medical domain knowledge for verification, and providing high-quality evidence in the absence of labeled data. The proposed solution is a three-stage pipeline system incorporating document retrieval, synonym-aware sentence selection, and machine reading comprehension-based semantic relation verification. The paper also introduces an interactive collaborative-training method to iteratively improve evidence accuracy.\",\n  \"Direct Inspiration\": [\"b8\", \"b23\"],\n  \"Indirect Inspiration\": [\"b32\"],\n  \"Other Inspiration\": [\"b9\", \"b14\", \"b24\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in verifying medical entity relations in data-driven medical knowledge graphs. It introduces a three-stage pipeline system for large-scale machine reading comprehension, which includes modules for document retrieval, synonym-aware sentence selection, and machine reading comprehension-based semantic relation verification. The novel methods introduced include the use of a fine-tuned ERNIE model, a synonym-aware sentence selection module, and an interactive collaborative-training method to improve evidence accuracy iteratively.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b32\": 0.9,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b14\": 0.8,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Handling medical term variants and synonyms.\",\n      \"Utilizing medical domain knowledge for entity relation verification.\",\n      \"Providing high-quality evidence without extensive labeled data.\"\n    ],\n    \"Algorithm\": \"A three-stage pipeline system consisting of Document Retrieval, Synonym-Aware Sentence Selection, and Machine Reading Comprehension-based Semantic Relation Verification (MSRV) with an Interactive Collaborative-Training (ICT) method.\"\n  },\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b32\": 0.9,\n    \"b23\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.75,\n    \"b14\": 0.7,\n    \"b6\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in medical relation verification, specifically handling medical synonyms, using domain-specific knowledge, and improving evidence quality without ample labeled data. The proposed method involves a three-stage pipeline: document retrieval, synonym-aware sentence selection, and machine reading comprehension-based verification, enhanced by an interactive collaborative-training method.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.85,\n    \"b14\": 0.8,\n    \"b24\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b10\": 0.65,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in verifying medical entity relations in knowledge graphs, focusing on synonym variations, utilization of medical domain knowledge, and lack of labeled evidence. The authors propose a three-stage pipeline system incorporating document retrieval, synonym-aware sentence selection, and machine reading comprehension-based semantic relation verification.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.85,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.75,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b31\": 0.7\n  }\n}\n```"], "6051b7739e795e94b6453a23": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the poor performance of unsupervised contact prediction for proteins with low-depth MSAs. The proposed algorithm, the Neural Potts Model (NPM), aims to improve contact prediction by training a Transformer model to predict the parameters of a Potts model energy function across many protein families with parameter sharing.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b19\": 1.0,\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b12\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b41\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting protein contact maps from multiple sequence alignments (MSAs) with low depth using a novel Neural Potts Model (NPM). The approach leverages parameter sharing across many protein families using a Transformer model to predict parameters of a Potts model energy function for each input sequence. The goal is to improve unsupervised contact prediction by sharing information across related sequence families, especially for proteins with limited evolutionary data.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b33\": 0.9,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b42\": 0.7,\n    \"b45\": 0.7,\n    \"b3\": 0.6,\n    \"b12\": 0.6,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.5,\n    \"b20\": 0.5,\n    \"b41\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Detecting co-evolutionary patterns in MSAs for proteins with low-depth MSAs.\",\n      \"Improving unsupervised contact prediction performance.\",\n      \"Generalizing information across many protein families using parameter sharing.\"\n    ],\n    \"inspirations\": [\n      \"The potential of protein language models [b0], [b33], [b19].\",\n      \"Issues with fitting Potts models to low-depth MSAs [b20], [b21], [b29].\",\n      \"Amortized optimization techniques [b36], [b23], [b32].\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b33\": 0.9,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b29\": 0.6,\n    \"b36\": 0.8,\n    \"b23\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b12\": 0.5,\n    \"b28\": 0.5,\n    \"b41\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper outlines the challenge of predicting protein contact maps from low-depth multiple sequence alignments (MSAs) using a neural network-based approach called the Neural Potts Model (NPM). This model generalizes across diverse protein families by leveraging parameter sharing and builds on the ideas from protein language modeling.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b33\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.9,\n    \"b21\": 0.9,\n    \"b29\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.8,\n    \"b3\": 0.8,\n    \"b12\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the poor performance of unsupervised contact prediction for proteins with low-depth MSAs. The authors introduce the Neural Potts Model (NPM), which leverages parameter sharing across multiple protein families to improve prediction accuracy. The NPM uses a Transformer model to predict the parameters of a Potts model energy function for each input sequence, inspired by the emerging field of protein language models.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b0\", \"b19\", \"b33\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b3\", \"b12\", \"b21\", \"b24\", \"b28\", \"b42\", \"b45\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b23\", \"b32\", \"b36\", \"b41\"]\n  }\n}\n```"], "60cd884d91e011329faa23c3": ["```json\n{\n  \"Summary\": \"The paper addresses the reliability of pre-trained masked language models (MLMs) as factual knowledge bases. It highlights three main paradigms: prompt-based retrieval, case-based analogy, and context-based inference. It finds that MLMs' predictions are highly influenced by prompts and context rather than their internal knowledge, questioning the reliability of MLMs for factual knowledge extraction. The paper proposes a novel algorithm to understand the impact of illustrative cases and context on MLM predictions.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1,\n    \"b16\": 0.9,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b29\": 0.7,\n    \"b26\": 0.6,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b25\": 0.5,\n    \"b1\": 0.4,\n    \"b18\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the reliability of pre-trained masked language models (MLMs) as factual knowledge bases. It investigates three paradigms: prompt-based retrieval, case-based analogy, and context-based inference. The paper finds that MLMs' predictions are often biased by prompts and contexts rather than actual internal knowledge, challenging previous findings. The study proposes novel methods to analyze MLMs' behavior and concludes that MLMs are currently unreliable for factual knowledge extraction.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.9,\n    \"b29\": 0.9,\n    \"b2\": 0.9,\n    \"b16\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b25\": 0.7,\n    \"b1\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper include the reliability of MLMs as factual knowledge bases, the prompt-bias in prompt-based retrieval, and the limitations of case-based and context-based paradigms.\",\n    \"inspirations\": \"The paper is inspired by the need to understand the underlying mechanisms of MLMs and their reliability in factual knowledge extraction.\"\n  },\n  \"Direct Inspiration\": {\n    \"b30\": 1,\n    \"b16\": 0.9,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b25\": 0.8,\n    \"b29\": 0.8,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b15\": 0.6,\n    \"b26\": 0.6,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper are the reliability and mechanisms of factual knowledge extraction from pre-trained masked language models (MLMs) in different paradigms: prompt-based retrieval, case-based analogy, and context-based inference. The paper aims to determine whether MLMs can serve as reliable factual knowledge bases and understand the factors influencing their predictions.\",\n    \"inspirations\": \"The paper investigates the behaviors of MLMs, identifies critical factors for performance, and assesses the influence of external information on predictions. It questions previous findings and demonstrates that MLMs are not reliable for factual knowledge extraction when using the prompt-based retrieval paradigm.\"\n  },\n  \"Direct Inspiration\": {\n    \"b30\": 1,\n    \"b16\": 0.9,\n    \"b37\": 0.9,\n    \"b2\": 0.8,\n    \"b26\": 0.8,\n    \"b10\": 0.8,\n    \"b29\": 0.8,\n    \"b0\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b44\": 0.6,\n    \"b18\": 0.6,\n    \"b17\": 0.6,\n    \"b23\": 0.6,\n    \"b5\": 0.6,\n    \"b9\": 0.6,\n    \"b46\": 0.6,\n    \"b33\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of evaluating whether MLMs can serve as reliable factual knowledge bases. It investigates the prediction mechanisms of MLMs across three paradigms: prompt-based retrieval, case-based analogy, and context-based inference. The study finds that prompt-based retrieval is heavily prompt-biased, case-based analogy primarily improves type recognition, and context-based inference often leaks the answer.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b16\": 0.85,\n    \"b30\": 0.95,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.8,\n    \"b10\": 0.75,\n    \"b26\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b25\": 0.65\n  }\n}\n```"], "619776e85244ab9dcb042f9b": ["```json\n{\n  \"Summary\": \"The main challenges addressed in the paper revolve around inferring protein structure from sequence data using computational methods. The authors propose using factored attention, a type of Markov Random Field (MRF), to improve the estimation of contacts in protein families. The inspiration for their approach comes from the success of BERT and GPT models in Natural Language Processing (NLP) and the limitations of existing Potts models. They introduce factored attention to share amino acid features across positions and reduce the number of parameters needed to O(L), making it more efficient and scalable.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b13\": 0.9,\n    \"b35\": 0.9,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b12\": 0.7,\n    \"b29\": 0.7,\n    \"b31\": 0.7,\n    \"b40\": 0.7,\n    \"b51\": 0.8,\n    \"b55\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b20\": 0.5,\n    \"b23\": 0.6,\n    \"b38\": 0.6,\n    \"b42\": 0.6,\n    \"b45\": 0.5,\n    \"b49\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inferring protein structure from sequence using novel attention-based models inspired by the success of NLP models like BERT and GPT. The proposed methods aim to improve upon Potts models by leveraging factored attention to share amino acid features and scale linearly with protein length. The main contributions include the introduction of factored attention and its application to protein families, as well as comparisons with existing models.\",\n  \"Direct Inspiration\": [\"b8\", \"b13\", \"b35\", \"b37\"],\n  \"Indirect Inspiration\": [\"b29\", \"b32\", \"b49\"],\n  \"Other Inspiration\": [\"b23\", \"b47\", \"b26\", \"b38\", \"b42\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inferring protein structure from sequences using novel attention-based models inspired by successes in natural language processing (NLP). The authors propose factored attention as an efficient method for predicting protein contacts, leveraging shared amino acid features and reducing the number of parameters compared to traditional Potts models.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b37\": 0.85,\n    \"b35\": 0.85,\n    \"b13\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b49\": 0.75,\n    \"b45\": 0.7,\n    \"b44\": 0.7,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.65,\n    \"b51\": 0.6,\n    \"b55\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inferring protein structure from sequence using Potts models and introduces a factored attention model inspired by advances in NLP, particularly BERT and GPT. The factored attention model shares amino acid features across positions and scales linearly in length, improving parameter efficiency and performance on protein families with fewer sequences.\",\n  \"Direct Inspiration\": [\"b8\", \"b13\", \"b29\", \"b32\", \"b35\", \"b37\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b31\", \"b51\", \"b40\", \"b47\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inferring protein structures from sequences using Potts models and introduces a factored attention model to improve the efficiency and performance of contact prediction. The authors draw inspiration from successful models in natural language processing (NLP), particularly BERT and GPT, and adapt these techniques to the context of protein sequence analysis.\",\n  \"Direct Inspiration\": [\"b8\", \"b37\", \"b35\", \"b29\", \"b13\"],\n  \"Indirect Inspiration\": [\"b12\", \"b51\", \"b55\", \"b40\"],\n  \"Other Inspiration\": [\"b2\", \"b31\", \"b49\"]\n}\n```"], "60fe92c45244ab9dcb66c543": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of dynamic functional connectivity (FC) analysis in single-sided deafness (SSD) using resting-state fMRI. It proposes a randomized window-based dynamic functional connectivity (rdFC) method to capture dynamic characteristics and augment the dataset by introducing window randomness. The method aims to differentiate between left single-sided deafness (LSSD), right single-sided deafness (RSSD), and healthy controls (HCs) more accurately compared to traditional static and dynamic FC methods. The authors hypothesize that rdFC will provide more information on brain plasticity and network reorganization in SSD patients.\",\n  \"Direct Inspiration\": {\n    \"b66\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.9,\n    \"b26\": 0.85,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b79\": 0.75,\n    \"b17\": 0.7,\n    \"b77\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges of understanding brain reorganization in single-sided deafness (SSD) and proposes a randomized window-based dynamic functional connectivity (rdFC) method to enhance the analysis of brain plasticity. The method aims to differentiate between left SSD, right SSD, and healthy controls more effectively than traditional static and dynamic functional connectivity methods.\",\n  \"Direct Inspiration\": {\n    \"b66\": 1.0,\n    \"b28\": 0.9,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b79\": 0.7,\n    \"b26\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.5,\n    \"b73\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding brain reorganization in single-sided deafness (SSD) using a novel randomized window-based dynamic functional connectivity (rdFC) method. It proposes rdFC to capture the dynamic characteristics of brain activity and augment the dataset by introducing window randomness. The study aims to differentiate between left single-sided deafness (LSSD), right single-sided deafness (RSSD), and healthy controls (HCs) more accurately than traditional static and dynamic functional connectivity methods.\",\n  \"Direct Inspiration\": {\n    \"b66\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b28\": 0.75,\n    \"b79\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges related to the understanding and classification of single-sided deafness (SSD) using a novel randomized window-based dynamic functional connectivity (rdFC) method. The primary inspirations are drawn from the limitations of existing sliding window approaches and the need for improved dynamic connectivity analysis in brain networks.\",\n  \"Direct Inspiration\": {\n    \"b66\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b28\": 0.7,\n    \"b20\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.4,\n    \"b47\": 0.4,\n    \"b79\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of understanding brain reorganization in patients with single-sided deafness (SSD) using dynamic functional connectivity (FC) methods. The authors propose a novel randomized window-based dynamic functional connectivity (rdFC) method to capture dynamic characteristics and augment datasets, hypothesizing that this method will better differentiate between left SSD, right SSD, and healthy controls compared to traditional static and dynamic FC methods.\",\n  \"Direct Inspiration\": {\n    \"b66\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.85,\n    \"b28\": 0.75,\n    \"b20\": 0.70,\n    \"b17\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b79\": 0.80,\n    \"b53\": 0.60\n  }\n}\n```"], "600fe86ad4150a363c2461f4": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in understanding the relationship between avatars and user behavior on SNS, emphasizing the need for automated methods to collect and classify user data via avatars. The paper proposes a semi-automated bootstrapping-like method to label avatars, investigates the relationship between avatar selection and SNS behavior, and validates the approach through a case study on identifying Small Sellers on Social Media (SSM).\",\n  \"Direct Inspiration\": {\n    \"b33\": 1,\n    \"b28\": 0.95,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b24\": 0.75,\n    \"b4\": 0.7,\n    \"b41\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.6,\n    \"b14\": 0.55,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of user profiling and community detection on social networking sites (SNS) by incorporating user avatar information. It introduces a novel semi-automated bootstrapping method for labeling avatars and explores the relationship between avatar selection and user behavior to identify special or suspicious types of SNS users. The study validates its approach through a case study on identifying small sellers on the Weibo platform.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b28\": 0.85,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.65,\n    \"b30\": 0.6,\n    \"b24\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of user profiling on social networking sites (SNS) using avatar images. It introduces a semi-automated bootstrapping-like method to classify avatars, investigates the relationship between avatar selection and user behavior, and validates this approach through a case study on identifying small sellers on Weibo. The main contributions include the development of an avatar dataset, behavioral analysis based on avatar types, and improved classification accuracy using avatars for user profiling.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b28\": 0.9,\n    \"b41\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b36\": 0.6,\n    \"b32\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.4,\n    \"b24\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of user profiling on social networking sites (SNS) by leveraging avatar images. The authors propose a novel semi-automatic bootstrapping method to label avatars, study the relationship between avatar selection and user behavior, and validate their approach through a case study on identifying small sellers on Weibo. The proposed method aims to overcome limitations of traditional user profiling techniques, especially for new or inactive users, by incorporating avatar information into intelligent prediction models.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b28\": 0.85,\n    \"b32\": 0.8,\n    \"b41\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b9\": 0.65,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.55,\n    \"b18\": 0.5,\n    \"b13\": 0.45,\n    \"b20\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the difficulty of profiling users based on limited social media activity and the small sample sizes used in previous research. The paper proposes a novel semi-automated bootstrapping method using Convolutional Neural Networks to label avatars, investigates the relationship between avatar selection and SNS behavior, and establishes a classification method to identify Small Sellers on Social Media.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b28\": 0.85,\n    \"b41\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.75,\n    \"b4\": 0.7,\n    \"b19\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b18\": 0.55\n  }\n}\n```"], "603e16ea91e01129ef28fcd2": ["```json\n{\n  \"Summary\": \"The paper tackles the issue of biases in language models trained on large, internet-crawled datasets. It proposes a novel self-diagnosis and self-debiasing algorithm that leverages the internal knowledge of language models to detect and reduce the probability of generating biased text, without relying on external resources or extensive retraining.\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.95,\n    \"b30\": 0.9,\n    \"b7\": 0.9,\n    \"b35\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b2\": 0.8,\n    \"b28\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of language models exhibiting biases due to the datasets they are trained on. It proposes self-diagnosis and self-debiasing algorithms that use the internal knowledge of language models to detect and mitigate undesired biases in generated text, without relying on external resources or predefined word lists.\",\n  \"Direct Inspiration\": [\"b30\", \"b34\", \"b7\"],\n  \"Indirect Inspiration\": [\"b35\", \"b16\", \"b22\", \"b39\"],\n  \"Other Inspiration\": [\"b13\", \"b15\", \"b9\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Bias in large language models due to training on large, unfiltered datasets\",\n      \"Need for mechanisms to address bias without exhaustive dataset curation\"\n    ],\n    \"Inspirations\": [\n      \"Explore self-diagnosis and self-debiasing in language models\",\n      \"Propose a decoding algorithm to reduce biased text generation\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b30\": 1,\n    \"b34\": 0.9,\n    \"b7\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b28\": 0.7,\n    \"b22\": 0.6,\n    \"b39\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b16\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of biases in large language models trained on extensive datasets that contain undesirable or harmful biases. The authors propose a novel approach for self-diagnosis and self-debiasing, which enables language models to use their internal knowledge to detect and reduce biases in generated text without relying on external resources.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b30\": 1,\n    \"b34\": 1,\n    \"b7\": 1,\n    \"b35\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b28\": 0.8,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper revolve around addressing biases in large language models, which are often trained on large, unfiltered datasets from the internet, leading to the reproduction or amplification of harmful biases. The paper proposes a novel self-diagnosis and self-debiasing algorithm that allows a language model to identify and reduce the probability of generating biased text using its internal knowledge and simple textual descriptions of undesired behaviors.\",\n    \"Direct Inspiration\": {\n        \"b7\": 1,\n        \"b30\": 1,\n        \"b34\": 1,\n        \"b35\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.9,\n        \"b13\": 0.9,\n        \"b20\": 0.8,\n        \"b26\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.7,\n        \"b10\": 0.6,\n        \"b27\": 0.6,\n        \"b28\": 0.5\n    }\n}\n```"], "619799ea91e011c822372f4b": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the complexity and inflexibility of existing end-to-end Relation Extraction (RE) models. These models often require long training times and are not easily adaptable to different domains or document lengths. The paper introduces REBEL, an autoregressive approach for RE that frames the task as a seq2seq problem. By using a novel triplet linearization and pre-training an Encoder-Decoder Transformer (BART) on a new, large-scale, distantly supervised dataset, REBEL achieves state-of-the-art performance. The approach is both efficient and flexible for various RE and RC tasks.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b20\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b28\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Relation Extraction (RE), particularly the complexity and inflexibility of end-to-end models. The proposed solution, REBEL, frames RE as a seq2seq task using BART, achieving state-of-the-art performance with a simpler and more flexible approach. The REBEL dataset, created with an NLI model, enhances training efficiency and adaptation to new domains.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b28\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b20\": 0.6,\n    \"b23\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents REBEL, an autoregressive approach to Relation Extraction (RE) framed as a seq2seq task, utilizing BART-large as the base model. The primary challenges addressed include the complexity and inflexibility of current end-to-end models, long training times, and issues with existing datasets' quality. The authors propose a novel triplet linearization method to improve decoding efficiency and use a new dataset derived from Wikipedia abstracts filtered through a Natural Language Inference model to enhance training.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.75,\n    \"b23\": 0.75,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.6,\n    \"b36\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces REBEL (Relation Extraction By End-to-end Language generation), an autoregressive approach that frames Relation Extraction (RE) as a seq2seq task. It addresses the limitations of traditional and end-to-end RE models by leveraging a simple triplet decomposition and pre-training an Encoder-Decoder Transformer (BART) using a new large-scale dataset. This approach improves efficiency, flexibility, and performance on various RE tasks while minimizing training times and adaptation requirements for new domains.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.95,\n    \"b23\": 0.90,\n    \"b15\": 0.85,\n    \"b10\": 0.90,\n    \"b17\": 0.95,\n    \"b28\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.75,\n    \"b34\": 0.75,\n    \"b21\": 0.75,\n    \"b38\": 0.75,\n    \"b35\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.65,\n    \"b18\": 0.60,\n    \"b2\": 0.60,\n    \"b26\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extracting relational facts from text, specifically improving the efficiency and flexibility of end-to-end Relation Extraction (RE) and Relation Classification (RC). The authors propose REBEL, an autoregressive approach that frames RE as a seq2seq task. They leverage a new dataset and pre-train an Encoder-Decoder Transformer (BART) to achieve state-of-the-art performance with fewer epochs of fine-tuning.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b20\": 0.5,\n    \"b23\": 0.5,\n    \"b15\": 0.4\n  }\n}\n```"], "60cd885291e011329faa23c4": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of event extraction, focusing on the complex structure of event records and the semantic gap between text and events. It proposes a novel sequence-to-structure generation paradigm called TEXT2EVENT, which directly extracts events from text in an end-to-end manner using a neural network-based architecture. The method is more data-efficient and leverages a constrained decoding algorithm and curriculum learning strategy.\",\n    \"Direct Inspiration\": {\n        \"b4\": 0.9,\n        \"b3\": 0.85,\n        \"b2\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b33\": 0.7,\n        \"b43\": 0.7,\n        \"b54\": 0.7,\n        \"b9\": 0.7,\n        \"b18\": 0.7,\n        \"b35\": 0.65,\n        \"b41\": 0.65,\n        \"b40\": 0.65,\n        \"b31\": 0.65,\n        \"b26\": 0.65,\n        \"b46\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b23\": 0.6,\n        \"b44\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of event extraction in natural language processing, particularly the complexity of event structures and the semantic gap between text and event records. The proposed solution, TEXT2EVENT, employs a sequence-to-structure generation paradigm that directly extracts events from text in an end-to-end manner, leveraging a neural network-based architecture. Key innovations include a constrained decoding algorithm for schema-based guidance and a curriculum learning algorithm for efficient training.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b2\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.75,\n    \"b9\": 0.75,\n    \"b23\": 0.7,\n    \"b43\": 0.7,\n    \"b54\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.6,\n    \"b35\": 0.6,\n    \"b41\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a new paradigm for event extraction called TEXT2EVENT, which aims to address the challenges of complex event structures and semantic gaps between text and events. The proposed method utilizes a sequence-to-structure generation paradigm, which is data-efficient and easy to implement compared to existing decomposition-based methods. The paper also introduces a constrained decoding algorithm and a curriculum learning strategy to enhance the effectiveness of the model.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b23\": 0.7,\n    \"b43\": 0.7,\n    \"b54\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b9\": 0.6,\n    \"b41\": 0.6,\n    \"b42\": 0.6,\n    \"b46\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in event extraction, particularly the complex structure of event records and the semantic gap between text and event. The authors propose TEXT2EVENT, an end-to-end sequence-to-structure generation paradigm that directly extracts events from text using a neural network-based architecture. The method is data-efficient, requiring only coarse parallel text-record annotations, and incorporates constrained decoding and curriculum learning algorithms to improve performance.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b3\": 1.0,\n    \"b18\": 0.9,\n    \"b43\": 0.9,\n    \"b54\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b46\": 0.8,\n    \"b42\": 0.75,\n    \"b41\": 0.75,\n    \"b35\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.7,\n    \"b31\": 0.7,\n    \"b26\": 0.7,\n    \"b9\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of event extraction in natural language processing, focusing on the complexity of event records and the semantic gap between text and event. It critiques existing decomposition-based methods for their data inefficiency and suboptimal architectures. The proposed solution, TEXT2EVENT, is a sequence-to-structure generation paradigm that extracts events in an end-to-end manner using a neural network-based architecture. The method is described as data-efficient, easy to implement, and capable of leveraging event schema knowledge through a constrained decoding algorithm and curriculum learning strategy.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b33\": 0.8,\n    \"b43\": 0.8,\n    \"b54\": 0.8,\n    \"b9\": 0.8,\n    \"b18\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b46\": 0.7,\n    \"b41\": 0.7,\n    \"b42\": 0.7,\n    \"b40\": 0.6,\n    \"b31\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```"], "60ebf1b291e011957137409f": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"Developing a unified entity-relation label space for joint entity relation extraction, addressing distinct learning problems and different label spaces for entities and relations.\",\n    \"inspiration\": \"The paper takes inspiration from existing methods for joint models in entity relation extraction and the use of modern pre-training tools like BERT.\"\n  },\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b30\": 0.9,\n    \"b3\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b25\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b13\": 0.55,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a unified entity-relation label space for joint entity and relation extraction in NLP. It proposes a novel approach that formulates the task as a table filling problem and introduces a biaffine attention mechanism for learning interactions between word pairs. The paper also implements structural constraints and a joint decoding algorithm to improve the performance of the model.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.9,\n    \"b25\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of unifying entity and relation extraction in NLP using a joint model with a novel unified label space. Inspired by existing methods, the paper proposes a biaffine attention mechanism and structural constraints to enhance the model.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b30\": 1.0,\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b17\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b22\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a unified label space for joint entity-relation extraction, proposing a novel model that uses a biaffine attention mechanism and a joint decoding algorithm. The model aims to overcome the limitations of existing methods by maintaining full model expressiveness and improving decoding efficiency.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b13\": 0.4,\n    \"b5\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on improving joint models for entity-relation extraction by proposing a unified label space for entities and relations. The main challenges identified are the separation of labels even in joint models and the difficulty of handling overlapping and isolated entities. The proposed solution involves a new input space, a biaffine attention mechanism, structural constraints, and a joint decoding algorithm.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b30\": 0.9,\n    \"b3\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b22\": 0.7,\n    \"b15\": 0.6,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```"], "61494d445244ab9dcbed36b1": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of factually inconsistent summaries generated by large pre-trained Transformers in abstractive summarization. The authors propose a novel contrastive learning framework, CLIFF, which aims to improve the faithfulness and factuality of summaries using both positive (reference) and negative (factually inconsistent) samples. The method involves constructing various types of negative samples and training the model to differentiate between correct and incorrect summaries.\",\n    \"Direct Inspiration\": {\n        \"b4\": 0.9,\n        \"b24\": 0.85,\n        \"b32\": 0.9,\n        \"b46\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b19\": 0.7,\n        \"b42\": 0.75,\n        \"b22\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b31\": 0.65,\n        \"b47\": 0.65,\n        \"b13\": 0.65,\n        \"b40\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating factually consistent summaries using abstractive summarization. It proposes a contrastive learning framework, CLIFF, to improve faithfulness by training models to distinguish between factually consistent (positive) and inconsistent (negative) summaries. The framework uses strategies like entity swapping, mask-and-fill with BART, and other techniques to create negative samples for training.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b46\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.85,\n    \"b32\": 0.9,\n    \"b42\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.75,\n    \"b22\": 0.8,\n    \"b12\": 0.8,\n    \"b40\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating faithful and informative summaries using large pre-trained Transformers. The proposed solution, CLIFF, leverages contrastive learning to improve the factual accuracy of summaries by training models on both positive and negative samples.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b46\": 1,\n    \"b42\": 0.9,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b20\": 0.6,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.5,\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of factually inconsistent content in abstractive summarization by proposing a new framework, CLIFF, that uses contrastive learning to improve the faithfulness and factuality of generated summaries. The approach involves constructing both positive and negative samples and fine-tuning pre-trained large models with a contrastive learning objective.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b20\": 0.85,\n    \"b14\": 0.9,\n    \"b24\": 0.8,\n    \"b46\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.75,\n    \"b42\": 0.7,\n    \"b31\": 0.65,\n    \"b47\": 0.65,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.6,\n    \"b13\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training abstractive summarization systems to generate both faithful and informative summaries. The proposed algorithm, CLIFF, uses contrastive learning to differentiate between factually consistent and inconsistent summaries. The main inspiration comes from recent advancements in contrastive learning and the need to correct factual inconsistencies in summaries generated by large pre-trained models.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b46\": 1.0,\n    \"b32\": 0.9,\n    \"b42\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b22\": 0.8,\n    \"b47\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```"], "60a7897991e011f90a51dd41": ["```json\n{\n  \"Summary\": \"The paper tackles the challenges in many-to-many multilingual neural machine translation (MNMT), particularly the performance gap between multilingual and bilingual models, and the focus on English-centric directions. The proposed solution, mCOLT, aims to reduce the representation gap of different languages through multilingual contrastive learning and data augmentation techniques, leading to improved translation quality, including zero-shot and unsupervised directions.\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.9,\n    \"b37\": 0.9,\n    \"b20\": 0.85,\n    \"b14\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.8,\n    \"b32\": 0.75,\n    \"b29\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b19\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in multilingual neural machine translation (NMT), particularly the performance gaps between multilingual and bilingual models, and the need for effective many-to-many translation for non-English language pairs. The proposed algorithm, multilingual COntrastive Learning framework for Translation (mCOLT), aims to reduce representation gaps between different languages using contrastive learning, leveraging both parallel and monolingual corpora.\",\n    \"Direct Inspiration\": {\n        \"b20\": 0.9,\n        \"b37\": 0.95,\n        \"b14\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.7,\n        \"b17\": 0.7,\n        \"b19\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b29\": 0.6,\n        \"b2\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in multilingual neural machine translation (NMT), particularly the performance gap between multilingual and bilingual models as the number of languages increases. The proposed algorithm, mCOLT, aims to close the representation gap between languages using contrastive learning, thereby improving translation quality for both English-centric and non-English directions.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b0\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7,\n    \"b2\": 0.65,\n    \"b17\": 0.6,\n    \"b16\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the performance gap in multilingual neural machine translation (NMT) models, particularly when compared to bilingual baselines and as the number of languages increases. The proposed algorithm, mCOLT, aims to reduce the representation gap between different languages through a multilingual contrastive learning framework. This involves using contrastive loss on aligned and non-aligned sentence pairs and leveraging monolingual data to create pseudo-pairs for training.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1,\n    \"b14\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.9,\n    \"b0\": 0.9,\n    \"b2\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.7,\n    \"b17\": 0.7,\n    \"b16\": 0.7,\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in multilingual neural machine translation (NMT), particularly the performance gap between multilingual and bilingual models and the emphasis on English-centric translations. The authors propose mCOLT, a contrastive learning framework, to minimize the representation gap between different languages and leverage both parallel and monolingual corpora to improve translation quality. Key contributions include the use of contrastive loss and aligned augmentation to create pseudo-pairs for training.\",\n  \"Direct Inspiration\": [\"b33\", \"b34\"],\n  \"Indirect Inspiration\": [\"b20\", \"b14\", \"b37\"],\n  \"Other Inspiration\": [\"b16\", \"b19\", \"b32\"]\n}\n```"], "60531d9f91e011547ddf545f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of advancing state-of-the-art in large-scale graph machine learning (ML) by introducing the OGB Large-Scale Challenge (OGB-LSC), which includes three large-scale datasets: MAG240M, WikiKG90M, and PCQM4M. The paper emphasizes the need for expressive graph neural networks (GNNs) to handle large-scale graphs effectively and discusses the baseline models and winning solutions from the ACM KDD Cup 2021 competition.\",\n  \"Direct Inspiration\": {\n    \"b40\": 0.9,\n    \"b46\": 0.85,\n    \"b50\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b15\": 0.7,\n    \"b20\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b31\": 0.6,\n    \"b63\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces the OGB Large-Scale Challenge (OGB-LSC) to facilitate the development of state-of-the-art graph ML models for massive datasets. It presents three large-scale datasets (MAG240M, WikiKG90M, and PCQM4M) and discusses the challenges and methods for handling large-scale graph ML. The paper emphasizes the importance of expressive models and provides baseline analyses and results from the ACM KDD Cup 2021.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b47\": 0.85,\n    \"b50\": 0.85,\n    \"b68\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b31\": 0.7,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b44\": 0.6,\n    \"b63\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of deploying accurate graph machine learning (ML) models at scale, which is hindered by the lack of suitable large-scale graph benchmarks. It introduces the OGB Large-Scale Challenge (OGB-LSC) with three datasets: MAG240M, WikiKG90M, and PCQM4M, aimed at advancing state-of-the-art graph ML. The paper highlights the need for expressive Graph Neural Networks (GNNs) and sophisticated algorithms to handle these large datasets, and it summarizes the methods and results from the ACM KDD Cup 2021, which was organized around these datasets.\",\n  \"Direct Inspiration\": [\"b63\", \"b41\", \"b23\", \"b25\", \"b17\", \"b58\", \"b20\"],\n  \"Indirect Inspiration\": [\"b2\", \"b8\", \"b9\", \"b71\"],\n  \"Other Inspiration\": [\"b1\", \"b59\", \"b37\", \"b7\", \"b56\", \"b70\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scaling machine learning on large-scale graph data, particularly in the context of graph neural networks (GNNs). It introduces three large-scale, realistic datasets (MAG240M, WikiKG90M, and PCQM4M) to facilitate advanced graph ML model development. The paper highlights the importance of expressive models for performance improvements on large datasets and summarizes techniques and results from the ACM KDD Cup 2021.\",\n  \"Direct Inspiration\": {\n    \"b63\": 0.9,\n    \"b41\": 0.9,\n    \"b23\": 0.9,\n    \"b25\": 0.8,\n    \"b17\": 0.8,\n    \"b58\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b71\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b42\": 0.6,\n    \"b57\": 0.6,\n    \"b11\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of advancing state-of-the-art in large-scale graph machine learning (ML) by introducing the OGB-LSC datasets, which are significantly larger than existing graph datasets. The primary inspiration comes from the need to handle large-scale graphs for applications such as social networks, recommender systems, and molecule simulations. The paper benchmarks various graph ML models and highlights the benefits of advanced expressive models for large-scale graph ML tasks.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b40\": 0.8,\n    \"b31\": 0.8,\n    \"b50\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.75,\n    \"b47\": 0.75,\n    \"b15\": 0.7,\n    \"b54\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b63\": 0.6,\n    \"b41\": 0.6,\n    \"b23\": 0.6,\n    \"b44\": 0.6\n  }\n}\n```"], "60a2401291e0115ec77b9cd9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately predicting protein-protein interactions (PPIs), especially for inter-novel-protein interactions, which are pairs of proteins where at least one protein was not seen during training. The authors propose a new evaluation framework and a graph neural network-based model (GNN-PPI) to incorporate protein correlations, achieving improved performance across different datasets.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 0.9,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.6,\n    \"b6\": 0.5,\n    \"b7\": 0.4\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting protein-protein interactions (PPIs) accurately, especially for inter-novel-protein interactions, which are not previously seen during training. The authors propose a new evaluation framework and a graph neural network-based model (GNN-PPI) to incorporate correlations between proteins for better performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b6\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately predicting protein-protein interactions (PPIs) especially for inter-novel-protein interactions, which are not seen during training. To tackle this, the authors propose a new evaluation framework that respects inter-novel-protein interactions and introduce a Graph Neural Network (GNN-PPI) model that incorporates protein correlations for improved prediction performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.7,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting protein-protein interactions (PPIs) with a focus on inter-novel-protein interactions, which are typically not well addressed by existing methods. The proposed solution includes a new evaluation framework that respects inter-novel-protein interactions and a graph neural network (GNN) based model (GNN-PPI) that incorporates correlations between proteins to improve prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in Protein-Protein Interaction (PPI) prediction, particularly the significant performance drop when models are tested on unseen datasets. The authors propose a new evaluation framework and introduce a graph neural network-based model (GNN-PPI) to incorporate correlations between proteins, achieving state-of-the-art performance in predicting inter-novel-protein interactions.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.7,\n    \"b5\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b6\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```"], "602b8eb491e0113d72356b4f": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the inefficiency and high computational cost of Graph Neural Networks (GNNs) on large-scale graphs, both at the algorithm and hardware levels. The paper introduces a novel method called Unified GNN Sparsification (UGS) to simultaneously prune the graph adjacency matrix and model weights, making no assumptions about the GNN architecture or graph structure. The method also generalizes the Lottery Ticket Hypothesis (LTH) to GNNs, defining a graph lottery ticket (GLT) that can achieve similar performance to the full model and graph but with significantly reduced inference costs.\",\n  \"Direct Inspiration\": [\"b38\", \"b21\"],\n  \"Indirect Inspiration\": [\"b52\", \"b64\", \"b12\"],\n  \"Other Inspiration\": [\"b30\", \"b8\", \"b1\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inefficiency of Graph Neural Networks (GNNs) in terms of computational cost and memory usage, particularly for large-scale graph applications. The authors propose an algorithm called Unified GNN Sparsification (UGS) to simultaneously prune the graph adjacency matrix and the model weights, thereby reducing the computational complexity without compromising performance. The paper also generalizes the Lottery Ticket Hypothesis (LTH) to GNNs for the first time, introducing the concept of Graph Lottery Tickets (GLTs).\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b70\": 0.8,\n    \"b52\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b64\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the inefficiency in training and inference of Graph Neural Networks (GNNs) due to their complex computational structure. It introduces a novel end-to-end optimization framework called Unified GNN Sparsification (UGS) to simultaneously prune the graph adjacency matrix and model weights. The paper extends the Lottery Ticket Hypothesis (LTH) to GNNs, defining a Graph Lottery Ticket (GLT) as a pair of core sub-dataset and sparse sub-network that can be jointly identified from the full graph and the original GNN model.\",\n    \"Direct Inspiration\": {\n        \"b21\": 1.0,\n        \"b38\": 0.9,\n        \"b64\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.75,\n        \"b31\": 0.7,\n        \"b52\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b28\": 0.6,\n        \"b70\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The paper addresses the inefficiency and high computational cost of Graph Neural Networks (GNNs) during training and inference, especially for large-scale and dense graphs. The primary challenge is to reduce the complexity of GNNs both in terms of graph structure and model parameters.\",\n    \"inspirations\": \"The authors propose an end-to-end framework called Unified GNN Sparsification (UGS), which simultaneously prunes the graph adjacency matrix and the model weights. They also generalize the lottery ticket hypothesis (LTH) to GNNs.\"\n  },\n  \"Direct Inspiration\": {\n    \"b21\": 0.95,\n    \"b38\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b52\": 0.80,\n    \"b70\": 0.75,\n    \"b12\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.60,\n    \"b34\": 0.55,\n    \"b53\": 0.50\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges addressed in the paper are the inefficiency and high computational cost associated with training and inference in Graph Neural Networks (GNNs) for large-scale graph applications. The proposed algorithm, Unified GNN Sparsification (UGS), aims to co-simplify both the input graph and the model weights to achieve ultra-efficient GNN inference. The paper introduces an end-to-end optimization framework and generalizes the Lottery Ticket Hypothesis (LTH) to GNNs for the first time.\",\n  \"Direct Inspiration\": [\"b38\", \"b21\"],\n  \"Indirect Inspiration\": [\"b70\", \"b52\"],\n  \"Other Inspiration\": [\"b15\", \"b64\"]\n}\n```"], "602b9c0891e0113d72356c5a": ["```json\n{\n  \"Summary\": \"The paper addresses the limitations of current GNN-based methods in knowledge-aware recommendation systems, specifically focusing on the issues of modeling user intents and relational paths. It proposes a new model, Knowledge Graph-based Intent Network (KGIN), which includes user intent modeling and relational path-aware aggregation to enhance the performance and interpretability of recommendations.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b37\": 0.9,\n    \"b40\": 0.9,\n    \"b46\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b32\": 0.7,\n    \"b42\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b11\": 0.6,\n    \"b25\": 0.6,\n    \"b31\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of effectively modeling user intents and relational paths in knowledge graph-based recommendation systems. It proposes a novel model, Knowledge Graph-based Intent Network (KGIN), which consists of user intent modeling and relational path-aware aggregation to improve recommendation performance and interpretability.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1,\n    \"b37\": 1,\n    \"b46\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b43\": 0.8,\n    \"b48\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of current GNN-based recommendation systems in capturing user intents and relational paths. It introduces the Knowledge Graph-based Intent Network (KGIN) to model user intents and relational paths effectively, improving recommendation performance and interpretability.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1,\n    \"b37\": 1,\n    \"b46\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.9,\n    \"b3\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.75,\n    \"b32\": 0.75,\n    \"b42\": 0.75\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include the lack of fine-grained user intent modeling and the limitations of node-based aggregation schemes in current GNN-based methods for knowledge-aware recommendations. The proposed solution, Knowledge Graph-based Intent Network (KGIN), addresses these challenges by (1) modeling user intents as attentive combinations of KG relations and (2) implementing a relational path-aware aggregation scheme for better capturing the interactions among relations.\",\n    \"Direct Inspiration\": {\n        \"b40\": 1,\n        \"b37\": 1,\n        \"b46\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b43\": 0.8,\n        \"b25\": 0.7,\n        \"b6\": 0.6,\n        \"b11\": 0.6,\n        \"b31\": 0.6,\n        \"b32\": 0.6,\n        \"b42\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.5,\n        \"b22\": 0.5,\n        \"b23\": 0.5,\n        \"b29\": 0.5,\n        \"b0\": 0.4,\n        \"b14\": 0.4,\n        \"b16\": 0.4,\n        \"b48\": 0.4,\n        \"b8\": 0.4,\n        \"b12\": 0.4,\n        \"b18\": 0.4,\n        \"b33\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of current GNN-based methods in modeling user intents and relational paths for knowledge-aware recommendation. It proposes the Knowledge Graph-based Intent Network (KGIN), which includes components for user intent modeling and relational path-aware aggregation. The model aims to enhance the performance and interpretability of recommendations by capturing finer-grained user intents and preserving relation dependencies in paths.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1.0,\n    \"b37\": 0.9,\n    \"b46\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5,\n    \"b42\": 0.5\n  }\n}\n```"], "6008324a9e795ed227f530f9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning effective graph representations, particularly in noisy real-world graphs. It proposes a self-supervised graph attention network (SuperGAT) to improve the performance of Graph Neural Networks (GNNs) by leveraging edge information for better relational importance assessment. The paper presents two variants of SuperGAT (SD and MX) and validates their effectiveness through experiments on synthetic and real-world datasets.\",\n  \"Direct Inspiration\": [\n    \"b52\",\n    \"b29\",\n    \"b59\",\n    \"b26\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b31\",\n    \"b51\"\n  ],\n  \"Other Inspiration\": [\n    \"b20\",\n    \"b37\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning effective graph representations in the presence of noisy edges by leveraging self-supervised graph attention mechanisms. It introduces the SuperGAT model, which utilizes edge information to improve attention mechanisms (GO and DP), aiming to enhance node classification and link prediction tasks. The paper proposes variants of SuperGAT that adapt to different graph characteristics, such as homophily and average degree, and demonstrates their effectiveness across multiple real-world datasets.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b59\": 1,\n    \"b52\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b27\": 0.7,\n    \"b24\": 0.7,\n    \"b13\": 0.7,\n    \"b46\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b51\": 0.6,\n    \"b53\": 0.6,\n    \"b30\": 0.6,\n    \"b37\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of learning effective graph representations in noisy, real-world graphs. It proposes a self-supervised graph attention network (SuperGAT) that leverages edge information to supervise attention mechanisms, improving node classification and link prediction performance. The main contributions include the development of SuperGAT with various attention mechanisms (GO, DP, SD, MX), an analysis of these mechanisms, and a recipe for designing graph attention based on graph characteristics such as homophily and average degree.\",\n    \"Direct Inspiration\": {\n        \"b29\": 0.9,\n        \"b59\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b26\": 0.8,\n        \"b37\": 0.8,\n        \"b38\": 0.8,\n        \"b52\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b31\": 0.6,\n        \"b51\": 0.6,\n        \"b53\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the complexity of learning to represent graphs, the noise in real-world graphs that leads to suboptimal representations in GNNs, and the inconsistency in the performance improvements of GATs across different datasets. The paper proposes the self-supervised graph attention network (SuperGAT) to address these challenges by leveraging edges that explicitly encode relational importance and using different attention mechanisms (GO, DP, SD, MX) to improve node classification and link prediction.\",\n  \"Direct Inspiration\": {\n    \"b29\": 0.9,\n    \"b59\": 0.9,\n    \"b26\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b52\": 0.75,\n    \"b31\": 0.7,\n    \"b51\": 0.7,\n    \"b24\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b53\": 0.6,\n    \"b46\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper revolve around improving the performance and consistency of graph neural networks (GNNs) when dealing with noisy real-world graphs. The paper proposes a self-supervised graph attention network (SuperGAT) that incorporates edge information to guide attention mechanisms, aiming to enhance the relational importance learning and node representation. The key contributions include the introduction of self-supervised attention models, analysis of classic attention forms (GO and DP), and the development of a design recipe for graph attention based on graph characteristics such as average degree and homophily.\",\n  \"Direct Inspiration\": {\n    \"1\": 0.9,\n    \"2\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"1\": 0.7,\n    \"2\": 0.6,\n    \"3\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"1\": 0.4,\n    \"2\": 0.3\n  }\n}\n```"], "5ff682abd4150a363cbaa623": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting deceptive behavior in real-life trial data using a multimodal system that combines verbal, acoustic, and visual modalities. The main contributions include introducing subject-level deception detection, exploring various features for deception detection, and presenting a semi-automatic and fully-automatic system for this purpose.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b12\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b10\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting deceptive behavior in real-life trial data using a multimodal approach that incorporates verbal, acoustic, and visual modalities. The authors propose a subject-level deception classification system, introduce novel methods for feature extraction, and conduct experiments to compare manual and automatic feature extraction techniques.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b2\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of detecting deceptive behavior in real-life trial data using a multimodal system that incorporates verbal, acoustic, and visual modalities. The proposed system aims to improve upon traditional polygraph tests and previous machine-learning methods by focusing on subject-level deception detection, which aligns better with the dataset's ground-truth. Key contributions include introducing subject-level deception detection, exploring a diverse set of features from multiple channels, and demonstrating the system's effectiveness through experiments and human evaluations.\",\n    \"Direct Inspiration\": {\n        \"b8\": 0.95\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.75,\n        \"b4\": 0.75,\n        \"b5\": 0.75,\n        \"b6\": 0.75,\n        \"b7\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b2\": 0.6,\n        \"b9\": 0.6,\n        \"b10\": 0.6,\n        \"b11\": 0.6,\n        \"b12\": 0.6,\n        \"b13\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the need for a reliable and efficient system to detect deceptive behavior in real-life trial data. The authors propose a novel subject-level deception detection system using multimodal data (verbal, acoustic, and visual) from real court trials. The main contributions include introducing subject-level deception detection, combining different features using early and late fusion methods, and presenting a semi-automatic and fully-automatic system for deception detection.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting deception in real-life trial data by introducing a multimodal system that uses verbal, acoustic, and visual modalities. It focuses on subject-level deception detection as opposed to video-level detection, arguing that this approach aligns better with the ground truth. The paper's contributions include the introduction of subject-level deception detection, the exploration of diverse feature sets, the implementation of a semi-automatic and a fully-automatic detection system, and a comparative study with human performance.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b9\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b5\": 0.75,\n    \"b6\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.65,\n    \"b11\": 0.65,\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```"], "613463b29e795e83a2c68842": ["```json\n{\n  \"Summary\": \"The paper proposes the Graph Robustness Benchmark (GRB) to address the limitations in evaluating the adversarial robustness of Graph Machine Learning (GML) models. Key challenges include unrealistic attack/defense scenarios, lack of a unified evaluation protocol, and lack of scalability. GRB provides a reproducible framework with refined attack/defense scenarios, scalable and unified evaluations, and public leaderboards.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b23\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of adversarial robustness in Graph Machine Learning (GML) models. It highlights limitations such as unrealistic attack/defense scenarios, lack of a unified evaluation protocol, and scalability issues. The proposed Graph Robustness Benchmark (GRB) aims to provide a reproducible, scalable, and unified framework for evaluating the adversarial robustness of GML models.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b21\": 0.85,\n    \"b22\": 0.85,\n    \"b23\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Unrealistic Attack/Defense Scenarios\",\n      \"Lack of A Unified Evaluation Protocol\",\n      \"Lack of Scalability\"\n    ],\n    \"Inspirations\": [\n      \"Proposing the Graph Robustness Benchmark (GRB) to provide a reproducible framework for fair evaluation of adversarial attacks and defenses on GML models under unified settings.\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b20\",\n    \"b21\",\n    \"b22\",\n    \"b23\",\n    \"b24\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b25\",\n    \"b26\",\n    \"b27\"\n  ],\n  \"Other Inspiration\": [\n    \"b10\",\n    \"b11\",\n    \"b12\",\n    \"b13\",\n    \"b14\",\n    \"b15\",\n    \"b16\",\n    \"b17\",\n    \"b18\",\n    \"b19\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of adversarial attacks on graph machine learning (GML) models, highlighting issues such as unrealistic attack/defense scenarios, lack of a unified evaluation protocol, and scalability problems. The proposed solution, Graph Robustness Benchmark (GRB), aims to provide a reproducible framework for fair evaluation of adversarial robustness in GML models by refining attack/defense scenarios, offering scalable and unified evaluations, and supporting reproducible and public leaderboards.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b21\": 0.9,\n    \"b22\": 0.9,\n    \"b23\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b26\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of adversarial robustness in Graph Machine Learning (GML) models, emphasizing the need for realistic attack/defense scenarios, unified evaluation protocols, and scalability. It proposes the Graph Robustness Benchmark (GRB) to provide a reproducible framework for evaluating adversarial robustness of GML models under unified settings.\",\n    \"Direct Inspiration\": {\n        \"b20\": 0.9,\n        \"b23\": 0.85,\n        \"b24\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.75,\n        \"b11\": 0.75,\n        \"b12\": 0.75,\n        \"b15\": 0.75,\n        \"b16\": 0.75,\n        \"b17\": 0.75,\n        \"b18\": 0.75,\n        \"b28\": 0.75,\n        \"b30\": 0.75,\n        \"b33\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b25\": 0.65,\n        \"b26\": 0.65,\n        \"b27\": 0.65,\n        \"b31\": 0.65\n    }\n}\n```"], "6063191391e0118c891f1cb5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of graph classification in the presence of imbalanced class distributions, which is exacerbated by structure diversity and the difficulty of applying traditional imbalanced learning methods to multi-task settings. The proposed solution, GraphDIVE, leverages a gating network to create subsets of semantically similar graphs and trains multiple experts on these subsets to improve classification performance, particularly for minority classes.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b33\": 0.7,\n    \"b59\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b50\": 0.6,\n    \"b21\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of class-imbalanced learning in graph classification, which is exacerbated by structure diversity and poor applicability in multi-task settings. The proposed solution, GraphDIVE, uses a gating network to group semantically similar graphs and trains multiple classifiers (experts) on these subsets to alleviate the bias towards majority classes. The method demonstrates state-of-the-art results in both single-task and multi-task settings.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.75,\n    \"b33\": 0.8,\n    \"b50\": 0.7,\n    \"b59\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b21\": 0.6,\n    \"b30\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge of the paper is addressing class imbalance in graph classification tasks, which is exacerbated by structural diversity and poor applicability in multi-task settings. The proposed algorithm, GraphDIVE, leverages a gating network and expert classifiers to group semantically similar graphs, alleviating the imbalance problem.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b33\": 0.7,\n    \"b59\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b50\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of graph classification, particularly focusing on the problem of imbalanced class distribution. The authors propose a novel framework called GraphDIVE, which uses a gating network to capture semantic structures and assign different experts to subsets of data, thereby alleviating the bias towards majority classes. The method is evaluated on public benchmarks and shows significant improvements.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.9,\n    \"b24\": 0.8,\n    \"b33\": 0.8,\n    \"b59\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b3\": 0.7,\n    \"b21\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in graph classification, specifically the issue of imbalanced class distribution in graph datasets. It proposes a novel framework called GraphDIVE that leverages a gating network to capture the semantic structure of datasets and uses multiple classifiers (experts) trained on different subsets to improve classification performance, especially for minority classes.\",\n  \"Direct Inspiration\": {\n    \"references\": [\n      \"b28\"\n    ],\n    \"confidence\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\n      \"b24\",\n      \"b33\",\n      \"b59\",\n      \"b30\",\n      \"b31\"\n    ],\n    \"confidence\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"references\": [\n      \"b25\",\n      \"b3\",\n      \"b21\",\n      \"b50\"\n    ],\n    \"confidence\": 0.7\n  }\n}\n```"], "60bdde338585e32c38af50f3": ["```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing Message Passing Neural Networks (MPNNs) in distinguishing non-isomorphic graphs and counting substructures due to their expressive power being equivalent to the 1-WL test. It proposes a new model that enhances the theoretical expressive power of MPNNs by designing graph convolution in the spectral domain. This model maintains linear memory and computational complexities, offers improved spectral capabilities, and achieves better performance in distinguishing graphs and counting substructures.\",\n  \"Direct Inspiration\": [\"b25\", \"b16\", \"b27\"],\n  \"Indirect Inspiration\": [\"b2\", \"b12\", \"b5\"],\n  \"Other Inspiration\": [\"b40\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing Message Passing Neural Networks (MPNNs) in distinguishing non-isomorphic graphs and counting substructures. The proposed method aims to improve the expressive power of MPNNs by designing graph convolution in the spectral domain with custom non-linear functions of eigenvalues and by masking the convolution support with desired length of receptive field. The proposed model claims to achieve linear memory and computational complexities while being theoretically more powerful than the 1-WL test and experimentally as powerful as PPGN.\",\n  \"Direct Inspiration\": [\"b25\", \"b27\", \"b16\"],\n  \"Indirect Inspiration\": [\"b2\", \"b5\", \"b40\"],\n  \"Other Inspiration\": [\"b0\", \"b31\", \"b1\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the limited expressive power of Message Passing Neural Networks (MPNNs) for distinguishing non-isomorphic graphs and counting substructures due to their equivalence to the 1-Weisfeiler-Lehman (1-WL) test. The paper proposes novel spectral domain graph convolution models, GNNML1 and GNNML3, which aim to enhance the expressive power of MPNNs while maintaining linear memory and computational complexities. The models leverage custom non-linear functions of eigenvalues and mask the convolution support to achieve better performance on graph isomorphism and substructure counting tasks.\",\n  \"Direct Inspiration\": {\"b16\": 1, \"b2\": 1},\n  \"Indirect Inspiration\": {\"b25\": 0.9, \"b40\": 0.8, \"b27\": 0.7},\n  \"Other Inspiration\": {\"b12\": 0.6, \"b5\": 0.6}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of Message Passing Neural Networks (MPNNs) in graph representation and proposes a new model that integrates spectral graph theory to enhance expressive power while maintaining linear computational complexity. The proposed method aims to overcome the weaknesses of MPNNs, such as their inability to distinguish non-isomorphic graphs that are not distinguishable by the 1-WL test and their limited spectral ability.\",\n  \"Direct Inspiration\": [\"b2\", \"b25\", \"b27\"],\n  \"Indirect Inspiration\": [\"b16\", \"b40\"],\n  \"Other Inspiration\": [\"b0\", \"b5\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of Message Passing Neural Networks (MPNNs) in distinguishing non-isomorphic graphs and counting substructures due to their equivalence to the 1-WL test. It proposes a new model that operates in the spectral domain to improve the expressive power while maintaining linear complexity, achieving comparable power to 3-WL equivalent models like PPGN.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.9,\n    \"b2\": 0.85,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.75,\n    \"b40\": 0.7,\n    \"b1\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b5\": 0.55\n  }\n}\n```"], "604b3a7391e0110eed64c36d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial attacks on DNNs using a novel physical-world attack method based on laser beams (AdvLB). This method leverages the unique properties of laser beams to create perturbations, providing high flexibility and stealthiness for attacks. The proposed method is evaluated through extensive experiments, demonstrating its effectiveness in both digital and physical settings. The paper also provides an in-depth analysis of the causes of prediction errors induced by AdvLB.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b1\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performing adversarial attacks on deep neural networks (DNNs) using laser beams, a novel method not previously explored. The proposed algorithm, Adversarial Laser Beam (AdvLB), leverages the unique properties of laser light for attacks, offering high flexibility and temporal stealthiness. The method involves formulating the laser beam's physical parameters and optimizing them to achieve high attack success rates in both digital and physical settings.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b20\": 0.7,\n    \"b17\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating a new type of physical-world adversarial attack using laser beams, called adversarial laser beam (AdvLB). This method leverages the properties of laser light to perform fast and flexible attacks on DNNs, overcoming limitations of previous 'sticker-pasting' methods. The AdvLB method is formulated with a group of controllable parameters and an optimization approach to search for the most effective parameters. The effectiveness of AdvLB is demonstrated through extensive experiments in both digital and physical settings.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b19\": 0.8,\n    \"b17\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b9\": 0.6,\n    \"b16\": 0.6,\n    \"b2\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the security of Deep Neural Networks (DNNs) against adversarial attacks in physical-world scenarios. The novel method proposed is the Adversarial Laser Beam (AdvLB) attack, which leverages laser beams as adversarial perturbations without requiring physical changes to the target object, offering flexibility and temporal stealthiness. The authors conduct comprehensive experiments and provide in-depth analysis of prediction errors caused by the laser beam.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a new type of physical-world adversarial attack on DNNs using a laser beam (AdvLB). The proposed attack leverages the unique properties of laser light to achieve high flexibility and stealthiness, making it a significant threat to systems like self-driving cars. The paper also provides a detailed methodology for optimizing the laser beam parameters and conducts extensive experiments to validate the effectiveness of the attack in both digital and physical settings.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```"], "6037652fd3485cfff1d903b1": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of scene classification in high spatial resolution (HSR) remote sensing images, emphasizing the need for a method that can automatically design network architectures. The proposed algorithm, SceneNet, is based on multi-objective neural evolution using evolutionary algorithms (EAs) for neural architecture search (NAS). The main contributions include a framework for EA-based NAS, flexible extraction of scene information, and a multi-objective trade-off for network design to balance computational complexity and accuracy.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b10\": 1,\n    \"b31\": 1,\n    \"b18\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.9,\n    \"b25\": 0.9,\n    \"b38\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.8,\n    \"b32\": 0.8,\n    \"b49\": 0.8,\n    \"b37\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing a satisfactory deep convolutional neural network (CNN) for remote sensing image scene classification without manual effort. To solve this problem, the authors propose SceneNet, an evolutionary algorithm (EA)-based neural architecture search (NAS) method that automatically discovers the optimal network architecture from the dataset itself. The approach involves flexible scene information extraction, powerful search capabilities, and multi-objective optimization to balance computational complexity and accuracy.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b25\": 1,\n    \"b38\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.9,\n    \"b29\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b43\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatically designing a deep convolutional neural network (CNN) for remote sensing scene classification using multi-objective neural evolution. The proposed solution, SceneNet, leverages an evolutionary algorithm-based neural architecture search (NAS) to balance computational complexity and classification accuracy, providing a flexible and powerful framework for optimizing network structures based on the dataset itself.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b25\": 1,\n    \"b14\": 1,\n    \"b38\": 0.9,\n    \"b32\": 0.9,\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b48\": 0.8,\n    \"b45\": 0.8,\n    \"b37\": 0.8,\n    \"b11\": 0.8,\n    \"b61\": 0.7,\n    \"b62\": 0.7,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b63\": 0.6,\n    \"b7\": 0.6,\n    \"b29\": 0.5,\n    \"b49\": 0.5,\n    \"b58\": 0.5,\n    \"b46\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scene classification in high spatial resolution (HSR) remote sensing images by proposing SceneNet, a framework for network architecture search based on multi-objective neural evolution. The main contributions include the automatic design of network architecture from data, the flexible extraction of scene information using evolutionary algorithms (EAs), and the trade-off between computational complexity and classification accuracy using multi-objective optimization.\",\n  \"Direct Inspiration\": [\"b8\", \"b10\", \"b31\", \"b18\"],\n  \"Indirect Inspiration\": [\"b24\", \"b25\", \"b38\", \"b44\"],\n  \"Other Inspiration\": [\"b52\", \"b54\", \"b53\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper outlines the challenges of remote sensing scene classification due to complex spatial and structural patterns. It proposes a novel framework, SceneNet, based on multi-objective neural evolution to automatically search for an optimal network architecture for scene classification. The methodology involves evolutionary algorithms (EAs) and multi-objective optimization to balance computational complexity and accuracy, leveraging the global search capability of EAs and local search capability of Bayesian optimization.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b10\": 0.8,\n    \"b31\": 0.8,\n    \"b18\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b38\": 0.6,\n    \"b44\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.5,\n    \"b48\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```"], "612c4c295244ab9dcbca2392": ["```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the joint extraction of entities and relations in text, overcoming the limitations of sequential and parallel encoding methods by proposing a joint encoding design with a partition filter network. The algorithm aims to ensure proper two-way interaction between Named Entity Recognition (NER) and Relation Extraction (RE) tasks while filtering out irrelevant information and mitigating error propagation.\",\n    \"Direct Inspiration\": {\n        \"b9\": 0.9,\n        \"b33\": 0.8,\n        \"b37\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.75,\n        \"b14\": 0.7,\n        \"b25\": 0.7,\n        \"b38\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.6,\n        \"b24\": 0.6,\n        \"b32\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of joint entity and relation extraction, aiming to overcome the limitations of traditional pipeline methods and previous joint methods that rely on sequential or parallel encoding. The proposed solution, the partition filter encoder, ensures proper two-way interaction between Named Entity Recognition (NER) and Relation Extraction (RE) tasks.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9,\n    \"b9\": 0.85,\n    \"b33\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b14\": 0.7,\n    \"b25\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b32\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of joint entity and relation extraction (JERE) by proposing a novel partition filter network designed specifically for joint encoding. The main challenges include the proper two-way interaction between Named Entity Recognition (NER) and Relation Extraction (RE) tasks and overcoming the flaws of sequential and parallel encoding methods.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9,\n    \"b33\": 0.85,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.75,\n    \"b21\": 0.7,\n    \"b25\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b17\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of joint entity and relation extraction, aiming to overcome the limitations of pipelined methods and improve the interaction between Named Entity Recognition (NER) and Relation Extraction (RE). It proposes a novel partition filter network that ensures two-way interaction between these tasks, enhancing the extraction of relational triples.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b33\": 0.9,\n    \"b37\": 0.9,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b14\": 0.8,\n    \"b25\": 0.8,\n    \"b27\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b29\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of joint entity and relation extraction, specifically focusing on overcoming the limitations of sequential and parallel encoding methods. The proposed solution is a partition filter encoder that ensures proper two-way interaction between Named Entity Recognition (NER) and Relation Extraction (RE). This novel method involves partitioning and filtering neuron contributions to generate task-specific features while maintaining a shared partition for balanced inter-task interaction.\",\n  \"Direct Inspiration\": [\"b37\", \"b33\", \"b9\"],\n  \"Indirect Inspiration\": [\"b21\", \"b32\"],\n  \"Other Inspiration\": [\"b10\", \"b14\"]\n}\n```"], "60c1b4f691e0112cf43c2226": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of document-level relation extraction (RE), which involves identifying relationships between entities across multiple sentences. The proposed algorithm, Document U-shaped Network (DocuNet), formulates document-level RE as a semantic segmentation task to capture global interdependency among entity pairs. The primary contributions include leveraging a U-Net structure for segmentation and introducing a balanced softmax method to handle imbalance in relation distribution.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b5\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b21\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b15\": 0.5,\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of relation extraction (RE) at the document level, overcoming limitations of sentence-level RE by capturing inter-sentence relations and global interactions among multiple entity pairs. It introduces a novel model, Document U-shaped Network (DocuNet), which formulates document-level RE as a semantic segmentation task, leveraging U-Net structure and balanced softmax method for better performance.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b8\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b14\": 0.9,\n    \"b21\": 0.9,\n    \"b31\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.8,\n    \"b25\": 0.8,\n    \"b15\": 0.8,\n    \"b20\": 0.8,\n    \"b22\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of extracting relations between entities that span multiple sentences within a document, which is not adequately handled by traditional sentence-level relation extraction (RE) methods. The authors propose DocuNet, a novel model that formulates document-level RE as a semantic segmentation task, utilizing a U-shaped segmentation module to capture global interdependency among entity pairs. The model leverages both local and global information and introduces a balanced softmax method to address the imbalance in relation distribution.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b8\": 0.8,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b11\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.65,\n    \"b25\": 0.6,\n    \"b15\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extracting document-level relations in information extraction, particularly focusing on inter-sentence relations. The proposed solution is a novel model called Document U-shaped Network (DocuNet), which formulates document-level relation extraction as a semantic segmentation task. This model captures global interdependency among multiple relation triples using an encoder module and a U-shaped segmentation module.\",\n  \"Direct Inspiration\": [\"b5\", \"b8\", \"b12\"],\n  \"Indirect Inspiration\": [\"b4\", \"b14\", \"b31\"],\n  \"Other Inspiration\": [\"b10\", \"b25\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extracting document-level relations in information extraction, proposing a novel model called Document U-shaped Network (DocuNet) that formulates the task as a semantic segmentation problem. The method aims to capture global interdependencies among multiple entity pairs within a document.\",\n  \"Direct Inspiration\": [\"b5\", \"b8\"],\n  \"Indirect Inspiration\": [\"b4\", \"b12\"],\n  \"Other Inspiration\": [\"b10\", \"b15\", \"b25\", \"b31\"]\n}\n```"], "60cd578491e011329faa21b0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of high computational costs and inference latency in large recommender systems by proposing a new knowledge distillation approach. The primary focus is on transferring relational knowledge from a pre-trained large model (teacher) to a compact model (student) using a methodology called Hierarchical Topology Distillation (HTD). The novel approach involves building a topological structure that represents relations in the teacher space based on similarity information and utilizing it to guide the learning of the student. The paper introduces HTD, which structures the topology hierarchically and transfers knowledge in multi-levels, effectively coping with the capacity gap between teacher and student models.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b9\": 0.9,\n    \"b11\": 0.8,\n    \"b21\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b17\": 0.6,\n    \"b23\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high computational costs and inference latency in large recommender systems (RS) by introducing a novel distillation approach called Hierarchical Topology Distillation (HTD). This method transfers relational knowledge from a teacher model to a student model, focusing on preserving topological structures in the representation space. The key contributions include the development of HTD for effectively transferring vast relational knowledge and the validation of its superiority through extensive experiments.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b7\": 1,\n    \"b19\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b21\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high computational costs and inference latency in large recommender systems (RS) by proposing a novel distillation approach called Hierarchical Topology Distillation (HTD). HTD transfers relational knowledge from a teacher model to a student model by preserving the topological structure of the teacher's representation space. This approach improves recommendation performance by focusing on important relations among entities, considering the capacity gap between the teacher and the student models.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b9\": 0.8,\n    \"b30\": 0.7,\n    \"b19\": 0.6\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.5,\n    \"b21\": 0.5,\n    \"b25\": 0.5\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of high inference latency in large recommender systems by proposing a novel Hierarchical Topology Distillation (HTD) approach. This approach effectively transfers relational knowledge from a large teacher model to a compact student model, addressing the capacity gap issue and improving recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b11\": 1,\n    \"b21\": 1,\n    \"b25\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b19\": 0.8,\n    \"b30\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high computational costs and inference latency in large recommender systems by proposing a new distillation approach. The novel method, Hierarchical Topology Distillation (HTD), effectively transfers relational knowledge from a teacher model to a student model, considering the huge capacity gap. HTD represents the topology hierarchically and transfers knowledge in multi-levels to improve recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b9\": 0.8,\n    \"b11\": 0.8,\n    \"b21\": 0.8,\n    \"b25\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b27\": 0.6,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5\n  }\n}\n```"], "60641c869e795e72406b65e2": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of high computational costs and long inference times in large-scale recommender systems by proposing a novel Bidirectional Distillation (BD) framework. This framework enables both the teacher and student recommenders to transfer knowledge to each other, enhancing each other's performance reciprocally. Key contributions include the development of a rank discrepancy-aware sampling scheme and extensive validation of the proposed framework's effectiveness through experiments.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b6\": 0.5,\n    \"b7\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of large-scale recommender systems (RS) with high computational costs and long inference times. It proposes a novel Bidirectional Distillation (BD) framework where both teacher and student models transfer knowledge to each other, leveraging rank discrepancy-aware sampling to improve performance.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b6\": 0.6,\n    \"b10\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high computational costs and long inference times in large recommender systems (RS). It proposes a novel Bidirectional Distillation (BD) framework where both teacher and student models transfer knowledge to each other, unlike existing unidirectional knowledge distillation methods.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b20\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges associated with the high computational costs and long inference times of large recommender systems (RS) by proposing a novel Bidirectional Distillation (BD) framework. This framework allows both the teacher and student models to transfer knowledge to each other, improving their performance collaboratively. The BD framework also introduces a rank discrepancy-aware sampling scheme to enhance the effectiveness of knowledge distillation.\",\n    \"Direct Inspiration\": [\n        \"b15\",\n        \"b22\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b8\",\n        \"b4\",\n        \"b6\"\n    ],\n    \"Other Inspiration\": [\n        \"b20\",\n        \"b24\",\n        \"b26\"\n    ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high computational costs and long inference times in large recommender systems by proposing a novel Bidirectional Distillation (BD) framework. This framework allows for bidirectional knowledge transfer between a teacher model and a student model, utilizing a rank discrepancy-aware sampling scheme to effectively enhance both models.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b20\": 0.4,\n    \"b26\": 0.4,\n    \"b27\": 0.3\n  }\n}\n```"], "61397b715244ab9dcb323a92": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of high computational cost and latency in large recommendation models through a proposed Dual Correction strategy for Distillation (DCD). This approach dynamically adjusts the knowledge transferred from a teacher model to a student model based on the student's prediction errors, and applies corrections on both user-side and item-side ranking information to improve recommendation performance.\",\n  \"Direct Inspiration\": [\"b8\"],\n  \"Indirect Inspiration\": [\"b18\", \"b25\", \"b6\", \"b13\"],\n  \"Other Inspiration\": [\"b10\", \"b16\", \"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing inference latency and computational cost in large recommender systems models by proposing a novel Dual Correction strategy for Distillation (DCD). The DCD method dynamically adjusts the knowledge transfer based on the prediction errors of the student model and incorporates both user-side and item-side ranking information to improve recommendation accuracy.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b18\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.6,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing inference latency in large recommendation models by proposing a novel Dual Correction strategy for Distillation (DCD). This method dynamically adjusts the knowledge to be distilled based on prediction errors and incorporates dual-side ranking information to improve recommendation accuracy.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b13\": 0.8,\n    \"b25\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b3\": 0.6,\n    \"b5\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high computational cost and latency in large recommendation models. It proposes a novel Dual Correction strategy for Distillation (DCD) to improve knowledge distillation by dynamically adjusting based on prediction errors and considering both user-side and item-side ranking information.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b6\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b18\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b3\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high computational cost and latency in large recommendation models by proposing a novel Dual Correction strategy for Distillation (DCD). The DCD strategy aims to dynamically correct the student model's prediction errors based on discrepancies between the teacher and student models, providing a comprehensive view by considering both user-side and item-side rankings.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```"], "6198833e5244ab9dcb119fa0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting 3D protein structures from single amino acid sequences without relying on MSAs. The authors introduce RGN2, an ML-based recurrent geometric network that uses a protein language model (AminoBERT) and the Frenet-Serret formulas to predict the geometry of protein backbones. This approach is efficient and has advantages for orphan and designed proteins where MSAs are not available.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b26\": 1.0,\n    \"b21\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b19\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of predicting 3D protein structures from single amino acid sequences without relying on multiple sequence alignments (MSAs). The authors propose a novel machine learning-based algorithm, RGN2, which utilizes a protein language model (AminoBERT) and the Frenet-Serret formulas to achieve this. RGN2 is designed to be computationally efficient and to perform well on orphan and designed proteins that lack sequence homologs.\",\n    \"Direct Inspiration\": {\n        \"b7\": 1.0,\n        \"b26\": 1.0,\n        \"b21\": 0.9,\n        \"b25\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b27\": 0.7,\n        \"b29\": 0.7,\n        \"b37\": 0.6,\n        \"b40\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b45\": 0.5,\n        \"b46\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting 3D protein structure from single amino acid sequences, particularly for proteins that lack sequence homologs. The authors propose a new ML-based model, RGN2, which uses a protein language model (AminoBERT) and Frenet-Serret formulas to improve prediction accuracy and computational efficiency.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b39\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting 3D protein structure from single amino acid sequences, particularly for proteins without homologous sequences. It introduces RGN2, a machine learning-based model that uses a protein language model (AminoBERT) and Frenet-Serret formulas for geometrical representation. This approach aims to provide accurate and efficient predictions without relying on multiple sequence alignments (MSAs). RGN2 is shown to outperform other methods like AlphaFold2 and RoseTTAFold on orphan proteins and is highly computationally efficient.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b21\": 0.9,\n    \"b25\": 0.9,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b29\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b27\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting 3D protein structures from single amino acid sequences without relying on multiple sequence alignments (MSAs). They propose an algorithm called RGN2 that uses a protein language model (AminoBERT) and the Frenet-Serret formulas to predict protein structures directly from single sequences. This method aims to provide fast and cost-effective structure predictions suitable for orphan proteins and protein design applications.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b26\": 0.8,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```"], "600832529e795ed227f53100": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of existing graph pooling methods in graph neural networks (GNNs) which either lose information by dropping nodes or have high computational complexity. The paper proposes the Graph Multiset Transformer (GMT), a novel graph pooling mechanism using a graph-structured attention unit to encode structural dependencies and enhance representation power. The GMT aims to be as powerful as the Weisfeiler-Lehman (WL) graph isomorphism test in distinguishing different graphs and demonstrates superior performance in graph classification, reconstruction, and generation tasks.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b46\": 0.85,\n    \"b52\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.75,\n    \"b48\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b5\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Limitations of existing graph pooling methods including node drop and node clustering\",\n      \"Loss of information and high computational complexity in current approaches\",\n      \"Need for a graph pooling function as powerful as the Weisfeiler-Lehman test\"\n    ],\n    \"inspirations\": [\n      \"Graph representation learning as a multiset encoding problem\",\n      \"Graph-structured attention unit as the fundamental building block for pooling\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b46\": 0.9,\n    \"b52\": 0.85,\n    \"b48\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.75,\n    \"b27\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.65,\n    \"b21\": 0.6,\n    \"b15\": 0.55\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the limitation of existing graph pooling methods in accurately representing the entire graph structure without significant information loss or high computational complexity. The proposed algorithm, Graph Multiset Transformer (GMT), aims to overcome these limitations by treating the graph pooling problem as a multiset encoding problem and utilizing graph-structured attention to enhance the representation power of the graph. The paper highlights its contributions in terms of improved graph classification, reconstruction, and generation tasks.\",\n    \"Direct Inspiration\": {\n        \"b52\": 0.9,\n        \"b48\": 0.8,\n        \"b23\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b46\": 0.7,\n        \"b4\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b27\": 0.5,\n        \"b15\": 0.5,\n        \"b43\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in graph pooling for graph neural networks (GNNs), specifically focusing on the limitations of node drop and node clustering methods. It proposes a new algorithm called Graph Multiset Transformer (GMT) to improve graph representation by treating graph pooling as a multiset encoding problem and utilizing a graph-structured attention unit to capture structural dependencies among nodes.\",\n  \"Direct Inspiration\": {\n    \"b52\": 1,\n    \"b48\": 0.9,\n    \"b4\": 0.9,\n    \"b23\": 0.9,\n    \"b46\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b43\": 0.8,\n    \"b27\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b16\": 0.5,\n    \"b21\": 0.5,\n    \"b1\": 0.4,\n    \"b50\": 0.4,\n    \"b10\": 0.3,\n    \"b11\": 0.3,\n    \"b8\": 0.3,\n    \"b31\": 0.2,\n    \"b47\": 0.2,\n    \"b37\": 0.2,\n    \"b51\": 0.2,\n    \"b32\": 0.2,\n    \"b22\": 0.2,\n    \"b34\": 0.2,\n    \"b25\": 0.2,\n    \"b3\": 0.2,\n    \"b35\": 0.2,\n    \"b29\": 0.2,\n    \"b39\": 0.2,\n    \"b49\": 0.2,\n    \"b38\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating effective graph neural network (GNN) pooling mechanisms that can accurately represent entire graphs while overcoming limitations of existing methods like node drop and node clustering. The proposed solution is the Graph Multiset Transformer (GMT), which utilizes a graph-structured attention unit to condense the graph into representative nodes and encode relationships between them. The GMT aims to achieve a representation power comparable to the Weisfeiler-Lehman (WL) graph isomorphism test and is validated on various graph-related tasks.\",\n  \"Direct Inspiration\": {\n    \"b52\": 0.9,\n    \"b48\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.7,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b43\": 0.6,\n    \"b27\": 0.5\n  }\n}\n```"], "60338c4491e011e54d039c4e": ["```json\n{\n  \"Summary\": \"The paper addresses the over-smoothing problem in GCN-based recommendation models, particularly when high-order neighbors are aggregated without distinguishing user interests. The proposed solution, IMP-GCN, groups users and their interacted items into subgraphs based on similar interests, thereby filtering out negative information propagation and alleviating over-smoothing.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b36\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a novel Interest-aware Messaging-Passing GCN (IMP-GCN) recommendation model to address the over-smoothing problem in existing GCN-based recommendation models. The primary challenge is the aggregation of dissimilar user embeddings, which leads to performance degradation. The proposed model groups users with similar interests into subgraphs and operates high-order graph convolutions within these subgraphs to filter out negative information propagation and maintain the uniqueness of user embeddings.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b13\": 1.0,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b25\": 0.6,\n    \"b41\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the over-smoothing problem in GCN-based recommendation models, which leads to homogenous user embeddings and performance degradation. The proposed solution, IMP-GCN, aims to group users with similar interests into subgraphs and limit high-order graph convolution operations within these subgraphs to filter out negative information propagation.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the over-smoothing problem in GCN-based recommendation systems, where embedding of users with dissimilar interests becomes similar due to high-order graph convolutions. The authors propose an Interest-aware Messaging-Passing GCN (IMP-GCN) model that constructs subgraphs based on user interests to mitigate this issue and improve recommendation performance.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1,\n        \"b32\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b36\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.5,\n        \"b14\": 0.5,\n        \"b20\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in this paper is the over-smoothing problem in GCN-based recommendation models. The proposed Interest-aware Messaging-Passing GCN (IMP-GCN) model aims to solve this by grouping users with similar interests into subgraphs and performing high-order graph convolutions within these subgraphs to filter out negative information propagation. The model adopts a simplified network structure similar to LightGCN.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b13\": 1,\n    \"b36\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.9,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```"], "612eef545244ab9dcbe125fc": ["```json\n{\n  \"Summary\": \"The paper addresses the issue of over-confident predictions and overfitting in graph neural networks (GNNs) caused by label bias from sub-graph sampling methods in large-scale graphs. The authors propose an Adaptive Label Smoothing (ALS) method to mitigate these issues by considering local neighborhood label distributions and global label relevance.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b16\": 0.8,\n    \"b21\": 0.85,\n    \"b40\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.7,\n    \"b47\": 0.65,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.55,\n    \"b51\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of over-confident prediction and overfitting in large-scale graph neural networks (GNNs) due to label bias in sub-graph sampling methods. The authors propose an Adaptive Label Smoothing (ALS) method characterized by three stages: label propagation, label refinement, and a smooth pacing function. The ALS method aims to regularize the sub-graph batch training by adjusting the label smoothing based on local neighborhood structure and global label relevance, which leads to better generalization performance in node classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b16\": 0.85,\n    \"b22\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.75,\n    \"b47\": 0.75,\n    \"b21\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.65,\n    \"b51\": 0.65,\n    \"b0\": 0.6,\n    \"b1\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the over-prediction and overfitting issues in large-scale graph neural networks (GNNs) caused by label bias in sub-graph sampling methods. It proposes an adaptive label smoothing (ALS) method to regularize representation learning on large-scale graphs. The primary contributions include analyzing the label bias problem, presenting a simple and memory-efficient ALS method, and proposing a label smoothing pacing function to allocate different smoothing strengths during training.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b16\": 0.9,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b29\": 0.6,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the overfitting and over-confident prediction issues in large-scale graph neural networks (GNNs) caused by label bias in sub-graph sampling methods. It proposes an adaptive label smoothing (ALS) method that consists of label propagation, label refinement, and a smooth pacing function to better regularize the model during training and improve generalization performance.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b14\": 0.9,\n    \"b16\": 0.9,\n    \"b21\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b40\": 0.7,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b27\": 0.6,\n    \"b36\": 0.6,\n    \"b49\": 0.6,\n    \"b59\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of overfitting and over-confident predictions in graph neural networks (GNNs) due to label bias in sub-graph sampling methods. The authors propose an adaptive label smoothing (ALS) method to regularize representation learning on large-scale graphs. The ALS method includes a label propagation preprocessing step, a label refinement step to learn global label relevance, and a smoothing pacing function to prevent over-regularization during the initial training phase. The empirical results demonstrate that ALS effectively alleviates overfitting and improves generalization performance in node classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b16\": 0.85,\n    \"b28\": 0.8,\n    \"b40\": 0.75,\n    \"b21\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.65,\n    \"b49\": 0.6,\n    \"b51\": 0.55,\n    \"b33\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.45,\n    \"b0\": 0.4,\n    \"b1\": 0.35,\n    \"b36\": 0.3\n  }\n}\n```"], "60b9a33be4510cd7c8f8b67d": ["```json\n{\n  \"Summary\": \"The paper tackles the challenge of accurately capturing user preferences in recommendation systems, addressing the variability in sequentiality across different recommendation scenarios and the neglect of semantic item relations in existing models. The authors propose a framework that combines sequential and semantic item relations through a hybrid item graph and uses simplified graph convolutions to smooth item embeddings for improved recommendations.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b5\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.8,\n    \"b4\": 0.8,\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in recommender systems (RS), specifically the dynamics in user preferences and the limitations of sequential models in capturing both sequential and semantic item relations. The proposed solution is a framework that combines sequential and semantic item graphs for embedding smoothing to improve recommendation accuracy. The method involves constructing a hybrid item graph, performing graph convolutions, and integrating the smoothed embeddings into sequential models like SASRec.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b3\", \"b5\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b0\", \"b1\", \"b2\", \"b4\", \"b6\", \"b7\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b8\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in sequential recommendation systems, particularly the varying sequentiality of user interactions and the lack of consideration for semantic item relations. The proposed solution is a framework that constructs a hybrid item graph combining both sequential and semantic item relations and performs graph convolutions to generate smoothed item embeddings. This method aims to enhance the generalization ability of sequential models like SASRec.\",\n    \"Direct Inspiration\": {\n        \"b3\": 1,\n        \"b5\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.8,\n        \"b2\": 0.8,\n        \"b4\": 0.7,\n        \"b6\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b7\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in capturing dynamic user preferences for recommender systems, particularly where sequential dependencies vary across different scenarios. It proposes a hybrid item graph that combines sequential and semantic item relations to smooth item embeddings and improve recommendation accuracy. The approach employs simplified graph convolution tailored for recommendation problems and evaluates the method using SASRec on three datasets.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b2\": 0.7,\n    \"b4\": 0.7,\n    \"b6\": 0.6,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n    \"Summary\": \"The paper aims to tackle the challenge of accurately capturing user preferences in recommendation systems, particularly addressing the issues with sequential recommendation models that do not always account for variations in user interactions and semantic item relations. The proposed method involves constructing a hybrid item graph that combines sequential and semantic item relations and applying simplified graph convolutions to smooth item embeddings, enhancing the performance of sequential recommendation models.\",\n    \"Direct Inspiration\": {\n        \"b3\": 1,\n        \"b5\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.8,\n        \"b2\": 0.7,\n        \"b4\": 0.7,\n        \"b7\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.5,\n        \"b6\": 0.5,\n        \"b8\": 0.5\n    }\n}\n```"], "611106eb91e0117e7f3cdab1": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"High computational intensity of graph pattern mining (GPM).\",\n      \"Inefficiency and limited generality of existing hardware accelerators.\",\n      \"Lack of memoization support in current accelerators leading to redundant computations.\"\n    ],\n    \"Inspirations\": [\n      \"To develop a pattern-aware GPM accelerator that offers both generality and performance.\",\n      \"Improving efficiency through massive multithreading and specialized processing engines.\",\n      \"Utilizing hardware support for memoizing connectivity information to minimize redundant computations.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"References\": [\"b43\", \"b56\", \"b57\", \"b45\", \"b89\"]\n  },\n  \"Indirect Inspiration\": {\n    \"References\": [\"b15\", \"b81\", \"b83\", \"b24\", \"b94\", \"b70\"]\n  },\n  \"Other Inspiration\": {\n    \"References\": [\"b18\", \"b6\", \"b59\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents FlexMiner, a software-hardware co-designed system for graph pattern mining (GPM). It addresses the inefficiency of existing hardware accelerators and software systems in terms of performance, generality, and ease of programming. FlexMiner introduces a pattern-aware depth-first search algorithm, specialized processing engines, and connectivity memoization to significantly improve the efficiency of GPM tasks.\",\n  \"Direct Inspiration\": {\n    \"b43\": 0.9,\n    \"b57\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.75,\n    \"b89\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b56\": 0.65,\n    \"b48\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is the inefficiency and lack of generality in existing hardware accelerators for Graph Pattern Mining (GPM). The paper introduces FlexMiner, a software-hardware co-designed GPM system that aims to provide both generality and performance while maintaining ease of programming. FlexMiner achieves this through a pattern-aware approach, massive multithreading, PE specialization, and connectivity memoization.\",\n  \"Direct Inspiration\": {\n    \"b43\": 0.9,\n    \"b57\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.8,\n    \"b56\": 0.8,\n    \"b89\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b83\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Computational complexity of graph pattern mining (GPM) due to massive combinatorial search space and expensive graph isomorphism tests.\",\n      \"Inefficiency and limited generality of existing hardware accelerators for GPM.\",\n      \"Lack of well-defined software/hardware interface for configuring/programming accelerators.\",\n      \"Redundant computations due to lack of support for memoization in existing accelerators.\"\n    ],\n    \"Inspirations\": [\n      \"Inspiration from state-of-the-art software GPM systems for algorithmic techniques.\",\n      \"Designing a flexible, pattern-aware accelerator that can handle a variety of GPM applications efficiently.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b43\", \"b57\", \"b56\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b45\", \"b89\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b81\", \"b83\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of graph pattern mining (GPM) and proposes FlexMiner, a software-hardware co-designed GPM system. FlexMiner aims to improve generality, performance, and ease of programming by leveraging pattern-aware techniques, massive multithreading, PE specialization, and connectivity memoization.\",\n  \"Direct Inspiration\": {\n    \"b57\": 1.0,\n    \"b43\": 1.0,\n    \"b45\": 1.0,\n    \"b89\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b56\": 0.8,\n    \"b48\": 0.7\n  }\n}\n```"], "61f753205aee126c0f9c2149": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include memory and compute efficiency for training large-scale language models. The paper introduces Megatron-Turing NLG 530B, a transformer-based language model with 530 billion parameters. Key innovations include a 3D parallel system combining data, pipeline, and tensor-slicing parallelism, and the development of high-quality training corpora and optimization techniques.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b61\": 0.85,\n    \"b55\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b50\": 0.7,\n    \"b35\": 0.7,\n    \"b65\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b59\": 0.55,\n    \"b53\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges and innovations involved in training the Megatron-Turing NLG 530B, a 530 billion parameter transformer-based language model. The primary challenges include memory and compute efficiency, as well as the limitations of existing parallelism strategies such as data, tensor, and pipeline parallelism. The paper also highlights the collaboration between NVIDIA's Megatron-LM and Microsoft's DeepSpeed to create a scalable 3D parallel system that combines data, pipeline, and tensor-slicing based parallelism.\",\n  \"Direct Inspiration\": {\n    \"b50\": 0.95,\n    \"b8\": 0.90,\n    \"b31\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b61\": 0.80,\n    \"b55\": 0.75,\n    \"b63\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b54\": 0.65,\n    \"b53\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of memory and compute efficiency in training large-scale language models, particularly focusing on the trade-offs in data, tensor, and pipeline parallelism. It introduces Megatron-Turing NLG 530B, a 530 billion parameter transformer-based language model. The authors propose a novel 3D parallelism system that combines data, tensor, and pipeline parallelism to achieve efficient training on large-scale GPU clusters.\",\n  \"Direct Inspiration\": {\n    \"b61\": 0.9,\n    \"b55\": 0.8,\n    \"b63\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b50\": 0.7,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b54\": 0.5,\n    \"b53\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper discusses the challenges and solutions in training large-scale language models, specifically focusing on memory and compute efficiency, as well as the trade-offs between different parallelism techniques such as data, tensor, and pipeline parallelism. The authors introduce Megatron-Turing NLG 530B, a language model with 530 billion parameters, and describe the innovations in parallelism techniques and training infrastructure that made its training feasible.\",\n    \"Direct Inspiration\": {\n        \"b7\": 1,\n        \"b8\": 0.95,\n        \"b31\": 0.9,\n        \"b55\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.85,\n        \"b35\": 0.85,\n        \"b50\": 0.85\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.8,\n        \"b25\": 0.8,\n        \"b40\": 0.8,\n        \"b41\": 0.75,\n        \"b53\": 0.75,\n        \"b54\": 0.75,\n        \"b59\": 0.7,\n        \"b61\": 0.75,\n        \"b63\": 0.75,\n        \"b65\": 0.75\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper discusses the challenges of training large-scale language models, specifically memory and compute efficiency, and the trade-offs of various parallelism strategies such as data, tensor, and pipeline parallelism. The authors introduce a transformer-based language model, Megatron-Turing NLG 530B (MT-NLG), which has 530 billion parameters. The model is trained using a 3D parallelism approach that combines data, tensor, and pipeline parallelism to achieve memory and compute efficiency. The work is built on innovations from NVIDIA Megatron-LM and Microsoft DeepSpeed, leveraging advanced hardware and software systems for large-scale model training.\",\n    \"Direct Inspiration\": {\n        \"b61\": 1,\n        \"b55\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b8\": 0.8,\n        \"b31\": 0.7,\n        \"b53\": 0.7,\n        \"b54\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b25\": 0.6,\n        \"b35\": 0.6,\n        \"b40\": 0.6,\n        \"b50\": 0.6,\n        \"b59\": 0.6,\n        \"b63\": 0.6,\n        \"b65\": 0.6\n    }\n}\n```"], "602ce77791e011c3e8f66a93": ["```json\n{\n  \"Summary\": \"The paper proposes TeraPipe, a high-performance synchronous model parallel training approach for large-scale Transformer-based language models. It leverages the token dimension for pipeline parallel training and uses a dynamic programming algorithm to compute optimal partitioning of the token dimension for the pipeline. The key challenges addressed include communication overheads and under-utilization of computational resources in existing parallel training methods.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.95,\n    \"b20\": 0.9,\n    \"b2\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b23\": 0.75,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.65,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient model parallel training for large Transformer-based language models. It proposes TeraPipe, a new pipeline parallelism method that leverages the token dimension to improve training throughput. The method introduces a dynamic programming algorithm to compute optimal token partitioning for the pipeline, which is orthogonal to existing model parallel techniques and can be combined with them for enhanced performance.\",\n  \"Direct Inspiration\": [\"b6\", \"b20\"],\n  \"Indirect Inspiration\": [\"b2\", \"b17\"],\n  \"Other Inspiration\": [\"b19\", \"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently training large-scale Transformer-based language models by proposing a novel pipeline parallel training method called TeraPipe. TeraPipe leverages the token dimension to create a fine-grained pipeline within a single training sequence, improving the training throughput for large models like GPT-3. The paper highlights the limitations of existing model-parallel training methods and introduces a dynamic programming algorithm to optimize the token dimension partitioning for maximum pipeline efficiency.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.75,\n    \"b19\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of efficiently training large-scale Transformer-based language models on existing hardware by introducing a new pipeline parallelism method, TeraPipe, which leverages the token dimension for parallel training. The authors propose a dynamic programming algorithm to compute the optimal partitioning of the token dimension and demonstrate significant speedup over state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of training large-scale Transformer-based language models (LMs) efficiently. It introduces TeraPipe, a novel method that leverages the token dimension for pipeline parallel training to overcome issues related to excessive communication overheads and pipeline bubbles in existing methods. TeraPipe uses a dynamic programming algorithm to compute optimal partitioning of the token dimension, resulting in significant speedup in training large models like GPT-3.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b6\": 1.0,\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b1\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```"], "60af645391e01165606816a9": ["```json\n{\n  \"Summary\": \"The paper presents the Focus Attention Mechanism (FAME) to address challenges in document summarization, specifically focusing on generating faithful and diverse summaries. FAME enhances transformer-based seq2seq architectures by introducing source-conditioned vocabulary biasing and Focus Sampling techniques inspired by human summarization processes.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b33\": 0.9,\n    \"b51\": 0.8,\n    \"b53\": 0.85,\n    \"b84\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b62\": 0.75,\n    \"b64\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.65,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenges of generating diverse and faithful summaries in document summarization using a novel Focus Attention Mechanism (FAME) in transformer-based seq2seq architectures. FAME aims to enhance source-side planning by dynamically biasing the vocabulary based on the input content, inspired by human summarization techniques. It introduces Focus Sampling to promote diversity while maintaining faithfulness in generated summaries.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b33\": 0.9,\n    \"b51\": 0.9,\n    \"b62\": 0.8,\n    \"b84\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.7,\n    \"b53\": 0.7,\n    \"b64\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b24\": 0.5,\n    \"b40\": 0.5,\n    \"b69\": 0.5 \n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in generating document summaries that are both faithful to the input and diverse. The authors propose a novel Focus Attention MEchanism (FAME) for transformer-based seq2seq architectures. FAME aims to enhance the consistency and topical relevance of generated summaries by introducing a dynamic source-conditioned vocabulary biasing layer. The method is evaluated using the BBC extreme summarization task and shows improvements in generating summaries that are faithful and diverse.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b33\": 0.9,\n    \"b51\": 0.9,\n    \"b53\": 0.9,\n    \"b84\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b62\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b37\": 0.7,\n    \"b40\": 0.7,\n    \"b70\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in document summarization, specifically generating summaries that are both faithful to the input and diverse. The authors propose a Focus Attention MEchanism (FAME) to enhance transformer-based seq2seq architectures, inspired by human summarization techniques. FAME integrates a dynamic source-conditioned vocabulary biasing layer to improve summary faithfulness and introduces Focus Sampling to promote diversity in generated summaries.\",\n    \"Direct Inspiration\": {\n        \"b51\": 0.9,\n        \"b21\": 0.85,\n        \"b33\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b84\": 0.8,\n        \"b62\": 0.8,\n        \"b53\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b24\": 0.7,\n        \"b26\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The main challenges outlined in the paper are generating summaries that are faithful to the input and ensuring diversity in the generated summaries. The proposed algorithm, FAME (Focus Attention MEchanism), enhances transformer-based seq2seq architectures by implementing a dynamic source-conditioned vocabulary biasing layer to improve the faithfulness and diversity of summaries. FAME introduces a novel Focus Sampling technique to achieve topically relevant and diverse outputs.\",\n    \"Direct Inspiration\": {\n        \"b21\": 0.95,\n        \"b33\": 0.95,\n        \"b51\": 0.9,\n        \"b53\": 0.9,\n        \"b62\": 0.85,\n        \"b84\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b24\": 0.8,\n        \"b29\": 0.75,\n        \"b32\": 0.75,\n        \"b40\": 0.75,\n        \"b70\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.7,\n        \"b28\": 0.7,\n        \"b34\": 0.7,\n        \"b52\": 0.7,\n        \"b74\": 0.7\n    }\n}\n```"], "61397b715244ab9dcb323aa9": ["```json\n{\n  \"Summary\": \"The paper discusses the challenges of generating high-quality abstractive summaries and proposes a novel method called SeqCo, which leverages contrastive learning to improve the similarity between the document and its summary. This method does not require negative examples and uses multiple views of the same meaning representation to train the model.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b14\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b1\": 0.55,\n    \"b26\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are related to improving the quality of abstractive summarization by ensuring that the generated summaries retain the key information from the source documents and are faithful to the original content. The authors propose a novel algorithm, SeqCo (Sequence Level Contrastive Learning), which utilizes contrastive learning to enhance the semantic similarity between the document and its summary, as well as between the generated summaries. This approach leverages the properties of the summarization task and dynamically generated summaries during training to improve performance.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b3\": 1,\n    \"b13\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.7,\n    \"b18\": 0.7,\n    \"b5\": 0.7,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b2\": 0.5,\n    \"b20\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is improving the quality of abstractive document summarization by explicitly modeling the similarity between the document and its summary. The authors propose a novel algorithm named SeqCo (Sequence Level Contrastive Learning) that leverages contrastive learning techniques to enhance the generation of summaries. This approach aims to map representations of a document and its summary (or generated summary) to the same vector space, thus improving the quality of the generated summaries.\",\n  \"Direct Inspiration\": [\n    \"b9\",\n    \"b3\"\n  ],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\n    \"b13\",\n    \"b14\",\n    \"b5\",\n    \"b18\",\n    \"b26\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving abstractive summarization by leveraging contrastive learning methods to ensure that the document and its summary convey the same meaning. The proposed model, SeqCo, employs contrastive learning without negative examples, drawing inspiration from similar approaches in computer vision. The primary innovation is the adaptation of contrastive learning to sequence-to-sequence models for text summarization, improving the quality and faithfulness of generated summaries.\",\n  \"Direct Inspiration\": [\"b9\", \"b3\"],\n  \"Indirect Inspiration\": [\"b13\", \"b14\", \"b5\"],\n  \"Other Inspiration\": [\"b26\", \"b18\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is improving abstractive summarization models by enhancing their ability to generate summaries that convey the same meaning as the original document. The paper introduces SeqCo, a Sequence Level Contrastive Learning model, which aims to map representations of a document and its summary to the same vector space, thereby improving the quality of generated summaries.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b26\": 0.7,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```"], "6125b0135244ab9dcb38b4e5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of data sparsity in session-based recommendation systems. It proposes a novel framework that combines self-supervised learning (SSL) with semi-supervised learning, specifically co-training, to create more informative self-supervision signals. The framework uses two asymmetric graph encoders trained on item and session views, respectively, to leverage complementary information and improve recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b42\": 1,\n    \"b56\": 1,\n    \"b43\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b16\": 0.7,\n    \"b29\": 0.6,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b1\": 0.5,\n    \"b6\": 0.5,\n    \"b21\": 0.4,\n    \"b5\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of data sparsity in session-based recommendation systems by proposing a novel framework that combines self-supervised learning (SSL) with semi-supervised learning (SSL) to create more informative self-supervision signals. The framework leverages co-training with two asymmetric graph encoders to improve recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b42\": 0.9,\n    \"b56\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b16\": 0.7,\n    \"b43\": 0.8,\n    \"b52\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b23\": 0.6,\n    \"b46\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of data sparsity in session-based recommendation systems by proposing a novel self-supervised framework that combines self-supervised learning (SSL) with semi-supervised learning (co-training). The framework uses two asymmetric graph encoders trained on different data views (item view and session view) to provide complementary information and enhance recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b42\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.9,\n    \"b29\": 0.85,\n    \"b56\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.75,\n    \"b43\": 0.7,\n    \"b52\": 0.65,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of data sparsity in session-based recommendation systems. The proposed solution combines self-supervised learning (SSL) with semi-supervised learning through a novel framework called self-supervised graph co-training. The method involves constructing two views (item and session views) from session data, training two asymmetric graph encoders using a co-training regime, and leveraging contrastive learning to refine item and session representations.\",\n  \"Direct Inspiration\": {\n    \"b42\": 1.0,\n    \"b56\": 0.9,\n    \"b43\": 0.8,\n    \"b29\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.6,\n    \"b0\": 0.5,\n    \"b16\": 0.4,\n    \"b10\": 0.3\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.2,\n    \"b20\": 0.2,\n    \"b23\": 0.2,\n    \"b26\": 0.2,\n    \"b52\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of data sparsity in session-based recommendation systems by proposing a novel framework that combines self-supervised learning (SSL) with semi-supervised learning via co-training. The proposed framework constructs two views (item view and session view) and uses asymmetric graph encoders trained under the co-training scheme to generate more informative self-supervision signals.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b42\": 0.9,\n    \"b56\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.7,\n    \"b29\": 0.6,\n    \"b50\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```"], "60b9a432e4510cd7c8fac114": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"How to take both the documents' content and their intent coverage into account for computing their similarity.\",\n      \"How to consider the complicated and dynamic relation of the query and documents during the document selection process.\"\n    ],\n    \"algorithm\": \"Graph4DIV, which models document similarity through intent coverage, leverages a dynamic intent graph, and uses Graph Convolutional Networks (GCN) for learning intent-aware document representations.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b17\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b8\", \"b13\", \"b15\", \"b31\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b2\", \"b37\", \"b38\", \"b47\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in search result diversification: accurately measuring document similarity based on intent coverage and considering dynamic relationships between queries and documents. The proposed method, Graph4DIV, uses a dynamic intent graph and graph convolutional networks (GCNs) to learn intent-aware document representations and context-aware query representation, improving the diversification performance over state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b31\": 0.9,\n    \"b38\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b37\": 0.7,\n    \"b47\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in search result diversification, specifically the need to account for both the documents' content and their intent coverage, and the complex, dynamic relationships between queries and documents during selection. Inspired by the limitations of existing methods, the authors propose Graph4DIV, leveraging a dynamic intent graph and Graph Convolutional Networks (GCN) to enhance document representation and achieve better diversification.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b38\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b47\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b11\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"How to account for both the documents' content and their intent coverage when computing their similarity.\",\n      \"How to consider the complicated and dynamic relation of the query and documents during the document selection process.\"\n    ],\n    \"inspirations\": [\n      \"Graph convolutional networks (GCNs) are adapted to a dynamic intent graph for learning the intent-aware document representations and the context-aware query representation.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b31\": 0.85,\n    \"b37\": 0.75,\n    \"b42\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.65,\n    \"b47\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in search result diversification, specifically focusing on how to effectively consider both document content and intent coverage to improve diversity ranking models. The proposed solution, Graph4DIV, leverages a dynamic intent graph and graph convolutional networks (GCNs) to directly model document similarity through intent coverage.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b37\": 0.7,\n    \"b47\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b13\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```"], "60641c109e795e72406b65a8": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of search result diversification, aiming to present relevant but diverse results at the top of a ranked list. It introduces a novel method that translates a diversity evaluation metric into a differentiable diversification-aware loss function. This function is optimized using deep neural networks, leveraging distributed representations of queries and documents. The method outperforms recent baselines in experiments on a public benchmark dataset.\",\n    \"Direct Inspiration\": {\n        \"b25\": 1.0,\n        \"b39\": 0.9,\n        \"b45\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b43\": 0.7,\n        \"b41\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.5,\n        \"b10\": 0.4,\n        \"b9\": 0.3\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The main challenges addressed in the paper are the inter-dependency among documents in search result diversification and the inefficiency of learning due to the exponentially large number of ranking lists.\",\n    \"inspirations\": \"The paper proposes a novel method that translates diversity evaluation metrics to differentiable diversification-aware loss functions. It leverages distributed representations of queries and documents and utilizes deep neural networks for learning to optimize these loss functions effectively.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b25\", \"b41\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b43\", \"b45\", \"b39\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b9\", \"b12\", \"b10\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of search result diversification by proposing a novel method that translates diversity evaluation metrics into differentiable loss functions. This method leverages distributed representations of queries and documents using deep neural networks. The main contributions include a new diversification-aware loss function, effective learning through gradient descent, and superior performance on benchmark datasets compared to recent baselines.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b39\": 0.9,\n    \"b41\": 0.8,\n    \"b43\": 0.7,\n    \"b45\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.6,\n    \"b12\": 0.5,\n    \"b10\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.4,\n    \"b26\": 0.4,\n    \"b31\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of search result diversification by proposing a novel method to translate diversity evaluation metrics into a differentiable diversification-aware loss function. This method leverages distributed representations of queries and documents using deep neural networks, allowing for effective end-to-end training and improved performance on benchmark datasets.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b39\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.8,\n    \"b43\": 0.7,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b31\": 0.5,\n    \"b29\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Search result diversification to address diverse user information needs and query ambiguity.\",\n      \"The inter-dependency among documents in learning-to-rank settings.\",\n      \"Expanding the effectiveness of learning-based approaches in the 'next document' paradigm with a large number of ranking lists.\"\n    ],\n    \"Inspirations\": [\n      \"Use of differentiable approximate ranks to estimate subtopic coverage and diversify search results.\",\n      \"Development of a neural network-based approach for learning distributed representations to improve diversification.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b43\": 1,\n    \"b45\": 1,\n    \"b17\": 0.9,\n    \"b39\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b13\": 0.7,\n    \"b33\": 0.7,\n    \"b22\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```"], "608be65c91e0112fc4e65c54": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of auditing recommender systems for envy-freeness, both at individual and group levels. It proposes an algorithm that ensures personalized recommendations are fair by exploring user preferences actively. The algorithm combines pure exploration in bandits with conservative constraints to maintain performance close to the audited system, and it provides theoretical guarantees and experimental validation.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b42\": 0.9,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b44\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.6,\n    \"b43\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of auditing recommender systems for preference-based fairness, specifically through the lens of envy-freeness. It proposes an algorithm that ensures conservative exploration to maintain performance close to the audited system while exploring user preferences to detect envy. The main contributions include formal definitions of envy-free recommendations for individuals and groups, and an active exploration process to audit for envy-freeness in personalized recommender systems.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b44\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.7,\n    \"b45\": 0.7,\n    \"b42\": 0.6,\n    \"b30\": 0.6,\n    \"b6\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.4,\n    \"b8\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are related to auditing recommender systems from the perspective of preference-based fairness, specifically focusing on envy-free recommendations. The authors propose a novel approach to audit for envy-freeness by allowing an auditor to replace a user's recommendations with those another user would receive in the same context. They extend envy-free recommendations to groups and propose an algorithm for auditing that maintains close performance to the audited system. The challenges include dealing with partially observed user preferences and balancing the trade-offs between statistical confidence, audit duration, and per-user cost of exploration.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b42\": 0.9,\n    \"b30\": 0.9,\n    \"b37\": 0.8,\n    \"b44\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.6,\n    \"b31\": 0.6,\n    \"b40\": 0.5,\n    \"b43\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b17\": 0.6,\n    \"b4\": 0.5,\n    \"b5\": 0.4,\n    \"b8\": 0.4,\n    \"b39\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of auditing recommender systems from the perspective of preference-based fairness, specifically focusing on envy-free recommendations. It proposes an algorithm that maintains performance close to the audited system while exploring user preferences actively to detect potential envy. The paper extends the concept of envy-freeness to groups and introduces a variant of pure exploration problems in bandits to tackle the auditing process.\",\n    \"Direct Inspiration\": {\n        \"b15\": 1,\n        \"b6\": 0.9,\n        \"b44\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b42\": 0.7,\n        \"b30\": 0.7,\n        \"b45\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b37\": 0.6,\n        \"b19\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are auditing recommender systems for preference-based fairness, specifically envy-freeness, and extending envy-freeness to groups. The proposed algorithm is an active exploration process that allows the auditor to replace a user's recommendations with those that another user would have received in the same context. The algorithm maintains performance close to the audited system while exploring user preferences and provides theoretical guarantees on envy-freeness auditing.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b42\": 1.0,\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.9,\n    \"b44\": 0.9,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b45\": 0.7\n  }\n}\n```"], "617b66765244ab9dcbb6a729": ["```json\n{\n  \"Summary\": \"The paper addresses the efficiency and scalability challenges of GCN-based models in collaborative filtering (CF) for recommendation systems. It proposes an ultra-simplified GCN model called UltraGCN, which skips explicit message passing layers, to achieve better training efficiency while maintaining or improving recommendation effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b24\": 0.5,\n    \"b27\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiency and complexity of current GNN-based Collaborative Filtering (CF) models, particularly focusing on the limitations of explicit message passing in large-scale recommendation systems. It introduces an ultra-simplified GCN model, UltraGCN, which eliminates explicit message passing layers and leverages a constraint loss to approximate infinite-layer graph convolutions. This approach aims to improve training efficiency and avoid issues such as over-smoothing.\",\n  \"Direct Inspiration\": [\"b3\", \"b9\", \"b16\"],\n  \"Indirect Inspiration\": [\"b4\", \"b15\"],\n  \"Other Inspiration\": [\"b1\", \"b26\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of inefficiency and scalability in Graph Convolutional Network (GCN)-based Collaborative Filtering (CF) models used in recommendation systems. It proposes an ultra-simplified GCN model named UltraGCN, which eliminates explicit message passing, instead using a constraint loss to directly approximate the infinite-layer graph convolution. This approach aims to improve training efficiency while maintaining or improving recommendation accuracy.\",\n    \"Direct Inspiration\": {\n        \"b9\": 1.0,\n        \"b26\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b16\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b27\": 0.6,\n        \"b24\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiency and complexity of current GCN-based models in collaborative filtering tasks, particularly in large-scale industrial applications. It proposes UltraGCN, an ultra-simplified GCN model that avoids explicit message passing and leverages a constraint loss to effectively capture high-order collaborative signals while mitigating over-smoothing issues.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b9\": 1.0,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.6,\n    \"b13\": 0.7,\n    \"b21\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b31\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency and training difficulty of current GCN-based collaborative filtering (CF) models, particularly when dealing with large graphs in industrial recommender systems. The paper proposes an ultra-simplified GCN model named UltraGCN, which bypasses explicit message passing layers to improve efficiency and retains effectiveness through a constraint loss mechanism. The novel approach allows for flexible adjustment of relationship importances and avoids over-smoothing through negative sampling.\",\n  \"Direct Inspiration\": [\"b9\"],\n  \"Indirect Inspiration\": [\"b3\", \"b16\"],\n  \"Other Inspiration\": [\"b12\", \"b24\", \"b27\", \"b26\"]\n}\n```"], "616e37435244ab9dcbd1a8c6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of memorization-based language models, specifically their difficulty in handling hard examples and the intensive memory requirements for memorizing the entire training data. The authors propose a novel language modeling scheme that utilizes a graph neural network (GNN) to reference similar contexts from the training data, thus facilitating an 'open-book' examination approach. This method integrates retrieved neighbors with the input using a directed heterogeneous graph, allowing the model to aggregate information from both inter-context and intra-context edges. The proposed GNN-LM combined with kNN-LM demonstrates significant performance improvements on language modeling benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.95,\n    \"b37\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.75,\n    \"b58\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.65,\n    \"b38\": 0.60,\n    \"b5\": 0.60,\n    \"b35\": 0.60\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper proposes a new language modeling scheme called GNN-LM that combines a base language model (LM) with a graph neural network (GNN) to reference similar contexts from the training corpus for better prediction. The main challenges addressed are the limitations of traditional memorization-based LMs to handle hard examples and the intensive memory requirements. The proposed method integrates retrieved neighbors with the input context using a directed heterogeneous graph and employs GNNs to aggregate information, thereby improving predictive performance.\",\n    \"Direct Inspiration\": {\n        \"b23\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.8,\n        \"b58\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.7,\n        \"b37\": 0.7,\n        \"b12\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of traditional memorization-based language models, which struggle with hard examples and require significant memory. It proposes a novel language modeling framework called GNN-LM, which leverages Graph Neural Networks (GNNs) to reference similar contexts from the training corpus. This approach is inspired by the idea that referencing is easier than memorization, akin to an open-book exam. The method constructs a directed heterogeneous graph to integrate retrieved neighboring contexts and employs a self-attention augmented GNN. The proposed method significantly improves language model performance on three benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b58\": 0.8,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in language modeling, specifically the limitations of memorization-based models. It introduces a new language modeling scheme, GNN-LM, which integrates Graph Neural Networks (GNNs) with traditional language models to reference similar contexts from the training set for better prediction. The proposed method aims to alleviate issues related to hard examples and memory intensity by leveraging an open-book examination strategy.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b58\": 0.6,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b49\": 0.5,\n    \"b12\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of traditional language models that rely on memorization by proposing a novel open-book examination strategy using a graph neural network (GNN) augmented language model (GNN-LM). This model references similar contexts from the training corpus to improve prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b38\": 0.6,\n    \"b5\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```"], "60583b509e795e4ac8d11a69": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of fully exploiting the multimodality side information of items in recommendation systems. It proposes a novel pre-training framework, namely Pre-trained Multimodal Graph Transformer (PMGT), which leverages an item multimodal graph to integrate item relationships and multimodal information. The key contributions include the decomposition of the learning objective into graph structure reconstruction and masked node feature reconstruction, and the development of an algorithm named Mini-batch Contextual Neighbors Sampling (MCNSampling) for scalable training.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b13\": 1.0,\n    \"b14\": 1.0,\n    \"b21\": 1.0,\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b17\": 0.75,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b10\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of fully exploiting the multimodality side information of items in recommendation systems. The proposed novel approach, Pre-trained Multimodal Graph Transformer (PMGT), uses an unsupervised pre-training framework on an item multimodal graph, which integrates various types of side information and item relationships. Key innovations include the Mini-batch Contextual Neighbors Sampling algorithm, attention mechanisms for aggregating multimodal information, and a diversity-promoting Transformer framework.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1,\n        \"b13\": 1,\n        \"b14\": 1,\n        \"b21\": 1,\n        \"b38\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.8,\n        \"b17\": 0.7,\n        \"b34\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.5,\n        \"b2\": 0.5,\n        \"b7\": 0.5,\n        \"b25\": 0.5,\n        \"b32\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of fully exploiting multimodality side information (text, images, videos) of items to improve recommendation accuracy. The proposed solution, PMGT (Pre-trained Multimodal Graph Transformer), uses an unsupervised pre-training framework on an item multimodal graph to learn item relationships and multimodality side information. Key components include the MCNSampling algorithm, attention mechanisms, and a diversity-promoting Transformer framework. The effectiveness is demonstrated through experiments on real datasets, showing superior performance over existing methods in item recommendation and CTR prediction.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b13\": 1,\n    \"b14\": 1,\n    \"b21\": 1,\n    \"b38\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b17\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b7\": 0.7,\n    \"b25\": 0.7,\n    \"b32\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents a novel pre-training framework called Pre-trained Multimodal Graph Transformer (PMGT) to exploit items' multimodality information through unsupervised learning. The main challenges addressed include the full exploitation of multimodality side information for item representations learning and scalable training on large-scale graph data. The proposed algorithm leverages deep learning, specifically graph neural networks (GNNs), to pre-train on an item multimodal graph, capturing both item relationships and their multimodality side information.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.95,\n    \"b13\": 0.90,\n    \"b14\": 0.90,\n    \"b21\": 0.85,\n    \"b38\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.70,\n    \"b19\": 0.70,\n    \"b20\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.60,\n    \"b17\": 0.60,\n    \"b34\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of fully exploiting multimodality side information of items in recommender systems. It introduces a novel pre-training framework called Pre-trained Multimodal Graph Transformer (PMGT) that leverages unsupervised learning to enhance item representations. Inspired by advancements in unsupervised pre-training strategies in NLP and graph data, the study proposes using a unified item multimodal graph and pre-training a graph neural network (GNN) to capture both item relationships and multimodality side information. Notable contributions include the decomposition of the learning objective into graph structure reconstruction and masked node feature reconstruction, and the development of a Mini-batch Contextual Neighbors Sampling (MCNSampling) algorithm for scalable training.\",\n  \"Direct Inspiration\": [\"b6\", \"b13\", \"b14\", \"b21\", \"b38\"],\n  \"Indirect Inspiration\": [\"b28\", \"b3\", \"b10\", \"b19\", \"b20\"],\n  \"Other Inspiration\": [\"b8\", \"b26\", \"b27\", \"b34\", \"b37\"]\n}\n```"], "613192755244ab9dcb9dfc54": ["```json\n{\n  \"Summary\": \"The paper introduces CodeT5, an encoder-decoder model designed for programming language (PL) and natural language (NL) tasks. The primary challenges addressed include the limitations of existing models (encoder-only and decoder-only) for both understanding and generation tasks, and the lack of integration of rich structural information in the code. The novel methods proposed include identifier-aware pre-training objectives and leveraging NL-PL pairs for better alignment. CodeT5 employs a denoising Seq2Seq objective and introduces identifier tagging and masked identifier prediction tasks to improve code comprehension.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b22\": 0.75,\n    \"b13\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b20\": 0.55,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents CodeT5, a pre-trained encoder-decoder model for programming language (PL) tasks, built on the T5 architecture. It aims to address challenges in code understanding and generation by incorporating identifier-aware objectives and leveraging NL-PL dual learning for better cross-modal alignment. Key contributions include a novel identifier-aware pre-training objective and extensive experiments demonstrating state-of-the-art performance on various code-related tasks.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b9\": 0.95,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b22\": 0.75,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.65,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses limitations in existing pre-trained models for programming languages (PL) which often neglect the structural information in code and are suboptimal for both understanding and generation tasks. The proposed model, CodeT5, leverages the T5 architecture and introduces novel identifier-aware objectives to better capture code semantics and improve cross-modal NL-PL alignment.\",\n    \"Direct Inspiration\": {\n        \"b23\": 1,\n        \"b9\": 0.9,\n        \"b11\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b10\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b22\": 0.65,\n        \"b13\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces CodeT5, a pre-trained encoder-decoder model for programming language tasks. It addresses the limitations of existing models by integrating token type information and leveraging developer-assigned identifiers to improve code comprehension. CodeT5 extends the T5 architecture with novel identifier-aware pre-training objectives and bimodal dual learning to enhance NL-PL alignment.\",\n  \"Direct Inspiration\": [\"b23\", \"b13\", \"b9\"],\n  \"Indirect Inspiration\": [\"b11\", \"b20\", \"b22\"],\n  \"Other Inspiration\": [\"b5\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the limitations of existing pre-trained models for programming language tasks, such as relying on either encoder-only or decoder-only models and ignoring the rich structural information in code. The paper proposes CodeT5, an encoder-decoder model that integrates token type information and developer-assigned identifiers to enhance code comprehension and generation.\",\n  \"Direct Inspiration\": [\"b9\", \"b23\"],\n  \"Indirect Inspiration\": [\"b11\", \"b13\"],\n  \"Other Inspiration\": [\"b10\"]\n}\n```"], "61ca80355244ab9dcba69477": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to measure and analyze counterfactual memorization in neural language models. The proposed algorithm involves training multiple models on subsets of training data and comparing the performance on examples that are either included or excluded from these subsets. The novel method aims to automatically handle issues like near-duplicate examples without heuristics, contrasting with traditional generation-time memorization techniques.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b8\": 0.7,\n    \"b25\": 0.8,\n    \"b1\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b5\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.5,\n    \"b34\": 0.5,\n    \"b26\": 0.4,\n    \"b12\": 0.4,\n    \"b33\": 0.5,\n    \"b35\": 0.4,\n    \"b16\": 0.4,\n    \"b2\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the issue of memorization in neural language models, specifically the distinction between counterfactual memorization and generation-time memorization. The paper proposes an algorithm to measure counterfactual memorization by comparing the performance of models trained with and without a specific example. Various experiments and analyses are conducted to elucidate the characteristics of memorization across different datasets and domains.\",\n    \"Direct Inspiration\": {\n        \"b19\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b6\": 0.9,\n        \"b30\": 0.7,\n        \"b31\": 0.7,\n        \"b34\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b8\": 0.6,\n        \"b25\": 0.6,\n        \"b1\": 0.6,\n        \"b35\": 0.5,\n        \"b26\": 0.5,\n        \"b12\": 0.5,\n        \"b33\": 0.5,\n        \"b16\": 0.5,\n        \"b2\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of counterfactual memorization in neural language models, proposing an empirical method to measure memorization by training multiple models on different subsets of data. The core contribution is the differentiation between counterfactual and generation-time memorization, with the former focusing on the impact of rare instances in training data.\",\n    \"Direct Inspiration\": [\"b19\"],\n    \"Indirect Inspiration\": [\"b6\", \"b16\", \"b26\", \"b33\"],\n    \"Other Inspiration\": [\"b3\", \"b25\", \"b12\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of counterfactual memorization in neural language models, proposing an algorithm to measure how the inclusion or exclusion of specific training examples affects model performance. This approach contrasts with generation-time memorization techniques and aims to identify rare information that is memorized. Experiments are conducted using datasets like RealNews, C4, and Wiki-40B:en.\",\n    \"Direct Inspiration\": {\n        \"b19\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b6\": 0.8,\n        \"b30\": 0.7,\n        \"b31\": 0.6,\n        \"b34\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5,\n        \"b33\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge discussed in the paper is the issue of memorization in neural language models, particularly distinguishing between generation-time memorization and counterfactual memorization. The paper proposes a method to quantify how the presence or absence of an example in the training set affects model performance by comparing models trained with and without the example. This method allows the identification of memorized content without relying on heuristics to handle near-duplicate examples.\",\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [\"b3\", \"b8\", \"b1\", \"b25\", \"b19\", \"b6\"],\n  \"Other Inspiration\": [\"b5\", \"b30\", \"b31\", \"b34\", \"b16\", \"b26\", \"b33\", \"b35\"]\n}\n```"], "60cd651e91e011329faa2244": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively augmenting graph data for graph neural networks (GNNs), specifically for semi-supervised node classification. The proposed method involves creating new graph topologies and node attributes, using a combination of these as inputs for GCN models, and integrating the node embeddings with an attentional mechanism and disparity constraint to improve classification performance.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b19\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b5\": 0.6,\n    \"b16\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of effectively augmenting graph data for the improvement of Graph Neural Networks (GNNs), particularly for semi-supervised node classification. The authors propose a novel graph data augmentation strategy, an attentional integrating model, and a disparity constraint based on the Hilbert-Schmidt independence criterion to capture diverse and informative features from augmented graph data.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b5\": 0.8,\n    \"b15\": 0.7,\n    \"b19\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively augmenting graph data for Graph Neural Networks (GNNs), specifically for semi-supervised node classification. It proposes a novel data augmentation strategy to create new graph topologies and node attributes, which are then integrated via an attentional mechanism. The method aims to improve the generalizability and performance of GNNs by capturing diverse information from augmented graph features.\",\n  \"Direct Inspiration\": [\"b9\", \"b2\"],\n  \"Indirect Inspiration\": [\"b5\", \"b20\", \"b15\", \"b19\", \"b25\"],\n  \"Other Inspiration\": [\"b16\", \"b30\", \"b4\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of augmenting graph data for Graph Convolutional Networks (GCNs) in semi-supervised node classification. It proposes a strategy to create new pairs of graph topologies and node attributes, integrates them using an attention mechanism, and applies a Hilbert-Schmidt independence criterion-based disparity constraint to capture diverse information.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b20\": 0.7,\n    \"b15\": 0.6,\n    \"b19\": 0.6,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively augmenting graph data for Graph Neural Networks (GNNs), particularly Graph Convolutional Networks (GCNs), to improve their performance on semi-supervised node classification tasks. The main contributions include proposing a graph data augmentation strategy, an attentional integration model, and a disparity constraint based on the Hilbert-Schmidt independence criterion to enhance the diversity of information captured from various augmented graph features.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b2\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.5,\n    \"b19\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```"], "5ff72c08d4150a363c54da3d": ["```json\n{\n  \"Summary\": \"This paper addresses the challenge of developing an efficient and effective graph kernel for structured data classification. Inspired by the combination of R\u00e9nyi entropy and the probability of steady state random walks, the authors propose a novel deep R\u00e9nyi entropy graph kernel. The key contributions include deriving the Shannon entropy of a graph via generalized entropy, deducing the second-order R\u00e9nyi entropy, and demonstrating superior performance on benchmark datasets.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b27\": 0.65,\n    \"b30\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b26\": 0.45,\n    \"b29\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the difficulty in constructing efficient and accurate graph kernels for structured data classification. The paper introduces a novel graph kernel based on deep R\u00e9nyi entropy, which leverages the properties of generalized entropy and steady state random walks on graphs to improve graph classification performance.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b21\": 0.85,\n        \"b22\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.75,\n        \"b26\": 0.7,\n        \"b29\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.65,\n        \"b6\": 0.6,\n        \"b27\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the challenge of developing efficient graph kernel functions for classifying structured data by utilizing R\u00e9nyi entropy to structure a deep representation of graphs. The novel approach combines R\u00e9nyi entropy and the probability of steady state random walks on graphs to generate a unique graph kernel. Key inspirations include prior work on entropy in pattern recognition and the application of entropy in graph-based classifications.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b30\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.75,\n    \"b20\": 0.80,\n    \"b21\": 0.85,\n    \"b22\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.70,\n    \"b27\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an efficient and effective graph kernel for structured data classification. It introduces a novel deep R\u00e9nyi entropy graph kernel (SREGK) by combining R\u00e9nyi entropy with the probability of steady-state random walks on graphs. The primary contributions include deriving Shannon entropy from generalized entropy, providing properties of second-order R\u00e9nyi entropy, and demonstrating the superiority of SREGK over other state-of-the-art graph classification algorithms on benchmark datasets.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b21\": 0.85,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.7,\n    \"b25\": 0.75,\n    \"b29\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b15\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in developing efficient and accurate graph kernels for structured data classification. It introduces a novel graph kernel based on deep R\u00e9nyi entropy and the probability of steady state random walks on graphs. The main contributions include deriving Shannon entropy and second-order R\u00e9nyi entropy from generalized entropy, and demonstrating the superiority of the proposed method using benchmark datasets.\",\n  \"Direct Inspiration\": [\"b20\", \"b21\", \"b22\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b2\", \"b30\", \"b26\"]\n}\n```"], "60d1538591e011c16f0cb454": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of understanding and interpreting the nature of adversarial examples in deep neural networks (DNNs). It proposes a new conceptual framework based on the idea that natural images lie on a low-dimensional manifold within the high-dimensional input space, and adversarial examples arise due to the mismatch between the dimensionality of the image manifold and the input space.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b10\": 0.8,\n        \"b11\": 0.8,\n        \"b18\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.7,\n        \"b14\": 0.6,\n        \"b20\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.75,\n        \"b13\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is understanding the phenomenon of adversarial examples in deep neural networks (DNNs). The authors propose a new conceptual framework that explains adversarial examples by considering the low-dimensional manifold of natural images and the high-dimensional decision boundaries of DNNs. The paper introduces the idea that adversarial examples arise due to the mismatch between the low-dimensional image manifold and the high-dimensional input space, and it experimentally validates this through various synthetic and natural image experiments.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b1\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b2\": 0.6,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the fragility of deep neural networks (DNNs) to adversarial perturbations and proposes a new conceptual framework (Dimpled Manifold Model) to understand adversarial examples. The framework suggests that adversarial examples arise due to the misalignment between the low-dimensional manifold of natural images and the high-dimensional input space of DNNs. The paper provides experimental evidence supporting this model and demonstrates that adversarial perturbations are predominantly off-manifold.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b1\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b14\": 0.8,\n    \"b13\": 0.8,\n    \"b2\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b18\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the perplexing phenomenon of adversarial examples in deep neural networks (DNNs) and proposes a new conceptual framework to understand them better. The primary challenges include the fragility of DNNs to small perturbations, the bizarre properties of adversarial examples, and the sensitivity of DNNs compared to human visual systems. The paper introduces a new mental model using the concept of low-dimensional image manifolds and explores the implications of this model through experiments.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b13\": 0.9,\n    \"b14\": 0.95,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b7\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.75,\n    \"b18\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the perplexing nature of adversarial examples in deep neural networks (DNNs). It proposes a new conceptual framework to understand adversarial examples by considering the low-dimensional manifold of natural images and the mismatched high-dimensional input space. The authors argue that adversarial examples arise due to the DNNs utilizing the perpendicular subspace to the image manifold, which leads to the unexpected and bizarre properties of adversarial examples.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b14\": 0.85,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b20\": 0.7,\n    \"b18\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```"], "600803da91e011d056eee7f3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating edge features into graph neural networks (GNNs), which are traditionally focused on node features. The proposed solution, edge-featured graph attention networks (EGATs), enhances the attention mechanism to include edge information, updates node and edge features symmetrically, and uses a multi-scale merge strategy to gather features from different iterations. The motivation comes from the limitations of existing GNNs in handling edge features effectively.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b9\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating edge features into graph attention networks (GATs) for node classification tasks. The proposed model, edge-featured graph attention networks (EGATs), extends GATs by incorporating edge features and designing a new attention mechanism that updates node and edge features simultaneously.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b9\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b2\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating edge features in graph neural networks (GNNs) for node classification tasks. Traditional GNNs primarily focus on node features, ignoring the potential contributions of edge features. The proposed model, Edge-featured Graph Attention Networks (EGATs), extends Graph Attention Networks (GATs) by integrating edge features into the attention mechanism, allowing for both node and edge feature updates. This model is designed to handle graphs with different preferences for node and edge features and aims to improve classification accuracy in real-world applications.\",\n  \"Direct Inspiration\": [\"b20\", \"b5\", \"b17\"],\n  \"Indirect Inspiration\": [\"b9\", \"b23\"],\n  \"Other Inspiration\": [\"b2\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing graph neural networks (GNNs) by introducing edge-featured graph attention networks (EGATs) that effectively incorporate edge features into the models. The primary challenges include the lack of edge feature integration in current GNNs and the need for a model that can handle both node and edge features simultaneously. The proposed EGATs enhance the original attention mechanism and introduce a multi-scale merge strategy to improve classification accuracy in graphs with varying preferences for node and edge features.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b1\": 0.4,\n    \"b21\": 0.4,\n    \"b12\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating edge features into graph neural networks (GNNs) for node classification tasks, proposing edge-featured graph attention networks (EGATs) as an extension of graph attention networks (GATs). This model enhances the attention mechanism to include edge features, redesigns traditional attention models to accept both node and edge features, and introduces a multi-scale merge strategy to gather features from different iterations.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b17\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b10\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```"], "608bdcae91e0112fc4e65b6b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving self-supervised learning (SSL) for Vision Transformers (ViT) compared to convolutional networks (convnets). It proposes a novel method called DINO, which leverages self-distillation without labels, momentum encoder, and multi-crop training to enhance ViT features for tasks like semantic segmentation and classification.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b27\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b17\": 0.9,\n    \"b32\": 0.7,\n    \"b52\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b55\": 0.6,\n    \"b61\": 0.6,\n    \"b65\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the adaptation of self-supervised learning (SSL) to Vision Transformer (ViT) architectures, identifying unique properties such as explicit scene layout information and superior performance in k-NN classification. The proposed method, DINO, leverages momentum encoder, multi-crop training, and smaller patches to enhance ViT feature quality. It interprets SSL as a form of knowledge distillation without labels, achieving state-of-the-art results on ImageNet.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b30\": 1.0,\n    \"b9\": 0.9,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b66\": 0.6,\n    \"b65\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b52\": 0.5,\n    \"b57\": 0.4,\n    \"b31\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper investigates whether self-supervised learning enhances Vision Transformers (ViT) compared to convolutional networks (convnets). It introduces a self-supervised method named DINO, which leverages self-distillation without labels. The study emphasizes the importance of momentum encoder, multi-crop training, and small patches in improving ViT's performance for tasks like semantic segmentation and k-NN classification.\",\n  \"Direct Inspiration\": [\"b16\", \"b27\", \"b30\", \"b32\"],\n  \"Indirect Inspiration\": [\"b9\", \"b14\", \"b17\", \"b65\"],\n  \"Other Inspiration\": [\"b52\", \"b66\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include understanding the unique properties of self-supervised Vision Transformer (ViT) features compared to supervised ViTs and convolutional networks. The proposed algorithm, DINO, utilizes a method of self-distillation with no labels to achieve improved performance in image classification and semantic segmentation tasks. Key components include the use of a momentum encoder, multi-crop training, and smaller patches to enhance feature quality.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b27\": 0.85,\n    \"b30\": 0.9,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.75,\n    \"b52\": 0.75,\n    \"b17\": 0.8,\n    \"b65\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b14\": 0.7,\n    \"b57\": 0.65,\n    \"b66\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges of the paper are to determine if self-supervised learning provides new properties to Vision Transformers (ViT) compared to convolutional networks (convnets), and whether these properties lead to better performance in tasks like semantic segmentation and k-NN classification. The paper introduces DINO, a self-supervised method that combines momentum encoder and multi-crop training to achieve these objectives.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b27\": 0.9,\n    \"b30\": 0.9,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b52\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b55\": 0.5,\n    \"b65\": 0.5\n  }\n}\n```"], "604216c391e0115d09aff27a": ["```json\n{\n  \"Summary\": \"The paper addresses the primary challenges of effectively utilizing structure-based and feature-based prior knowledge in semi-supervised learning on graph-structured data. It proposes a knowledge distillation framework that extracts knowledge from an arbitrary pretrained Graph Neural Network (GNN) (teacher model) and injects it into a student model. The student model is designed as a combination of parameterized label propagation and a feature-based 2-layer MLP, which preserves structure-based and feature-based prior knowledge, respectively. The framework aims to achieve improved classification accuracy and better interpretability.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b13\": 0.9,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b28\": 0.6,\n    \"b31\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.4,\n    \"b11\": 0.3,\n    \"b3\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of traditional GNN models in fully utilizing prior knowledge such as structure-based and feature-based priors. It proposes a knowledge distillation framework that transfers knowledge from a pretrained GNN (teacher model) to a student model designed with two simple prediction mechanisms: parameterized label propagation and feature-based 2-layer MLP. This approach aims to improve interpretability and classification accuracy.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b10\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.9,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of semi-supervised learning on graph-structured data, specifically the underutilization of prior knowledge and the complexity of prediction mechanisms in GNNs. It proposes a knowledge distillation framework to transfer knowledge from a pretrained GNN (teacher model) to a student model designed with two simple prediction mechanisms: label propagation and feature transformation.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b10\": 1,\n    \"b13\": 1,\n    \"b29\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b28\": 0.8,\n    \"b20\": 0.8,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b3\": 0.6,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of semi-supervised learning on graph-structured data by proposing a knowledge distillation framework. The framework combines parameterized label propagation and feature-based MLP to enhance prediction accuracy and interpretability by leveraging both structure-based and feature-based prior knowledge from pretrained GNN models.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b10\": 0.9,\n    \"b13\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.8,\n    \"b29\": 0.75,\n    \"b31\": 0.7,\n    \"b6\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b3\": 0.55,\n    \"b23\": 0.5,\n    \"b18\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in semi-supervised learning on graph-structured data, focusing on the limitations of traditional Graph Neural Networks (GNNs) in fully utilizing prior knowledge like homophily and feature-based priors. The authors propose a knowledge distillation framework where a student model combines parameterized label propagation and a feature-based 2-layer MLP to enhance interpretability and performance. The student model benefits from both GNN and prior knowledge, leading to improved classification accuracy.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.95,\n    \"b10\": 0.9,\n    \"b13\": 0.85,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b29\": 0.75,\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b11\": 0.7,\n    \"b23\": 0.7\n  }\n}\n```"], "5ff6870ad4150a363cc412b0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality, audio-corresponding talking face videos by proposing a novel audio-to-video-to-words framework called AVWnet. This framework leverages a multi-level attentive generation network and a reverse lip-reading network to refine and align the generated video with the input audio.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b25\": 0.7,\n    \"b42\": 0.7,\n    \"b26\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b27\": 0.5,\n    \"b41\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality, audio-corresponding talking face videos by proposing a novel coarse-to-fine learning module and an end-to-end neural architecture built upon a temporal-dependent GAN framework. The primary issues tackled include the difficulty of learning shared representations between audio and image modalities, and optimizing lip semantic alignment to achieve photo-realistic visual content.\",\n  \"Direct Inspiration\": [\"b3\", \"b25\"],\n  \"Indirect Inspiration\": [\"b4\", \"b26\", \"b42\"],\n  \"Other Inspiration\": [\"b11\", \"b27\", \"b41\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Generating high-quality, audio-corresponding talking face videos that maintain semantic alignment and photo-realistic visual content.\",\n    \"Inspirations\": \"Coarse-to-fine learning module, end-to-end neural architecture, temporal-dependent GAN framework.\"\n  },\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b4\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.7,\n    \"b27\": 0.7,\n    \"b42\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating high-quality talking face videos from audio input, focusing on achieving photo-realistic visual content and optimizing lip semantic alignment. The proposed model, AVWnet, employs a coarse-to-fine learning module, temporal-dependent GAN framework, multi-level attention mechanism, and a lip-reading network to generate fine-grained talking face videos.\",\n    \"Direct Inspiration\": {\n        \"inspired by\": [\"b3\"],\n        \"motivated by\": [\"b3\"]\n    },\n    \"Indirect Inspiration\": {\n        \"following\": [\"b3\", \"b4\", \"b25\"],\n        \"we use\": [\"b3\", \"b4\", \"b25\"]\n    },\n    \"Other Inspiration\": {\n        \"the pioneering work\": [\"b3\", \"b4\", \"b25\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality, audio-corresponding talking face videos. It proposes a coarse-to-fine learning module and an end-to-end neural architecture built upon a temporal-dependent GAN framework. Key contributions include a multi-level attentive generation network, a semantic video reinterpretation module, and multi-purpose discriminators for adversarial learning.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.95,\n    \"b4\": 0.85,\n    \"b25\": 0.9,\n    \"b42\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.7,\n    \"b41\": 0.65,\n    \"b27\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6\n  }\n}\n```"], "60d55eec91e01153881e85da": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the complexity and necessity of negative samples in self-supervised learning (SSL) models for graph neural networks (GNNs). The proposed solution, CCA-SSG, introduces a non-contrastive and non-discriminative objective inspired by Canonical Correlation Analysis (CCA) that maximizes the correlation between two augmented views of the same input while decorrelating different feature dimensions of a single view's representation. This approach simplifies the model architecture and improves efficiency without relying on negative samples or other complex components.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b13\": 0.8,\n    \"b1\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b47\": 0.7,\n    \"b14\": 0.7,\n    \"b56\": 0.6,\n    \"b57\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b5\": 0.5,\n    \"b38\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a novel non-contrastive and non-discriminative self-supervised learning approach for graphs based on Canonical Correlation Analysis (CCA). This method aims to address the complexity and inefficiency of existing contrastive learning methods by eliminating the need for negative samples and additional components such as mutual information estimators and asymmetric architectures. The proposed method, CCA-SSG, focuses on maximizing the correlation between two augmented views of the same input and decorrelating different feature dimensions to prevent dimensional collapse. It shows competitive performance across various benchmarks and theoretically aligns with the Information Bottleneck Principle.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b9\": 1,\n    \"b10\": 1,\n    \"b13\": 1,\n    \"b1\": 1,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.8,\n    \"b43\": 0.8,\n    \"b36\": 0.8,\n    \"b52\": 0.8,\n    \"b8\": 0.7,\n    \"b44\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.6,\n    \"b5\": 0.5,\n    \"b11\": 0.5,\n    \"b47\": 0.5,\n    \"b14\": 0.5,\n    \"b56\": 0.5,\n    \"b57\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces Canonical Correlation Analysis inspired Self-Supervised Learning on Graphs (CCA-SSG), a method aiming to learn node representations without relying on complex designs like negative samples, mutual information estimators, or asymmetric architectures. The primary challenges addressed include avoiding the use of negative samples and simplifying the model architecture while maintaining superior performance. The algorithm is inspired by the principles of Canonical Correlation Analysis (CCA) and Information Bottleneck Principle.\",\n    \"Direct Inspiration\": [\"b17\", \"b9\", \"b10\", \"b13\", \"b1\", \"b3\"],\n    \"Indirect Inspiration\": [\"b11\", \"b5\", \"b38\", \"b45\", \"b4\", \"b47\", \"b14\", \"b56\", \"b57\"],\n    \"Other Inspiration\": [\"b42\", \"b43\", \"b36\", \"b52\", \"b8\", \"b44\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of self-supervised learning (SSL) on graphs by proposing a novel, simple, and efficient approach called Canonical Correlation Analysis inspired Self-Supervised Learning on Graphs (CCA-SSG). The primary challenges include the reliance on complex designs, architectures, and negative samples in existing contrastive learning methods. The proposed method introduces a non-contrastive and non-discriminative feature-level objective inspired by Canonical Correlation Analysis (CCA) to maximize the correlation between two augmented views of the same input while decorrelating different feature dimensions. This approach aims to discard augmentation-variant information and preserve augmentation-invariant information, preventing dimensional collapse and effectively leveraging the Information Bottleneck Principle under SSL settings.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b13\": 0.9,\n    \"b1\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b5\": 0.7,\n    \"b38\": 0.7,\n    \"b47\": 0.6,\n    \"b14\": 0.6,\n    \"b56\": 0.6,\n    \"b57\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b52\": 0.5,\n    \"b8\": 0.5,\n    \"b44\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in self-supervised learning (SSL) on graphs, particularly those associated with contrastive methods requiring complex architectures and negative samples. It proposes a novel, simple, and effective approach named Canonical Correlation Analysis inspired Self-Supervised Learning on Graphs (CCA-SSG). The approach is inspired by Canonical Correlation Analysis (CCA) and aims to maximize the correlation between augmented views of the same input while decorrelating different dimensions of a single view's representation.\",\n  \"Direct Inspiration\": [\"b17\", \"b9\", \"b10\", \"b13\", \"b3\"],\n  \"Indirect Inspiration\": [\"b42\", \"b43\", \"b36\", \"b8\", \"b44\"],\n  \"Other Inspiration\": [\"b6\", \"b45\", \"b4\", \"b39\", \"b15\", \"b11\", \"b5\", \"b47\", \"b14\", \"b32\", \"b56\", \"b57\", \"b38\"]\n}\n```"], "60bdde338585e32c38af50fc": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of generalizing Graph Neural Networks (GNNs) from small to large graphs, particularly focusing on the difficulty arising from differences in local graph structures between training and test sets. The authors introduce the concept of 'd-patterns' to capture these local structures and propose methods to improve size generalization using self-supervised learning (SSL) and semi-supervised learning (SSL) approaches.\",\n    \"Direct Inspiration\": {\n        \"Weisfeiler & Lehman, 1968\": [\"b32\"],\n        \"Xu et al., 2018\": [\"b32\"]\n    },\n    \"Indirect Inspiration\": {},\n    \"Other Inspiration\": {\n        \"Preferential attachment (PA) model\": [\"b1\"],\n        \"Graph neural networks\": [\"b4\", \"b24\", \"b45\", \"b15\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generalizing Graph Neural Networks (GNNs) from small to large graphs, particularly when the local structures of graphs depend on their size. The authors propose the concept of d-patterns to capture local structures and show that discrepancies in d-pattern distributions between training and test sets can lead to poor generalization. They introduce a self-supervised learning task to improve generalization and demonstrate its effectiveness empirically.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.7,\n    \"b6\": 0.7,\n    \"b17\": 0.6,\n    \"b27\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b24\": 0.5,\n    \"b45\": 0.5,\n    \"b15\": 0.5,\n    \"b40\": 0.5,\n    \"b23\": 0.5,\n    \"b22\": 0.5,\n    \"b11\": 0.5,\n    \"b29\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of size generalization in Graph Neural Networks (GNNs), focusing on cases where local structures in graphs depend on graph size. The authors introduce the concept of d-patterns to capture local structures and propose theoretical and empirical methods to improve size generalization, including a novel self-supervised learning task.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b7\": 0.8,\n    \"b17\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b24\": 0.7,\n    \"b45\": 0.7,\n    \"b15\": 0.7,\n    \"b27\": 0.75,\n    \"b30\": 0.75,\n    \"b40\": 0.75,\n    \"b23\": 0.75,\n    \"b22\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of size generalization in graph neural networks (GNNs), focusing on how local graph structures (d-patterns) affect the ability of GNNs to generalize from small to large graphs. It proposes methods to improve size generalization through self-supervised learning and domain adaptation techniques.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.7,\n    \"b11\": 0.7,\n    \"b29\": 0.6,\n    \"b6\": 0.6,\n    \"b17\": 0.6,\n    \"b27\": 0.5,\n    \"b30\": 0.5,\n    \"b40\": 0.5,\n    \"b23\": 0.5,\n    \"b22\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.4,\n    \"b24\": 0.4,\n    \"b45\": 0.4,\n    \"b15\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generalizing Graph Neural Networks (GNNs) from small to large graphs, particularly focusing on the discrepancy in local structures between different graph sizes. The authors propose the notion of d-patterns to study the expressivity of GNNs and suggest methods to improve size generalization through self-supervised learning and domain adaptation techniques.\",\n  \"Direct Inspiration\": {\n    \"b32\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b17\": 0.75,\n    \"b27\": 0.7,\n    \"b30\": 0.7,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b24\": 0.6,\n    \"b45\": 0.6,\n    \"b15\": 0.6,\n    \"b2\": 0.5,\n    \"b11\": 0.5,\n    \"b29\": 0.5,\n    \"b6\": 0.5,\n    \"b7\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```"], "6020dfe19e795e62379b0ded": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of session-based recommendation systems, which must generate accurate recommendations from short user sessions without historical data. The proposed solution is a dual sparse attention network (DSAN) that employs both self-attention and vanilla attention mechanisms to capture user preferences more effectively. Key contributions include the development of a target embedding to represent user preferences and an adaptively sparse attention mechanism to filter out unrelated items.\",\n    \"Direct Inspiration\": {\n        \"b9\": 0.9,\n        \"b22\": 0.9,\n        \"b11\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.8,\n        \"b8\": 0.8,\n        \"b21\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b17\": 0.6,\n        \"b12\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of making accurate session-based recommendations with limited user information. It proposes a novel dual sparse attention network (DSAN) to improve the accuracy and interpretability of recommendations by learning a target embedding and applying an adaptively sparse attention mechanism.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b22\": 0.9,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b21\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b23\": 0.6,\n    \"b17\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of making accurate session-based recommendations with limited user information. It highlights the limitations of traditional attention mechanisms and proposes a Dual Sparse Attention Network (DSAN), which incorporates self-attention and vanilla attention layers with an adaptively sparse transformation function to improve recommendation accuracy.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b9\": 0.9,\n    \"b11\": 0.8,\n    \"b22\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.6,\n    \"b21\": 0.6,\n    \"b12\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.4,\n    \"b19\": 0.4,\n    \"b17\": 0.3,\n    \"b1\": 0.3,\n    \"b0\": 0.2,\n    \"b6\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately predicting the next click in session-based recommendation systems, especially when historical user data is limited. It proposes a dual sparse attention network (DSAN) to overcome limitations of existing attention-based methods. The DSAN model uses a self-attention network to learn a target embedding and a vanilla attention network to distinguish the importance of different items within a session. An adaptively sparse transformation function is introduced to pick out useful items and assign zero weight to unrelated items, enhancing the model's interpretability and performance.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b22\": 1,\n    \"b11\": 1,\n    \"b14\": 0.9,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b17\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b19\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of accurately predicting user preferences in session-based recommendation systems with limited historical data. It proposes a Dual Sparse Attention Network (DSAN) that utilizes self-attention and vanilla attention mechanisms to learn a target embedding and improve recommendation accuracy. The model introduces an adaptively sparse attention mechanism to filter out unrelated items in the session, enhancing the interpretability and performance of the recommendation system.\",\n    \"Direct Inspiration\": {\n        \"b9\": 0.9,\n        \"b22\": 0.85,\n        \"b11\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.75,\n        \"b21\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b14\": 0.65,\n        \"b17\": 0.6,\n        \"b12\": 0.55\n    }\n}\n```"], "600fe7c3d4150a363c22cdd5": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges are to efficiently and accurately provide session-based social recommendations (SSR) by leveraging social network information while maintaining the efficiency of session-based recommendation (SR).\",\n    \"inspirations\": \"The paper is inspired by the need to incorporate social network influences into session-based recommendations to improve accuracy without sacrificing efficiency.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b12\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b7\", \"b19\", \"b23\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b17\", \"b18\", \"b2\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Session-based Recommendation (SR) and its variant, Session-based Social Recommendation (SSR), by proposing an efficient framework called SE-Frame. The framework leverages social network data to improve recommendation accuracy and efficiency by constructing a heterogeneous knowledge graph and using a heterogeneous graph neural network (HGNN). The framework is flexible, allowing integration with existing SR models, and aims to capture cross-session item transitions and social influences effectively.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b17\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b19\": 0.7,\n    \"b23\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the inefficiency of existing session-based social recommendation (SSR) methods, specifically DGRec, which uses a GNN to aggregate preferences from social influences. The proposed algorithm, SE-Frame, constructs a heterogeneous knowledge graph from social networks and user behaviors to create social-aware user and item representations, which are then used to efficiently generate recommendations during inference.\",\n    \"Direct Inspiration\": {\n        \"b12\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b17\": 0.8,\n        \"b18\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.5,\n        \"b19\": 0.5,\n        \"b23\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the efficiency and accuracy of session-based social recommendation (SSR) systems. It proposes a novel framework called Social-aware Efficient Framework (SE-Frame), which enhances existing session-based recommendation (SR) models by incorporating social network information. This is achieved by constructing a heterogeneous knowledge graph and using a graph neural network to learn social-aware user and item representations. SE-Frame can adapt any existing SR model for SSR, maintaining the efficiency of the original model while leveraging social network knowledge.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b2\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b19\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the efficiency challenges in Session-based Social Recommendation (SSR) by proposing a new framework called Social-aware Efficient Framework (SE-Frame). SE-Frame leverages a heterogeneous knowledge graph to integrate social networks, user-item interactions, and item transitions, which are then used to produce social-aware user and item representations. These representations allow existing Session-based Recommendation (SR) models to be adapted for SSR, improving both accuracy and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b19\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b18\": 0.7,\n    \"b2\": 0.6\n  }\n}\n```"], "61c2974b5244ab9dcbcf1ee1": ["```json\n{\n  \"Summary\": \"The paper tackles the challenge of emotion recognition in conversation (ERC) by addressing three main challenges: context dependency, speaker dependency, and distinguishing semantically similar emotions. The proposed algorithm employs a dialogue-level Transformer for modeling long-range context dependencies, supervised contrastive learning (SCL) to improve sentiment categorization, and an auxiliary response generation task to enhance context information capture. The model, CoG-BART, uses BART as the backbone and incorporates both contrastive and generative loss to improve performance on ERC tasks.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b13\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b12\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three main challenges in emotion recognition in conversation (ERC): context dependency, speaker dependency, and differentiating semantically similar emotions. The authors propose using a dialogue-level Transformer, supervised contrastive learning (SCL), and an auxiliary response generation task to tackle these challenges. They introduce CoG-BART, which enhances BART with contrastive and generative loss, achieving state-of-the-art results on four ERC datasets.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b13\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b19\": 0.7,\n    \"b23\": 0.7,\n    \"b3\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b11\": 0.5,\n    \"b0\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in emotion recognition in conversation (ERC) such as contextual dependency, speaker dependency, and the differentiation of semantically similar emotions. The proposed method, CoG-BART, enhances a BART-based model by introducing supervised contrastive learning (SCL) and an auxiliary response generation task to improve the model's performance in these areas.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b24\": 0.8,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b19\": 0.7,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three main challenges in emotion recognition in conversation (ERC): (1) context dependency, where emotions are influenced by the surrounding utterances; (2) speaker dependency, where a speaker's emotion can be influenced by another speaker's utterance; and (3) distinguishing between semantically similar emotions. The authors propose a new model, CoG-BART, which incorporates a dialogue-level Transformer to model context dependencies, supervised contrastive learning to distinguish between similar emotions, and an auxiliary response generation task to enhance context understanding.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.9,\n    \"b19\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.7,\n    \"b3\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the task of Emotion Recognition in Conversation (ERC), which involves assigning emotion labels to utterances in a dialogue based on context and speaker information. The primary challenges identified include the contextual dependency of emotions, the influence of speaker interactions, and the difficulty of distinguishing semantically similar emotions. The authors propose a model called CoG-BART, which utilizes a dialogue-level Transformer for context modeling, supervised contrastive learning to improve sentiment differentiation, and an auxiliary response generation task to enhance contextual understanding.\",\n    \"Direct Inspiration\": {\n        \"b24\": 1.0,\n        \"b13\": 1.0,\n        \"b8\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.8,\n        \"b7\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b12\": 0.6,\n        \"b23\": 0.6\n    }\n}\n```"], "605aa23ce4510cd7c86b1f51": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of segmenting remote sensing images with uncertain scales in high-resolution aerial imagery. The proposed solution is an Adaptive Feature Selection (AFS) module that employs attention mechanisms to learn weight contributions of each feature block at different scales. Key networks like PSPNet, DeepLabV3, and U-Net are used with the AFS module to validate its efficiency and generalization.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Difficulty in segmenting ground objects with uncertain scales in high-resolution aerial imagery using conventional models.\",\n    \"inspirations\": \"Adaptive feature selection to handle multiple scales using attention mechanisms.\"\n  },\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b6\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of segmenting ground objects with uncertain scales in high-resolution aerial imagery. It proposes an adaptive feature selection (AFS) module to enhance the performance of semantic segmentation networks by learning weight contributions of each feature block at different scales. The method is validated using U-Net, PSPNet, and DeepLabV3 on two benchmarks, showing significant improvements in segmentation accuracy.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b6\": 0.9,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of segmenting ground objects with uncertain scales in high-resolution aerial imagery. The authors propose an adaptive feature selection (AFS) module that uses an attention mechanism to learn weight contributions of feature blocks at different scales. The novel method is integrated with PSPNet, DeepLabV3, and U-Net, yielding significant improvements in the segmentation performance on the Vaihingen and WHU Building datasets.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.9,\n    \"b15\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b11\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of segmenting ground objects with uncertain scales in high-resolution aerial imagery. The authors propose an adaptive feature selection (AFS) module that learns weight contributions of each feature block at different scales using an attention mechanism. This module is integrated into existing networks like PSPNet, DeepLabV3, and U-Net to improve their performance on remote sensing image segmentation tasks.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b15\": 0.8,\n    \"b10\": 0.75,\n    \"b11\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```"], "60a7959191e011f90a51de12": ["```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are how to systematically combine the conversation and recommendation components for unified policy learning, and how to deal with sample efficiency issues in a large action space. The proposed solution, UNICORN, leverages a dynamic weighted graph to model changing interrelationships among users, items, and attributes during conversations, using a graph-based Markov Decision Process (MDP) environment. The paper also highlights the importance of integrating the recommendation and conversation components to improve policy learning for multi-round conversational recommendation (MCR).\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b13\": 1,\n    \"b25\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b40\": 0.8,\n    \"b49\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b26\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Combining conversation and recommendation components for unified policy learning.\",\n      \"Dealing with sample efficiency issues due to a large action space in UCRPL.\"\n    ],\n    \"inspirations\": [\n      \"Graph structure to integrate recommendation and conversation components.\",\n      \"Dynamic weighted graph for modeling changing interrelationships among users, items, and attributes.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b13\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b40\": 0.7,\n    \"b49\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in multi-round conversational recommendation (MCR) systems. It aims to formulate a unified policy learning problem that integrates three core decision-making processes: what to ask, which items to recommend, and when to ask or recommend. The proposed solution, UNICORN, employs a graph-based reinforcement learning framework to improve sample efficiency and policy learning by leveraging the dynamic interrelationships among users, items, and attributes.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b13\": 1,\n    \"b25\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b40\": 0.7,\n    \"b49\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b18\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in this paper are the integration of conversation and recommendation components in a Conversational Recommender System (CRS) and dealing with sample efficiency issues in a unified policy learning framework. The paper proposes a novel framework, UNICORN, which leverages a dynamic weighted graph to model interactions and employs a graph-based Markov Decision Process (MDP) environment.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b13\": 1.0,\n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b40\": 0.8,\n    \"b49\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.7,\n    \"b9\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in multi-round conversational recommendation (MCR), focusing on integrating conversation and recommendation components into a unified policy learning framework, named UNICORN. The key challenges include combining these components systematically and dealing with sample efficiency issues due to large action spaces. The proposed solution leverages a dynamic weighted graph and graph-based reinforcement learning to enhance decision-making and sample efficiency.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b13\": 1,\n    \"b25\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b40\": 0.7,\n    \"b49\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```"], "60641c289e795e72406b65b3": ["```json\n{\n  \"Summary\": \"The paper targets challenges in graph neural networks (GNNs), particularly in capturing complex structural information, managing high complexity in constructing receptive fields, and handling irregularities in adaptive receptive fields. The proposed STAR-GNN model integrates structural information into the construction of receptive fields using anonymous random walks (ARWs) and mutual information (MI), and introduces a novel aggregator for irregular receptive fields.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b19\": 1,\n    \"b21\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.8,\n    \"b18\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b22\": 0.6,\n    \"b28\": 0.6,\n    \"b10\": 0.5,\n    \"b14\": 0.5,\n    \"b12\": 0.5,\n    \"b20\": 0.5,\n    \"b24\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of modeling complex structural information, constructing adaptive receptive fields with high complexity, and handling irregularities in traditional aggregation approaches within Graph Neural Networks (GNNs). The authors propose the STAR-GNN model, which uses self-supervised structural modeling and mutual information for constructing receptive fields and a novel sub-graph aggregator for better representation learning from irregular receptive fields. The model is validated through experiments demonstrating its effectiveness compared to strong baselines.\",\n  \"Direct Inspiration\": [\"b2\", \"b19\", \"b21\"],\n  \"Indirect Inspiration\": [\"b18\", \"b30\", \"b34\"],\n  \"Other Inspiration\": [\"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of modeling complex structural information, constructing high-complexity receptive fields, and dealing with irregularities in adaptive receptive fields in Graph Neural Networks (GNNs). It proposes a novel GNN model, STAR-GNN, which integrates structural information into the construction process of receptive fields and achieves better aggregation of information. The key contributions include a self-supervised structural modeling method based on anonymous random walks (ARWs), an efficient greedy algorithm for constructing receptive fields using mutual information, and a novel aggregator for irregular receptive fields.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b19\": 0.9,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b30\": 0.7,\n    \"b34\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.4,\n    \"b22\": 0.4,\n    \"b28\": 0.4,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses key challenges in modeling complex structural information in graphs, constructing adaptive receptive fields, and handling the irregularities of these fields. The proposed STAR-GNN model integrates structural information using anonymous random walks and mutual information to construct optimal receptive fields and introduce a novel aggregator for irregular receptive fields.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b21\": 0.95,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.85,\n    \"b30\": 0.8,\n    \"b34\": 0.75,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in modeling complex structural information in graphs, constructing adaptive receptive fields with high complexity, and dealing with irregularities in these fields. To tackle these, it proposes a novel GNN model called STAR-GNN, which integrates structural information through anonymous random walks and mutual information to construct receptive fields and introduces a new aggregator for irregular fields.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b19\": 1.0,\n    \"b21\": 0.9,\n    \"b30\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.75,\n    \"b34\": 0.7,\n    \"b20\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6\n  }\n}\n```"], "60619e1391e0112c88b982ba": ["```json\n{\n  \"Summary\": \"This paper addresses the challenges of extending 2D CNN architectures to 3D point clouds, specifically focusing on achieving SE(3) equivariance while maintaining computational efficiency. It introduces a novel SE(3) separable convolution approach inspired by the Inception module and a group attentive pooling mechanism to effectively fuse SE(3)-equivariant features. The proposed methods are validated through experiments demonstrating superior performance in tasks requiring rotation invariance.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1.0,\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.8,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b3\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of extending 2D CNN architectures to 3D point cloud data, specifically focusing on SE(3)-equivariant features. It introduces SE(3) separable convolution to mitigate computational costs and a novel attention mechanism for fusing SE(3)-equivariant features.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1.0,\n    \"b4\": 0.9,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.8,\n    \"b13\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Extending CNN architecture to analyze point clouds\",\n      \"Computational cost of convolutions over SE(3) space\",\n      \"Effectively harnessing equivariant features without losing structural information\"\n    ],\n    \"novel_methods\": [\n      \"SE(3) separable point convolution\",\n      \"Group attentive pooling (GA pooling)\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b40\", \"b2\", \"b6\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b42\", \"b4\", \"b3\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b5\", \"b11\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of extending 2D convolutional neural networks (CNNs) to 3D point cloud data, particularly focusing on SE(3) equivariance. The main contributions include the introduction of SE(3) separable convolution to reduce computational costs, an attention mechanism for fusing SE(3) equivariant features, and the demonstration of improved performance on 3D tasks such as shape alignment.\",\n  \"Direct Inspiration\": [\"b40\", \"b6\", \"b4\"],\n  \"Indirect Inspiration\": [\"b42\", \"b3\", \"b5\", \"b11\"],\n  \"Other Inspiration\": [\"b15\", \"b28\", \"b47\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extending 2D CNNs to 3D point clouds with SE(3) equivariant features, proposing SE(3) separable point convolution (SPConv) and an attention mechanism for fusing equivariant features.\",\n  \"Direct Inspiration\": [\"b40\"],\n  \"Indirect Inspiration\": [\"b6\", \"b3\"],\n  \"Other Inspiration\": [\"b4\", \"b42\", \"b5\"]\n}\n```"], "6142b6175244ab9dcbc98070": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in unsupervised keyphrase extraction (UKE), proposing a novel method that jointly models local and global context. The method involves computing similarity between candidate phrases and the whole document for global context, and a new centrality computation method for local context, based on the importance of document boundaries. The approach aims to improve accuracy in extracting keyphrases, especially in long scientific documents.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.95,\n    \"b28\": 0.9,\n    \"b5\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b29\": 0.75,\n    \"b2\": 0.7,\n    \"b1\": 0.65,\n    \"b0\": 0.6,\n    \"b27\": 0.55,\n    \"b24\": 0.5,\n    \"b13\": 0.45,\n    \"b22\": 0.4,\n    \"b4\": 0.35\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of keyphrase extraction by proposing a novel unsupervised keyphrase extraction (UKE) model that jointly models the local and global context of input documents. The authors introduce a rank algorithm with three components: phrase-document similarity for global context, boundary-aware centrality for local context, and a combination of both for ranking. The proposed method outperforms existing models, especially on long scientific documents.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b28\": 1.0,\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b27\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b29\": 0.6,\n    \"b2\": 0.6,\n    \"b1\": 0.6,\n    \"b24\": 0.6,\n    \"b13\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of unsupervised keyphrase extraction (UKE), especially the limitations of existing methods that rely on simple similarity measures between candidate phrases and the document. The proposed algorithm jointly models local and global context to improve keyphrase extraction. Key components include phrase-document similarity for global context, boundary-aware centrality for local context, and the combination of these measures for ranking candidate phrases.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b27\": 0.8,\n    \"b18\": 0.7,\n    \"b29\": 0.7,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b24\": 0.6,\n    \"b13\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of accurately extracting keyphrases from documents using an unsupervised method. The proposed algorithm improves keyphrase extraction by jointly modeling both local and global context. This is achieved through a novel graph-based centrality computation that emphasizes boundary-aware centrality, which accounts for the position of phrases in a document.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b28\": 0.8,\n    \"b5\": 0.7,\n    \"b0\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.6,\n    \"b29\": 0.6,\n    \"b2\": 0.6,\n    \"b27\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.5,\n    \"b13\": 0.4,\n    \"b22\": 0.4,\n    \"b4\": 0.4,\n    \"b30\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving unsupervised keyphrase extraction (UKE) methods by jointly modeling local and global contexts of input documents. The proposed algorithm includes calculating phrase-document similarity for global context and introducing a boundary-aware centrality method for local context to enhance ranking accuracy.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b28\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b27\": 0.7,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b29\": 0.5,\n    \"b2\": 0.5\n  }\n}\n```"], "614012c05244ab9dcb816576": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in temporal relation extraction in natural language, focusing on capturing event hierarchies and asymmetrical relations. The authors propose novel methods using hyperbolic spaces, specifically the Poincar\u00e9 ball model, to improve event embedding and temporal reasoning.\",\n  \"Direct Inspiration\": [\"b19\", \"b6\", \"b7\"],\n  \"Indirect Inspiration\": [\"b21\", \"b37\", \"b10\"],\n  \"Other Inspiration\": [\"b32\", \"b29\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately detecting events and their temporal relations through the use of hyperbolic spaces. The proposed methods aim to overcome the limitations of Euclidean spaces in capturing hierarchical and asymmetrical structures of temporal relations.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b32\": 0.9,\n    \"b7\": 0.9,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b21\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extracting temporal relations among events, which exhibit complex hierarchical and asymmetric structures. The proposed approaches leverage hyperbolic spaces to more effectively encode these temporal relations, overcoming limitations of traditional Euclidean embeddings.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b7\": 1.0,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b28\": 0.75,\n    \"b29\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b37\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper involve accurately detecting events and their hierarchical and chronological properties in natural language, and encoding these properties in appropriate representations for effective temporal reasoning.\",\n    \"inspirations\": \"The authors were inspired by recent works on learning non-Euclidean embeddings, particularly Poincar\u00e9 embeddings, to propose learning event embeddings in hyperbolic spaces for temporal relation extraction.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b6\", \"b19\", \"b7\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b21\", \"b37\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b10\", \"b32\", \"b29\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of temporal relation extraction in natural language processing, particularly focusing on the hierarchical and chronological properties of events. The proposed solution involves learning event embeddings in hyperbolic spaces, specifically using the Poincar\u00e9 ball model, to capture asymmetrical relations and hierarchical structures. The paper introduces two main approaches: (1) an embedding learning method with a novel angular loss and (2) an end-to-end hyperbolic neural network for temporal relation detection.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b6\": 1,\n    \"b37\": 0.9,\n    \"b21\": 0.9,\n    \"b7\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b28\": 0.8,\n    \"b29\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b17\": 0.7,\n    \"b4\": 0.7,\n    \"b35\": 0.7,\n    \"b18\": 0.7,\n    \"b1\": 0.7,\n    \"b33\": 0.7,\n    \"b8\": 0.6,\n    \"b16\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```"], "60cb2a281bc21f07d0811386": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised image representation learning by proposing a novel BoW-based self-supervised approach named OBoW. It introduces a fully online teacher-student learning scheme, a dynamic BoW prediction module, and emphasizes contextual reasoning through multi-scale BoW reconstruction targets and aggressive data augmentation.\",\n  \"Direct Inspiration\": [\"b24\", \"b32\"],\n  \"Indirect Inspiration\": [\"b7\", \"b30\"],\n  \"Other Inspiration\": [\"b8\", \"b26\", \"b45\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in unsupervised image representation learning by proposing a novel BoW-based self-supervised approach that overcomes limitations of previous methods like BoWNet. The main contributions include a fully online teacher-student learning scheme, dynamic BoW prediction, and enhanced contextual reasoning through multi-scale BoW targets and aggressive data augmentation.\",\n    \"Direct Inspiration\": [\"b24\", \"b32\"],\n    \"Indirect Inspiration\": [\"b7\", \"b30\"],\n    \"Other Inspiration\": [\"b3\", \"b5\", \"b6\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a novel BoW-based self-supervised learning method, overcoming limitations of previous approaches by implementing a fully online teacher-student learning scheme, dynamic BoW prediction module, and enhanced contextual reasoning skills through multi-scale BoW reconstruction targets and aggressive data augmentation.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b32\": 1,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the development of a more efficient and effective BoW-based self-supervised learning approach for image representations. The authors propose a novel online teacher-student learning scheme to overcome the limitations of static teachers and to enhance contextual reasoning skills in the learned representations.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b30\": 0.7,\n    \"b26\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of the BoWNet approach for self-supervised image representation learning, proposing a novel fully online teacher-student learning scheme for BoW-based self-supervised training. The proposed method aims to learn richer and more powerful image representations while overcoming the limitations of having a static pre-trained teacher network. The paper introduces a dynamic BoW prediction module and emphasizes the importance of contextual reasoning skills.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b32\": 0.9,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.7,\n    \"b71\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```"], "609a6d14e4510cd7c88e5ded": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of large-scale rice mapping using remote sensing data, specifically focusing on overcoming limitations in traditional methods such as missing data due to weather conditions and inefficiencies in computation. It proposes a novel approach using deep convolutional neural networks (DCNNs), particularly U-Net, to perform semantic segmentation of multi-temporal Sentinel-1 SAR images for accurate rice mapping.\",\n  \"Direct Inspiration\": {\n    \"b53\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b55\": 0.8,\n    \"b26\": 0.7,\n    \"b24\": 0.7,\n    \"b75\": 0.7,\n    \"b1\": 0.6,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b52\": 0.5,\n    \"b54\": 0.5,\n    \"b44\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of large-scale rice mapping using multi-temporal SAR images, specifically Sentinel-1. The authors propose using an adapted U-Net for semantic segmentation to exploit spatial and polarization characteristics from multi-temporal datasets. This approach aims to overcome limitations of previous methods, such as low computational efficiency and lack of global spatial information.\",\n  \"Direct Inspiration\": {\n    \"b53\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b8\": 0.8,\n    \"b24\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b20\": 0.6,\n    \"b37\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of large-scale rice mapping using multi-temporal Sentinel-1 SAR images. The primary challenges include the limitations of traditional crop classification methods, phenological similarities of images in different years, and computational inefficiency. The authors propose using an adapted U-Net for semantic segmentation to exploit spatial and polarization characteristics from multi-temporal data sets, leveraging historical data for training and fine-tuning the deep learning network.\",\n  \"Direct Inspiration\": {\n    \"b53\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b20\": 0.8,\n    \"b72\": 0.8,\n    \"b55\": 0.8,\n    \"b61\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.5,\n    \"b24\": 0.5,\n    \"b32\": 0.5,\n    \"b30\": 0.5,\n    \"b59\": 0.5,\n    \"b75\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of large-scale rice mapping using remote sensing, specifically tackling the limitations of traditional crop classification methods and leveraging multi-temporal Sentinel-1 SAR images within a deep convolutional neural network (DCNN) framework. The key challenges include handling cloud cover issues in optical imagery, exploiting temporal correlation, and improving computational efficiency. The authors propose an adapted U-Net architecture for semantic segmentation of rice fields, incorporating spatial and polarization characteristics, and utilizing historical data for model training and fine-tuning.\",\n  \"Direct Inspiration\": {\n    \"b53\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b24\": 0.8,\n    \"b44\": 0.7,\n    \"b1\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b20\": 0.6,\n    \"b72\": 0.6,\n    \"b55\": 0.6,\n    \"b30\": 0.6,\n    \"b59\": 0.6,\n    \"b75\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the difficulty in accurately mapping rice crops at large scales using remote sensing techniques, particularly due to cloud cover during critical periods and the limitations of traditional classification methods. The paper introduces a novel approach using multi-temporal Sentinel-1 SAR images and an adapted U-Net deep learning model for semantic segmentation to improve the accuracy and efficiency of rice mapping.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b53\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b26\", \"b24\", \"b44\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b6\", \"b37\", \"b20\"]\n  }\n}\n```"], "608036c091e011772654fbb7": ["```json\n{\n  \"Summary\": \"The paper addresses the scalability and flexibility challenges in existing Graph Neural Networks (GNNs) by introducing a novel framework called Graph Multi-layer Perceptron (GMLP). The key contributions include a new feature message passing (FMP) abstraction that separates neural network updates from message passing and leverages multiple messages over different levels of localities to improve predictive accuracy while maintaining high scalability and efficiency.\",\n  \"Direct Inspiration\": [\"b6\", \"b29\"],\n  \"Indirect Inspiration\": [\"b4\", \"b8\", \"b11\", \"b12\", \"b13\", \"b21\", \"b24\", \"b33\"],\n  \"Other Inspiration\": [\"b16\", \"b26\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in Graph Neural Networks (GNNs): scalability and inflexibility. The proposed Graph Multi-layer Perceptron (GMLP) framework introduces a novel feature message passing (FMP) abstraction to separate neural network updates from message passing, enhancing scalability and flexibility. The GMLP framework allows for adaptive message aggregation at different levels of locality, achieving better accuracy and efficiency compared to existing GNN methods.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b29\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b11\": 0.8,\n    \"b13\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b26\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses scalability and flexibility issues in existing Graph Neural Networks (GNNs) and proposes a novel framework called Graph Multi-layer Perceptron (GMLP). GMLP introduces a new feature message passing (FMP) abstraction that allows for efficient and scalable feature aggregation and updates node representations by leveraging multiple messages over different levels of localities. The framework is designed to achieve high scalability and efficiency while maintaining or improving predictive accuracy.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b29\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b11\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b32\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing Graph Neural Networks (GNNs), specifically their scalability and inflexibility in handling large-scale graphs with high-dimensional features. The proposed Graph Multi-layer Perceptron (GMLP) framework introduces a novel feature message passing (FMP) abstraction that separates neural network updates from message passing. This framework offers high scalability, flexibility, and efficiency by preprocessing messages in a distributed manner and utilizing multiple messages over different localities. The contributions include a new message passing abstraction, various scalable GMLP variants, and superior performance and efficiency in experiments.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b29\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b11\": 0.9,\n    \"b13\": 0.9,\n    \"b33\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b16\": 0.7,\n    \"b31\": 0.7,\n    \"b12\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the scalability and flexibility challenges in existing Graph Neural Networks (GNNs) by proposing a novel framework called Graph Multi-layer Perceptron (GMLP). GMLP introduces a new feature message passing (FMP) abstraction that separates the neural network update from the message passing and leverages multiple messages over different levels of localities to update the node's final representation. This approach aims to achieve high scalability and efficiency while maintaining flexibility in node representation aggregation.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b29\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b11\": 0.85,\n    \"b13\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.75,\n    \"b33\": 0.7\n  }\n}\n```"], "60d3f7c191e0112ca5d1872a": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of data noise in distantly supervised relation extraction (DSRE) systems and proposes a contrastive instance learning (CIL) method to improve the Multi-Instance Learning (MIL) framework's efficiency. The key contribution is to leverage all instances in the MIL framework while maintaining robustness against DS data noise.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b1\": 0.6,\n    \"b22\": 0.7,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training robust and unbiased relation extraction (RE) systems under distant supervision (DS) data noise. It critiques the existing multi-instance learning (MIL) framework for its inefficiency in utilizing abundant instances within MIL bags. The proposed solution is a contrastive-based method, specifically contrastive instance learning (CIL), that aims to make the MIL framework more data-efficient while maintaining accuracy. The paper's contributions include a novel CIL method, evaluations on three public DSRE benchmarks demonstrating its effectiveness, and an ablation study validating the positive/negative pair construction strategy.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b6\": 0.6,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b13\": 0.7,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of dealing with noisy data in distantly supervised relation extraction (DSRE). It critiques the inefficiency of the Multi-Instance Learning (MIL) framework, which, while effective at noise reduction, does not make full use of all instances available. The authors propose a novel contrastive instance learning (CIL) method to better utilize instances within the MIL framework, aiming to improve both the efficiency and accuracy of DSRE models.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b10\": 0.6,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b11\": 0.5,\n    \"b6\": 0.4,\n    \"b22\": 0.4,\n    \"b14\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training a robust and unbiased relation extraction (RE) system under the noise introduced by distant supervision (DS) data. The authors propose a novel contrastive instance learning (CIL) method to improve the data efficiency of the multi-instance learning (MIL) framework, which has been traditionally used to mitigate DS noise. The CIL method aims to utilize each instance within MIL bags more effectively by ensuring instances with the same relational triples are close in semantic space, while those with different relational triples are far apart.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.9,\n    \"b10\": 0.8,\n    \"b11\": 0.85,\n    \"b13\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b6\": 0.75,\n    \"b14\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training a robust and unbiased Relation Extraction (RE) system under noisy distantly supervised data. It identifies the inefficiency in the Multi-Instance Learning (MIL) framework's use of bag-level representations and proposes a novel contrastive instance learning (CIL) method to enhance the MIL framework by effectively leveraging all instances within the bags. The proposed CIL method aims to bring instances sharing the same relational triples closer in the semantic space while distancing those with different relational triples.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b10\": 0.5,\n    \"b11\": 0.5,\n    \"b13\": 0.5,\n    \"b14\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```"], "613829e85244ab9dcb15f1c7": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inadequacy of modeling Math Word Problems (MWP) as a simple generation task due to the significant difference between mathematical expressions and natural language sequences. The proposed solution is a multi-task framework called Generate & Rank, which includes a generator to produce candidate expressions and a ranker to differentiate between correct and incorrect expressions. This framework is based on the BART pre-trained language model and introduces tree-based disturbances and an online update mechanism to enhance the training process.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b15\": 0.9,\n    \"b36\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.7,\n    \"b31\": 0.6,\n    \"b34\": 0.6,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in solving math word problems (MWPs) using natural language processing (NLP) techniques, particularly the shortcomings of LSTM-based sequence-to-sequence (Seq2Seq) models. The authors propose a novel multi-task framework called Generate & Rank, which includes a generator to produce candidate expressions and a ranker to distinguish between correct and incorrect expressions. The framework is built on BART, a pre-trained language model, and introduces methods such as tree-based disturbance and online update mechanisms to improve model performance. Experimental results show significant improvements over state-of-the-art models, particularly in generating longer mathematical expressions.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b15\": 0.7,\n    \"b33\": 0.85,\n    \"b34\": 0.75,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b16\": 0.65,\n    \"b28\": 0.6,\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in solving math word problems (MWP) by proposing a multi-task framework called Generate & Rank. The primary challenge discussed is the inadequacy of modeling MWP solely as a generation task due to the significant difference between mathematical expressions and natural language sequences. The proposed algorithm includes a generator and a ranker, jointly trained to generate candidate expressions and rank them to distinguish correct ones. The framework is built on BART, a pre-trained language model, and introduces innovative methods such as tree-based disturbance and an online update mechanism to enhance the ranker's performance.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b33\": 0.9,\n    \"b34\": 0.85,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b10\": 0.75,\n    \"b16\": 0.7,\n    \"b28\": 0.7,\n    \"b31\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses solving math word problems (MWP) using a multi-task framework named Generate & Rank. The main challenges include the insufficiency of treating MWP purely as a generation task, the significant impact of minor mistakes in mathematical expressions, and the degradation in performance for longer expressions. The authors propose a dual-module approach involving a generator and a ranker, both based on the BART pre-trained language model. The model is jointly trained with generation and ranking tasks to improve the selection of correct expressions from generated candidates. The effectiveness of the proposed model is demonstrated through extensive experiments and ablation studies on datasets like Math23K and MAWPS.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.85,\n    \"b31\": 0.8,\n    \"b34\": 0.8,\n    \"b10\": 0.75,\n    \"b15\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b16\": 0.6,\n    \"b0\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in solving math word problems (MWP) by proposing a multi-task framework called Generate & Rank. This framework introduces a generator to produce candidate expressions and a ranker to distinguish between correct and incorrect expressions. The model is built on BART, a pre-trained language model, and further fine-tuned for the specific tasks. The study aims to improve the accuracy and robustness of MWP solvers, particularly for longer expressions, through joint training and innovative candidate generation methods.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b34\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```"], "60488f5991e0115491a5ca82": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of out-of-distribution (OOD) graph classification where training and test graphs differ in size and vertex attributes. It introduces a causal model inspired by Stochastic Block Models (SBMs) and graphon random graph models, and proposes a graph representation method based on induced homomorphism densities and Graph Neural Networks (GNNs) to achieve environment-invariant representations capable of extrapolating to OOD test data.\",\n  \"Direct Inspiration\": {\n    \"b78\": 1.0,\n    \"b141\": 0.9,\n    \"b49\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b110\": 0.7,\n    \"b144\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b64\": 0.5,\n    \"b145\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is learning graph representation for out-of-distribution (OOD) inductive tasks where training and test distributions differ, specifically in graph sizes and vertex attributes. The authors propose a graph representation method based on causal models and Graph Neural Networks (GNNs) that is invariant to train/test distribution shifts, enabling robust extrapolation.\",\n  \n  \"Direct Inspiration\": {\n    \"b78\": 1,\n    \"b6\": 1,\n    \"b141\": 1,\n    \"b144\": 0.9\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b64\": 0.8,\n    \"b49\": 0.8,\n    \"b145\": 0.8\n  },\n  \n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b117\": 0.7,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning graph representations for out-of-distribution (OOD) inductive tasks, where the training and test graphs have distinct characteristics. The authors propose a causal model based on graphons and introduce a graph representation method that is invariant to train/test distribution shifts, leveraging Graph Neural Networks (GNNs) and inspired by Stochastic Block Models (SBMs) and the work of Lov\u00e1sz and Szegedy.\",\n  \"Direct Inspiration\": {\n    \"b78\": 1.0,\n    \"b64\": 1.0,\n    \"b49\": 1.0,\n    \"b145\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b141\": 0.8,\n    \"b144\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b110\": 0.7,\n    \"b112\": 0.7,\n    \"b56\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Learning graph representations for out-of-distribution (OOD) tasks.\",\n      \"Ensuring robustness of inductive graph neural networks (GNNs) to distribution shifts.\",\n      \"Designing graph classifiers invariant to train/test distribution shifts without access to test samples.\"\n    ],\n    \"inspirations\": [\n      \"Causal model for graph classification tasks.\",\n      \"Graph representation method invariant to distribution shifts.\",\n      \"Empirical results comparing traditional methods with proposed invariant representations.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b78\": 1,\n    \"b6\": 0.9,\n    \"b64\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b49\": 0.7,\n    \"b145\": 0.7,\n    \"b141\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b18\": 0.5,\n    \"b57\": 0.5,\n    \"b76\": 0.5,\n    \"b101\": 0.5,\n    \"b110\": 0.5,\n    \"b122\": 0.5,\n    \"b34\": 0.4,\n    \"b117\": 0.4,\n    \"b112\": 0.3,\n    \"b97\": 0.3,\n    \"b96\": 0.3,\n    \"b85\": 0.2,\n    \"b88\": 0.2,\n    \"b140\": 0.2,\n    \"b128\": 0.2,\n    \"b132\": 0.2,\n    \"b124\": 0.1,\n    \"b104\": 0.1,\n    \"b22\": 0.1,\n    \"b26\": 0.1,\n    \"b27\": 0.1,\n    \"b1\": 0.1,\n    \"b29\": 0.1,\n    \"b31\": 0.1,\n    \"b37\": 0.1,\n    \"b40\": 0.1,\n    \"b41\": 0.1,\n    \"b58\": 0.1,\n    \"b68\": 0.1,\n    \"b86\": 0.1,\n    \"b91\": 0.1,\n    \"b106\": 0.1,\n    \"b107\": 0.1,\n    \"b109\": 0.1,\n    \"b115\": 0.1,\n    \"b114\": 0.1,\n    \"b115\": 0.1,\n    \"b131\": 0.1,\n    \"b133\": 0.1,\n    \"b127\": 0.1,\n    \"b135\": 0.1,\n    \"b142\": 0.1,\n    \"b12\": 0.1,\n    \"b13\": 0.1,\n    \"b15\": 0.1,\n    \"b98\": 0.1,\n    \"b51\": 0.1,\n    \"b27\": 0.1,\n    \"b112\": 0.1,\n    \"b144\": 0.1,\n    \"b97\": 0.1,\n    \"b56\": 0.1,\n    \"b30\": 0.1,\n    \"b67\": 0.1,\n    \"b16\": 0.1,\n    \"b18\": 0.1,\n    \"b57\": 0.1,\n    \"b76\": 0.1,\n    \"b101\": 0.1,\n    \"b110\": 0.1\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning graph representations for out-of-distribution (OOD) inductive tasks where the training and test graphs have distinct characteristics. It proposes a causal model and introduces a graph representation method invariant to train/test distribution shifts based on the work of Lov\u00e1sz and Szegedy and Graph Neural Networks (GNNs).\",\n  \"Direct Inspiration\": {\n    \"b78\": 1.0,\n    \"b64\": 1.0,\n    \"b49\": 1.0,\n    \"b145\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b18\": 0.8,\n    \"b57\": 0.8,\n    \"b76\": 0.8,\n    \"b101\": 0.8,\n    \"b110\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b34\": 0.7,\n    \"b117\": 0.7\n  }\n}\n```"], "6059cb9691e011ed950a5c21": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of visual object tracking, including occlusion, deformation, and appearance changes, which are exacerbated by temporal error accumulation during online processing. The proposed method introduces a transformer architecture to the visual tracking community to effectively convey rich temporal information across frames, leveraging the attention mechanism of the transformer to establish pixel-wise correspondence across frames. This novel transformer-assisted tracking framework improves the performance of both Siamese and DCF-based tracking paradigms and sets new state-of-the-art records on several benchmarks.\",\n    \"Direct Inspiration\": {\n        \"b49\": 1,\n        \"b14\": 0.9,\n        \"b2\": 0.95\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.85,\n        \"b45\": 0.8,\n        \"b31\": 0.75,\n        \"b30\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.6,\n        \"b41\": 0.6,\n        \"b25\": 0.6,\n        \"b39\": 0.6,\n        \"b26\": 0.6,\n        \"b60\": 0.6,\n        \"b28\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in visual object tracking, particularly occlusion, deformation, and appearance changes amplified by temporal error accumulation. The proposed solution leverages a transformer architecture to bridge isolated video frames and convey rich temporal information, enhancing the tracking performance. The transformer is modified to suit visual tracking, separating the encoder and decoder into two branches and incorporating instance normalization and slimming designs for efficiency.\",\n    \"Direct Inspiration\": {\n        \"b49\": 1.0,\n        \"b2\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.8,\n        \"b31\": 0.8,\n        \"b45\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b41\": 0.6,\n        \"b25\": 0.6,\n        \"b15\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenges in visual object tracking such as occlusion, deformation, and appearance changes, and introduces a transformer-assisted tracking framework to enhance temporal information propagation across video frames. The new approach leverages the transformer's attention mechanism to bridge isolated video frames and improve tracking performance.\",\n  \"Direct Inspiration\": {\n    \"b49\": 1.0,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b31\": 0.7,\n    \"b21\": 0.7,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b59\": 0.5,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The given paper addresses the challenges in visual object tracking, particularly the issues of occlusion, deformation, and appearance changes, which are further amplified in the online tracking process. The authors propose a novel transformer-assisted tracking framework that leverages the rich temporal information in video data. By integrating a modified transformer architecture into popular tracking frameworks, the paper aims to enhance the mutual reinforcement of features across frames and improve the tracking model's performance.\",\n  \"Direct Inspiration\": {\n    \"b49\": 1,\n    \"b2\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b30\": 0.75,\n    \"b31\": 0.8,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b59\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in visual object tracking due to occlusion, deformation, and appearance changes, which are further amplified by temporal error accumulation. The proposed solution introduces a novel transformer-assisted tracking framework leveraging the encoder-decoder structure of transformers to bridge isolated video frames and convey rich temporal information. This framework aims to improve the tracking process by mutually reinforcing template features and propagating temporal contexts, thus enhancing target representation and localization.\",\n  \"Direct Inspiration\": {\n    \"b49\": 1.0,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b21\": 0.5,\n    \"b41\": 0.5\n  }\n}\n```"], "60c887f2e4510cd7c8f781b6": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Designing an effective feature representation for target appearance modeling.\",\n      \"Dealing with occlusion, deformation, and view changes in videos.\",\n      \"Leveraging temporal information for more robust tracking.\"\n    ],\n    \"novel_methods\": [\n      \"Motion-Aware Recurrent Neural Network (MA-RNN) for spatial-temporal representation.\",\n      \"Context-aware displacement attention (CADA) module for motion capture and feature alignment.\",\n      \"Monotonic bounding box regression (mBBR) for accurate target scale estimation.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"inspired_by\": [\"b62\"],\n    \"motivated_by\": [\"b7\"]\n  },\n  \"Indirect Inspiration\": [\n    \"b1\",\n    \"b13\",\n    \"b16\",\n    \"b25\",\n    \"b30\"\n  ],\n  \"Other Inspiration\": [\n    \"b0\",\n    \"b21\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenges in visual tracking, specifically the need for effective spatial-temporal feature representation to handle occlusion, deformation, and other variabilities in target appearance. The proposed solution, MA-RNN, combines a context-aware displacement attention (CADA) module and an RNN for robust long-term tracking. The paper also introduces a monotonic bounding box regression (mBBR) method for accurate scale estimation.\",\n  \"Direct Inspiration\": [\n    \"b62\",\n    \"b13\",\n    \"b7\",\n    \"b1\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b21\",\n    \"b30\",\n    \"b31\"\n  ],\n  \"Other Inspiration\": [\n    \"b17\",\n    \"b25\",\n    \"b16\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in visual tracking, particularly the need for effective feature representation to handle occlusion, deformation, and view changes. The proposed solution is a Motion-Aware Recurrent Neural Network (MA-RNN) that includes a context-aware displacement attention (CADA) module and an RNN for long-term spatial-temporal representation.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b62\": 1,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b16\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in visual tracking, specifically in designing spatial-temporal representations to handle occlusion, deformation, and view changes. It proposes a Motion-Aware Recurrent Neural Network (MA-RNN) incorporating a context-aware displacement attention (CADA) module and long-term representation using RNNs. It also introduces a monotonic bounding box regression (mBBR) for accurate target scale estimation.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b7\": 0.95,\n        \"b62\": 0.88\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.75,\n        \"b21\": 0.7,\n        \"b30\": 0.73,\n        \"b31\": 0.72\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.65,\n        \"b25\": 0.68,\n        \"b26\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of visual tracking, specifically the need for effective feature representation to handle occlusion, deformation, and view changes. The proposed solution is a Motion-Aware Recurrent Neural Network (MA-RNN) that includes a Context-Aware Displacement Attention (CADA) module to capture motion dynamics and align spatial features. The paper also introduces a Monotonic Bounding Box Regression (mBBR) for accurate target scale estimation.\",\n  \"Direct Inspiration\": [\"b62\", \"b13\"],\n  \"Indirect Inspiration\": [\"b7\", \"b1\", \"b16\", \"b30\"],\n  \"Other Inspiration\": [\"b0\", \"b25\"]\n}\n```"], "6173f1bc91e0118698c04819": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of instruction STLB (iSTLB) misses in server workloads, which are a significant performance bottleneck due to large code footprints and contention in the second-level TLB shared between instruction and data translations. The proposed solution is Morrigan, a novel iSTLB prefetcher composed of two modules: IRIP (Irregular Instruction TLB Prefetcher) and SDP (Small Delta Prefetcher). These modules aim to efficiently prefetch iSTLB misses using variable length Markov chains and a new frequency-based replacement policy.\",\n  \"Direct Inspiration\": [\"b48\", \"b56\", \"b59\"],\n  \"Indirect Inspiration\": [\"b47\", \"b73\"],\n  \"Other Inspiration\": [\"b11\", \"b16\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of handling instruction STLB (iSTLB) misses in server workloads, which significantly degrade performance due to large code footprints and high instruction growth rates. It proposes a novel microarchitectural iSTLB prefetcher named Morrigan, composed of two modules: IRIP and SDP, which leverage Markov chains and small delta prefetching, respectively, to mitigate iSTLB misses.\",\n  \"Direct Inspiration\": {\n    \"b48\": 1,\n    \"b56\": 1,\n    \"b59\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b47\": 0.8,\n    \"b63\": 0.8,\n    \"b73\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b41\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the significant performance degradation caused by instruction STLB (iSTLB) misses in modern server workloads, which is exacerbated by the growing instruction footprint of server applications. The proposed solution is Morrigan, a novel iSTLB prefetcher composed of two specialized prefetch engines: the Irregular Instruction TLB Prefetcher (IRIP) and the Small Delta Prefetcher (SDP). Morrigan aims to prefetch iSTLB misses more effectively than existing methods, leveraging new prediction and replacement policies to improve performance.\",\n  \"Direct Inspiration\": {\n    \"b47\": 1.0,\n    \"b48\": 0.9,\n    \"b56\": 0.8,\n    \"b59\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b41\": 0.6,\n    \"b73\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b63\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the critical issue of instruction STLB (iSTLB) misses causing performance bottlenecks in server workloads due to their large code footprints. It introduces Morrigan, a novel microarchitectural iSTLB prefetcher composed of two specialized prefetching modules (IRIP and SDP) to mitigate these misses.\",\n    \"Direct Inspiration\": {\n        \"b48\": 1,\n        \"b56\": 1,\n        \"b59\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b47\": 0.9,\n        \"b73\": 0.8,\n        \"b63\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.7,\n        \"b16\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the performance degradation caused by instruction STLB (iSTLB) misses in modern server workloads. The paper introduces Morrigan, a novel microarchitectural iSTLB prefetcher composed of two complementary modules: the Irregular Instruction TLB Prefetcher (IRIP) using a new replacement policy (RLFU), and the Small Delta Prefetcher (SDP). The paper's contributions include the characterization of iSTLB misses, the ineffectiveness of existing dSTLB and instruction cache prefetchers for iSTLB misses, and the proposal of Morrigan, which provides significant performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b59\": 1,\n    \"b48\": 0.9,\n    \"b56\": 0.9,\n    \"b47\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b63\": 0.7,\n    \"b73\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```"], "60641d3f9e795e72406b663f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving alignment between source documents and generated summaries in abstractive summarization tasks. It proposes a novel set-level matching heuristic to create better pseudo summaries for training extractor-abstractor frameworks. The new approach aims to balance the advantages of sentence-level and summary-level methods by assuming many-to-many alignments, and it integrates reinforcement learning to optimize the model.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b2\": 0.9,\n    \"b27\": 0.8,\n    \"b14\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b28\": 0.65,\n    \"b16\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of creating accurate and informative abstractive summaries from long text documents by improving the alignment between pseudo summaries and generated summaries. The authors propose a novel set-level matching heuristics that divides reference summaries into clusters and matches non-overlapping sets of sentences from the source document. They also introduce a keyword-based extractor and a comprehensive reinforcement learning framework to enhance the performance of the abstractive summarization model.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b6\": 0.85,\n        \"b14\": 0.95,\n        \"b27\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.75,\n        \"b19\": 0.8,\n        \"b21\": 0.75,\n        \"b23\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.7,\n        \"b5\": 0.7,\n        \"b9\": 0.7,\n        \"b22\": 0.7,\n        \"b28\": 0.7,\n        \"b32\": 0.7,\n        \"b35\": 0.75\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving abstractive summarization by introducing a novel set-level matching heuristic. This heuristic aims to enhance the alignment between source documents and their summaries by clustering sentences into disjoint sets based on keywords and maximizing ROUGE scores. The proposed framework consists of three main components: a keyword-based extractor, an abstractor, and a comprehensively rewarded reinforcement learning mechanism. The authors emphasize the importance of set-level alignment and propose a comprehensive reward system to better train the model.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b2\": 0.9,\n    \"b27\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b22\": 0.6,\n    \"b32\": 0.5,\n    \"b35\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorrect alignments in abstractive summarization using encoder-decoder models. It proposes a novel set-level matching heuristic and a keyword-based extractor to improve the quality of pseudo summaries and enhance the alignment between the encoder and decoder. The approach involves pretraining the extractor and abstractor with pseudo summaries and finetuning them using a comprehensive reward system in reinforcement learning.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b6\": 1.0,\n    \"b27\": 1.0,\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b32\": 0.6,\n    \"b35\": 0.6,\n    \"b16\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the creation of better pseudo summaries for training an extractor-abstractor framework for abstractive summarization. The proposed algorithm introduces set-level matching heuristics that use keywords to improve alignment between pseudo summaries and reference summaries. The paper also proposes a comprehensive reward system in reinforcement learning to enhance extraction and abstraction quality.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b6\": 0.9,\n    \"b21\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b22\": 0.7\n  }\n}\n```"], "600fe843d4150a363c23fd0d": ["```json\n{\n  \"Summary\": \"The paper presents a novel model for generating short text summaries in Chinese using keyword templates and fine-tuning the BERT pre-trained language model. The primary challenges addressed include the inefficiency of BERT in generating high-quality short text summaries and the labor-intensive nature of manually creating templates. The proposed model improves data preprocessing and introduces a keyword-based sentence division method to generate higher-quality summaries.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b7\": 1,\n    \"b8\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.9,\n    \"b5\": 0.9,\n    \"b28\": 0.9,\n    \"b29\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.8,\n    \"b10\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b16\": 0.8,\n    \"b18\": 0.8,\n    \"b20\": 0.8,\n    \"b21\": 0.8,\n    \"b22\": 0.8,\n    \"b23\": 0.8,\n    \"b24\": 0.8,\n    \"b25\": 0.8,\n    \"b26\": 0.8,\n    \"b27\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality short text summaries in Chinese using pre-trained language models, specifically BERT. The novel approach introduced involves a keyword template-based method for data preprocessing and sentence division, significantly improving the performance of BERT in this task. The method includes extracting keywords from reference summaries and using them to guide the sentence division and summary generation process.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b7\": 0.9,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b8\": 0.8,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving the performance of BERT-based models for generating short text summaries in Chinese. It introduces a novel preprocessing method based on keyword templates and a modified sentence division approach to enhance the quality of the summaries.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1,\n        \"b7\": 0.9,\n        \"b28\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.7,\n        \"b29\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.5,\n        \"b8\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality Chinese short text summaries using pre-trained language models, particularly BERT. It proposes a novel approach that incorporates keyword templates for sentence division, improving BERT's performance in this task. The model named BSA*, uses a different data preprocessing method and fine-tunes BERT for better results.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b8\": 0.8,\n    \"b19\": 0.85,\n    \"b28\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b29\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b3\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the application of pre-trained language models to generate high-quality Chinese short text summaries. The authors propose a novel short text summary generation model based on keyword templates to enhance BERT's performance in this context.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.7,\n    \"b29\": 0.6\n  }\n}\n```"], "609a6ea9e4510cd7c891957a": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the accurate extraction of spatial and semantic information from very high resolution (VHR) remotely sensed images, which is complicated by factors like interclass fuzziness, intraclass discrepancies, and the large scale differences between objects. The proposed solution is a novel duplex path element extraction network, termed the duplex restricted network (DRN) with guided upsampling, which includes a content capture normalization module (CCNM) based on atrous convolutions, a detachable enhancement structure (DES), and a concentration-aware guided upsampling (CAGU) strategy.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b12\": 1.0,\n    \"b19\": 1.0,\n    \"b20\": 1.0,\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.8,\n    \"b42\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b14\": 0.5,\n    \"b36\": 0.5,\n    \"b37\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in the semantic segmentation of very high resolution (VHR) remotely sensed images, specifically focusing on issues related to spectral information affected by illumination and noise, compatibility of classifications due to size differences of earth objects, interclass fuzziness, and intraclass discrepancies. The authors propose a novel duplex restricted network (DRN) with guided upsampling to effectively utilize the information encoded by the backbone. The proposed framework includes a content capture normalization module (CCNM) based on atrous convolution, a detachable enhancement structure (DES) for obtaining different characteristics from the same receptive field, and a novel upsampling strategy (CAGU) to achieve higher performance.\",\n  \"Direct Inspiration\": [\n    \"b19\",\n    \"b38\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b10\",\n    \"b12\",\n    \"b20\",\n    \"b33\"\n  ],\n  \"Other Inspiration\": [\n    \"b14\",\n    \"b39\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of semantic segmentation in very high resolution (VHR) remotely sensed images, particularly focusing on the issues of spectral information distortion, interclass fuzziness, and intraclass discrepancies. The proposed solution is a novel duplex path element extraction network, termed the duplex restricted network (DRN) with guided upsampling. Key components include the content capture normalization module (CCNM) based on atrous convolution, a detachable enhancement structure (DES), and a novel upsampling strategy called Concentrate-Aware Guided Upsampling (CAGU).\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b12\": 0.9,\n    \"b20\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.7,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of accurately extracting spatial and semantic information from very high resolution (VHR) remotely sensed images, focusing on issues like spectral information interference, interclass fuzziness, and intraclass discrepancies. The proposed solution is a novel duplex path element extraction network called the duplex restricted network (DRN) with guided upsampling. The key components of DRN include a content capture normalization module (CCNM), a detachable enhancement structure (DES), and concentration-aware guided upsampling (CAGU).\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b20\": 1.0,\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b12\": 0.9,\n    \"b19\": 0.9,\n    \"b33\": 0.9,\n    \"b42\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of accurate semantic segmentation of very high-resolution (VHR) remotely sensed images, specifically focusing on the issues of spectral information distortion, scale differences, and the need for deeper context relations. The authors propose a novel duplex path element extraction network, termed the duplex restricted network (DRN) with guided upsampling, which includes components like the content capture normalization module (CCNM), detachable enhancement structure (DES), and concentration-aware guided upsampling (CAGU). These modules aim to effectively utilize and enhance the information encoded by the backbone network, VGG-16.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b20\": 1,\n    \"b33\": 1,\n    \"b38\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b12\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b39\": 0.5\n  }\n}\n```"], "5ff4305191e01130648dc2af": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is finding the best prompt to add to the input to make a language model output the desired answer, leveraging pretrained language models (PLMs) for downstream tasks. The proposed algorithm, WARP (Word-level Adversarial ReProgramming), is inspired by Adversarial Reprogramming and aims to optimize prompts using stochastic gradient descent. The method is evaluated on sentence classification and natural language inference tasks, showing significant improvements over existing methods such as AutoPrompt and comparable performance to fine-tuning approaches.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b9\": 0.7,\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of pretrained language models on downstream tasks using a novel method called WARP (Word-level Adversarial RePrograming). This method aims to find the best input-level prompts to exploit language model capabilities better than manually designed prompts, drawing inspiration from Adversarial Reprogramming.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b8\": 0.6,\n    \"b4\": 0.5,\n    \"b7\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.4,\n    \"b0\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of finding effective prompts for language models to perform specific NLP tasks. It proposes a novel technique called Word-level Adversarial ReProgramming (WARP), inspired by Adversarial Reprogramming, to optimize these prompts. The paper evaluates WARP on sentence classification and natural language inference tasks, showing it outperforms existing methods like the frozen-features approach and AutoPrompt.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b7\": 0.8,\n    \"b9\": 0.7,\n    \"b0\": 0.7,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b8\": 0.5,\n    \"b1\": 0.5,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on improving language model outputs by finding the best prompts to add to input, using a technique called Word-level Adversarial ReProgramming (WARP). This method is inspired by adversarial reprogramming and aims to outperform existing techniques like frozen-features and AutoPrompt in tasks such as sentence classification and natural language inference.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b7\": 0.7,\n    \"b9\": 0.8,\n    \"b0\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of leveraging pretrained language models for natural language processing tasks by introducing a novel technique called WARP (Word-level Adversarial ReProgramming). This method aims to find the best prompt to be added to the input to make a language model output the desired answer. The method is inspired by Adversarial Reprogramming and is evaluated on sentence classification and natural language inference tasks, showing improved performance over existing methods like AutoPrompt.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b7\": 0.7,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b1\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.5,\n    \"b8\": 0.5,\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```"], "6037828e91e011d7c73cd499": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving text-to-image synthesis using large-scale autoregressive transformers. The authors propose a two-stage training procedure involving a dVAE and a 12-billion parameter sparse transformer trained on 250 million image-text pairs. The key contributions are the high fidelity of generated images and the ability to perform complex tasks like image-to-image translation without custom approaches.\",\n    \"Direct Inspiration\": {\n        \"b25\": 1,\n        \"b35\": 0.9,\n        \"b46\": 0.9,\n        \"b33\": 0.8,\n        \"b28\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b31\": 0.7,\n        \"b49\": 0.7,\n        \"b13\": 0.6,\n        \"b4\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.5,\n        \"b9\": 0.5,\n        \"b36\": 0.5,\n        \"b12\": 0.5,\n        \"b47\": 0.5,\n        \"b52\": 0.5,\n        \"b44\": 0.5,\n        \"b37\": 0.5,\n        \"b11\": 0.5,\n        \"b21\": 0.5,\n        \"b45\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high fidelity images from text descriptions using large-scale generative models. The authors propose a method involving a 12-billion parameter autoregressive transformer trained on 250 million image-text pairs. The primary inspiration for this work includes advancements in generative adversarial networks, autoregressive transformers, and two-stage training procedures incorporating discrete variational autoencoders.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b28\": 1.0,\n    \"b46\": 0.9,\n    \"b49\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b35\": 0.6,\n    \"b25\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b47\": 0.4,\n    \"b38\": 0.3\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating high-quality images from text descriptions using large-scale generative models. It proposes a novel approach by training a 12-billion parameter autoregressive transformer on 250 million image-text pairs, demonstrating significant improvements in visual fidelity and the ability to perform complex tasks like image-to-image translation.\",\n    \"Direct Inspiration\": {\n        \"b25\": 1.0,\n        \"b35\": 0.9,\n        \"b46\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b28\": 0.7,\n        \"b33\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b49\": 0.5,\n        \"b47\": 0.5,\n        \"b43\": 0.4\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating high-fidelity images from text descriptions using a large-scale autoregressive transformer model. The proposed method involves training a transformer on image-text pairs, with a two-stage training procedure to handle image tokens efficiently. The authors demonstrate significant improvements in image quality and zero-shot generalization over prior methods.\",\n    \"Direct Inspiration\": {\n        \"b25\": 1,\n        \"b35\": 1,\n        \"b46\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b28\": 0.8,\n        \"b33\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b49\": 0.7,\n        \"b13\": 0.6,\n        \"b47\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving visual fidelity in text-to-image synthesis by training a 12-billion parameter autoregressive transformer on a large dataset of 250 million image-text pairs. The novel contributions include using a two-stage training procedure with a discrete variational autoencoder (dVAE) and autoregressively modeling text and image tokens as a single data stream. The paper claims significant improvements in image quality and zero-shot generalization capabilities.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b25\", \"b35\", \"b46\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b28\", \"b33\", \"b12\", \"b49\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b42\", \"b47\", \"b4\"]\n  }\n}\n```"], "61850e9691e01121084ca0ce": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving item representation learning in recommendation systems, particularly for long-tail item distributions and sparse data. It proposes a self-supervised learning (SSL) framework using a two-tower DNN architecture, leveraging contrastive learning and data augmentation techniques inspired by SimCLR to enhance item representations.\",\n  \"Direct Inspiration\": [\"b5\"],\n  \"Indirect Inspiration\": [\"b32\", \"b54\", \"b56\"],\n  \"Other Inspiration\": [\"b13\", \"b34\", \"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the effective modeling of interactions between queries and items in recommendation systems with large and sparse data. The authors propose a self-supervised learning (SSL) framework using two-tower deep neural networks (DNNs) to improve item representation, especially for long-tail item distributions. The novel approach involves augmenting data by masking input information and applying contrastive loss to learn robust embeddings. This framework aims to enhance generalization and performance in recommendation tasks, leveraging SSL to optimize both an auxiliary self-supervised loss and the primary supervised loss.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b54\": 0.8,\n    \"b56\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b2\": 0.5,\n    \"b37\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving item representations in large-scale recommendation systems with highly sparse data. The proposed algorithm leverages self-supervised learning (SSL) techniques to enhance data augmentation and representation learning, particularly for long-tail item distributions. The approach includes a framework inspired by SimCLR, contrastive learning, and a novel data augmentation method tailored for categorical features.\",\n    \"Direct Inspiration\": {\n        \"b5\": 1.0,\n        \"b32\": 0.9,\n        \"b54\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.8,\n        \"b56\": 0.7,\n        \"b17\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b37\": 0.6,\n        \"b60\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving item representation in recommendation systems, particularly under conditions of long-tail distributions and sparse data. The authors propose a self-supervised learning (SSL) framework that leverages data augmentation and contrastive loss within a two-tower deep neural network architecture. They demonstrate the effectiveness of this approach through offline and live experiments, showing significant performance improvements over existing techniques.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.9,\n    \"b54\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.8,\n    \"b56\": 0.7,\n    \"b60\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is improving item representations in recommendation systems with sparse data distributions, particularly for long-tail items. The authors propose a self-supervised learning (SSL) framework leveraging auxiliary tasks to enhance item representations, specifically using two-tower DNNs and contrastive learning techniques inspired by the SimCLR framework.\",\n  \"Direct Inspiration\": [\"b5\", \"b32\", \"b54\"],\n  \"Indirect Inspiration\": [\"b13\", \"b56\"],\n  \"Other Inspiration\": [\"b21\", \"b34\", \"b46\"]\n}\n```"], "60c1b1ca91e0112cf43c21f5": ["```json\n{\n    \"Summary\": \"The paper proposes a socially-aware self-supervised tri-training framework (SEPT) to improve recommendation systems by combining SSL and tri-training. The primary challenge addressed is the misclassification of similar nodes as negatives due to homophily in user-item interactions. The novel approach involves leveraging social relations to create supplementary views and using tri-training to mine self-supervision signals from these views.\",\n    \"Direct Inspiration\": [\n        \"b39\",\n        \"b18\",\n        \"b28\",\n        \"b36\",\n        \"b45\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b43\",\n        \"b46\"\n    ],\n    \"Other Inspiration\": [\n        \"b3\",\n        \"b37\",\n        \"b40\",\n        \"b41\",\n        \"b42\",\n        \"b10\"\n    ]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Addressing the issue of false negatives in self-supervised learning for recommendation systems\",\n      \"Integrating social relations to capture homophily among users\"\n    ],\n    \"Inspirations\": [\n      \"Utilizing self-supervised graph representation learning and graph contrastive learning\",\n      \"Combining tri-training with SSL for improved user representation and recommendation\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b18\": 0.9,\n    \"b28\": 0.9,\n    \"b36\": 0.9,\n    \"b45\": 0.9,\n    \"b46\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b37\": 0.8,\n    \"b40\": 0.8,\n    \"b41\": 0.8,\n    \"b42\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b43\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving recommendation quality in recommender systems by proposing a novel socially-aware self-supervised tri-training framework (SEPT). This framework integrates self-supervised learning (SSL) and tri-training to leverage social relations for data augmentation, thereby capturing user homophily and improving recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b18\": 1.0,\n    \"b28\": 1.0,\n    \"b36\": 1.0,\n    \"b45\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b37\": 0.8,\n    \"b40\": 0.8,\n    \"b41\": 0.8,\n    \"b42\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.9,\n    \"b10\": 0.7,\n    \"b19\": 0.7,\n    \"b43\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving recommendation quality in self-supervised learning (SSL) by integrating a socially-aware SSL framework with tri-training. The proposed method aims to tackle the issue of false negatives in contrastive learning by leveraging social relations to create supplementary views, thus enhancing user representations through multi-view encoding.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b28\": 0.9,\n    \"b36\": 0.9,\n    \"b45\": 0.9,\n    \"b46\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b26\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b37\": 0.7,\n    \"b40\": 0.7,\n    \"b41\": 0.7,\n    \"b42\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving recommendation quality in recommender systems by proposing a socially-aware self-supervised tri-training framework that combines tri-training with self-supervised learning (SSL). The framework leverages social relations to capture user homophily and creates multiple views for mining self-supervision signals, thus enhancing the recommendation performance. The proposed method integrates tri-training with SSL for the first time in this context.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b28\": 0.9,\n    \"b36\": 0.8,\n    \"b45\": 1,\n    \"b46\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.7,\n    \"b43\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b40\": 0.5,\n    \"b42\": 0.5\n  }\n}\n```"], "61dcf5495244ab9dcb1fb68d": ["```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing collaborative filtering methods which fail to encode collaborative signals explicitly and struggle with sparse interaction data. It proposes a Supervised Contrastive Learning (SCL) framework tailored for recommendation systems. The SCL framework introduces supervised information to ensure similar users/items are closer in the representation space, enhancing recommendation performance. Additionally, a new data augmentation method called node replication is proposed to improve the diversity and robustness of the representations.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b12\": 0.85,\n    \"b13\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing collaborative filtering methods in recommendation systems, especially in handling sparse interaction data and the representation of similar users/items. It proposes a Supervised Contrastive Learning (SCL) framework, incorporating supervised information for better representation learning and a new data augmentation method called node replication to improve recommendation diversity and robustness.\",\n  \"Direct Inspiration\": [\"b12\", \"b13\", \"b14\"],\n  \"Indirect Inspiration\": [\"b11\"],\n  \"Other Inspiration\": [\"b9\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the limitations of traditional collaborative filtering methods and proposes a Supervised Contrastive Learning (SCL) paradigm to improve recommendation system performance. The key challenges include handling data sparsity and enhancing representation learning by incorporating supervised information and novel data augmentation techniques.\",\n    \"Direct Inspiration\": [\"b11\", \"b12\", \"b13\", \"b14\"],\n    \"Indirect Inspiration\": [\"b9\", \"b10\"],\n    \"Other Inspiration\": [\"b15\", \"b16\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving recommendation systems by proposing a Supervised Contrastive Learning (SCL) method. This method introduces supervised information to ensure similar users/items have closer representations, improving the recommendation performance. Additionally, a new data augmentation method called node replication is proposed to enhance the diversity and robustness of recommendations.\",\n    \"Direct Inspiration\": {\n        \"b11\": 0.9,\n        \"b13\": 0.85,\n        \"b14\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.75,\n        \"b9\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.65,\n        \"b5\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing collaborative filtering methods in recommendation systems, particularly the issues arising from sparse interaction data and the use of inappropriate contrastive learning frameworks. The authors propose a supervised contrastive learning (SCL) paradigm tailored to recommendation tasks, which incorporates supervised information and node replication for data augmentation to improve recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.95,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.75\n  }\n}\n```"], "6006a8e291e0111a1b6a21e2": ["```json\n{\n\t\"Summary\": \"The paper addresses the challenge of improving social recommendation systems by integrating high-order user relations using hypergraphs and graph convolutional networks. The proposed Multi-channel Hypergraph Convolutional Network (MHCN) constructs hypergraphs from triangular motifs to capture complex user relationships and integrates a self-supervised learning task to enhance user representation learning.\",\n\t\"Direct Inspiration\": {\n\t\t\"Inspired by\": [\"b3\"],\n\t\t\"Motivated by\": [\"b13\", \"b34\"]\n\t},\n\t\"Indirect Inspiration\": [\"b8\", \"b37\", \"b38\", \"b39\", \"b55\"],\n\t\"Other Inspiration\": [\"b51\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge addressed in the paper is the inability of existing GNN-based social recommendation models to capture complex high-order user relations beyond simple pairwise interactions.\",\n    \"Inspirations\": \"The paper is inspired to enhance social recommendation by integrating hypergraph modeling and graph convolutional networks to exploit high-order user relations.\"\n  },\n  \"Direct Inspiration\": {\n    \"Inspired by\": [\"b3\"],\n    \"Motivated by\": [\"b13\", \"b34\"]\n  },\n  \"Indirect Inspiration\": {\n    \"Following\": [\"b40\", \"b51\"],\n    \"We use\": [\"b50\"]\n  },\n  \"Other Inspiration\": {\n    \"Other relevant works\": [\"b8\", \"b17\", \"b37\", \"b38\", \"b39\", \"b55\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the limitation of existing GNN-based social recommendation models that only exploit simple pairwise user relations and ignore complex high-order relations. It proposes a Multi-channel Hypergraph Convolutional Network (MHCN) that utilizes hypergraphs to model high-order user relations. The model aggregates multiple user embeddings from different channels, each encoding a different type of high-order relation. Additionally, a self-supervised task is integrated into the training to maximize mutual information between user representations at different levels of the hypergraph structure.\",\n    \"Direct Inspiration\": {\n        \"b3\": 1,\n        \"b8\": 1,\n        \"b13\": 1,\n        \"b34\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b37\": 0.9,\n        \"b38\": 0.9,\n        \"b39\": 0.9,\n        \"b55\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b14\": 0.8,\n        \"b35\": 0.8,\n        \"b50\": 0.8,\n        \"b51\": 0.8\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of exploiting high-order user relations in social recommendation systems, which traditional GNNs-based models fail to capture. The proposed solution is a Multi-channel Hypergraph Convolutional Network (MHCN) that models complex high-order relations among users using hypergraphs. The MHCN constructs multiple hypergraphs based on different high-order motifs and integrates a self-supervised task to enhance the recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b37\": 0.85,\n    \"b38\": 0.85,\n    \"b39\": 0.85,\n    \"b55\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b13\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.75,\n    \"b14\": 0.75,\n    \"b35\": 0.75,\n    \"b51\": 0.75,\n    \"b59\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The paper addresses the limitation of GNNs-based social recommendation models which only exploit simple pairwise user relations and ignore high-order relations among users. The proposed solution is a Multi-channel Hypergraph Convolutional Network (MHCN) that models complex high-order relations among users using hypergraphs.\",\n    \"inspirations\": \"The paper takes inspiration from the success of GNNs in various areas, the concept of hypergraphs for modeling complex relations, and self-supervised learning techniques to enhance the training process.\"\n  },\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b37\": 1,\n    \"b39\": 1,\n    \"b55\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b13\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.7,\n    \"b38\": 0.7,\n    \"b51\": 0.6\n  }\n}\n```"], "61136afa5244ab9dcbd3a9ce": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of contrastive learning in unpaired image-to-image translation by generating more challenging negative samples. The authors propose a novel framework, NEGCUT, which includes a negative generator to produce instance-wise hard negative examples. The framework aims to enhance the quality of the translated images by maintaining fine-grained correspondence of structures and textures.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b51\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b58\": 0.7,\n    \"b34\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving contrastive learning for unpaired image-to-image translation by introducing a novel framework called NEGCUT, which focuses on generating instance-wise hard negative examples through adversarial learning. The method aims to enhance the performance of contrastive learning by producing more challenging negative samples that push the positive examples closer to the query examples.\",\n  \"Direct Inspiration\": [\"b39\", \"b36\"],\n  \"Indirect Inspiration\": [\"b58\", \"b30\", \"b32\"],\n  \"Other Inspiration\": [\"b43\", \"b51\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is improving the performance of contrastive learning for unpaired image-to-image translation by addressing the inadequacy of negative samples in existing methods. The authors propose a novel framework, NEGCUT, which generates instance-wise hard negative examples through adversarial learning to enhance the effectiveness of contrastive learning.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1.0,\n    \"b39\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b58\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.6,\n    \"b34\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of contrastive learning for unpaired image-to-image translation by generating harder negative examples. The proposed NEGCUT framework introduces instance-wise hard negative example generation through adversarial learning to enhance the quality of translated images by making the negative samples more challenging, thus pushing the encoder to learn more distinguishing representations.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b58\": 0.7,\n    \"b20\": 0.6,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving contrastive learning performance for unpaired image-to-image translation by generating hard negative examples. It introduces the NEGCUT framework, which employs a novel negative generator to produce instance-wise hard negative samples. The approach involves adversarial training between the encoder network and the negative generator to enhance the quality and consistency of the translated images.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b58\": 0.8,\n    \"b6\": 0.75,\n    \"b32\": 0.7,\n    \"b21\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.6\n  }\n}\n```"], "60d1419c91e011c16f0cb393": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in novelty detection, particularly the limitations of GAN-based approaches such as mode-dropping, instable training, and low discriminative ability. The proposed solution is a novel decoder-encoder framework that integrates a generative network, a contrastive network, and a mutual information estimator to improve the discriminative latent representation for novelty detection.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b5\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b2\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b6\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses three main challenges in GAN-based novelty detection: mode-dropping, unstable training, and low discriminative ability. It introduces a novel decoder-encoder framework that employs contrastive learning and a mutual information estimator to improve the discriminative power of latent features and stabilize training.\",\n    \"Direct Inspiration\": {\n        \"b0\": 0.9,\n        \"b5\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b2\": 0.7,\n        \"b3\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.6,\n        \"b6\": 0.6,\n        \"b8\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in novelty detection, particularly focusing on issues with GAN-based methods such as mode-dropping, unstable training, and low discriminative ability of latent features. The authors propose a novel decoder-encoder framework incorporating contrastive learning and mutual information estimation to overcome these challenges.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b5\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b3\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.7,\n        \"b4\": 0.7,\n        \"b6\": 0.6,\n        \"b7\": 0.6,\n        \"b8\": 0.6,\n        \"b9\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the problems of mode-dropping, unstable training, and low discriminative ability in GAN-based methods for novelty detection. The authors propose a novel decoder-encoder framework to address these issues by using contrastive learning and a mutual information estimator to learn discriminative latent features. Key inspirations include various GAN-based methods and self-supervised learning techniques, along with recent advancements in contrastive learning.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b5\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b2\": 0.7,\n    \"b9\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b4\": 0.55,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of mode-dropping, unstable training, and low discriminative ability in GAN-based novelty detection methods by proposing a novel decoder-encoder framework. This framework employs contrastive learning and a mutual information estimator to improve the discriminative power of latent features for distinguishing between normal and novelty samples.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b5\": 0.9,\n    \"b2\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```"], "60ff9e095244ab9dcb02272b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of selecting negative samples for contrastive learning in object-centric models. It proposes various negative sampling strategies to improve the performance of the Contrastive Structured World Model (C-SWM) in dynamic environments like Atari games.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b16\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b22\", \"b24\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b13\", \"b21\", \"b29\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving object-centric representations in dynamic environments using contrastive learning. The main contributions include introducing new negative sampling strategies to enhance prediction accuracy and extending the Atari datasets.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of selecting effective negative samples for contrastive learning in object-centric world models to improve prediction accuracy and representation quality. The authors propose and evaluate several negative sampling strategies, demonstrating their impact on performance in various environments.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b22\": 0.7,\n    \"b24\": 0.7,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b29\": 0.5,\n    \"b3\": 0.5,\n    \"b27\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving object-centric contrastive learning by optimizing negative sampling strategies to enhance performance in dynamic environments such as Atari games. It evaluates different heuristics for selecting negative samples and demonstrates their impact on prediction accuracy and representation quality.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b6\": 0.5,\n    \"b14\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively sampling negative examples for contrastive structured world models (C-SWMs) to improve prediction accuracy and object representation in dynamic environments. It proposes new heuristics for negative sampling and demonstrates their impact through experiments on grid-world and Atari datasets.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b28\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```"], "60d6992491e011839f53ce47": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of long-term time series forecasting by proposing Autoformer, a decomposition architecture that handles intricate temporal patterns and breaks the bottleneck of computational efficiency and information utilization. The Autoformer incorporates a novel Auto-Correlation mechanism inspired by stochastic process theory to discover dependencies based on series periodicity and aggregate similar sub-series from underlying periods.\",\n    \"Direct Inspiration\": {\n        \"b12\": 0.95,\n        \"b28\": 0.95\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.85,\n        \"b31\": 0.85,\n        \"b19\": 0.85\n    },\n    \"Other Inspiration\": {\n        \"b45\": 0.80,\n        \"b24\": 0.80,\n        \"b21\": 0.80\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of long-term time series forecasting by introducing a novel architecture called Autoformer. The primary challenges include handling intricate temporal patterns and breaking the bottleneck of computation efficiency and information utilization. The Autoformer utilizes a decomposition architecture and an Auto-Correlation mechanism to address these challenges, achieving state-of-the-art accuracy on six benchmarks.\",\n    \"Direct Inspiration\": {\n        \"b12\": 0.9,\n        \"b28\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b31\": 0.8,\n        \"b19\": 0.7,\n        \"b45\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.6,\n        \"b24\": 0.6,\n        \"b27\": 0.6,\n        \"b33\": 0.6,\n        \"b39\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of long-term time series forecasting by proposing a novel model called Autoformer. The model tackles two main issues: handling intricate temporal patterns and improving computation efficiency. The key inspirations include decomposition methods for time series and stochastic process theory, which are integrated into the Auto-Correlation mechanism used in Autoformer.\",\n    \"Direct Inspiration\": {\n        \"b12\": 0.9,\n        \"b28\": 0.9,\n        \"b5\": 0.8,\n        \"b31\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.7,\n        \"b24\": 0.6,\n        \"b21\": 0.6,\n        \"b45\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b33\": 0.5,\n        \"b39\": 0.5,\n        \"b11\": 0.4,\n        \"b15\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in long-term time series forecasting, specifically handling intricate temporal patterns and improving computational efficiency. The proposed Autoformer model incorporates a decomposition architecture and introduces an Auto-Correlation mechanism inspired by stochastic process theory to achieve state-of-the-art accuracy in long-term forecasting.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b19\": 0.7,\n    \"b31\": 0.7,\n    \"b39\": 0.6,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of long-term time series forecasting, particularly the difficulty in handling intricate temporal patterns and the computational inefficiency of existing Transformer-based models. The proposed solution, Autoformer, introduces a decomposition architecture and an Auto-Correlation mechanism to improve both accuracy and efficiency in long-term forecasting tasks.\",\n    \"Direct Inspiration\": {\n        \"b12\": 0.9,\n        \"b28\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b39\": 0.75,\n        \"b24\": 0.7,\n        \"b21\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b31\": 0.6,\n        \"b45\": 0.6\n    }\n}\n```"], "5ff68477d4150a363cbdf18c": ["```json\n{\n  \"Summary\": \"The paper primarily addresses the challenges of Automatic Speech Recognition (ASR) for low-resource languages, which lack transcribed speech data, pronunciation dictionaries, and language knowledge. It focuses on the application and improvement of neural network-based acoustic models and end-to-end structures to enhance the performance of low-resource speech recognition.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b19\": 0.85,\n    \"b44\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.75,\n    \"b17\": 0.75,\n    \"b22\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b45\": 0.7,\n    \"b48\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of Automatic Speech Recognition (ASR) for low-resource languages, where there is a lack of transcribed speech data, pronunciation dictionaries, and language knowledge. The paper proposes using advanced neural network models such as Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN), and end-to-end structures like Connectionist Temporal Classification (CTC) and attention-based models to improve ASR performance under these constraints.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b16\": 0.9,\n    \"b19\": 0.8,\n    \"b21\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.6,\n    \"b27\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.4,\n    \"b44\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper revolve around improving Automatic Speech Recognition (ASR) for low-resource languages, which lack extensive transcribed speech data, pronunciation dictionaries, and other linguistic resources. The paper proposes advancements in acoustic models using neural networks, specifically RNNs and CNNs, and end-to-end models that integrate multiple traditional modules into a single network for joint training. These methods aim to enhance ASR performance in low-resource scenarios, addressing issues like data scarcity, model complexity, and training efficiency.\",\n  \"Direct Inspiration\": [\"b12\", \"b14\", \"b16\", \"b22\", \"b40\", \"b42\"],\n  \"Indirect Inspiration\": [\"b13\", \"b19\", \"b21\", \"b32\", \"b44\", \"b45\", \"b49\", \"b50\"],\n  \"Other Inspiration\": [\"b25\", \"b27\", \"b37\", \"b46\", \"b55\", \"b57\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of Automatic Speech Recognition (ASR) for low-resource languages, focusing on the development and improvement of neural network-based acoustic models and end-to-end structures. It highlights the scarcity of transcribed speech data, pronunciation dictionaries, language knowledge, and text annotation for many dialects and minority languages as the primary obstacle. The paper proposes the use of advanced deep learning techniques such as RNN, CNN, and end-to-end models like CTC and attention-based models to overcome these challenges.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b12\": 0.8,\n    \"b15\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b19\": 0.7,\n    \"b21\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b17\": 0.6,\n    \"b43\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed by the paper is the difficulty of achieving effective Automatic Speech Recognition (ASR) for low-resource languages, which lack sufficient transcribed speech data, pronunciation dictionaries, and language knowledge. The paper reviews various neural network-based acoustic models and end-to-end approaches to improve ASR performance under these conditions.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b14\": 0.9,\n    \"b16\": 0.9,\n    \"b19\": 0.9,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b21\": 0.8,\n    \"b30\": 0.8,\n    \"b34\": 0.8,\n    \"b42\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.7,\n    \"b50\": 0.7,\n    \"b56\": 0.7,\n    \"b57\": 0.7,\n    \"b59\": 0.7\n  }\n}\n```"], "60d3d1c891e0112ca5d18569": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the instability of one-shot voice conversion for unseen speakers due to unreliable speaker embedding extracted from a single utterance. The proposed algorithm integrates residual networks (ResNet) and squeeze-and-excitation (SE) networks to extract discriminative frame-level speaker information, followed by an attention mechanism and a statistic pooling layer to generate robust utterance-level speaker embeddings.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b28\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.75,\n    \"b19\": 0.8,\n    \"b20\": 0.7,\n    \"b26\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the instability of one-shot voice conversion approaches for unseen speakers due to unreliable speaker embedding extraction from one utterance. The proposed algorithm is a deep discriminative speaker encoder integrating ResNet and squeeze-and-excitation (SE) networks, along with an attention mechanism, to improve the robustness of one-shot voice conversion by extracting more reliable speaker embeddings.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.8,\n    \"b25\": 0.8,\n    \"b26\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the instability of one-shot voice conversion systems for unseen speakers. The proposed solution is a deep discriminative speaker encoder that integrates a residual network (ResNet) and a squeeze-and-excitation (SE) network to extract discriminative frame-level speaker information, and utilizes an attention mechanism to weight this information appropriately. The method aims to improve the robustness of one-shot voice conversion in terms of speech quality and speaker similarity.\",\n  \"Direct Inspiration\": [\"b27\", \"b28\"],\n  \"Indirect Inspiration\": [\"b19\", \"b24\"],\n  \"Other Inspiration\": [\"b17\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the robustness of one-shot voice conversion for unseen speakers. It proposes a deep discriminative speaker encoder (DDSE) that integrates a residual network (ResNet) and squeeze-and-excitation (SE) network, combined with an attention mechanism and a statistics pooling layer to extract and emphasize speaker-specific features effectively.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b28\": 0.9,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b26\": 0.7,\n    \"b16\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the robustness of one-shot voice conversion for unseen speakers. The proposed solution is a deep discriminative speaker encoder (DDSE) that integrates a residual network (ResNet), squeeze-and-excitation (SE) network, and attention mechanism to extract and emphasize speaker-related information effectively.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b18\": 0.9,\n    \"b19\": 1.0,\n    \"b27\": 0.9,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b24\": 0.7,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```"], "60af638091e0116560681699": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving reasonable recognition performance for low-resource languages in end-to-end (E2E) automatic speech recognition (ASR) models. It proposes two novel algorithms, MetaAdapter and SimAdapter, to enhance cross-lingual ASR by leveraging parameter-efficient fine-tuning techniques. The algorithms aim to exploit the similarities between different languages and prevent overfitting by focusing on adapter modules.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b3\": 0.9,\n    \"b12\": 0.85,\n    \"b11\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b5\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b13\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving effective automatic speech recognition (ASR) for low-resource languages using end-to-end (E2E) models. The authors propose novel algorithms, MetaAdapter and SimAdapter, to improve cross-lingual ASR by leveraging knowledge from rich-resource languages. The paper focuses on parameter-efficient fine-tuning using adapter modules to mitigate overfitting and enhance performance.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b10\": 0.85,\n    \"b11\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.75,\n    \"b5\": 0.75,\n    \"b9\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include achieving reasonable recognition performance for low-resource languages in end-to-end (E2E) automatic speech recognition (ASR) models. The paper proposes two novel algorithms: MetaAdapter and SimAdapter, which focus on parameter-efficient fine-tuning to prevent overfitting. The paper aims to leverage the similarity between source and target languages using meta-learning and attention mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b12\": 1.0,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.6,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b14\": 0.5,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving automatic speech recognition (ASR) for low-resource languages using transfer learning methods. It proposes two novel algorithms, MetaAdapter and SimAdapter, to enhance cross-lingual ASR performance by leveraging similarities between source and target languages and efficiently fine-tuning models with adapter modules.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b12\": 0.90,\n    \"b11\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.80,\n    \"b7\": 0.75,\n    \"b9\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b13\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving cross-lingual ASR performance for low-resource languages by proposing two novel algorithms, MetaAdapter and SimAdapter, that leverage adapter modules for parameter-efficient adaptation.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b8\": 0.9,\n    \"b12\": 0.9,\n    \"b11\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b5\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```"], "609678d2e4510cd7c80db7fe": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of Automatic Speech Recognition (ASR) for low-resource languages, specifically focusing on Persian. It proposes the use of transfer learning to improve Persian ASR by first training a neural network on a high-resource language (English) and then fine-tuning it on Persian data.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b18\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b20\": 0.8,\n    \"b17\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is improving Automatic Speech Recognition (ASR) for low-resource languages, specifically Persian, which has not been sufficiently studied. The paper proposes a transfer learning approach where a neural network is first trained on a high-resource language (English) and then fine-tuned on a low-resource language (Persian). The key inspirations include the need to enhance Persian ASR and the effectiveness of transfer learning in ASR.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Automatic Speech Recognition (ASR) for low-resource languages, specifically Persian. It proposes using transfer learning to improve ASR performance by first training a model on a high-resource language (English) and then fine-tuning it on Persian data. The authors highlight the lack of sufficient research and data for Persian ASR and aim to alleviate this gap by leveraging transfer learning techniques.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.9,\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an Automatic Speech Recognition (ASR) system for the Persian language, which is considered low-resource. The authors propose using transfer learning from a high-resource language (English) to improve Persian ASR performance. They employ a fully convolutional neural network and use Connectionist Temporal Classification (CTC) loss for training. The evaluation of the method is conducted using the Persian FarsDat corpus.\",\n  \"Direct Inspiration\": [\"b5\", \"b18\"],\n  \"Indirect Inspiration\": [\"b14\", \"b19\", \"b20\"],\n  \"Other Inspiration\": [\"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Automatic Speech Recognition (ASR) for low-resource languages, specifically focusing on Persian. It proposes using transfer learning to improve Persian ASR by leveraging a pre-trained model on a high-resource language (English). The proposed method involves training a fully convolutional neural network on the English LibriSpeech corpus and fine-tuning it on the Persian FarsDat corpus. The main contributions are improving ASR accuracy and reducing training time for low-resource languages using transfer learning.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.5\n  }\n}\n```"], "5ff44fff91e01130648dc553": ["```json\n{\n  \"Summary\": \"The paper addresses the limitations of current Graph Neural Networks (GNNs) in dealing with real-world networks that exhibit both assortative and disassortative properties. It proposes a novel Frequency Adaptation Graph Convolutional Network (FAGCN) that adaptively utilizes both low-frequency and high-frequency signals to improve the performance across different types of networks.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b9\": 0.8,\n    \"b25\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the fundamental challenge of whether GNNs comprehensively exploit the information in node features when learning node representation. It highlights the limitations of using low-frequency information in GNNs, particularly in disassortative networks, and introduces FAGCN, which adaptively aggregates low-frequency and high-frequency signals to improve performance across different types of networks.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b12\": 0.85,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b14\": 0.6,\n    \"b9\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b24\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the limitations of current GNNs in handling both assortative and disassortative networks, particularly the over-reliance on low-frequency signals. The proposed algorithm, FAGCN, aims to adaptively aggregate both low-frequency and high-frequency signals to better suit diverse network types. The paper is inspired by the need to address the over-smoothing problem and the inadequacy of low-frequency information alone in certain networks.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b9\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b7\": 0.5,\n    \"b13\": 0.5,\n    \"b23\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of current Graph Neural Networks (GNNs) in handling disassortative networks by proposing the Frequency Adaptation Graph Convolutional Network (FAGCN). The primary challenges identified are the over-reliance on low-frequency signals which leads to over-smoothing and poor performance on disassortative networks. The proposed FAGCN adaptively integrates low-frequency, high-frequency, and raw signals to improve performance across different network types.\",\n  \"Direct Inspiration\": [\n    \"b9\",\n    \"b22\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b14\",\n    \"b19\",\n    \"b20\"\n  ],\n  \"Other Inspiration\": [\n    \"b13\",\n    \"b23\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing graph neural networks (GNNs) that primarily rely on low-frequency information which may not be suitable for disassortative networks. It proposes a novel model, FAGCN, which adaptively integrates low-frequency and high-frequency signals to improve performance across different types of networks.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.95,\n    \"b13\": 0.9,\n    \"b22\": 0.9,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b9\": 0.75,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```"], "602a443791e011a1a44cbdae": ["```json\n{\n  \"Summary\": \"The paper addresses two key limitations of graph neural networks (GNNs): oversmoothing and poor performance on heterophily graphs. It proposes a robust, generalized model, GGCN, that incorporates signed messages and degree corrections, along with a decaying combination of node representations, to address these issues.\",\n  \"Direct Inspiration\": [\"b2\", \"b7\", \"b24\"],\n  \"Indirect Inspiration\": [\"b10\", \"b13\", \"b26\", \"b33\"],\n  \"Other Inspiration\": [\"b15\", \"b28\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in Graph Neural Networks (GNNs): the \\\"oversmoothing\\\" problem and poor performance on heterophily graphs. The authors propose a new model, GGCN, to tackle these issues by analyzing the dynamics around oversmoothing and integrating mechanisms like signed messages and degree corrections.\",\n  \"Direct Inspiration\": [\"b2\"],\n  \"Indirect Inspiration\": [\"b7\", \"b24\"],\n  \"Other Inspiration\": [\"b13\", \"b26\", \"b33\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in Graph Neural Networks (GNNs): the oversmoothing problem and poor performance on heterophily graphs. The authors propose a new model, GGCN, which incorporates signed messages and degree corrections to improve performance under these conditions. They also provide a theoretical analysis to understand the dynamics of node representations and the factors influencing oversmoothing and heterophily.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary limitations of Graph Neural Networks (GNNs): the oversmoothing problem and their poor performance on heterophily graphs. The authors propose a robust, generalized GCN model called GGCN that incorporates signed messages and degree corrections to tackle these issues. The paper provides theoretical analysis and empirical results demonstrating the effectiveness of GGCN in improving performance on heterophily datasets and preventing oversmoothing.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b15\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b13\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges with Graph Neural Networks (GNNs): the oversmoothing problem and poor performance on heterophily graphs. The proposed solution involves a theoretically-grounded analysis of these issues and presents a robust model, GGCN, which incorporates signed messages and degree corrections to overcome these limitations.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b26\": 0.9,\n    \"b15\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b24\": 0.6,\n    \"b13\": 0.6,\n    \"b28\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.4,\n    \"b17\": 0.4\n  }\n}\n```"], "606c48e691e0114248cd01a0": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in graph representation learning, particularly in non-homophilous settings where connected nodes may not share the same class label. It proposes new non-homophilous graph datasets that are larger and more diverse than those currently used, and introduces a new metric to better capture homophily. The paper benchmarks simple methods, general GNNs, and non-homophilous GNNs on these datasets to understand their performance in diverse settings.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b43\": 1.0,\n    \"b59\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b42\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.8,\n    \"b23\": 0.8,\n    \"b52\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of current non-homophilous graph datasets and proposes new datasets with improved properties. It introduces a new metric to measure homophily, re-evaluates strong simple methods for graph learning, and benchmarks these along with existing GNNs on the proposed datasets.\",\n  \"Direct Inspiration\": [\n    \"b43\",\n    \"b59\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b10\",\n    \"b42\"\n  ],\n  \"Other Inspiration\": [\n    \"b1\",\n    \"b25\",\n    \"b3\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of machine learning on non-homophilous graph data, proposing new datasets and metrics to better capture and evaluate graph learning methods in such settings. It critiques existing datasets for their limitations and introduces a new homophily metric to better detect the presence or absence of homophily in graphs. The paper also benchmarks several methods on the proposed datasets to evaluate their performance in diverse settings.\",\n    \"Direct Inspiration\": {\n        \"b43\": 1,\n        \"b59\": 1,\n        \"b10\": 0.9,\n        \"b42\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8,\n        \"b3\": 0.8,\n        \"b52\": 0.7,\n        \"b23\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b25\": 0.6,\n        \"b6\": 0.6,\n        \"b32\": 0.5,\n        \"b35\": 0.5,\n        \"b9\": 0.5,\n        \"b54\": 0.5,\n        \"b58\": 0.5,\n        \"b21\": 0.4,\n        \"b29\": 0.4,\n        \"b51\": 0.4,\n        \"b7\": 0.4,\n        \"b13\": 0.4,\n        \"b31\": 0.4,\n        \"b20\": 0.3,\n        \"b40\": 0.3,\n        \"b4\": 0.3,\n        \"b11\": 0.3,\n        \"b56\": 0.2,\n        \"b57\": 0.2,\n        \"b47\": 0.2,\n        \"b16\": 0.2,\n        \"b0\": 0.2,\n        \"b5\": 0.2,\n        \"b14\": 0.2,\n        \"b18\": 0.2,\n        \"b39\": 0.2,\n        \"b38\": 0.2,\n        \"b44\": 0.2,\n        \"b15\": 0.2,\n        \"b46\": 0.2,\n        \"b8\": 0.2,\n        \"b41\": 0.2,\n        \"b22\": 0.1,\n        \"b30\": 0.1,\n        \"b53\": 0.1,\n        \"b45\": 0.1,\n        \"b3\": 0.1\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of evaluating graph neural networks (GNNs) in non-homophilous settings by proposing new datasets and a novel metric for measuring homophily. The authors aim to improve the empirical evaluation of GNNs in diverse settings, overcoming limitations of previous non-homophilous graph datasets.\",\n  \"Direct Inspiration\": {\n    \"b43\": 1,\n    \"b59\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b42\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.8,\n    \"b52\": 0.8,\n    \"b23\": 0.8,\n    \"b3\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of evaluating graph neural networks (GNNs) in non-homophilous settings. It critiques existing non-homophilous datasets for being limited in size and scope and proposes new, larger datasets that better represent real-world applications. Additionally, it introduces a new metric for measuring homophily and benchmarks various GNNs and simple methods on the proposed datasets.\",\n  \"Direct Inspiration\": [\"b43\", \"b59\", \"b10\"],\n  \"Indirect Inspiration\": [\"b3\", \"b42\", \"b52\"],\n  \"Other Inspiration\": [\"b1\", \"b23\"]\n}\n```"], "61a8843a6750f87bf87020bf": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the limitations of Graph Neural Networks (GNNs) in capturing long-range dependencies due to oversmoothing and the limited receptive field of GNNs. The proposed solution, Graph Transformer (GraphTrans), integrates a Transformer subnetwork with a standard GNN to improve long-range dependency learning and achieve state-of-the-art results in graph classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b6\": 1.0,\n    \"b10\": 1.0,\n    \"b19\": 1.0,\n    \"b23\": 1.0,\n    \"b26\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b4\": 0.8,\n    \"b20\": 0.8,\n    \"b22\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b14\": 0.7,\n    \"b29\": 0.7,\n    \"b24\": 0.7,\n    \"b31\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge discussed in the paper is the limitation of GNNs in capturing long-range dependencies due to oversmoothing and limited receptive fields. The novel approach proposed by the authors involves integrating a Transformer subnetwork with a GNN layer stack, termed GraphTrans, to improve the learning of long-range dependencies. Direct inspiration is taken from attention mechanisms in computer vision, which have shown success in modeling long-range dependencies.\",\n    \"Direct Inspiration\": [\"b3\", \"b6\"],\n    \"Indirect Inspiration\": [\"b10\", \"b23\", \"b37\"],\n    \"Other Inspiration\": [\"b24\", \"b19\", \"b26\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The main challenge is the limitation of GNNs in capturing long-range dependencies due to oversmoothing and limited receptive fields. The proposed solution, GraphTrans, combines GNNs for local representation learning with a Transformer subnetwork for global reasoning.\",\n    \"inspirations\": \"The method is inspired by attention mechanisms in computer vision and Transformer applications in text classification.\"\n  },\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b3\": 1.0,\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.9,\n    \"b37\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.7,\n    \"b14\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of graph neural networks (GNNs) in learning long-range dependencies, which is difficult due to issues like oversmoothing with deep layers. The authors propose a novel architecture called Graph Transformer (GraphTrans) that combines a GNN subnetwork for local neighborhood learning with a Transformer subnetwork for global reasoning and long-range dependency modeling. The method is inspired by attention mechanisms in computer vision and NLP, and it uses a special '<CLS>' token for graph-level readout.\",\n  \"Direct Inspiration\": [\"b10\", \"b3\", \"b6\"],\n  \"Indirect Inspiration\": [\"b37\", \"b23\", \"b11\"],\n  \"Other Inspiration\": [\"b19\", \"b26\", \"b29\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of GNNs' limited ability to capture long-range dependencies due to oversmoothing when increasing depth. The proposed solution, GraphTrans, combines GNN layers for local representation learning with a Transformer subnetwork for global pairwise node interactions, inspired by computer vision methods.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b3\": 0.9,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.8,\n    \"b23\": 0.8,\n    \"b26\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b29\": 0.6,\n    \"b1\": 0.5\n  }\n}\n```"], "614076d45244ab9dcbfe64ef": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an Automatic Speech Recognition (ASR) system for the low-resource language Sanskrit. The authors propose a solution using a CTC-based end-to-end system with CNN and bidirectional GRU architecture, employing data augmentation techniques such as SpecAugment.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b20\": 0.9,\n    \"b21\": 0.8,\n    \"b26\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.6,\n    \"b19\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.4,\n    \"b2\": 0.4,\n    \"b3\": 0.4,\n    \"b4\": 0.4,\n    \"b22\": 0.4,\n    \"b23\": 0.4,\n    \"b24\": 0.4,\n    \"b25\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an Automatic Speech Recognition (ASR) system for low-resource languages, specifically Sanskrit. It proposes an end-to-end speech recognition system based on Connectionist Temporal Classification (CTC) and a neural network architecture with residual convolutional neural networks (CNN) and bidirectional gated recurrent units (GRU). Feature augmentation using SpecAugment is employed to increase the amount of training data.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b20\": 1,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b26\": 0.6,\n    \"b27\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b5\": 0.5,\n    \"b18\": 0.4,\n    \"b22\": 0.4,\n    \"b23\": 0.4,\n    \"b24\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of building a Sanskrit ASR system, particularly focusing on data scarcity and the limitations of conventional ASR systems. The proposed solution involves an end-to-end system using CTC-based architecture with residual CNN and bidirectional GRU, and employs data augmentation techniques like SpecAugment.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.9,\n    \"b21\": 0.8,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b29\": 0.6,\n    \"b31\": 0.5,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on developing an Automatic Speech Recognition (ASR) system for the Sanskrit language, addressing the challenges posed by low-resource languages and the limitations of conventional ASR systems. The authors propose an architecture based on residual convolutional neural networks (CNN) and bidirectional gated recurrent units (GRU), using a feature augmentation technique called SpecAugment to increase the amount of data available for training.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b20\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b26\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b1\": 0.5,\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the development of an Automatic Speech Recognition (ASR) system for the low-resource language Sanskrit, leveraging end-to-end trained systems over conventional ASR systems. The proposed architecture utilizes a CTC-based scheme with residual CNNs and bidirectional GRUs, incorporating data augmentation techniques such as SpecAugment to overcome the data scarcity issue.\",\n  \"Direct Inspiration\": [\"b14\", \"b20\"],\n  \"Indirect Inspiration\": [\"b21\", \"b26\", \"b27\", \"b28\"],\n  \"Other Inspiration\": [\"b5\", \"b29\"]\n}\n```"], "60e2d9135244ab9dcbf71aed": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing End-to-End (E2E) Automatic Speech Recognition (ASR) systems for four less-resourced Ethiopian languages (Amharic, Oromo, Tigrigna, and Wolaytta). The novelty lies in investigating the use of different modeling units (characters and phones) and leveraging multilingual data to improve ASR performance, particularly for languages with limited resources.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b14\": 0.9,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b18\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b15\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper involve the application of End-to-End (E2E) ASR systems to less-resourced Ethiopian languages, focusing on overcoming the scarcity of training data. The authors propose a multilingual E2E approach, utilizing speech data from related and less-related languages, and investigate different modeling units (characters and phones) to improve ASR performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b14\": 0.8,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b18\": 0.7,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing End-to-End (E2E) Automatic Speech Recognition (ASR) systems for less-resourced Ethiopian languages. It explores the use of different modeling units (characters and phones) and the utilization of multilingual data to enhance the performance of these systems. The primary focus is on leveraging data from related and even less related languages to mitigate the issue of training data scarcity.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b22\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b1\": 0.7,\n    \"b2\": 0.6,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b7\": 0.5,\n    \"b8\": 0.5,\n    \"b9\": 0.5,\n    \"b10\": 0.5,\n    \"b11\": 0.5,\n    \"b12\": 0.5,\n    \"b13\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing efficient End-to-End (E2E) Automatic Speech Recognition (ASR) systems for four less-resourced Ethiopian languages. The primary algorithms and methods proposed include multilingual E2E ASR systems using character and phone-based units and leveraging speech data from related languages to combat data scarcity.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 0.9,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b14\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing End-to-End (E2E) Automatic Speech Recognition (ASR) systems for four less-resourced Ethiopian languages: Amharic, Oromo, Tigrigna, and Wolaytta. The main inspiration comes from the success of hybrid HMM-DNN systems over HMM-GMM systems and the potential of E2E frameworks despite their data-intensive nature. The authors aim to improve E2E ASR performance by leveraging multilingual (ML) data, including data from related languages and the GlobalPhone corpus.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.6,\n    \"b18\": 0.6,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```"], "60800be191e011772654f86b": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the limitation of traditional collaborative filtering (CF) approaches in multimedia recommendation systems, which often fail to comprehensively capture item-item relationships due to their focus on user-item interactions. The proposed solution, LATTICE, aims to mine the latent structures beneath multimodal features to better model these item relationships. The algorithm introduces a novel modality-aware structure learning layer, graph convolutional layers to inject high-order item relationships, and a flexible integration with downstream CF methods.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b34\": 0.9,\n    \"b19\": 0.85,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.7,\n    \"b37\": 0.7,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5,\n    \"b28\": 0.5,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing multimedia recommendation systems by explicitly modeling item relationships using multimodal features. The proposed LATTICE model aims to mine latent structures from multimodal features, integrating them with traditional collaborative filtering methods to improve item representation and recommendation accuracy. The key components of LATTICE include modality-aware graph structure learning, graph convolutional layers, and combination with downstream collaborative filtering methods.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b19\": 1,\n    \"b33\": 1,\n    \"b34\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b36\": 0.8,\n    \"b37\": 0.8,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.6,\n    \"b41\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of effectively utilizing multimodal features (such as images, texts, and videos) in recommender systems to discover comprehensive candidate items. The novel algorithm, LATTICE, learns modality-aware item structures from multimodal features and integrates these structures to form latent multimodal item graphs. These graphs are then used in graph convolutions to enhance item embeddings, which are combined with Collaborative Filtering (CF) methods.\",\n    \"Direct Inspiration\": {\n        \"b34\": 1,\n        \"b37\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.8,\n        \"b29\": 0.7,\n        \"b36\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.5,\n        \"b22\": 0.5,\n        \"b2\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of traditional collaborative filtering (CF) in multimedia recommendation systems by proposing a novel model named LATTICE. This model mines latent structures from multimodal features to learn better item representations, integrating these with collaborative signals to improve recommendation accuracy. The main challenges outlined include the inability of traditional models to capture genuine item-item relations and the cold-start problem.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1, \n    \"b19\": 1,\n    \"b33\": 1,\n    \"b34\": 1,\n    \"b36\": 1,\n    \"b37\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b39\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b22\": 0.6,\n    \"b2\": 0.6,\n    \"b31\": 0.6,\n    \"b0\": 0.6,\n    \"b25\": 0.6,\n    \"b35\": 0.6,\n    \"b20\": 0.6,\n    \"b4\": 0.6,\n    \"b28\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the need for recommender systems to comprehensively discover candidate items by not only leveraging collaborative filtering but also by explicitly modeling item relationships through multimodal features. The proposed algorithm, LATTICE, introduces a novel framework that mines latent structures from multimodal features and integrates them with collaborative signals to enhance item recommendations.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b19\": 1,\n    \"b33\": 1,\n    \"b34\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b37\": 0.9,\n    \"b36\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.8\n  }\n}\n```"], "6180ac445244ab9dcb793def": ["```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in multimedia recommendation: (1) the lack of explicit modeling of item-item relationships, particularly semantic relations, and (2) the inadequacy of fine-grained multimodal fusion in existing methods. The proposed MICRO model introduces a novel method for mining latent semantic item-item relationships from multimodal features and performing fine-grained multimodal fusion to enhance item representations. The model comprises four key components: a modality-aware structure learning layer, graph convolutions on learned modality-aware graphs, a multimodal contrastive framework, and the integration of enhanced item representations into collaborative filtering models.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b8\": 1.0,\n    \"b12\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of multimedia recommendation systems, focusing on explicitly modeling item-item relationships and conducting fine-grained multimodal fusion. The proposed method, MICRO, consists of learning modality-aware item structures, performing graph convolutions, and applying a multimodal contrastive framework for self-supervised learning.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b9\": 0.9,\n    \"b12\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b14\": 0.7,\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b18\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b21\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of information overload in personalized recommender systems by proposing a novel method, MICRO, to mine latent semantic item-item relationships from multimodal features and perform fine-grained multimodal fusion. Two main challenges highlighted are the failure of previous methods to explicitly model item-item relationships and the lack of fine-grained multimodal fusion. The proposed method involves four key components: modality-aware structure learning, graph convolutions, a multimodal contrastive framework, and enhanced item representations infusion.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b12\": 0.9,\n    \"b13\": 0.9,\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b14\": 0.7,\n    \"b19\": 0.7,\n    \"b11\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges outlined in the paper include the inability of existing multimedia recommendation methods to explicitly model item-item relationships and the lack of fine-grained multimodal fusion. The proposed method, MICRO, aims to mine latent semantic item-item relationships from multimodal features and conduct fine-grained multimodal fusion. The approach involves modality-aware structure learning, graph convolutions, and a contrastive multimodal fusion framework.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b8\": 1,\n    \"b9\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.9,\n    \"b12\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.8,\n    \"b16\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of information overload in personalized recommender systems by introducing a novel method called MICRO. This method focuses on mining latent semantic item-item relationships and conducting fine-grained multimodal fusion to improve item representations and recommendations. The core contributions include learning modality-aware item structures, performing graph convolutions, and employing a contrastive multimodal fusion framework.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b8\": 1.0,\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b14\": 0.7\n  }\n}\n```"], "611106eb91e0117e7f3cdab0": ["```json\n{\n  \"Summary\": \"The paper addresses the problem of inclusion victims in inclusive last-level caches (LLCs) in chip-multiprocessors (CMPs). The proposed solution, Zero Inclusion Victim (ZIV) LLC, aims to eliminate inclusion victims while retaining the benefits of inclusive LLCs, such as simplified coherence protocols. The primary contribution is a novel LLC design that employs a minimal global replacement policy to avoid generating inclusion victims, thereby improving performance and reducing cache misses.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b10\": 0.75,\n    \"b15\": 0.75,\n    \"b22\": 0.75,\n    \"b28\": 0.75,\n    \"b29\": 0.75,\n    \"b37\": 0.75,\n    \"b38\": 0.75,\n    \"b43\": 0.75,\n    \"b50\": 0.75,\n    \"b53\": 0.75,\n    \"b61\": 0.75,\n    \"b64\": 0.75\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges associated with inclusive LLC designs, particularly inclusion victims, which lead to performance degradation, security vulnerabilities, and limitations on cache sizes. The authors propose a novel inclusive LLC design, the Zero Inclusion Victim (ZIV) LLC, that eliminates inclusion victims while maintaining the advantages of an inclusive LLC.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b63\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b59\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.65,\n    \"b45\": 0.6,\n    \"b6\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges associated with inclusive last-level caches (LLCs) in multi-core processors, specifically the issue of inclusion victims. The authors propose the Zero Inclusion Victim (ZIV) LLC, a novel inclusive LLC design that eliminates inclusion victims while retaining the advantages of inclusive LLCs. This design involves a global victim selection scheme to avoid inclusion victims and supports larger mid-level caches while providing performance close to non-inclusive LLCs.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b63\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Performance degradation due to live inclusion victims in inclusive LLCs.\",\n      \"Cross-core timing side-channel attacks facilitated by inclusion victims.\",\n      \"Limitations on mid-level cache size due to inclusion victims.\"\n    ],\n    \"proposed_solution\": \"Zero Inclusion Victim (ZIV) LLC design to eliminate inclusion victims while retaining the advantages of inclusive LLCs.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b19\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b3\", \"b6\", \"b17\", \"b59\", \"b63\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b9\", \"b10\", \"b15\", \"b22\", \"b28\", \"b29\", \"b37\", \"b38\", \"b43\", \"b50\", \"b53\", \"b61\", \"b64\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of inclusion victims in inclusive last-level caches (LLCs) of multiprocessors, which lead to performance degradation and security vulnerabilities. Motivated by the need to eliminate inclusion victims while retaining the benefits of inclusive LLCs, the paper proposes the Zero Inclusion Victim (ZIV) LLC design. This novel approach ensures no inclusion victims by allowing global victim selection only when necessary, thus maintaining the advantages of inclusive LLCs and improving performance and security.\",\n    \"Direct Inspiration\": {\n        \"b19\": 0.9,\n        \"b59\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b63\": 0.8,\n        \"b9\": 0.7,\n        \"b23\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b45\": 0.6,\n        \"b46\": 0.55,\n        \"b11\": 0.5\n    }\n}\n```"], "60d53e1b91e01153881e8486": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the computational inefficiency and scalability issues of traditional shallow embedding methods for large knowledge graphs (KGs). Inspired by subword embeddings used in NLP, the authors propose NodePiece, an anchor-based approach to learn a fixed-size vocabulary for KGs, which reduces parameter complexity, increases generalization, and effectively represents new unseen entities.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b36\": 0.9,\n    \"b35\": 0.9,\n    \"b9\": 0.9,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b22\": 0.8,\n    \"b27\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.7,\n    \"b28\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient and scalable representation learning for large knowledge graphs (KGs) by proposing a novel method called NodePiece. NodePiece leverages anchor-based tokenization inspired by subword embeddings in NLP to reduce parameter complexity, increase generalization, and handle out-of-sample entities effectively. The core contributions involve a fixed-size vocabulary of anchors and relation types, and an encoder function that maps tokenized nodes to embeddings, showing competitive results across various KG tasks.\",\n  \"Direct Inspiration\": [\"b4\", \"b35\", \"b36\"],\n  \"Indirect Inspiration\": [\"b18\", \"b33\", \"b23\"],\n  \"Other Inspiration\": [\"b37\", \"b38\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scalable representation learning for large knowledge graphs (KGs) by proposing a novel approach, NodePiece, which reduces parameter complexity through anchor-based tokenization inspired by subword embeddings in NLP. This approach allows for efficient and generalized embeddings of nodes using a fixed-size vocabulary, tackling issues such as large memory requirements and out-of-sample representation learning.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b9\": 1,\n    \"b22\": 1,\n    \"b27\": 1,\n    \"b35\": 1,\n    \"b36\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b29\": 0.8,\n    \"b33\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b2\": 0.7,\n    \"b7\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b21\": 0.7,\n    \"b23\": 0.7,\n    \"b25\": 0.7,\n    \"b26\": 0.7,\n    \"b28\": 0.7,\n    \"b30\": 0.7,\n    \"b31\": 0.7,\n    \"b32\": 0.7,\n    \"b34\": 0.7,\n    \"b37\": 0.7,\n    \"b40\": 0.7,\n    \"b41\": 0.7,\n    \"b42\": 0.7,\n    \"b44\": 0.7,\n    \"b45\": 0.7,\n    \"b46\": 0.7,\n    \"b47\": 0.7,\n    \"b48\": 0.7,\n    \"b49\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing parameter complexity in representation learning tasks on large knowledge graphs (KGs) by introducing NodePiece, an anchor-based tokenization approach inspired by subword embeddings in NLP. NodePiece constructs a fixed-size vocabulary to represent nodes, leveraging anchors and relations to create combinatorial sequences, thus enabling efficient and scalable node embeddings.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b36\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b27\": 0.7,\n    \"b9\": 0.6,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b37\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the computational difficulty of training on large knowledge graphs (KGs) due to the parameterization of each unique atom in the graph. The proposed algorithm, NodePiece, draws inspiration from subword embeddings in NLP to tokenize entities in large graphs, reducing parameter complexity, increasing generalization, and handling unseen entities using a fixed vocabulary. NodePiece relies on anchor-based tokenization and a proper encoder function to achieve these goals.\",\n  \"Direct Inspiration\": [\"b4\", \"b36\", \"b35\"],\n  \"Indirect Inspiration\": [\"b18\", \"b22\", \"b27\"],\n  \"Other Inspiration\": [\"b37\", \"b9\", \"b29\"]\n}\n```"], "6173f1c391e0118698c04c2b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of synthesizing diversified and realistic talking faces from audio, specifically focusing on incorporating talking styles into the synthesis process. It proposes a two-stage framework involving a latent-style-fusion (LSF) model for 3D talking face synthesis and a photorealistic rendering stage.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b35\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b13\": 0.6,\n    \"b14\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of synthesizing stylized talking faces by incorporating different talking styles into audio-driven talking face synthesis. The authors propose a two-stage framework that constructs style codes from video clips and uses a latent-style-fusion (LSF) model to synthesize 3D talking faces, which are then rendered photo-realistically. This method aims to overcome limitations of previous works that assume each identity has only one talking style and require substantial synchronized audio-visual data for each identity.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b26\": 0.8,\n    \"b11\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7,\n    \"b16\": 0.7,\n    \"b33\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of synthesizing stylized talking faces by incorporating talking styles from arbitrary video clips. It introduces a two-stage framework using the Ted-HD dataset and a latent-style-fusion (LSF) model to synthesize 3D talking faces with diversified and realistic talking styles. The paper focuses on overcoming limitations of previous methods that require substantial data and assume each identity has only one talking style.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b35\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of synthesizing stylized talking faces that can reflect diversified talking styles from arbitrary video clips. It proposes a two-stage framework that first synthesizes 3D talking faces and then renders them photo-realistically. The key innovation is the use of style codes derived from video clips to capture talking styles and the latent-style-fusion (LSF) model to combine these codes with audio features for synthesis.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b35\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b11\": 0.7,\n    \"b14\": 0.6,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of synthesizing stylized talking faces by imitating styles from arbitrary video clips. The proposed framework overcomes existing limitations by not requiring substantial synchronized audio-visual data for each identity and allowing for multiple talking styles per identity. The framework includes a two-stage process: synthesizing 3D talking faces and rendering them photo-realistically. The key contributions include the collection of the Ted-HD dataset, the definition of style codes based on 3DMM parameters, and the development of a latent-style-fusion (LSF) model for synthesizing stylized talking faces.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b35\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b29\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b28\": 0.6,\n    \"b34\": 0.6,\n    \"b38\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```"], "6197105b6750f82d7d73b020": ["```json\n{\n  \"Summary\": \"The paper addresses the expressivity and scalability challenges in Graph Neural Networks (GNNs) by proposing a novel 'decoupling' design principle. This involves separating the GNN depth (number of layers) from the scope (neighborhood size) to enhance both computational efficiency and model accuracy. The authors introduce SHADOW-GNN, which uses shallow subgraphs for message passing while maintaining deeper layers for better expressivity.\",\n  \"Direct Inspiration\": {\n    \"b47\": 1.0,\n    \"b32\": 0.9,\n    \"b48\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b53\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b10\": 0.5,\n    \"b42\": 0.5,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Scalability challenge due to neighbor explosion\",\n      \"Expressivity challenge in GNNs\"\n    ],\n    \"inspirations\": [\n      \"Using a local view on graphs to decouple the GNN depth and scope\",\n      \"Proposing SHADOW-GNN to improve both expressivity and scalability\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b47\": 1.0,\n    \"b32\": 0.9,\n    \"b16\": 0.8,\n    \"b48\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.7,\n    \"b10\": 0.7,\n    \"b0\": 0.6,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b6\": 0.5,\n    \"b53\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the expressivity and scalability challenges of Graph Neural Networks (GNNs) by proposing a 'decoupling' design that separates the GNN depth and scope. This novel approach leads to the creation of SHADOW-GNN, which enhances GNN performance by using shallow subgraphs while increasing model depth.\",\n  \"Direct Inspiration\": {\n    \"b47\": 0.9,\n    \"b32\": 0.8,\n    \"b48\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b42\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b53\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\"Scalability challenge (neighbor explosion)\", \"Expressivity challenge\"],\n    \"algorithm\": \"Decoupling the GNN depth and scope to form SHADOW-GNN, which uses subgraph extraction and deeper models to improve expressivity and scalability\"\n  },\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [\n    \"b48\",\n    \"b26\",\n    \"b16\",\n    \"b32\",\n    \"b0\",\n    \"b31\",\n    \"b29\"\n  ],\n  \"Other Inspiration\": [\n    \"b20\",\n    \"b10\",\n    \"b6\",\n    \"b5\",\n    \"b59\",\n    \"b7\",\n    \"b53\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the scalability and expressivity challenges in Graph Neural Networks (GNNs) by proposing a design principle to decouple the depth and scope of GNNs. The proposed solution, SHADOW-GNN, uses a local view of the graph to extract shallow subgraphs for efficient and expressive message passing.\",\n  \"Direct Inspiration\": {\n    \"b48\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.9,\n    \"b47\": 0.9,\n    \"b31\": 0.8,\n    \"b20\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b29\": 0.7,\n    \"b6\": 0.7,\n    \"b5\": 0.7,\n    \"b59\": 0.7,\n    \"b52\": 0.7,\n    \"b7\": 0.7,\n    \"b53\": 0.7\n  }\n}\n```"], "61fb47e15aee126c0f873a34": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the sensitivity of prompt-based learning methods to trivial variations and the large model sizes required, which complicate their practical deployment. The proposed algorithm combines prompt-based learning with co-training to improve performance using unlabeled data. This is achieved by using outputs from a large prompt-based model as one view and a pre-trained representation from a smaller language model as another view, iteratively training both models to enhance accuracy without additional labeled data.\",\n  \"Direct Inspiration\": {\n    \"b41\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b9\": 0.7,\n    \"b18\": 0.6,\n    \"b17\": 0.6,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of prompt-based learning, particularly its sensitivity to prompt variations and the need for large models. It proposes leveraging co-training with unlabeled data to improve performance without relying heavily on labeled data. The method combines outputs from a large prompt-based model and a smaller pre-trained model, focusing on calibration and ensembling of prompts.\",\n  \"Direct Inspiration\": {\n    \"b41\": 1,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b19\": 0.8,\n    \"b28\": 0.8,\n    \"b14\": 0.7,\n    \"b20\": 0.7,\n    \"b11\": 0.6,\n    \"b32\": 0.6,\n    \"b8\": 0.6,\n    \"b36\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b17\": 0.5,\n    \"b29\": 0.5,\n    \"b12\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in prompt-based learning, particularly sensitivity to prompt variations and the need for labeled data, by proposing a co-training approach using unlabeled data. It combines outputs from large prompt-based models with pre-trained representations from smaller models to improve performance without extensive labeled data.\",\n    \"Direct Inspiration\": {\n        \"b41\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.9,\n        \"b18\": 0.8,\n        \"b17\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.7,\n        \"b20\": 0.7,\n        \"b19\": 0.7,\n        \"b36\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the sensitivity of prompt-based learning to trivial variations and the challenges associated with the large models used, such as T0 and GPT-3. To overcome these issues, the authors propose combining co-training with prompt-based learning to leverage unlabeled data and improve performance without relying heavily on labeled data. They use complementary representations from large prompt-based models and smaller pre-trained models like DeBERTa to iteratively improve model training and calibration.\",\n    \"Direct Inspiration\": [\"b41\", \"b1\"],\n    \"Indirect Inspiration\": [\"b9\", \"b18\", \"b17\"],\n    \"Other Inspiration\": [\"b20\", \"b19\", \"b36\", \"b29\", \"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the challenges of sensitivity in prompt-based learning methods and the large model sizes used, which hinder practical deployment. The authors propose combining co-training with prompt-based learning to improve performance using unlabeled data, leveraging a large prompt-based model and a smaller language model.\",\n  \"Direct Inspiration\": [\"b41\", \"b9\"],\n  \"Indirect Inspiration\": [\"b18\", \"b17\", \"b1\", \"b28\"],\n  \"Other Inspiration\": [\"b2\", \"b20\", \"b14\"]\n}\n```"], "619472d35244ab9dcbd2dc0b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in training Graph Neural Networks (GNNs) on large-scale real-world graphs. The proposed algorithm, Learn Locally, Correct Globally (LLCG), aims to reduce communication overhead while preserving accuracy through periodic averaging and global server corrections. The primary inspirations come from distributed optimization techniques and previous efforts in distributed GNN training.\",\n  \"Direct Inspiration\": [\"b30\", \"b38\"],\n  \"Indirect Inspiration\": [\"b26\", \"b18\", \"b0\", \"b41\", \"b33\", \"b28\"],\n  \"Other Inspiration\": [\"b29\", \"b36\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient distributed training of Graph Neural Networks (GNNs) on large-scale real-world graphs. The proposed algorithm, Learn Locally, Correct Globally (LLCG), is designed to reduce communication overhead while preserving model accuracy by applying periodic averaging and server correction steps.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b30\", \"b38\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b26\", \"b18\", \"b0\", \"b41\", \"b33\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": []\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively training Graph Neural Networks (GNNs) on large-scale real-world graphs. The authors propose a communication-efficient distributed GNN training method, Learn Locally, Correct Globally (LLCG), which aims to reduce communication overhead while maintaining high accuracy by utilizing periodic averaging and global server correction.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1.0,\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b41\": 0.7,\n    \"b33\": 0.7,\n    \"b26\": 0.7,\n    \"b18\": 0.7,\n    \"b0\": 0.6\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively training Graph Neural Networks (GNNs) on large-scale real-world graphs using a novel method called Learn Locally, Correct Globally (LLCG). The main challenges include communication overhead, information loss due to graph partitioning, and privacy concerns. The proposed LLCG method aims to reduce communication overhead while maintaining training accuracy by leveraging periodic averaging and server correction steps.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1,\n    \"b38\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b42\": 0.9,\n    \"b41\": 0.8,\n    \"b33\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently training Graph Neural Networks (GNNs) on large-scale real-world graphs by proposing a communication-efficient distributed GNN training method called Learn Locally, Correct Globally (LLCG). The key challenge is the dependency between nodes in a graph, which makes data parallelism and graph partitioning non-trivial, leading to significant storage/communication overhead and privacy concerns. The proposed LLCG method aims to reduce communication overhead and address privacy concerns while maintaining high accuracy.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.9,\n    \"b38\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.7,\n    \"b41\": 0.6,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.5,\n    \"b36\": 0.5\n  }\n}\n```"], "615bc2735244ab9dcbdbccf2": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the data-hungry nature of GNNs, the inadequacy of existing motif mining techniques for molecular graphs, and the challenge of unifying multi-level self-supervised pre-training tasks. The proposed MGSSL introduces a novel motif generation task, a general motif-based generative pre-training framework, and multi-level self-supervised pre-training using the Frank-Wolfe algorithm.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b13\": 1,\n    \"b14\": 1,\n    \"b15\": 1,\n    \"b31\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b12\": 0.9,\n    \"b29\": 0.9,\n    \"b50\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.8,\n    \"b9\": 0.8,\n    \"b24\": 0.8,\n    \"b51\": 0.8,\n    \"b53\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Graph Neural Networks (GNNs) for molecular property prediction with limited labeled data by proposing Motif-based Graph Self-Supervised Learning (MGSSL) and Multi-level self-supervised pre-training. The key innovation of MGSSL lies in introducing a motif generation task and multi-level self-supervised pre-training to capture rich structural and semantic information from graph motifs.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b15\": 0.8,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.6,\n    \"b29\": 0.6,\n    \"b26\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.4,\n    \"b50\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving molecular property prediction using Graph Neural Networks (GNNs) by introducing Motif-based Graph Self-Supervised Learning (MGSSL) and Multi-level self-supervised pre-training. The paper is inspired by recent advances in deep learning, particularly self-supervised learning (SSL) in NLP and CV. The authors propose novel motif generation tasks and a multi-level pre-training framework to capture rich structural and semantic information from graph motifs, leveraging the BRICS algorithm and the Frank-Wolfe algorithm for adaptive task weighting.\",\n    \"Direct Inspiration\": [\"b4\", \"b12\", \"b13\", \"b14\"],\n    \"Indirect Inspiration\": [\"b1\", \"b31\", \"b50\"],\n    \"Other Inspiration\": [\"b3\", \"b15\", \"b29\", \"b51\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of data scarcity in molecular property prediction using Graph Neural Networks (GNNs) by proposing a motif-based graph self-supervised learning (MGSSL) framework. This framework introduces a novel motif generation task and multi-level self-supervised pre-training to capture the rich structural and semantic information of graph motifs. Key inspirations include self-supervised learning advancements in NLP and CV, and existing GNN pre-training methods.\",\n  \"Direct Inspiration\": [\"b4\", \"b13\", \"b14\", \"b29\"],\n  \"Indirect Inspiration\": [\"b31\", \"b50\", \"b51\"],\n  \"Other Inspiration\": [\"b3\", \"b15\", \"b53\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of data scarcity and overfitting in molecular property prediction using Graph Neural Networks (GNNs). It proposes a novel self-supervised learning framework, MGSSL, which introduces motif-based generative pre-training and multi-level self-supervised pre-training to capture rich structural and semantic information from graph motifs.\",\n  \"Direct Inspiration\": [\"b13\", \"b14\"],\n  \"Indirect Inspiration\": [\"b3\", \"b15\", \"b31\", \"b29\"],\n  \"Other Inspiration\": [\"b7\", \"b50\"]\n}\n```"], "5ff68eedd4150a363cd9084a": ["```json\n{\n  \"Summary\": \"The paper identifies the challenges of insider attacks and proposes a user behavior evaluation method based on fuzzy logic to address these challenges. It introduces a finite automaton to characterize normal user behavior and a fuzzy reasoning system for detecting deviations from normal behavior.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1.0,\n    \"b39\": 1.0,\n    \"b40\": 1.0,\n    \"b41\": 1.0,\n    \"b42\": 1.0,\n    \"b43\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of user behavior fluctuation in intrusion detection by proposing a method based on fuzzy logic to evaluate user behavior. The proposed method involves constructing user behavior profiles using finite automata and employing fuzzy reasoning to detect deviations from these profiles.\",\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [\"b13\", \"b14\", \"b15\", \"b16\", \"b17\", \"b18\", \"b19\", \"b20\", \"b21\", \"b22\"],\n  \"Other Inspiration\": [\"b23\", \"b24\", \"b25\", \"b26\", \"b27\", \"b29\", \"b30\", \"b31\", \"b32\", \"b33\", \"b34\", \"b35\", \"b36\", \"b37\", \"b38\", \"b39\", \"b40\", \"b41\", \"b42\", \"b43\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"User behavior is complex and variable making it difficult to represent behavior with precise thresholds.\",\n      \"Existing methods such as HMM have high computation costs.\",\n      \"Current detection methods struggle with uncertainty and fluctuation in user behavior.\"\n    ],\n    \"inspirations\": [\n      \"Tolerating uncertainty and fluctuation in user behavior.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b38\": 1,\n    \"b39\": 1,\n    \"b40\": 1,\n    \"b41\": 1,\n    \"b42\": 1,\n    \"b43\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6,\n    \"b36\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting user behavior anomalies in computer systems, particularly focusing on the issues of uncertainty and fluctuation in user behavior. The proposed solution involves a fuzzy logic-based approach for user-level intrusion detection that builds normal user behavior profiles using finite state automata and then evaluates deviations using fuzzy reasoning.\",\n  \"Direct Inspiration\": [\"b38\", \"b39\", \"b40\", \"b41\", \"b42\", \"b43\"],\n  \"Indirect Inspiration\": [\"b23\", \"b24\", \"b25\", \"b30\", \"b31\", \"b32\", \"b34\", \"b35\"],\n  \"Other Inspiration\": [\"b0\", \"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b6\", \"b7\", \"b8\", \"b9\", \"b10\", \"b11\", \"b12\", \"b13\", \"b14\", \"b15\", \"b16\", \"b17\", \"b18\", \"b19\", \"b20\", \"b21\", \"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of user behavior fluctuation in user-level intrusion detection. It proposes a novel method based on fuzzy logic to handle the uncertainty and fluctuation in user behavior. The key contribution is the development of a fuzzy reasoning system and the use of finite automata to represent user behavior profiles.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1.0,\n    \"b39\": 1.0,\n    \"b40\": 1.0,\n    \"b41\": 1.0,\n    \"b42\": 1.0,\n    \"b43\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b24\": 0.8,\n    \"b25\": 0.8,\n    \"b34\": 0.8,\n    \"b35\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b14\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5,\n    \"b33\": 0.5,\n    \"b37\": 0.5\n  }\n}\n```"], "619bb9c11c45e57ce96c86e9": ["```json\n{\n  \"Summary\": \"The paper addresses the increasing issue of transient faults in integrated circuits. The primary challenge is detecting control flow errors (CFEs) within basic blocks efficiently without significant performance overhead. The proposed algorithm, CCFCA, introduces a novel method by using virtual basic blocks and configurable fault tolerance mechanisms to improve detection accuracy and reduce redundant instructions.\",\n  \"Direct Inspiration\": {\n    \"Inspired by\": [\"b25\"]\n  },\n  \"Indirect Inspiration\": {\n    \"Adopted methods from\": [\"b13\", \"b18\", \"b24\"]\n  },\n  \"Other Inspiration\": {\n    \"Additional Influences\": [\"b22\", \"b23\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of detecting control flow errors (CFEs) within basic blocks in software-based methods. The proposed algorithm, CCFCA, aims to reduce overhead and improve fault tolerance efficiency by assigning unique signatures, updating signatures at virtual edges, and optimizing detection regions through criticality analysis.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b13\": 0.9,\n    \"b25\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b24\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b23\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting control flow errors (CFEs) caused by transient faults in integrated circuits. It proposes a novel algorithm that combines time invariants, unique signatures, and virtual edges to improve detection efficiency and reduce overhead.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b18\": 0.8,\n    \"b22\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting control flow errors (CFEs) within basic blocks in software-based methods without additional hardware. It presents an algorithm that assigns unique signatures to basic blocks and updates signatures at virtual edges to reduce detection overhead and improve fault tolerance efficiency.\",\n  \"Direct Inspiration\": [\"b13\", \"b25\"],\n  \"Indirect Inspiration\": [\"b24\"],\n  \"Other Inspiration\": [\"b18\", \"b22\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"High cost and low detection rate of existing detection methods in the detection of inner-block CFEs; confusion with virtual edges; need for configurability to reduce redundant instructions and improve tolerance efficiency.\",\n    \"inspirations\": \"Inspired by previous software-based control flow error detection methods and aiming to improve on their limitations.\"\n  },\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b18\": 1.0,\n    \"b24\": 1.0,\n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```"], "6006a6f091e0111a1b6a21ad": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the problem of learning from a limited number of training samples, which often leads to models overfitting on these few samples and failing to generalize well. The authors propose a novel approach to calibrate the biased distribution of few-shot samples into a more accurate approximation of the ground truth distribution, but in the feature space rather than the original data space. They assume a Gaussian distribution for feature vectors and transfer statistics from many-shot classes to few-shot classes based on class similarity, enabling better generalization and improved performance in few-shot learning tasks.\",\n  \"Direct Inspiration\": {\n    \"b29\": 0.9,\n    \"b21\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b23\": 0.7,\n    \"b6\": 0.65,\n    \"b27\": 0.6,\n    \"b32\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.5,\n    \"b18\": 0.45\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of few-shot learning by proposing a distribution calibration strategy that leverages the statistics of base classes to estimate the distribution for novel classes. The approach aims to improve generalization by training models on feature distributions that better approximate the ground truth distribution.\",\n    \"Direct Inspiration\": {\n        \"b29\": 1.0,\n        \"b21\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b23\": 0.8,\n        \"b27\": 0.7,\n        \"b6\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b24\": 0.6,\n        \"b12\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot learning by proposing a distribution calibration strategy that adjusts the feature space distribution to better approximate the ground truth. This method leverages statistics from base classes to calibrate the distribution of novel classes, allowing for more accurate sample generation and improved model generalization. The approach does not require extra learnable parameters and can be integrated with any classifier and feature extractor.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b23\": 0.7,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b27\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot learning by proposing a distribution calibration strategy in the feature space to better approximate the ground truth distribution. This approach leverages the statistical properties of base classes with sufficient samples to improve the performance on novel classes with limited samples.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b29\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b23\": 0.7,\n    \"b26\": 0.7,\n    \"b32\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the challenge of few-shot learning, where models must generalize well with very few training samples. The authors propose a novel distribution calibration strategy in the feature space to improve the performance of few-shot classification tasks. By assuming feature vectors follow a Gaussian distribution, they transfer statistics from many-shot classes to few-shot classes based on class similarity, thus generating more accurate and diverse samples for training.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b29\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b23\": 0.75,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b6\": 0.55,\n    \"b32\": 0.5\n  }\n}\n```"], "610a271c5244ab9dcba8a3de": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed by the paper is the distributional shift problem in Graph Neural Networks (GNNs) caused by biased sampling of labeled data for training, leading to overfitting and poor generalization.\",\n    \"inspirations\": \"The paper proposes the Shift-Robust GNN (SR-GNN) framework to adapt biased samples to conform more closely to the distributional characteristics of an IID sample. It includes regularization for hidden layers in traditional GNNs and instance reweighting for linearized GNN models.\"\n  },\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b4\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b21\": 0.7,\n    \"b5\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b16\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of distributional shift in Graph Neural Networks (GNNs), particularly in the context of semi-supervised learning (SSL). The proposed Shift-Robust GNN (SR-GNN) framework aims to adapt biased samples of labeled nodes to more closely match the distributional characteristics of an independent and identically distributed (IID) sample of the graph. The paper presents methods to mitigate this shift for both traditional deep GNN models and linearized GNN models through regularization and instance reweighting.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b38\": 0.85,\n    \"b20\": 0.8,\n    \"b34\": 0.82\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b36\": 0.78,\n    \"b27\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b21\": 0.6,\n    \"b5\": 0.68,\n    \"b18\": 0.55,\n    \"b19\": 0.55,\n    \"b16\": 0.6,\n    \"b37\": 0.6,\n    \"b24\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of distributional shift in graph neural networks (GNNs) due to biased training data. It proposes the Shift-Robust GNN (SR-GNN) framework to adapt labeled node samples to conform to an IID distribution for better generalization. The approach includes regularization for traditional GNN models and instance reweighting for linearized GNN models. The effectiveness of SR-GNN is demonstrated through experiments with biased training sets.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b15\": 0.9,\n    \"b33\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b21\": 0.7,\n    \"b5\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b36\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b4\": 0.6,\n    \"b3\": 0.6,\n    \"b35\": 0.6,\n    \"b27\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the issue of distributional shift caused by biased training labels in graph-based semi-supervised learning (SSL) using Graph Neural Networks (GNNs). The proposed algorithm, Shift-Robust GNN (SR-GNN), aims to mitigate this bias by adapting the biased sample of labeled nodes to more closely conform to the IID distribution of the entire graph. This involves regularization over hidden layers for traditional GNNs and instance reweighting for linearized models.\",\n    \"Direct Inspiration\": {\n        \"b14\": 1.0,\n        \"b15\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b21\": 0.7,\n        \"b18\": 0.6,\n        \"b36\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.5,\n        \"b6\": 0.5,\n        \"b4\": 0.5,\n        \"b33\": 0.5,\n        \"b19\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of distributional shift in Graph Neural Networks (GNNs) for semi-supervised learning (SSL). The authors propose a framework called Shift-Robust GNN (SR-GNN) to adapt biased samples of labeled nodes to more closely conform to the distributional characteristics of an IID sample. The framework includes regularization methods for traditional GNNs and instance reweighting methods for linearized GNN models.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b15\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b19\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```"], "6205d3ef5aee126c0ff1cfe3": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The main challenge addressed by the paper is the labor-intensive and time-consuming process of manually constructing taxonomies, which is exacerbated by the emergence of numerous new concepts in the era of big data. The paper proposes an algorithm, TaxoEnrich, to automatically expand and complete existing taxonomies by learning better representations for each candidate position, leveraging both semantic and structural information.\",\n    \"Inspirations\": \"The paper is inspired by recent studies on taxonomy expansion and completion, particularly focusing on leveraging both hierarchical information from existing taxonomies and semantic embeddings generated using pretrained language models.\"\n  },\n  \"Direct Inspiration\": {\n    \"b33\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9,\n    \"b30\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.8,\n    \"b11\": 0.75,\n    \"b29\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"time-consuming and labor-intensive construction of taxonomies\",\n      \"handling large number of emerging new concepts\",\n      \"need for automatic taxonomy expansion and completion\",\n      \"limited representation power of existing models\"\n    ],\n    \"inspirations\": [\n      \"to leverage both semantic and structural information for better taxonomy completion performance\",\n      \"to propose TaxoEnrich framework\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatic taxonomy expansion and completion to handle the large number of emerging new concepts. It introduces the TaxoEnrich framework, which consists of novel components such as taxonomy-contextualized embedding generation, sequential feature encoder, and query-aware sibling encoder to improve taxonomy completion performance.\",\n  \"Direct Inspiration\": [\"b33\", \"b30\"],\n  \"Indirect Inspiration\": [\"b18\", \"b29\"],\n  \"Other Inspiration\": [\"b1\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of automatic taxonomy expansion and completion, which is crucial due to the labor-intensive nature of manual taxonomy construction. The proposed TaxoEnrich framework introduces novel methods such as taxonomy-contextualized embedding generation, sequential feature encoders, and query-aware sibling encoders to improve the performance of taxonomy completion and expansion tasks.\",\n    \"Direct Inspiration\": {\n        \"reference_number\": [\"b33\", \"b30\"]\n    },\n    \"Indirect Inspiration\": {\n        \"reference_number\": [\"b18\"]\n    },\n    \"Other Inspiration\": {\n        \"reference_number\": [\"b11\", \"b29\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatic taxonomy completion and expansion, which is traditionally labor-intensive. The authors propose TaxoEnrich, a novel framework that leverages both semantic and structural information to improve taxonomy completion. Key components include taxonomy-contextualized embedding generation, sequential feature encoding, and a query-aware sibling encoder. Extensive experiments demonstrate that TaxoEnrich significantly enhances performance over previous methods.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b18\": 0.9,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b4\": 0.7\n  }\n}\n```"], "61850e9691e01121084ca036": ["```json\n{\n  \"Summary\": \"The paper introduces PyTorch Geometric Temporal, an open-source Python library tailored for spatiotemporal machine learning. The framework is inspired by existing geometric deep learning libraries within the PyTorch ecosystem and addresses the limitations of existing frameworks that operate on static graphs with fixed topologies. The main contributions include the release of the library, data loaders and iterators for spatiotemporal datasets, new benchmark datasets, and an evaluation of the framework's performance on real-world tasks.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b39\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b45\": 0.7,\n    \"b67\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b29\": 0.65,\n    \"b32\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing geometric deep learning frameworks which assume fixed topology and static node features, and are not designed for spatiotemporal data. It introduces PyTorch Geometric Temporal, an open-source Python library for spatiotemporal machine learning that reuses existing neural network layers in a modular manner and provides data loaders and iterators for spatiotemporal datasets. The framework is evaluated on real-world datasets, demonstrating its scalability and forecasting capabilities.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b39\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b44\": 0.7,\n    \"b45\": 0.7,\n    \"b46\": 0.7,\n    \"b62\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performing deep learning on spatiotemporal graph-structured data, which current frameworks do not support. It proposes PyTorch Geometric Temporal, a framework inspired by existing geometric deep learning libraries, to handle spatiotemporal datasets efficiently.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b39\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.6,\n    \"b44\": 0.6,\n    \"b45\": 0.7,\n    \"b46\": 0.6,\n    \"b62\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.5,\n    \"b67\": 0.5,\n    \"b24\": 0.7,\n    \"b25\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performing deep learning on spatiotemporal graph-structured data, which existing geometric deep learning frameworks are not designed to handle. The proposed solution is PyTorch Geometric Temporal, an open-source Python library designed with an API inspired by existing geometric deep learning libraries in the PyTorch ecosystem. The library is built to be user-friendly and scalable, providing data loaders and iterators for spatiotemporal datasets and releasing new benchmark datasets for various domains.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b39\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.6,\n    \"b44\": 0.6,\n    \"b45\": 0.6,\n    \"b46\": 0.6,\n    \"b62\": 0.6,\n    \"b67\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b11\": 0.5,\n    \"b2\": 0.5,\n    \"b34\": 0.5,\n    \"b58\": 0.5,\n    \"b16\": 0.5,\n    \"b29\": 0.5,\n    \"b32\": 0.5,\n    \"b13\": 0.5,\n    \"b49\": 0.5,\n    \"b0\": 0.4,\n    \"b40\": 0.4,\n    \"b10\": 0.4,\n    \"b15\": 0.4,\n    \"b27\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes PyTorch Geometric Temporal, a deep learning library for spatiotemporal machine learning. It aims to address the limitations of existing geometric deep learning frameworks which operate on static graph-structured data. The proposed framework offers a modular design, efficient data loaders, and new benchmark datasets, making it suitable for various real-world applications like epidemiological forecasting and web traffic management.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b39\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b11\": 0.7,\n    \"b29\": 0.7,\n    \"b32\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```"], "603e18b291e01129ef28fcff": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Inference complexity and customization for Bayesian Probabilistic Topic Models (BPTMs)\",\n      \"Scalability issues with large text collections and parallel computing\",\n      \"Integration challenges with deep neural networks (DNNs)\"\n    ],\n    \"Inspirations\": [\n      \"Emergence of Neural Topic Models (NTMs) leveraging DNNs and deep generative models\",\n      \"Improvement in performance, efficiency, and usability of topic modeling\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b36\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b51\": 0.8,\n    \"b67\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b54\": 0.7,\n    \"b42\": 0.7,\n    \"b70\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges associated with Bayesian probabilistic topic models (BPTMs) and the integration of deep neural networks (DNNs) for improved performance, efficiency, and usability in topic modeling. It introduces neural topic models (NTMs) as a solution, providing a taxonomy, comprehensive review, and discussion of future research directions.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b36\": 0.9,\n    \"b67\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b51\": 0.7,\n    \"b54\": 0.6,\n    \"b42\": 0.6,\n    \"b61\": 0.5,\n    \"b70\": 0.5,\n    \"b25\": 0.5,\n    \"b34\": 0.5,\n    \"b23\": 0.5,\n    \"b13\": 0.5,\n    \"b5\": 0.5,\n    \"b28\": 0.5,\n    \"b59\": 0.5,\n    \"b1\": 0.5\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of traditional Bayesian probabilistic topic models (BPTMs) in the context of big data and deep learning: high inference complexity, difficulty in scaling, and integration issues with deep neural networks (DNNs). The authors propose a taxonomy for neural topic models (NTMs), provide an overview and comparison of existing approaches, and discuss future directions.\",\n    \"Direct Inspiration\": {\n        \"b36\": 0.9,\n        \"b27\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b51\": 0.75,\n        \"b54\": 0.7,\n        \"b70\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b34\": 0.6,\n        \"b67\": 0.6,\n        \"b5\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in traditional Bayesian probabilistic topic models (BPTMs), particularly their inference complexity, scalability, and integration with deep neural networks (DNNs). The authors propose a taxonomy of neural topic models (NTMs), provide a comprehensive review of NTMs, and identify future research directions.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b36\": 1.0,\n    \"b51\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b67\": 0.9,\n    \"b42\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b3\": 0.7,\n    \"b35\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of Bayesian probabilistic topic models (BPTMs) in the era of big data and deep learning, specifically focusing on their inference complexity, scalability, and integration with deep neural networks (DNNs). It proposes a taxonomy of neural topic models (NTMs) and provides a comprehensive review, categorizing existing models, discussing applications, and identifying future research directions.\",\n  \"Direct Inspiration\": [\"b27\", \"b36\"],\n  \"Indirect Inspiration\": [\"b33\", \"b42\", \"b67\"],\n  \"Other Inspiration\": [\"b3\", \"b5\", \"b54\"]\n}\n```"], "619321425244ab9dcbbb4aba": ["```json\n{\n    \"Summary\": \"The paper introduces iBOT, a novel framework for Masked Image Modeling (MIM) using an online tokenizer. It addresses challenges in capturing high-level visual semantics and proposes a self-distillation approach where the tokenizer and target model are jointly optimized. The framework is shown to achieve significant performance improvements on various benchmarks and tasks.\",\n    \"Direct Inspiration\": {\n        \"b7\": 1,\n        \"b3\": 0.9,\n        \"b36\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.7,\n        \"b17\": 0.7,\n        \"b24\": 0.6,\n        \"b16\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b30\": 0.5,\n        \"b8\": 0.5,\n        \"b6\": 0.5,\n        \"b35\": 0.5,\n        \"b29\": 0.4,\n        \"b2\": 0.4,\n        \"b41\": 0.3,\n        \"b31\": 0.3\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying Masked Language Modeling (MLM) principles to Vision Transformers (ViT) for computer vision tasks, which involves designing a visual tokenizer that can handle the continuous nature of image data. The proposed algorithm, iBOT, performs Masked Image Modeling (MIM) with an online tokenizer that is jointly optimized with the target model, enabling it to capture high-level visual semantics and achieve state-of-the-art performance in various benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying Masked Language Modeling (MLM) principles, successful in NLP, to Vision Transformers (ViT) for computer vision tasks. It introduces iBOT, a framework for Masked Image Modeling (MIM) with an online tokenizer. The novel approach formulates MIM as a knowledge distillation task and proposes self-distillation to improve performance by jointly optimizing the tokenizer and target model. The paper demonstrates significant advancements in various benchmarks and downstream tasks, indicating iBOT's superior capability in capturing visual semantics.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b19\": 0.8,\n    \"b36\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b17\": 0.65,\n    \"b35\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a more effective Masked Image Modeling (MIM) method for Vision Transformers (ViTs) by introducing iBOT, a framework that integrates an online tokenizer for self-distillation. The key inspiration comes from Masked Language Modeling (MLM) in NLP and the concept of knowledge distillation. The proposed method aims to overcome the limitations of previous tokenizers that struggle with semantic abstraction and adaptability.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b17\": 0.8,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is developing an effective masked image modeling (MIM) framework for training Vision Transformers (ViT), similar to the success of Masked Language Modeling (MLM) in natural language processing (NLP). The proposed solution, iBOT (image BERT pre-training with Online Tokenizer), aims to optimize the tokenizer and target model jointly, addressing issues with previous tokenizers that were either identity mappings or discrete VAEs. The novel approach involves self-distillation with an online tokenizer to effectively capture high-level visual semantics and improve performance across multiple benchmarks and downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b7\": 0.8,\n    \"b36\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.6,\n    \"b19\": 0.6,\n    \"b17\": 0.6,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5\n  }\n}\n```"], "61a596635244ab9dcbdfe47f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of over-squashing in graph neural networks (GNNs) using a geometric perspective. The authors propose a new combinatorial edge-based curvature called Balanced Forman curvature and present a curvature-based graph rewiring method, Stochastic Discrete Ricci Flow (SDRF), to improve GNN performance.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b24\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.75,\n    \"b38\": 0.7,\n    \"b52\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b26\": 0.55,\n    \"b6\": 0.55,\n    \"b2\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the over-squashing phenomenon in message passing neural networks (MPNNs) on graphs, which is caused by bottlenecks in graph topology. It proposes the Jacobian of node representations as a measure, introduces a new curvature called Balanced Forman curvature, and presents a curvature-based graph rewiring method called Stochastic Discrete Ricci Flow (SDRF) to alleviate the bottleneck and improve GNN performance.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b24\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b38\": 0.6,\n    \"b42\": 0.6,\n    \"b52\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b18\": 0.5,\n    \"b26\": 0.5,\n    \"b37\": 0.4,\n    \"b31\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of over-squashing and bottleneck phenomena in message passing neural networks (MPNNs) on graphs. Using differential geometry, it introduces a new combinatorial edge-based curvature called Balanced Forman curvature, and proposes a curvature-based method for graph rewiring termed Stochastic Discrete Ricci Flow (SDRF). The paper theoretically and experimentally demonstrates that this method improves MPNN performance by alleviating over-squashing.\",\n  \"Direct Inspiration\": [\"b0\", \"b24\"],\n  \"Indirect Inspiration\": [\"b18\", \"b12\", \"b38\", \"b52\", \"b35\"],\n  \"Other Inspiration\": [\"b26\", \"b6\", \"b2\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of over-squashing in graph neural networks (GNNs) using tools from differential geometry. It introduces the concept of Balanced Forman curvature as a measure to identify bottlenecks in graph topology that lead to over-squashing and proposes a curvature-based graph rewiring method called Stochastic Discrete Ricci Flow (SDRF) to alleviate these bottlenecks.\",\n    \"Direct Inspiration\": [\"b0\", \"b24\"],\n    \"Indirect Inspiration\": [\"b12\", \"b38\", \"b52\", \"b35\"],\n    \"Other Inspiration\": [\"b18\", \"b23\", \"b37\"]\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the over-squashing phenomenon in message passing graph neural networks (GNNs) from a geometric perspective. It introduces the Jacobian of node representations to measure over-squashing, proposes a new combinatorial edge-based curvature called Balanced Forman curvature, and presents a curvature-based graph rewiring method named Stochastic Discrete Ricci Flow (SDRF) to alleviate the bottleneck and improve GNN performance.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.95,\n    \"b24\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.75,\n    \"b35\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b38\": 0.55,\n    \"b52\": 0.5\n  }\n}\n```"], "615e65735244ab9dcbf21713": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of entity linking (EL) by proposing a novel approach, EntQA, which flips the traditional pipeline of mention detection followed by entity disambiguation. Instead, it performs entity disambiguation first and then identifies mentions, using a dual encoder retriever and a deep cross-attention reader. This method is inspired by recent advancements in dense entity retrieval and open-domain question answering (QA).\",\n  \"Direct Inspiration\": [\"b39\", \"b5\"],\n  \"Indirect Inspiration\": [\"b6\", \"b23\"],\n  \"Other Inspiration\": [\"b10\", \"b11\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of entity linking (EL) by proposing a novel approach, EntQA, which flips the traditional order of mention detection (MD) and entity disambiguation (ED). Instead of finding mentions first and then linking them to entities, the authors propose to find relevant entities first and then identify their mentions in the document. This approach resolves the inherent difficulties in the traditional MD\u2192ED pipeline and leverages recent advancements in dense entity retrieval and open-domain QA. The model, EntQA, is shown to outperform existing methods on multiple datasets without the need for a mention-candidates dictionary or extensive model-specific pretraining.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b6\": 0.8,\n    \"b23\": 0.85,\n    \"b39\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b17\": 0.6,\n    \"b31\": 0.65,\n    \"b34\": 0.7,\n    \"b42\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.5,\n    \"b40\": 0.55,\n    \"b11\": 0.5,\n    \"b44\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the intractable nature of the entity linking (EL) task, which involves both extracting entity mentions and linking them to entries in a knowledge base (KB). The authors propose a novel approach, EntQA, which inverts the traditional order of mention detection (MD) followed by entity disambiguation (ED) by solving ED first. This method leverages recent advancements in dense entity retrieval and open-domain QA to improve performance, eliminating the need for a hardcoded mention-candidates dictionary and extensive pretraining. The approach is shown to be effective and efficient through various experiments.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b39\": 0.8,\n    \"b6\": 0.7,\n    \"b23\": 0.6\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.5,\n    \"b44\": 0.5,\n    \"b40\": 0.4\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.3,\n    \"b22\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of entity linking (EL) by reversing the traditional approach of mention detection followed by entity disambiguation, proposing instead to perform entity disambiguation before mention detection. This method, called EntQA, leverages recent advances in dense entity retrieval and open-domain question answering (QA) to achieve state-of-the-art results without relying on a mention-candidates dictionary or expensive pretraining.\",\n  \"Direct Inspiration\": [\"b39\", \"b5\", \"b6\", \"b23\"],\n  \"Indirect Inspiration\": [\"b11\", \"b44\", \"b30\"],\n  \"Other Inspiration\": [\"b8\", \"b34\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the intractability of the entity linking (EL) task due to the need to identify mentions and link them to entries in a large knowledge base (KB). The authors propose the EntQA model, which innovatively solves the EL task by reversing the traditional order of mention detection (MD) and entity disambiguation (ED). Instead of predicting mentions first, EntQA predicts candidate entities (as questions) and then identifies their mentions (as answer spans). The model leverages recent advancements in dense entity retrieval and open-domain question answering (QA), using a dual encoder retriever and deep cross-attention reader to achieve strong performance without relying on hardcoded dictionaries or extensive pretraining.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b23\": 0.85,\n    \"b39\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.7,\n    \"b15\": 0.65,\n    \"b11\": 0.6,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b44\": 0.5,\n    \"b31\": 0.5\n  }\n}\n```"], "6201df495aee126c0f64dbb4": ["```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are handling out-of-distribution (OOD) generalization for node-level tasks on graph-structured data and developing a new learning approach based on an invariance principle. The proposed algorithm, Explore-to-Extrapolate Risk Minimization (EERM), aims to enhance GNNs' robustness to distribution shifts by minimizing the mean and variance of risks from multiple environments simulated by adversarial context generators.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9,\n        \"b22\": 0.8,\n        \"b44\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b34\": 0.75,\n        \"b36\": 0.7,\n        \"b33\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b55\": 0.6,\n        \"b60\": 0.65,\n        \"b29\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the non-IID nature of nodes in a graph and the necessity to incorporate structural information for predictions. The authors propose a new learning approach based on an invariance principle to handle OOD generalization for node-level tasks on graphs.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b55\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b44\", \"b22\", \"b3\", \"b33\", \"b36\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b34\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in this paper include handling out-of-distribution (OOD) generalization for node-level tasks on graph-structured data, accommodating the non-IID nature of nodes in graphs, and leveraging structural information for robust prediction. The proposed algorithm, Explore-to-Extrapolate Risk Minimization (EERM), aims to address these issues by fragmenting graphs into ego-graphs, re-formulating invariant assumptions inspired by the Weisfeiler-Lehman test, and devising a new learning approach that minimizes the mean and variance of risks from multiple environments simulated by adversarial context generators.\",\n  \"Direct Inspiration\": {\n    \"b55\": 1,\n    \"b34\": 0.9,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b44\": 0.8,\n    \"b33\": 0.8,\n    \"b36\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b1\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of out-of-distribution (OOD) generalization for node-level tasks on graph-structured data, which is inherently non-IID. It proposes a novel approach called Explore-to-Extrapolate Risk Minimization (EERM) to enhance the robustness of Graph Neural Networks (GNNs) to distribution shifts. The method involves fragmenting graphs into ego-graphs, utilizing invariance principles inspired by the Weisfeiler-Lehman test, and employing adversarial context generators to simulate diverse environments for stable learning.\",\n  \"Direct Inspiration\": {\n    \"b55\": 1.0,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b44\": 0.75,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.65,\n    \"b11\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Out-of-Distribution (OOD) generalization for node-level tasks on graph-structured data. It proposes a new learning approach based on an invariance principle, entitled Explore-to-Extrapolate Risk Minimization (EERM), to enhance the robustness of Graph Neural Networks (GNNs) against distribution shifts. The methodology includes fragmenting graphs into ego-graphs, re-formulating invariant assumptions based on the Weisfeiler-Lehman test, and using adversarial context generators to simulate diverse environments.\",\n  \"Direct Inspiration\": [\"b55\", \"b34\"],\n  \"Indirect Inspiration\": [\"b3\", \"b44\"],\n  \"Other Inspiration\": [\"b1\", \"b36\", \"b11\", \"b33\"]\n}\n```"], "60c1781591e0112cf43c1fa2": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"Incompleteness of taxonomies due to low frequency of multi-word concepts in large-scale text corpora, making it difficult to extract or learn embeddings for these concepts.\",\n    \"inspirations\": \"Inspired by recent advances in text generation, proposing a new task 'taxonomy generation' to identify valid positions for new concepts within an existing taxonomy and generate their names token by token.\"\n  },\n  \"Direct Inspiration\": {\n    \"b18\": 0.95,\n    \"b34\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.75,\n    \"b23\": 0.80,\n    \"b39\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.60,\n    \"b5\": 0.65,\n    \"b12\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in this paper is the incompleteness of taxonomies due to the low frequency of many multi-word concepts in the text corpus, which makes them difficult to extract and embed. The proposed algorithm, GenTaxo, generates concept names token by token using relational contexts from existing taxonomies, leveraging both sentence-based and graph-based representations. It includes pre-training for token prediction and a binary classifier for validating candidate positions.\",\n  \"Direct Inspiration\": [\"b18\", \"b34\"],\n  \"Indirect Inspiration\": [\"b23\", \"b15\", \"b39\"],\n  \"Other Inspiration\": [\"b37\", \"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the incompleteness of concepts in taxonomies, particularly when concepts are rare in large text corpora, which hampers their extraction and embedding learning. The paper introduces GenTaxo, a novel algorithm that generates concept names at candidate positions within an existing taxonomy using an encoder-decoder scheme. The algorithm leverages recent advances in text generation to tackle the issue of low-frequency concept names.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b23\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b37\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of taxonomy completion, particularly when concepts are infrequent or missing in large text corpora. It proposes a novel method called GenTaxo, which generates concept names token by token based on relational contexts, using an encoder-decoder framework. GenTaxo++ is also introduced to enhance extraction-based methods by iteratively expanding the set of new concepts.\",\n  \"Direct Inspiration\": [\"b18\", \"b34\"],\n  \"Indirect Inspiration\": [\"b5\", \"b23\", \"b39\"],\n  \"Other Inspiration\": [\"b3\", \"b8\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the incompleteness of taxonomies due to the low frequency of multi-word concepts in large-scale text corpora. The authors propose GenTaxo, a novel approach that generates concept names token by token using an encoder-decoder scheme with pre-training on large corpora and a combination of sequence and graph encodings.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.9,\n    \"b15\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b37\": 0.6,\n    \"b3\": 0.5\n  }\n}\n```"], "61c1ab455244ab9dcb9c616a": ["```json\n{\n  \"Summary\": \"The paper addresses the inadequacies of traditional methods that focus solely on reducing instruction cache misses for performance improvement in modern processors. The authors propose a new guideline and a method to estimate processor performance by focusing on reducing miss regions rather than instruction cache misses, thus optimizing the design process without needing to simulate the entire processor pipeline repetitively.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b7\": 0.8,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b16\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is that reducing the number of instruction cache misses does not necessarily lead to a reduction in executed cycles in modern processors. The paper proposes a new guideline for improving processor performance by focusing on reducing the number of miss regions rather than reducing instruction cache misses. Additionally, the paper introduces a method for estimating processor performance without simulating the entire processor pipeline, significantly reducing simulation time.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving processor performance by proposing a new guideline that focuses on reducing miss regions rather than merely reducing instruction cache misses. It introduces a method for estimating processor performance without simulating the entire processor pipeline.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b7\": 0.85,\n    \"b8\": 0.85,\n    \"b9\": 0.85,\n    \"b10\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is that reducing the number of instruction cache misses does not necessarily lead to a reduction in the number of executed cycles in modern processors due to the decoupled nature of instruction cache access and branch prediction. The paper proposes a new guideline for improving processor performance that focuses on reducing the number of miss regions instead of reducing instruction cache misses. It also introduces a method for estimating processor performance without simulating the entire processor pipeline.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b9\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge that reducing instruction cache misses in modern processors does not necessarily translate to performance improvements due to the decoupled front-end design. The authors propose a new guideline focusing on reducing miss regions rather than cache misses and introduce a method to estimate processor performance without simulating the entire pipeline.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b7\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.5,\n    \"b13\": 0.5,\n    \"b14\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```"], "6173f1bc91e0118698c047df": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of increasing efficiency and performance in out-of-order processors by introducing a hardware-software cooperative technique. The proposed mechanism involves not flushing all instructions after a branch misprediction, detecting and retaining instructions that reconverge to the correct path to save energy and improve performance.\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.9,\n    \"b10\": 0.8,\n    \"b17\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.75,\n    \"b3\": 0.7,\n    \"b29\": 0.7,\n    \"b18\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b9\": 0.65,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge is improving the efficiency and performance of out-of-order processors by minimizing the penalty of branch mispredictions. The proposed mechanism selectively flushes only the instructions that need to be re-fetched and re-executed, leveraging software-hardware cooperation to identify and retain independent instructions. This approach aims to maximize instruction reuse while requiring minimal changes to the core micro-architecture.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.7,\n    \"b33\": 0.7,\n    \"b35\": 0.6,\n    \"b17\": 0.6,\n    \"b26\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b0\": 0.4,\n    \"b1\": 0.4,\n    \"b9\": 0.4,\n    \"b22\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in branch prediction for out-of-order processors, focusing on improving efficiency and performance. The proposed algorithm involves a hardware-software cooperative mechanism to selectively flush only those instructions that are truly dependent on a mispredicted branch, thereby enhancing performance and reducing energy consumption.\",\n  \"Direct Inspiration\": [\"b34\", \"b10\", \"b11\", \"b33\"],\n  \"Indirect Inspiration\": [\"b35\", \"b17\", \"b26\", \"b18\"],\n  \"Other Inspiration\": [\"b9\", \"b36\", \"b28\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of increasing the efficiency and performance of conventional out-of-order processors by proposing a mechanism that avoids flushing all instructions after a mispredicted branch. This mechanism involves software-hardware cooperation, where the software indicates independent code fragments, and the hardware selectively flushes only the instructions dependent on the branch miss. The goal is to maximize instruction reuse and minimize changes to the core micro-architecture.\",\n    \"Direct Inspiration\": {\n        \"b34\": 1.0,\n        \"b10\": 1.0,\n        \"b35\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b9\": 0.8,\n        \"b18\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.6,\n        \"b40\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the efficiency and performance of out-of-order processors by selectively flushing only the necessary instructions after a branch misprediction. The proposed mechanism leverages a combination of software and hardware cooperation to maximize instruction reuse and minimize changes to the core micro-architecture.\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.7,\n    \"b33\": 0.7,\n    \"b3\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b18\": 0.6,\n    \"b35\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```"], "61a839675244ab9dcbb150ba": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the transferability of supervised pretraining methods compared to unsupervised pretraining methods. It identifies the MLP projector as a key factor for enhancing transferability and proposes the SL-MLP model, which incorporates an MLP projector into supervised learning. The paper demonstrates that the MLP projector preserves intra-class variation, reduces feature distribution distance, and decreases feature redundancy, thereby improving transferability. Extensive experiments and theoretical analysis support these findings.\",\n  \"Direct Inspiration\": [\"b7\", \"b9\", \"b23\", \"b24\"],\n  \"Indirect Inspiration\": [\"b4\", \"b15\", \"b18\", \"b52\"],\n  \"Other Inspiration\": [\"b16\", \"b41\", \"b57\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenge of improving the transferability of supervised pretraining methods in computer vision tasks. It proposes adding a multilayer perceptron (MLP) projector to supervised learning (SL) methods to enhance their transferability, making them comparable or even superior to unsupervised pretraining methods. The MLP projector is shown to preserve intra-class variation, reduce feature distribution distance, and decrease feature redundancy, thereby improving performance on downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b7\": 0.95,\n    \"b15\": 0.9,\n    \"b23\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b17\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b19\": 0.7,\n    \"b52\": 0.7,\n    \"b53\": 0.7,\n    \"b57\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the transferability of supervised pretraining methods in computer vision by introducing an MLP projector. The authors assert that the MLP projector, previously mainly used in unsupervised learning, is crucial for enhancing transferability. They demonstrate empirically and theoretically that adding an MLP projector can make supervised pretraining methods perform comparably or even better than unsupervised ones.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b23\": 0.85,\n    \"b24\": 0.85,\n    \"b41\": 0.8,\n    \"b57\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b16\": 0.7,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b8\": 0.6,\n    \"b10\": 0.6,\n    \"b15\": 0.6,\n    \"b17\": 0.6,\n    \"b19\": 0.6,\n    \"b27\": 0.6,\n    \"b38\": 0.6,\n    \"b46\": 0.6,\n    \"b52\": 0.6,\n    \"b53\": 0.6,\n    \"b56\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the transferability of supervised pretraining methods in computer vision. It identifies the multilayer perceptron (MLP) projector as a key factor for enhancing transferability, which has been under-explored in previous works. The authors propose adding an MLP projector to supervised learning methods to bridge the transferability gap between supervised and unsupervised pretraining.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b23\": 0.95,\n    \"b24\": 0.9,\n    \"b41\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b9\": 0.75,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b48\": 0.7,\n    \"b27\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the transferability of supervised pretraining methods, which traditionally perform worse than unsupervised methods on various transfer tasks. The key proposition and novel contribution of the paper is the insertion of a Multilayer Perceptron (MLP) projector before the classifier in supervised learning (SL-MLP), which significantly enhances the transferability of these models. The authors provide empirical evidence and theoretical analysis to prove that the MLP projector is the core factor for this improvement. They demonstrate that adding an MLP projector can reduce feature redundancy, preserve intra-class feature variation, and decrease feature distribution distance between pretraining and evaluation datasets, leading to better performance on downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b24\": 0.85,\n    \"b41\": 0.8,\n    \"b57\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b9\": 0.7,\n    \"b17\": 0.7,\n    \"b16\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b18\": 0.6,\n    \"b52\": 0.55\n  }\n}\n```"], "6078310791e011f5ecc9dc49": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of explaining graph neural networks (GNNs) with a novel method called Gem. The primary challenge is the lack of interpretability in GNNs, which hinders their application in decision-critical settings. The proposed solution, Gem, uses principles of causality, particularly Granger causality, to provide interpretable explanations without requiring knowledge of the internal model structure of the target GNN.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b14\": 0.7,\n    \"b23\": 0.7,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b21\": 0.5,\n    \"b0\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of providing interpretable explanations for Graph Neural Networks (GNNs) using a new methodology called Gem, based on causal explanation models. The primary challenge is that existing GNNs operate as black boxes, lacking explicit declarative knowledge representations, which hinders their applicability in decision-critical settings. The proposed solution leverages the principles of Granger causality to generate interpretable explanations for any GNN without requiring access to the internal model structure or parameters.\",\n  \"Direct Inspiration\": [\"b4\", \"b23\"],\n  \"Indirect Inspiration\": [\"b1\", \"b18\"],\n  \"Other Inspiration\": [\"b0\", \"b6\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is the lack of interpretability in graph neural networks (GNNs) despite their predictive success. The proposed algorithm, Gem, aims to provide interpretable explanations for any GNNs on graphs using causal explanation models, specifically leveraging the concept of Granger causality to train a graph generative model for this purpose.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b4\": 1,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b14\": 0.8,\n    \"b22\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed by the paper is the lack of interpretability in graph neural networks (GNNs). Existing methods like PGExplainer rely on internal model structures and motifs, which may not be available or reasonable for all datasets. This limits the generalizability and applicability of GNNs in decision-critical settings.\",\n    \"inspirations\": \"The paper introduces a novel methodology called Gem, which uses causal explanation models, specifically built upon Granger causality, to provide interpretable explanations for any GNNs without requiring prior knowledge of the internal model structure or motifs.\"\n  },\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b4\": 0.95,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.85,\n    \"b22\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b12\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the lack of interpretability and explanatory structure in graph neural networks (GNNs), which hinders their applicability in decision-critical settings. The proposed algorithm, Gem, aims to provide interpretable explanations for GNNs using causal explanation models. The methodology builds on the notion of Granger causality, incorporating various graph rules to ensure valid and human-intelligible explanations. The approach is model-agnostic and does not require retraining the original model.\",\n    \"Direct Inspiration\": [\"b4\", \"b5\"],\n    \"Indirect Inspiration\": [\"b14\", \"b18\", \"b22\", \"b23\"],\n    \"Other Inspiration\": [\"b1\", \"b3\", \"b6\"]\n}\n```"], "619798dc5244ab9dcb152293": ["```json\n{\n  \"Summary\": \"This paper reviews existing graph neural network (GNN) models for node classification, introduces a new taxonomy of these models, and presents several popular algorithms for each category. It compares these algorithms based on comprehensive experiments and provides an objective analysis. The paper also discusses challenges of existing GNN models and suggests future research directions.\",\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [\n    \"b30\",\n    \"b31\",\n    \"b32\",\n    \"b33\",\n    \"b34\",\n    \"b35\",\n    \"b36\",\n    \"b37\",\n    \"b38\"\n  ],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the application of neural networks to graph data for the node classification task. The paper reviews various graph neural network (GNN) models, introduces a new taxonomy of these models, and compares several popular algorithms through comprehensive experiments. The authors discuss the challenges of existing GNN models and suggest future research directions.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.9,\n    \"b31\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.6,\n    \"b37\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper outlines the challenges of applying neural network techniques to graph structures, which have arbitrary sizes, complex topological structures, and unfixed node ordering. The paper reviews existing graph neural network (GNN) models for node classification, introduces a new taxonomy of these models, presents several popular algorithms for each category, and provides a comprehensive experimental comparison. The challenges and future research directions for GNNs are also discussed.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.9,\n    \"b31\": 0.9,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7,\n    \"b36\": 0.7,\n    \"b37\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.6,\n    \"b40\": 0.6,\n    \"b41\": 0.6,\n    \"b42\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges of applying neural networks to graph data due to its non-Euclidean structure, arbitrary size, complex topology, and unfixed node ordering. It introduces various graph neural network models for node classification, presents a new taxonomy of these models, and compares several popular algorithms through comprehensive experiments.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.9,\n    \"b31\": 0.85,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.75,\n    \"b34\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.6,\n    \"b37\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the difficulty in applying existing neural network paradigms to graph-structured data due to its non-Euclidean nature. The paper proposes a comprehensive review of graph neural network (GNN) models for node classification, introduces a new taxonomy of these models, compares several popular algorithms based on comprehensive experiments, and discusses existing challenges and future research directions.\",\n    \"Direct Inspiration\": {\n        \"b30\": 0.9,\n        \"b31\": 0.9,\n        \"b32\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b33\": 0.7,\n        \"b34\": 0.7,\n        \"b35\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b36\": 0.6,\n        \"b37\": 0.6,\n        \"b38\": 0.6\n    }\n}\n```"], "61e781655244ab9dcbf9a418": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality, real-time, audio-driven talking-face videos. It introduces a novel Dynamic Convolution Kernel (DCK) technique to improve multi-modal fusion in a fully convolutional neural network, overcoming limitations of prior methods that rely on fixed positions or parametric models.\",\n  \"Direct Inspiration\": [\"b4\", \"b25\"],\n  \"Indirect Inspiration\": [\"b18\", \"b20\", \"b21\"],\n  \"Other Inspiration\": [\"b1\", \"b3\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in generating real-time, high-quality, identity-preserving talking-face videos synchronized with input audio, incorporating natural head motion. It proposes a novel Dynamic Convolution Kernel (DCK) technique within a fully convolutional neural network (FCNN) framework to effectively fuse audio and video modalities.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b4\": 0.9,\n    \"b25\": 0.95,\n    \"b26\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.75,\n    \"b21\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.65,\n    \"b22\": 0.65,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"aligning multi-modal features from audio and video\",\n      \"preserving identity and achieving high-quality synchronization between audio and video\",\n      \"handling different head poses and natural head motion\"\n    ],\n    \"inspirations\": [\n      \"developing a novel Dynamic Convolution Kernel (DCK) technique\",\n      \"creating a fully convolutional network (FCNN) for multi-modal generation tasks\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b20\", \"b21\", \"b22\", \"b23\", \"b24\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b1\", \"b4\", \"b3\", \"b25\", \"b26\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b18\", \"b19\", \"b8\", \"b9\", \"b10\", \"b38\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality audio-driven talking-face videos that can handle diverse head poses and natural head motion. The proposed solution is a novel dynamic convolution kernel (DCK) technique integrated into a fully convolutional neural network (FCNN), which adapts to different audio inputs to produce realistic talking-face videos. The approach aims to overcome limitations of existing methods, such as fixed frame alignment and the use of parametric models, by leveraging DCKs that change dynamically with different inputs.\",\n  \"Direct Inspiration\": [\"b20\", \"b21\", \"b23\", \"b24\", \"b25\"],\n  \"Indirect Inspiration\": [\"b3\", \"b4\", \"b26\"],\n  \"Other Inspiration\": [\"b18\", \"b1\", \"b8\", \"b9\", \"b10\", \"b38\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating high-quality, real-time audio-driven talking-face videos that maintain identity, synchronization, and natural head motion. It introduces a novel Dynamic Convolution Kernel (DCK) technique to work with Fully Convolutional Neural Networks (FCNN) for handling multi-modal inputs, leveraging pre-trained audio networks and U-net architecture.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b4\": 0.8,\n        \"b20\": 0.85,\n        \"b25\": 0.8,\n        \"b26\": 0.75\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.65,\n        \"b21\": 0.7,\n        \"b24\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b22\": 0.55,\n        \"b23\": 0.55\n    }\n}\n```"], "60cad57691e011b32937411b": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges the paper addresses include the difficulty of deriving effective sentence embeddings from pre-trained BERT models without supervision and the sub-optimal performance of existing methods relying on pooling strategies.\",\n    \"inspirations\": \"The paper takes inspiration from contrastive learning methods in computer vision and leverages a self-guided mechanism to improve sentence representation learning without requiring data augmentation.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b11\", \"b21\", \"b29\", \"b27\", \"b39\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b12\", \"b22\", \"b24\", \"b30\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b10\", \"b15\", \"b25\", \"b26\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of deriving effective sentence embeddings from pre-trained BERT models without labeled datasets. The authors propose a self-guided contrastive learning method leveraging intermediate BERT representations as positive samples and optimizing the NT-Xent loss for better sentence representation learning.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b11\", \"b21\", \"b29\", \"b27\", \"b38\", \"b39\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b12\", \"b24\", \"b22\", \"b14\", \"b16\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b10\", \"b30\", \"b17\", \"b26\", \"b15\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively deriving sentence embeddings from pre-trained BERT models, especially in unsupervised settings. It proposes a novel contrastive learning method with a self-guidance mechanism to optimize BERT for sentence representation without requiring data augmentation. The method leverages intermediate BERT hidden representations as positive samples, and customizes the NT-Xent loss for better performance.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b29\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b38\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The paper addresses the challenge of deriving effective sentence embeddings from pre-trained Transformer models like BERT without supervision. It identifies the limitations of current methods that rely on naive use of the [CLS] token or mean pooling of the last layer.\",\n    \"inspirations\": \"The paper is inspired by various prior works that have explored contrastive learning and different pooling methods for sentence embeddings. It also draws on the NT-Xent loss used in computer vision for contrastive learning.\"\n  },\n  \"Direct Inspiration\": {\n    \"References\": [\"b11\", \"b21\", \"b29\", \"b39\"]\n  },\n  \"Indirect Inspiration\": {\n    \"References\": [\"b12\", \"b27\", \"b30\"]\n  },\n  \"Other Inspiration\": {\n    \"References\": [\"b7\", \"b38\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of deriving effective sentence embeddings from pre-trained BERT models without supervision. It proposes a novel contrastive learning method leveraging self-guidance to optimize BERT sentence embeddings by recycling intermediate hidden representations as positive samples.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b21\": 0.9,\n    \"b29\": 0.8,\n    \"b39\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b27\": 0.6,\n    \"b38\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b37\": 0.5,\n    \"b22\": 0.4\n  }\n}\n```"], "60c58a2791e011368ce8c24f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning robust and transferable graph neural networks (GNNs) in a self-supervised manner without extensive labels. It proposes a novel method called Adversarial Graph Contrastive Learning (AD-GCL) which pairs Graph Contrastive Learning (GCL) with adversarial training to optimize augmentation strategies and maximize mutual information.\",\n  \"Direct Inspiration\": {\n    \"b47\": 1,\n    \"b48\": 1,\n    \"b24\": 0.9,\n    \"b40\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b22\": 0.7,\n    \"b25\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.6,\n    \"b31\": 0.6,\n    \"b41\": 0.5,\n    \"b42\": 0.5,\n    \"b43\": 0.5,\n    \"b44\": 0.5,\n    \"b45\": 0.5,\n    \"b46\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of self-supervised learning for Graph Neural Networks (GNNs) due to the difficulty in obtaining large amounts of annotated graph data. It proposes a novel principle called Adversarial Graph Contrastive Learning (AD-GCL), which pairs Graph Contrastive Learning (GCL) with adversarial training to reduce redundant information and improve the robustness and transferability of GNNs. The primary inspiration comes from the InfoMax principle and the Information Bottleneck (IB) principle.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b24\": 1.0,\n    \"b40\": 1.0,\n    \"b47\": 1.0,\n    \"b48\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b21\": 0.8,\n    \"b22\": 0.8,\n    \"b23\": 0.8,\n    \"b25\": 0.8,\n    \"b26\": 0.8,\n    \"b27\": 0.8,\n    \"b28\": 0.8,\n    \"b29\": 0.8,\n    \"b30\": 0.8,\n    \"b31\": 0.8,\n    \"b39\": 0.8,\n    \"b41\": 0.8,\n    \"b42\": 0.8,\n    \"b43\": 0.8,\n    \"b44\": 0.8,\n    \"b45\": 0.8,\n    \"b46\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b7\": 0.5,\n    \"b8\": 0.5,\n    \"b14\": 0.5,\n    \"b15\": 0.5,\n    \"b51\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Graph Neural Networks (GNNs) in a self-supervised manner to avoid redundant information capture, which can deteriorate performance in downstream tasks. The authors propose a novel algorithm, AD-GCL, which pairs Graph Contrastive Learning (GCL) with adversarial training to optimize graph data augmentation strategies and avoid overemphasis on node proximity. The AD-GCL framework aims to maximize the mutual information between the original and augmented graphs while minimizing redundant information, inspired by the Information Bottleneck (IB) principle.\",\n  \"Direct Inspiration\": {\n    \"b41\": 1.0,\n    \"b42\": 1.0,\n    \"b43\": 1.0,\n    \"b44\": 1.0,\n    \"b45\": 1.0,\n    \"b46\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9,\n    \"b22\": 0.9,\n    \"b23\": 0.9,\n    \"b24\": 0.9,\n    \"b25\": 0.9,\n    \"b26\": 0.9,\n    \"b27\": 0.9,\n    \"b28\": 0.9,\n    \"b29\": 0.9,\n    \"b30\": 0.9,\n    \"b31\": 0.9,\n    \"b39\": 0.9,\n    \"b47\": 0.9,\n    \"b48\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.8,\n    \"b50\": 0.8,\n    \"b51\": 0.8,\n    \"b52\": 0.8,\n    \"b53\": 0.8,\n    \"b54\": 0.8,\n    \"b55\": 0.8,\n    \"b56\": 0.8,\n    \"b57\": 0.8,\n    \"b58\": 0.8,\n    \"b59\": 0.8,\n    \"b60\": 0.8,\n    \"b61\": 0.8,\n    \"b62\": 0.8,\n    \"b63\": 0.8,\n    \"b64\": 0.8,\n    \"b65\": 0.8,\n    \"b66\": 0.8,\n    \"b67\": 0.8,\n    \"b68\": 0.8,\n    \"b69\": 0.8,\n    \"b70\": 0.8,\n    \"b71\": 0.8,\n    \"b72\": 0.8,\n    \"b73\": 0.8,\n    \"b74\": 0.8,\n    \"b75\": 0.8,\n    \"b76\": 0.8,\n    \"b77\": 0.8,\n    \"b78\": 0.8,\n    \"b79\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in graph representation learning (GRL) with a focus on developing a novel self-supervised learning framework for graph neural networks (GNNs). The proposed algorithm, AD-GCL, combines graph contrastive learning (GCL) with adversarial training to optimize the augmentation process, thereby removing redundant information and capturing essential information relevant to downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b24\": 1,\n    \"b30\": 0.9,\n    \"b39\": 0.9,\n    \"b40\": 1,\n    \"b47\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.5,\n    \"b42\": 0.5,\n    \"b48\": 0.5,\n    \"b72\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning robust and transferable graph neural networks (GNNs) in a self-supervised manner by proposing a novel principle termed Adversarial Graph Contrastive Learning (AD-GCL). AD-GCL pairs graph contrastive learning (GCL) with adversarial training to avoid capturing redundant information during representation learning. The proposed method optimizes a mutual information maximization principle and a learnable graph data augmentation strategy to enhance the performance of GNNs on various graph-level tasks.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b24\": 1.0,\n    \"b47\": 1.0,\n    \"b40\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b30\": 0.8,\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.6,\n    \"b41\": 0.6,\n    \"b52\": 0.6,\n    \"b72\": 0.6\n  }\n}\n```"], "6059b75e91e011ed950a5a13": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of redundant and noisy information in graph-structured data and the difficulty in obtaining subgraph-level annotations for supervised learning. It proposes a novel Subgraph Information Bottleneck (SIB) algorithm to recognize compressed yet predictive subgraphs without requiring explicit subgraph annotations. The approach leverages mutual information estimators and bi-level optimization to enhance graph classification, interpretation, and denoising.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.95,\n    \"b30\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.80,\n    \"b22\": 0.75,\n    \"b23\": 0.70,\n    \"b24\": 0.65,\n    \"b31\": 0.60\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.50,\n    \"b1\": 0.50,\n    \"b3\": 0.50,\n    \"b12\": 0.50,\n    \"b13\": 0.50\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of recognizing predictive yet compressed subgraphs in graph-structured data. The authors propose the Subgraph Information Bottleneck (SIB) to identify subgraphs that are most predictive of graph labels without requiring subgraph annotations. SIB leverages the mutual information estimator from Deep Variational Information Bottleneck (VIB) and introduces a bi-level optimization scheme with a novel connectivity loss to stabilize training.\",\n  \"Direct Inspiration\": [\"b23\", \"b25\", \"b30\"],\n  \"Indirect Inspiration\": [\"b21\", \"b22\", \"b24\"],\n  \"Other Inspiration\": [\"b0\", \"b1\", \"b9\", \"b13\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in graph learning, specifically focusing on recognizing predictive subgraphs in noisy and redundant graph structures. The proposed Subgraph Information Bottleneck (SIB) algorithm aims to identify a compressed subgraph that retains maximum predictive information about the graph labels without explicit subgraph-level annotations. This is achieved through a novel bi-level optimization scheme, continuous relaxation strategy, and connectivity loss to stabilize the training process.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.95,\n    \"b30\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.75,\n    \"b9\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.65,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of recognizing predictive and compressed subgraphs in graph data, particularly dealing with noisy and redundant structures. The authors propose a novel Subgraph Information Bottleneck (SIB) principle, advancing the Information Bottleneck (IB) principle to handle irregular graph data, aiming to identify subgraphs that most predict certain graph labels without explicit subgraph annotations.\",\n  \"Direct Inspiration\": [\"b25\", \"b30\"],\n  \"Indirect Inspiration\": [\"b0\", \"b1\", \"b9\", \"b12\", \"b13\", \"b21\"],\n  \"Other Inspiration\": [\"b23\", \"b24\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of noisy and redundant structures in real-world graphs, proposing the Subgraph Information Bottleneck (SIB) to recognize compressed subgraphs crucial for graph label prediction. This is inspired by the Information Bottleneck (IB) principle but adapted for irregular graph data. The SIB introduces a bi-level optimization scheme and a novel connectivity loss to stabilize training and ensure effective subgraph recognition.\",\n  \"Direct Inspiration\": [\"b25\", \"b30\"],\n  \"Indirect Inspiration\": [\"b21\", \"b23\"],\n  \"Other Inspiration\": [\"b0\", \"b1\", \"b9\"]\n}\n```"], "60c567e691e011368ce8c0f2": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of combining multiple self-supervised learning (SSL) tasks for graph neural networks (GNNs) to learn better node representations without labeled data. The main contributions include proposing a pseudo-homophily measure to evaluate node embeddings, developing an automated framework for SSL task search (AUTOSSL), and demonstrating significant performance improvements on various datasets.\",\n  \"Direct Inspiration\": [\n    \"b12\",\n    \"b13\",\n    \"b15\",\n    \"b16\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b7\",\n    \"b8\",\n    \"b9\",\n    \"b11\"\n  ],\n  \"Other Inspiration\": [\n    \"b17\",\n    \"b18\",\n    \"b19\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper aims to address the challenge of automatically composing self-supervised learning (SSL) tasks to improve node representation learning on graphs. The authors propose an automated framework, AUTOSSL, which employs pseudo-homophily as a surrogate measure to guide the search for optimal task weights without using labeled data. The key contributions include the introduction of the pseudo-homophily measure, two strategies for efficient task search (evolution algorithm and differentiable search), and extensive experimental validation demonstrating significant performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b13\": 0.85,\n    \"b14\": 0.8,\n    \"b15\": 0.9,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b31\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.8,\n    \"b20\": 0.75,\n    \"b29\": 0.7,\n    \"b39\": 0.7,\n    \"b41\": 0.65,\n    \"b42\": 0.65,\n    \"b44\": 0.65,\n    \"b45\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning effective node representations from graph data using self-supervised learning (SSL) tasks. The key innovation is the proposed AUTOSSL framework, which combines multiple SSL tasks to optimize node embeddings without labeled data. It introduces a pseudo-homophily measure to guide the search for optimal task combinations and employs both evolutionary and differentiable search strategies.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b13\": 0.8,\n    \"b15\": 0.85,\n    \"b16\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b29\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatically composing various self-supervised learning (SSL) tasks to learn better node representations in graph neural networks (GNNs). It proposes an automated framework called AUTOSSL that uses a pseudo-homophily measure to evaluate node embeddings without access to labeled data. The framework employs two strategies: an evolutionary algorithm and a differentiable search via meta-gradient descent, to automatically search for optimal task weights.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b11\": 0.9,\n    \"b12\": 0.95,\n    \"b13\": 0.95,\n    \"b14\": 0.9,\n    \"b15\": 0.85,\n    \"b16\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.75,\n    \"b18\": 0.75,\n    \"b19\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b20\": 0.65,\n    \"b21\": 0.65,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6,\n    \"b36\": 0.6,\n    \"b37\": 0.55,\n    \"b38\": 0.55,\n    \"b39\": 0.55,\n    \"b40\": 0.55,\n    \"b41\": 0.5,\n    \"b42\": 0.5,\n    \"b43\": 0.5,\n    \"b44\": 0.5,\n    \"b45\": 0.5,\n    \"b46\": 0.5,\n    \"b47\": 0.5,\n    \"b48\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of combining multiple self-supervised learning (SSL) tasks for graph neural networks (GNNs) to improve node representation learning without labeled data. The key innovations include the introduction of a pseudo-homophily measure to evaluate node embeddings and the development of an automated framework, AUTOSSL, which uses evolutionary and differentiable search strategies to optimize the combination of SSL tasks.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b13\": 1.0,\n    \"b15\": 1.0,\n    \"b16\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b29\": 0.7,\n    \"b30\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b32\": 0.5,\n    \"b33\": 0.5,\n    \"b34\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```"], "61ee18dc5244ab9dcb6a9330": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of Knowledge Graph (KG) completion by proposing a novel method called TKGC. It leverages noisy data from diverse web pages and prior knowledge in a KG symbiotically to add missing facts inside the KG and discover new facts outside the KG, while resolving data noises. Key features include a holistic fact scoring model, value alignment networks, and a semi-supervised truth inference model.\",\n  \"Direct Inspiration\": {\n    \"b57\": 1.0,\n    \"b59\": 1.0,\n    \"b64\": 1.0,\n    \"b67\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b58\": 0.8,\n    \"b60\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b62\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incomplete Knowledge Graphs (KGs) and proposes a novel trustworthy KG completion method called TKGC. The method leverages noisy data from diverse web pages and prior knowledge in a KG symbiotically to add missing facts both inside and outside the KG while resolving data noises. The key contributions include a holistic fact scoring model, value alignment networks, and a semi-supervised truth inference model.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b57\": 0.8,\n    \"b59\": 0.8,\n    \"b64\": 0.8,\n    \"b67\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b58\": 0.7,\n    \"b60\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of completing Knowledge Graphs (KGs) by introducing a novel method called TKGC. This method leverages noisy data from diverse web pages and prior knowledge in a KG symbiotically to improve KG completion. The paper proposes three main components: holistic fact scoring, value alignment networks, and semi-supervised truth inference to handle the challenges of multi-sourced noisy data and unseen entities.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b57\": 0.9,\n    \"b58\": 0.9,\n    \"b60\": 0.9,\n    \"b67\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b54\": 0.8,\n    \"b66\": 0.8,\n    \"b62\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b19\": 0.7,\n    \"b64\": 0.7,\n    \"b71\": 0.7,\n    \"b72\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of Knowledge Graph (KG) completion, particularly the incorporation of noisy data from multiple sources and the inclusion of unseen entities. The proposed method, TKGC, uses a holistic fact scoring model, value alignment networks, and semi-supervised truth inference to tackle these challenges.\",\n    \"Direct Inspiration\": {\n        \"b57\": 1.0,\n        \"b59\": 1.0,\n        \"b64\": 1.0,\n        \"b67\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b58\": 0.8,\n        \"b60\": 0.8,\n        \"b6\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b54\": 0.6,\n        \"b66\": 0.6,\n        \"b11\": 0.6,\n        \"b16\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the incompleteness of knowledge graphs (KGs), particularly with regard to noisy data from diverse web pages. The proposed solution, TKGC, leverages noisy data and prior knowledge symbiotically to add missing facts inside the KG and discover new facts outside the KG. The core components include a holistic fact scoring model, value alignment networks, and a semi-supervised truth inference mechanism.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b57\": 1.0,\n    \"b59\": 1.0,\n    \"b64\": 1.0,\n    \"b67\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b54\": 0.8,\n    \"b66\": 0.8,\n    \"b58\": 0.8,\n    \"b60\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b55\": 0.6,\n    \"b62\": 0.6,\n    \"b71\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```"], "60377c2991e01137f314dc4f": ["```json\n{\n  \"Summary\": \"The paper addresses the processing overhead issues in binary optimizers, specifically BOLT, by proposing two key techniques: parallel processing and selective optimizations. The proposed methods significantly reduce overheads by leveraging function-level parallelism and optimizing only portions of the binary, respectively.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b15\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b9\": 0.6,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the significant overheads in binary optimization by introducing two main techniques: parallel processing and selective optimizations in the context of the BOLT binary optimizer. The challenges include overcoming the processing time and memory usage, especially for large-scale production environments. By employing these techniques, the authors demonstrate substantial reductions in overheads while maintaining or even improving the performance of the optimized binaries.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b8\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high processing overheads in binary optimization, particularly when using BOLT, a state-of-the-art binary optimizer. It introduces two novel techniques: parallel processing and selective optimizations, to reduce the overheads significantly. The paper demonstrates substantial reductions in processing time and memory usage through these methods.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b15\": 0.8,\n    \"b19\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of processing overheads in binary optimization using BOLT. It proposes parallel processing and selective optimizations to reduce build time and memory usage while maintaining performance. The proposed techniques are evaluated on real-world data-center and open-source workloads, demonstrating significant overhead reductions.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b14\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on addressing the scalability issues of binary optimization specifically in the context of BOLT, a binary optimizer. It introduces two techniques to reduce processing overheads: parallel processing and selective optimizations. These techniques are aimed at significantly reducing BOLT's processing time and memory usage while maintaining or improving the performance of the optimized binaries.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b12\": 0.6,\n    \"b4\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```"], "6163ab265244ab9dcbf95df5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of inferring per-AS BGP community usage behavior. It introduces a novel passive algorithm to categorize ASes based on their tagging and forwarding behaviors. The algorithm is validated using simulations and real-world data, and its application to data from major route collectors demonstrates high accuracy.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b25\", \"b10\", \"b7\", \"b14\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b2\", \"b8\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b6\", \"b23\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inferring per-AS BGP community usage behavior. It introduces a novel passive algorithm that utilizes constraints to classify ASes based on their tagging and forwarding behaviors. The algorithm is validated through simulations and real-world experiments, and its application to data from major route collectors shows high accuracy. This work fills a critical gap in understanding BGP community usage, which can enhance various network measurement and operational tasks.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b10\": 0.9,\n    \"b7\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.6,\n    \"b22\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.4,\n    \"b23\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inferring per-AS (Autonomous System) BGP (Border Gateway Protocol) community usage behavior, which is often difficult to understand due to the lack of public community conventions and the arbitrary modifications by individual ASes. The authors propose a novel passive algorithm to infer this behavior by grouping ASes into categories based on their tagging and forwarding actions. The algorithm is validated through simulations and real-world experiments using data from route collectors, demonstrating high accuracy. Understanding per-AS community usage is crucial for various measurement and operational network research efforts.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b25\": 1,\n    \"b14\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b23\": 0.8,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b8\": 0.6,\n    \"b15\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inferring per-AS BGP community usage behavior, proposing a novel passive algorithm to classify ASes based on their tagging and forwarding behaviors. The algorithm is validated through simulations and real-world experiments, and its applicability is demonstrated using data from major route collectors.\",\n  \"Direct Inspiration\": [\"b6\", \"b25\", \"b14\"],\n  \"Indirect Inspiration\": [\"b10\", \"b7\"],\n  \"Other Inspiration\": [\"b4\", \"b22\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the difficulty in understanding BGP community usage due to the lack of a globally defined semantic and the arbitrary modifications made by individual ASes. The proposed algorithm aims to infer per-AS community usage behavior by categorizing ASes into tagger, silent, cleaner, and forward groups. The algorithm is validated using simulations and real-world data, and its results are made publicly available.\",\n  \"Direct Inspiration\": [\"b25\", \"b14\"],\n  \"Indirect Inspiration\": [\"b7\", \"b10\", \"b23\"],\n  \"Other Inspiration\": [\"b2\", \"b8\"]\n}\n```"], "5ff68a77d4150a363cccd680": ["```json\n{\n  \"Summary\": \"The primary challenges in the paper revolve around translating traditional OS concepts to FPGAs and addressing the difficulties in programming, deploying, and securely managing FPGAs in data centers. The proposed solution, Coyote, combines OS abstractions into a unified runtime for FPGA-based applications, providing functionalities such as a uniform execution environment, virtual memory, physical memory management, communication, scheduling, and networking with minimal overhead.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b62\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b15\": 0.8,\n    \"b61\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b38\": 0.7,\n    \"b50\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the difficulties in programming, deploying, and securely managing FPGAs in datacenters and cloud providers. The authors propose Coyote, a comprehensive, holistic approach combining a coherent set of OS abstractions in a single unified runtime for FPGA-based applications. Coyote provides a uniform execution environment, portability layer, virtual memory, physical memory management, communication, spatial and temporal scheduling, and an analog of software processes or tasks for user logic.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b62\": 0.95,\n    \"b13\": 0.9,\n    \"b61\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b50\": 0.65,\n    \"b57\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.5,\n    \"b47\": 0.45\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of integrating traditional OS concepts into FPGA-based systems to create a comprehensive, unified runtime environment. The proposed solution, Coyote, combines OS abstractions such as virtual memory, process management, and scheduling, enabling efficient resource management and application portability on FPGAs.\",\n    \"Direct Inspiration\": {\n        \"b31\": 0.9,\n        \"b62\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.7,\n        \"b15\": 0.7,\n        \"b61\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.6,\n        \"b36\": 0.6,\n        \"b43\": 0.6,\n        \"b50\": 0.6,\n        \"b57\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of implementing traditional OS concepts in an FPGA environment and introduces Coyote, a comprehensive runtime that combines OS abstractions for FPGA-based applications. The primary challenges include effective resource allocation, scheduling, virtual memory management, and providing a uniform execution environment.\",\n    \"Direct Inspiration\": {\n        \"b31\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.8,\n        \"b15\": 0.8,\n        \"b61\": 0.8,\n        \"b62\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b50\": 0.7,\n        \"b57\": 0.7,\n        \"b43\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the difficulties in programming, deploying, and securely managing FPGAs in datacenters and cloud environments. The paper introduces Coyote, a unified runtime for FPGA-based applications that combines OS abstractions to manage resources, provide a uniform execution environment, and support multi-tenancy with minimal overhead.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b62\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b15\": 0.8,\n    \"b61\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b45\": 0.5,\n    \"b47\": 0.5\n  }\n}\n```"], "6051ce4091e011c24e5990b0": ["```json\n{\n  \"Summary\": \"The paper addresses the problem of class imbalance in node classification on graphs using Graph Neural Networks (GNNs). The authors propose a novel framework, GraphSMOTE, which extends the SMOTE algorithm to generate synthetic minority nodes in an expressive embedding space acquired by a GNN-based feature extractor. The framework also includes an edge generator to predict links for the synthetic nodes, forming an augmented balanced graph to facilitate node classification by GNNs.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b13\": 0.8,\n    \"b18\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of class imbalance in node classification tasks on graphs using Graph Neural Networks (GNNs). It introduces a novel framework, GraphSMOTE, which extends traditional synthetic minority oversampling techniques to the graph domain. The framework includes a feature extractor, a synthetic node generator, an edge generator, and a GNN-based classifier. The main contributions are the introduction of the class imbalance problem for GNNs, the design of a new framework to address this issue, and empirical validation showing the effectiveness of GraphSMOTE.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b13\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b21\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of imbalanced node classification in graphs using graph neural networks (GNNs). It introduces a novel framework, GraphSMOTE, which extends synthetic minority oversampling techniques to graph data. The key innovations include using an edge predictor to generate reliable relation information and performing interpolation in an intermediate embedding space to ensure high-quality synthetic samples.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b13\": 0.7,\n    \"b18\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b22\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of imbalanced node classification in graphs using a novel framework called GraphSMOTE. The main contribution is the extension of synthetic minority over-sampling techniques to graph data, incorporating an edge predictor and performing interpolation in an intermediate embedding space. The goal is to balance class distribution and improve the performance of GNNs for minority classes.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b13\": 0.8,\n    \"b18\": 0.8,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve addressing the imbalanced node classification problem in graphs using Graph Neural Networks (GNNs). The proposed algorithm, GraphSMOTE, extends traditional over-sampling methods to graph data by generating synthetic minority nodes in the latent space and predicting edges to maintain graph structure.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b13\": 0.7,\n    \"b18\": 0.7,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b43\": 0.5\n  }\n}\n```"], "612c4c2c5244ab9dcbca280d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of extrapolation in transformer language models, particularly focusing on the inefficiencies of existing position embedding methods like sinusoidal, rotary, and T5 bias. It introduces a novel method called Attention with Linear Biases (ALiBi), which eliminates position embeddings and instead uses a linearly decreasing penalty to facilitate efficient extrapolation. The proposed method shows significant improvements in extrapolation capabilities without additional computational costs.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.9,\n    \"b32\": 0.9,\n    \"b38\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.6,\n    \"b2\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b4\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extrapolation in transformer-based language models, specifically the limitations of sinusoidal position embeddings. The proposed solution is Attention with Linear Biases (ALiBi), which eliminates position embeddings and instead uses a linearly decreasing penalty proportional to the distance between key and query pairs. This method is shown to be efficient, maintaining strong performance even with longer sequences and reducing computational costs.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1.0,\n    \"b36\": 0.9,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extrapolation in transformer-based language models, specifically targeting the limitations of sinusoidal position embeddings. The authors propose a novel algorithm called Attention with Linear Biases (ALiBi) to overcome this issue, which introduces a linearly decreasing penalty to attention scores based on the distance between keys and queries. This method eliminates the need for position embeddings and allows for efficient extrapolation while maintaining or improving performance compared to existing methods.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1.0,\n    \"b36\": 0.8,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b2\": 0.6,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b11\": 0.4,\n    \"b15\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inefficiency of transformer-based language models in extrapolating to longer sequences than those they were trained on, particularly due to the limitations of existing position embedding methods like sinusoidal, rotary, and T5 bias. The proposed algorithm, ALiBi (Attention with Linear Biases), introduces a simple, efficient method to facilitate extrapolation by adding a linearly decreasing penalty to attention scores based on the distance between key and query tokens, eliminating the need for position embeddings and enabling training on shorter sequences while maintaining strong performance on longer sequences.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1,\n    \"b32\": 0.9,\n    \"b35\": 0.9,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.7,\n    \"b2\": 0.7,\n    \"b0\": 0.6,\n    \"b4\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficient extrapolation in transformer-based language models. It introduces Attention with Linear Biases (ALiBi) as a novel method to enable better extrapolation without additional computational costs, unlike other methods such as sinusoidal position embeddings and T5 bias.\",\n    \"Direct Inspiration\": [\"b38\", \"b32\", \"b36\", \"b35\"],\n    \"Indirect Inspiration\": [\"b0\", \"b11\", \"b2\"],\n    \"Other Inspiration\": [\"b19\", \"b20\"]\n}\n```"], "612d9dc35244ab9dcbdfa1d4": ["```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper involve leveraging Pretrained Language Models (PLMs) for both Graph-to-Text (G2T) and Text-to-Graph (T2G) generation tasks. The authors propose using Reinforcement Learning (RL), specifically Self-Critical Sequence Training (SCST), to fine-tune PLMs for these tasks. Significant improvements over previous state-of-the-art results for the WebNLG+ 2020 Challenge are demonstrated.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b9\": 0.8,\n    \"b16\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving text-to-graph (T2G) and graph-to-text (G2T) generation tasks using Pretrained Language Models (PLMs) by leveraging Reinforcement Learning (RL), specifically Self-Critical Sequence Training (SCST). The proposed method aims to enhance performance metrics like BLEU, METEOR, and chrF++ by directly optimizing these metrics during training. The work demonstrates improvements over existing methods, particularly those used in the WebNLG+ 2020 Challenge.\",\n  \n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b14\": 0.9\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b48\": 0.8,\n    \"b13\": 0.7,\n    \"b9\": 0.6\n  },\n  \n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b10\": 0.5,\n    \"b20\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of generating text from graphs (G2T) and vice versa (T2G) using pre-trained language models (PLMs) and reinforcement learning (RL). The authors propose using Self-Critical Sequence Training (SCST) for both tasks, achieving state-of-the-art results in the WebNLG+ 2020 Challenge and the TEKGEN dataset.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b9\": 0.85,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b10\": 0.7,\n    \"b20\": 0.65,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.55,\n    \"b11\": 0.5,\n    \"b12\": 0.45,\n    \"b17\": 0.4,\n    \"b14\": 0.35\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating text from graphs (G2T) and graphs from text (T2G) using Pretrained Language Models (PLMs) and proposes the use of Reinforcement Learning (RL), specifically Self-Critical Sequence Training (SCST), to enhance performance. The paper's main contributions include the introduction of RL-based sequence training for bi-directional text and graph generation, demonstration of improved performance over existing systems, and adaptation of the TEKGEN corpus for these tasks.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b13\": 0.7,\n    \"b9\": 0.7,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of generating text from graphs (G2T) and generating graphs from text (T2G) using Pretrained Language Models (PLMs) like T5 and BART. The core contribution is the use of Reinforcement Learning (RL), particularly Self-Critical Sequence Training (SCST), to improve performance for these tasks.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b6\": 0.8,\n    \"b10\": 0.8,\n    \"b20\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b9\": 0.6\n  }\n}\n```"], "605aa227e4510cd7c86afd92": ["```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in text generation using autoregressive models: the discrepancy between training and evaluation objectives, and the exposure bias problem. It proposes an algorithm called GOLD (Generation by Off-policy Learning from Demonstrations) which utilizes off-policy policy gradient with importance weighting, leveraging human judgment approximations as reward functions. The method aims to bridge the gap between training and evaluation by focusing on high-quality output generation.\",\n  \"Direct Inspiration\": [\"b15\", \"b13\", \"b36\"],\n  \"Indirect Inspiration\": [\"b19\", \"b14\", \"b35\"],\n  \"Other Inspiration\": [\"b60\", \"b8\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in text generation using autoregressive models, primarily focusing on the discrepancies between training (using MLE) and evaluation objectives, and the problem of exposure bias. The authors propose an algorithm named GOLD (Generation by Off-policy Learning from Demonstrations) to bridge the gap between training and evaluation by using off-policy policy gradient with importance weighting and reward functions that approximate human judgment of output quality.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b13\": 0.9,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b48\": 0.7,\n    \"b42\": 0.7,\n    \"b43\": 0.7,\n    \"b18\": 0.7,\n    \"b55\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b58\": 0.6,\n    \"b46\": 0.6,\n    \"b20\": 0.6,\n    \"b28\": 0.6,\n    \"b23\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the discrepancies between training and evaluation objectives in autoregressive text generation models, specifically focusing on the issues of over-generalization and exposure bias. The authors propose an algorithm called GOLD, which leverages off-policy policy gradient with importance weighting to better match training and evaluation objectives and improve output quality.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b15\", \"b13\", \"b36\"],\n    \"confidence\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b19\", \"b48\", \"b42\", \"b3\", \"b43\", \"b18\", \"b55\", \"b35\", \"b60\", \"b8\"],\n    \"confidence\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b58\", \"b28\", \"b23\", \"b46\", \"b20\", \"b34\", \"b0\", \"b9\", \"b38\", \"b25\", \"b63\", \"b32\", \"b5\", \"b29\", \"b60\", \"b8\", \"b45\", \"b54\", \"b26\", \"b18\", \"b56\", \"b64\"],\n    \"confidence\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in text generation using autoregressive models, particularly focusing on the discrepancies between training (MLE) and evaluation objectives. The proposed solution, GOLD (Generation by Off-policy Learning from Demonstrations), aims to bridge this gap by using off-policy policy gradient with importance weighting and reward functions that approximate human judgment.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b15\", \"b13\", \"b36\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b60\", \"b8\", \"b46\", \"b20\", \"b34\", \"b0\", \"b9\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b19\", \"b48\", \"b42\", \"b3\", \"b43\", \"b18\", \"b55\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of discrepancies between training and evaluation objectives in text generation models, specifically focusing on issues like over-generalization and exposure bias in maximum likelihood estimation (MLE) approaches. It proposes a novel algorithm called GOLD (Generation by Off-policy Learning from Demonstrations) which uses off-policy policy gradient with importance weighting to optimize the model performance by approximating human judgment of output quality. The results show that GOLD outperforms traditional MLE and reinforcement learning fine-tuning methods in various text generation tasks.\",\n  \"Direct Inspiration\": {\n    \"b46\": 0.9,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b13\": 0.75,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b60\": 0.6,\n    \"b8\": 0.65,\n    \"b58\": 0.5\n  }\n}\n```"], "616ce5a15244ab9dcbacfc10": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the representation power of Message-Passing Graph Neural Networks (MP-GNNs) by incorporating learnable positional encodings (PE). The proposed framework, MP-GNNs-LSPE, decouples structural and positional representations to enhance expressivity while maintaining linear complexity. The approach is validated on standard molecular benchmarks, showing significant performance improvements over state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b36\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b44\": 0.75,\n    \"b50\": 0.7,\n    \"b62\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b30\": 0.55,\n    \"b41\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of message-passing GNNs (MP-GNNs) in differentiating nodes with the same local structure by proposing a novel framework, MP-GNNs-LSPE, which combines structural and positional representations to enhance the expressivity of node embeddings. This approach aims to improve the performance of MP-GNNs while maintaining linear complexity, making it suitable for large-scale applications.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.95,\n    \"b30\": 0.9,\n    \"b18\": 0.85,\n    \"b41\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b62\": 0.75,\n    \"b50\": 0.7,\n    \"b44\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.65,\n    \"b17\": 0.6,\n    \"b63\": 0.55\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving the expressivity of Message-Passing Graph Neural Networks (MP-GNNs) by incorporating learnable structural and positional encodings (LSPE). The proposed approach aims to overcome limitations in differentiating nodes with identical local structures and enhance performance in large-scale applications while maintaining linear complexity.\",\n    \"Direct Inspiration\": {\n        \"b8\": 0.9,\n        \"b30\": 0.9,\n        \"b41\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b36\": 0.7,\n        \"b18\": 0.7,\n        \"b44\": 0.7,\n        \"b62\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.6,\n        \"b50\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the representation power of Message-Passing Graph Neural Networks (MP-GNNs) by introducing a novel framework (MPGNNs-LSPE) that decouples structural and positional information. This approach aims to improve node embedding expressivity while maintaining linear complexity for large-scale applications. The primary inspiration is to alleviate limitations related to node representation in arbitrary graphs, particularly by learning positional representations that can be integrated with structural GNNs.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1.0,\n    \"b18\": 0.95,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.85,\n    \"b30\": 0.8,\n    \"b41\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b50\": 0.7,\n    \"b62\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the representation power of Message-Passing Graph Neural Networks (MP-GNNs) by proposing a novel framework (MPGNNs-LSPE) that learns both structural and positional representations. This approach aims to overcome the limitations of traditional MP-GNNs, such as their inability to differentiate nodes with the same 1-hop local structure and issues related to the over-squashing phenomenon in long-distance node interactions.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.95,\n    \"b44\": 0.90,\n    \"b18\": 0.88\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.85,\n    \"b30\": 0.82,\n    \"b41\": 0.80,\n    \"b50\": 0.78\n  },\n  \"Other Inspiration\": {\n    \"b62\": 0.75,\n    \"b29\": 0.72\n  }\n}\n```"], "6023dd1991e0119b5fbd98ed": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently and accurately representing 3D molecular graphs in machine learning models. It proposes a novel spherical message passing (SMP) method that uses edge-based 1-hop information to significantly reduce computational complexity while maintaining complete and physically meaningful 3D molecular representations. This method is realized in the SphereNet model, which outperforms existing methods without increasing computational costs.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.6,\n    \"b29\": 0.5,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a novel message passing method for 3D molecular graphs. The proposed algorithm, Spherical Message Passing (SMP), aims to efficiently and accurately represent 3D molecules by incorporating relative 3D information such as distance, angle, and torsion. This method reduces training complexity and improves computational efficiency, making it suitable for large-scale molecules.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b25\": 0.8,\n    \"b45\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b40\": 0.7,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper aims to develop a novel message passing method for 3D molecular graphs, specifically focusing on a spherical message passing (SMP) scheme that is both efficient and approximately complete in representing 3D molecules. The main challenges include high computational complexity in existing methods and the need for accurate and invariant 3D molecular learning. SphereNet is introduced as a solution, leveraging SMP to achieve efficient and accurate predictions.\",\n    \"Direct Inspiration\": {\n        \"b17\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.8,\n        \"b47\": 0.6,\n        \"b25\": 0.7,\n        \"b45\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5,\n        \"b11\": 0.4,\n        \"b53\": 0.3\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient and accurate 3D molecular learning by proposing a novel message passing method, termed spherical message passing (SMP). This is designed to work within the spherical coordinate system (SCS) to reduce computational complexity while maintaining accurate representations of 3D molecular structures. The proposed method, SphereNet, incorporates distance, angle, and torsion information to improve molecular predictions.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b11\": 0.6,\n    \"b50\": 0.6,\n    \"b58\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a novel and efficient message passing method for 3D molecular graphs. The authors propose the Spherical Message Passing (SMP) scheme, which incorporates distance, angle, and torsion information in the spherical coordinate system to reduce computational complexity and maintain accuracy in 3D molecular learning. The proposed SphereNet, based on SMP, achieves superior performance on various datasets without increasing the computational budget.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b45\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b5\": 0.6\n  }\n}\n```"], "60338c9891e011e54d039c5a": ["```json\n{\n  \"Summary\": \"The paper proposes Molecular Contrastive Learning of Representations (MolCLR) via Graph Neural Networks to address challenges in molecular representation learning. The main challenges include thorough representation of molecular information, the enormous magnitude of chemical space, and insufficient labeled data. MolCLR uses a self-supervised learning framework with three molecule graph augmentation strategies: atom masking, bond deletion, and subgraph removal. The model shows state-of-the-art performance on several downstream molecular classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b36\": 1.0,\n    \"b59\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b33\": 0.7,\n    \"b44\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b66\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents 'Molecular Contrastive Learning of Representations (MolCLR)' via Graph Neural Networks to address three major challenges in molecular representation learning: preserving molecular structural information, generalizing across an enormous chemical space, and dealing with insufficient labeled data. The proposed MolCLR employs self-supervised learning with contrastive loss and introduces three molecule graph augmentation strategies: atom masking, bond deletion, and subgraph removal. The framework is shown to outperform other methods in multiple molecular benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.9,\n    \"b36\": 0.8,\n    \"b59\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.6,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.5,\n    \"b44\": 0.5,\n    \"b65\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three primary challenges in molecular representation: difficulty in representing molecular information thoroughly, the vast chemical space, and insufficient labeled data. The proposed MolCLR framework uses self-supervised learning with Graph Neural Networks to learn molecular representations by contrasting positive and negative molecule graph pairs. The paper introduces three augmentation strategies: atom masking, bond deletion, and subgraph removal. The MolCLR framework achieves state-of-the-art performance on various molecular benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.9,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.7,\n    \"b59\": 0.6,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.5,\n    \"b27\": 0.4,\n    \"b44\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes MolCLR, a self-supervised learning framework for molecular representation learning, addressing challenges in representing molecular information, generalizing across chemical space, and dealing with limited labeled data. This is achieved through the use of GNNs and three novel molecule graph augmentation strategies: atom masking, bond deletion, and subgraph removal.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b36\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b24\": 0.8,\n    \"b33\": 0.7,\n    \"b59\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b44\": 0.6,\n    \"b27\": 0.5,\n    \"b58\": 0.5,\n    \"b65\": 0.5,\n    \"b66\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Difficulty in representing molecular information thoroughly.\",\n      \"Enormous magnitude of chemical space.\",\n      \"Insufficient labeled data for molecular learning tasks.\"\n    ],\n    \"Inspirations\": [\n      \"Propose MolCLR, a self-supervised learning framework for molecular representation learning.\",\n      \"Introduce three molecule graph augmentation strategies: atom masking, bond deletion, and subgraph removal.\",\n      \"Utilize Graph Neural Networks (GNNs) for molecular representation.\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b25\",\n    \"b36\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b12\",\n    \"b33\",\n    \"b59\"\n  ],\n  \"Other Inspiration\": [\n    \"b24\",\n    \"b35\",\n    \"b44\"\n  ]\n}\n```"], "5ff68d16d4150a363cd46017": ["```json\n{\n  \"Summary\": \"The paper presents PANIC, a new scalable and high-performance programmable NIC for multi-tenant networks that supports a wide variety of offloads and composes them into isolated offload chains. The primary challenges addressed include offload variety, dynamic offload chaining, multi-tenant isolation, support for variable-performance offloads, and high-performance interconnect. The proposed architecture leverages a non-blocking switching fabric, a programmable RMT pipeline, a central scheduler, and self-contained compute units.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b23\": 1,\n    \"b61\": 1,\n    \"b62\": 1,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.8,\n    \"b48\": 0.8,\n    \"b54\": 0.7,\n    \"b38\": 0.7,\n    \"b49\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b47\": 0.5,\n    \"b44\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the increasing gap between network line-rates and CPU processing rates by introducing PANIC, a scalable and high-performance programmable NIC designed for multi-tenant networks. PANIC supports a variety of offloads, ensures offload chaining, provides multi-tenant isolation, and accommodates variable-performance offloads. The design leverages existing work on reconfigurable match+action (RMT) switches and incorporates a non-blocking switching fabric to enable flexible chaining of offloads.\",\n    \"Direct Inspiration\": {\n        \"b19\": 0.9,\n        \"b61\": 0.9,\n        \"b62\": 0.9,\n        \"b23\": 0.9,\n        \"b14\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b28\": 0.8,\n        \"b47\": 0.8,\n        \"b44\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.7,\n        \"b66\": 0.7,\n        \"b52\": 0.7,\n        \"b49\": 0.7,\n        \"b48\": 0.7,\n        \"b27\": 0.7,\n        \"b20\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the widening gap between network line-rates and CPU processing capabilities by proposing PANIC, a scalable and high-performance programmable NIC for multi-tenant networks. PANIC supports a variety of offloads, dynamic offload chaining, multi-tenant isolation, and variable-performance offloads, overcoming the limitations of existing NIC designs.\",\n    \"Direct Inspiration\": {\n        \"b19\": 0.9,\n        \"b61\": 0.9,\n        \"b62\": 0.9,\n        \"b23\": 0.85,\n        \"b14\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b28\": 0.8,\n        \"b47\": 0.75,\n        \"b64\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.7,\n        \"b27\": 0.7,\n        \"b66\": 0.7,\n        \"b48\": 0.65,\n        \"b49\": 0.65,\n        \"b52\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing a programmable NIC that can handle diverse offloads, dynamic offload chaining, multi-tenant isolation, and variable performance offloads. The proposed solution, PANIC, leverages a non-blocking switching fabric, a central scheduler, and self-contained compute units to achieve high performance and scalability.\",\n  \"Direct Inspiration\": [\"b19\", \"b61\", \"b62\", \"b23\", \"b14\"],\n  \"Indirect Inspiration\": [\"b28\", \"b10\", \"b69\", \"b27\", \"b66\", \"b9\", \"b52\", \"b49\", \"b48\", \"b70\"],\n  \"Other Inspiration\": [\"b47\", \"b44\", \"b53\", \"b63\", \"b11\", \"b38\", \"b33\", \"b45\", \"b42\", \"b56\", \"b43\", \"b26\", \"b32\", \"b64\", \"b31\", \"b50\", \"b41\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include the need for offload variety, dynamic offload chaining, multi-tenant isolation, support for offloads with variable and below line-rate performance, and high-performance interconnect in programmable NICs. The paper presents PANIC, a new scalable and high-performance programmable NIC that addresses these challenges by drawing inspiration from recent work on reconfigurable (RMT) switches and implementing a novel design that includes a programmable RMT pipeline, a switching fabric, a central scheduler, and self-contained compute units.\",\n    \"Direct Inspiration\": {\n        \"b19\": 1,\n        \"b61\": 1,\n        \"b62\": 1,\n        \"b23\": 1,\n        \"b14\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b47\": 0.8,\n        \"b28\": 0.8,\n        \"b10\": 0.8,\n        \"b49\": 0.8,\n        \"b66\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b34\": 0.6,\n        \"b20\": 0.6,\n        \"b54\": 0.6\n    }\n}\n```"], "61deaabd5244ab9dcbd697fc": ["```json\n{\n  \"Summary\": \"The paper addresses the problem of BGP anomaly detection, particularly focusing on worm attacks like Slammer, Nimda, and Code Red. The authors aim to create less computationally intensive models by using fewer features without significantly compromising accuracy. They propose and optimize SVM and MLP models for this purpose.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b8\": 1,\n    \"b9\": 1,\n    \"b10\": 1,\n    \"b15\": 1,\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b13\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.6,\n    \"b20\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting BGP anomalies using machine learning models that require fewer features, aiming to reduce computational intensity without significantly compromising accuracy. They propose optimizing SVM and MLP models specifically for detecting anomalies caused by worm attacks like Slammer, Nimda, and Code Red.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b8\": 0.8,\n    \"b13\": 0.85,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b10\": 0.7,\n    \"b11\": 0.65,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.5,\n    \"b18\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting BGP anomalies in a computationally efficient manner by using machine learning models with fewer features, focusing on real-time data. It explores the optimization of SVM and MLP models for this purpose.\",\n  \"Direct Inspiration\": [\"b9\", \"b13\", \"b15\", \"b17\"],\n  \"Indirect Inspiration\": [\"b7\", \"b8\", \"b10\", \"b11\", \"b12\"],\n  \"Other Inspiration\": [\"b18\", \"b20\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the detection of BGP anomalies due to various attacks like Slammer, Nimda, and Code Red, using machine learning models that are computationally efficient. The authors propose using SVM and MLP models with fewer features to retain high accuracy in real-time detection.\",\n    \"Direct Inspiration\": {\n        \"b15\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b8\": 0.8,\n        \"b9\": 0.7,\n        \"b10\": 0.7,\n        \"b13\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b17\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Detecting BGP anomalies in a computationally efficient manner using fewer features without compromising accuracy.\",\n    \"Inspirations\": \"The necessity to improve upon existing BGP anomaly detection methods, specifically the use of machine learning techniques.\"\n  },\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b8\": 1.0,\n    \"b9\": 1.0,\n    \"b10\": 0.9,\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b18\": 0.6,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b19\": 0.5\n  }\n}\n```"], "61debdf75244ab9dcbea2128": ["```json\n{\n    \"Summary\": \"The paper addresses the issue of BGP prefix hijacking and proposes a new mitigation strategy called outsourcing mitigation. The primary challenge is enhancing the effectiveness of this mitigation technique by selecting appropriate mitigators. The authors introduce a new metric to evaluate mitigators and propose a selection strategy named ARS. They also analyze various factors affecting mitigation effectiveness.\",\n    \"Direct Inspiration\": [\"b17\"],\n    \"Indirect Inspiration\": [\"b12\", \"b18\"],\n    \"Other Inspiration\": [\"b19\", \"b20\", \"b21\", \"b22\", \"b23\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of BGP prefix hijacking and attempts to mitigate its impact using a novel outsourcing mitigation strategy. The authors propose a new metric for evaluating the mitigation effectiveness of the hijack-mitigation simulation, and a mitigator selection strategy called ARS. The study also analyzes different characteristics of ASes that influence mitigation effectiveness and extends the findings to mitigate route leaks.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.9,\n        \"b18\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.7,\n        \"b20\": 0.7,\n        \"b21\": 0.7,\n        \"b22\": 0.7,\n        \"b23\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the mitigation of BGP prefix hijacking. The authors propose a new metric for evaluating the effectiveness of mitigators and introduce a mitigator selection strategy called ARS. The paper aims to improve the hijacking mitigation process by selecting the most effective ASes (Autonomous Systems) for outsourcing mitigation.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b17\", \"b18\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b12\", \"b20\", \"b21\", \"b22\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b23\", \"b24\", \"b25\", \"b27\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of BGP prefix hijacking and proposes a novel mitigation strategy called outsourcing mitigation, which involves selecting appropriate Autonomous Systems (ASes) to redirect hijacked traffic back to the victim. The paper introduces a new metric for evaluating the effectiveness of mitigators and proposes a selection strategy named ARS. The effectiveness of different AS characteristics on mitigation is analyzed, and deployment strategies are evaluated using the proposed metric.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b18\": 0.7,\n    \"b23\": 0.6,\n    \"b27\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of BGP prefix hijacking, proposing an outsourcing mitigation approach that relies on selecting effective mitigators to redirect hijacked traffic back to the victim AS. The primary challenge is ensuring the effectiveness of mitigators, which can vary significantly based on their network characteristics and relationships.\",\n  \"Direct Inspiration\": [\"b17\"],\n  \"Indirect Inspiration\": [\"b12\", \"b18\"],\n  \"Other Inspiration\": [\"b19\", \"b23\", \"b24\"]\n}\n```"], "616ce5a55244ab9dcbacff30": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge addressed in this paper is to enable zero-shot generalization to new tasks using large language models. The paper focuses on the limitations of implicit multitask learning in pretraining and proposes a new supervised, explicitly multitask training approach to improve robustness and generalization to unseen tasks.\",\n    \"Inspirations\": \"The authors are inspired by previous work on large language models, multitask learning, and natural language prompting. They aim to build on these methods to create a more effective model for zero-shot generalization.\"\n  },\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b55\": 0.9,\n    \"b92\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b66\": 0.7,\n    \"b34\": 0.7,\n    \"b95\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b76\": 0.6,\n    \"b49\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving zero-shot generalization in large language models by using explicit multitask training with natural language prompts. The authors propose a method to train models on a diverse mixture of tasks and evaluate their generalization to unseen tasks. The approach aims to enhance robustness to prompt wording and task variety.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b92\": 0.9,\n    \"b55\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b66\": 0.8,\n    \"b34\": 0.7,\n    \"b95\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b86\": 0.6,\n    \"b94\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving zero-shot generalization in large language models through explicit multitask supervision. The authors propose a multitask training approach using natural language prompts and evaluate its effectiveness in generalizing to unseen tasks.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b34\": 0.9,\n    \"b55\": 0.9,\n    \"b66\": 0.8,\n    \"b92\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b37\": 0.7,\n    \"b76\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.6,\n    \"b69\": 0.6,\n    \"b86\": 0.6,\n    \"b95\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on enhancing zero-shot generalization in large language models through explicit multitask training. It aims to train models in a supervised and massively multitask fashion using diverse natural language prompts to improve generalization to unseen tasks without requiring massive scale.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b55\": 0.9,\n    \"b92\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.7,\n    \"b66\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b65\": 0.6,\n    \"b76\": 0.6,\n    \"b95\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper involve improving zero-shot generalization of large language models to unseen tasks and ensuring robustness to prompt wording. The authors propose a method of explicit multitask training using a diverse mixture of natural language prompts to improve generalization without requiring massive scale. Their approach involves training a T5 encoder-decoder model on a large set of tasks and evaluating its performance on unseen tasks.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b55\": 0.9,\n    \"b92\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b66\": 0.8,\n    \"b95\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.7,\n    \"b76\": 0.7\n  }\n}\n```"], "618b38575244ab9dcb710ce2": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of learning effective node embeddings for 'orphaned' nodes in a graph, particularly in cases where nodes have few or no neighbors, which existing Graph Neural Networks (GNNs) fail to handle. The proposed approach, Cold Brew, uses a teacher-student knowledge distillation framework to distill the knowledge of a GNN teacher into a multilayer perceptron (MLP) student. This method allows the student to learn a mapping from node features to node embeddings without relying on neighborhood information, thus improving generalization to tail and cold-start nodes. The paper introduces a novel metric called Feature-Contribution Ratio (FCR) to quantify the contribution of node features relative to the adjacency structure and uses it to optimize the model architecture.\",\n    \"Direct Inspiration\": {\n        \"b23\": 1.0,\n        \"b24\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b25\": 0.8,\n        \"b26\": 0.8,\n        \"b27\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b7\": 0.6,\n        \"b8\": 0.6,\n        \"b9\": 0.6,\n        \"b10\": 0.6,\n        \"b11\": 0.6,\n        \"b18\": 0.6,\n        \"b19\": 0.6,\n        \"b29\": 0.6,\n        \"b30\": 0.6,\n        \"b31\": 0.5,\n        \"b32\": 0.5,\n        \"b33\": 0.5,\n        \"b34\": 0.5,\n        \"b35\": 0.5,\n        \"b36\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the Strict Cold Start (SCS) problem in large-scale, real-world graphs where some nodes have no neighbors, making existing GNNs ineffective. The proposed algorithm, Cold Brew, leverages a teacher-student knowledge distillation framework to distill the knowledge of a GNN teacher into a multilayer perceptron (MLP) student. Key contributions include the design of Cold Brew to generalize better to tail and SCS nodes, the introduction of the Feature-Contribution Ratio (FCR) metric for model architecture selection, and extensive experimental validation of the framework.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b24\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b26\": 0.8,\n    \"b27\": 0.8,\n    \"b29\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b31\": 0.5,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning effective node embeddings for 'orphaned' nodes in graphs, which have very few or no connections. It introduces a framework called Cold Brew, which uses teacher-student knowledge distillation to transfer knowledge from a GNN teacher to an MLP student. The framework aims to generalize better to tail and cold-start nodes by discovering latent neighborhoods and using a novel metric called Feature-Contribution Ratio (FCR) to optimize model architectures.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b8\": 0.8,\n    \"b25\": 0.7,\n    \"b26\": 0.7,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Graph Neural Networks (GNNs) for nodes with few or no connections, specifically targeting the Strict Cold Start (SCS) problem. It proposes a framework called Cold Brew, which uses a teacher-student knowledge distillation approach to train a multilayer perceptron (MLP) student to generalize better to tail and isolated nodes. The paper introduces the Feature-Contribution Ratio (FCR) metric to quantify the contribution of node features relative to the adjacency structure of the graph.\",\n  \"Direct Inspiration\": [\"b23\", \"b24\"],\n  \"Indirect Inspiration\": [\"b25\", \"b26\", \"b27\"],\n  \"Other Inspiration\": [\"b8\", \"b18\", \"b19\", \"b29\", \"b30\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generalizing Graph Neural Networks (GNNs) to tail and Strict Cold Start (SCS) nodes, where neighborhood information is missing or noisy. The proposed solution, Cold Brew, utilizes a teacher-student knowledge distillation framework to distill the knowledge of a GNN teacher into a multilayer perceptron (MLP) student. The paper introduces a novel Feature-Contribution Ratio (FCR) metric to disentangle and quantify the contributions of node features versus neighborhood structure, aiding in architecture selection for improved model performance under SCS conditions.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b24\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b26\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b29\": 0.7,\n    \"b30\": 0.7\n  }\n}\n```"], "60bdde338585e32c38af4ee9": ["```json\n{\n  \"Summary\": \"The primary challenge of the paper is to learn informative graph representations in a self-supervised manner, capturing both local-instance and global-semantic structures. The proposed method, GraphLoG, addresses this by using a combination of GNNs and a hierarchical prototype-based approach along with an online EM algorithm.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b6\": 0.85,\n    \"b26\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.75,\n    \"b40\": 0.75,\n    \"b46\": 0.75,\n    \"b50\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.65,\n    \"b12\": 0.65,\n    \"b10\": 0.65,\n    \"b23\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of learning informative graph representations in a self-supervised manner, addressing both local-instance and global-semantic structures. The proposed GraphLoG framework aims to align embeddings of correlated graphs/subgraphs and introduces hierarchical prototypes to model global structures, utilizing an online EM algorithm for optimization.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b41\": 0.8,\n    \"b12\": 0.8,\n    \"b23\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b26\": 0.7,\n    \"b32\": 0.7,\n    \"b50\": 0.7,\n    \"b46\": 0.7,\n    \"b37\": 0.6,\n    \"b24\": 0.6,\n    \"b58\": 0.6,\n    \"b21\": 0.6,\n    \"b40\": 0.6,\n    \"b34\": 0.6,\n    \"b43\": 0.6,\n    \"b3\": 0.6,\n    \"b9\": 0.6,\n    \"b1\": 0.6,\n    \"b57\": 0.6,\n    \"b60\": 0.6,\n    \"b11\": 0.6,\n    \"b31\": 0.6,\n    \"b22\": 0.6,\n    \"b48\": 0.6,\n    \"b5\": 0.6,\n    \"b33\": 0.6,\n    \"b7\": 0.6,\n    \"b38\": 0.6,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b52\": 0.5,\n    \"b0\": 0.5,\n    \"b27\": 0.5,\n    \"b59\": 0.5,\n    \"b32\": 0.5,\n    \"b54\": 0.5,\n    \"b2\": 0.5,\n    \"b44\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning informative graph representations in a self-supervised manner, balancing both local-instance and global-semantic structures. The proposed GraphLoG framework uses GNNs and an online EM algorithm to achieve this, demonstrating significant performance improvements in tasks like molecular property prediction.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b50\": 1,\n    \"b46\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b26\": 0.8,\n    \"b37\": 0.8,\n    \"b40\": 0.8,\n    \"b58\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b9\": 0.7,\n    \"b3\": 0.7,\n    \"b43\": 0.7,\n    \"b33\": 0.6,\n    \"b44\": 0.6,\n    \"b5\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning graph representations in a self-supervised manner, aiming to capture both local-instance and global-semantic structures. The authors propose the GraphLoG framework, which combines local similarity preservation and hierarchical prototype-based global structure learning, using an online EM algorithm for model optimization.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b37\": 0.9,\n    \"b46\": 0.8,\n    \"b50\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b9\": 0.7,\n    \"b26\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b43\": 0.6,\n    \"b48\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning graph representations in a self-supervised manner, focusing on capturing both local-instance and global-semantic structures. The proposed GraphLoG framework aligns embeddings of correlated graphs/subgraphs and employs hierarchical prototypes to model global structures, using an online EM algorithm for optimization.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b6\": 0.9,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b50\": 0.7,\n    \"b46\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.6,\n    \"b3\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```"], "6008346b9e795ed227f53211": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of multi-objective molecule design for drug discovery, which requires generating diverse, novel molecules that satisfy multiple properties without relying on expert-annotated or lab-collected data. The proposed MARS algorithm uses Markov chain Monte Carlo sampling with an adaptive molecular graph editing proposal to generate high-quality candidate molecules.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b36\": 0.7,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b23\": 0.4,\n    \"b20\": 0.3 \n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multi-objective molecule design for drug discovery, aiming to generate molecules that satisfy multiple properties, are diverse and novel, and do not rely on expert annotated or wet experimental data. The proposed method, MArkov moleculaR Sampling (MARS), uses an iterative fragment-editing process guided by a Graph Neural Network (GNN) model and annealed Markov chain Monte Carlo (MCMC) sampling to efficiently explore the chemical space and generate high-quality molecules.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b36\": 0.7,\n    \"b13\": 0.6,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper focuses on the problem of multi-objective molecule design for drug discovery, proposing MArkov moleculaR Sampling (MARS) as a solution. The main challenges tackled include satisfying multiple properties, producing diverse and novel molecules, and not relying on external annotated data. The MARS algorithm uses Markov chain Monte Carlo sampling and Graph Neural Networks to iteratively edit molecular fragments, aiming to meet these criteria.\",\n    \"Direct Inspiration\": {\n        \"b16\": 1.0,\n        \"b36\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b23\": 0.75,\n        \"b13\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.65,\n        \"b24\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multi-objective molecule design for drug discovery, proposing the MArkov moleculaR Sampling (MARS) method. The primary challenges include satisfying multiple properties with high scores (C1), producing diverse and novel molecules (C2), and avoiding reliance on expert-annotated or experimental data (C3). The MARS method involves iterative editing of molecular graphs using Graph Neural Networks (GNNs) and annealed Markov chain Monte Carlo sampling to generate candidate molecules.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b8\": 0.6,\n    \"b13\": 0.6,\n    \"b36\": 0.6,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multi-objective molecule design for drug discovery, emphasizing efficiency, diversity, and independence from expert annotated data. The proposed MArkov moleculaR Sampling (MARS) method leverages adaptive fragment-editing with Graph Neural Networks (GNNs) and annealed Markov chain Monte Carlo sampling to meet these criteria.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b19\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b8\": 0.6,\n    \"b32\": 0.6,\n    \"b23\": 0.5\n  }\n}\n```"], "61fb47e05aee126c0f8739ae": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of transferring relational knowledge in graph neural networks (GNNs). The authors propose a methodology and additional metrics for evaluating GNN transfer learning, create synthetic graph classification tasks with community structure, and evaluate the transferability of several popular GNNs on both real and synthetic datasets.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b9\": 0.7,\n    \"b12\": 0.65,\n    \"b13\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.55,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Lack of insight into transferring relational knowledge in GNNs\",\n      \"No comparison of the generalisability of different GNNs on downstream task performance\",\n      \"Need for a model-agnostic and task-agnostic framework for transfer learning experiments with GNNs\"\n    ],\n    \"contributions\": [\n      \"Providing a methodology and metrics for evaluating GNN transfer learning\",\n      \"Developing a novel method for creating synthetic graph classification tasks with community structure\",\n      \"Evaluating the transferability of several popular GNNs on both real and synthetic datasets\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": {\n      \"b8\": 0.9,\n      \"b9\": 0.8,\n      \"b10\": 0.8,\n      \"b21\": 0.85,\n      \"b22\": 0.85\n    }\n  },\n  \"Indirect Inspiration\": {\n    \"references\": {\n      \"b4\": 0.7,\n      \"b19\": 0.75,\n      \"b23\": 0.75\n    }\n  },\n  \"Other Inspiration\": {\n    \"references\": {\n      \"b12\": 0.6,\n      \"b13\": 0.65,\n      \"b24\": 0.6\n    }\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of transfer learning in Graph Neural Networks (GNNs), specifically focusing on the lack of a model-agnostic and task-agnostic framework for transferring relational knowledge. The paper proposes a methodology for evaluating GNN transfer learning, introduces new metrics, and creates synthetic graph classification tasks to study the transferability of GNNs.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b23\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b19\": 0.7,\n    \"b24\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve the lack of a model-agnostic and task-agnostic framework and standard benchmark datasets for transfer learning with Graph Neural Networks (GNNs). The paper proposes a methodology and additional metrics for evaluating GNN transfer learning, a novel method for creating synthetic graph classification tasks with community structure, and evaluates the transferability of several popular GNNs on both real and synthetic datasets.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b22\": 0.85,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b8\": 0.65,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.55,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the lack of insight into transferring relational knowledge with GNNs and the absence of a model-agnostic and task-agnostic framework for GNN transfer learning experiments. The paper proposes a novel method for creating synthetic graph classification tasks and evaluates the transferability of several popular GNNs on both real and synthetic datasets.\",\n    \"Direct Inspiration\": {\n        \"b4\": 0.9,\n        \"b22\": 0.85,\n        \"b23\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.75,\n        \"b9\": 0.75,\n        \"b10\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b24\": 0.7,\n        \"b25\": 0.65\n    }\n}\n```"], "60c7fd7891e0110a2be238b0": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the scarcity of labeled data for molecular property prediction using Graph Neural Networks (GNNs) and Deep Neural Networks (DNNs). The authors propose a novel Geometry Enhanced Molecular representation learning method (GEM) for Chemical Representation Learning (ChemRL), which includes a geometry-based GNN (GeoGNN) and various geometry-level self-supervised learning tasks. This method aims to encode both topology and geometry information of molecules to improve prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b39\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b21\": 0.6,\n    \"b10\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the scarcity of labeled data for molecular property prediction using GNNs and DNNs. The proposed algorithm, ChemRL-GEM, enhances geometric representation learning by incorporating geometry-based GNN architecture (GeoGNN) and geometry-level self-supervised learning tasks. This aims to capture both the topology and 3D spatial structure of molecules, addressing limitations in existing methods that only consider topology.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b39\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b29\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b16\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of molecular property prediction in computational drug and materials discovery, focusing on the scarcity of labeled data and the need to incorporate molecular geometry information into graph neural networks (GNNs). The authors propose a novel Geometry Enhanced Molecular representation learning method (GEM) which uses a geometry-based GNN (GeoGNN) to encode both topology and geometry information. They also design various geometry-level self-supervised learning tasks to pre-train the model.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b39\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b34\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b21\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed by the paper is the scarcity of labeled data for molecular property prediction using Graph Neural Networks (GNNs) and Deep Neural Networks (DNNs). Additionally, the paper tackles the limitation of existing molecular representation methods that only encode topology information, neglecting crucial three-dimensional (3D) spatial structures.\",\n    \"inspirations\": \"The paper is inspired by the success of self-supervised learning methods in Natural Language Processing (NLP) and Computer Vision (CV), and recent studies utilizing large-scale molecules with self-supervised methodology to pre-train molecular representations.\"\n  },\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b39\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b21\": 0.7,\n    \"b10\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b43\": 0.5,\n    \"b44\": 0.5,\n    \"b41\": 0.5,\n    \"b29\": 0.5,\n    \"b13\": 0.5,\n    \"b34\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the scarcity of labeled data for molecular property prediction using GNNs and DNNs, along with the limitation of existing methods that only encode topology information without adequately capturing the 3D spatial geometry of molecules. The authors propose a novel Geometry Enhanced Molecular representation learning method (GEM) for Chemical Representation Learning (ChemRL) to address these issues. The proposed method includes a geometry-based GNN (GeoGNN) that encodes both topology and geometry information, and multiple geometry-level self-supervised learning tasks to learn the molecular 3D spatial knowledge.\",\n    \"Direct Inspiration\": {\n        \"b22\": 1.0,\n        \"b39\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b41\": 0.85,\n        \"b29\": 0.8,\n        \"b9\": 0.75,\n        \"b21\": 0.75,\n        \"b10\": 0.75,\n        \"b16\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.7,\n        \"b34\": 0.7\n    }\n}\n```"], "615fb6ef5244ab9dcb9c3c26": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the over-complication of GNN-based QA systems for knowledge-aware reasoning. The authors propose a simpler, efficient, and effective model named Graph Soft Counter (GSC) inspired by the analysis of existing GNN modules using Sparse Variational Dropout (SparseVD). Their proposed model aims to perform better by reducing complexity and focusing on essential graph operations.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b26\": 1,\n    \"b64\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.8,\n    \"b22\": 0.8,\n    \"b54\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the over-complication of current GNN modules for QA and the need for efficient yet effective reasoning mechanisms. The paper introduces a novel Graph Soft Counter (GSC) to address these challenges by simplifying the reasoning process, focusing on counting mechanisms within knowledge graphs.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b64\": 1,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.8,\n    \"b47\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b45\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is improving question-answering (QA) systems by effectively utilizing both implicit knowledge encoded in pre-trained language models (LMs) and explicit knowledge stored in structured knowledge graphs (KGs). The paper proposes a novel Graph Soft Counter (GSC) algorithm to simplify and enhance the efficiency of graph neural networks (GNNs) for knowledge-aware QA reasoning. The paper is motivated by the need to revisit and simplify existing GNN-based QA systems, which are found to be over-parameterized and complex for the tasks they perform.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b64\": 0.9,\n    \"b10\": 0.85,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b54\": 0.75,\n    \"b22\": 0.7,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b47\": 0.7,\n    \"b17\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the complexity and over-parameterization of existing GNN-based QA systems and their limited reasoning capabilities. The proposed algorithm, Graph Soft Counter (GSC), addresses these challenges by simplifying the graph neural module to perform efficient and interpretable soft counting of edges and nodes, thereby achieving better QA performance with fewer parameters.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b10\": 0.9,\n    \"b64\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.8,\n    \"b45\": 0.7,\n    \"b22\": 0.7,\n    \"b54\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b47\": 0.6,\n    \"b4\": 0.6,\n    \"b46\": 0.5,\n    \"b11\": 0.5,\n    \"b13\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving question answering (QA) systems by leveraging both implicit knowledge from pre-trained language models and explicit knowledge from knowledge graphs. It critiques the complexity and over-parameterization of current Graph Neural Network (GNN) modules used in QA systems, proposing a simpler yet effective model called the Graph Soft Counter (GSC). The GSC uses a minimalist approach to handle knowledge-aware reasoning, focusing on edge and node counting within knowledge graphs.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b10\": 1.0,\n    \"b64\": 1.0,\n    \"b35\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.8,\n    \"b22\": 0.8,\n    \"b54\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.5,\n    \"b47\": 0.5,\n    \"b4\": 0.5,\n    \"b19\": 0.5,\n    \"b17\": 0.5,\n    \"b8\": 0.5\n  }\n}\n```"], "60b9a149e4510cd7c8f68c9c": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include capturing contextual emotional changes in real-time during conversations, considering emotion persistence, context dependence, and contagiousness. The proposed algorithm, ERLDK, is a multimodal emotion recognition model for conversational videos based on reinforcement learning and domain knowledge. It aims to maintain real-time performance while including contextual information through the concept of emotion-pairs and utilizes a dueling deep-Q-network (DDQN) to learn emotion state transformations.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.75,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.65,\n    \"b8\": 0.6,\n    \"b9\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of emotion recognition in conversational videos (ERC), emphasizing the unique characteristics of ERC such as context dependence, persistence, and contagiousness. It proposes a multimodal emotion recognition model (ERLDK) based on reinforcement learning (RL) and domain knowledge. The model aims to maintain real-time performance while capturing contextual information by defining 'emotion-pairs' as the smallest emotion stage unit of a conversation. The model integrates text, visual, and audio inputs, leveraging a dueling deep-Q-network (DDQN) for learning and domain knowledge for result revision.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the complex and dynamic nature of emotion recognition in conversational videos, with an emphasis on real-time performance and context dependence, persistence, and contagiousness of emotions. The proposed solution is a multimodal emotion recognition model (ERLDK) based on reinforcement learning (RL) and domain knowledge, integrating text, visual, and audio inputs to achieve this goal.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b11\": 0.9,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in emotion recognition in conversational videos (ERC). The proposed ERLDK model combines reinforcement learning (RL) and domain knowledge to achieve real-time multimodal emotion recognition using text, visual, and audio inputs. The concept of emotion-pair is introduced to maintain real-time performance while including contextual information.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b10\": 0.85,\n    \"b11\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b8\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of emotion recognition in conversational videos (ERC) by proposing a multimodal emotion recognition model based on reinforcement learning (RL) and domain knowledge (ERLDK). The paper highlights the unique characteristics of ERC such as context dependence, persistence, and contagiousness, and aims to improve both the accuracy and real-time performance of emotion recognition.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.9,\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b11\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b3\": 0.7\n  }\n}\n```"], "60b9a2aae4510cd7c8f7e9c8": ["```json\n{\n  \"Summary\": \"The paper addresses the critical challenge of discovering CPU RTL vulnerabilities, which are difficult to patch due to the hard-wired nature of CPUs. The authors propose DIFUZZRTL, an RTL fuzzer that utilizes both dynamic and differential testing approaches to efficiently find RTL bugs. The novel methods include a new execution coverage metric tailored for RTL designs and a systematic way to explore all possible input spaces of RTL designs. DIFUZZRTL was designed to overcome the limitations of previous methods such as RFuzz.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b33\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b4\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of detecting CPU RTL bugs, particularly focusing on the complexities of RTL designs and the limitations of existing fuzzing techniques. It proposes DIFUZZRTL, an RTL fuzzer that uses dynamic and differential testing approaches to efficiently discover vulnerabilities in RTL designs.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focused on the challenges of identifying CPU RTL bugs, particularly due to the complex nature of RTL designs and the vast number of states involved. The proposed algorithm, DIFUZZRTL, uses dynamic testing (coverage-guided fuzzing) and differential testing approaches to efficiently discover RTL bugs.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b33\": 0.6,\n    \"b34\": 0.6\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are the difficulties in exploring the vast state space of RTL designs and the need for a systematic way to explore all possible input spaces of RTL designs. The algorithm proposed by the author is DIFUZZRTL, an RTL fuzzer designed to discover CPU RTL vulnerabilities using a dynamic testing approach (coverage-guided fuzzing) and a differential-testing approach. DIFUZZRTL tackles the challenges by using a new execution coverage metric tailored for RTL designs and providing systematic mechanisms to test a newly designed input format for CPU RTLs.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1,\n        \"b35\": 0.9,\n        \"b4\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b33\": 0.7,\n        \"b34\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.6,\n        \"b10\": 0.6,\n        \"b11\": 0.6,\n        \"b12\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces DIFUZZRTL, an RTL fuzzer designed to identify CPU RTL vulnerabilities by employing a dynamic and differential testing approach. The primary challenges addressed include the need for a new execution coverage metric tailored for RTL designs and a systematic way to explore all possible input spaces of RTL designs.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b34\": 0.9,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b37\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```"], "608035fe91e011772654fba7": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating position information into transformer-based models, which are inherently position-agnostic. It proposes a novel approach called Rotary Position Embedding (RoPE), which uses rotation matrices to encode relative position information into the context representations. This method is shown to be compatible with linear self-attention and to perform well on long text sequences.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b3\": 0.9,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b6\": 0.7,\n    \"b10\": 0.6,\n    \"b16\": 0.6,\n    \"b21\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.4,\n    \"b8\": 0.4,\n    \"b15\": 0.4,\n    \"b12\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed in the paper is the incorporation of position information in transformer-based models, which are inherently position-agnostic. The proposed algorithm, Rotary Position Embedding (RoPE), aims to encode relative position by multiplying the context representations with a rotation matrix, providing a clear theoretical interpretation and compatibility with linear self-attention.\",\n    \"inspirations\": \"The paper draws on previous works on absolute and relative position encoding, as well as neural ODEs and complex space modeling for position information.\"\n  },\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b6\": 0.9,\n    \"b2\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b15\": 0.7,\n    \"b10\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b12\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating position information into transformer-based models, which are inherently position-agnostic. It proposes a novel rotary position encoding (RoPE) method that multiplies context representations with a rotation matrix, providing a clear theoretical interpretation and compatibility with linear self-attention. The paper demonstrates RoPE's superior performance in handling long texts compared to previous models.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b2\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b10\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b1\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of encoding position information in transformer-based models for natural language processing. The authors propose a novel rotary position encoding (RoPE) method which uses a rotation matrix to encode relative position information multiplicatively, and demonstrate its effectiveness compared to existing additive methods.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b2\": 0.9,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b15\": 0.7,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of incorporating position information into transformer-based models, which are inherently position-agnostic. The authors propose a novel Rotary Position Encoding (RoPE) method to encode relative position information through a rotation matrix. This method aims to provide a clear theoretical interpretation and improved compatibility with linear self-attention mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b16\": 0.8,\n    \"b10\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b6\": 0.65,\n    \"b15\": 0.6\n  }\n}\n```"], "600832539e795ed227f53101": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in Graph Representation Learning (GRL) by proposing a novel algorithm, GTTF, which unifies various GRL methods using efficient tensor operations. GTTF enables scalable and unbiased learning for large graphs through stochastic graph traversal and function specialization.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b13\": 0.9,\n    \"b10\": 0.8,\n    \"b11\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.6,\n    \"b9\": 0.5,\n    \"b5\": 0.4,\n    \"b26\": 0.3\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.2,\n    \"b8\": 0.1,\n    \"b22\": 0.1,\n    \"b27\": 0.1\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed in the paper is the scalability and efficiency of Graph Representation Learning (GRL) methods, particularly for large graphs. The proposed solution is the GTTF algorithm, which unifies various GRL methods using efficient tensor operations and specialized functions for scalability and unbiased gradient estimates.\",\n    \"inspirations\": \"The inspirations for the GTTF algorithm include existing GRL methods such as node2vec, DeepWalk, and various stochastic sampling methods like GraphSAGE, FastGCN, LADIES, and GraphSAINT. The paper aims to improve upon these methods by offering a more scalable and efficient approach.\"\n  },\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b11\": 1.0,\n    \"b13\": 1.0,\n    \"b17\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b9\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b8\": 0.6,\n    \"b20\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a stochastic graph traversal algorithm (GTTF) that unifies various graph representation learning (GRL) methods. The GTTF algorithm is based on tensor operations and can scale to large graphs, providing unbiased gradient estimates. The main contributions include the development of the GTTF algorithm, its application to existing GRL methods, and its scalability to large graphs.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b13\": 0.9,\n    \"b11\": 0.85,\n    \"b17\": 0.85,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b7\": 0.75,\n    \"b9\": 0.7,\n    \"b16\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.65,\n    \"b8\": 0.65,\n    \"b22\": 0.65,\n    \"b23\": 0.6,\n    \"b24\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces GTTF, a stochastic graph traversal algorithm using tensor operations for graph representation learning. It addresses the challenges of unifying various GRL methods, scaling to large graphs, and ensuring unbiased gradient estimates. GTTF efficiently handles graph traversal and incorporates functions to recover and extend existing GRL methods.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b17\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b11\": 0.75,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b16\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling graph representation learning (GRL) methods to large graphs. It introduces a novel stochastic graph traversal algorithm called GTTF, which uses tensor operations allowing for efficient and scalable graph learning. The algorithm can be specialized to recover a variety of existing GRL methods and introduces new ones with minimal code modifications.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b17\": 0.85,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b16\": 0.65,\n    \"b19\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b7\": 0.6,\n    \"b24\": 0.6,\n    \"b26\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```"], "60a3ab7991e01115219ffaa9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in Graph Neural Networks (GNNs) for node classification tasks, particularly focusing on the distinction between homophily and heterophily datasets. The proposed algorithm introduces three unique design considerations: Soft-selection of features using the softmax function, Hop-Normalization, and unique mapping of features. These strategies aim to improve classification accuracy and scalability in large graph datasets.\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b9\": 0.7,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b13\": 0.5,\n    \"b7\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving GNN models for node classification in heterophily datasets. The proposed model, FSGNN, separates feature propagation and learning, introduces soft-selection of features, and employs hop-normalization.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b1\": 0.4,\n    \"b13\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in node classification tasks using Graph Neural Networks (GNNs), particularly in datasets with heterophily. It proposes a new GNN model called Feature Selection Graph Neural Network (FSGNN) that separates feature propagation and learning, introduces soft-selection of features, and hop-normalization to improve model performance.\",\n  \"Direct Inspiration\": [\"b14\", \"b34\"],\n  \"Indirect Inspiration\": [\"b6\", \"b26\", \"b15\"],\n  \"Other Inspiration\": [\"b4\", \"b1\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of node classification in heterophily datasets using Graph Neural Networks (GNNs). It proposes a new model called Feature Selection Graph Neural Network (FSGNN) that separates feature propagation and learning, employs soft-selection of features, and introduces Hop-Normalization to improve classification accuracy and scalability.\",\n  \"Direct Inspiration\": [\"b14\", \"b34\"],\n  \"Indirect Inspiration\": [\"b6\", \"b15\", \"b22\"],\n  \"Other Inspiration\": [\"b4\", \"b11\", \"b13\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on improving node classification in Graph Neural Networks (GNNs) with a novel architecture that separately handles feature propagation and learning. The main challenges addressed include dealing with heterophily in datasets where connected nodes have different labels and distinguishing important features from noise. The proposed model, FSGNN, employs three key strategies: soft-selection of features using the softmax function, hop-normalization, and unique mapping of features.\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.9,\n    \"b14\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b26\": 0.7,\n    \"b15\": 0.6,\n    \"b4\": 0.6,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b20\": 0.5,\n    \"b13\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```"], "60cab50d91e011b329373f81": ["```json\n{\n  \"Summary\": \"This paper addresses the challenge of link prediction in graphs by proposing a novel approach called Neural Bellman-Ford Networks (NBFNet). The main inspiration comes from traditional path-based methods and generalizes the Bellman-Ford algorithm to improve scalability and interpretability while leveraging the power of graph neural networks (GNNs). The solution incorporates learned operators within the Bellman-Ford framework to achieve state-of-the-art performance in both transductive and inductive settings.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b4\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b68\": 0.8,\n    \"b45\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7,\n    \"b41\": 0.7,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting interactions between nodes in graphs, aiming to combine the interpretability and scalability of traditional path-based methods with the high model capacity of graph neural networks (GNNs). The proposed solution, Neural Bellman-Ford Networks (NBFNet), generalizes traditional methods like Katz index, personalized PageRank, and graph distance, and enhances them with neural network components to achieve inductive setting generalization and maintain low time complexity.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.85,\n    \"b5\": 0.8,\n    \"b41\": 0.75,\n    \"b29\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.65,\n    \"b47\": 0.6,\n    \"b57\": 0.55,\n    \"b51\": 0.5,\n    \"b68\": 0.45,\n    \"b45\": 0.4,\n    \"b8\": 0.35\n  },\n  \"Other Inspiration\": {\n    \"b72\": 0.3,\n    \"b54\": 0.25,\n    \"b36\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of traditional path-based and GNN-based link prediction methods by proposing a novel Neural Bellman-Ford Networks (NBFNet) framework. This framework combines the advantages of both approaches, ensuring generalization in inductive settings, interpretability, high model capacity, and scalability. The authors introduce a path formulation for link prediction and solve it with a generalized Bellman-Ford algorithm, further enhancing it with neural functions for improved flexibility and performance.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b54\": 0.75,\n    \"b68\": 0.7,\n    \"b45\": 0.7,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b47\": 0.6,\n    \"b58\": 0.6,\n    \"b29\": 0.55,\n    \"b41\": 0.55,\n    \"b36\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the limitations of traditional link prediction methods and GNN-based methods in terms of generalization in the inductive setting, interpretability, high model capacity, and scalability. The proposed solution, Neural Bellman-Ford Networks (NBFNet), is inspired by traditional path-based methods and aims to combine their advantages with the state-of-the-art performance of GNNs. NBFNet generalizes the Bellman-Ford algorithm with learned operators and can be applied to both homogeneous and knowledge graphs for link prediction.\",\n  \"Direct Inspiration\": [\"b3\", \"b4\", \"b68\"],\n  \"Indirect Inspiration\": [\"b29\", \"b41\", \"b36\"],\n  \"Other Inspiration\": [\"b5\", \"b51\", \"b67\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of link prediction in graph machine learning, aiming to combine the interpretability and scalability of traditional path-based methods with the high model capacity of graph neural networks (GNNs). The authors propose Neural Bellman-Ford Networks (NBFNet), which generalizes path-based formulations using learned operators in the Bellman-Ford algorithm.\",\n  \"Direct Inspiration\": [\"b4\", \"b3\"],\n  \"Indirect Inspiration\": [\"b68\", \"b45\", \"b5\", \"b67\", \"b57\", \"b30\", \"b51\", \"b70\", \"b64\", \"b8\"],\n  \"Other Inspiration\": [\"b31\", \"b47\", \"b58\", \"b72\", \"b54\"]\n}\n```"], "620b19c45aee126c0f7e690a": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of short-term passenger flow prediction in urban rail transit (URT) systems, proposing a spatiotemporal network based on Graph Convolutional Network (GCN) and Generative Adversarial Network (GAN) (Graph-GAN). The main contributions include incorporating topological information of the URT network, applying GAN for higher prediction accuracy, and simplifying the model framework.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b42\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.7,\n        \"b21\": 0.6,\n        \"b9\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b17\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the need for accurate short-term passenger flow prediction in URT systems to alleviate congestion and ensure safety. The proposed algorithm, Graph-GAN, combines a simplified static GCN to incorporate topological information and a GAN to generate high-accuracy predictions. The algorithm aims to improve prediction accuracy while maintaining model simplicity.\",\n  \"Direct Inspiration\": {\n    \"references\": []\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b9\", \"b50\", \"b42\", \"b26\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b6\", \"b21\", \"b30\", \"b10\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Accurately predicting short-term passenger flow in URT systems to manage congestion\",\n      \"Incorporating topological information between stations\",\n      \"Balancing prediction accuracy with model complexity\"\n    ],\n    \"Innovations\": [\n      \"Proposed a Graph-GAN framework\",\n      \"Utilized a simplified and static GCN to incorporate topological information\",\n      \"Applied GAN for higher prediction accuracy\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b42\": 1.0,\n    \"b50\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b30\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately predicting short-term passenger flow in urban rail transit (URT) systems to alleviate congestion issues. The authors propose a novel spatiotemporal network based on Graph Convolutional Network (GCN) and Generative Adversarial Network (GAN) to improve prediction accuracy and capture complex spatial and temporal dependencies. The paper highlights the use of a simplified and static GCN to incorporate topological information and a GAN framework to generate high-accuracy predictions through adversarial training.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b50\": 0.85,\n    \"b42\": 0.8,\n    \"b26\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.6,\n    \"b30\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b21\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the accurate short-term passenger flow prediction in urban rail transit (URT) systems to alleviate traffic congestion. The authors proposed a spatiotemporal network based on the Graph Convolution Network (GCN) and Generative Adversarial Network (GAN), called Graph-GAN, to capture the topological relationships between subway stations and improve prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b42\": 0.9,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b50\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.5,\n    \"b30\": 0.4,\n    \"b36\": 0.3\n  }\n}\n```"], "60b6d13c91e011903fc2b816": ["```json\n{\n  \"Summary\": \"The paper addresses the high computational costs of Transformers due to the quadratic time and space complexity of attention operations. The authors propose an Attention Free Transformer (AFT) that avoids dot product attention by combining key and value with learned position biases, resulting in memory complexity linear with respect to input and model sizes. They introduce variants like AFT-local and AFT-conv to further optimize performance and efficiency.\",\n  \"Direct Inspiration\": [\"b10\", \"b12\"],\n  \"Indirect Inspiration\": [\"b3\", \"b4\", \"b6\", \"b7\"],\n  \"Other Inspiration\": [\"b13\", \"b16\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the scalability issue of Transformers, which is primarily due to the quadratic computational cost of the attention operation. The authors propose a new model, the Attention Free Transformer (AFT), which eliminates the standard dot product attention and instead uses a weighted average of values combined with the query through element-wise multiplication. This results in a memory complexity linear with respect to both the input and model sizes. They introduce variants like AFT-local and AFT-conv to enhance computational efficiency and performance.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b10\": 0.95,\n    \"b12\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b13\": 0.8,\n    \"b14\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b16\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the high computational costs associated with Transformers, particularly due to the quadratic time and space complexity of attention operations. The authors propose a novel model called the Attention Free Transformer (AFT) that addresses these issues by eliminating the need for standard dot product attention. AFT maintains direct interaction between any two points in the context but achieves a memory complexity that is linear with respect to both input and model sizes. The paper introduces various AFT variants such as AFT-local, AFT-simple, and AFT-conv to enhance computational and parameter efficiency.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b7\": 0.8,\n    \"b10\": 0.9,\n    \"b12\": 0.9,\n    \"b16\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b8\": 0.6,\n    \"b11\": 0.65,\n    \"b13\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.55,\n    \"b14\": 0.5,\n    \"b22\": 0.5,\n    \"b24\": 0.45 \n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of the high computational costs associated with Transformers, specifically targeting the quadratic time and space complexity of the attention operations. The proposed solution is the Attention Free Transformer (AFT), which avoids dot product attention and introduces novel mechanisms such as AFT-local and AFT-conv to improve efficiency and performance.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b3\": 0.6,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed in the paper is the high computational cost associated with Transformers due to their quadratic time and space complexity in performing attention operations, especially for large context sizes.\",\n    \"algorithm\": \"The proposed algorithm, Attention Free Transformer (AFT), avoids the standard dot product attention by combining key and value with learned position biases and performing element-wise multiplication with the query. This results in a memory complexity linear to both the input and model sizes.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b10\", \"b12\", \"b13\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b6\", \"b7\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b3\", \"b16\"]\n  }\n}\n```"], "60cb2a281bc21f07d081122b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient multimodal fusion from unaligned multimodal sequences in human emotion recognition. The proposed Progressive Modality Reinforcement (PMR) approach introduces a message hub for three-way interactions across modalities and a dynamic filter mechanism for better feature reinforcement. The paper aims to improve upon the limitations of the existing MulT model by promoting effective information flow and leveraging high-level features in a supervised manner.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b21\": 0.6,\n    \"b12\": 0.6,\n    \"b18\": 0.6,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.5,\n    \"b25\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b4\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient multimodal fusion from asynchronous sequences in human multimodal emotion recognition. It proposes the Progressive Modality Reinforcement (PMR) approach, which introduces a message hub to exchange information between modalities and a dynamic filter mechanism to determine the proportions of reinforced features. The PMR approach aims to improve upon the limitations of previous models, particularly the MulT model, by promoting effective information flow across all modalities and leveraging high-level features for modality reinforcement.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b12\": 0.6,\n    \"b21\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in human multimodal emotion recognition, specifically focusing on the asynchronous nature of multimodal data and the inefficiency of existing fusion methods. It proposes the Progressive Modality Reinforcement (PMR) approach, introducing a message hub for three-way interactions and a dynamic filter mechanism to enhance the fusion of high-level features.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b18\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multimodal emotion recognition from video clips, focusing on the asynchrony between different modalities like language, facial gestures, and acoustic behaviors. It proposes a Progressive Modality Reinforcement (PMR) approach that introduces a message hub for effective multimodal fusion from unaligned sequences. The method leverages crossmodal attention and dynamic filter mechanisms to improve sentiment prediction performance.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b18\": 0.6,\n    \"b12\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multimodal emotion recognition from video clips, particularly focusing on the issue of asynchronous multimodal streams. The proposed solution, Progressive Modality Reinforcement (PMR), introduces a message hub to exchange information across modalities and a dynamic filter mechanism for efficient multimodal fusion. The approach aims to improve the fusion of unaligned multimodal sequences by leveraging high-level features and ensuring effective information flow across modalities.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b12\": 0.8,\n    \"b18\": 0.8,\n    \"b23\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```"], "6173f1bc91e0118698c04812": ["```json\n{\n  \"Summary\": \"The paper addresses the CPU frontend bottleneck problem, particularly focusing on BTB misses that significantly contribute to frontend stalls. The authors propose three microarchitectural techniques: Branch Target Deduplication, BTB Partitioning, and Delta Branch Target Encoding to improve BTB storage efficiency and performance.\",\n  \"Direct Inspiration\": {\n    \"b44\": 0.9,\n    \"b55\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7,\n    \"b29\": 0.7,\n    \"b31\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7,\n    \"b36\": 0.7,\n    \"b39\": 0.7,\n    \"b42\": 0.7,\n    \"b43\": 0.7,\n    \"b48\": 0.7,\n    \"b49\": 0.7,\n    \"b53\": 0.7,\n    \"b58\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the CPU frontend bottleneck problem caused by BTB misses in modern processors. It proposes three main techniques: Branch Target Deduplication, BTB Partitioning, and Delta Branch Target Encoding to improve BTB storage efficiency and reduce frontend stalls.\",\n    \"Direct Inspiration\": {\n        \"b44\": 0.9,\n        \"b55\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.7,\n        \"b52\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.5,\n        \"b8\": 0.4,\n        \"b25\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the CPU frontend bottleneck problem, focusing on BTB capacity issues that lead to frontend stalls. It proposes three main techniques: Branch Target Deduplication, BTB Partitioning, and Delta Branch Target Encoding to improve BTB storage efficiency and overall CPU performance.\",\n  \"Direct Inspiration\": {\n    \"b44\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b55\": 0.8,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the CPU frontend bottleneck, particularly focusing on Branch Target Buffer (BTB) capacity issues, which are a significant contributor to frontend stalls in modern processors. The authors propose three main techniques to improve BTB storage efficiency: Branch Target Deduplication, BTB Partitioning, and Delta Branch Target Encoding, resulting in substantial IPC gains.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b14\": 0.6,\n    \"b26\": 0.6,\n    \"b37\": 0.6,\n    \"b48\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the CPU frontend bottleneck problem, specifically focusing on the BTB capacity issue leading to frontend stalls. It proposes three main techniques: Branch Target Deduplication, BTB Partitioning, and Delta Branch Target Encoding, which significantly improve BTB storage efficiency and overall performance.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1.0,\n    \"b53\": 0.9,\n    \"b55\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b1\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```"], "6180ac435244ab9dcb793c15": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing sparse matrix storage format selection for Graph Neural Networks (GNNs) to enhance performance. It proposes a machine learning-based approach using XGBoost to predict the optimal storage format dynamically during runtime.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b26\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of optimizing sparse matrix storage formats to enhance the performance of graph neural networks (GNNs). It proposes a machine learning-based approach using XGBoost to dynamically select the optimal sparse matrix storage format during runtime, which adapts to the changing sparsity of matrices processed in GNN layers.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.95\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8,\n        \"b17\": 0.85,\n        \"b10\": 0.75,\n        \"b16\": 0.8,\n        \"b26\": 0.7,\n        \"b23\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.5,\n        \"b22\": 0.5,\n        \"b7\": 0.6,\n        \"b12\": 0.65,\n        \"b13\": 0.65,\n        \"b18\": 0.65,\n        \"b20\": 0.65\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of optimizing sparse matrix multiplication (SpMM) in Graph Neural Networks (GNNs) by selecting the most efficient sparse matrix storage format. The authors propose a machine learning-based approach using XGBoost to predict the optimal storage format dynamically during runtime, achieving significant performance improvements.\",\n    \"Direct Inspiration\": [\"b6\", \"b26\", \"b23\"],\n    \"Indirect Inspiration\": [\"b45\", \"b22\", \"b20\"],\n    \"Other Inspiration\": [\"b12\", \"b13\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing sparse matrix storage format selection to enhance the performance of Graph Neural Networks (GNNs). It introduces a machine learning-based approach using XGBoost to predict the optimal storage format for sparse matrix multiplication (SpMM) operations in GNNs, considering the dynamic sparsity of matrices during GNN execution.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b26\": 1,\n    \"b23\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b18\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b22\": 0.7,\n    \"b0\": 0.7,\n    \"b17\": 0.7,\n    \"b10\": 0.7,\n    \"b1\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing sparse matrix storage formats for improving the performance of graph neural networks (GNNs). The authors propose a machine learning-based approach using XGBoost to predict the most suitable sparse matrix storage format for GNN execution, which adapts dynamically based on the input data and GNN layer characteristics.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b13\": 0.8,\n    \"b18\": 0.8,\n    \"b20\": 0.8,\n    \"b26\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b22\": 0.5,\n    \"b0\": 0.5,\n    \"b45\": 0.5,\n    \"b10\": 0.4,\n    \"b17\": 0.4,\n    \"b1\": 0.4\n  }\n}\n```"], "60fbf11991e011f0c0fa4c68": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently training Graph Neural Networks (GNNs) on ReRAM-based manycore architectures. The proposed solution, ReGraphX, combines heterogeneous ReRAMs as Processing Elements (PEs) and a 3D Network-on-Chip (NoC) architecture to handle the heavy data exchange patterns inherent in GNN training. The design aims to reduce communication latency and increase throughput by leveraging the benefits of 3D NoC for long-range and multicast communication.\",\n  \"Direct Inspiration\": [\"b5\", \"b6\", \"b7\", \"b8\", \"b11\"],\n  \"Indirect Inspiration\": [\"b3\", \"b9\", \"b10\", \"b12\"],\n  \"Other Inspiration\": [\"b1\", \"b4\", \"b13\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing an efficient architecture for GNN training using ReRAM-based hardware. The proposed solution, ReGraphX, leverages the benefits of 3D NoC to mitigate the communication bottlenecks inherent in traditional planar architectures. The main contributions include a detailed study of GNN computation and communication patterns, the design of a heterogeneous ReRAM architecture, and the use of a 3D NoC to enhance performance and energy efficiency.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.95,\n    \"b7\": 0.85,\n    \"b11\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.70,\n    \"b8\": 0.65,\n    \"b12\": 0.60\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.50,\n    \"b10\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively training Graph Neural Networks (GNNs) using ReRAM-based architectures. The authors propose ReGraphX, a 3D NoC-enabled manycore architecture that combines heterogeneous ReRAM processing elements for computation and a high-throughput 3D NoC for communication. The novel contributions include an efficient NoC architecture designed based on the traffic patterns of GNN training and the use of smaller ReRAM crossbars to handle the sparse adjacency matrix efficiently.\",\n  \"Direct Inspiration\": [\"b5\", \"b6\", \"b7\", \"b11\", \"b12\"],\n  \"Indirect Inspiration\": [\"b3\", \"b4\", \"b8\", \"b10\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating a heterogeneous architecture that effectively combines the design principles of DNNs and graph analytics for GNN training using ReRAM-based architectures. It proposes a novel 3D NoC-enabled manycore architecture, ReGraphX, to reduce communication bottlenecks in GNN training.\",\n  \"Direct Inspiration\": [\"b5\", \"b6\", \"b7\"],\n  \"Indirect Inspiration\": [\"b8\", \"b10\", \"b11\", \"b12\"],\n  \"Other Inspiration\": [\"b3\", \"b4\", \"b9\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accelerating GNN training by proposing a heterogeneous manycore architecture called ReGraphX, which combines ReRAM-based processing elements and a 3D network-on-chip (NoC) communication backbone. The key contributions include an in-depth study of GNN computation and communication patterns, the design of a high-performance 3D NoC architecture, and the integration of graph partitioning for scalable training.\",\n  \"Direct Inspiration\": [\"b5\", \"b7\"],\n  \"Indirect Inspiration\": [\"b6\", \"b11\", \"b12\"],\n  \"Other Inspiration\": [\"b3\", \"b15\"]\n}\n```"], "5ff8818491e011c83266c705": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing I-cache misses in data center applications. It proposes the I-SPY prefetching technique that includes two novel mechanisms: conditional prefetching and prefetch coalescing. Conditional prefetching uses execution context to decide when to execute prefetch instructions, while prefetch coalescing reduces the static code footprint by combining multiple prefetches into a single instruction.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b20\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b21\": 0.85,\n    \"b32\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing I-cache misses in data center applications by proposing a novel instruction prefetching technique called I-SPY. I-SPY introduces two main mechanisms: conditional prefetching and prefetch coalescing, which aim to improve prefetch accuracy and reduce static code footprint, respectively. The research is motivated by the limitations of existing prefetching techniques, such as AsmDB, which fall short of achieving ideal performance.\",\n  \"Direct Inspiration\": [\"b1\", \"b20\"],\n  \"Indirect Inspiration\": [\"b2\", \"b3\", \"b7\", \"b8\", \"b9\", \"b11\", \"b12\"],\n  \"Other Inspiration\": [\"b6\", \"b21\", \"b32\", \"b30\", \"b33\", \"b35\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing I-cache misses in data center applications, which are a critical performance bottleneck. It introduces I-SPY, a prefetching technique that combines conditional prefetching and prefetch coalescing to achieve near-ideal application speedup. The paper emphasizes the need to efficiently predict I-cache misses, timely prefetch instructions, inject prefetches at the right program locations, and minimize unnecessary prefetches.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.7,\n    \"b9\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b7\": 0.5,\n    \"b8\": 0.5,\n    \"b32\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing I-cache misses in data center applications, which are becoming increasingly complex. The authors propose a novel prefetching technique called I-SPY, which includes two main mechanisms: conditional prefetching and prefetch coalescing. Conditional prefetching aims to accurately predict and prefetch I-cache misses by using execution context, while prefetch coalescing reduces the static code footprint by combining multiple prefetches into a single instruction.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b9\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing I-cache misses in data center applications by proposing I-SPY, a novel instruction prefetching technique. I-SPY combines conditional prefetching and prefetch coalescing to achieve near-ideal speedup, improving both miss coverage and prefetch accuracy while minimizing the static and dynamic code footprints.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b3\": 0.7,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b18\": 0.6,\n    \"b21\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b19\": 0.5,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b27\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b33\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6,\n    \"b36\": 0.6,\n    \"b37\": 0.6,\n    \"b38\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```"], "61c145c85244ab9dcb851858": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of achieving accurate text classification and rationale extraction while improving model robustness and explainability. The proposed AT-BMC framework combines task prediction and rationale extraction with adversarial training and boundary match constraints.\",\n  \"Direct Inspiration\": {\n    \"Inspired by [b10]\": 1,\n    \"Inspired by [b2]\": 1,\n    \"Inspired by [b5]\": 1,\n    \"Inspired by [b17]\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"Motivated by [b26]\": 0.8,\n    \"Motivated by [b19]\": 0.7,\n    \"Motivated by [b32]\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"Inspired by [b31]\": 0.6,\n    \"Inspired by [b8]\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of achieving both high prediction performance and model explainability in text classification tasks, particularly focusing on the robustness of models to adversarial attacks. The proposed approach, AT-BMC, integrates adversarial training (AT) and boundary match constraint (BMC) to jointly optimize classification and rationale extraction. The method aims to improve generalization, robustness, and accuracy, combining discrete and embedding space perturbations with a novel boundary matching technique.\",\n    \"Direct Inspiration\": {\n        \"b10\": 0.9,\n        \"b5\": 0.85,\n        \"b2\": 0.8,\n        \"b17\": 0.75\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.7,\n        \"b26\": 0.65,\n        \"b6\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.55,\n        \"b32\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of achieving accurate text classification and extracting prediction rationales while enhancing model explainability and robustness. The novel contributions include a joint classification and rationale extraction framework (AT-BMC) that integrates mixed adversarial training and boundary match constraints.\",\n    \"Direct Inspiration\": [\"b10\", \"b5\", \"b17\"],\n    \"Indirect Inspiration\": [\"b2\", \"b8\", \"b26\"],\n    \"Other Inspiration\": [\"b29\", \"b32\"]\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges addressed in the paper are the difficulty in understanding neural model predictions and enhancing model robustness against adversarial attacks. The proposed algorithm, AT-BMC, jointly performs classification and rationale extraction with mixed Adversarial Training (AT) and Boundary Match Constraint (BMC).\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b17\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b5\": 0.9,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b8\": 0.7,\n    \"b19\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving both prediction performance and model explainability in text classification tasks by extracting rationales. The authors propose a novel joint classification and rationale extraction framework (AT-BMC) that incorporates mixed Adversarial Training (AT) and Boundary Match Constraint (BMC). The framework is designed to enhance model robustness and explanation accuracy, particularly against adversarial attacks.\",\n    \"Direct Inspiration\": {\n        \"b10\": 1,\n        \"b5\": 1,\n        \"b2\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b17\": 0.8,\n        \"b8\": 0.8,\n        \"b26\": 0.7,\n        \"b19\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b32\": 0.6,\n        \"b29\": 0.6,\n        \"b21\": 0.6\n    }\n}\n```"], "5ff6864fd4150a363cc233dc": ["```json\n{\n  \"Summary\": \"The paper proposes a novel model testing and verification methodology, PyH2, which leverages property-based random testing (PBT) in Python to address key challenges in open-source hardware verification. The challenges include the need for comprehensive verification methodologies, systematic testing of design generators, automated random testing frameworks, and novel approaches for testing object-oriented hardware.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b7\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of verifying open-source hardware, particularly in the context of heterogeneous SoC architectures, by introducing a new methodology called PyH2. PyH2 leverages property-based random testing (PBT) in Python to build test benches and models, offering a comprehensive and productive open-source verification framework.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1,\n    \"b3\": 1,\n    \"b4\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the complexities in SoC design and verification due to heterogeneous architectures, the need for productive open-source verification methodologies, and difficulties in random testing of hardware blocks. The proposed solution, PyH2, addresses these challenges by leveraging property-based random testing (PBT) and the Python ecosystem to build test benches and models, with significant improvements over existing methodologies.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b1\", \"b2\"],\n    \"phrases\": [\n      \"Compared to BlueCheck [b1], a prior PBT framework for hardware, the key distinctions are as follows.\",\n      \"We see coverage-guided mutational fuzzing (e.g., RFUZZ [b2]) as complementary to PBT.\"\n    ]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b3\", \"b4\", \"b5\"],\n    \"phrases\": [\n      \"PyH2 leverages PyMTL3 [b3], [b4] to build Python test benches to drive register-transfer-level (RTL) simulations with PyMTL3 models and/or external SystemVerilog models leveraging PyMTL3's Verilator cosimulation support.\",\n      \"PyH2 is supported by the whole Python ecosystem, among which three main packages form the foundation of PyH2 (PyMTL3, pytest, and hypothesis).\"\n    ]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b10\", \"b11\", \"b12\"],\n    \"phrases\": [\n      \"We quantitatively evaluated CRT, IDT, and PyH2G using the PyOCN [b10] ring network generator against four real-world bugs.\",\n      \"It is possible to combine auto-shrinking with other sophisticated random program generators [b11] by carefully using PyH2P random strategies.\",\n      \"PyH2P can also leverage Symbolic-QED [b12] by applying QED transformations to generated random programs and performing bounded model checking to accelerate bug discovery.\"\n    ]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in testing and verification of open-source hardware, proposing a new methodology called PyH2 that uses property-based random testing in Python. The main inspirations for this methodology come from the need to create comprehensive, productive, and automated testing frameworks for open-source hardware, which are currently lacking compared to closed-source hardware. PyH2 leverages the Python ecosystem to build test benches and models, addressing the critical need for effective testing of design generators, processors, and object-oriented hardware components.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in verifying open-source hardware, particularly design generators, processors, and object-oriented hardware. It proposes PyH2, a new model testing and verification methodology leveraging property-based random testing in Python, to overcome these challenges.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```"], "619799b791e011c822372962": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of PCB routing, specifically focusing on simultaneous escape routing (SER) to improve routability and reduce design costs. The proposed method uses SAT-based techniques to achieve a crossing-free escape order and efficient area routing.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1,\n        \"b2\": 1,\n        \"b3\": 0.9,\n        \"b5\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.7,\n        \"b6\": 0.6\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high-density PCB routing by proposing a novel method that coordinates simultaneous escape routing (SER) and area routing. The method uses SAT formulation to optimize layer usage, ensure cross-free escape orders, and facilitate differential pair (DP) routing and length-matching constraints.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.8,\n    \"b3\": 0.85,\n    \"b4\": 0.85,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the complexity of PCB routing, particularly in escape routing and area routing. The proposed algorithm aims to improve simultaneous escape routing (SER) and area routing through a SAT-based concurrent hierarchical approach, which ensures minimal layer usage and crossing-free escape orders while satisfying length-matching constraints.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 1.0,\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in PCB routing, particularly in simultaneous escape routing (SER) and area routing. It introduces a novel method using SAT-based concurrent hierarchical SER to minimize layers and ensure a crossing-free escape order. This method aims to mitigate issues such as via insertion and length-matching constraints, ultimately improving the feasibility and efficiency of routing in dense PCB designs.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1,\n        \"b5\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.9,\n        \"b3\": 0.9,\n        \"b4\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.8\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in PCB routing, particularly escape routing and area routing. It proposes a novel SAT-based concurrent hierarchical SER method to minimize the number of layers used and ensure a crossing-free escape order, which is then optimized for wirelength and group length-matching constraints. This method also includes a hierarchical routing grid map to reduce computational complexity.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b1\", \"b2\", \"b3\", \"b4\", \"b5\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b6\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": []\n  }\n}\n```"], "604b321391e0110eed64c2c1": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of Long-form question answering (LFQA) by integrating retrieval and text generation components. The system proposed by the authors, which uses a sparse Transformer variant conditioned over Wikipedia paragraphs returned by a REALM-style retriever, aims to improve LFQA performance on the ELI5 dataset. Key challenges include the reliability of retrieval in generating quality answers and the limitations of using ROUGE-L as an evaluation metric. The paper also highlights issues with significant train/validation overlap in the ELI5 dataset and the insufficiency of current methods to detect question paraphrases.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1,\n        \"b9\": 0.9,\n        \"b31\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.7,\n        \"b25\": 0.6,\n        \"b26\": 0.6,\n        \"b37\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.5,\n        \"b32\": 0.4,\n        \"b36\": 0.4,\n        \"b4\": 0.3\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Long-form question answering (LFQA) by integrating retrieval and text generation components, focusing on the ELI5 dataset. It proposes a state-of-the-art system using a sparse Transformer variant and investigates the effect of retrieval on generation quality. The paper identifies significant train/validation overlap in ELI5 and issues with using ROUGE-L for evaluation, offering suggestions for better evaluation methods.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.95,\n    \"b13\": 0.9,\n    \"b31\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b26\": 0.75,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.65,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of Long-form question answering (LFQA) by integrating a retrieval component with a text generation component. It uses methods inspired by previous work such as ORQA, REALM, DPR, and pretrained language models. The novel contributions include a state-of-the-art system for ELI5 using a sparse Transformer variant and recommendations for better evaluation methods.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.95,\n    \"b13\": 0.9,\n    \"b25\": 0.85,\n    \"b31\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.75,\n    \"b26\": 0.7,\n    \"b37\": 0.65\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of Long-form Question Answering (LFQA) by integrating retrieval and text generation components. The key challenges include evaluating the relevance of retrieved documents, overcoming significant train/validation overlap in datasets, and addressing the inadequacies of ROUGE-L as an evaluation metric. The authors propose a state-of-the-art LFQA system using a sparse Transformer variant and a REALM-style retriever. Their analysis reveals that retrieval does not significantly impact generation quality due to dataset overlaps and evaluation metric limitations.\",\n  \"Direct Inspiration\": [\"b9\", \"b13\", \"b31\"],\n  \"Indirect Inspiration\": [\"b18\", \"b26\", \"b25\", \"b37\"],\n  \"Other Inspiration\": [\"b32\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Long-form question answering (LFQA) by highlighting issues in retrieval and generation quality, specifically examining how conditioning over Wikipedia paragraphs returned by a REALM-style retriever impacts performance. It proposes a novel system using a sparse Transformer variant and discusses the problem of train/validation overlap in the ELI5 dataset, questioning the reliability of ROUGE-L as a metric.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b23\": 0.6,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```"], "619321425244ab9dcbbb4a76": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of rigid body protein-protein docking, proposing the EQUIDOCK model that leverages deep learning to predict the 3D structure of protein complexes. The method aims to improve computational efficiency and accuracy by incorporating SE(3)-equivariance, commutativity, and novel theoretical constraints, avoiding reliance on candidate sampling, task-specific features, or pre-computed meshes.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b64\": 0.8,\n    \"b13\": 0.8,\n    \"b60\": 0.8,\n    \"b57\": 0.8,\n    \"b54\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b45\": 0.5,\n    \"b61\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the problem of rigid body protein-protein docking by proposing EQUIDOCK, a deep learning-based method that directly predicts the SE(3) transformation for protein complexes without using heavy candidate sampling, ranking, or templates. The method incorporates principles of SE(3)-equivariance and commutativity and introduces novel theoretical constraints and a new type of graph matching network.\",\n    \"Direct Inspiration\": [\"b36\"],\n    \"Indirect Inspiration\": [\"b1\", \"b8\", \"b64\", \"b60\", \"b54\", \"b57\"],\n    \"Other Inspiration\": [\"b6\", \"b15\", \"b39\", \"b26\", \"b72\", \"b44\", \"b10\", \"b59\", \"b22\", \"b19\", \"b17\", \"b53\", \"b68\", \"b51\", \"b30\", \"b70\", \"b24\", \"b0\", \"b76\", \"b35\", \"b63\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of rigid body protein-protein docking using deep learning models for direct prediction of protein complex structures. It proposes EQUIDOCK, a fast, end-to-end method that predicts the SE(3) transformation to place one protein at the correct location and orientation with respect to another. The method incorporates the principles of SE(3)-equivariance and commutativity and uses novel theoretical results for model constraints. EQUIDOCK combines SE(3)-equivariant graph matching networks, an attention-based keypoint selection algorithm, and a differentiable superimposition model.\",\n  \n  \"Direct Inspiration\": {\n    \"b36\": 1.0,\n    \"b1\": 0.9\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b64\": 0.7,\n    \"b13\": 0.7,\n    \"b60\": 0.7,\n    \"b54\": 0.7,\n    \"b57\": 0.7\n  },\n  \n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b61\": 0.5,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is the computational prediction of the 3D structure of a protein-protein complex given the 3D structures of the two proteins in an unbound state. The paper proposes EQUIDOCK, a novel, fast, end-to-end method for rigid body docking that directly predicts the SE(3) transformation to place one protein at the correct location and orientation with respect to the other. This approach incorporates SE(3)-equivariance, commutativity, and novel theoretical constraints, leveraging deep learning models without heavy candidate sampling or hand-crafted features.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1,\n    \"b56\": 1,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b64\": 0.8,\n    \"b13\": 0.7,\n    \"b60\": 0.7,\n    \"b54\": 0.7,\n    \"b57\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b72\": 0.6,\n    \"b44\": 0.6,\n    \"b53\": 0.6,\n    \"b61\": 0.6,\n    \"b45\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of rigid body protein-protein docking by developing EQUIDOCK, a deep learning method that directly predicts the SE(3) transformation to place one protein correctly with respect to another. The authors introduce novel theoretical constraints and a combination of SE(3)-equivariant graph matching networks, attention-based keypoint selection, and differentiable superimposition.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1.0,\n    \"b56\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b54\": 0.75,\n    \"b57\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b13\": 0.7,\n    \"b60\": 0.65\n  }\n}\n```"], "606ee87691e011aa47b6ac71": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing near neighbor search in high-dimensional spaces, which is critical for various machine learning applications. The authors propose the novel method of graph reordering to improve cache locality and thus accelerate the search process. They integrate six graph ordering algorithms into the hierarchical navigable small-world graph (HNSW) and demonstrate significant query time improvements on large datasets.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b3\": 0.8,\n    \"b39\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b12\": 0.75,\n    \"b22\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b6\": 0.55,\n    \"b8\": 0.65,\n    \"b9\": 0.5,\n    \"b10\": 0.55,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b24\": 0.55,\n    \"b25\": 0.55,\n    \"b28\": 0.5,\n    \"b31\": 0.6,\n    \"b34\": 0.5,\n    \"b35\": 0.5,\n    \"b36\": 0.5,\n    \"b38\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accelerating near neighbor search by proposing a novel optimization technique: graph reordering to improve cache performance during search queries. The authors integrate recent graph ordering algorithms into the hierarchical navigable small-world graph (HNSW) index and demonstrate significant speed improvements on large datasets.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1,\n    \"b3\": 0.9,\n    \"b1\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.75,\n    \"b18\": 0.7,\n    \"b6\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b31\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving near neighbor search by using graph reordering as a cache optimization technique. This involves integrating graph ordering algorithms into the hierarchical navigable small-world graph (HNSW) to speed up query times and reduce cache miss rates for large embedding datasets.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b3\": 1,\n    \"b39\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b17\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b6\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies the primary challenge of latency in near neighbor search for high-dimensional datasets, critical to applications in machine learning, and proposes a novel approach using graph reordering to improve cache efficiency and speed up the search process. The authors integrate six recent graph ordering algorithms into the HNSW graph index and demonstrate significant speedups in query time on large datasets.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b12\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of improving the efficiency of near neighbor search, a critical task in machine learning, through graph reordering. It integrates and evaluates various graph ordering algorithms within the hierarchical navigable small-world (HNSW) graph index, demonstrating significant performance improvements in query time and cache miss rate on large datasets.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b3\": 1,\n    \"b39\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b12\": 0.7,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b31\": 0.4\n  }\n}\n```"], "619716445244ab9dcb189946": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in applying masked signal modeling to computer vision, specifically dealing with the differences in locality, signal level, and continuity between visual and language data. The authors propose a simple framework, SimMIM, which uses random masking of image patches and a linear layer for pixel value regression, achieving competitive performance.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b6\": 0.9,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b29\": 0.7,\n    \"b30\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges of the paper involve adapting masked signal modeling from NLP to computer vision, addressing issues like strong pixel locality, raw and low-level nature of visual signals, and the continuous nature of visual signals. The paper proposes a simple framework called SimMIM which involves random masking of image patches and raw pixel value regression, achieving competitive performance in visual recognition tasks.\",\n  \"Direct Inspiration\": [\"b0\", \"b6\"],\n  \"Indirect Inspiration\": [\"b11\", \"b29\", \"b30\"],\n  \"Other Inspiration\": [\"b12\", \"b14\", \"b31\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper revolve around adapting masked signal modeling from NLP to computer vision. The main challenges include dealing with the strong locality of images, predicting low-level continuous visual signals rather than high-level discrete text tokens, and aligning classification-based masked language modeling with continuous visual signals. The paper proposes a simple framework, SimMIM, which leverages random masking, raw pixel regression, and lightweight prediction heads to achieve competitive performance in visual recognition tasks.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1,\n        \"b6\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.8,\n        \"b35\": 0.75,\n        \"b31\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b24\": 0.6,\n        \"b14\": 0.5,\n        \"b20\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of learning visual representations through masked signal modeling by proposing a simple framework, SimMIM, that applies random masking to image patches and uses a linear layer for pixel regression. It aims to bridge the modality gap between NLP and computer vision without relying on complex designs.\",\n    \"Direct Inspiration\": [\"b0\", \"b6\"],\n    \"Indirect Inspiration\": [\"b12\", \"b30\"],\n    \"Other Inspiration\": [\"b1\", \"b14\", \"b35\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a simple framework called SimMIM for masked image modeling (MIM) in the computer vision domain. The primary challenges addressed include adapting masked signal modeling from NLP to visual signals, handling the continuous nature of visual data, and achieving effective representation learning with minimal complexity. The key insights include random masking of image patches, raw pixel value regression, and the use of a lightweight prediction head.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b6\": 0.9,\n    \"b12\": 0.8,\n    \"b35\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.6,\n    \"b31\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.4,\n    \"b29\": 0.4\n  }\n}\n```"], "618895205244ab9dcb40a5db": ["```json\n{\n  \"Summary\": \"The paper identifies key challenges in graph-based tasks, particularly in unsupervised learning settings with limited labels. It critiques current graph contrastive learning (GCL) methods for their reliance on domain-agnostic graph augmentations, which can corrupt task-relevant information. The paper proposes several strategies to improve GCL, inspired by principles from visual contrastive learning (VCL).\",\n  \"Direct Inspiration\": [\"b18\", \"b45\", \"b65\"],\n  \"Indirect Inspiration\": [\"b6\", \"b39\", \"b50\", \"b56\", \"b59\"],\n  \"Other Inspiration\": [\"b31\", \"b52\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of graph contrastive learning (GCL) by critiquing current domain-agnostic graph augmentations (DAGA) and proposing improvements inspired by visual contrastive learning (VCL). The key limitations identified include the destruction of task-relevant information, weak discriminability, and strong inductive bias of random models. The paper proposes context-aware graph augmentations (CAGA) and highlights the need for rigorous evaluation practices.\",\n  \"Direct Inspiration\": [\"b6\", \"b18\", \"b45\", \"b65\"],\n  \"Indirect Inspiration\": [\"b1\", \"b19\", \"b50\", \"b56\", \"b59\"],\n  \"Other Inspiration\": [\"b31\", \"b38\", \"b39\", \"b52\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of domain-agnostic graph augmentations (DAGA) in graph contrastive learning (GCL) and proposes more context-aware strategies to improve task-relevant information preservation. It also highlights the strong inductive bias of random models and the need for robust evaluation practices.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b18\": 0.9,\n    \"b45\": 0.9,\n    \"b65\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.8,\n    \"b47\": 0.8,\n    \"b64\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.7,\n    \"b50\": 0.7,\n    \"b56\": 0.7,\n    \"b59\": 0.7,\n    \"b72\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in graph contrastive learning (GCL) by identifying flaws in domain-agnostic graph augmentations, evaluating the impact of these augmentations on task-relevant information, and proposing new evaluation practices to overcome these limitations.\",\n  \"Direct Inspiration\": [\"b6\", \"b18\", \"b45\", \"b65\"],\n  \"Indirect Inspiration\": [\"b39\", \"b50\", \"b56\", \"b59\"],\n  \"Other Inspiration\": [\"b19\", \"b38\", \"b52\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of graph contrastive learning (GCL) frameworks, particularly focusing on the inadequacies of domain-agnostic graph augmentations (DAGA). It highlights how these augmentations destroy task-relevant information, leading to weakly discriminative representations. The paper proposes several strategies inspired by visual contrastive learning (VCL) to improve GCL, including context-aware graph augmentations and better evaluation practices.\",\n  \"Direct Inspiration\": [\"b6\", \"b18\", \"b45\", \"b65\"],\n  \"Indirect Inspiration\": [\"b46\", \"b47\", \"b64\"],\n  \"Other Inspiration\": [\"b5\", \"b14\", \"b39\", \"b50\"]\n}\n```"], "611106eb91e0117e7f3cdac9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges faced by the semiconductor industry in a post-Moore's Law era, focusing on the increasing manufacturing costs, complexity of design rules, and the relentless demand for more computational power. The proposed solution is a chiplet-based approach to SoC construction, which partitions a traditionally monolithic silicon chip into multiple smaller die or 'chiplets' and reintegrates them to function as a single, logical SoC. This approach aims to mitigate the economic and technical difficulties posed by the slowing pace of Moore's Law.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b6\": 0.9,\n    \"b21\": 0.8,\n    \"b26\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.6,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b9\": 0.4,\n    \"b13\": 0.4,\n    \"b22\": 0.4,\n    \"b23\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges faced by the semiconductor industry in a post-Moore's Law era, particularly the rising manufacturing costs, design complexities, and the need for more computational power. The proposed solution is the chiplet approach, which partitions a monolithic SoC into multiple smaller die or 'chiplets'. This approach aims to reduce costs, improve yields, and enable more efficient use of silicon, thereby extending the benefits of Moore's Law.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.95,\n    \"b13\": 0.9,\n    \"b6\": 0.85,\n    \"b21\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b7\": 0.75,\n    \"b19\": 0.75,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b20\": 0.6,\n    \"b27\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges faced by the semiconductor industry as Moore's Law slows down, leading to increased manufacturing costs, complexity in design rules, and higher demand for computational power. AMD's novel approach of using chiplet-based architectures is proposed as a solution to continue delivering high-performance computing capabilities in a post-Moore's Law world. The chiplet approach partitions a monolithic SoC into multiple smaller die that are reintegrated to function as a single SoC, addressing economic and performance challenges.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b13\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b7\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges posed by the slowing pace of Moore's Law, rising manufacturing costs, and increased design complexity in the semiconductor industry. It proposes a novel chiplet-based architecture to partition monolithic SoCs into multiple smaller die to mitigate these challenges and deliver high-performance, cost-effective solutions. The paper discusses the technical and economic benefits of this approach and its application in AMD's high-performance CPU server space and other markets.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b5\": 0.9,\n    \"b7\": 0.8,\n    \"b29\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b12\": 0.7,\n    \"b14\": 0.7,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b21\": 0.6,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the emerging challenges in the semiconductor industry due to the slowing pace of Moore's Law, rising manufacturing costs, and increased complexity of design rules. AMD proposes a novel chiplet-based architecture to tackle these challenges, aiming to partition traditionally monolithic silicon chips into multiple smaller 'chiplets' to maintain cost efficiency, yield rates, and performance in a post-Moore's Law world.\",\n  \"Direct Inspiration\": [\"b3\", \"b5\", \"b7\", \"b19\", \"b29\"],\n  \"Indirect Inspiration\": [\"b6\", \"b21\", \"b26\", \"b30\"],\n  \"Other Inspiration\": [\"b12\", \"b14\", \"b22\", \"b23\"]\n}\n```"], "611106eb91e0117e7f3cdad1": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of exploiting a new timing channel in modern Intel/AMD processors via the micro-op cache. It introduces a detailed characterization of the micro-op cache, reverse engineers undocumented features, and proposes a framework for generating high-bandwidth timing channel exploits.\",\n  \"Direct Inspiration\": [\"b10\", \"b11\", \"b12\", \"b13\", \"b14\"],\n  \"Indirect Inspiration\": [\"b0\", \"b1\", \"b2\", \"b3\", \"b4\"],\n  \"Other Inspiration\": [\"b19\", \"b20\", \"b21\", \"b22\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of covertly transmitting secret information via a new timing channel in modern Intel/AMD processors, specifically leveraging the micro-op cache. The proposed framework characterizes the micro-op cache and develops a principled approach for generating high-bandwidth timing channel exploits that can bypass existing defenses.\",\n  \"Direct Inspiration\": [\n    \"b10\",\n    \"b11\",\n    \"b12\",\n    \"b13\",\n    \"b14\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b19\",\n    \"b20\",\n    \"b21\",\n    \"b22\",\n    \"b23\",\n    \"b24\",\n    \"b25\",\n    \"b26\",\n    \"b27\",\n    \"b28\",\n    \"b29\",\n    \"b30\",\n    \"b31\"\n  ],\n  \"Other Inspiration\": [\n    \"b0\",\n    \"b1\",\n    \"b2\",\n    \"b3\",\n    \"b4\",\n    \"b5\",\n    \"b6\",\n    \"b7\",\n    \"b8\",\n    \"b9\",\n    \"b32\",\n    \"b33\",\n    \"b34\",\n    \"b35\",\n    \"b36\",\n    \"b37\",\n    \"b38\",\n    \"b39\",\n    \"b40\",\n    \"b41\",\n    \"b42\",\n    \"b43\",\n    \"b44\",\n    \"b45\",\n    \"b46\",\n    \"b47\",\n    \"b48\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies a new timing channel based on the micro-op cache in modern Intel/AMD processors, characterizes its undocumented features, and proposes a framework for generating high-bandwidth micro-op cache-based timing channel exploits.\",\n  \"Direct Inspiration\": [\"b10\", \"b11\", \"b12\", \"b13\", \"b14\"],\n  \"Indirect Inspiration\": [\"b32\", \"b33\", \"b34\", \"b35\", \"b36\", \"b37\"],\n  \"Other Inspiration\": [\"b0\", \"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b6\", \"b7\", \"b8\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper involve the novel discovery and exploitation of the micro-op cache in modern Intel/AMD processors as a side channel for covert information transmission. The authors propose a detailed characterization of the micro-op cache and develop a framework for generating high-bandwidth micro-op cache-based timing channel exploits.\",\n    \"Direct Inspiration\": [\n        \"b10\",\n        \"b14\",\n        \"b19\",\n        \"b32\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b0\",\n        \"b5\",\n        \"b33\",\n        \"b36\"\n    ],\n    \"Other Inspiration\": [\n        \"b38\"\n    ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper outlines the challenge of microarchitectural side-channel attacks, specifically focusing on a new timing channel exploit using the micro-op cache in modern Intel/AMD processors. The authors propose a framework for generating high-bandwidth timing channel exploits by characterizing and understanding the micro-op cache's undocumented features such as its replacement policies and partitioning.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b11\": 0.85,\n    \"b12\": 0.85,\n    \"b13\": 0.85,\n    \"b14\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b1\": 0.75,\n    \"b2\": 0.75,\n    \"b3\": 0.75,\n    \"b4\": 0.75,\n    \"b5\": 0.75,\n    \"b6\": 0.75,\n    \"b7\": 0.75,\n    \"b8\": 0.75,\n    \"b9\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.7,\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7,\n    \"b36\": 0.7,\n    \"b37\": 0.7,\n    \"b38\": 0.7,\n    \"b39\": 0.7,\n    \"b40\": 0.7,\n    \"b41\": 0.7,\n    \"b42\": 0.7,\n    \"b43\": 0.7,\n    \"b44\": 0.7,\n    \"b45\": 0.7,\n    \"b46\": 0.7,\n    \"b47\": 0.7,\n    \"b48\": 0.7\n  }\n}\n```"], "61c145c85244ab9dcb851867": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatic diagnosis systems that need to inquire implicit symptoms from patients to make accurate disease diagnoses. The proposed method, Diaformer, uses a Transformer-based architecture with a symptom attention framework and three orderless training mechanisms to improve efficiency and explainability in the symptom inquiry process.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b20\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b7\": 0.7,\n    \"b16\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenges of automatic diagnosis by proposing a Sequence Generation (SG) task approach to improve the efficiency and explainability of multi-step reasoning processes. It introduces a new model 'Diaformer' based on Transformer architecture, incorporating a symptom attention framework and three orderless training mechanisms to address the discrepancy between the sequential generation and the disorder of given implicit symptoms sets.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b20\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b18\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.75,\n    \"b16\": 0.7,\n    \"b9\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatic diagnosis through multi-step reasoning and proposes a novel Sequence Generation (SG) approach using a Transformer-based model (Diaformer). The method aims to improve the efficiency and explainability of the diagnostic process by modeling symptom inquiry as a sequence generation task, introducing a symptom attention framework, and three orderless training mechanisms to handle implicit symptoms.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b20\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.8,\n    \"b16\": 0.8,\n    \"b9\": 0.7,\n    \"b4\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency and lack of explainability in the existing RL-based methods for automatic diagnosis, which do not directly capture the correlation among symptoms and the standard diagnosis paradigm. The proposed algorithm, Diaformer, reformulates the automatic diagnosis task as a Sequence Generation (SG) task, leveraging a Transformer-based architecture with a symptom attention framework and three orderless training mechanisms to improve the efficiency and accuracy of symptom inquiry and disease diagnosis.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b20\": 1,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b8\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in automatic diagnosis, focusing on capturing dynamics and uncertainties in multi-step reasoning with limited data. The proposed method reformulates this as a Sequence Generation (SG) task, introducing a Transformer-based model (Diaformer) with a symptom attention framework and three orderless training mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b20\": 1.0,\n    \"b5\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```"], "60cafe9e91e011b3293742e4": ["```json\n{\n    \"Summary\": \"The main challenge addressed by the paper is the efficient generation of molecular conformers, which are low-energy 3D structures of molecules. The proposed model, GEOMOL, is designed to generate high-quality, representative, diverse, and generalizable low-energy 3D conformational ensembles from molecular graphs using a fast, end-to-end trainable approach that avoids the drawbacks of existing Distance Geometry (DG) techniques and force field (FF) energy optimizations.\",\n    \"Direct Inspiration\": {\n        \"b33\": 0.9,\n        \"b46\": 0.9,\n        \"b53\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.7,\n        \"b40\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b13\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of molecular conformer generation (MCG), focusing on predicting low-energy 3D conformations of small molecules from their molecular graphs. The main challenges include the vast 3D structure space, constraints imposed by the molecular graph, and the limitations of existing stochastic and rule-based methods. The proposed model, GEOMOL, is an end-to-end trainable, non-autoregressive ML model that avoids Distance Geometry (DG) techniques and explicitly models essential molecular geometry elements.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b46\": 0.85,\n    \"b53\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.7,\n    \"b21\": 0.65,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.55,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of molecular conformer generation (MCG) by introducing GEOMOL, a fast ML generative model designed to predict high-quality, representative, diverse, and generalizable low-energy 3D conformational ensembles from molecular graphs. Key challenges include the vast 3D structure space and the limitations of existing stochastic and systematic methods. GEOMOL is end-to-end trainable, non-autoregressive, and avoids the drawbacks of Distance Geometry techniques. It incorporates SE(3)-invariant modeling, explicitly predicts essential molecular geometry elements, and uses a tailored generative loss for diverse conformer generation without adversarial training.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1,\n    \"b46\": 1,\n    \"b53\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.8,\n    \"b13\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of molecular conformer generation (MCG), aiming to predict the ensemble of low-energy 3D conformations of small molecules based on their molecular graph. The key contributions include an end-to-end trainable model called GEOMOL, which avoids Distance Geometry techniques, models conformers in an SE(3)-invariant manner, explicitly predicts essential molecular geometry elements, and promotes diverse conformer ensembles using a tailored generative loss. The challenges tackled include the high dimensionality of the conformer space, limitations of existing stochastic and rule-based methods, and issues with force-field energy optimizations.\",\n  \"Direct Inspiration\": {\n    \"b46\": 0.95,\n    \"b53\": 0.9,\n    \"b33\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.75,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.65,\n    \"b1\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of molecular conformer generation (MCG), proposing the GEOMOL model which avoids traditional Distance Geometry (DG) techniques and directly predicts geometric quantities required for 3D conformer reconstruction. The challenges include the large size of 3D structure space, the need for SE(3) invariance, and the limitations of existing stochastic and rule-based methods. GEOMOL introduces an end-to-end trainable, non-autoregressive model that predicts torsion angles and local 3D structures and uses optimal transport-based loss to promote diversity in conformer ensembles.\",\n  \"Direct Inspiration\": {\n    \"b46\": 1.0,\n    \"b33\": 1.0,\n    \"b53\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.6,\n    \"b1\": 0.5,\n    \"b5\": 0.5\n  }\n}\n```"], "61a884d96750f87bf87020f3": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Weak generalization of GNNs due to small and sparsely labeled graph datasets.\",\n    \"Inspirations\": \"Data augmentation in image recognition and recent advancements in leveraging unlabeled data and advanced augmentation methods.\"\n  },\n  \"Direct Inspiration\": [\n    \"b27\",\n    \"b28\",\n    \"b29\",\n    \"b30\",\n    \"b31\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b34\",\n    \"b35\",\n    \"b37\"\n  ],\n  \"Other Inspiration\": [\n    \"b40\",\n    \"b41\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of weak generalization of Graph Neural Networks (GNNs) due to small and sparsely labeled graph datasets. It proposes a novel framework called Metropolis-Hastings Data Augmentation (MH-Aug) to draw augmented graphs from an explicit target distribution with the desired strength and diversity for semi-supervised learning. The framework leverages the Metropolis-Hastings algorithm to obtain augmented samples and adopts consistency training to utilize regularizers for unlabeled data.\",\n  \"Direct Inspiration\": [\"b30\", \"b31\", \"b38\"],\n  \"Indirect Inspiration\": [\"b27\", \"b28\", \"b29\"],\n  \"Other Inspiration\": [\"b12\", \"b15\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Weak generalization of GNNs due to small and sparsely labeled graph datasets, difficulty in designing effective and label-preserving data augmentation for graphs.\",\n    \"Inspirations\": \"Adopting advanced augmentation methods and leveraging unlabeled data for semi-supervised learning.\"\n  },\n  \"Direct Inspiration\": [\"b30\", \"b31\"],\n  \"Indirect Inspiration\": [\"b27\", \"b28\", \"b29\"],\n  \"Other Inspiration\": [\"b12\", \"b13\", \"b14\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of weak generalization in Graph Neural Networks (GNNs) due to small and sparsely labeled datasets. It proposes a novel data augmentation framework called Metropolis-Hastings Data Augmentation (MH-Aug) to generate augmented graphs from an explicit target distribution. This method aims to control the strength and diversity of augmentation and improve the performance of GNNs in semi-supervised learning settings.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.9,\n    \"b31\": 0.9,\n    \"b34\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.7,\n    \"b28\": 0.7,\n    \"b29\": 0.7,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of weak generalization in Graph Neural Networks (GNNs) due to small and sparsely labeled graph datasets. The authors propose a novel framework, Metropolis-Hastings Data Augmentation (MH-Aug), which uses the Metropolis-Hastings algorithm to generate augmented graphs from an explicit target distribution with desired strength and diversity for semi-supervised learning. The framework aims to improve the performance of GNNs by leveraging unlabeled data and utilizing consistency training regularizers.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.9,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b28\": 0.8,\n    \"b29\": 0.8,\n    \"b34\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```"], "61d3bae45244ab9dcba351c0": ["```json\n{\n    \"Summary\": \"The paper proposes a novel GNN framework, termed KerGNNs, which combines GNNs and graph kernels to improve the expressivity of GNNs beyond the limitations of the 1-WL isomorphism test. The primary challenges addressed include the limited expressivity of traditional GNNs and the hand-crafted feature limitations of graph kernels. Key contributions include a subgraph-based node aggregation algorithm, a new kernel perspective to generalize CNNs into the graph domain, and improved model interpretability through visualizing trained graph filters.\",\n    \"Direct Inspiration\": [\"b10\", \"b12\", \"b28\"],\n    \"Indirect Inspiration\": [\"b4\", \"b8\", \"b15\"],\n    \"Other Inspiration\": [\"b14\", \"b32\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of Message Passing Neural Networks (MPNNs) in graph isomorphism tests and proposes a novel subgraph-based node aggregation algorithm combining Graph Neural Networks (GNNs) and graph kernels to enhance expressivity. The proposed KerGNN framework leverages the advantages of both methods, introduces trainable graph filters, and provides interpretability through visualizing trained graph filters.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b12\": 1.0,\n    \"b15\": 0.9,\n    \"b4\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b8\": 0.6,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b21\": 0.5,\n    \"b34\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the expressivity limits of Graph Neural Networks (GNNs) and proposes a subgraph-based node aggregation algorithm, termed KerGNNs, which combines GNNs with graph kernels to enhance their expressivity. The main contributions include using neighborhood subgraph topology combined with kernel methods for GNN neighborhood aggregation, generalizing CNNs into the graph domain through kernel methods, and improving model interpretability by visualizing trained graph filters.\",\n  \"Direct Inspiration\": [\"b32\", \"b15\", \"b26\", \"b14\", \"b10\", \"b12\"],\n  \"Indirect Inspiration\": [\"b23\", \"b16\", \"b19\", \"b8\", \"b4\", \"b28\", \"b1\", \"b21\", \"b34\"],\n  \"Other Inspiration\": [\"b27\", \"b36\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations in the expressivity of current Graph Neural Networks (GNNs) and proposes a subgraph-based node aggregation algorithm that combines GNNs and graph kernels into one framework. The proposed model, named KerGNNs, leverages the advantages of both approaches to improve expressivity beyond the 1-WL algorithm and provide better interpretability.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1,\n    \"b15\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.85,\n    \"b12\": 0.85,\n    \"b14\": 0.75,\n    \"b27\": 0.75,\n    \"b36\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.7,\n    \"b16\": 0.65,\n    \"b19\": 0.65,\n    \"b8\": 0.65,\n    \"b4\": 0.65,\n    \"b28\": 0.6,\n    \"b21\": 0.5,\n    \"b34\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations in the expressivity of Graph Neural Networks (GNNs) by proposing a novel subgraph-based node aggregation algorithm that combines GNNs and graph kernels. The main contributions include overcoming the 1-Weisfeiler-Lehman (1-WL) isomorphism test limitations, generalizing Convolutional Neural Networks (CNNs) into the graph domain, and enhancing model interpretability through the visualization of trained graph filters.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b12\": 0.9,\n    \"b15\": 0.8,\n    \"b26\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.6,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b32\": 0.5\n  }\n}\n```"], "6168f19c5244ab9dcbe2f9a1": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving multilingual, multisource, open-domain QA systems to enhance the experience for non-English speaking users. It introduces CROSS-GENQA, a technique for generating full-sentence answers using sentence candidates in multiple languages. The authors also present the GEN-TYDIQA dataset, extending TyDiQA with human-generated answers in Arabic, Bengali, English, Russian, and Japanese. The paper highlights the importance of human-generated answers for evaluation and shows that CROSS-GENQA outperforms existing models.\",\n  \"Direct Inspiration\": [\"b25\"],\n  \"Indirect Inspiration\": [\"b13\", \"b21\", \"b41\", \"b66\"],\n  \"Other Inspiration\": [\"b6\", \"b42\", \"b24\", \"b32\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving coverage for multilingual, multisource, open-domain QA systems by leveraging large-scale language models and introducing a novel dataset (GEN-TYDIQA) for generating fluent, human-like answers in multiple languages. The proposed system, CROSSGENQA, combines document retrieval, answer sentence selection (AS2), and generative models to produce complete sentence answers in a cross-lingual setting.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b13\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b66\": 0.7,\n    \"b29\": 0.6,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is improving multilingual, open-domain question answering (QA) for non-English speaking users. The authors propose a new method, CROSS-GENQA, that generates full-sentence answers using sentence candidates written in multiple languages. They also introduce a new dataset, GEN-TYDIQA, which extends TyDiQA to include human-generated answers in Arabic, Bengali, English, Russian, and Japanese.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b21\": 0.9,\n    \"b66\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7,\n    \"b45\": 0.7,\n    \"b62\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving multilingual, multisource, open-domain QA for non-English languages. It introduces CROSSGENQA, a system that generates full-sentence answers from multilingual sources, and the GEN-TYDIQA dataset, which extends TyDiQA with human-generated answers in multiple languages.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b21\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b62\": 0.8,\n    \"b41\": 0.8,\n    \"b66\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b50\": 0.7,\n    \"b57\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving multilingual, multisource, open-domain Question Answering (QA) systems. The main contribution is the introduction of CROSSGENQA, a generative model that creates full-sentence answers using sentence candidates written in multiple languages, including languages different from the question and English. Another significant contribution is the creation of the GEN-TYDIQA dataset, which extends the TyDiQA dataset by including human-generated, fluent, self-contained answers in multiple languages.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.85,\n    \"b21\": 0.80,\n    \"b41\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b66\": 0.70,\n    \"b45\": 0.60,\n    \"b62\": 0.55\n  }\n}\n```"], "611106eb91e0117e7f3cdaa9": ["```json\n{\n  \"Summary\": \"The paper addresses performance and complexity challenges in virtual memory (VM) systems for big-memory architectures. The proposed solution, Midgard, introduces an intermediate address space to reduce the overhead of virtual-to-physical address translation by leveraging cache hierarchies. Midgard integrates virtual memory areas (VMAs) into hardware, aiming to simplify address translation and improve performance without sacrificing programmability.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b15\": 0.9,\n    \"b64\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.7,\n    \"b27\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b7\": 0.6,\n    \"b14\": 0.6,\n    \"b25\": 0.6,\n    \"b56\": 0.6,\n    \"b60\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance and implementation complexity challenges of virtual memory (VM) in emerging big-memory systems. It proposes the Midgard abstraction to mitigate VM overheads by redirecting most translation activity to cache hierarchies rather than specialized hardware, thus simplifying address translation and improving performance.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b15\": 0.9,\n    \"b64\": 0.9,\n    \"b31\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b27\": 0.7,\n    \"b49\": 0.7,\n    \"b50\": 0.7,\n    \"b37\": 0.65,\n    \"b54\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b40\": 0.6,\n    \"b12\": 0.55,\n    \"b18\": 0.55,\n    \"b25\": 0.5,\n    \"b56\": 0.5,\n    \"b60\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of performance and implementation complexity in virtual memory (VM) abstractions for big-memory systems. It proposes Midgard, an intermediate address space that leverages larger cache hierarchies to reduce address translation overheads, thus improving VM performance without compromising programmability.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b15\": 0.9,\n    \"b64\": 0.9,\n    \"b31\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.7,\n    \"b24\": 0.7,\n    \"b60\": 0.7,\n    \"b27\": 0.6,\n    \"b37\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b49\": 0.5,\n    \"b50\": 0.5,\n    \"b12\": 0.45,\n    \"b18\": 0.45,\n    \"b35\": 0.45,\n    \"b42\": 0.4,\n    \"b53\": 0.4,\n    \"b46\": 0.35,\n    \"b47\": 0.35,\n    \"b66\": 0.35,\n    \"b34\": 0.3,\n    \"b39\": 0.3,\n    \"b58\": 0.3,\n    \"b59\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of virtual memory (VM) abstraction in big-memory systems, focusing on performance and complexity issues related to address translation. It proposes the Midgard abstraction as a novel solution, which introduces an intermediate address space leveraging existing OS concepts of VMAs to optimize memory access and translation.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b10\": 1.0,\n    \"b15\": 1.0,\n    \"b64\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b2\": 0.7,\n    \"b27\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b18\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.6,\n    \"b49\": 0.6,\n    \"b50\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance and complexity challenges faced by the Virtual Memory (VM) abstraction in emerging big-memory systems. It proposes an innovative solution called Midgard, which introduces an intermediate address space leveraging Virtual Memory Areas (VMAs) to reduce the overhead of address translation and improve system performance.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.85,\n    \"b15\": 0.8,\n    \"b64\": 0.8,\n    \"b31\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b27\": 0.55,\n    \"b49\": 0.5,\n    \"b50\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.4,\n    \"b24\": 0.4,\n    \"b60\": 0.4,\n    \"b12\": 0.35,\n    \"b18\": 0.35\n  }\n}\n```"], "6059b47291e011ed950a59c6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of synthesizing high-fidelity audio-driven facial video sequences that are capable of natural speaking styles and photo-realistic streaming results. The proposed algorithm, AD-NeRF, utilizes neural radiance fields (NeRF) to map audio features directly to dynamic neural radiance fields without intermediate representations. This method aims to overcome issues such as semantic mismatches and limitations in current GAN-based methods by providing better image quality and flexibility in head pose and background manipulation.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.95,\n    \"b22\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.75,\n    \"b27\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.65,\n    \"b20\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of synthesizing high-fidelity, audio-driven facial video sequences without introducing intermediate representations that may cause information loss. The proposed method, AD-NeRF, leverages neural radiance fields (NeRF) for this purpose, allowing for high-quality image rendering and advanced editing tasks.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b34\": 0.6,\n    \"b43\": 0.55,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in synthesizing high-fidelity audio-driven facial video sequences, particularly focusing on the limitations of existing methods that use intermediate representations causing information loss. The novel contribution, AD-NeRF, proposes a direct mapping of audio features to dynamic neural radiance fields without intermediate modalities, leading to better lip motion and natural talking head results. The method decomposes neural radiance fields into two branches for head and torso, enabling advanced editing tasks such as pose manipulation and background replacement.\",\n    \"Direct Inspiration\": [\"b14\", \"b22\"],\n    \"Indirect Inspiration\": [],\n    \"Other Inspiration\": [\"b0\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of synthesizing high-fidelity audio-driven facial video sequences without using intermediate face representations, which often lead to information loss and semantic mismatches. The authors propose an audio-driven neural radiance fields (AD-NeRF) model that directly maps audio features to dynamic neural radiance fields, enabling more accurate and natural talking-head sequences. The key innovations include the direct mapping of audio features to neural radiance fields, the decomposition of neural radiance fields into head and torso components, and the capability for advanced editing tasks like pose manipulation and background replacement.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.7,\n    \"b43\": 0.6,\n    \"b33\": 0.6,\n    \"b37\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Synthesizing high-fidelity audio-driven facial video sequences\",\n      \"Faithfully relating audio signals and face deformations\",\n      \"Avoiding information loss from intermediate representations\",\n      \"Rendering full talking-head sequences including pose manipulation and background replacement\"\n    ],\n    \"Inspirations\": [\n      \"Dynamic NeRF for modeling appearance and dynamics of a human face\",\n      \"Neural radiance fields for scene representation\"\n    ]\n  },\n  \"Direct Inspiration\": [\"b14\", \"b22\"],\n  \"Indirect Inspiration\": [\"b0\", \"b20\"],\n  \"Other Inspiration\": [\"b43\", \"b35\", \"b33\"]\n}\n```"], "61147c059e795e94de704b33": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality talking face videos with personalized head poses from audio signals. It proposes a novel deep neural network model that leverages 3D facial animation and a memory-augmented GAN module to achieve this. The method is distinct in its ability to generalize to arbitrary target persons, unlike previous methods that required extensive training data for specific individuals.\",\n  \"Direct Inspiration\": [\n    \"b10\",\n    \"b11\",\n    \"b5\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b1\",\n    \"b3\",\n    \"b8\",\n    \"b7\",\n    \"b38\"\n  ],\n  \"Other Inspiration\": [\n    \"b2\",\n    \"b13\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"Generating high-quality talking face video with personalized head pose and lip synchronization from audio and short video input.\",\n        \"inspirations\": \"Combining audio and visual information for realistic head movement and facial expression, utilizing 3D face reconstruction and GANs.\"\n    },\n    \"Direct Inspiration\": {\n        \"b11\": \"Pose-from-speech inference\",\n        \"b38\": \"3D face reconstruction method\"\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": \"Audio to facial landmarks and video generation\",\n        \"b3\": \"Joint embedding of face and audio signal\",\n        \"b7\": \"Conditional recurrent adversarial network\",\n        \"b8\": \"Joint audio-visual representation\"\n    },\n    \"Other Inspiration\": {\n        \"b6\": \"High-quality talking face video generation for a specified person\"\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating high-quality talking face videos with personalized head poses from audio inputs. It proposes a novel deep neural network model that incorporates 3D face reconstruction and a memory-augmented GAN to achieve realistic video synthesis. The model uniquely combines audio and short video inputs to learn personalized talking behavior and refine the rendered frames into realistic animations.\",\n    \"Direct Inspiration\": {\n        \"b11\": 0.9,\n        \"b1\": 0.85,\n        \"b3\": 0.85,\n        \"b8\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.7,\n        \"b38\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b13\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating high-quality talking face videos with personalized head poses from audio signals. The authors propose a novel deep neural network model augmented with a memory-augmented GAN module to achieve realistic video frames, bridging the gap between audio-visual-driven head pose learning and realistic video generation.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b1\", \"b3\", \"b6\", \"b7\", \"b8\", \"b11\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b2\", \"b10\", \"b38\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b13\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality talking face videos with personalized head poses driven by audio signals. The proposed method introduces a deep neural network model that integrates 3D facial animation and a novel memory-augmented GAN module to achieve realistic video synthesis. The solution involves learning from both audio and video inputs to model personalized talking behavior, including lip synchronization and head movements.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b3\": 0.9,\n    \"b1\": 0.85,\n    \"b2\": 0.8,\n    \"b11\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b10\": 0.65,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.55,\n    \"b13\": 0.5\n  }\n}\n```"], "605aa2abe4510cd7c86bf129": ["```json\n{\n  \"Summary\": \"The paper addresses the need for an accurate driver stress detection system by proposing a multimodal fusion model based on an attentional CNN-LSTM network. The model integrates eye data, vehicle dynamics data, and environmental data to improve classification performance of driver stress levels.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b30\": 0.9,\n    \"b59\": 0.88,\n    \"b64\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.8,\n    \"b38\": 0.78,\n    \"b33\": 0.75,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.65,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting driver stress levels using a multimodal fusion model based on attentional CNN-LSTM networks. The proposed model integrates eye data, vehicle dynamics data, and environmental data to classify driver stress levels into low, medium, and high. Key contributions include the use of self-attention mechanisms to weigh features from different modalities and improve classification performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b30\": 1.0,\n    \"b59\": 1.0,\n    \"b64\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b45\": 0.8,\n    \"b61\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b27\": 0.6,\n    \"b46\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately detecting driver stress levels using multimodal data (eye data, vehicle dynamics data, and environmental data). It proposes a novel multimodal fusion model based on an attentional CNN-LSTM network, utilizing self-attention mechanisms to better integrate and weigh features from different modalities.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b30\": 1.0,\n    \"b50\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.8,\n    \"b43\": 0.8,\n    \"b29\": 0.8,\n    \"b45\": 0.8,\n    \"b61\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b59\": 0.6,\n    \"b64\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting driver stress levels accurately, which is crucial for reducing road traffic crashes. The authors propose a multimodal fusion model based on an attentional CNN-LSTM network to integrate eye data, vehicle data, and environmental data. This model aims to improve feature extraction and classification performance for driver stress detection.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b59\": 1,\n    \"b64\": 0.9,\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.8,\n    \"b6\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b62\": 0.7,\n    \"b42\": 0.7,\n    \"b30\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are the detection of driver stress levels using multimodal data, including eye data, vehicle dynamics data, and environmental data. The paper proposes a multimodal fusion model based on an attentional CNN-LSTM network to effectively fuse features from these different modalities and improve the performance of driver stress level classification.\",\n    \"Direct Inspiration\": [\"b1\", \"b30\", \"b59\", \"b64\"],\n    \"Indirect Inspiration\": [\"b5\", \"b33\", \"b50\"],\n    \"Other Inspiration\": [\"b2\", \"b4\", \"b6\", \"b42\", \"b45\"]\n}\n```"], "60de83cf91e0110ac15e45f4": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of computational efficiency in cross-lingual language model pre-training. It introduces XLM-E, which leverages ELECTRA-style tasks (multilingual replaced token detection and translation replaced token detection) to achieve significant computational speedup and competitive performance on cross-lingual benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b12\": 1,\n    \"b6\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.9,\n    \"b14\": 0.8,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high computation costs in cross-lingual pre-training for language models. It proposes the XLM-E model, which employs ELECTRA-style discriminative tasks for more efficient and effective cross-lingual pre-training. The model introduces two new tasks: multilingual replaced token detection (MRTD) and translation replaced token detection (TRTD). These tasks aim to reduce computation costs while maintaining high performance in cross-lingual transfer and understanding.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b6\": 0.9,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b14\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of high computation costs in cross-lingual pre-training for language models and introduces XLM-E, a model that uses ELECTRA-style discriminative pre-training tasks to achieve significant computational efficiency and competitive cross-lingual transferability. The key contributions are the introduction of multilingual replaced token detection and translation replaced token detection tasks.\",\n    \"Direct Inspiration\": {\n        \"b10\": 1,\n        \"b12\": 0.9,\n        \"b6\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.7,\n        \"b11\": 0.6,\n        \"b7\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5,\n        \"b8\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of computational efficiency in cross-lingual language model pre-training. It introduces XLM-E, which uses ELECTRA-style tasks for more efficient and effective cross-lingual transfer. The key contributions include two discriminative pre-training tasks and a novel pre-training algorithm that significantly reduces computation cost while maintaining high performance in cross-lingual understanding tasks.\",\n  \"Direct Inspiration\": [\"b10\"],\n  \"Indirect Inspiration\": [\"b6\", \"b12\"],\n  \"Other Inspiration\": [\"b2\", \"b11\", \"b7\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the high computational cost and inefficiency of existing cross-lingual pre-training models. The proposed XLM-E model introduces more compute-efficient and sample-efficient pre-training tasks, specifically multilingual replaced token detection and translation replaced token detection. The model leverages ELECTRA-style tasks to achieve significant computation cost reduction while maintaining competitive performance in cross-lingual understanding tasks.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b6\": 0.9,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b14\": 0.4,\n    \"b45\": 0.4,\n    \"b15\": 0.4\n  }\n}\n```"], "6201df4b5aee126c0f64e029": ["```json\n{\n  \"Summary\": \"The paper addresses the limitations of Graph Neural Networks (GNNs) such as limited expressiveness, over-smoothing, and over-squashing by integrating structural information into a Transformer architecture. The proposed method, Structure-Aware Transformer (SAT), introduces a flexible structure-aware self-attention mechanism that captures both structural and attributed similarities between nodes.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9,\n    \"b28\": 0.8,\n    \"b39\": 0.8,\n    \"b6\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.6,\n    \"b1\": 0.6,\n    \"b22\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.4,\n    \"b34\": 0.4,\n    \"b19\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of Graph Neural Networks (GNNs) such as limited expressiveness, over-smoothing, and over-squashing by introducing a structure-aware transformer (SAT). This new transformer model incorporates structural information into the self-attention mechanism, making it more expressive and effective for graph representation learning.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b39\": 0.8,\n    \"b25\": 0.7,\n    \"b23\": 0.7,\n    \"b3\": 0.7,\n    \"b32\": 0.7,\n    \"b1\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b22\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges of limited expressiveness, over-smoothing, and over-squashing in Graph Neural Networks (GNNs) by proposing a novel structure-aware transformer (SAT) that incorporates both local and global graph structure into the self-attention mechanism. This method extends the traditional self-attention mechanism by integrating subgraph representations to capture structural interactions between nodes, and demonstrates superior performance on multiple benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b28\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.8,\n    \"b24\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b17\": 0.5,\n    \"b9\": 0.5,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the limited expressiveness, over-smoothing, and over-squashing of graph neural networks (GNNs). The proposed algorithm is the structure-aware transformer (SAT), which introduces a flexible structure-aware self-attention mechanism that explicitly considers the graph structure to capture structural interaction between nodes.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.9,\n    \"b6\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b22\": 0.6,\n    \"b19\": 0.5,\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in this paper involve the limited expressiveness, over-smoothing, and over-squashing in Graph Neural Networks (GNNs). The proposed algorithm, the Structure-Aware Transformer (SAT), introduces a flexible self-attention mechanism that explicitly considers the graph structure to capture structural interactions between nodes. This approach aims to provide more expressive node representations and address the shortcomings of existing position-aware transformers.\",\n  \"Direct Inspiration\": [\"b37\", \"b28\"],\n  \"Indirect Inspiration\": [\"b6\", \"b22\", \"b39\"],\n  \"Other Inspiration\": [\"b11\", \"b12\"]\n}\n```"], "600830ff9e795ed227f5308f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of capturing the multi-level structure of proteins (primary, secondary, tertiary, and quaternary structures) in a unified learning algorithm. It proposes a novel end-to-end protein learning algorithm using a multi-graph data structure and a new convolution operator that incorporates both intrinsic and extrinsic geometric properties. The method also includes protein-specific pooling operations to handle varying protein sizes.\",\n  \"Direct Inspiration\": [\"b32\", \"b22\"],\n  \"Indirect Inspiration\": [\"b45\", \"b2\", \"b38\", \"b48\", \"b4\", \"b14\"],\n  \"Other Inspiration\": [\"b41\", \"b34\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The paper addresses the challenge of capturing the multi-level structure of proteins, including primary, secondary, tertiary, and quaternary structures, in a unified learning model. Traditional methods often focus on one or a subset of these levels, leading to incomplete representations.\",\n    \"inspirations\": \"The paper is inspired by the success of convolutional neural networks (CNNs) in image processing and point cloud learning, and aims to extend these concepts to the domain of protein structures.\"\n  },\n  \"Direct Inspiration\": {\n    \"b32\": 0.95,\n    \"b22\": 0.9,\n    \"b27\": 0.85,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b7\": 0.7,\n    \"b38\": 0.7,\n    \"b45\": 0.7,\n    \"b36\": 0.65,\n    \"b35\": 0.65,\n    \"b4\": 0.65,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.5,\n    \"b26\": 0.5,\n    \"b23\": 0.5,\n    \"b43\": 0.5,\n    \"b42\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of capturing the multi-level structural invariances of proteins, which are often missed by existing methods that focus on just one or a subset of structural levels. It proposes a novel end-to-end protein learning algorithm using a multi-graph representation to simultaneously incorporate primary, secondary, and tertiary structures of proteins and introduces a new convolution operator inspired by differential geometry.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b4\": 0.6,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.5,\n    \"b36\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a novel end-to-end protein learning algorithm capable of explicitly incorporating the multi-level structure of proteins to capture the resulting different invariances. It proposes a multi-graph data structure to represent primary, secondary, and tertiary structures and defines a new convolution operator using intrinsic and extrinsic distances. Additionally, the paper proposes protein-specific pooling operations to handle proteins of varying sizes, enabling the detection of features at different scales.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b7\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b36\": 0.6,\n    \"b45\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately modeling protein structures by capturing invariances across multiple structural levels (primary, secondary, tertiary, and quaternary). The authors propose a novel end-to-end protein learning algorithm using a multi-graph representation and a new convolution operator inspired by differential geometry of surfaces to capture intrinsic and extrinsic distances.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b32\": 0.85,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b45\": 0.75,\n    \"b2\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b35\": 0.65\n  }\n}\n```"], "60af77829e795e6b8e55c84c": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the construction of taxonomic trees using pretrained language models, specifically focusing on the task of organizing a set of input terms into a taxonomic tree. The proposed algorithm, CTP, finetunes pretrained models to predict pairwise parent-child relations and then reconciles these predictions with a maximum spanning tree algorithm. The approach is tested both with and without web-retrieved glosses, showing significant improvements in F1 score over previous methods.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b25\": 0.6,\n    \"b12\": 0.6,\n    \"b33\": 0.6,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of constructing taxonomic trees automatically, which are typically incomplete and expensive when curated manually. To tackle this, the authors propose a novel approach called CTP (Constructing Taxonomies from Pretrained Models), which uses pretrained language models to predict parent-child relations and the Chu-Liu-Edmonds algorithm to reconcile these predictions into a tree-structured taxonomy. The approach is evaluated on the WORDNET dataset and its multilingual variants, showing significant improvements in F1 score over previous methods.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the construction of taxonomic trees using pretrained language models without relying on direct corpus statistics at test time. The proposed algorithm, CTP, fine-tunes pretrained language models to predict pairwise parent-child relations and reconciles these predictions into a tree structure using the Chu-Liu-Edmonds algorithm. The paper also explores the use of web-retrieved glosses to further improve the accuracy of taxonomy construction.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b10\": 0.8,\n    \"b11\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b18\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of constructing taxonomic trees using pretrained language models (CTP) rather than relying on traditional corpus statistics. The proposed CTP method involves two main steps: parenthood prediction using fine-tuned pretrained models and tree reconciliation with a maximum spanning tree algorithm. The approach is tested across multiple languages using synset alignments from the Open Multilingual Wordnet, and it shows significant improvements in F1 scores compared to previous methods.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b20\": 0.8,\n        \"b10\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b4\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of constructing taxonomic trees using pretrained language models (CTP). It highlights the limitations of manually curated taxonomies and traditional methods relying on corpus statistics. The proposed algorithm fine-tunes pretrained models to predict parent-child relations and uses the Chu-Liu-Edmonds algorithm for tree reconciliation. The paper shows significant improvement in F1 score for constructing WORDNET subtrees, especially with the use of web-retrieved glosses.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6\n  }\n}\n```"], "619ba6861c45e57ce99e8f8c": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include handling the weakly supervised learning regime for social media posts, the lack of explainability in deep learning models, and the importance of temporal dependency between posts for depression detection. The authors propose using a Transformer architecture to address these limitations, introducing a new suite of tests called DEPRESSION CHECKLIST, and leveraging multimodal data for better model performance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b6\": 0.9,\n    \"b8\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b7\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of current methodologies in detecting depression using social media data with a focus on Multiple Instance Learning (MIL), the lack of explainability in deep learning models, and the importance of temporal dependencies between posts. The authors propose a new suite of tests called DEPRESSION CHECKLIST and aim to utilize Transformer architectures to model dependencies and multimodality in social media posts.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b4\": 0.7,\n    \"b10\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b7\": 0.5,\n    \"b9\": 0.5,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of detecting depression from social media posts using a Multiple Instance Learning (MIL) approach and Transformer architecture. Key challenges include the weakly supervised nature of social media data, the need for explainability in deep learning models, and the importance of temporal dependency between posts. The authors propose a new suite of tests called DEPRESSION CHECKLIST and aim to leverage the attention mechanism in Transformers for improved performance.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b2\", \"b10\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b6\", \"b8\", \"b1\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b7\", \"b9\", \"b3\", \"b4\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the proper formulation of social media posts as a Multiple Instance Learning (MIL) task, the lack of explainability in deep learning models, and the importance of considering temporal dependencies in posts for depression detection. The proposed algorithm leverages a Transformer architecture to model dependencies between posts and incorporates a DEPRESSION CHECKLIST to evaluate model performance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b8\": 1.0,\n    \"b1\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.6,\n    \"b9\": 0.6,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving depression detection using social media data by leveraging a Multiple Instance Learning (MIL) approach and deep learning models. Key inspirations include handling social media posts as a MIL task, improving explainability of deep learning models, and considering temporal dependency in posts. The authors propose a specialized evaluation suite called DEPRESSION CHECKLIST and aim to use Transformer architecture to model dependencies and multimodality in social media posts.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b8\": 1.0,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.7,\n    \"b7\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6\n  }\n}\n```"], "6020defd9e795e62379b0d54": ["```json\n{\n  \"Summary\": \"The paper addresses the problem of multimodal learning with severely missing modality, proposing a Bayesian meta-learning based solution. The main challenges include handling incomplete modalities in both training and testing, ensuring flexibility and efficiency, and overcoming the limitations of generative approaches like AE, VAE, and GAN.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b14\": 0.9,\n    \"b18\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b7\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b40\": 0.6,\n    \"b49\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multimodal learning with severely missing modalities, proposing a Bayesian meta-learning framework to improve flexibility and efficiency in such scenarios. The model effectively handles missing data during both training and testing phases and uses feature reconstruction and uncertainty-guided feature regularization to achieve robust performance.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b3\", \"b14\", \"b18\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b22\", \"b41\", \"b30\", \"b40\", \"b6\", \"b13\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b19\", \"b28\", \"b4\", \"b7\", \"b17\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multimodal learning with severely missing modality, proposing a Bayesian meta-learning framework to handle up to 90% missing modality in training data. The method aims to achieve flexibility in handling missing modalities during training and testing, and efficiency in learning from incomplete data.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9,\n    \"b4\": 0.8,\n    \"b7\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b13\": 0.7,\n    \"b19\": 0.7,\n    \"b40\": 0.7,\n    \"b49\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of multimodal learning with severely missing modalities, proposing a Bayesian meta-learning algorithm to handle high ratios of missing data (up to 90%). Two key objectives are flexibility and efficiency, which are achieved through a feature reconstruction network and a feature regularization approach.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9,\n        \"b14\": 0.8,\n        \"b18\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.6,\n        \"b19\": 0.65,\n        \"b40\": 0.7,\n        \"b41\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.5,\n        \"b7\": 0.55,\n        \"b17\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multimodal learning with severely missing modality, proposing a Bayesian meta-learning framework to achieve flexibility and efficiency in handling missing modalities during training and testing. The key contributions include a novel feature reconstruction network and uncertainty-guided feature regularization.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.95,\n    \"b18\": 0.9,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b40\": 0.8,\n    \"b19\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b7\": 0.7\n  }\n}\n```"], "60da8fc20abde95dc965f7f3": ["```json\n{\n  \"Summary\": \"The primary challenge discussed in the paper is the difficulty in labeling time-series data, which impacts the effectiveness of deep learning methods requiring large labeled datasets. The proposed algorithm, TS-TCC, addresses this by introducing a novel self-supervised learning framework that employs temporal and contextual contrasting to learn robust representations from time-series data without requiring extensive labeled datasets. Key innovations include the use of both weak and strong data augmentations, a temporal contrasting module for cross-view prediction, and a contextual contrasting module to enhance discriminative features.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b6\": 0.85,\n    \"b10\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b11\": 0.65,\n    \"b1\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of representation learning for time-series data using self-supervised learning methods. The paper proposes a novel framework called TS-TCC that employs temporal and contextual contrasting to learn robust and discriminative representations from time-series data. The proposed methods include specific data augmentation strategies and a combination of temporal and contextual contrasting modules.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b10\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of learning effective representations from time-series data, which generally lack easily recognizable patterns and require specialized annotation. The proposed Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC) is designed to tackle these challenges by employing data augmentations to create different views of the input data and using novel temporal and contextual contrasting modules to learn robust, discriminative representations.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1,\n        \"b7\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b5\": 0.8,\n        \"b6\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.7,\n        \"b11\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying deep learning methods to time-series data, which is harder to label than images. The proposed TS-TCC framework aims to learn robust and discriminative time-series representations using a novel combination of temporal and contextual contrasting. The framework includes innovative data augmentation techniques and a tough cross-view prediction task to enhance the robustness of the learned representations.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b7\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b6\": 0.9,\n    \"b10\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b1\": 0.65,\n    \"b5\": 0.85\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of labeling time-series data, which is harder compared to images due to the lack of human-recognizable patterns. It proposes a novel Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC). The framework includes simple data augmentations, a temporal contrasting module for robust representation learning, and a contextual contrasting module for discriminative representation learning.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.6,\n    \"b5\": 0.75,\n    \"b10\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b11\": 0.55\n  }\n}\n```"], "6020def79e795e62379b0d51": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of extending conventional unimodal Machine Comprehension (MC) systems, which only accept textual inputs, to a multimodal system that incorporates both audio and textual inputs. The primary challenges include effectively bridging the gap between textual and audio domains and enabling the multimodal MC model to work in unimodal scenarios. The proposed solution includes a Dynamic Inter-and Intra-modality Attention (DIIA) model for multimodality feature fusion and a Multimodal Knowledge Distillation (MKD) module to transfer knowledge from multimodal to unimodal scenarios.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b18\": 0.8,\n    \"b25\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of integrating audio and textual inputs for machine comprehension. The main contributions are the proposal of a novel problem of Audio-Oriented Multimodal Machine Comprehension, a Dynamic Inter-and Intra-modality Attention (DIIA) model, and a Multimodal Knowledge Distillation (MKD) module to handle unimodal scenarios.\",\n  \"Direct Inspiration\": [\"b13\", \"b28\"],\n  \"Indirect Inspiration\": [\"b18\", \"b5\", \"b17\", \"b25\", \"b38\"],\n  \"Other Inspiration\": [\"b19\", \"b23\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of Machine Comprehension (MC) with a novel focus on Audio-Oriented Multimodal Machine Comprehension. The primary challenges identified include the effective fusion of textual and audio modalities and enabling the multimodal model to work in unimodal scenarios. The proposed solutions include the Dynamic Inter-and Intra-modality Attention (DIIA) model and the Multimodal Knowledge Distillation (MKD) module.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b5\": 0.7,\n    \"b17\": 0.7,\n    \"b25\": 0.7,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b21\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a machine comprehension system that can handle both audio and textual inputs, termed Audio-Oriented Multimodal Machine Comprehension. The primary challenges include effectively bridging the gap between textual and audio domains for multimodal tasks and enabling the multimodal model to work in unimodal scenarios. The proposed solution involves a Dynamic Inter-and Intra-modality Attention (DIIA) model for feature fusion and a Multimodal Knowledge Distillation (MKD) module to transfer knowledge for unimodal tasks.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b28\": 1.0,\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b5\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of multimodal machine comprehension (MC) by integrating both audio and textual inputs, tackling the difficulties of modality disparity and enabling unimodal scenario application. It introduces the Dynamic Inter-and Intra-modality Attention (DIIA) model for effective multimodal feature fusion and a Multimodal Knowledge Distillation (MKD) module to adapt the model to unimodal inputs.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b38\": 0.8,\n    \"b28\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.6,\n    \"b5\": 0.6,\n    \"b17\": 0.6,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.4,\n    \"b12\": 0.4\n  }\n}\n```"], "610cb0785244ab9dcb1cd8f7": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve the complexity of multi-label recognition, particularly due to varying object locations and sizes, high computational costs of existing methods, and difficulties in optimization and interpretation of attention models. The paper proposes a simple class-specific residual attention (CSRA) module to address these challenges, achieving superior accuracy with negligible computational cost. The CSRA module builds upon the concept of global max pooling to enhance multi-label recognition performance.\",\n  \"Direct Inspiration\": {\n    \"b38\": 0.9,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in multi-label recognition by proposing a class-specific residual attention (CSRA) module. The CSRA module is simple, easy to train, and has negligible computational cost while achieving superior accuracy. It integrates a class-specific spatial pooling approach to enhance the performance of pretrained models in multi-label tasks.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b18\": 0.85,\n    \"b8\": 0.75,\n    \"b9\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.6,\n    \"b2\": 0.6,\n    \"b14\": 0.55,\n    \"b28\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of multi-label image classification, focusing on semantic relations, object proposals, and attention mechanisms. It proposes a novel Class-Specific Residual Attention (CSRA) module that is simple, efficient, and effective, achieving state-of-the-art performance on several datasets.\",\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [\"b14\", \"b3\", \"b6\", \"b8\", \"b9\", \"b18\", \"b19\", \"b28\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in multi-label recognition, particularly focusing on the difficulty of learning a single deep representation that fits objects with varying locations and sizes. The authors propose a class-specific residual attention (CSRA) module to improve multi-label recognition, which is simple, easy to train, and has negligible computational cost. The CSRA module incorporates spatial attention for each object class separately and achieves superior accuracy.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1.0,\n    \"b25\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b2\": 0.8,\n    \"b19\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multi-label recognition in computer vision, proposing a class-specific residual attention (CSRA) module to improve spatial attention for each object class separately. The CSRA module is simple, easy to train, and has negligible computational cost. The paper emphasizes the advantage of adding a global max-pooling layer to the usual global average pooling, which consistently improves multi-label recognition performance.\",\n  \"Direct Inspiration\": {\n    \"b38\": 0.9,\n    \"b25\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b2\": 0.7,\n    \"b29\": 0.6,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b18\": 0.5,\n    \"b8\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```"], "6020df0d9e795e62379b0d63": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of protein secondary structure prediction (PSSP) on low-quality PSSM by proposing a novel framework called PSSM-Distil. This framework enhances low-quality PSSM using knowledge distillation from a high-quality PSSM-based teacher network and employs contrastive learning to align the enhanced PSSM distribution with high-quality ones. The method leverages pre-trained BERT models to provide auxiliary information and demonstrates significant improvements in prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b8\": 1,\n    \"b22\": 1,\n    \"b38\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b16\": 0.8,\n    \"b18\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b7\": 0.6,\n    \"b11\": 0.6,\n    \"b23\": 0.6,\n    \"b25\": 0.6,\n    \"b28\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6,\n    \"b36\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of protein secondary structure prediction (PSSP) on low-quality Position-Specific Scoring Matrix (PSSM) data. It proposes a novel framework, PSSM-Distil, which enhances low-quality PSSM by aligning its distribution to high-quality PSSM using a teacher-student network, knowledge distillation, and contrastive learning.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b10\": 0.85,\n    \"b8\": 0.8,\n    \"b22\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b38\": 0.65,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.55,\n    \"b17\": 0.5,\n    \"b14\": 0.45,\n    \"b26\": 0.4,\n    \"b28\": 0.35\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of protein secondary structure prediction (PSSP) for sequences with low-quality Position-Specific Scoring Matrices (PSSM). The proposed framework, PSSM-Distil, enhances low-quality PSSM by aligning its distribution to high-quality PSSM using a teacher-student network, knowledge distillation, and contrastive learning. The method also integrates auxiliary information from a pre-trained BERT model to handle extremely low-quality PSSM cases, significantly improving prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b10\": 0.9,\n    \"b8\": 0.85,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.7,\n    \"b33\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b18\": 0.6,\n    \"b29\": 0.6,\n    \"b36\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of protein secondary structure prediction (PSSP) with low-quality Position-Specific Scoring Matrix (PSSM). The proposed model, PSSM-Distil, enhances low-quality PSSM by aligning its distribution to high-quality ones using a teacher-student network framework with knowledge distillation and contrastive learning.\",\n    \"Direct Inspiration\": {\n        \"b8\": 0.9,\n        \"b22\": 0.85,\n        \"b4\": 0.8,\n        \"b10\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b38\": 0.75,\n        \"b33\": 0.75,\n        \"b26\": 0.75,\n        \"b14\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.7,\n        \"b36\": 0.7,\n        \"b37\": 0.7,\n        \"b25\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of protein secondary structure prediction (PSSP) for low-quality PSSMs by proposing a novel framework called PSSM-Distil. This framework uses a teacher-student network for knowledge distillation and contrastive learning to enhance the low-quality PSSMs and align them with high-quality ones. The model also incorporates auxiliary information from a pre-trained BERT model to improve PSSP performance significantly.\",\n    \"Direct Inspiration\": {\n        \"b4\": 0.95,\n        \"b10\": 0.9,\n        \"b15\": 0.85,\n        \"b8\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b38\": 0.75,\n        \"b33\": 0.7,\n        \"b26\": 0.7,\n        \"b14\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.65,\n        \"b18\": 0.6,\n        \"b16\": 0.6,\n        \"b29\": 0.6\n    }\n}\n```"], "60b6e7b191e011903fc2b99a": ["```json\n{\n  \"Summary\": \"The paper identifies the limitation of the Graph Attention Network (GAT) in computing dynamic attention and proposes a modified version called GATv2, which addresses this limitation by altering the order of internal operations to achieve dynamic attention. The main contributions include theoretical analysis, formal definitions, and empirical validation across various benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b53\": 1.0,\n    \"b52\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b55\": 0.7,\n    \"b56\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b9\": 0.4,\n    \"b24\": 0.4,\n    \"b39\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge outlined in the paper is that the standard Graph Attention Networks (GATs) use a static form of attention, which limits their expressiveness. The proposed solution, GATv2, modifies the order of operations in the attention function to allow for dynamic attention, making it more expressive. The paper introduces formal definitions for analyzing the expressive power of graph attention mechanisms, theoretical proofs, and empirical evaluations demonstrating the superiority of GATv2 over GAT across multiple benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b53\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b52\": 0.8,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b55\": 0.5,\n    \"b32\": 0.4,\n    \"b56\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies the primary challenge of standard Graph Attention Networks (GAT) being unable to compute dynamic attention, which limits their expressiveness. To overcome this, the authors propose a new variant called GATv2 that modifies the internal operations of GAT to achieve dynamic attention. The paper provides theoretical and empirical evidence demonstrating that GATv2 outperforms GAT in various benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b53\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b52\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b17\": 0.6,\n    \"b39\": 0.6,\n    \"b46\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the limitation of Graph Attention Networks (GATs) in computing dynamic attention, which hinders their expressiveness. The authors propose a modification called GATv2 to overcome this limitation and demonstrate its superior performance through theoretical and empirical evaluations.\",\n    \"Direct Inspiration\": {\n        \"reference\": \"b53\",\n        \"confidence_score\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b3\", \"b52\"],\n        \"confidence_scores\": [0.8, 0.8]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b21\", \"b62\", \"b39\"],\n        \"confidence_scores\": [0.6, 0.6, 0.5]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitation of Graph Attention Networks (GATs) in computing dynamic attention and proposes a modified version called GATv2. The primary challenge is that GATs use a static form of attention, hindering their expressiveness. The authors introduce a simple fix to the GAT attention function, making GATv2 capable of dynamic attention and thus more expressive. The paper provides theoretical and empirical evidence showing the superiority of GATv2 over GAT in multiple benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b53\": 1.0,\n    \"b52\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b21\": 0.7,\n    \"b62\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b24\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```"], "602b9ff191e0113d72356c8d": ["```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing computation-aware neural architecture encoding methods in Neural Architecture Search (NAS), particularly their representation power and pre-training effectiveness. The authors propose a new method named CATE (Computation-Aware Transformer-based Encoding) which utilizes a Transformer-based model with a masked language modeling objective to improve the efficiency and generalization of architecture encodings.\",\n  \"Direct Inspiration\": {\n    \"b62\": 1,\n    \"b53\": 1,\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.8,\n    \"b46\": 0.8,\n    \"b29\": 0.7,\n    \"b52\": 0.7,\n    \"b60\": 0.7,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b55\": 0.6,\n    \"b61\": 0.6,\n    \"b41\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper focuses on the limitations of existing computation-aware neural architecture encodings in NAS and proposes a novel method called CATE (Computation-Aware Transformer-based Encoding). The primary challenges identified include the representation power and effectiveness of pre-training objectives in current encoders. The paper takes inspiration from BERT's MLM objective and addresses limitations by leveraging paired computationally similar architectures and a Transformer-based model with a causal mask and pairwise pre-training scheme.\",\n    \"Direct Inspiration\": {\n        \"b62\": 1.0,\n        \"b53\": 0.9,\n        \"b8\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b51\": 0.7,\n        \"b34\": 0.7,\n        \"b40\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b60\": 0.6,\n        \"b46\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing computation-aware neural architecture encoding methods in Neural Architecture Search (NAS), specifically focusing on representation power and the effectiveness of pre-training objectives. The proposed method, CATE (Computation-Aware Transformer-based Encoding), aims to improve these limitations by using a Transformer-based model with a masked language modeling objective to encode computationally similar architecture pairs.\",\n  \"Direct Inspiration\": {\n    \"b62\": 1,\n    \"b53\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b46\": 0.8,\n    \"b34\": 0.7,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b60\": 0.6,\n    \"b39\": 0.6,\n    \"b50\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of current architecture encoders in Neural Architecture Search (NAS), focusing on the representation power and effectiveness of pre-training objectives. It introduces a new computation-aware neural architecture encoding method named CATE (Computation-Aware Transformer-based Encoding) that aims to alleviate these limitations by using paired computationally similar architectures and a Transformer-based model with a masked language modeling (MLM) objective.\",\n  \"Direct Inspiration\": {\n    \"b62\": 1.0,\n    \"b51\": 1.0,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b60\": 0.8,\n    \"b39\": 0.7,\n    \"b53\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing computation-aware architecture encoding methods for Neural Architecture Search (NAS). It proposes a new method called CATE (Computation-Aware Transformer-based Encoding), which uses a Transformer model to encode paired computationally similar architectures. CATE aims to improve on the representation power and pre-training objectives of previous methods by employing a novel pairwise pre-training scheme and the MLM objective. The main contributions are evaluated against existing methods, demonstrating superior performance in various NAS subroutines and generalization beyond the training search space.\",\n  \"Direct Inspiration\": {\n    \"b62\": 1.0,\n    \"b53\": 0.9,\n    \"b51\": 0.9,\n    \"b34\": 0.8,\n    \"b29\": 0.8,\n    \"b52\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.7,\n    \"b60\": 0.7,\n    \"b8\": 0.6,\n    \"b19\": 0.5,\n    \"b40\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.4,\n    \"b50\": 0.4,\n    \"b39\": 0.4\n  }\n}\n```"], "61a5f4fe6750f84218691f6b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of high computational cost and model interference in Neural Architecture Search (NAS). It proposes a novel zero-shot proxy, Zen-Score, which measures the expressivity of deep neural networks and correlates with model accuracy. Zen-Score is computationally efficient, lightweight, and data-free. Based on Zen-Score, the paper introduces Zen-NAS, a NAS algorithm that maximizes expressivity under inference budgets, achieving state-of-the-art performance on various datasets.\",\n  \"Direct Inspiration\": [\"b33\", \"b30\", \"b46\", \"b13\", \"b59\"],\n  \"Indirect Inspiration\": [\"b10\", \"b22\", \"b38\", \"b8\", \"b27\", \"b43\"],\n  \"Other Inspiration\": [\"b3\", \"b34\", \"b45\", \"b62\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing high-performance deep neural networks with reduced computational cost in Neural Architecture Search (NAS). The authors propose Zen-Score, a zero-cost proxy for measuring network expressivity, and subsequently introduce Zen-NAS, a novel zero-shot NAS algorithm.\",\n  \"Direct Inspiration\": {\n    \"b38\": 0.9,\n    \"b30\": 0.9,\n    \"b33\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.8,\n    \"b13\": 0.8,\n    \"b59\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b43\": 0.7,\n    \"b8\": 0.65,\n    \"b27\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges include the high computational cost of accuracy predictors in NAS algorithms, the inefficiency of one-shot methods due to supernet training, and the problem of model interference in supernet-based methods.\",\n    \"algorithm\": \"The proposed algorithm, Zen-NAS, uses a novel zero-cost proxy called Zen-Score for efficient NAS. The Zen-Score measures the expressivity of a network and correlates with model accuracy, enabling fast and lightweight computation.\"\n  },\n  \"Direct Inspiration\": {\n    \"b38\": 0.9,\n    \"b30\": 0.9,\n    \"b3\": 0.8,\n    \"b34\": 0.8,\n    \"b33\": 0.7,\n    \"b46\": 0.7,\n    \"b13\": 0.7,\n    \"b59\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.6,\n    \"b5\": 0.6,\n    \"b65\": 0.6,\n    \"b53\": 0.6,\n    \"b4\": 0.6,\n    \"b62\": 0.6,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b22\": 0.5,\n    \"b43\": 0.5,\n    \"b8\": 0.5,\n    \"b27\": 0.5,\n    \"b18\": 0.5,\n    \"b35\": 0.5,\n    \"b51\": 0.5,\n    \"b36\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing high-performance deep neural networks with reduced computational cost. It proposes a novel zero-shot proxy called Zen-Score for Neural Architecture Search (NAS) that measures the expressivity of a neural network without the need for extensive training. The Zen-Score is designed to be computationally efficient, lightweight, and data-free. The paper introduces the Zen-NAS algorithm, which maximizes the Zen-Score of target networks within inference budgets, achieving state-of-the-art performance on multiple datasets.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1,\n    \"b30\": 1,\n    \"b46\": 1,\n    \"b13\": 1,\n    \"b59\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b62\": 0.8,\n    \"b45\": 0.7,\n    \"b3\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.6,\n    \"b29\": 0.6,\n    \"b25\": 0.6,\n    \"b60\": 0.6,\n    \"b61\": 0.6,\n    \"b5\": 0.6,\n    \"b65\": 0.6,\n    \"b53\": 0.6,\n    \"b58\": 0.6,\n    \"b68\": 0.6,\n    \"b38\": 0.6,\n    \"b10\": 0.6,\n    \"b22\": 0.6,\n    \"b8\": 0.6,\n    \"b27\": 0.6,\n    \"b43\": 0.6,\n    \"b46\": 0.6,\n    \"b35\": 0.6,\n    \"b51\": 0.6,\n    \"b36\": 0.6,\n    \"b0\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the high computational costs associated with existing Neural Architecture Search (NAS) methods, particularly the one-shot methods that still require training a huge supernet. The authors propose a novel zero-cost proxy called Zen-Score to measure the expressivity of deep neural networks efficiently, leading to the development of the Zen-NAS algorithm.\",\n  \"Direct Inspiration\": {\n    \"b38\": 0.9,\n    \"b30\": 0.9,\n    \"b33\": 0.8,\n    \"b46\": 0.8,\n    \"b59\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b8\": 0.7,\n    \"b22\": 0.7,\n    \"b13\": 0.7,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b6\": 0.6,\n    \"b51\": 0.6,\n    \"b35\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```"], "603cb22b91e011aeee1505fc": ["```json\n{\n  \"Summary\": \"The paper addresses the problem of learning node representations that are both fair and stable in Graph Neural Networks (GNNs). It identifies a key connection between counterfactual fairness and stability, proposing a novel framework, NIFTY, which enforces these properties both in the objective function and the GNN architecture. The framework introduces a novel objective function and a method for improving neural message passing. Theoretical analysis and experiments demonstrate the effectiveness of NIFTY in achieving fair and stable representations without sacrificing predictive performance.\",\n  \"Direct Inspiration\": [\"b7\", \"b38\", \"b4\", \"b5\", \"b16\"],\n  \"Indirect Inspiration\": [\"b18\", \"b30\", \"b33\"],\n  \"Other Inspiration\": [\"b3\", \"b15\", \"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the simultaneous achievement of fairness and stability in Graph Neural Networks (GNNs). The authors propose a novel framework, NIFTY, which unifies these two aspects by leveraging counterfactual fairness and stability via Lipschitz continuity. The framework introduces a triplet-based objective function and layer-wise weight normalization using Lipschitz constants to enforce these properties in both the objective function and the GNN architecture.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b38\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b13\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are ensuring fairness and stability in Graph Neural Networks (GNNs). The authors propose a novel framework, NIFTY (uNIfying Fairness and stabiliTY), to achieve these objectives by connecting counterfactual fairness and stability. The framework includes an objective function that optimizes for both fairness and stability and introduces a method for improving neural message passing through layer-wise weight normalization using the Lipschitz constant.\",\n  \"Direct Inspiration\": [\"b7\", \"b38\"],\n  \"Indirect Inspiration\": [\"b18\", \"b30\", \"b33\"],\n  \"Other Inspiration\": [\"b4\", \"b5\", \"b16\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning node representations in Graph Neural Networks (GNNs) that are both fair and stable. It identifies a key connection between counterfactual fairness and stability and proposes a novel framework, NIFTY, which enforces these properties in both the objective function and the GNN architecture. The framework introduces a triplet-based objective function and a method for layer-wise weight normalization using the Lipschitz constant.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b38\": 0.9,\n    \"b4\": 0.95,\n    \"b5\": 0.95,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.85,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.75,\n    \"b33\": 0.75\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of ensuring that Graph Neural Networks (GNNs) produce node representations that are both fair and stable. The novel method proposed, NIFTY, leverages the connection between counterfactual fairness and stability to enforce these properties in both the objective function and the GNN architecture. The framework uses a triplet-based objective and layer-wise weight normalization using the Lipschitz constant to achieve this.\",\n    \"Direct Inspiration\": [\n        \"b7\",\n        \"b38\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b4\",\n        \"b5\",\n        \"b16\"\n    ],\n    \"Other Inspiration\": [\n        \"b3\",\n        \"b10\",\n        \"b20\"\n    ]\n}\n```"], "6173f1c391e0118698c04c59": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving emotion recognition by leveraging multimodal data, specifically focusing on eye movements and EEG signals. It proposes a novel method of generating high-dimensional multimodal features using a conditional generative adversarial network (CGAN) based on single modality input, primarily eye movement features, to circumvent the practical limitations of EEG signal acquisition.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b17\": 0.8,\n    \"b16\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.75,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.65,\n    \"b23\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving emotion recognition by using single-modal eye movement features to generate synthetic multimodal high-dimensional features, thereby reducing dependency on complex and impractical EEG data collection. The proposed method utilizes bimodal deep autoencoders and conditional generative adversarial networks (CGANs) to achieve this goal.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b17\": 0.9,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b23\": 0.4,\n    \"b15\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effective emotion recognition using multimodal data, specifically EEG and eye movement signals. It proposes a novel algorithm to generate synthetic multimodal features from single-modal eye movement data using conditional generative adversarial networks (CGANs), thereby reducing the dependency on EEG data during testing while preserving the benefits of multimodal fusion during training.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b17\": 1,\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b21\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b19\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of emotion recognition by exploring multimodal feature generation using a single modality to circumvent the limitations of EEG data collection. The authors propose a method that combines eye movement features with conditional generative adversarial networks (CGANs) to generate high-dimensional multimodal features for emotion recognition.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.9,\n    \"b17\": 0.9,\n    \"b19\": 0.8,\n    \"b23\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b21\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of emotion recognition by leveraging multimodal fusion, specifically combining EEG signals and eye movements, to improve accuracy while reducing practical limitations. The authors propose a novel method using conditional generative adversarial networks (CGANs) to generate multimodal features from single eye movement features for emotion recognition.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b17\": 0.9,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.7,\n    \"b25\": 0.65,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.5,\n    \"b21\": 0.4\n  }\n}\n```"], "60f2b0bb5244ab9dcbbb0bd1": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of reducing load-to-use latency in processors while avoiding the overheads of traditional pipeline prefetching and intermediate storage solutions. The authors propose a method to detect load-to-load reuse early in the pipeline using address predictions, thus reducing L1 accesses and register file pressure while maintaining performance benefits.\",\n    \"Direct Inspiration\": [\"b38\", \"b26\"],\n    \"Indirect Inspiration\": [\"b14\", \"b31\", \"b36\"],\n    \"Other Inspiration\": [\"b11\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing load latency by combining pipeline prefetching with register reuse, load coalescing, and efficient validation mechanisms. It proposes a novel solution that leverages early address predictions to detect and reuse load-to-load data through the physical register file (PRF), thus reducing L1 cache accesses and improving performance. The proposed method, AT-RT, introduces an address-to-register mapping table, enabling efficient temporal and spatial reuse of prefetched data without additional storage overhead.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1,\n    \"b31\": 0.9,\n    \"b14\": 0.85,\n    \"b26\": 0.8,\n    \"b36\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b11\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is reducing load latency and improving performance in processors without increasing pressure on the L1 cache or requiring extra storage. The proposed solution, AT-RT, leverages early address prediction to detect load-to-load reuse, thereby reducing unnecessary memory accesses and reusing data in the physical register file (PRF). This approach avoids the overheads associated with pipeline prefetching and extra storage, while maintaining performance benefits.\",\n    \"Direct Inspiration\": {\n        \"b38\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.9,\n        \"b26\": 0.9,\n        \"b31\": 0.8,\n        \"b36\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.7,\n        \"b27\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing load latency and L1 cache pressure by proposing an innovative solution that combines early address prediction, register reuse, load coalescing, and validation mechanisms. The proposed AT-RT (Address Tag-Register Tag) technique enhances temporal and spatial reuse of prefetched data while minimizing additional storage and data movement overheads.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b14\", \"b26\", \"b31\", \"b38\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b11\", \"b36\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b1\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing load latency in CPUs by combining pipeline prefetching with register reuse. It proposes using address predictions early in the pipeline to detect load-to-load reuse, reducing the need for L1 cache accesses and extra data storage. The solution, called AT-RT, leverages the physical register file for data storage and validation, improving both temporal and spatial reuse while minimizing memory accesses and energy overhead.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b31\": 0.8,\n    \"b38\": 0.9,\n    \"b36\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.6,\n    \"b4\": 0.6,\n    \"b11\": 0.5,\n    \"b27\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.4,\n    \"b5\": 0.4,\n    \"b13\": 0.4,\n    \"b17\": 0.4\n  }\n}\n```"], "60cb2a281bc21f07d0810fbf": ["```json\n{\n  \"Summary\": \"The primary challenges addressed in this paper are efficient search and supernet optimization over a large search space, and reducing the extra complexity brought by model ensemble. The paper proposes a one-shot neural ensemble architecture search (NEAS) approach to search for lightweight ensemble models. Key contributions include a novel metric called diversity score to drop inferior candidates during supernet training and a layer sharing mechanism to reduce model complexity.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b11\": 0.8,\n    \"b36\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b3\": 0.6,\n    \"b22\": 0.6,\n    \"b47\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are efficient search and supernet optimization over a large search space, and reducing the extra complexity brought by model ensemble. The proposed algorithm, NEAS, includes a diversity score to progressively drop inferior candidates during supernet training and a layer-sharing strategy to reduce complexity. The paper emphasizes the effectiveness of the proposed methods in improving the ranking ability of the trained supernet and achieving state-of-the-art performance on tasks such as ImageNet and COCO object detection.\",\n  \"Direct Inspiration\": [\"b16\", \"b8\"],\n  \"Indirect Inspiration\": [\"b9\", \"b36\", \"b11\"],\n  \"Other Inspiration\": [\"b1\", \"b33\", \"b39\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in combining Neural Architecture Search (NAS) with ensemble models: (1) efficient search and optimization in a large search space, and (2) reducing the extra complexity brought by model ensemble. The proposed NEAS (Neural Ensemble Architecture Search) approach introduces a diversity score metric to drop inferior candidates during training and a layer-sharing mechanism to reduce complexity. The method aims to find lightweight ensemble models with improved performance and generalization ability.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b47\": 0.9,\n    \"b33\": 0.8,\n    \"b39\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing neural network architectures and proposes a one-shot neural ensemble architecture search (NEAS) to create robust, accurate, and efficient ensemble models. The primary challenges include the efficient search and supernet optimization over a large search space and reducing the extra complexity brought by model ensemble. To tackle these, the paper introduces a novel metric called diversity score and a layer-sharing mechanism.\",\n  \"Direct Inspiration\": {\n    \"inspired by the effectiveness of ensemble\": [\"b33\", \"b39\", \"b14\", \"b8\"]\n  },\n  \"Indirect Inspiration\": {\n    \"Neural Architecture Search (NAS)\": [\"b47\"],\n    \"Object detection\": [\"b11\"],\n    \"Image classification\": [\"b7\"],\n    \"Implicit ensemble methods\": [\"b33\", \"b39\", \"b14\", \"b8\"]\n  },\n  \"Other Inspiration\": {\n    \"Diversity-guided search space shrinking\": [\"b16\"],\n    \"Layer sharing mechanism\": [\"b15\", \"b25\", \"b29\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing robust and efficient ensemble models through Neural Architecture Search (NAS). It introduces a novel NEAS approach that combines diversity-guided search space shrinking and layer sharing mechanisms to optimize model performance under resource constraints.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b16\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b9\", \"b11\", \"b36\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b47\", \"b33\", \"b39\", \"b8\"]\n  }\n}\n```"], "608be74791e0112fc4e65c66": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of object tracking, specifically the complexity and efficiency of tracking models, which hinder their deployment in real-world applications. The proposed solution, LightTrack, automates the design of lightweight models using neural architecture search (NAS), focusing on achieving a balance between tracking performance and computational cost. The paper introduces a novel one-shot NAS algorithm tailored for object tracking and a lightweight search space comprising depthwise separable convolutions and inverted residual structures.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b6\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b53\": 0.75,\n    \"b30\": 0.7,\n    \"b42\": 0.65,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.55,\n    \"b3\": 0.5,\n    \"b1\": 0.48,\n    \"b38\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the complexity and efficiency of object tracking models. The proposed algorithm, LightTrack, addresses these challenges by automating the design of lightweight models using neural architecture search (NAS). The paper aims to balance tracking performance and computational costs, enabling deployment on resource-limited hardware platforms.\",\n  \"Direct Inspiration\": [\"b38\", \"b1\", \"b17\", \"b42\", \"b20\"],\n  \"Indirect Inspiration\": [\"b8\", \"b27\", \"b53\", \"b6\"],\n  \"Other Inspiration\": [\"b2\", \"b28\", \"b49\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing efficient and lightweight object tracking models, leveraging a novel approach using neural architecture search (NAS). The proposed LightTrack system aims to balance performance and computational cost by automating the design of lightweight tracking architectures, overcoming limitations of existing methods like model compression and handcrafted model design.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b27\": 0.9,\n    \"b53\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b38\": 0.8,\n    \"b42\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b20\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing lightweight and efficient object tracking models for deployment on resource-constrained hardware. It proposes a novel neural architecture search (NAS) algorithm called LightTrack, which uses a one-shot NAS approach to automatically search for compact architectures. The paper highlights the need for efficient model design due to the limitations of existing heavy models like SiamRPN++ and Ocean, and introduces a search space with lightweight building blocks to balance performance and computational costs.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b17\": 0.8,\n    \"b27\": 0.8,\n    \"b53\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b8\": 0.7,\n    \"b20\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of heavy and expensive object tracking models in computer vision, proposing a novel method called LightTrack. LightTrack uses neural architecture search (NAS) to automate the design of lightweight models, achieving efficient tracking on resource-limited hardware. Key contributions include a new one-shot NAS formulation for object tracking, a lightweight search space, and a dedicated search pipeline.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b6\", \"b17\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b1\", \"b38\", \"b8\", \"b42\", \"b20\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b27\", \"b53\"]\n  }\n}\n```"], "600833119e795ed227f53161": ["```json\n{\n  \"Summary\": \"The paper addresses the high computational cost of Neural Architecture Search (NAS) by proposing zero-cost proxies for evaluating neural network architectures. These proxies use metrics derived from pruning-at-initialization methods to score entire networks using a single minibatch of data. The goal is to significantly reduce the computation time required for NAS while maintaining or improving the accuracy of network rankings.\",\n  \"Direct Inspiration\": [\"b17\", \"b32\", \"b29\"],\n  \"Indirect Inspiration\": [\"b36\", \"b20\"],\n  \"Other Inspiration\": [\"b27\", \"b18\", \"b0\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the high computational cost of evaluating neural network architectures in Neural Architecture Search (NAS). The authors propose using zero-cost proxies inspired by pruning-at-initialization techniques to score and rank neural networks, aiming to reduce the computation time required for NAS. Key contributions include adapting pruning metrics for entire networks, comparing zero-cost proxies to conventional NAS proxies, performing ablations on NAS benchmarks, and integrating zero-cost metrics with various NAS algorithms to achieve significant speedups.\",\n  \"Direct Inspiration\": [\"b17\", \"b32\", \"b29\", \"b36\", \"b20\"],\n  \"Indirect Inspiration\": [\"b19\", \"b31\"],\n  \"Other Inspiration\": [\"b2\", \"b4\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The paper addresses the high computational cost of neural architecture search (NAS) by proposing zero-cost proxies that require minimal data and computational resources. These proxies aim to rank neural networks effectively without full training, thereby reducing the time and resources needed for NAS.\",\n    \"Inspirations\": \"The paper is inspired by recent pruning-at-initialization work, which involves computing per-parameter saliency metrics before training to inform parameter pruning. The authors adapt these metrics to score entire neural networks for NAS.\"\n  },\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b32\": 0.9,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b1\": 0.7,\n    \"b19\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on addressing the high computational cost of neural architecture search (NAS) by proposing zero-cost proxies that require minimal data and computational resources for evaluating neural network architectures. The primary challenges include reducing the time and resources needed for NAS evaluation phases and integrating these zero-cost metrics within existing NAS algorithms effectively. Key inspirations come from recent advancements in pruning-at-initialization techniques that use saliency metrics to inform parameter pruning.\",\n  \"Direct Inspiration\": [\"b17\", \"b32\", \"b29\"],\n  \"Indirect Inspiration\": [\"b36\", \"b20\"],\n  \"Other Inspiration\": [\"b24\", \"b1\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is the high computational cost of evaluating candidate architectures in neural architecture search (NAS). The proposed solution is to use zero-cost proxies inspired by recent pruning-at-initialization work to score and rank neural networks using a single minibatch of data. The paper introduces several novel methods such as adapting pruning-at-initialization metrics for NAS, detailed comparison with conventional proxies, ablations on various NAS benchmarks, and integration with existing NAS algorithms.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.95,\n    \"b32\": 0.90,\n    \"b29\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.80,\n    \"b20\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.60,\n    \"b1\": 0.55\n  }\n}\n```"], "6085415691e01180c31e92d5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot learning in NLP by introducing Pattern-Exploiting Training (PET) and its iterative variant (iPET). These methods use natural language patterns to reformulate input examples into cloze-style phrases, allowing for effective semi-supervised learning. The approach leverages task descriptions and combines standard supervised learning with pre-trained language models (PLMs) to improve performance on small datasets.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b26\": 0.9,\n    \"b29\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.6,\n    \"b18\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b50\": 0.4,\n    \"b43\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot learning in NLP tasks by proposing Pattern-Exploiting Training (PET) and its iterative variant iPET. These methods use natural language patterns to reformulate input examples into cloze-style phrases, enabling pretrained language models to effectively handle few-shot learning scenarios.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0,\n    \"b26\": 0.9,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b27\": 0.75,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot learning in NLP by introducing Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples into cloze-style phrases using natural language patterns to aid in task understanding. The approach is enhanced with an iterative variant called iPET, which trains multiple generations of models on increasingly larger datasets.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b26\": 0.8,\n    \"b2\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.6,\n    \"b20\": 0.6,\n    \"b14\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b50\": 0.4,\n    \"b43\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot learning in NLP tasks, where limited labeled data hampers model performance. It introduces Pattern-Exploiting Training (PET), a semi-supervised training procedure that leverages natural language patterns to reformulate input examples into cloze-style phrases. PET is combined with a pre-trained language model (PLM) and iteratively finetuned to improve performance on small datasets. The iterative variant, iPET, further enhances the training process by progressively increasing training set sizes.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b28\": 0.9,\n    \"b14\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b12\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b20\": 0.6,\n    \"b5\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot learning in NLP tasks using a small number of labeled examples. It introduces Pattern-Exploiting Training (PET) and its iterative variant iPET, which use natural language patterns to reformulate input examples into cloze-style phrases, leveraging pretrained language models for better performance.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0,\n    \"b26\": 0.9,\n    \"b29\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.6,\n    \"b5\": 0.5,\n    \"b12\": 0.4\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.3,\n    \"b20\": 0.3\n  }\n}\n```"], "61bff4265244ab9dcb79c422": ["```json\n{\n  \"Summary\": \"The paper presents a novel framework for learning to simulate complex dynamics via constraint satisfaction, termed Constraint-based Graph Network Simulator (C-GNS). The key challenges addressed include modeling complex physical systems with strong interactions and achieving accurate simulations without knowledge of the true underlying constraints. The proposed C-GNS leverages a learned constraint function implemented as a Graph Neural Network and uses a gradient-based solver for predicting future states.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.8,\n    \"b32\": 0.8,\n    \"b45\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7,\n    \"b21\": 0.7,\n    \"b13\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of simulating complex dynamics by proposing a novel framework called 'Constraint-based Graph Network Simulator' (C-GNS). This framework defines a learned constraint function implemented as a Graph Neural Network (GNN) that indicates whether a future state is consistent with current and previous states. The method uses a gradient-based solver to iteratively refine a proposed state to minimize the learned constraint, and is trained end-to-end using observed trajectory data.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b7\": 1,\n    \"b45\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b30\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents a framework called Constraint-based Graph Network Simulator (C-GNS), which employs a learned constraint function to simulate complex dynamics via constraint satisfaction. The primary challenge addressed is the accurate simulation of physical systems with strong interactions, which are difficult to model using explicit forward models. The C-GNS framework leverages a Graph Neural Network to implement the constraint function and uses a gradient-based solver to iteratively refine state predictions, enabling the simulator to achieve higher accuracy in various physical domains such as ropes, bouncing balls, and splashing fluids.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b36\": 0.7,\n    \"b45\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b24\": 0.6,\n    \"b44\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the development of the Constraint-based Graph Network Simulator (C-GNS), a machine learning framework that simulates complex dynamics via constraint satisfaction rather than explicit forward models. The primary challenges addressed include capturing the time dynamics of physical systems and improving predictive accuracy over state-of-the-art methods. The proposed method utilizes a Graph Neural Network (GNN) to learn a constraint function and employs a gradient-based solver for iterative refinement.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.9,\n    \"b32\": 0.85,\n    \"b7\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.75,\n    \"b21\": 0.7,\n    \"b13\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of simulating complex physical dynamics by proposing a novel framework called 'Constraint-based Graph Network Simulator' (C-GNS). Unlike traditional explicit forward models, C-GNS uses a learned constraint function implemented as a Graph Neural Network (GNN) to predict future states. The model is trained end-to-end on observed trajectory data and does not require knowledge of the true underlying constraints. This approach allows for more accurate and flexible simulations across various domains such as ropes, bouncing balls, and splashing fluids.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b7\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.9,\n    \"b32\": 0.8,\n    \"b45\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b30\": 0.65,\n    \"b8\": 0.7,\n    \"b25\": 0.55\n  }\n}\n```"], "6153e0215244ab9dcb39a8be": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of collaborative filtering (CF) in recommendation systems, emphasizing the often-overlooked roles of loss functions and negative sampling strategies. It proposes a novel cosine contrastive loss (CCL) to optimize CF models and introduces SimpleX, a simple yet effective CF model that integrates matrix factorization and user behavior modeling.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b2\": 0.8,\n    \"b9\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b11\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in collaborative filtering (CF) models, particularly focusing on the importance of loss functions and negative sampling strategies. It proposes a new loss function called cosine contrastive loss (CCL) and introduces a simple yet effective CF model named SimpleX.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b7\", \"b37\"],\n    \"confidence\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b4\", \"b2\", \"b9\", \"b21\"],\n    \"confidence\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b10\", \"b38\", \"b29\"],\n    \"confidence\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in collaborative filtering (CF) models, focusing on the importance of loss functions and negative sampling. It proposes a new loss function, Cosine Contrastive Loss (CCL), and a simple yet effective CF model called SimpleX. The model integrates matrix factorization and user behavior modeling, and it is evaluated on 11 benchmark datasets, showing superior performance compared to state-of-the-art models.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b2\": 0.8,\n    \"b9\": 0.8,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5,\n    \"b38\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Collaborative Filtering (CF) models for personalized recommendations by focusing on loss functions and negative sampling strategies rather than complex interaction encoders. The authors propose a novel Cosine Contrastive Loss (CCL) inspired by contrastive loss used in computer vision, and introduce a simple yet effective CF model named SimpleX.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b37\": 0.9,\n    \"b4\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b9\": 0.7,\n    \"b21\": 0.7,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b38\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.5,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in collaborative filtering (CF) for recommendation systems, especially focusing on the choice of loss functions and negative sampling strategies. It proposes a novel cosine contrastive loss (CCL) tailored for CF, demonstrating its superiority through comprehensive experiments. The SimpleX model, designed with simplicity in mind, integrates matrix factorization and user behavior modeling, showing significant improvements over state-of-the-art models in both effectiveness and efficiency.\",\n  \"Direct Inspiration\": [\"b7\", \"b37\"],\n  \"Indirect Inspiration\": [\"b4\", \"b2\", \"b38\", \"b21\", \"b9\"],\n  \"Other Inspiration\": [\"b10\", \"b11\", \"b29\"]\n}\n```"], "6037670cd3485cfff1db3c4d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of temporal action localization in untrimmed videos, which involves identifying the occurrence of specific actions and determining their temporal boundaries. The primary difficulties include cluttered backgrounds, variations in appearance and motion, and differing action durations. The proposed method, STAN (Supervised Temporal Attention Network), learns segment weights to exploit informative video segments, capturing both appearance and motion features, and utilizes a localization module for classifying actions and determining boundaries.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b24\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b27\": 0.65,\n    \"b28\": 0.6,\n    \"b33\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include addressing cluttered backgrounds, large variances in appearance and motion, low resolution, and variable action durations in temporal action localization within untrimmed videos. The proposed algorithm, STAN, uses a supervised attention mechanism to learn segment weights, capturing temporal relationships via LSTM, and computing weights for static appearance and dynamic motion features to effectively classify and localize actions.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b13\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b27\": 0.8,\n    \"b28\": 0.8,\n    \"b31\": 0.8,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b36\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of temporal action localization in untrimmed videos, focusing on cluttered backgrounds, appearance and motion variances, and varying action durations. The proposed algorithm, STAN (Supervised Temporal Attention Network), exploits informative video segments by learning segment weights using a supervised attention mechanism. STAN includes a segment-level attention module, a feature-level attention module, and a localization module to dynamically learn segment weights, aggregate features, and classify action proposals, respectively.\",\n  \"Direct Inspiration\": [\"b1\", \"b13\", \"b24\"],\n  \"Indirect Inspiration\": [\"b12\", \"b36\", \"b37\"],\n  \"Other Inspiration\": [\"b9\", \"b27\", \"b28\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of temporal action localization in untrimmed videos, focusing on cluttered backgrounds, large variances of appearance and motion, and low resolution. It proposes a novel method called the Supervised Temporal Attention Network (STAN) which includes a segment-level attention module, a feature-level attention module, and a localization module to exploit informative video segments by learning video segment weights.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b13\": 0.85,\n    \"b24\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b31\": 0.75,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of temporal action localization in untrimmed videos, such as cluttered backgrounds, varied appearance and motion, and variable action durations. The proposed method, STAN (Supervised Temporal Attention Network), exploits informative video segments by learning segment weights to improve action recognition and boundary prediction. STAN comprises a segment-level attention module, a feature-level attention module, and a localization module, which together enhance the representation of video segments and improve temporal action localization accuracy.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b6\": 0.9,\n    \"b9\": 0.8,\n    \"b13\": 0.85,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b27\": 0.75,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.7,\n    \"b33\": 0.65\n  }\n}\n```"], "60376650d3485cfff1da05d6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of video recognition by breaking down the fusion of object, scene, and action features into two pairwise feature relation modeling processes. It introduces a Semantics Attention Module (SAM) to refine the 'weak' feature with the guidance of the 'strong' feature using attention mechanisms. The proposed approach aims to improve recognition accuracy while reducing computational effort.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7,\n    \"b24\": 0.7,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in this paper is the difficulty of correlation learning among high-dimensional features for video recognition, particularly when combining scene, object, and action features. The authors propose a Semantics Attention Network (SAN) with two Semantics Attention Modules (SAMs) to improve the fusion process by modeling pairwise feature relationships explicitly using attention mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.9,\n    \"b13\": 0.85,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.75,\n    \"b55\": 0.7,\n    \"b56\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the difficulty of correlation learning in high-dimensional features for video recognition. The paper proposes a Semantics Attention Module (SAM) that refines 'weak' features using 'strong' features through attention mechanisms and combines them using a residual design. The novel approach involves breaking down the fusion of object, scene, and action features into a two-step process, which improves video recognition accuracy while reducing computational effort.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b11\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving video recognition by effectively combining object, scene, and action features. The key contribution of the paper is the introduction of a Semantics Attention Module (SAM) that refines weak features using strong features through attention mechanisms. This approach is implemented in a Semantics Attention Network (SAN) and demonstrates improved accuracy and computational efficiency over alternative methods.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b11\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b18\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b19\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents a novel Semantics Attention Network (SAN) employing Semantics Attention Modules (SAMs) to improve video recognition. The primary challenge addressed is the effective fusion of object, scene, and action features for better video classification. The proposed approach models pairwise feature relationships to refine 'weak' features using 'strong' features through attention mechanisms, overcoming the limitations of direct feature concatenation.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b11\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b24\": 0.8,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```"], "604f1ca49e795e5feaac54da": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of action recognition in videos by proposing a Temporal Cross-Layer Correlation (TCLC) network that uncovers local and global structures from video data. The paper introduces a cross-layer attention method and a center-guided attention method to leverage features from multi-level granularity in a unified way.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b11\": 0.9,\n    \"b12\": 0.9,\n    \"b13\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b27\": 0.6,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b22\": 0.5,\n    \"b24\": 0.5,\n    \"b20\": 0.5,\n    \"b5\": 0.5,\n    \"b35\": 0.4,\n    \"b36\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in action recognition: (1) typical video pooling methods fail to account for sequence order, and (2) multi-scale features from convolutional layers are not well exploited. To tackle these challenges, the authors propose a Temporal Cross-Layer Correlation network and introduce cross-layer and center-guided attention methods. They also highlight the use of an unsupervised context reconstruction block to enhance context exploration and feature learning.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b11\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b14\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in action recognition from videos: (1) capturing both static and motion information in videos while considering the temporal order, and (2) effectively leveraging multi-scale features from convolutional layers. The authors propose a Temporal Cross-Layer Correlation (TCLC) network to address these challenges by using context-aware reconstruction and cross-layer attention mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.7,\n    \"b13\": 0.6,\n    \"b14\": 0.5,\n    \"b16\": 0.4,\n    \"b17\": 0.4,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.4,\n    \"b20\": 0.3,\n    \"b35\": 0.3,\n    \"b36\": 0.3,\n    \"b37\": 0.3\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are: 1) Developing discriminative features that capture both static and motion information from videos while considering the temporal order and context of frames. 2) Effectively leveraging multi-scale features from convolutional layers for better classification performance. The proposed algorithm addresses these challenges by introducing a Temporal Cross-Layer Correlation network that includes an RNN-based context reconstruction module and an attention module to aggregate features from different layers in a unified way.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b18\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b11\", \"b12\", \"b14\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b13\", \"b16\", \"b17\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include effectively capturing both static and motion information from videos and leveraging multi-scale features from convolutional layers for action recognition. The algorithm proposed by the authors involves a Temporal Cross-Layer Correlation network that incorporates context-aware reconstruction and cross-layer attention mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b11\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b27\": 0.7,\n    \"b31\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b5\": 0.6,\n    \"b13\": 0.5,\n    \"b15\": 0.5\n  }\n}\n```"], "605aa39ae4510cd7c86d6a3e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of dynamic emotion recognition across multiple modalities by proposing a compact, efficient, and modality-agnostic graph-based approach. The novel contributions include the Learnable Graph Inception Network (L-GrIN) which integrates a new type of graph convolution layer, a graph inception module, learnable graph structure, and a graph-to-vector pooling function. This approach achieves state-of-the-art performance on several benchmarks with fewer trainable parameters.\",\n  \"Direct Inspiration\": [\"b5\", \"b10\", \"b12\"],\n  \"Indirect Inspiration\": [\"b3\", \"b9\", \"b11\"],\n  \"Other Inspiration\": [\"b14\", \"b42\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient and scalable emotion recognition across multiple modalities in resource-constrained devices. It proposes a novel graph convolution network (GCN) architecture, Learnable Graph Inception Network (L-GrIN), which includes a new graph convolution layer, a graph inception module, and learnable graph structure and pooling functions. The proposed model aims to achieve state-of-the-art accuracy with fewer trainable parameters by adopting a graph-based approach.\",\n  \"Direct Inspiration\": [\"b5\", \"b10\", \"b11\", \"b12\"],\n  \"Indirect Inspiration\": [\"b14\", \"b42\"],\n  \"Other Inspiration\": [\"b3\", \"b9\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of emotion recognition across various modalities (video, audio, motion sensors) using a modality-agnostic, compact, and efficient graph-based model. The novel contributions include a Learnable Graph Inception Network (L-GrIN) with new graph convolution layers, graph inception modules, learnable graph structures, and learnable graph-to-vector pooling functions.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.75,\n    \"b12\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.65,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of dynamic emotion recognition across multiple modalities (video, audio, motion capture) using a novel graph-based approach. The primary contributions include a modality-agnostic graph approach, a novel architecture termed L-GrIN with several innovative components, and state-of-the-art performance on benchmark emotion recognition databases.\",\n    \"Direct Inspiration\": {\n        \"b10\": 1.0,\n        \"b11\": 1.0,\n        \"b12\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b5\": 0.8,\n        \"b14\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b42\": 0.7,\n        \"b47\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of dynamic emotion recognition using a compact, efficient, and scalable graph-based model suitable for resource-constrained devices. It proposes a novel Graph Convolution Network (GCN) architecture called Learnable Graph Inception Network (L-GrIN), which includes components such as non-linear spectral graph convolution, graph inception layer, learnable pooling, and learnable adjacency matrix.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b5\", \"b10\", \"b11\", \"b12\", \"b14\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b3\", \"b9\", \"b42\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b13\", \"b18\", \"b22\", \"b23\", \"b24\", \"b25\", \"b26\", \"b27\", \"b28\", \"b29\", \"b30\", \"b31\", \"b32\", \"b33\", \"b34\", \"b35\", \"b36\", \"b37\", \"b38\", \"b39\", \"b40\", \"b41\", \"b43\"]\n    }\n}\n```"], "6037666fd3485cfff1da31a7": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of robust visual object tracking under varying target appearances by proposing an Adaptive Attribute-Aware Discriminative Correlation Filter (A3DCF) method. The main contributions include channel-wise spatial attention learning and post-processing of attribute-related spatial patterns to reduce irrelevant information and enhance discriminative capabilities.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b12\": 0.9,\n    \"b13\": 0.9,\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b1\": 0.7,\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the redundancy and inconsistency in multi-channel feature maps used in Discriminative Correlation Filter (DCF) based visual object tracking. The proposed algorithm, Adaptive Attribute-Aware DCF (A3DCF), introduces an adaptive spatial attention mechanism to emphasize channel-specific discriminative features and reduce irrelevant information. This is achieved by simultaneously optimizing filter coefficients and channel-wise binary spatial attention patterns, thus enhancing the discriminative capability and robustness of the tracking model.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9,\n        \"b12\": 0.9,\n        \"b13\": 0.85,\n        \"b15\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.75,\n        \"b5\": 0.7,\n        \"b38\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.6,\n        \"b8\": 0.6,\n        \"b9\": 0.6,\n        \"b18\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in visual object tracking, particularly the issues of information redundancy in multi-channel features and the need for adaptive spatial attention mechanisms. The proposed Adaptive Attribute-Aware Discriminative Correlation Filters (A3 DCF) method aims to enhance tracking performance by optimizing spatial configurations for each feature map, reducing irrelevant information, and highlighting discriminative elements.\",\n  \"Direct Inspiration\": [\"b3\", \"b12\", \"b13\"],\n  \"Indirect Inspiration\": [\"b4\", \"b5\", \"b38\"],\n  \"Other Inspiration\": [\"b1\", \"b2\", \"b8\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of visual object tracking under unconstrained scenarios, focusing on reducing redundancy in multi-channel feature maps and enhancing discriminative capability using an adaptive attribute-aware scheme. The proposed A3DCF method achieves robustness by emphasizing channel-specific discriminative features and employing adaptive spatial attention patterns.\",\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [\"b3\", \"b4\", \"b5\", \"b12\", \"b13\", \"b15\"],\n  \"Other Inspiration\": [\"b1\", \"b7\", \"b8\", \"b9\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Handling appearance variations in visual object tracking\",\n      \"Reducing irrelevant and redundant information in multi-channel features\",\n      \"Improving spatial attention mechanisms for better feature discrimination\"\n    ],\n    \"Proposed Algorithm\": \"Adaptive Attribute-Aware Discriminative Correlation Filters (A3DCF)\"\n  },\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b12\": 1,\n    \"b13\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b14\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```"], "61f8a4c35aee126c0fee034d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of long-term time series forecasting, particularly the issues with RNNs and Transformers in maintaining global properties of time series. It proposes the Frequency Enhanced Decomposition Transformer (FEDformer) which incorporates seasonal-trend decomposition and Fourier analysis to improve forecasting accuracy and computational efficiency.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.95,\n    \"b33\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.85,\n    \"b34\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.75,\n    \"b37\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in long-term time series forecasting, particularly the limitations of RNN-type methods and vanilla Transformer methods. To overcome these, the authors propose a Frequency Enhanced Decomposed Transformer (FEDformer) that incorporates seasonal-trend decomposition and Fourier analysis to better capture global properties of time series. Key contributions include a novel architecture for seasonal-trend decomposition, Fourier and Wavelet enhanced blocks, and a random selection method for Fourier components to achieve linear computational complexity.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b21\": 0.9,\n    \"b33\": 0.9,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b22\": 0.7,\n    \"b31\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b17\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of long-term time series forecasting, particularly the limitations of RNN and Transformer-based methods in capturing the overall characteristics and distribution of time series data. The proposed method, FEDformer, incorporates a seasonal-trend decomposition approach and combines Fourier analysis with the Transformer-based method to improve prediction accuracy and computational efficiency.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b31\": 0.7,\n    \"b37\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of long-term time series forecasting by proposing a Frequency Enhanced Decomposition Transformer (FEDformer). The primary contributions include incorporating seasonal-trend decomposition into the Transformer model, combining Fourier and Wavelet analyses with Transformer, and achieving linear computational complexity through random selection of Fourier components. The proposed model aims to better capture global properties of time series and reduce computational costs without performance drops.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b21\": 0.9,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b22\": 0.6,\n    \"b31\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in long-term time series forecasting, specifically the limitations of RNN-type methods and Transformer-based methods. The proposed solution, FEDformer, incorporates seasonal-trend decomposition and Fourier analysis to improve the prediction accuracy and computational efficiency.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b6\", \"b21\", \"b33\", \"b34\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b22\", \"b31\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b10\", \"b37\"]\n  }\n}\n```"], "607963ca91e011e7b3d0cc49": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of source address spoofing in the Internet, particularly in the context of DDoS attacks. It proposes a novel solution called SEC, which combines ECDHE key agreement and RPKI for secure key negotiation, and introduces a lightweight tag generation algorithm that is compatible with existing network functionalities. The system aims to be secure, efficient, and compatible with partial deployment.\",\n  \"Direct Inspiration\": [\"b9\", \"b10\", \"b11\"],\n  \"Indirect Inspiration\": [\"b1\", \"b2\", \"b4\"],\n  \"Other Inspiration\": [\"b12\", \"b13\", \"b15\", \"b16\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies the challenge of source address spoofing in the context of DDoS attacks and proposes a secure, efficient, and compatible (SEC) source address validation scheme. The SEC system uses Elliptic Curve Diffie-Hellman Ephemeral (ECDHE) key agreement and Resource Public Key Infrastructure (RPKI) for secure key negotiation and a lightweight tag generation algorithm for source address validation.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of source address spoofing in network traffic, particularly focusing on Distributed Denial-of-Service (DDoS) attacks. The proposed solution, SEC, uses a combination of Elliptic Curve Diffie-Hellman Ephemeral (ECDHE) key agreement and Resource Public Key Infrastructure (RPKI) for secure key negotiation, and introduces a lightweight tag generation algorithm to efficiently and securely validate source addresses.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b10\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper proposes SEC, a secure, efficient, and compatible source address validation scheme based on packet tags. The key challenges addressed include preventing Man-in-the-Middle (MitM) attacks, reducing computational overhead, and ensuring compatibility with network functionalities. The proposed system combines Elliptic Curve Diffie-Hellman Ephemeral (ECDHE) key agreement and Resource Public Key Infrastructure (RPKI) for secure key negotiation, and designs a lightweight tag generation algorithm for efficient processing. It also introduces a timestamp field to resist replay attacks.\",\n    \"Direct Inspiration\": {\n        \"b9\": 1,\n        \"b10\": 1,\n        \"b11\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.9,\n        \"b15\": 0.8,\n        \"b16\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.7,\n        \"b18\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of source address spoofing in the context of DDoS attacks. The proposed solution, SEC, is a secure, efficient, and compatible source address validation scheme based on packet tags. It combines ECDHE key agreement and RPKI for secure key negotiation, uses a lightweight tag generation algorithm, and incorporates a timestamp to resist replay attacks. The system design ensures compatibility with existing network functionalities and provides high deployment incentives.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b10\": 1.0,\n    \"b11\": 1.0,\n    \"b15\": 0.9,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```"], "609a6dc2e4510cd7c88fbc3e": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is managing the metadata required by temporal prefetchers without incurring high latency, bandwidth consumption, and energy costs associated with off-chip metadata storage. The proposed solution, Triage, is a novel temporal prefetcher that uses on-chip metadata storage by repurposing a portion of the last-level cache (LLC), and it includes a dynamic cache partitioning scheme and a new metadata representation to efficiently manage and compress metadata.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b0\": 0.8,\n    \"b5\": 0.8,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of managing off-chip metadata for temporal prefetchers, which incurs high latency, bandwidth consumption, and energy costs. The proposed solution, Triage, is a temporal prefetcher that requires no off-chip metadata by repurposing a portion of the LLC for storing prefetcher metadata. The Triage prefetcher significantly reduces off-chip traffic overhead, energy consumption, and hardware complexity while offering competitive performance in bandwidth-constrained environments.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b0\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b11\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is managing the high latency, bandwidth consumption, and energy costs associated with retrieving large metadata for temporal prefetchers from off-chip DRAM. The proposed solution, Triage, is a temporal prefetcher that eliminates the need for off-chip metadata by storing only the most frequently used metadata in the last-level cache (LLC) and introducing a novel compressed metadata representation for regular accesses.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b3\", \"b4\", \"b5\", \"b11\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b0\", \"b1\", \"b6\", \"b7\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b8\", \"b9\", \"b10\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the high latency, bandwidth consumption, and energy costs associated with managing off-chip metadata for temporal prefetchers. The proposed solution, Triage, is a temporal prefetcher that requires no off-chip metadata. It repurposes a portion of the last-level cache (LLC) to store metadata, uses a dynamic cache partitioning scheme, and introduces a new compressed metadata representation for regular access patterns.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b1\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents Triage, a novel temporal prefetcher that requires no off-chip metadata, addressing challenges of high latency, DRAM bandwidth consumption, and energy costs associated with traditional temporal prefetchers. Triage uses on-chip last-level cache (LLC) space for metadata storage, employing a Hawkeye replacement policy for identifying important metadata and a novel compressed representation for regular accesses.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b11\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b1\": 0.65\n  }\n}\n```"], "601bbb7391e011fcd67ef13a": ["```json\n{\n  \"Summary\": \"The paper addresses the issue of instruction cache misses in modern processors and proposes a novel instruction prefetcher called MANA. The main challenges identified include the inefficiency of existing prefetchers (RDIP, Shotgun, PIF) due to high storage costs and the need for a storage-efficient design. The proposed MANA prefetcher leverages spatial correlation to create compact metadata records and minimize distinct records, achieving high performance with low storage.\",\n  \"Direct Inspiration\": [\"b18\", \"b29\", \"b30\"],\n  \"Indirect Inspiration\": [\"b5\", \"b23\", \"b25\", \"b31\"],\n  \"Other Inspiration\": [\"b22\", \"b39\", \"b43\", \"b44\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of instruction cache misses in modern processors and introduces the MANA prefetcher, which focuses on spatial correlation to minimize storage overhead while maximizing prefetching efficiency. The paper evaluates existing prefetchers like RDIP, Shotgun, and PIF, highlighting their limitations in storage efficiency and performance under constrained storage budgets.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b29\": 0.9,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of instruction cache misses in modern processors, which degrade performance. It introduces the MANA prefetcher, which leverages spatial correlation to create compact metadata records and minimizes storage overhead. The paper emphasizes the importance of storage efficiency in designing a strong instruction prefetcher and compares MANA with RDIP, Shotgun, and PIF prefetchers, showing superior performance with lower storage requirements.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b29\": 0.9,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.7,\n    \"b5\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of instruction cache misses by introducing MANA prefetcher, which aims to be storage-efficient. The key innovations include leveraging spatial correlation to create compact metadata records and chaining spatial-metadata records to take advantage of temporal correlation, thus minimizing storage cost while maintaining high performance.\",\n    \"Direct Inspiration\": {\n        \"b18\": 1.0,\n        \"b29\": 0.9,\n        \"b30\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b25\": 0.7,\n        \"b31\": 0.7,\n        \"b44\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.4,\n        \"b39\": 0.4,\n        \"b43\": 0.4,\n        \"b53\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of instruction cache misses in processors and proposes the MANA prefetcher, which aims to be storage-efficient while achieving high performance. The paper is inspired by various prefetching techniques, specifically focusing on reducing storage costs and improving hit ratios by leveraging spatial and temporal correlations.\",\n  \"Direct Inspiration\": {\n    \"PIF\": \"b18\",\n    \"RDIP\": \"b29\",\n    \"Shotgun\": \"b30\"\n  },\n  \"Indirect Inspiration\": {\n    \"SHIFT\": \"b25\"\n  },\n  \"Other Inspiration\": {\n    \"TIFS\": \"b19\",\n    \"Boomerang\": \"b31\"\n  }\n}\n```"], "61f8a4c35aee126c0fee04fa": ["```json\n{\n  \"Summary\": \"The paper addresses the trade-off between interpretability and prediction accuracy in graph learning models by proposing a novel attention mechanism called Graph Stochastic Attention (GSAT). GSAT introduces stochasticity to constrain information flow, thereby improving both interpretability and prediction accuracy without relying on potentially biased assumptions.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9,\n    \"b36\": 0.9,\n    \"b33\": 0.85,\n    \"b42\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b12\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b19\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating inherently interpretable Graph Neural Networks (GNNs) that do not sacrifice prediction accuracy. The proposed Graph Stochastic Attention (GSAT) mechanism introduces stochasticity into the attention to constrain information flow, which helps in providing reliable interpretation of the model's predictions.\",\n  \"Direct Inspiration\": [\"b7\", \"b42\"],\n  \"Indirect Inspiration\": [\"b37\", \"b36\", \"b33\"],\n  \"Other Inspiration\": [\"b10\", \"b25\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of balancing prediction accuracy and inherent interpretability in Graph Neural Networks (GNNs). It proposes a novel Graph Stochastic Attention (GSAT) mechanism that uses the Information Bottleneck (IB) principle to inject stochasticity into attention, constraining information flow and providing reliable model interpretation without compromising accuracy.\",\n  \"Direct Inspiration\": [\"b37\", \"b36\", \"b33\"],\n  \"Indirect Inspiration\": [\"b42\", \"b25\", \"b31\", \"b7\"],\n  \"Other Inspiration\": [\"b4\", \"b19\", \"b10\", \"b12\", \"b24\", \"b2\", \"b45\", \"b32\", \"b17\", \"b26\", \"b39\", \"b22\", \"b15\", \"b31\", \"b13\", \"b9\", \"b35\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in graph learning tasks, specifically the trade-off between prediction accuracy and interpretability, and the limitations of post-hoc interpretability methods. The proposed solution, Graph Stochastic Attention (GSAT), introduces a novel attention mechanism rooted in the information bottleneck (IB) principle to provide inherently interpretable graph neural networks (GNNs) without compromising on prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b33\": 1,\n    \"b42\": 1,\n    \"b25\": 0.9,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.8,\n    \"b36\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b39\": 0.6,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of building inherently interpretable Graph Neural Networks (GNNs) without sacrificing prediction accuracy. It introduces a novel attention mechanism called Graph Stochastic Attention (GSAT) that injects stochasticity to control information flow, thereby providing reliable model interpretation. The method is based on the Information Bottleneck (IB) principle and overcomes limitations of existing post-hoc interpretation methods and attention mechanisms in GNNs.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b37\": 0.9,\n    \"b36\": 0.9,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b31\": 0.7,\n    \"b42\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```"], "61bc001b5244ab9dcba3fb36": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning task-relevant graph structures for Graph Neural Networks (GNNs), particularly in noisy or incomplete data scenarios. The proposed method, VIB-GSL, leverages the Information Bottleneck (IB) principle to mask irrelevant features and learn an optimal graph structure, thus improving the robustness and generalization of GNNs.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b27\": 0.8,\n    \"b30\": 0.8,\n    \"b1\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.6,\n    \"b20\": 0.6,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b36\": 0.5,\n    \"b7\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed by the paper is the need for an adaptive and robust graph structure learning method that can handle noisy, incomplete, or non-graph-structured data. The proposed algorithm, VIB-GSL, advances the Information Bottleneck (IB) principle to learn an optimal graph structure by masking irrelevant features and learning a new graph structure based on the masked features. This approach is designed to be robust to noise and adaptable to various graph representation learning tasks.\",\n  \"Direct Inspiration\": [\"b24\", \"b27\", \"b30\"],\n  \"Indirect Inspiration\": [\"b1\", \"b3\", \"b13\"],\n  \"Other Inspiration\": [\"b4\", \"b7\", \"b36\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of graph representation learning, particularly the issue of noisy or incomplete graph structures and the mismatch between raw graphs and task-specific optimal graphs. The proposed algorithm is a Variational Information Bottleneck guided Graph Structure Learning framework (VIB-GSL), which aims to learn a task-relevant graph structure that is robust to noise by employing a feature masking and structure learning method.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b27\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning task-relevant graph structures for graph representation learning, particularly in noisy or incomplete graphs. It proposes a novel Variational Information Bottleneck guided Graph Structure Learning (VIB-GSL) framework to generate a new graph structure that distills actionable information for downstream tasks. The method focuses on masking irrelevant features and learning a new graph structure based on the Information Bottleneck principle.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b27\": 0.9,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in graph representation learning, particularly the issues with noisy, incomplete, or task-irrelevant graph structures. The authors propose a novel Variational Information Bottleneck guided Graph Structure Learning framework (VIB-GSL) to adaptively learn optimal graph structures for downstream tasks. VIB-GSL employs the Information Bottleneck principle to generate an IB-Graph by masking irrelevant node features and learning a new graph structure based on these features, making the representation robust to noise and irrelevant information.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b27\": 0.9,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b4\": 0.6,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```"], "5ff68afbd4150a363cce0f22": ["```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies in the current two-layered scheduling approach for DNN computation on hardware accelerators, particularly GPUs. It introduces RAMMER, a holistic DNN compiler framework that unifies inter- and intra-operator scheduling through novel abstractions like rTask and vDevice. The goal is to achieve better performance and hardware utilization by managing parallelism more effectively.\",\n  \"Direct Inspiration\": [\"b13\", \"b18\"],\n  \"Indirect Inspiration\": [\"b40\", \"b9\", \"b6\"],\n  \"Other Inspiration\": [\"b10\", \"b44\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency of current two-layer scheduling approaches for DNN computation on hardware devices, which incurs significant performance limitations. The proposed solution, RAMMER, unifies inter- and intra-operator scheduling through a novel abstraction called rTask, enabling a more holistic optimization that can be applied across diverse DNN accelerators.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b18\": 0.95,\n    \"b13\": 0.85,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.75,\n    \"b31\": 0.7,\n    \"b24\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the inefficiencies in scheduling deep neural network (DNN) computations on hardware accelerators due to the existing two-layered scheduling approach. RAMMER, a deep learning compiler, is proposed to unify inter- and intra-operator scheduling through a novel abstraction called rTask, which enables fine-grained scheduling and reduces runtime overhead by moving scheduling decisions to compile time.\",\n    \"Direct Inspiration\": {\n        \"b13\": 0.9,\n        \"b18\": 0.85,\n        \"b40\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.7,\n        \"b9\": 0.65,\n        \"b6\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.6,\n        \"b44\": 0.55,\n        \"b49\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies in the two-layer scheduling approach for DNN computation on hardware devices, proposing a novel unified scheduling framework called RAMMER. RAMMER introduces a fine-grained scheduling abstraction (rTask, rOperator, and vEU) to exploit both inter- and intra-operator parallelism, aiming to significantly improve performance across various hardware accelerators.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b13\": 0.9,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b9\": 0.7,\n    \"b11\": 0.7,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b15\": 0.4,\n    \"b34\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the inefficiencies in DNN computation scheduling on hardware devices. It proposes RAMMER, a deep learning compiler that unifies inter and intra-operator scheduling using a novel abstraction called rTask.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b13\", \"b18\"],\n        \"comments\": \"The authors explicitly mention using TensorFlow [b13] and techniques from kernel tuners [b18] to optimize DNN computation.\"\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b9\", \"b10\", \"b11\"],\n        \"comments\": \"The authors compare the performance of RAMMER with existing frameworks and libraries such as cuDNN [b9], TensorRT [b10], and ONNX Runtime [b11].\"\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b40\", \"b44\"],\n        \"comments\": \"The authors use compile-time profiling techniques [b40] and reference ResNeXt model [b44] to illustrate the effectiveness of their approach.\"\n    }\n}\n```"], "5ffd8ad891e01106b32411ad": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of blind face restoration, which involves recovering high-quality faces from degraded images. The primary innovation is the use of Generative Facial Prior (GFP) encapsulated in pretrained Generative Adversarial Networks (GANs) to improve the restoration process. The proposed method, GFP-GAN, incorporates a degradation removal module and a pretrained face GAN, connected via a direct latent code mapping and Channel-Split Spatial Feature Transform (CS-SFT) layers. This combination aims to balance realness and fidelity in the restored images.\",\n    \"Direct Inspiration\": {\n        \"b35\": 0.9,\n        \"b36\": 0.9,\n        \"b76\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.7,\n        \"b45\": 0.7,\n        \"b55\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.6,\n        \"b47\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of blind face restoration, especially in real-world scenarios with complex degradation. It proposes a novel GFP-GAN framework that leverages generative facial priors from pretrained GANs like StyleGAN to achieve high-fidelity and realistic face restoration. Key innovations include the use of Channel-Split Spatial Feature Transform (CS-SFT) layers and a combination of various loss functions to balance realness and fidelity.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b36\": 1.0,\n    \"b45\": 0.9,\n    \"b63\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b35\": 0.7,\n    \"b53\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b8\": 0.5,\n    \"b69\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of blind face restoration from low-quality inputs with unknown degradation by leveraging Generative Facial Prior (GFP) from pretrained GANs. The authors propose a novel GFP-GAN framework that incorporates generative priors using Channel-Split Spatial Feature Transform (CS-SFT) layers, which balance realness and fidelity. The method also employs facial component loss with local discriminators and identity preserving loss to enhance facial details and maintain fidelity.\",\n  \"Direct Inspiration\": [\"b35\", \"b36\"],\n  \"Indirect Inspiration\": [\"b45\", \"b47\"],\n  \"Other Inspiration\": [\"b19\", \"b55\", \"b63\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include restoring high-quality faces from low-quality counterparts suffering from various degradations (e.g., noise, blur, compression artifacts) in real-world scenarios, where previous methods using facial priors often fail due to limited texture information and dependency on high-quality references. The proposed algorithm, GFP-GAN, leverages Generative Facial Prior (GFP) encapsulated in pretrained face GANs to provide rich and diverse priors for joint face restoration and color enhancement. The method includes a novel architecture with Channel-Split Spatial Feature Transform (CS-SFT) layers and introduces facial component loss with local discriminators to improve fidelity and perceptual details.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1,\n    \"b36\": 1,\n    \"b19\": 0.9,\n    \"b55\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.8,\n    \"b5\": 0.8,\n    \"b69\": 0.8,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b53\": 0.6,\n    \"b18\": 0.6,\n    \"b47\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of blind face restoration from low-quality inputs with unknown degradation. The authors propose a novel GFP-GAN framework that leverages Generative Facial Prior (GFP) from pretrained face GANs to achieve a balance of fidelity and texture faithfulness in a single forward pass. The proposed method involves a degradation removal module, a direct latent code mapping, and Channel-Split Spatial Feature Transform (CS-SFT) layers.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1,\n    \"b36\": 1,\n    \"b19\": 0.8,\n    \"b55\": 0.8,\n    \"b53\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.7,\n    \"b69\": 0.7,\n    \"b8\": 0.7,\n    \"b5\": 0.7,\n    \"b47\": 0.6,\n    \"b46\": 0.6,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b34\": 0.5,\n    \"b31\": 0.5,\n    \"b64\": 0.4,\n    \"b63\": 0.4\n  }\n}\n```"], "61e781675244ab9dcbf9a686": ["```json\n{\n    \"Summary\": {\n        \"challenges\": [\n            \"Noisy information in original graph topology\",\n            \"Reliance on label information in supervised GSL methods\",\n            \"Bias in learned edge distribution\",\n            \"Limitation on downstream tasks\"\n        ],\n        \"inspiration\": \"Developing an unsupervised graph structure learning (GSL) paradigm using self-supervised contrastive learning to overcome the challenges of noisy topology, label reliance, edge distribution bias, and limited applicability of existing supervised GSL methods.\"\n    },\n    \"Direct Inspiration\": [\n        \"b24\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b6\",\n        \"b10\",\n        \"b11\",\n        \"b19\",\n        \"b57\"\n    ],\n    \"Other Inspiration\": [\n        \"b21\",\n        \"b31\",\n        \"b44\"\n    ]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges addressed in the paper are the reliance on noisy and potentially inaccurate graph structures in GNNs, the dependence on labeled data in existing supervised GSL methods, and the poor generalization of learned structures to multiple downstream tasks. The proposed algorithm, SUBLIME, introduces an unsupervised graph structure learning framework that leverages self-supervised contrastive learning to optimize graph structures without external label guidance, aiming to provide a universal and edge-unbiased topology.\",\n    \"Direct Inspiration\": {\n        \"b10\": 1.0,\n        \"b11\": 1.0,\n        \"b19\": 1.0,\n        \"b44\": 1.0,\n        \"b57\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b39\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b24\": 0.6,\n        \"b21\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of noisy, redundant, and missing connections in graph neural networks (GNNs) and the reliance on supervised learning methods. It proposes an unsupervised learning paradigm called SUBLIME, which uses contrastive learning to optimize the graph structure without external labels. The method constructs an 'anchor graph' to guide structure optimization and employs a bootstrapping mechanism to update the anchor graph with learned edges.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b11\": 1.0,\n    \"b24\": 1.0,\n    \"b39\": 1.0,\n    \"b57\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b19\": 0.8,\n    \"b52\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b44\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in this paper is the reliance of Graph Neural Networks (GNNs) on noisy and imperfect graph structures, which negatively impacts their performance. The paper proposes a novel unsupervised learning paradigm called SUBLIME (StrUcture Bootstrapping contrastive LearnIng fraMEwork) that aims to learn and optimize graph structures without the need for label information. The approach leverages self-supervised contrastive learning to maximize the mutual information between an anchor graph and the learned structure, promoting a balanced edge distribution and generalization to various downstream tasks.\",\n  \"Direct Inspiration\": [\"b6\", \"b10\", \"b11\", \"b19\", \"b44\", \"b57\"],\n  \"Indirect Inspiration\": [\"b24\", \"b31\", \"b38\", \"b58\"],\n  \"Other Inspiration\": [\"b21\", \"b39\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of noisy, redundant, and missing connections in graph structures used by Graph Neural Networks (GNNs). It proposes SUBLIME, an unsupervised graph structure learning method leveraging contrastive learning to optimize graph structures without relying on labeled data. Key contributions include the introduction of an unsupervised learning paradigm for graph structure learning, the development of the SUBLIME algorithm, and extensive evaluations showing its effectiveness.\",\n  \"Direct Inspiration\": [\n    \"b24\",\n    \"b10\",\n    \"b11\",\n    \"b57\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b21\",\n    \"b31\"\n  ],\n  \"Other Inspiration\": [\n    \"b6\",\n    \"b19\",\n    \"b44\"\n  ]\n}\n```"], "608a961fe4510cd7c870dcac": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of instruction cache (I-cache) misses and pipeline stalls in modern processors by proposing enhancements to Fetch-Directed Prefetching (FDP). The novel methods include using taken-only branch target history and post-fetch correction (PFC) to improve the accuracy and efficiency of FDP.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1.0,\n        \"b28\": 0.9,\n        \"b29\": 0.9,\n        \"b31\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b24\": 0.7,\n        \"b25\": 0.7,\n        \"b26\": 0.7,\n        \"b32\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.5,\n        \"b14\": 0.5,\n        \"b27\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of instruction cache (I-cache) misses and frequent pipeline stalls in modern processors by proposing an improved Fetch-Directed Prefetching (FDP) mechanism. It introduces two key enhancements: taken-only branch target history and post-fetch correction, to improve FDP's effectiveness and bridge the gap between industry and academia in frontend design.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b26\": 0.9,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges related to instruction cache (I-cache) misses and proposes improvements to Fetch-Directed Prefetching (FDP). The main contributions include a comprehensive FDP microarchitecture, taken-only branch target history, and post-fetch correction (PFC) to improve performance and bridge the gap between industry and academic frontend designs.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b14\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b32\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the limitations of Fetch-Directed Prefetching (FDP) in handling I-cache misses due to branch mispredictions and proposes two enhancements: taken-only branch target history and post-fetch correction (PFC). These enhancements aim to improve FDP's performance and bridge the gap between academic and commercial CPU designs.\",\n    \"Direct Inspiration\": {\n        \"b8\": 0.9,\n        \"b28\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.7,\n        \"b26\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.5,\n        \"b13\": 0.5,\n        \"b14\": 0.5,\n        \"b32\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of instruction cache (I-cache) misses in modern processors due to increasing instruction footprints, which cause frequent pipeline stalls. It focuses on improving Fetch-Directed Prefetching (FDP) by proposing two enhancements: taken-only branch target history and post-fetch correction (PFC). These enhancements aim to make FDP more tolerant of BTB-miss not-taken branches and to quickly recover from misprediction penalties.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b28\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.6,\n    \"b33\": 0.6,\n    \"b34\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b14\": 0.5,\n    \"b38\": 0.5\n  }\n}\n```"], "60658db591e011d10ad61319": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper are the representation of non-Euclidean data, particularly graphs, in a way that can be effectively processed by neural networks. The paper aims to extend the algebraic structure beyond real numbers to hypercomplex numbers to enhance the expressive power of Graph Neural Networks (GNNs).\",\n    \"inspirations\": \"The paper is inspired by previous work on geometric deep learning, hypercomplex number systems, and the parameter efficiency techniques in deep learning.\"\n  },\n  \"Direct Inspiration\": {\n    \"b46\": 0.95,\n    \"b30\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b5\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.55,\n    \"b39\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of applying deep learning methods to non-Euclidean domains, specifically focusing on graph data which poses significant topological challenges. The proposed solution introduces Parameterized Hypercomplex Graph Neural Networks (PHC-GNNs) that leverage hypercomplex algebras to improve node and graph representations. Key contributions include the development of PHM-layers, novel initialization and regularization techniques, and empirical evidence supporting the model's effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b30\": 0.85,\n    \"b46\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b9\": 0.75,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.65,\n    \"b10\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of geometric deep learning in non-Euclidean domains, particularly focusing on graph neural networks (GNNs) and their extensions to hypercomplex algebras. The authors propose Parameterized Hypercomplex Graph Neural Networks (PHC-GNNs) which combine the expressiveness of GNNs and hypercomplex algebras, introduce novel initialization and regularization techniques for the PHM-layer, and demonstrate the effectiveness of these models in graph property prediction tasks.\",\n    \"Direct Inspiration\": {\n        \"b30\": 0.9,\n        \"b46\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.7,\n        \"b10\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b16\": 0.6,\n        \"b24\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the representation of graphs in non-Euclidean space and the development of graph-based learning methods that extend beyond Euclidean topology. The proposed algorithm, Parameterized Hypercomplex Graph Neural Networks (PHC-GNNs), addresses these challenges by using hypercomplex algebras to improve node and graph representations. The paper emphasizes the importance of hypercomplex number systems and their algebraic properties for neural network applications.\",\n  \"Direct Inspiration\": {\n    \"b46\": 1.0,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b9\": 0.6,\n    \"b28\": 0.5,\n    \"b42\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in geometric deep learning, particularly in graph neural networks (GNNs) where data is represented in non-Euclidean domains. The authors propose Parameterized Hypercomplex Graph Neural Networks (PHC-GNNs) to improve graph representation learning by leveraging hypercomplex algebras. They introduce novel initialization and regularization techniques for PHM-layers and demonstrate the effectiveness of their model in achieving state-of-the-art performance with lower memory footprint.\",\n  \n  \"Direct Inspiration\": {\n    \"b30\": 0.9,\n    \"b46\": 0.8,\n    \"b32\": 0.7\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b9\": 0.6,\n    \"b11\": 0.5,\n    \"b39\": 0.5\n  },\n  \n  \"Other Inspiration\": {\n    \"b2\": 0.4,\n    \"b28\": 0.4\n  }\n}\n```"], "60a4db4091e011e398b0cf06": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in implementing and evaluating hardware-software cooperative techniques for cross-layer performance optimization in general-purpose processors. It introduces MetaSys, a full-system FPGA-based infrastructure with a rich hardware-software interface, optimized metadata management, and modularized components to facilitate rapid implementation and evaluation of diverse cross-layer techniques in real hardware.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b17\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b27\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of implementing and evaluating hardware-software cooperative techniques in real hardware, which is difficult due to the need for extensive cross-layer changes. It introduces MetaSys, a full-system FPGA-based infrastructure, to facilitate the rapid development and evaluation of cross-layer techniques. MetaSys includes a rich hardware-software interface, low-overhead metadata management, and modular components to support various optimizations.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b19\": 0.9,\n    \"b17\": 0.85,\n    \"b18\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b22\": 0.75,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b27\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the optimization of general-purpose processors through hardware-software cooperative techniques. The proposed solution, MetaSys, is an FPGA-based infrastructure designed to evaluate and implement diverse cross-layer techniques in real hardware. Key components include a rich hardware-software interface, metadata management support, and modular components for various optimizations.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b26\": 0.7,\n    \"b27\": 0.7,\n    \"b28\": 0.7,\n    \"b29\": 0.7,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces MetaSys, a novel FPGA-based infrastructure designed to implement and evaluate cross-layer performance optimizations in real hardware. MetaSys addresses challenges such as providing a dynamic hardware-software interface, low-overhead metadata management, and flexible module implementation for various hardware optimizations. The system is inspired by and builds upon previous works such as XMem and tagged memory systems.\",\n  \"Direct Inspiration\": [\"b21\"],\n  \"Indirect Inspiration\": [\"b19\", \"b26\", \"b27\", \"b28\", \"b29\"],\n  \"Other Inspiration\": [\"b17\", \"b18\", \"b20\", \"b24\", \"b25\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents MetaSys, a full-system FPGA-based infrastructure designed to improve the performance and efficiency of general-purpose processors through hardware-software cooperative techniques. MetaSys introduces a rich hardware-software interface for dynamically communicating metadata at runtime, tagged memory-based metadata management, and modular components for implementing various cross-layer optimizations. Key challenges addressed include the need for rich dynamic hardware-software interfaces, low-overhead metadata management, and interfaces to multiple hardware components. MetaSys is inspired by prior works like XMem and offers improvements in flexibility, runtime metadata communication, and system optimization.\",\n  \"Direct Inspiration\": [\"b21\"],\n  \"Indirect Inspiration\": [\"b17\", \"b18\", \"b19\"],\n  \"Other Inspiration\": [\"b26\", \"b27\", \"b28\", \"b29\", \"b30\"]\n}\n```"], "6008326e9e795ed227f53106": ["```json\n{\n  \"Summary\": \"The paper focuses on addressing the challenges in HardWare-aware Neural Architecture Search (HW-NAS), specifically the collection of hardware efficiency data and benchmarking HW-NAS algorithms. It introduces HW-NAS-Bench, a public dataset designed to democratize HW-NAS research and provide a unified benchmark for reproducibility and accessibility, covering NAS-Bench-201 and FBNet search spaces with hardware-cost data for various devices.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1,\n    \"b52\": 0.9,\n    \"b12\": 0.9,\n    \"b24\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b23\": 0.7,\n    \"b39\": 0.7,\n    \"b42\": 0.7,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b6\": 0.6,\n    \"b29\": 0.6,\n    \"b49\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"HW-NAS requires extensive hardware efficiency data collection, which is time-consuming and complex.\",\n      \"Benchmarking HW-NAS algorithms is difficult due to the requirement of significant computational resources and varying hardware devices, search spaces, and hyperparameters.\"\n    ],\n    \"Inspirations\": [\n      \"Development of HW-NAS-Bench to democratize HW-NAS research and provide a unified benchmark.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b45\": 0.9,\n    \"b42\": 0.8,\n    \"b5\": 0.8,\n    \"b11\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b52\": 0.6,\n    \"b12\": 0.6,\n    \"b24\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the main challenges in HardWare-aware Neural Architecture Search (HW-NAS), which include the time-consuming and expertise-demanding process of collecting hardware efficiency data and the difficulty of benchmarking HW-NAS algorithms due to differences in hardware devices, search spaces, and hyperparameters. The authors propose HW-NAS-Bench, a comprehensive public dataset and framework designed to democratize HW-NAS research, making it accessible to non-hardware experts and providing a unified benchmark for reproducibility. HW-NAS-Bench includes hardware-cost data collection pipelines for six commonly used hardware devices and covers two SOTA NAS search spaces: NAS-Bench-201 and FBNet.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.7,\n    \"b24\": 0.6,\n    \"b37\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.5,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses significant challenges in Hardware-aware Neural Architecture Search (HW-NAS), particularly focusing on the need for hardware efficiency data collection and benchmarking. The proposed solution, HW-NAS-Bench, aims to democratize HW-NAS research and provide a unified benchmark dataset for reproducibility and accessibility.\",\n  \"Direct Inspiration\": [\"b45\", \"b11\"],\n  \"Indirect Inspiration\": [\"b52\", \"b12\", \"b24\", \"b37\", \"b38\"],\n  \"Other Inspiration\": [\"b42\", \"b5\", \"b6\", \"b29\", \"b49\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses major challenges in HW-NAS, such as the time-consuming collection of hardware efficiency data and the difficulty in benchmarking HW-NAS algorithms. The proposed HW-NAS-Bench framework aims to democratize HW-NAS research and make it more reproducible and accessible by providing a unified benchmark dataset with comprehensive hardware-cost data on six commonly used hardware devices.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1.0,\n    \"b12\": 0.9,\n    \"b52\": 0.9,\n    \"b37\": 0.8,\n    \"b42\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b11\": 0.7,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.5,\n    \"b14\": 0.5,\n    \"b6\": 0.4,\n    \"b29\": 0.4\n  }\n}\n```"], "6173f1bc91e0118698c047f4": ["```json\n{\n  \"Summary\": \"The paper addresses the significant frontend stalls in data center applications caused by BTB misses. It proposes Twig, a novel profile-guided BTB prefetching mechanism that introduces software BTB prefetching and BTB prefetch coalescing to mitigate these stalls without requiring changes to the BTB organization.\",\n  \"Direct Inspiration\": {\n    \"b38\": 0.9,\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.7,\n    \"b40\": 0.7,\n    \"b67\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.6,\n    \"b14\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the significant frontend stalls in processors due to large instruction footprints in data center applications. The paper proposes a novel profile-guided BTB prefetching mechanism called Twig, which introduces software BTB prefetching and BTB prefetch coalescing to mitigate BTB misses without requiring hardware modifications.\",\n  \"Direct Inspiration\": {\n    \"b43\": 1.0,\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b67\": 0.9,\n    \"b31\": 0.7,\n    \"b20\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b33\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is significant frontend stalls in data center applications caused by large instruction footprints that lead to BTB misses. The paper proposes Twig, a profile-guided BTB prefetching mechanism that introduces software BTB prefetching and BTB prefetch coalescing to address these stalls.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b43\", \"b38\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b14\", \"b33\", \"b34\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b66\", \"b67\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of frontend stalls in data center applications due to BTB misses. It proposes a novel profile-guided BTB prefetching mechanism called Twig, which includes software BTB prefetching and BTB prefetch coalescing techniques to mitigate these stalls without requiring hardware modifications.\",\n  \"Direct Inspiration\": {\n    \"b43\": 1.0,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b67\": 0.8,\n    \"b66\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.6,\n    \"b33\": 0.6,\n    \"b34\": 0.6,\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge of the paper is reducing frontend stalls in data center applications caused by Branch Target Buffer (BTB) misses. The paper proposes a novel profile-guided BTB prefetching mechanism called 'Twig,' which introduces software BTB prefetching and BTB prefetch coalescing techniques to address these challenges and improve performance.\",\n  \"Direct Inspiration\": [\"b38\", \"b43\"],\n  \"Indirect Inspiration\": [\"b14\", \"b36\"],\n  \"Other Inspiration\": [\"b31\", \"b56\"]\n}\n```"], "618c94d76750f8456cb3e8be": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of query-focused summarization (QFS) without relying on query-specific datasets. It proposes a decomposition of QFS into query modeling and conditional language modeling, using generic summarization data to train both components. The novel approach leverages a Unified Masked Representation (UMR) inspired by the Cloze task to bridge the gap between generic summaries and QFS queries.\",\n    \"Direct Inspiration\": {\n        \"b35\": 1.0,\n        \"b22\": 0.9,\n        \"b20\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b39\": 0.8,\n        \"b34\": 0.7,\n        \"b25\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b9\": 0.6,\n        \"b13\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is creating query-focused summaries (QFS) without access to large query-related datasets. The proposed algorithm decomposes QFS into two subtasks: query modeling and conditional language modeling. The method leverages generic summarization data and introduces a Unified Masked Representation (UMR) to render queries and summaries in a unified format for training and testing.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1,\n    \"b22\": 1,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.8,\n    \"b34\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b27\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performing Query Focused Summarization (QFS) using generic summarization datasets due to the lack of large-scale QFS-specific datasets. It proposes a novel approach that decomposes QFS into query modeling and conditional language modeling, leveraging a Unified Masked Representation (UMR) inspired by the Cloze task to transform generic summarization data into proxy queries. The method does not require additional query-related resources and achieves state-of-the-art results in evidence ranking and abstractive summarization.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1,\n    \"b22\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.8,\n    \"b34\": 0.8,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of query-focused summarization (QFS) with limited training data by proposing a weakly supervised system that leverages generic summarization datasets. The key innovation is decomposing QFS into query modeling and conditional language modeling, using a Unified Masked Representation (UMR) inspired by the Cloze task. The approach is designed to function without additional query-related resources and achieves state-of-the-art results on QFS benchmarks.\",\n    \"Direct Inspiration\": [\"b35\", \"b22\", \"b20\"],\n    \"Indirect Inspiration\": [\"b31\", \"b11\", \"b25\", \"b9\", \"b39\", \"b34\", \"b18\", \"b30\", \"b4\", \"b7\"],\n    \"Other Inspiration\": [\"b6\", \"b36\", \"b2\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the lack of large-scale query-focused summarization (QFS) datasets and the mismatch between QA datasets and QFS scenarios. The algorithm proposed decomposes abstractive QFS into query modeling and conditional language modeling. It uses generic summarization data for both learning an evidence ranking model and generating conditional language models. The approach is inspired by the Cloze task and aims to use generic data to simulate QFS scenarios.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b22\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.7,\n    \"b30\": 0.6,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.5,\n    \"b33\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```"], "5ff880e391e011c83266b1ac": ["```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is the high overhead and intrusiveness of existing performance profiling tools, which are mitigated by the proposed K-LEB tool. K-LEB is a kernel module that provides precise, non-intrusive, and low-overhead performance counter data collection at high periodicity without requiring source code access.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b10\": 0.85,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b9\": 0.65,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.55,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the need for high-speed, non-intrusive, low-overhead performance evaluation tools for modern computing systems. The paper introduces K-LEB, a kernel-space performance counter profiling tool that addresses these challenges by enabling high-frequency, non-intrusive, low-overhead collection of performance data without requiring source code access or kernel patches.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.6,\n    \"b8\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.4,\n    \"b18\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges addressed in the paper are the overhead and intrusiveness of current performance profiling tools, and the need for high-frequency, non-intrusive performance counter data collection. The proposed K-LEB tool addresses these by using a kernel module for high periodicity data collection without the need for source code, reducing overhead compared to existing tools.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b11\": 0.6,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed by the paper are the overhead, intrusiveness, and timing constraints of existing performance profiling tools. The proposed algorithm, K-LEB, aims to provide precise, nonintrusive, low overhead, high periodicity performance counter data collection without requiring source code access, utilizing a kernel module for efficient data collection and storage.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b12\": 0.9,\n    \"b10\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b8\": 0.6,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high overhead, intrusiveness, and low sampling frequency in performance counter-based profiling tools. The proposed solution, K-LEB, is a kernel module that provides non-intrusive, low overhead, and high-frequency performance counter data collection without requiring source code access.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.95,\n    \"b10\": 0.85,\n    \"b14\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b9\": 0.70,\n    \"b11\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.60,\n    \"b6\": 0.55\n  }\n}\n```"], "60cd651691e011329faa2242": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of making large neural network models, specifically BERT, more computationally efficient for both small and large-scale deployment. The authors propose using structured and unstructured sparsification of attention weights combined with compiler optimizations in the TVM framework to achieve this goal. The key contributions are the implementation of block sparsity and integration with the TVM compiler to speed up inference times without significantly sacrificing accuracy.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.85,\n    \"b4\": 0.85,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.75,\n    \"b20\": 0.7,\n    \"b21\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of making large language models both efficient and accessible for practical applications by employing sparsification and pruning techniques. The authors implemented structured and unstructured sparsification of BERT's attention weights and incorporated Block Sparse Row (BSR) optimizations into the TVM compiler to enhance inference speed on CPUs. The key contributions include demonstrating the interaction between sparsity algorithms and compiler optimizations and achieving notable performance improvements through these integrations.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b11\": 0.8,\n    \"b2\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b9\": 0.65,\n    \"b21\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of making large language models, specifically BERT, more computationally efficient through the use of sparsification and pruning techniques. It proposes the integration of these techniques with TVM compiler optimizations to enhance performance on commodity hardware without significantly compromising accuracy.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.5,\n    \"b13\": 0.6,\n    \"b9\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the computational cost of large language models and proposes methods for structured and unstructured sparsification of BERT models. It integrates these methods with the TVM compiler to optimize inference speed and performance on sparse neural networks. Key challenges include making large models accessible for small and large-scale research, and embedding them into low-cost, real-time human interactions.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b8\": 0.6,\n    \"b7\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.4,\n    \"b21\": 0.3\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of making large language models more accessible and computationally efficient through sparsification and pruning. The authors implement both unstructured and structured sparsification of BERT's attention weights and optimize performance using the TVM compiler. Key inspirations include methods for pruning neural networks, the lottery ticket hypothesis, and block sparsity optimizations.\",\n    \"Direct Inspiration\": {\n        \"b3\": 1.0,\n        \"b4\": 1.0,\n        \"b11\": 1.0,\n        \"b16\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8,\n        \"b8\": 0.9,\n        \"b9\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.7,\n        \"b7\": 0.7,\n        \"b13\": 0.6\n    }\n}\n```"], "5ff8816291e011c83266c074": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of high energy consumption and latency in deploying deep neural networks (DNNs) on conventional von Neumann hardware platforms, particularly focusing on resistive random access memory (ReRAM)-based processing-in-memory (PIM) paradigms. The authors propose a novel single-spiking data format to improve energy efficiency and introduce ReSiPE, a ReRAM-based Single-spiking PIM Engine, to support this data format and enhance overall performance.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the high energy consumption and latency overhead in deploying DNNs on conventional hardware, particularly with ReRAM-based PIM designs. The paper proposes a novel single-spiking data format and the ReSiPE engine to decouple power consumption from data representation, enhancing power efficiency and reducing area and energy overhead.\",\n  \"Direct Inspiration\": [\"b10\", \"b11\", \"b12\"],\n  \"Indirect Inspiration\": [\"b6\", \"b8\"],\n  \"Other Inspiration\": [\"b14\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the high energy consumption and latency overhead in deploying deep neural networks (DNNs) on conventional von Neumann hardware. It proposes a novel ReRAM-based PIM design called ReSiPE using a single-spiking data format to enhance power efficiency, area, and performance. The key contributions include recognizing the energy bottleneck in ReRAM-based PIMs, proposing the single-spiking data format for MVM computation, and evaluating the efficiency of ReSiPE with significant power reduction and efficiency improvement.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b8\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the high power consumption and inefficiency of conventional ReRAM-based Processing-in-Memory (PIM) designs for deep neural networks (DNNs). It proposes a novel single-spiking data format and introduces ReSiPE, a ReRAM-based Single-spiking PIM Engine, to enhance power efficiency and performance.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b10\", \"b12\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b8\", \"b11\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b6\", \"b14\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the high energy consumption and latency overhead in deploying deep neural networks (DNNs) on conventional von Neumann hardware platforms. The authors propose a novel ReRAM-based single-spiking processing-in-memory (PIM) engine named ReSiPE, which leverages a single-spiking data format to improve power efficiency, area, and performance.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b11\": 0.85,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b8\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b14\": 0.65\n  }\n}\n```"], "6062f2b691e0118c891f1987": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing memory-load latency in high-performance computing by proposing a novel memory hierarchy level predictor. This predictor enables direct cache level lookups, bypassing unnecessary levels and reducing access latency while maintaining low access costs and simplicity. The proposed solution demonstrates significant performance improvements and energy savings, particularly for graph analytics and scientific computing applications.\",\n  \"Direct Inspiration\": [\"b19\", \"b22\", \"b24\", \"b28\", \"b35\"],\n  \"Indirect Inspiration\": [\"b25\"],\n  \"Other Inspiration\": [\"b10\", \"b11\", \"b12\", \"b15\", \"b16\", \"b17\", \"b20\", \"b29\", \"b37\", \"b38\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high memory-load latency in deep cache hierarchies by proposing a novel level predictor that enables non-sequential cache lookups. This approach improves performance by predicting the cache level where a block resides, bypassing unnecessary lookups and reducing access latency and energy consumption.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b22\": 1,\n    \"b24\": 0.9,\n    \"b28\": 1,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing memory-load latency in high-performance computing by proposing a novel per-core level predictor. This predictor allows for non-sequential (parallel) cache lookups, bypassing certain cache levels to reduce latency. The proposed method incorporates a metadata cache (LocMap) and a history-based Popular Level Detector to predict the cache level where a memory block resides, thereby improving performance and energy efficiency.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b25\": 0.9,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b24\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is achieving low memory-load latency in high-performance computing, particularly in scenarios where traditional cache hierarchies introduce significant delays. The proposed solution is a novel per-core level predictor that enables direct lookup of the cache level where a memory block resides, thereby reducing access latency. The predictor consists of a metadata cache (LocMap) and a history-based Popular Level Detector that work together to bypass certain cache levels and perform partial parallel lookups, ultimately improving performance and reducing energy consumption.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b22\": 0.85,\n    \"b24\": 0.8,\n    \"b28\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.75,\n    \"b31\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.65,\n    \"b11\": 0.65,\n    \"b12\": 0.65,\n    \"b15\": 0.65,\n    \"b16\": 0.65,\n    \"b17\": 0.65,\n    \"b20\": 0.65,\n    \"b29\": 0.65,\n    \"b37\": 0.65,\n    \"b38\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high memory-load latency in deep cache hierarchies and proposes a novel level predictor that enables direct lookup of specific cache levels. This predictor aims to reduce access latency while maintaining low energy consumption and simplicity of the memory hierarchy.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b25\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.9,\n    \"b24\": 0.8,\n    \"b28\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b31\": 0.7\n  }\n}\n```"], "607fffde91e011772654f757": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of under-clustering and over-clustering in self-supervised learning for computer vision tasks. The proposed solution is a novel self-supervised learning framework using median triplet loss, which aims to balance the number of negative sample pairs to enhance learning efficiency and achieve state-of-the-art performance.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.9,\n    \"b5\": 0.9,\n    \"b18\": 0.9,\n    \"b6\": 0.9,\n    \"b11\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.8,\n    \"b47\": 0.8,\n    \"b1\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of under-clustering and over-clustering in self-supervised learning, particularly in contrastive learning. It proposes a novel self-supervised learning framework using a median triplet loss to balance the use of negative samples and improve learning efficiency.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b6\": 0.7,\n    \"b18\": 0.7,\n    \"b20\": 0.7,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b47\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of under-clustering and over-clustering in self-supervised learning methods, particularly in contrastive learning. The authors propose a novel self-supervised learning framework using median triplet loss to improve learning efficiency.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1,\n        \"b18\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.9,\n        \"b5\": 0.9,\n        \"b20\": 0.9,\n        \"b8\": 0.8,\n        \"b32\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.7,\n        \"b47\": 0.7,\n        \"b11\": 0.6,\n        \"b10\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenges of under-clustering and over-clustering in self-supervised learning, particularly in contrastive learning methods. The proposed median triplet loss framework aims to resolve these issues by balancing the number of negative samples used, thereby improving learning efficiency and achieving state-of-the-art performance on various benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1, \n    \"b32\": 1,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b18\": 0.8,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b47\": 0.6,\n    \"b1\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of under-clustering and over-clustering in existing self-supervised learning methods, particularly in contrastive learning. It proposes a novel self-supervised learning framework using a median triplet loss to improve learning efficiency.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b18\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7,\n    \"b39\": 0.7\n  }\n}\n```"], "609e5cf891e0113e7e2e0192": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in one-class collaborative filtering (OCCF) problems, particularly focusing on the sparsity of implicit feedback and the absence of negative labels. The proposed solution, BUIR, eliminates the need for negative sampling by using a student-teacher network to bootstrap user-item representations and employs stochastic data augmentation inspired by self-supervised learning techniques.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b4\": 0.8,\n    \"b8\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b26\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the one-class collaborative filtering (OCCF) problem by proposing a novel framework called BUIR that eliminates the need for negative sampling. BUIR leverages a student-teacher-like network, bootstrapping-based self-supervised learning, and stochastic data augmentation to effectively learn user-item interactions despite data sparsity.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b8\": 1,\n    \"b29\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b4\": 0.8,\n    \"b11\": 0.8,\n    \"b27\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b13\": 0.7,\n    \"b18\": 0.7,\n    \"b26\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of one-class collaborative filtering (OCCF) in recommender systems, particularly the sparsity of implicit feedback and the absence of negative labels. The proposed BUIR framework eliminates the need for negative sampling by using a student-teacher network structure and momentum-based updating to avoid collapsed solutions. The paper also introduces stochastic data augmentation to tackle data sparsity.\",\n  \"Direct Inspiration\": [\"b6\", \"b8\", \"b29\"],\n  \"Indirect Inspiration\": [\"b2\", \"b4\"],\n  \"Other Inspiration\": [\"b3\", \"b10\", \"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge in the paper is the extreme sparsity of implicit feedback and the lack of negative labels in one-class collaborative filtering (OCCF) problems. The paper proposes BUIR, a novel OCCF framework that eliminates the need for negative sampling. BUIR uses a student-teacher-like network with distinct online and target encoders, and employs a momentum-based moving average to update the target encoder. The framework also incorporates a stochastic data augmentation technique inspired by self-supervised learning to address data sparsity.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b8\": 0.9,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b10\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of one-class collaborative filtering (OCCF) problems, primarily the extreme sparseness of implicit feedback and the absence of negative labels. The proposed algorithm, BUIR, eliminates the need for negative sampling by using a student-teacher-like network with two distinct encoder networks (online and target encoders) and employs stochastic data augmentation inspired by self-supervised learning techniques.\",\n  \n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b29\": 1.0\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b2\": 0.7,\n    \"b4\": 0.7\n  },\n  \n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```"], "60e7a74091e011dcbc23af3d": ["```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the reliance on negative sampling techniques in existing Collaborative Filtering (CF) models, which introduces computational and memory costs and may lead to future positive samples being treated as negative. The proposed algorithm, SELFCF, leverages self-supervised learning (SSL) to learn user and item representations based solely on positive samples, circumventing the need for negative sampling. The framework introduces three novel output perturbation techniques (historical embedding, embedding dropout, and edge pruning) to generate different but invariant views of user/item embeddings for contrastive learning.\",\n    \"Direct Inspiration\": {\n        \"b5\": 1.0,\n        \"b9\": 1.0,\n        \"b10\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b14\": 0.7,\n        \"b15\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b30\": 0.6,\n        \"b31\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of negative sampling in collaborative filtering (CF) by proposing a self-supervised learning (SSL) framework called SELFCF. SELFCF learns user and item representations based solely on positive interactions using posterior perturbation techniques.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b9\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge addressed in the paper is the reliance on negative sampling techniques in Collaborative Filtering (CF) models, which introduces additional computation and memory costs and may still treat future positive samples as negative. The proposed solution, SELFCF, aims to learn latent representations of users/items solely based on positively observed interactions using self-supervised learning (SSL) with novel posterior embedding perturbation techniques.\",\n    \"Inspirations\": \"The main inspirations for this paper are recent advancements in self-supervised learning models like BYOL and SIMSIAM, as well as the only existing framework for CF without negative samples, BUIR.\"\n  },\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b9\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b13\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper introduces SELFCF, a self-supervised collaborative filtering framework that learns user and item representations solely from positive interactions. The key challenges addressed include the limitations of negative sampling in existing CF methods and the difficulty of applying SSL techniques from vision/NLP domains to recommender systems. The paper proposes novel methods for output perturbation to generate diverse but invariant user/item embeddings and compares the performance of SELFCF against existing models.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b9\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a novel self-supervised collaborative filtering framework, SELFCF, which learns latent representations of users/items solely based on positively observed interactions. The primary challenges addressed include the reliance of existing CF models on negative sampling and the inefficiencies in handling large-scale data. SELFCF introduces posterior embedding perturbation techniques to generate contrastive pairs, thereby eliminating the need for negative samples.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b9\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.6,\n    \"b14\": 0.5,\n    \"b15\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.4,\n    \"b30\": 0.4,\n    \"b31\": 0.4\n  }\n}\n```"], "5ff68878d4150a363cc74ab5": ["```json\n{\n  \"Summary\": \"The paper addresses two main challenges in survival prediction on whole-slide images (WSIs): how to better represent WSIs for hazard prediction and how to incorporate ranking order information in the survival prediction process. The proposed method, RankSurv, introduces a ranking-based survival prediction framework using hypergraph representation and a Bayesian-based ranking prediction strategy.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b13\": 0.8,\n    \"b21\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b9\": 0.75,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b19\": 0.65,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in survival prediction on gigapixel histopathological whole-slide images: representing the WSI for hazard prediction and incorporating ranking order information in the prediction process. The proposed algorithm, RankSurv, introduces a hypergraph representation for hazard prediction and a ranking-based prediction process using pairwise survival data.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b21\": 0.85,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b9\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b20\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in survival prediction on gigapixel histopathological whole-slide images (WSIs): representing the WSI for hazard prediction and incorporating ranking order information in the survival prediction process. The proposed RankSurv method introduces a hypergraph representation for hazard prediction and a ranking-based prediction process using pairwise survival data.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Better representation of WSI for hazard prediction\",\n      \"Incorporating ranking order information in survival prediction\"\n    ],\n    \"Proposed Algorithm\": [\n      \"Ranking-based survival prediction method (RankSurv)\",\n      \"Hypergraph representation for learning high-order correlations\",\n      \"Pairwise survival data for ranking-based prediction\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b6\": 1,\n    \"b9\": 1,\n    \"b21\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b18\": 0.9,\n    \"b19\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b10\": 0.7,\n    \"b12\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in survival prediction on gigapixel histopathological whole-slide images (WSIs): better representation of WSIs for hazard prediction and incorporation of ranking order information in the survival prediction process. The proposed method, RankSurv, introduces a hypergraph representation to learn high-order correlations among WSI patches and a ranking-based prediction process using pairwise survival data.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b6\": 0.9,\n    \"b9\": 0.9,\n    \"b21\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b8\": 0.7\n  }\n}\n```"], "61aed0dc5244ab9dcb3a77fd": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the limitations and practical challenges posed by the increasing size of Transformer models, which are unsustainable in terms of utilization, deployment, interpretation, and environmental impact. The paper proposes an innovative approach of 'external attention' to complement self-attention by integrating external knowledge sources, thereby reducing the dependency on large-scale models and improving transparency and explainability. The proposed method is validated using the CommonsenseQA task, showing significant improvements in performance.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1.0,\n    \"b19\": 0.9,\n    \"b43\": 0.8,\n    \"b50\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b31\": 0.7,\n    \"b46\": 0.6,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.5,\n    \"b10\": 0.5,\n    \"b22\": 0.4,\n    \"b9\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the unsustainability and limitations of scaling up Transformer-based models in terms of size and computational requirements. The proposed algorithm introduces an external attention mechanism to leverage external knowledge sources, reducing the dependency on large models while improving performance on NLP tasks such as commonsense reasoning.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1,\n        \"b44\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.9,\n        \"b19\": 0.9,\n        \"b31\": 0.9,\n        \"b41\": 0.8,\n        \"b43\": 0.9,\n        \"b50\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b46\": 0.7,\n        \"b22\": 0.7,\n        \"b9\": 0.7,\n        \"b15\": 0.6,\n        \"b48\": 0.6,\n        \"b26\": 0.6,\n        \"b37\": 0.6,\n        \"b10\": 0.6,\n        \"b47\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Transformer-based models, particularly their large size and the corresponding practical issues such as deployment and environmental impact. The authors propose an external attention mechanism that integrates external knowledge sources (e.g., knowledge graphs, dictionaries, training data) to enhance the model's reasoning capabilities without increasing the model's size. They demonstrate their approach using the CommonsenseQA task, significantly improving performance while maintaining model transparency and explainability.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b19\": 0.9,\n    \"b43\": 1,\n    \"b44\": 1,\n    \"b50\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b31\": 0.7,\n    \"b10\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.6,\n    \"b35\": 0.5,\n    \"b41\": 0.5,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of large Transformer models by proposing an external attention mechanism that incorporates external knowledge sources to improve performance on commonsense reasoning tasks. This approach reduces the reliance on large-scale models and enhances transparency and explainability.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b50\": 0.9,\n    \"b43\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b46\": 0.7,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Current Transformer-based models are unsustainable due to their large size and associated practical challenges.\",\n      \"Despite large models, their understanding and generation capabilities still lag behind humans.\",\n      \"Need for a model that can leverage external knowledge in a non-parametric and symbolic way to improve performance without scaling up.\"\n    ],\n    \"inspirations\": [\n      \"Inspired by human intelligence, which uses external resources like search engines and dictionaries.\",\n      \"Aimed to make Transformer models look outward by integrating external knowledge sources.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b4\", \"b43\", \"b44\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b12\", \"b19\", \"b31\", \"b35\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b9\", \"b22\", \"b41\", \"b50\"]\n  }\n}\n```"], "618e3f7c5244ab9dcb5fbb36": ["```json\n{\n  \"Summary\": \"The paper addresses the Author Name Disambiguation (AND) problem by proposing a novel framework called MFAND, which integrates multiple types of information (raw document features, fusion features, local and global structural information) to enhance disambiguation accuracy.\",\n  \"Direct Inspiration\": [\"b13\", \"b16\", \"b18\"],\n  \"Indirect Inspiration\": [\"b8\", \"b12\", \"b17\"],\n  \"Other Inspiration\": [\"b11\", \"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the Author Name Disambiguation (AND) problem by proposing a framework called MFAND, which integrates multiple features (raw document, fusion, local structural, and global structural information) using a novel encoder R3JG. The framework aims to enhance the generalization ability and precision of author name disambiguation through a combination of various similarity graphs and binary classification.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.95,\n    \"b12\": 0.90,\n    \"b13\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.80,\n    \"b16\": 0.75,\n    \"b17\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.65,\n    \"b19\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the Author Name Disambiguation (AND) problem by proposing a novel framework called MFAND, which integrates multiple features (raw document features, fusion features, local and global structural information) to enhance disambiguation accuracy. The key innovations include the construction of multiple similarity graphs, the use of a novel encoder (R3JG), and the application of pruning strategies to reduce noise.\",\n  \"Direct Inspiration\": [\"b11\", \"b13\", \"b19\"],\n  \"Indirect Inspiration\": [\"b8\", \"b12\", \"b18\"],\n  \"Other Inspiration\": [\"b15\", \"b16\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the Author Name Disambiguation (AND) problem by proposing a novel framework named MFAND, which integrates multiple types of information: raw document features, fusion features, local structural information, and global structural information. The framework includes a novel encoder called R3JG and a binary classification model for disambiguation. The proposed method aims to overcome limitations in existing approaches by considering all types of information comprehensively.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b13\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b15\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the Author Name Disambiguation (AND) problem by proposing a novel framework called MFAND, which integrates multiple types of information\u2014raw document features, fusion features, local structural information, and global structural information. The MFAND framework employs an encoder (R3JG) to integrate and reconstruct these features and uses a binary classification model for disambiguation.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b15\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```"], "60c31f669e795e9243fd1670": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in evaluating heterogeneous graph neural networks (HGNNs) due to inconsistent data processing and experimental settings. It finds that homogeneous GNNs like GAT can outperform HGNNs with proper inputs. The paper introduces the Heterogeneous Graph Benchmark (HGB) for consistent evaluation and proposes a new model, Simple-HGN, inspired by GAT, enhanced with type embedding, residual connections, and L2 normalization.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b11\": 0.7,\n    \"b35\": 0.7,\n    \"b42\": 0.6,\n    \"b44\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of evaluating and improving Heterogeneous Graph Neural Networks (HGNNs) by identifying their defects and proposing a unified benchmarking framework called Heterogeneous Graph Benchmark (HGB). Additionally, it introduces a simplified yet effective HGNN model called Simple-HGN based on GAT, enhanced by three techniques: learnable type embedding, residual connections, and L2 normalization.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.85,\n    \"b11\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.75,\n    \"b43\": 0.7,\n    \"b44\": 0.7,\n    \"b19\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the heterogeneity in graphs and the inconsistent performance and evaluation of HGNNs. The authors propose the Simple-HGN model, inspired by GAT, with enhancements such as learnable type embedding, residual connections, and L2 normalization. The paper also introduces the Heterogeneous Graph Benchmark (HGB) for robust and open HGNN developments.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b42\": 0.6,\n    \"b11\": 0.6,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of heterogeneous graph neural networks (HGNNs) in terms of performance and fair benchmarking. It highlights the issues with existing HGNNs, such as data leakage, high resource consumption, and the need for consistent validation techniques. The authors propose the Heterogeneous Graph Benchmark (HGB) and introduce a new model, Simple-HGN, inspired by the Graph Attention Network (GAT), to provide a fair and robust comparison platform for HGNNs.\",\n  \"Direct Inspiration\": [\"b31\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b11\", \"b16\", \"b19\", \"b35\", \"b42\", \"b43\", \"b44\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies key challenges in the field of heterogeneous graph neural networks (HGNNs), including the underestimation of simple homogeneous GNNs, issues with data leakage, and the unnecessary complexity of meta-paths in heterogeneous datasets. The authors propose a new model, Simple-HGN, inspired by GAT, to address these challenges and provide a unified benchmark for evaluating HGNNs.\",\n  \"Direct Inspiration\": [\"b20\", \"b31\"],\n  \"Indirect Inspiration\": [\"b35\", \"b42\", \"b43\", \"b11\"],\n  \"Other Inspiration\": [\"b17\", \"b19\"]\n}\n```"], "6153e0205244ab9dcb39a409": ["```json\n{\n  \"Summary\": \"The paper addresses three key challenges in existing prefetcher designs: reliance on a single program feature for prefetch prediction, lack of inherent system awareness, and inability to customize the prefetcher design for different workloads and system configurations. The proposed solution, Pythia, uses a reinforcement learning (RL) framework to formulate prefetching as an RL problem where the RL-agent learns to make accurate, timely, and system-aware prefetch decisions by interacting with the processor and memory subsystem.\",\n  \"Direct Inspiration\": {\n    \"b63\": 1,\n    \"b123\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.9,\n    \"b77\": 0.8,\n    \"b79\": 0.7,\n    \"b111\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b47\": 0.6,\n    \"b121\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses challenges in existing prefetchers, including the use of single program features, lack of system awareness, and inability to customize designs for different workloads. The proposed solution, Pythia, uses reinforcement learning to address these issues, aiming to provide accurate, timely, and system-aware prefetching.\",\n  \"Direct Inspiration\": {\n    \"b63\": 0.9,\n    \"b123\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b77\": 0.8,\n    \"b110\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b54\": 0.6,\n    \"b55\": 0.6,\n    \"b79\": 0.6,\n    \"b111\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three primary challenges in the design of hardware prefetchers: (1) reliance on a single program feature, (2) lack of inherent system awareness, and (3) inability to customize the prefetcher for diverse workloads and system configurations. The proposed solution, Pythia, is a reinforcement learning (RL)-based prefetcher that autonomously learns to make prefetch decisions using multiple program features and system-level feedback information, specifically memory bandwidth usage. The RL-based approach allows Pythia to adapt and optimize its prefetching strategy dynamically, providing higher performance improvements across various workloads and system configurations.\",\n  \"Direct Inspiration\": {\n    \"b63\": 1.0,\n    \"b123\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b77\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b107\": 0.6,\n    \"b91\": 0.6,\n    \"b117\": 0.6,\n    \"b118\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies three main challenges in prior prefetcher designs: reliance on a single program feature, lack of system awareness, and inability to customize the prefetcher design for different workloads and system configurations. The proposed solution, Pythia, introduces a reinforcement learning-based prefetching framework that learns using multiple program features and system-level feedback, and can be easily customized via configuration registers.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b77\": 0.85,\n    \"b63\": 0.8,\n    \"b123\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b54\": 0.7,\n    \"b55\": 0.7,\n    \"b72\": 0.7,\n    \"b111\": 0.75,\n    \"b79\": 0.75,\n    \"b121\": 0.75,\n    \"b110\": 0.7,\n    \"b14\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b91\": 0.6,\n    \"b117\": 0.6,\n    \"b118\": 0.6,\n    \"b104\": 0.55,\n    \"b60\": 0.55,\n    \"b113\": 0.55,\n    \"b115\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three main challenges in existing prefetchers: reliance on a single program feature for prediction, lack of inherent system awareness, and inability to customize the prefetcher design for different workloads. The proposed solution, Pythia, utilizes a reinforcement learning (RL) framework to make adaptive prefetch decisions. The RL-based prefetcher learns from system-level feedback and can be customized in silicon via configuration registers to optimize performance across various workloads and system configurations.\",\n  \"Direct Inspiration\": {\n    \"b63\": 1,\n    \"b123\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b77\": 0.8,\n    \"b110\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b79\": 0.6,\n    \"b121\": 0.6\n  }\n}\n```"], "62008da15aee126c0fbd19e0": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the oversmoothing problem and bottlenecks in traditional Graph Neural Networks (GNNs). The novel algorithm proposed, GraphCON (Graph-Coupled Oscillator Network), utilizes time-discretizations of a specific class of ordinary differential equations (ODEs) to model the dynamics of coupled oscillators in a graph structure. This approach mitigates oversmoothing by construction, provides flexibility in using any standard GNN layer as a coupling function, and achieves competitive performance across various graph learning tasks.\",\n  \"Direct Inspiration\": {\n    \"b54\": 1.0,\n    \"b39\": 0.9,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b9\": 0.8,\n    \"b19\": 0.8,\n    \"b31\": 0.7,\n    \"b57\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b28\": 0.6,\n    \"b16\": 0.5,\n    \"b36\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Oversmoothing and bottlenecks in traditional GNNs.\",\n    \"Inspirations\": \"Graph-coupled oscillators, differential equations from physics.\"\n  },\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b31\": 1.0,\n    \"b57\": 1.0,\n    \"b39\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b50\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes GraphCON, a novel GNN framework inspired by the dynamics of non-linear forced and damped oscillators coupled via the adjacency structure of the graph. The main contributions include flexibility in accommodating standard GNN layers, mathematical formulation to mitigate oversmoothing, and extensive empirical evaluation demonstrating competitive performance.\",\n  \"Direct Inspiration\": [\"b1\", \"b43\", \"b65\", \"b60\", \"b9\", \"b19\", \"b8\", \"b55\"],\n  \"Indirect Inspiration\": [\"b38\", \"b39\", \"b0\", \"b24\", \"b57\", \"b31\", \"b44\", \"b62\", \"b11\"],\n  \"Other Inspiration\": [\"b36\", \"b17\", \"b23\", \"b50\", \"b22\", \"b63\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying Graph Neural Networks (GNNs) to a variety of tasks while mitigating common issues like oversmoothing. The authors propose GraphCON, a novel framework inspired by the dynamics of coupled oscillators, to improve GNN expressiveness and stability.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b19\": 0.9,\n    \"b9\": 0.9,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.8,\n    \"b39\": 0.8,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b36\": 0.7,\n    \"b43\": 0.7,\n    \"b60\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces GraphCON, a novel framework for learning on graphs inspired by the dynamics of graph-coupled oscillators. It addresses the oversmoothing problem inherent to traditional GNNs by leveraging the dynamic behavior of a specific class of ODEs, which are discretized to form deep neural networks. The key contributions include the flexibility to incorporate any GNN layer, mathematical formulation of oversmoothing, and extensive empirical evaluations showing competitive performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b8\": 0.8,\n    \"b9\": 0.9,\n    \"b31\": 0.85,\n    \"b39\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.75,\n    \"b57\": 0.7,\n    \"b19\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.65,\n    \"b36\": 0.65,\n    \"b55\": 0.6,\n    \"b16\": 0.55\n  }\n}\n```"], "6164fcc15244ab9dcb24ce5d": ["```json\n{\n  \"Summary\": \"The paper addresses two main challenges in improving the state of the art of Open-Domain Question Answering (ODQA) systems: (1) the independent assumption among passages, and (2) the efficiency bottleneck of the FiD model. The authors propose KG-FiD, a novel solution leveraging knowledge graphs (KG) and Graph Neural Networks (GNNs) to establish relational dependencies among retrieved passages and to re-rank and prune these passages for each query.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b3\": 0.7,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges addressed in the paper are the independent assumption among passages in FiD and the high computational cost of FiD's generative reader. The proposed KG-FiD method leverages knowledge graphs (KG) to establish relational dependencies among retrieved passages and employs Graph Neural Networks (GNNs) for passage reranking and pruning to address these issues.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1.0,\n        \"b16\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.8,\n        \"b19\": 0.7,\n        \"b26\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.5,\n        \"b7\": 0.4\n    }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"Challenges\": [\n            \"Independent assumption among passages is not justified.\",\n            \"Efficiency bottleneck due to high computational cost.\"\n        ],\n        \"Inspirations\": [\n            \"Inspired by the success of the Fusion-in-Decoder (FiD) model.\"\n        ]\n    },\n    \"Direct Inspiration\": [\"b8\"],\n    \"Indirect Inspiration\": [\"b11\", \"b16\"],\n    \"Other Inspiration\": [\"b19\", \"b3\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in Open-Domain Question Answering (ODQA) by improving the Fusion-in-Decoder (FiD) model. The primary challenges highlighted are: 1) the independent assumption among retrieved passages that fails to leverage semantic relationships, and 2) high computational costs associated with the FiD model. The proposed KG-FiD model uses a knowledge graph (KG) and Graph Neural Networks (GNNs) for passage re-ranking and pruning to address these issues, significantly improving efficiency while maintaining or improving performance.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.75,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in the Fusion-in-Decoder (FiD) model for Open-Domain Question Answering (ODQA): (1) lack of leveraging semantic relationships among retrieved passages and (2) high computational cost. The proposed solution, KG-FiD, integrates knowledge graphs (KG) and Graph Neural Networks (GNNs) to rerank and prune retrieved passages, aiming to improve both accuracy and efficiency.\",\n  \"Direct Inspiration\": [\"b8\", \"b16\"],\n  \"Indirect Inspiration\": [\"b11\", \"b19\"],\n  \"Other Inspiration\": [\"b3\", \"b7\", \"b22\", \"b26\"]\n}\n```"], "5ff8818491e011c83266c730": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving memory access latency in modern processors for applications with large memory footprints and poor locality. It proposes a novel software-assisted record-and-replay (RnR) hardware prefetcher that allows programmers to control when and which data to prefetch, achieving high miss coverage and prefetch accuracy.\",\n  \"Direct Inspiration\": [\"b8\", \"b23\", \"b36\", \"b50\", \"b51\", \"b57\"],\n  \"Indirect Inspiration\": [\"b18\", \"b26\", \"b29\", \"b37\", \"b42\", \"b46\", \"b55\"],\n  \"Other Inspiration\": [\"b3\", \"b5\", \"b6\", \"b7\", \"b11\", \"b19\", \"b28\", \"b34\", \"b39\", \"b45\", \"b49\", \"b53\", \"b56\", \"b58\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of bridging the processor-memory frequency gap, particularly for applications with large memory footprints and poor locality. It proposes a novel software-assisted record-and-replay (RnR) hardware prefetcher, which records memory access patterns and uses this information to improve prefetching accuracy and timing. The inspirations for this work come from observations on repetitive irregular memory access patterns in applications like sparse matrix-vector multiplication and iterative graph algorithms.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.9,\n    \"b50\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b23\": 0.6,\n    \"b51\": 0.5,\n    \"b57\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.4,\n    \"b46\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting and prefetching data for applications with large memory footprints and poor locality, proposing a novel software-assisted record-and-replay (RnR) hardware prefetcher. The RnR prefetcher allows programmers to control when and which data to prefetch, achieving high accuracy and coverage for applications with long repeating irregular memory access patterns.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b8\": 1.0,\n    \"b50\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b36\": 0.8,\n    \"b51\": 0.8,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.7,\n    \"b55\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently prefetching data for applications with large memory footprints and poor locality, particularly those with repetitive irregular memory access patterns. The proposed solution is a novel software-assisted record-and-replay (RnR) hardware prefetcher that allows programmers to control when and which data to prefetch, thereby achieving high miss coverage and prefetching accuracy.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.95,\n    \"b23\": 0.9,\n    \"b36\": 0.9,\n    \"b50\": 0.9,\n    \"b51\": 0.85,\n    \"b57\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b46\": 0.75\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of prefetching for applications with large memory footprints and poor locality, particularly focusing on repetitive irregular memory access patterns. It proposes a novel software-assisted record-and-replay (RnR) hardware prefetcher to accurately identify when and which data to prefetch, leveraging software knowledge to improve prediction accuracy and timeliness.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b36\", \"b23\", \"b57\", \"b50\", \"b51\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b7\", \"b8\", \"b45\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b46\", \"b55\"]\n  }\n}\n```"], "611c96255244ab9dcb65c2c0": ["```json\n{\n    \"Summary\": \"The paper proposes Task-aligned One-stage Object Detection (TOOD) to address the misalignment between classification and localization in one-stage object detectors. The main contributions include the design of a Task-aligned head (T-head) to enhance interaction between tasks and Task Alignment Learning (TAL) to explicitly align the tasks during training.\",\n    \"Direct Inspiration\": {\n        \"b28\": 1.0,\n        \"b25\": 0.9,\n        \"b7\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.7,\n        \"b8\": 0.6,\n        \"b29\": 0.6,\n        \"b14\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in object detection related to the misalignment between classification and localization tasks in one-stage object detectors. The proposed solution is a Task-aligned One-stage Object Detection (TOOD) framework, which includes a new head structure (T-head) and Task Alignment Learning (TAL). The T-head enhances interaction between tasks, while TAL aligns the tasks by a sample assignment scheme and task-aligned loss.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0,\n    \"b25\": 0.9,\n    \"b12\": 0.8,\n    \"b7\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.6,\n    \"b14\": 0.6,\n    \"b8\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.4,\n    \"b9\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in object detection related to the misalignment between object classification and localization tasks. It proposes a Task-aligned One-stage Object Detection (TOOD) method, incorporating a new head structure (T-head) and a Task Alignment Learning (TAL) approach to enhance the interaction and alignment between these tasks.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b12\": 0.9,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b7\": 0.6,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Misalignment between classification and localization tasks in one-stage object detectors.\",\n      \"Independence of classification and localization leading to inconsistency in predictions.\",\n      \"Task-agnostic sample assignment causing inaccurate predictions.\"\n    ],\n    \"inspirations\": [\n      \"Inspired by the limitations of recent one-stage detectors like ATSS and FCOS.\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b25\",\n    \"b28\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b3\",\n    \"b9\",\n    \"b20\",\n    \"b21\"\n  ],\n  \"Other Inspiration\": [\n    \"b12\",\n    \"b7\",\n    \"b8\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the misalignment between object classification and localization in one-stage object detectors. The proposed algorithm, Task-aligned One-stage Object Detection (TOOD), introduces a new head structure (T-head) and a Task Alignment Learning (TAL) approach to enhance the interaction between the two tasks and align their predictions more accurately.\",\n    \"Direct Inspiration\": {\n        \"b28\": 1,\n        \"b25\": 0.9,\n        \"b12\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.7,\n        \"b9\": 0.7,\n        \"b20\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.6,\n        \"b8\": 0.5\n    }\n}\n```"], "6173f1b891e0118698c0465b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of conversational group detection by leveraging a Graph Neural Network (GNN) to predict pairwise affinities and clustering people into conversational groups using the Dominant Sets (DS) algorithm. The GNN architecture allows for reasoning about node and edge information, reducing feature engineering and leveraging relational features.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b19\": 0.6,\n    \"b31\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of conversational group detection by leveraging human proxemics and conversational formations through a novel approach using Graph Neural Networks (GNN). The proposed method aims to predict pairwise affinities to cluster people into conversational groups, reducing feature engineering and explicitly leveraging relational features. The main contributions include the novel use of GNN for group detection, experiments on varied datasets, and the open-sourcing of the code for reproducibility.\",\n    \"Direct Inspiration\": [\"b29\"],\n    \"Indirect Inspiration\": [\"b1\", \"b14\", \"b22\"],\n    \"Other Inspiration\": [\"b24\", \"b39\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of conversational group detection by leveraging a Graph Neural Network (GNN) to predict pairwise affinities and cluster people into conversational groups. Key inspirations and advancements include the utilization of human proxemics, F-Formations, and a more general message-passing architecture over previous Deep Set architectures.\",\n  \"Direct Inspiration\": [\"b29\"],\n  \"Indirect Inspiration\": [\"b1\", \"b22\", \"b14\"],\n  \"Other Inspiration\": [\"b24\", \"b39\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of conversational group detection using a Graph Neural Network (GNN) to predict pairwise affinities and cluster people into conversational groups. The proposed method aims to leverage relational features more explicitly and reduce feature engineering compared to previous methods.\",\n  \"Direct Inspiration\": [\"b29\"],\n  \"Indirect Inspiration\": [\"b1\", \"b22\"],\n  \"Other Inspiration\": [\"b14\", \"b24\", \"b39\", \"b17\", \"b4\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of conversational group detection by proposing a novel approach using a Graph Neural Network (GNN) to predict pairwise affinities and cluster people into conversational groups. The paper is primarily inspired by previous work on human proxemics and conversational formations, particularly leveraging a GNN for its architecture. Key contributions include the introduction of a message-passing GNN architecture, the use of datasets with varied input features to demonstrate efficacy, and the open-sourcing of the code for reproducibility.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b4\": 0.7\n  }\n}\n```"], "60b9a1d7e4510cd7c8f70e7c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning user preferences from implicit feedback in collaborative filtering (CF) systems. It critiques the use of fixed user-item graphs in current neural graph models and proposes an enhanced graph learning method that iteratively learns both the graph structure and node embeddings. The proposed model aims to better differentiate between true negative and false negative interactions, leading to improved recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b2\": 0.85,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b11\": 0.7,\n    \"b25\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b30\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to learn users' preferences from implicit feedback-based collaborative filtering (CF) while addressing the issue of noisy and missing false negative interactions in the user-item bipartite graph. The authors propose an Enhanced Graph Learning Network (EGLN) that includes an iterative process of residual graph learning and node embedding learning to adaptively enhance the user-item graph structure for better CF performance. Key inspirations include neural graph collaborative filtering models, mutual information maximization techniques, and adaptive graph learning strategies.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b11\": 0.85,\n    \"b31\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b15\": 0.75,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning users' preferences from implicit feedback in collaborative filtering (CF) by proposing an enhanced graph learning approach. This approach involves iteratively learning user-item embeddings and an adaptive user-item graph structure to better capture the mixed nature of positive and negative feedback from unobserved behaviors. The proposed model aims to improve the performance of CF models by addressing the limitations of fixed graph structures and leveraging mutual information maximization between local and global graph properties.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b31\": 0.90,\n    \"b13\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.80,\n    \"b11\": 0.75,\n    \"b32\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.65,\n    \"b30\": 0.60\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of learning user preferences from implicit feedback within collaborative filtering (CF) systems. The authors propose an Enhanced Graph Learning Network (EGLN) approach, which involves an adaptive user-item graph structure that iteratively learns from user and item embeddings. This approach aims to overcome the limitations of fixed graph structures that do not differentiate between true negative and false negative feedback.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b31\": 0.8,\n        \"b3\": 0.7,\n        \"b11\": 0.7,\n        \"b32\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.6,\n        \"b25\": 0.6,\n        \"b30\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b17\": 0.4,\n        \"b18\": 0.4,\n        \"b37\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning user preferences from implicit feedback for collaborative filtering (CF) in recommendation systems. The authors propose an Enhanced Graph Learning Network (EGLN) that iteratively learns an adaptive user-item graph structure to better capture the difference between true negative and false negative interactions. The novel approach combines residual graph learning with node embedding learning to improve recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b3\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b32\": 0.6,\n    \"b37\": 0.5\n  }\n}\n```"], "6082aaa4e4510cd7c85b2d7f": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of accelerating data query operations in data centers, focusing on latency, generality, and hardware complexity. It proposes QEI, an accelerator design that uses Configurable Finite Automata (CFA) to abstract query operations, enabling efficient processing of different data structures with shared hardware resources. The integration scheme tightly couples the accelerator to the CPU core while optimizing memory-level parallelism.\",\n    \"Direct Inspiration\": {\n        \"b40\": 1,\n        \"b44\": 1,\n        \"b47\": 1,\n        \"b78\": 1,\n        \"b80\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b16\": 0.8,\n        \"b24\": 0.8,\n        \"b53\": 0.8,\n        \"b61\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.7,\n        \"b25\": 0.7,\n        \"b36\": 0.7,\n        \"b43\": 0.7,\n        \"b68\": 0.7,\n        \"b71\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing a versatile, efficient, and balanced accelerator for data query operations in data centers, focusing on reducing latency, improving generality, and minimizing design complexity and cost. The proposed solution, QEI, abstracts various data query operations using configurable finite automata (CFA) and integrates the accelerator into the CPU with a novel scheme to balance throughput and latency.\",\n  \"Direct Inspiration\": [\"b6\", \"b40\"],\n  \"Indirect Inspiration\": [\"b44\", \"b47\", \"b78\", \"b80\"],\n  \"Other Inspiration\": [\"b16\", \"b61\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges related to data query operations in data centers, aiming to design a versatile, efficient, low-latency, and high-throughput accelerator. It proposes QEI, an accelerator that uses a configurable finite automaton (CFA) to process various data query operations. The primary components of QEI include a Query State Table, a CFA Execution Engine, and a Data Processing Unit, integrated into the CPU to balance throughput, latency, and design complexity.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1,\n    \"b44\": 0.9,\n    \"b47\": 0.8,\n    \"b78\": 0.85,\n    \"b80\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b61\": 0.75,\n    \"b68\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b14\": 0.6,\n    \"b71\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving data query operations in data centers by proposing QEI, a versatile and efficient accelerator. The main challenges addressed are generality, latency, and design complexity of query accelerators. QEI abstracts query operations using configurable finite automata (CFA) and integrates tightly with the CPU to balance throughput, latency, and design complexity.\",\n  \"Direct Inspiration\": [\"b40\", \"b44\", \"b78\", \"b80\"],\n  \"Indirect Inspiration\": [\"b16\", \"b47\", \"b61\"],\n  \"Other Inspiration\": [\"b6\", \"b14\", \"b71\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include generality, latency, design complexity, and cost of accelerators for data query operations in data center workloads. The paper proposes QEI, an efficient and versatile accelerator design based on a configurable finite automaton (CFA) model to address these challenges.\",\n  \"Direct Inspiration\": {\n    \"b40\": 0.95,\n    \"b78\": 0.9,\n    \"b47\": 0.85,\n    \"b44\": 0.85,\n    \"b80\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.75,\n    \"b61\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.65,\n    \"b27\": 0.65,\n    \"b48\": 0.65,\n    \"b65\": 0.65,\n    \"b8\": 0.6,\n    \"b16\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```"], "6142b6175244ab9dcbc98149": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the tightly coupled nature of existing frameworks for evaluating algorithms on spatial accelerators, which limits flexibility and reusability. The authors propose a unified ecosystem, Union, with modular abstractions to address this issue. This framework integrates various mappers and cost models to evaluate tensor operations efficiently across different domains, leveraging the MLIR infrastructure.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b10\": 1.0,\n    \"b12\": 0.9,\n    \"b20\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b11\": 0.7,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b18\": 0.4,\n    \"b34\": 0.4,\n    \"b36\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of evaluating tensor operations on spatial accelerators by introducing a unified framework called Union. This framework leverages MLIR infrastructure to create unified abstractions for workload, mapping, and hardware, allowing for interoperability and reusability among different high-level languages, mappers, and cost models. This reduces the burden on computer architects, compiler researchers, and algorithm designers by providing a modular and flexible approach to exploring new algorithms and accelerator designs.\",\n    \"Direct Inspiration\": {},\n    \"Indirect Inspiration\": {\n        \"b9\": 0.9,\n        \"b10\": 0.85,\n        \"b21\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.75,\n        \"b20\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of evaluating new algorithms and their mappings on spatial accelerator architectures by developing a modular framework with unified abstractions for workload, mapping, and hardware. The key inspirations include the need for a flexible, composable, and reusable ecosystem, and leveraging the MLIR infrastructure to integrate with different high-level languages or frameworks.\",\n  \"Direct Inspiration\": [\"b9\", \"b10\"],\n  \"Indirect Inspiration\": [\"b3\", \"b6\", \"b21\"],\n  \"Other Inspiration\": [\"b0\", \"b1\", \"b2\", \"b7\", \"b8\", \"b11\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of evaluating new algorithms on spatial accelerators, focusing on the need for a flexible, composable, and reusable framework. It introduces 'Union', a unified ecosystem leveraging unified abstractions at every level to integrate different mappers, cost models, and high-level frameworks for tensor computations.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b18\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of evaluating new algorithms and their mappings on spatial accelerators by proposing a unified framework called Union. This framework integrates various high-level languages, mappers, and cost models using unified abstractions to enhance interoperability and reusability. The primary contributions include a plug-and-play ecosystem, new unified abstractions for tensor operations, operation-level/loop-level analysis, and showcasing how the framework can handle various workloads on diverse accelerators.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b12\": 0.7,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```"], "5ff8818491e011c83266c6e7": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of TLB misses and high address translation overheads in modern CPUs. It proposes a novel predictive replacement policy for TLBs called Control-flow History Reuse Prediction (CHIRP), which improves TLB performance by better correlating to TLB reuse behavior using a specially designed signature. The paper aims to reduce L2 TLB misses and enhance overall system performance without increasing TLB size.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b54\": 0.9,\n    \"b2\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b50\": 0.7,\n    \"b51\": 0.6,\n    \"b37\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b55\": 0.8,\n    \"b66\": 0.7,\n    \"b67\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the high overhead and performance degradation caused by TLB misses in modern computing systems. The paper proposes the Control-flow History Reuse Prediction (CHIRP) mechanism, a novel predictive replacement policy designed specifically for TLBs. CHIRP improves prediction accuracy and performance by better correlating to TLB reuse behavior, as opposed to traditional cache-oriented predictive policies that do not adapt well to TLBs.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b1\": 1,\n    \"b2\": 1,\n    \"b54\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b50\": 0.5,\n    \"b51\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving TLB performance without increasing TLB size. It proposes a novel predictive replacement policy, Control-flow History Reuse Prediction (CHIRP), which better correlates with TLB reuse behavior. The study explores adapting cache replacement policies to TLBs, identifies unique challenges with TLB accesses, and presents a solution that improves prediction accuracy and reduces L2 TLB misses by 28.21% on average over LRU.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.95,\n        \"b2\": 0.9,\n        \"b54\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.85,\n        \"b55\": 0.85\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.75,\n        \"b35\": 0.75,\n        \"b37\": 0.75\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of virtual-to-physical address translation being expensive, particularly due to TLB misses, which significantly affect system performance. The authors propose a novel mechanism, Control-flow History Reuse Prediction (CHIRP), to improve TLB performance by predicting reuse behavior more accurately using a combination of program features and efficient indexing techniques.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1,\n    \"b54\": 1,\n    \"b55\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b35\": 0.7,\n    \"b37\": 0.7,\n    \"b38\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b50\": 0.5,\n    \"b51\": 0.5,\n    \"b52\": 0.5,\n    \"b56\": 0.5,\n    \"b57\": 0.5,\n    \"b58\": 0.5,\n    \"b59\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving TLB (Translation Lookaside Buffer) performance without increasing its size. The proposed solution is a novel predictive replacement policy called Control-flow History Reuse Prediction (CHIRP), which builds on previous work in cache replacement policies and adapts them for TLBs. The key contributions include the implementation and adaptation of policies from data caches and branch target buffers to the TLB, the introduction of a new predictive replacement policy tailored for TLBs, and a detailed study of TLB behavior and reuse prediction mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b54\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b2\": 0.7,\n    \"b55\": 0.7,\n    \"b67\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b50\": 0.5,\n    \"b56\": 0.5\n  }\n}\n```"], "60dd2a4191e011cc85cbcc0b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving sample quality of diffusion models on high-entropy data, particularly on class-conditional ImageNet generation. It introduces cascaded diffusion models (CDM) and conditioning augmentation techniques to enhance sample fidelity without relying on classifiers.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b9\": 1,\n    \"b15\": 1,\n    \"b20\": 1,\n    \"b23\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b17\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.8,\n    \"b14\": 0.8,\n    \"b26\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Improve the sample quality of diffusion models on difficult high-entropy data without using extra image classifiers.\",\n    \"inspirations\": \"Use cascading pipelines and conditioning augmentation to enhance sample fidelity in class-conditional ImageNet generation.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b9\", \"b15\", \"b20\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b23\", \"b17\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b1\", \"b3\", \"b14\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are improving the sample quality of diffusion models on high-entropy data and developing techniques to enhance class-conditional ImageNet sample quality. The novel methods introduced include the use of cascading pipelines and conditioning augmentation to improve the performance of diffusion models.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b20\": 0.9,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b14\": 0.4,\n    \"b23\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the sample quality of diffusion models on high-entropy data, specifically for class-conditional ImageNet generation. It introduces the use of cascades and conditioning augmentation to enhance high-resolution image generation, achieving results superior to BigGAN-Deep.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.95,\n    \"b15\": 0.9,\n    \"b20\": 0.85,\n    \"b23\": 0.92\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b3\": 0.7,\n    \"b17\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b25\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper aims to improve the sample quality of diffusion models on high-entropy data by using cascaded diffusion pipelines and conditioning augmentation. The main contributions include demonstrating the effectiveness of cascading for high-resolution data and introducing conditioning augmentation to alleviate compounding error.\",\n  \"Direct Inspiration\": [\n    \"b9\",\n    \"b15\",\n    \"b20\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b1\",\n    \"b3\",\n    \"b17\",\n    \"b23\"\n  ],\n  \"Other Inspiration\": [\n    \"b14\",\n    \"b26\"\n  ]\n}\n```"], "61aed0d85244ab9dcb3a70c5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of synthesizing audio-driven photo-realistic portraits with clear visual quality, natural head motions, and synced lip movements. It highlights the difficulty in generating natural and synchronous lip movements for one-shot speakers due to the varied speech styles of different individuals. The proposed solution involves learning a consistent speaking style from a single speaker and then animating videos for arbitrary speakers using a speaker-independent Audio-Visual Correlation Transformer (AVCT). Key contributions include the development of AVCT, the use of phonemes to represent audio signals, and a relative motion transfer module to reduce motion gaps between different identities.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b3\": 0.8,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b47\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating photo-realistic talking face videos with natural head motions and accurate lip synchronization from unseen speakers and voices. The proposed method involves learning a consistent speech style from a specific speaker and transferring it to arbitrary speakers. The key contributions include an Audio-Visual Correlation Transformer (AVCT) to obtain keypoint-based dense motion fields from audio signals, the use of phonemes to represent audio signals, and a relative motion transfer module to reduce the motion gap between different identities.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7,\n    \"b29\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge of the paper is to synthesize natural and synchronous lip movements for one-shot speakers while maintaining visual quality and natural head motions. The algorithm proposed involves a novel one-shot talking face generation framework that learns a consistent speaking style from a single speaker and then animates videos of arbitrary speakers. The key components are the Audio-Visual Correlation Transformer (AVCT), use of phonemes to represent audio signals, and a relative motion transfer module.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b31\": 0.70,\n    \"b45\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.55,\n    \"b5\": 0.50\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of synthesizing audio-driven, photo-realistic talking face videos with synchronized lip movements and natural head motions. The proposed method learns a consistent speaking style from a specific speaker and then generalizes it to arbitrary speakers, employing a speaker-independent Audio-Visual Correlation Transformer (AVCT) and a relative motion transfer module to achieve this.\",\n  \"Direct Inspiration\": [\"b24\"],\n  \"Indirect Inspiration\": [\"b31\"],\n  \"Other Inspiration\": [\"b3\", \"b20\", \"b42\", \"b45\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in synthesizing audio-driven photo-realistic portraits, focusing on achieving natural head motions, synced lip movements, and clear visual quality. The proposed method involves a new one-shot talking face generation framework that learns a consistent speaking style from a single speaker and transfers it to arbitrary speakers using an Audio-Visual Correlation Transformer (AVCT). The AVCT utilizes phonemes and keypoint-based motion fields to generate dense motion fields and vivid lip movements.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.75,\n    \"b3\": 0.7,\n    \"b45\": 0.7,\n    \"b42\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```"], "61baae6d5244ab9dcb6463e3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of closed information extraction (cIE) by proposing GenIE, an autoregressive end-to-end model that scales to large numbers of entities and relations. It leverages a sequence-to-sequence BART model and introduces a novel bi-level constrained generation strategy to ensure valid triplet generation, aiming to overcome limitations of previous pipeline-based approaches and error propagation.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b34\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.9,\n    \"b49\": 0.8,\n    \"b54\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of closed information extraction (cIE) from unstructured text using an autoregressive end-to-end model called GenIE. The proposed model leverages a BART transformer architecture and a novel bi-level constrained generation strategy to ensure the generation of valid (subject, relation, object) triplets. The primary challenges involve scaling to a large number of entities and relations, and mitigating error propagation in traditional pipeline methods.\",\n  \"Direct Inspiration\": [\n    \"b11\",\n    \"b27\",\n    \"b34\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b20\",\n    \"b54\"\n  ],\n  \"Other Inspiration\": [\n    \"b28\",\n    \"b13\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of extracting structured semantic information from unstructured texts, specifically focusing on closed information extraction (cIE) using a transformer encoder-decoder model called GenIE. The primary challenges include handling large schemas with many entities and relations, avoiding error propagation in pipeline architectures, and maintaining valid entity and relation identifiers during generation. The proposed solution is an autoregressive end-to-end model employing constrained beam search to ensure valid triplet generation.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b14\": 0.9,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.8,\n    \"b54\": 0.8,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b48\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extracting structured semantic information from unstructured texts, specifically focusing on closed information extraction (cIE). The proposed solution, GenIE, is an autoregressive end-to-end model that leverages a sequence-to-sequence BART model and a novel bi-level constrained generation strategy to ensure only valid triplets are generated. The method seeks to overcome limitations of traditional pipeline architectures and existing end-to-end methods by scaling to large knowledge base schemas and addressing data imbalances.\",\n  \"Direct Inspiration\": [\"b34\", \"b11\", \"b27\"],\n  \"Indirect Inspiration\": [\"b54\", \"b46\", \"b37\"],\n  \"Other Inspiration\": [\"b20\", \"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extracting structured semantic information from unstructured text, specifically focusing on closed information extraction (cIE). The authors propose GenIE, an autoregressive end-to-end model that uses a BART transformer architecture and a novel bi-level constrained generation strategy. The model aims to scale cIE to handle realistic KB schemas with millions of entities and relations, outperforming traditional pipeline methods that suffer from error accumulation.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b27\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.7,\n    \"b49\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b54\": 0.5,\n    \"b20\": 0.4,\n    \"b13\": 0.3\n  }\n}\n```"], "615d13fe5244ab9dcb637c13": ["```json\n{\n  \"Summary\": \"The paper introduces Autoregressive Diffusion Models (ARDMs), which combine order-agnostic autoregressive models (OA-ARMs) and discrete diffusion models to generate variables in any order with fewer steps. ARDMs are demonstrated to be efficient in modeling steps and capable of parallelized inference and generation processes. The key challenges addressed include the architectural constraints of ARMs, the inefficiency of sampling in ARMs, and the need for large numbers of network calls in discrete diffusion models.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b46\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b20\": 0.8,\n    \"b24\": 0.7,\n    \"b42\": 0.9,\n    \"b41\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the need for a more efficient generative model that does not require a pre-specified generation order and can reduce the number of steps required for sampling and inference. The proposed algorithm, Autoregressive Diffusion Models (ARDMs), aims to address these challenges by combining the strengths of autoregressive models and discrete diffusion models. Key contributions include the introduction of ARDMs, derivation of their equivalence to absorbing diffusion models, and showing that ARDMs can have parallelized inference and generation processes.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b41\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.8,\n    \"b42\": 0.75,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.65,\n    \"b33\": 0.6,\n    \"b20\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces Autoregressive Diffusion Models (ARDMs), which combine order-agnostic autoregressive models and discrete diffusion models, aiming to address the limitations of ARMs and discrete diffusion models in flexibility and efficiency. The ARDMs can generate data in any order and are capable of parallelized inference and generation processes.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b41\": 1,\n    \"b42\": 1,\n    \"b46\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.9,\n    \"b20\": 0.8,\n    \"b24\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b33\": 0.7,\n    \"b52\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces Autoregressive Diffusion Models (ARDMs), which combine the strengths of autoregressive models (ARMs) and discrete diffusion models to generate data in any order. The key challenges addressed include the limitations of ARMs in requiring a pre-specified generation order and the inefficiency of discrete diffusion models due to a large number of steps required for good performance. The novel contributions include the ability to generate data in arbitrary orders, parallelized inference and generation processes, and competitive performance in lossless compression tasks.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b46\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.75,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b24\": 0.55,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces Autoregressive Diffusion Models (ARDMs), which generalize order agnostic autoregressive models and discrete diffusion models. ARDMs generate variables in any order and can upscale variables. The paper highlights ARDMs' efficiency in modeling steps, parallelized inference and generation, and competitive performance in lossless compression.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b46\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.8,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b24\": 0.5,\n    \"b33\": 0.5,\n    \"b41\": 0.6\n  }\n}\n```"], "603e03f291e01129ef28fb7d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively capturing both global and local information in images for visual recognition tasks using transformer architectures. The authors propose a novel Transformer-iN-Transformer (TNT) model that enhances feature representation by dividing input images into patches and sub-patches, allowing for fine-grained visual information extraction.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b4\": 0.8,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b1\": 0.6,\n    \"b3\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The paper addresses the challenge of capturing both global and local information in images using transformer architectures, which are mainly inspired by their success in NLP tasks but need adaptation for visual tasks due to the high diversity and semantic gap in images.\",\n    \"Inspirations\": \"The proposed Transformer-iN-Transformer (TNT) model builds on the concept of dividing images into patches and sub-patches to better capture visual information with fine granularity. The architecture is inspired by existing transformer-based models like ViT and aims to enhance feature representation by embedding a sub-transformer within the main transformer model.\"\n  },\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b4\": 0.9,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b30\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.75,\n    \"b34\": 0.7,\n    \"b36\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing feature representation ability in visual transformers by proposing a novel Transformer-iN-Transformer (TNT) architecture. This new architecture divides input images into patches and sub-patches, embedding a sub-transformer to capture details and relationships at a finer granularity level, aiming for better accuracy and computational efficiency.\",\n  \"Direct Inspiration\": [\n    \"b2\",\n    \"b4\",\n    \"b8\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b3\",\n    \"b30\"\n  ],\n  \"Other Inspiration\": [\n    \"b1\",\n    \"b7\",\n    \"b36\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the effective representation of natural images for visual recognition using transformer architectures. The authors propose a novel Transformer-iN-Transformer (TNT) architecture to improve feature representation by dividing images into patches and sub-patches, thereby capturing both global and local information.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b30\": 0.9,\n    \"b2\": 0.85,\n    \"b4\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.75,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.65,\n    \"b7\": 0.6,\n    \"b1\": 0.55\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces the Transformer-iN-Transformer (TNT) architecture for visual recognition, addressing the challenge of capturing both global and local information in images. Inspired by the success of transformers in NLP and their application in visual tasks, the TNT model aims to enhance feature representation by dividing images into patches (visual sentences) and sub-patches (visual words). The architecture incorporates an inner transformer to model relationships among visual words and an outer transformer for sentence embeddings, achieving better accuracy and efficiency than existing transformer models.\",\n    \"Direct Inspiration\": {\n        \"b8\": 0.9,\n        \"b2\": 0.8,\n        \"b4\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b7\": 0.7,\n        \"b36\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b30\": 0.6\n    }\n}\n```"], "61397b715244ab9dcb323b24": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Effectively encode nodes into informative embeddings for downstream tasks in graph representation learning, addressing semantic errors and multiplexity in real-world graphs.\",\n    \"Inspirations\": \"Inspired by contrastive learning and semantic clustering methods to simultaneously capture node-level and cluster-level information, and the use of alignment regularization to handle multiplex heterogeneous graphs.\"\n  },\n  \"Direct Inspiration\": [\"b26\"],\n  \"Indirect Inspiration\": [\"b17\", \"b38\"],\n  \"Other Inspiration\": [\"b59\", \"b75\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively encoding nodes in graph representation learning. It proposes a novel X-GOAL framework, which includes the GOAL framework for capturing node-level and cluster-level information in graphs, and an alignment regularization for combining embeddings from multiple graph layers without extra neural network modules. The method aims to reduce semantic errors and improve the mutual information across layers, leading to better performance in various downstream tasks.\",\n  \"Direct Inspiration\": [\"b26\"],\n  \"Indirect Inspiration\": [\"b17\", \"b38\"],\n  \"Other Inspiration\": [\"b59\", \"b22\", \"b41\", \"b11\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively encoding nodes in graph representation learning by proposing the GOAL and X-GOAL frameworks. These frameworks capture both node-level and cluster-level information and introduce a nimble alignment regularization to combine embeddings across different layers without extra neural network modules. The key contributions include reducing semantic errors, maximizing mutual information, and achieving superior performance on various real-world datasets compared to state-of-the-art methods.\",\n  \"Direct Inspiration\": [\"b26\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b17\", \"b22\", \"b38\", \"b59\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively encoding nodes in graph representation learning by proposing a novel X-GOAL framework. This framework captures both node-level and cluster-level information through a GOAL framework and alignment regularization. X-GOAL aims to reduce semantic errors and maximize mutual information across different graph layers, outperforming state-of-the-art methods in various downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.9,\n    \"b38\": 0.85,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b59\": 0.75,\n    \"b66\": 0.7,\n    \"b52\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in graph representation learning, specifically in encoding nodes into informative embeddings for downstream tasks. The proposed GOAL framework captures both node-level and cluster-level information to reduce semantic errors. Additionally, the X-GOAL framework incorporates an alignment regularization to handle multiplex heterogeneous graphs by aligning layer-specific embeddings without extra neural network modules.\",\n  \"Direct Inspiration\": [\"b26\"],\n  \"Indirect Inspiration\": [\"b17\", \"b38\", \"b57\"],\n  \"Other Inspiration\": [\"b41\", \"b52\", \"b75\"]\n}\n```"], "60c2bb1091e0117e30ca26e7": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of synthesizing 3D talking faces from audio speech signals, focusing on applications such as video editing, lip-sync dubbing, and personalized avatars. It proposes an algorithm with key components including personalized speaker-specific models, 3D pose normalization, lighting normalization, and data-efficient learning.\",\n    \"Direct Inspiration\": {\n        \"b18\": 0.95,\n        \"b32\": 0.90,\n        \"b29\": 0.88\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.85,\n        \"b31\": 0.80,\n        \"b12\": 0.78,\n        \"b37\": 0.77\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.75,\n        \"b22\": 0.72,\n        \"b13\": 0.70\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Synthesizing 3D talking faces driven by audio speech signals\",\n      \"Achieving high visual fidelity and realism under varying lighting and viewpoint conditions\",\n      \"Efficiently training personalized speaker-specific models with limited data\",\n      \"Normalizing pose and lighting variations to enable data-efficient learning\"\n    ],\n    \"inspirations\": [\n      \"Methods for regressing facial motion from audio\",\n      \"Predicting 3D facial meshes from audio for VR and gaming\",\n      \"Combining 3D prediction with high-quality rendering\",\n      \"Universal models like Wav2Lip\",\n      \"Personalized models like NVP\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b29\", \"b32\", \"b18\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b31\", \"b37\", \"b10\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b12\", \"b4\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of synthesizing 3D talking faces driven by an audio speech signal, with applications in video editing, lip-sync dubbing, personalized 3D avatars, and multimedia communication. The authors propose a novel deep learning approach that uses speaker-specific videos and 3D facial tracking to train personalized models efficiently. Key contributions include a method for 3D pose and lighting normalization, a data-efficient learning framework, and an auto-regressive texture prediction model for smooth video synthesis.\",\n    \"Direct Inspiration\": {\n        \"b18\": 0.9,\n        \"b32\": 0.85,\n        \"b29\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.75,\n        \"b6\": 0.75,\n        \"b31\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.6,\n        \"b10\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of synthesizing 3D talking faces driven by an audio speech signal, achieving high visual fidelity and data-efficient learning. It proposes a method that includes 3D pose and lighting normalization, personalized speaker-specific models, and an auto-regressive texture prediction model for smooth video synthesis.\",\n  \"Direct Inspiration\": {\n    \"b32\": 0.9,\n    \"b18\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.7,\n    \"b31\": 0.65,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b6\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of synthesizing 3D talking faces from audio speech signals with high visual fidelity and realism. The authors propose a deep learning model that uses personalized speaker-specific videos for training, incorporating techniques such as 3D pose and lighting normalization to enhance data efficiency and output quality. The approach aims to overcome limitations of previous methods in terms of 3D facial articulation, lighting variations, and data requirements.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b32\": 0.85,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.75,\n    \"b5\": 0.7,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.65,\n    \"b37\": 0.6\n  }\n}\n```"], "6209c8265aee126c0f1e7faa": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of Out-Of-Distribution (OOD) generalization in Graph Neural Networks (GNNs). The primary challenge is the mismatch between training and test data distributions, affecting the performance of graph representation learning models. The authors propose a novel framework called Graph Out-Of-Distribution generalization (GOOD) that leverages Structural Causal Models (SCMs) to generalize the invariance principle to graphs. This involves identifying an invariant subgraph that remains stable across different environments and using it for accurate predictions.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b3\": 1,\n    \"b55\": 1,\n    \"b56\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.8,\n    \"b58\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.6,\n    \"b59\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Out-Of-Distribution (OOD) generalization in graph neural networks (GNNs) by generalizing the invariance principle from causality to graphs. The proposed framework, Graph Out-Of-Distribution generalization (GOOD), involves decomposing a GNN model into a featurizer and a classifier to identify invariant subgraphs that remain stable across distribution shifts.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b3\": 1.0,\n    \"b55\": 1.0,\n    \"b56\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.7,\n    \"b59\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in Out-Of-Distribution (OOD) generalization for Graph Neural Networks (GNNs) by leveraging the invariance principle from causality. The primary contribution is the introduction of the Graph Out-Of-Distribution generalization (GOOD) framework, which decomposes the prediction process into subgraph identification and classification to ensure stability across distribution shifts.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b1\": 0.9,\n    \"b55\": 0.9,\n    \"b56\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.7,\n    \"b39\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b58\": 0.5,\n    \"b17\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of out-of-distribution (OOD) generalization in graph neural networks (GNNs) by extending the invariance principle from Euclidean data to graph data. It introduces the GOOD framework to identify invariant subgraphs for stable predictions under distribution shifts.\",\n  \"Direct Inspiration\": [\"b3\", \"b56\"],\n  \"Indirect Inspiration\": [\"b1\", \"b55\", \"b58\", \"b38\"],\n  \"Other Inspiration\": [\"b28\", \"b39\", \"b60\", \"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in this paper include the out-of-distribution (OOD) generalization failures of graph neural networks (GNNs) due to distribution shifts at both the structure-level and attribute-level of graphs. The paper proposes the Graph Out-Of-Distribution generalization (GOOD) framework, which decomposes the GNN model into a featurizer to identify invariant subgraphs and a classifier to make predictions based on these subgraphs. The framework relies on structural causal models (SCMs) to characterize the graph generation process and uses a contrastive strategy to ensure the identifiability of invariant subgraphs.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b3\": 0.85,\n    \"b55\": 0.8,\n    \"b56\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.7,\n    \"b60\": 0.65,\n    \"b39\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b58\": 0.55,\n    \"b38\": 0.5\n  }\n}\n```"], "6125b6a95244ab9dcb41d6bc": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of optimizing the design and mapping space of custom accelerators, specifically targeting systolic arrays for GEMM workloads. It proposes a novel ML-based approach to learn and predict optimal configurations, significantly reducing the cost and time of traditional DSE cycles. The core contribution is the development and evaluation of the AIRCHITECT neural network model, which achieves high prediction accuracy for optimal design parameters.\",\n    \"Direct Inspiration\": {\n        \"b7\": 1,\n        \"b4\": 0.9,\n        \"b13\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.8,\n        \"b29\": 0.8,\n        \"b3\": 0.7,\n        \"b6\": 0.7,\n        \"b15\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.6,\n        \"b26\": 0.6,\n        \"b21\": 0.5,\n        \"b12\": 0.5,\n        \"b18\": 0.5,\n        \"b10\": 0.4\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of optimizing custom accelerator architectures using ML models, specifically focusing on systolic arrays for GEMM workloads. It introduces AIRCHITECT, a neural network designed to learn design space and predict optimal configurations, achieving high accuracy and significantly reducing design iteration costs.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9,\n        \"b6\": 0.9,\n        \"b15\": 0.9,\n        \"b16\": 0.9,\n        \"b26\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.7,\n        \"b21\": 0.7,\n        \"b12\": 0.7,\n        \"b18\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.5,\n        \"b13\": 0.5,\n        \"b14\": 0.5,\n        \"b29\": 0.5,\n        \"b10\": 0.5,\n        \"b20\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing custom architecture design for ML acceleration using a data-driven approach. It proposes using ML models to learn and predict optimal design and mapping configurations, significantly reducing the cost and time associated with Design Space Exploration (DSE). The proposed method is validated through three case studies involving systolic arrays and demonstrates high prediction accuracy with the custom neural network architecture, AIRCHITECT.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b6\": 0.8,\n    \"b7\": 0.85,\n    \"b15\": 0.8,\n    \"b16\": 0.75,\n    \"b26\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.6,\n    \"b12\": 0.5,\n    \"b13\": 0.6,\n    \"b14\": 0.55,\n    \"b18\": 0.5,\n    \"b21\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.4,\n    \"b1\": 0.45,\n    \"b10\": 0.35,\n    \"b20\": 0.4,\n    \"b29\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing custom architecture design for ML accelerators using a data-driven approach. It proposes a novel ML model, AIRCHITECT, to learn and predict optimal design configurations efficiently. The proposed method aims to reduce the time-consuming iterative design space exploration (DSE) process by formulating it as a ML classification problem.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b13\": 1.0,\n    \"b21\": 1.0,\n    \"b12\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b6\": 0.8,\n    \"b15\": 0.8,\n    \"b16\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b14\": 0.7,\n    \"b29\": 0.7,\n    \"b10\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing the design and mapping of custom hardware architectures, specifically targeting systolic arrays for ML acceleration. The authors propose a novel ML-based approach, AIRCHITECT, to predict optimal architecture configurations, effectively transforming traditional DSE tasks into ML classification problems. This approach aims to significantly reduce the iterative design costs by enabling constant time predictions.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b4\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b6\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b18\": 0.6,\n    \"b21\": 0.6,\n    \"b0\": 0.5,\n    \"b1\": 0.5,\n    \"b10\": 0.5,\n    \"b20\": 0.5,\n    \"b29\": 0.5\n  }\n}\n```"], "60c3211a9e795e9243fd1673": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently computing cohesive answers for knowledge graph (KG) exploration by formulating a quadratic group Steiner tree problem (QGSTP), which incorporates semantic distances between entities. The authors propose an exact algorithm named B3F to solve this problem.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b13\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b3\": 0.9,\n    \"b6\": 0.7,\n    \"b8\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b12\": 0.6,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently computing a cohesive answer in knowledge graph exploration. It proposes an exact algorithm, B3F, to solve the quadratic group Steiner tree problem (QGSTP), which considers both vertex weights and semantic distances.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b0\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.75,\n    \"b10\": 0.75,\n    \"b7\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently computing cohesive answers in Knowledge Graph (KG) exploration. The authors propose a novel algorithm, B3F, which is an exact algorithm for the Quadratic Group Steiner Tree Problem (QGSTP). This problem extends the classic Group Steiner Tree (GST) problem by incorporating semantic distances between graph elements to enhance cohesiveness, making it more complex to solve.\",\n  \"Direct Inspiration\": [\n    \"b13\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b1\",\n    \"b3\",\n    \"b0\",\n    \"b8\"\n  ],\n  \"Other Inspiration\": [\n    \"b6\",\n    \"b10\",\n    \"b7\",\n    \"b16\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of computing cohesive answers for knowledge graph (KG) exploration by proposing the Quadratic Group Steiner Tree Problem (QGSTP) and introducing the B3F algorithm to solve it efficiently. The primary inspiration comes from the need to improve semantic cohesiveness in answers derived from KGs, which was not adequately addressed by prior methods focusing solely on weights of vertices and edges.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b3\": 0.9,\n    \"b13\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b6\": 0.7,\n    \"b8\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b16\": 0.6,\n    \"b9\": 0.5,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficiently computing cohesive answers in knowledge graph (KG) exploration, particularly for users with vague information needs. The authors propose an algorithm named B3F to solve the quadratic group Steiner tree problem (QGSTP), which incorporates semantic distances between entities in the KG to produce more cohesive answers. The algorithm is designed to find an optimum solution by iteratively exploring the search space using a best-first search approach, leveraging combinatorial methods for lower bound estimation.\",\n    \"Direct Inspiration\": [\"b1\", \"b13\"],\n    \"Indirect Inspiration\": [\"b3\", \"b0\", \"b8\", \"b9\"],\n    \"Other Inspiration\": [\"b6\", \"b10\"]\n}\n```"], "60641c219e795e72406b65b0": ["```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the privacy protection issues in personalized search. The authors propose a novel federated learning framework named FedPS to train a shared personalized search model without exposing user privacy. Two specific implementations, FedPSFlat and FedPSProxy, are introduced to handle data heterogeneity and communication efficiency challenges.\",\n  \"Direct Inspiration\": {\n    \"b46\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b3\": 0.7,\n    \"b27\": 0.8,\n    \"b35\": 0.6,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b8\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of privacy protection in personalized search by proposing a federated learning-based framework, FedPS, that allows for the training of personalized models without exposing user data. The primary inspirations are methods to mask user identities and obfuscate queries, as well as federated learning techniques to ensure data privacy.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b46\": 0.8,\n    \"b27\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b36\": 0.7,\n    \"b48\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b8\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses user privacy protection in personalized search by introducing a federated learning framework, FedPS. It proposes two models, FedPSFlat and FedPSProxy, to train a high-quality personalized search model without collecting user data on a central server.\",\n  \"Direct Inspiration\": {\n    \"b46\": 0.95,\n    \"b3\": 0.90,\n    \"b48\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.85,\n    \"b8\": 0.80,\n    \"b20\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.75,\n    \"b36\": 0.70\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of privacy protection in personalized search by proposing a federated learning-based framework, FedPS, to enable privacy-preserving personalized search. The framework uses client devices to store and process user data locally, avoiding the need to centrally collect sensitive information. Two models, FedPSFlat and FedPSProxy, are implemented to tackle issues such as data heterogeneity and server performance bottlenecks.\",\n  \"Direct Inspiration\": {\n    \"b46\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b3\": 0.7,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.65,\n    \"b48\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges in the paper are ensuring user privacy in personalized search while maintaining high search accuracy. The proposed solution, FedPS, uses federated learning to train a personalized ranking model without storing user data on a central server. The paper introduces two models: FedPSFlat and FedPSProxy, to address data heterogeneity and communication issues.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b46\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b48\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b8\": 0.4\n  }\n}\n```"], "61bc00155244ab9dcba3eb06": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of catastrophic forgetting in Variational Autoencoders (VAEs) during Lifelong Learning (LLL). It proposes a novel theoretical framework called Lifelong ELBO (LELBO) for analyzing VAE's forgetting behavior. Additionally, it introduces a dynamic expansion graph model (DEGM) that facilitates transfer learning and model size reduction.\",\n    \"Direct Inspiration\": [\"b3\", \"b18\", \"b33\", \"b44\"],\n    \"Indirect Inspiration\": [\"b22\", \"b25\", \"b39\"],\n    \"Other Inspiration\": [\"b3\", \"b33\", \"b44\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of catastrophic forgetting in Variational Autoencoders (VAEs) during lifelong learning (LLL). It proposes a novel theoretical framework for analyzing VAE's forgetting behavior and introduces a new generative latent variable model and dynamic expansion graph model (DEGM) to balance task performance and model size.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1.0,\n        \"b33\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b22\": 0.8,\n        \"b34\": 0.8,\n        \"b3\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b43\": 0.6,\n        \"b44\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of catastrophic forgetting in Variational Autoencoders (VAEs) during lifelong learning (LLL). It proposes a novel theoretical framework for analyzing VAE's forgetting behavior and introduces a new generative latent variable model to balance task performance and model size. The paper also introduces a new benchmark for probability density estimation under LLL settings.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b33\": 1,\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b22\": 0.7,\n    \"b44\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5,\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in this paper are the catastrophic forgetting in VAEs during lifelong learning (LLL) and the tightness of ELBO. The proposed algorithms include the development of a novel theoretical framework for analyzing VAE's forgetting behaviour, a new generative latent variable model, and a new benchmark for probability density estimation in LLL. The paper introduces a novel dynamic expansion graph model (DEGM) to address these challenges.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1,\n        \"b33\": 1,\n        \"b43\": 0.9,\n        \"b22\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.7,\n        \"b8\": 0.6,\n        \"b39\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.5,\n        \"b12\": 0.5,\n        \"b49\": 0.5,\n        \"b16\": 0.5,\n        \"b25\": 0.5,\n        \"b28\": 0.5,\n        \"b40\": 0.5,\n        \"b9\": 0.4,\n        \"b11\": 0.4,\n        \"b26\": 0.4\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges addressed in this paper include improving the performance of Variational Autoencoders (VAEs) under the lifelong learning (LLL) setting, particularly in mitigating catastrophic forgetting and ensuring the tightness of the Evidence Lower Bound (ELBO). The authors propose a novel theoretical framework, the Lifelong ELBO (LELBO), to evaluate this tightness and introduce a dynamic expansion graph model (DEGM) to enhance model capacity and efficiency.\",\n    \"Direct Inspiration\": {\n        \"b18\": 1.0,\n        \"b0\": 0.9,\n        \"b33\": 0.9,\n        \"b44\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b11\": 0.7,\n        \"b43\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b25\": 0.6,\n        \"b40\": 0.6\n    }\n}\n```"], "60641c2a9e795e72406b65b4": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the problem of missing node attributes in heterogeneous graphs, which affects the performance of graph neural network (GNN) models. The proposed solution is the HGNN-AC framework, which uses topological relationships and attention mechanisms to complete missing node attributes by aggregating attributes from neighboring nodes.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b29\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of missing node attributes in heterogeneous graphs, which significantly impacts the performance of Graph Neural Networks (GNNs) on these graphs. It proposes a novel framework, HGNN-AC, to complete node attributes by leveraging topological relationships and attention mechanisms. The framework aims to improve the performance of GNN-based models by filling in missing attributes through learned aggregation from neighboring nodes with attributes.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed in this paper is the problem of missing attributes for some types of nodes in heterogeneous graphs, which significantly affects the performance of graph neural network (GNN)-based models.\",\n    \"inspirations\": \"The paper proposes a general framework for Heterogeneous Graph Neural Network via Attribute Completion (HGNN-AC) to solve this problem by using topological relationships and attention mechanisms to complete node attributes.\"\n  },\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b38\": 1.0,\n    \"b39\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of missing attributes for certain node types in heterogeneous graphs. It proposes HGNN-AC, a framework that uses topological relationships and attention mechanisms to complete these missing attributes, significantly improving the performance of existing GNN-based models.\",\n    \"Direct Inspiration\": {\n        \"b10\": 1,\n        \"b38\": 1,\n        \"b43\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b39\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.7,\n        \"b24\": 0.6,\n        \"b37\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of missing node attributes in heterogeneous graphs, which affects the performance of Graph Neural Networks (GNNs). The authors propose a novel framework called Heterogeneous Graph Neural Network via Attribute Completion (HGNN-AC) to complete missing node attributes by leveraging topological relationships and weighted aggregation of attributed nodes using an attention mechanism.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b29\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```"], "603230baaf79179a99b354a0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of dealing with the heterogeneity of knowledge graphs (KGs) and the need for effectively aggregating multiple types of semantic information. It proposes a novel heterogeneous graph neural network (GNN) framework that uses attention mechanisms to assign importance weights to different relation-paths and employs diverse aggregator functions to handle the complex structure of KGs.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.95,\n    \"b40\": 0.9,\n    \"b41\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.85,\n    \"b19\": 0.85,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b17\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of heterogeneity in Knowledge Graphs (KGs) and proposes a novel heterogeneous relation attention networks (HRAN) framework. The HRAN framework aggregates neighbor features in a hierarchical manner using attention mechanisms to capture various types of semantic information. The main contributions include a novel end-to-end HRAN framework, utilization of attention mechanisms to learn the importance of relation-paths, and the adoption of three effective aggregator functions to handle large-scale heterogeneous graphs.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b40\": 1,\n    \"b41\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b18\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of embedding heterogeneous knowledge graphs (KGs) and proposes a novel heterogeneous Graph Neural Networks (GNN) framework based on an attention mechanism. The proposed method aggregates neighbor features under each relation-path and learns the importance of different relation-paths to generate embeddings. This approach captures various semantic information and selectively aggregates informative features, outperforming state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b18\": 0.75,\n    \"b19\": 0.7,\n    \"b40\": 0.68\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.6,\n    \"b35\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning knowledge graph (KG) embeddings, focusing on the intrinsic heterogeneity of KGs which contain various types of entities and relations. The proposed solution is a novel heterogeneous graph neural network (GNN) framework based on attention mechanisms, which aggregates and weights different relation-paths to generate embedding representations. The framework, named HRAN, captures various types of semantic information and selectively aggregates informative features.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b18\": 0.6,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.8,\n    \"b35\": 0.7,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of embedding representation in Knowledge Graphs (KGs) by handling their inherent heterogeneity. The authors propose a novel heterogeneous GNN framework called HRAN, which uses attention mechanisms to weigh different relation paths and aggregate entity features hierarchically. This approach aims to capture various types of semantic information and selectively aggregate informative features for effective KG embedding.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b40\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b41\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b22\": 0.55,\n    \"b23\": 0.5\n  }\n}\n```"], "617d22045244ab9dcbd01de7": ["```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the need for methods that can identify spatial domains and spatially variable genes (SVGs) in spatial transcriptomics (SRT) data, considering gene expression, spatial location, and histology simultaneously. The novel approach introduced by the authors is SpaGCN, a graph convolutional network-based method that integrates these three aspects to jointly identify spatial domains and SVGs, providing a comprehensive picture of spatial gene expression gradients.\",\n    \"Direct Inspiration\": {\n        \"b15\": 0.9,\n        \"b16\": 0.85,\n        \"b17\": 0.87\n    },\n    \"Indirect Inspiration\": {\n        \"b22\": 0.7,\n        \"b23\": 0.75,\n        \"b24\": 0.73\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.6,\n        \"b19\": 0.65,\n        \"b20\": 0.62,\n        \"b21\": 0.67,\n        \"b25\": 0.68,\n        \"b26\": 0.64,\n        \"b27\": 0.63,\n        \"b28\": 0.61,\n        \"b29\": 0.66,\n        \"b30\": 0.65,\n        \"b31\": 0.62,\n        \"b32\": 0.61,\n        \"b33\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying spatial domains and spatially variable genes (SVGs) in spatial transcriptomics (SRT) data, integrating gene expression, spatial location, and histology using a graph convolutional network (SpaGCN). The SpaGCN method jointly considers spatial domain detection and SVG identification, providing a comprehensive picture of spatial gene expression gradients in tissues, and is versatile across various SRT platforms.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b16\": 0.9,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces SpaGCN, a graph convolutional network-based approach that integrates gene expression, spatial location, and histology information to jointly identify spatial domains and spatially variable genes (SVGs) in tissue samples. SpaGCN improves over existing methods by ensuring that SVGs have spatial expression patterns, thus providing better biological interpretability and versatility in analyzing various spatial transcriptomics (SRT) data.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b16\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b23\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b26\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the development of SpaGCN, a graph convolutional network-based method that integrates gene expression, spatial location, and histology information to identify spatial domains and spatially variable genes (SVGs) in tissue samples. SpaGCN addresses the limitations of existing methods by jointly considering spatial domain and SVG identification, ensuring that detected SVGs have spatial expression patterns.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b15\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b23\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5,\n    \"b29\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating gene expression, spatial location, and histology to identify spatial domains and spatially variable genes (SVGs) in tissue samples. It introduces SpaGCN, a graph convolutional network-based approach that jointly considers these factors to improve the accuracy and biological interpretability of detected spatial patterns. The novel method is evaluated against existing methods like stLearn, BayesSpace, SpatialDE, and SPARK, demonstrating superior performance in various datasets.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b17\": 1,\n    \"b24\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.9,\n    \"b22\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.8,\n    \"b10\": 0.8,\n    \"b4\": 0.8\n  }\n}\n```"], "6040b47e91e011a0653f0762": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating heterogeneous entity knowledge into pre-trained language models, proposing OAG-BERT to jointly model texts and heterogeneous entity knowledge via pre-training over the OAG. Key innovations include heterogeneous entity type embedding, span-aware entity masking, and entity-aware 2D positional encoding.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b50\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b17\": 0.8,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b39\": 0.6,\n    \"b10\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating heterogeneous entity knowledge into pre-trained language models, specifically for academic domains. It introduces OAG-BERT, which incorporates various types of entity knowledge using novel techniques such as heterogeneous entity type embeddings, span-aware entity masking, and entity-aware 2D positional encoding.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b8\": 1,\n    \"b25\": 1,\n    \"b46\": 1,\n    \"b50\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b29\": 0.8,\n    \"b44\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b4\": 0.7,\n    \"b9\": 0.7,\n    \"b34\": 0.7,\n    \"b49\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of integrating domain entity knowledge into pre-trained language models, which is crucial for many entity-related downstream tasks. The proposed solution, OAG-BERT, incorporates heterogeneous entity knowledge from the OAG (Open Academic Graph) corpus. Key innovations include heterogeneous entity type embeddings, span-aware entity masking, and entity-aware 2D positional encoding. The model is evaluated on various downstream tasks, demonstrating its superior performance in integrating entity knowledge.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b50\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b25\": 0.8,\n    \"b8\": 0.7,\n    \"b46\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating heterogeneous entity knowledge into pre-trained language models, specifically targeting the academic field. The authors propose OAG-BERT, which incorporates a novel span-aware entity masking strategy, heterogeneous entity type embeddings, and entity-aware 2D positional encoding to enhance the model's ability to handle various entity types and their relationships. They evaluate the model on multiple downstream tasks, demonstrating its effectiveness in zero-shot inference, heterogeneous graph learning, and author name disambiguation.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b50\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b25\": 0.8,\n    \"b17\": 0.8,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating domain entity knowledge into pre-trained language models, particularly in the context of heterogeneous academic entities. It proposes OAG-BERT, a model that incorporates heterogeneous entity type embeddings, span-aware entity masking, and entity-aware 2D positional encoding to enhance performance on entity-related tasks. The model is evaluated on various downstream applications, demonstrating its effectiveness in traditional NLP tasks, zero-shot entity inference, and heterogeneous graph learning.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b50\": 1,\n    \"b17\": 0.9,\n    \"b44\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b1\": 0.8,\n    \"b36\": 0.7,\n    \"b46\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b39\": 0.6,\n    \"b30\": 0.6,\n    \"b29\": 0.6,\n    \"b49\": 0.5,\n    \"b2\": 0.5,\n    \"b16\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```"], "61f002d991e01166c0b3c4b9": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the limitations of current molecular property prediction methods due to incomplete molecular representations and inefficiencies in the training process. The paper proposes a curriculum-based learning approach, CurrMG, to address these issues by rearranging training data based on their difficulty coefficients, thereby enabling better training of molecule graph learning models.\",\n  \"Direct Inspiration\": [\"b20\", \"b25\", \"b26\"],\n  \"Indirect Inspiration\": [\"b17\", \"b18\", \"b19\", \"b22\"],\n  \"Other Inspiration\": [\"b7\", \"b8\", \"b9\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges are incomplete molecular representations and inefficiencies in the model training process for molecular property prediction. The proposed algorithm, CurrMG, focuses on optimizing the training phase by using a curriculum-based learning approach to improve model performance.\",\n    \"Inspirations\": \"CurrMG is inspired by curriculum learning, which involves training models on data from easy to difficult, as first proposed by Bengio in 2009. The method leverages cheminformatics domain knowledge to calculate difficulty coefficients for molecular data.\"\n  },\n  \"Direct Inspiration\": {\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the inefficiency in molecular property prediction due to incomplete molecular representations and the limited capacity of traditional machine learning models. The proposed solution, CurrMG, is a curriculum-based learning approach aimed at optimizing the training phase of molecule graph learning models by rearranging training data based on their difficulty coefficients. This method is inspired by curriculum learning and aims to improve model generalization and performance in molecular property prediction tasks.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b21\": 0.9,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b18\": 0.75,\n    \"b19\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b26\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving molecular property prediction by focusing on the training process of molecule graph learning models through a curriculum-based learning approach named CurrMG. It integrates cheminformatics domain knowledge to calculate difficulty coefficients for molecular structures and uses these coefficients to guide the training sequence from easy to difficult data, improving model performance and generalization.\",\n    \"Direct Inspiration\": {\n        \"b20\": 1,\n        \"b21\": 0.9,\n        \"b22\": 0.9,\n        \"b23\": 0.8,\n        \"b24\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b17\": 0.7,\n        \"b18\": 0.6,\n        \"b19\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b25\": 0.5,\n        \"b26\": 0.5,\n        \"b22\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in molecular property prediction by proposing a curriculum-based learning approach called CurrMG. The approach aims to improve the training process of molecule graph learning models by rearranging training data based on difficulty coefficients calculated through cheminformatics domain knowledge.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b22\": 0.8,\n    \"b23\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b26\": 0.7\n  }\n}\n```"], "617a16125244ab9dcbdb3fc6": ["```json\n{\n  \"Summary\": \"The primary challenges identified in this paper are the lack of large, high-quality datasets covering different non-homophilous applications, the inadequacy of current graph minibatching techniques and scalable methods in non-homophilous settings, and the non-scalability of prior non-homophilous methods. The authors propose a novel model, LINKX, which outperforms existing graph learning methods on large-scale non-homophilous datasets and addresses the mentioned deficiencies.\",\n  \"Direct Inspiration\": {\n    \"b57\": 1,\n    \"b81\": 1,\n    \"b76\": 1,\n    \"b70\": 1,\n    \"b31\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b16\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b78\": 0.7,\n    \"b2\": 0.7,\n    \"b40\": 0.6,\n    \"b42\": 0.6,\n    \"b59\": 0.6,\n    \"b41\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three primary challenges in graph learning: the lack of large, high-quality datasets for non-homophilous applications, the inadequacy of current graph minibatching techniques and scalable methods in non-homophilous settings, and the scalability issues of prior non-homophilous methods. The authors propose a novel model, LINKX, which outperforms existing methods on large-scale non-homophilous datasets and maintains strong performance with a simple minibatching procedure. The paper also introduces new non-homophilous datasets and benchmarks for evaluation.\",\n  \"Direct Inspiration\": {\n    \"b81\": 1.0,\n    \"b57\": 0.9,\n    \"b76\": 0.8,\n    \"b70\": 0.8,\n    \"b31\": 0.8,\n    \"b78\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b16\": 0.7,\n    \"b14\": 0.7,\n    \"b72\": 0.7,\n    \"b35\": 0.7,\n    \"b34\": 0.7,\n    \"b15\": 0.7,\n    \"b79\": 0.6,\n    \"b25\": 0.6,\n    \"b56\": 0.6,\n    \"b38\": 0.6,\n    \"b71\": 0.6,\n    \"b68\": 0.6,\n    \"b37\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b45\": 0.5,\n    \"b2\": 0.5,\n    \"b54\": 0.5,\n    \"b12\": 0.5,\n    \"b24\": 0.5,\n    \"b10\": 0.5,\n    \"b43\": 0.5,\n    \"b80\": 0.5,\n    \"b16\": 0.5,\n    \"b14\": 0.5,\n    \"b72\": 0.5,\n    \"b35\": 0.5,\n    \"b34\": 0.5,\n    \"b8\": 0.5,\n    \"b53\": 0.5,\n    \"b60\": 0.5,\n    \"b65\": 0.5,\n    \"b47\": 0.5,\n    \"b52\": 0.5,\n    \"b19\": 0.5,\n    \"b9\": 0.5,\n    \"b62\": 0.5,\n    \"b20\": 0.5,\n    \"b78\": 0.5,\n    \"b23\": 0.4,\n    \"b77\": 0.4,\n    \"b13\": 0.4,\n    \"b75\": 0.4,\n    \"b2\": 0.4,\n    \"b3\": 0.4,\n    \"b66\": 0.4,\n    \"b40\": 0.4,\n    \"b42\": 0.4,\n    \"b59\": 0.4,\n    \"b30\": 0.4,\n    \"b41\": 0.4,\n    \"b40\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in graph learning methods, especially in non-homophilous settings. It introduces LINKX, a novel model that outperforms existing methods on large-scale non-homophilous datasets. The contributions include collecting diverse non-homophilous datasets, analyzing scalable methods and minibatching, and proposing LINKX for better performance.\",\n  \"Direct Inspiration\": [\"b78\", \"b81\"],\n  \"Indirect Inspiration\": [\"b57\", \"b16\", \"b14\", \"b76\"],\n  \"Other Inspiration\": [\"b31\", \"b70\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three primary challenges in non-homophilous graph learning: the lack of large, high-quality datasets, the inefficacy of current graph minibatching techniques in non-homophilous settings, and the scalability issues of prior non-homophilous methods. The proposed model, LINKX, overcomes these challenges by separately embedding the adjacency matrix and node features, then combining them with multilayer perceptrons and simple transformations. This approach allows for effective training and evaluation in large-scale non-homophilous graphs.\",\n  \"Direct Inspiration\": {\n    \"b78\": 1,\n    \"b81\": 0.9,\n    \"b57\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b76\": 0.7,\n    \"b70\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b14\": 0.6,\n    \"b54\": 0.6,\n    \"b24\": 0.6,\n    \"b10\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in graph learning for non-homophilous settings, specifically the lack of large, high-quality datasets, the poor performance of current graph minibatching techniques, and the scalability issues of existing non-homophilous methods. The authors propose a novel model, LINKX, which outperforms existing methods on large-scale non-homophilous datasets and maintains strong performance with a simple minibatching procedure.\",\n  \"Direct Inspiration\": {\n    \"b81\": 1.0,\n    \"b57\": 0.9,\n    \"b16\": 0.8,\n    \"b72\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b70\": 0.7,\n    \"b31\": 0.7,\n    \"b0\": 0.6,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b78\": 0.5,\n    \"b76\": 0.5\n  }\n}\n```"], "611106eb91e0117e7f3cdaa3": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the efficient scheduling of deep neural network (DNN) accelerators. The authors propose a constrained-optimization-based approach called CoSA to schedule DNN accelerators. CoSA formulates the DNN scheduling problem as a prime-factor allocation problem that determines tiling sizes for different memory levels, relative loop ordering to exploit reuse, and how computation should be executed spatially and temporally. This approach avoids the need for exhaustive brute-force search or expensive feedback-driven methods, offering significant performance improvements and reduced scheduling time.\",\n  \"Direct Inspiration\": [\"b13\", \"b22\", \"b48\", \"b70\", \"b14\", \"b37\"],\n  \"Indirect Inspiration\": [\"b2\", \"b66\", \"b7\", \"b11\", \"b12\", \"b29\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient DNN scheduling for accelerators, proposing CoSA, a constrained-optimization-based approach. CoSA aims to overcome the computational limits of brute-force and feedback-driven methods by formulating the scheduling problem as a prime-factor allocation problem that can be solved deterministically.\",\n  \"Direct Inspiration\": [\"b13\", \"b22\", \"b48\", \"b70\", \"b14\", \"b37\"],\n  \"Indirect Inspiration\": [\"b2\", \"b66\"],\n  \"Other Inspiration\": [\"b7\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently scheduling DNN accelerators, which involves optimizing the spatial and temporal execution of DNN layers to enhance performance and energy efficiency. The proposed method, CoSA, formulates this scheduling as a constrained-optimization problem and solves it using mixed-integer programming. This approach contrasts with previous brute-force or feedback-driven methods by being more efficient and requiring no iterative search.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b14\": 0.9,\n    \"b22\": 0.8,\n    \"b48\": 0.9,\n    \"b70\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b37\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b29\": 0.7,\n    \"b66\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.6,\n    \"b55\": 0.6,\n    \"b5\": 0.6,\n    \"b44\": 0.6,\n    \"b39\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently scheduling DNN layers on specialized accelerators, considering the vast number of possible schedules and the complexity of hardware architectures. The proposed algorithm, CoSA, formulates the scheduling problem as a constrained-optimization problem, allowing for deterministic solutions without iterative search, which outperforms state-of-the-art approaches while significantly reducing scheduling time.\",\n  \"Direct Inspiration\": {\n    \"b48\": 0.95,\n    \"b13\": 0.85,\n    \"b22\": 0.80,\n    \"b70\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b14\": 0.75,\n    \"b37\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.65,\n    \"b12\": 0.65,\n    \"b29\": 0.65,\n    \"b66\": 0.65,\n    \"b7\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficiently scheduling deep neural network (DNN) layers on specialized accelerators, given the vast number of possible schedules and the need for high performance and energy efficiency. The proposed solution, CoSA, formulates the DNN scheduling as a constrained-optimization problem that can be solved deterministically in one pass, leveraging the regularities in DNN layers and hardware constraints. This approach contrasts with previous brute-force and feedback-driven methods that are computationally expensive and less adaptable to new hardware.\",\n    \"Direct Inspiration\": {\n        \"b48\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.9,\n        \"b22\": 0.9,\n        \"b70\": 0.9,\n        \"b14\": 0.8,\n        \"b37\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.7,\n        \"b12\": 0.7\n    }\n}\n```"], "61120a875244ab9dcb0efdbc": ["```json\n{\n  \"Summary\": \"This paper addresses the challenges of memory disaggregation (MemDisagg) in data centers, proposing a hardware-based solution that balances cost, performance, and scalability. The proposed system, Clio, includes a hardware-based memory node (MN) with integrated virtual memory and network stack, designed to handle TBs of memory and thousands of application processes efficiently while minimizing tail latency.\",\n  \"Direct Inspiration\": {\n    \"b39\": 0.9,\n    \"b62\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b54\": 0.8,\n    \"b63\": 0.8,\n    \"b53\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b41\": 0.6,\n    \"b67\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the limitations of existing memory disaggregation approaches, such as cost, performance, scalability, and security issues. The novel methods introduced include a hardware-based memory disaggregation solution called Clio, which integrates a virtual memory system with a network stack on a dedicated hardware platform (CBoard), aiming to provide low-latency, high-throughput, and scalable memory access.\",\n  \"Direct Inspiration\": {\n    \"b39\": 0.9,\n    \"b54\": 0.8,\n    \"b63\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b62\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b28\": 0.6,\n    \"b31\": 0.6,\n    \"b49\": 0.6,\n    \"b52\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of memory disaggregation (MemDisagg) in data centers, focusing on the limitations of server-based and raw device-based approaches. It proposes Clio, a hardware-based MemDisagg solution with programmable hardware that integrates a virtual memory system and network stack to achieve high throughput, low latency, and scalability. The design principles involve eliminating unnecessary states, moving non-critical tasks to software, and ensuring deterministic hardware pipelines.\",\n  \"Direct Inspiration\": {\n    \"b54\": 0.9,\n    \"b63\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b25\": 0.7,\n    \"b37\": 0.7,\n    \"b38\": 0.7,\n    \"b53\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined include overcoming memory capacity limitations in servers, enhancing performance and scalability of memory disaggregation (MemDisagg) systems, and minimizing tail latency. The paper proposes a hardware-based MemDisagg solution (Clio) that integrates a virtual memory system with a customized hardware network stack, designed to efficiently handle virtual memory requests over the network. Clio aims to host large amounts of memory, support thousands of concurrent application processes, and achieve low latency and high throughput.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b54\": 1.0,\n    \"b63\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.8,\n    \"b53\": 0.8,\n    \"b62\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.7,\n    \"b25\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in memory disaggregation (MemDisagg) for data centers, focusing on the design and implementation of a hardware-based solution that balances cost, performance, and scalability. By proposing a system named Clio, the authors explore new requirements and benefits of MemDisagg, aiming to overcome limitations of existing server-based and raw device-based approaches.\",\n  \"Direct Inspiration\": {\n    \"b54\": 1,\n    \"b63\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b23\": 0.7,\n    \"b53\": 0.7,\n    \"b41\": 0.7,\n    \"b32\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5,\n    \"b37\": 0.5,\n    \"b38\": 0.5,\n    \"b14\": 0.4,\n    \"b28\": 0.4,\n    \"b31\": 0.4,\n    \"b49\": 0.4,\n    \"b52\": 0.4\n  }\n}\n```"], "6135869d5244ab9dcbd3743a": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of generating paraphrases with specific styles using Exemplar-Guided Paraphrase Generation (EGPG). The main issue is the inconsistent content and style representation by traditional models. The proposed solution incorporates contrastive learning to improve content and style encodings in an end-to-end encoder-decoder paradigm with multiple losses.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.95,\n        \"b19\": 0.9,\n        \"b14\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b17\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.7,\n        \"b21\": 0.65,\n        \"b13\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is generating paraphrases that retain the content of the source sentence while adopting the style of an exemplar sentence. The proposed solution incorporates contrastive learning to design an end-to-end encoder-decoder paradigm with multiple losses to improve content and style representations.\",\n  \"Direct Inspiration\": [\"b2\", \"b14\"],\n  \"Indirect Inspiration\": [\"b19\"],\n  \"Other Inspiration\": [\"b3\", \"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of Exemplar-Guided Paraphrase Generation (EGPG), specifically the difficulty in learning accurate representations for both content and style. The authors propose an end-to-end encoder-decoder model enhanced with contrastive learning losses to improve the quality of generated paraphrases by distinguishing between similar and different content and style features.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b17\": 0.7,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.5,\n    \"b8\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Exemplar-Guided Paraphrase Generation (EGPG), particularly in learning accurate content and style representations. It proposes incorporating contrastive learning into an end-to-end encoder-decoder paradigm with multiple losses to improve content and style encoding and generation accuracy.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.95,\n    \"b2\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.75,\n    \"b15\": 0.7,\n    \"b3\": 0.65,\n    \"b17\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b27\": 0.55,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating paraphrases that maintain content consistency and style similarity by incorporating contrastive learning into the exemplar-guided paraphrase generation (EGPG) framework. The proposed end-to-end encoder-decoder model uses content and style encoders with contrastive losses to improve content and style representation, leading to better performance on benchmark datasets.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1,\n        \"b14\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b17\": 0.8,\n        \"b19\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.6,\n        \"b25\": 0.5,\n        \"b8\": 0.5\n    }\n}\n```"], "60badc5391e01102e59b6ac9": ["```json\n{\n  \"Summary\": \"This paper addresses the challenge of providing effective emotional support (ES) through dialog systems, which is a complex task due to the necessity of diverse conversational skills and a lack of relevant corpora. The authors propose an Emotional Support Conversation (ESC) Framework grounded on Hill's Helping Skills Theory and develop the ESConv dataset to facilitate research in this area.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b37\": 0.9,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b17\": 0.7,\n    \"b25\": 0.7,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b20\": 0.5,\n    \"b24\": 0.5,\n    \"b34\": 0.5,\n    \"b23\": 0.5,\n    \"b27\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of providing effective Emotional Support (ES) through dialog systems, emphasizing the need for reducing users' emotional distress and assisting them with their challenges. The authors propose an Emotional Support Conversation (ESC) Framework based on the Helping Skills Theory, adapting it to dialog systems with three stages: Exploration, Comforting, and Action. They also introduce a dataset called ESConv to facilitate research, ensuring rich annotations and quality examples. The framework and dataset aim to improve dialog systems' ability to provide effective ES by utilizing various support strategies.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b22\": 0.8,\n        \"b37\": 0.8,\n        \"b17\": 0.7,\n        \"b25\": 0.7,\n        \"b27\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b30\": 0.6,\n        \"b31\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of providing emotional support (ES) through dialog systems by defining the task of Emotional Support Conversation (ESC) and proposing an ESC Framework grounded on the Helping Skills Theory. It highlights the importance of exploring the help-seeker's problems, expressing empathy, and offering actionable suggestions to improve emotional support. The paper also constructs an Emotional Support Conversation dataset (ESConv) and evaluates state-of-the-art models, demonstrating significant improvements in emotional support provision.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b24\": 0.7,\n    \"b37\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b25\": 0.6,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of providing effective Emotional Support (ES) in dialog systems. It proposes an Emotional Support Conversation (ESC) Framework based on Helping Skills Theory and constructs the ESConv dataset to facilitate research on data-driven ES dialog systems. The ESC Framework consists of three stages: Exploration, Comforting, and Action, each containing several support strategies. This work aims to improve emotional support in dialog systems through rich annotation, quality conversations, and training supporters.\",\n    \n    \"Direct Inspiration\": [\"b4\", \"b22\", \"b37\"],\n    \n    \"Indirect Inspiration\": [\"b10\", \"b21\", \"b24\", \"b28\", \"b30\"],\n    \n    \"Other Inspiration\": [\"b1\", \"b3\", \"b6\", \"b25\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of providing effective Emotional Support (ES) through dialog systems. It proposes an Emotional Support Conversation (ESC) framework based on the Helping Skills Theory, designed to reduce users' emotional distress and help them work through challenges. The framework is divided into three stages: Exploration, Comforting, and Action, each containing several support strategies. The authors also introduce a dataset, ESConv, to facilitate research in this domain.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b25\": 0.5,\n    \"b6\": 0.5,\n    \"b27\": 0.5\n  }\n}\n```"], "620b19c25aee126c0f7e64b0": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the need to effectively model user preferences in product search, which is distinct from web search due to the short and often uninformative textual representations of products, as well as the rich relational data and diverse user-item interactions involved. The proposed algorithm addresses these challenges by utilizing a user successive behavior graph (SBG) to capture local and global user behavior patterns, employing graph convolution to learn enriched product representations, and using jumping graph convolution layers to alleviate the over-smoothing problem.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b1\": 1.0,\n    \"b2\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b6\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving personalized product search on e-commerce platforms by utilizing diverse relational data and various user-item interactions. It proposes a novel approach that constructs a user successive behavior graph (SBG) to capture local and global user behavior patterns and employs graph convolution with jumping connections to enrich product representations and alleviate the over-smoothing problem.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b2\": 1,\n    \"b26\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b5\": 0.7,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving personalized product search on e-commerce platforms by leveraging a user successive behavior graph (SBG) and graph convolution techniques. The novel approach aims to capture implicit user preference signals and collaborative patterns, while addressing the over-smoothing problem in graph convolution. The proposed method is integrated with the ZAM model and evaluated on Amazon benchmarks, showing significant performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b2\": 1,\n    \"b26\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.9,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b35\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving personalized product search on e-commerce platforms by modeling user preferences more effectively. It proposes using a user successive behavior graph (SBG) and graph convolution with jumping connections to capture both local and global user behavior patterns, thereby enhancing product representations. The approach aims to overcome the limitations of existing methods that either rely on noisy long-term user behavior or insufficient short-term behavior data.\",\n  \"Direct Inspiration\": [\"b0\", \"b2\", \"b26\"],\n  \"Indirect Inspiration\": [\"b1\", \"b5\", \"b6\"],\n  \"Other Inspiration\": [\"b24\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving personalized product search on e-commerce platforms by leveraging user behavioral data. The proposed approach introduces a successive behavior graph (SBG) and employs graph convolution with jumping connections to model both local and global user behavior patterns, aiming to capture more useful user preference signals while mitigating the over-smoothing problem. The method is integrated into the ZAM model and evaluated on Amazon benchmarks, showing significant improvements over existing methods.\",\n  \"Direct Inspiration\": [\"b0\", \"b2\", \"b26\"],\n  \"Indirect Inspiration\": [\"b1\", \"b5\", \"b6\"],\n  \"Other Inspiration\": [\"b35\"]\n}\n```"], "61dd329e6750f82ad426acbd": ["```json\n{\n  \"Summary\": \"The paper addresses two main challenges: (1) lack of formal boundaries in existing cost models for DNN operators, and (2) the massive search space for optimal mappings on accelerators. The authors propose a novel approach using the Maestro Data-Centric (MDC) notation and a decoupled off-chip/on-chip mapping strategy implemented in a tool called Marvel to reduce the search space and improve performance and energy efficiency.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b23\", \"b24\", \"b33\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b11\", \"b15\", \"b43\", \"b53\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": []\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges: 1) defining formal boundaries for cost models in kernel optimizers and 2) the combinatorial explosion in mapping space for generic templated spatial accelerators. The proposed solutions include conformable DNN operators using the Maestro Data-Centric (MDC) notation and a decoupled off-chip/on-chip approach for mapping space exploration, implemented in the Marvel tool.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b24\": 0.9,\n    \"b33\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b15\": 0.85,\n    \"b43\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.75,\n    \"b53\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges: the lack of formal boundaries in existing cost models for kernel optimizers, and the combinatorial explosion in the search space for optimal mappings on generic spatial accelerators. The proposed approach leverages the Maestro Data-Centric (MDC) notation to express mappings and introduces conformability rules to ensure all possible mappings of an operator are expressible in this notation. Additionally, the paper presents a decoupled off-chip/on-chip approach to reduce the search space and optimize data movement, leading to significant performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b24\": 0.9,\n    \"b33\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b15\": 0.75,\n    \"b43\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.7,\n    \"b47\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses two primary challenges: 1) the lack of formal boundaries in existing cost models for DNN kernel optimizers, and 2) the difficulty in searching for optimal mappings due to the vast space of legal mappings on accelerator configurations. To tackle these challenges, the authors propose using the Maestro Data-Centric (MDC) notation to define conformability rules for DNN operators and introduce a decoupled off-chip/on-chip approach for mapping space exploration. They implemented these methods in a tool named Marvel, significantly reducing the mapping space while improving performance and energy efficiency.\",\n    \"Direct Inspiration\": {\n        \"b23\": 0.95,\n        \"b24\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b33\": 0.85,\n        \"b11\": 0.8,\n        \"b53\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.7,\n        \"b42\": 0.65,\n        \"b29\": 0.6,\n        \"b50\": 0.6,\n        \"b56\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses two primary challenges: 1) the lack of formal boundaries in existing cost models for kernel optimizers, which leads to adaptability challenges, and 2) the difficulty in searching for optimal mappings due to the massive space of legal mappings for operators on accelerator configurations. The proposed solution involves using the Maestro Data-Centric (MDC) notation to express mappings and introducing conformability rules to ensure all possible mappings are expressible. Additionally, a decoupled approach to mapping space exploration is introduced to reduce the search space size and optimize data movement, implemented in a tool called Marvel.\",\n    \"Direct Inspiration\": {\n        \"b23\": 1.0,\n        \"b24\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.8,\n        \"b33\": 0.8,\n        \"b53\": 0.8,\n        \"b43\": 0.7,\n        \"b15\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.6,\n        \"b16\": 0.6,\n        \"b42\": 0.6,\n        \"b47\": 0.6,\n        \"b21\": 0.6\n    }\n}\n```"], "5ff880e391e011c83266b1aa": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the high energy consumption of mobile devices during gaming due to constant hardware interactions and event-driven computations. The proposed solution, SNIP (Selecting Necessary InPuts), aims to reduce energy consumption by selectively processing only necessary events, leveraging memoization techniques to avoid redundant computations. The solution involves profiling mobile applications to build lookup tables that capture necessary inputs, using machine learning techniques to shrink these tables for practical use.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 0.9,\n    \"b5\": 0.85,\n    \"b6\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b4\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing energy consumption in mobile gaming by introducing SNIP, a novel approach that avoids processing redundant and repeated events. The core idea involves selective event processing and leveraging a machine learning technique (PFI) to shrink memoization tables.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b6\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the significant energy consumption in mobile devices caused by gaming applications due to redundant event processing. The proposed solution, SNIP (Selecting Necessary InPuts), aims to reduce end-to-end energy consumption by selectively processing only necessary events. This involves the use of a machine learning technique called Permutation Feature Importance (PFI) to identify essential inputs and reduce the size of memoization tables, thereby minimizing redundant computations.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b3\": 1.0,\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b10\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b11\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of excessive energy consumption in mobile devices during gaming by developing a holistic solution called SNIP (Selecting Necessary InPuts). The primary inspiration is the selective event processing to avoid redundant computations, which can save significant energy. The technique is inspired by memoization but extends beyond to handle large granularities and redundant events.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b12\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.4,\n    \"b7\": 0.3,\n    \"b8\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the high energy consumption of mobile devices during gaming due to redundant event processing. The proposed solution, SNIP (Selecting Necessary InPuts), aims to reduce energy consumption by selectively processing only necessary events using a memoization technique enhanced by machine learning for feature importance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.8,\n    \"b4\": 0.85,\n    \"b5\": 0.95,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b7\": 0.6,\n    \"b8\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.55,\n    \"b10\": 0.55,\n    \"b11\": 0.55,\n    \"b12\": 0.6,\n    \"b13\": 0.7,\n    \"b14\": 0.6\n  }\n}\n```"], "616e99985244ab9dcb2a15e3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting drug-drug interactions (DDIs), overcoming limitations such as the use of fewer data sources, imbalanced datasets, and the inability to predict new drugs. The proposed model, HIN-DDI, integrates multiple data sources and leverages a heterogeneous information network (HIN) with meta-path-based topological features to enhance prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b1\": 1.0,\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenges of predicting drug-drug interactions (DDIs) by proposing the HIN-DDI model. The challenges include utilizing multiple data sources, addressing imbalanced datasets, and predicting interactions for new drugs. The model integrates various data sources, creates a heterogeneous information network (HIN), and uses meta-paths to capture complex drug interactions.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.7,\n    \"b24\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b1\": 0.65,\n    \"b2\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined are utilizing fewer data sources, imbalanced datasets, and lack of predicting new drugs. The paper proposes a novel DDI prediction model, HIN-DDI, which integrates multiple data sources, incorporates drug-based interactions on a heterogeneous information network (HIN), uses meta-path topological features, addresses imbalanced data distribution, and predicts new drugs.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b10\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b0\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses key challenges in DDI prediction, including the utilization of fewer data sources, imbalanced datasets, and the lack of prediction for new drugs. The proposed model, HIN-DDI, integrates multiple data sources into a heterogeneous information network (HIN), utilizes meta-paths to extract topological features, and employs machine learning models to predict DDIs.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.95,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.85,\n    \"b1\": 0.8,\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.75,\n    \"b9\": 0.75,\n    \"b10\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in drug-drug interaction (DDI) prediction, including limited data sources, imbalanced datasets, and prediction for new drugs. It proposes a novel model, HIN-DDI, which integrates multiple data sources using heterogeneous information networks (HIN) and meta-path topological features to improve prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.8,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b8\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.65\n  }\n}\n```"], "6093be5091e01172572d298f": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of representing hardware dataflows in tensor computations on spatial architectures, proposing a novel relation-centric notation called TENET. This notation aims to cover a complete design space and enable accurate performance modeling, overcoming the limitations of existing compute-centric and data-centric notations.\",\n    \"Direct Inspiration\": [\"b78\", \"b95\", \"b64\"],\n    \"Indirect Inspiration\": [\"b63\", \"b67\"],\n    \"Other Inspiration\": [\"b61\", \"b70\", \"b81\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper include the limitations of existing dataflow notations (compute-centric and data-centric) for tensor operations on spatial architectures, such as expressiveness and accurate performance modeling. The proposed solution is TENET, a framework that uses relation-centric notation to model hardware dataflow, data assignment, interconnection, and spacetime-stamp map relations, providing more optimization opportunities and accurate performance metrics.\",\n  \"Direct Inspiration\": {\n    \"1\": \"b64\",\n    \"2\": \"b78\",\n    \"3\": \"b95\"\n  },\n  \"Indirect Inspiration\": {\n    \"1\": \"b67\",\n    \"2\": \"b63\"\n  },\n  \"Other Inspiration\": {\n    \"1\": \"b81\",\n    \"2\": \"b61\"\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces TENET, a framework for modeling tensor computation dataflows on spatial architectures. It addresses the limitations of existing compute-centric and data-centric notations by proposing a relation-centric notation that provides a more expressive and complete design space. TENET allows accurate performance modeling, covering data reuse, latency, and bandwidth requirements. The framework includes definitions for dataflow, data assignment, interconnection, and space-time-stamp mapping relations.\",\n  \"Direct Inspiration\": {\n    \"b63\": 0.9,\n    \"b64\": 0.9,\n    \"b78\": 0.85,\n    \"b95\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b67\": 0.8,\n    \"b70\": 0.75,\n    \"b81\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b61\": 0.7,\n    \"b58\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of representing and optimizing hardware dataflows for tensor computations on spatial architectures. It proposes TENET, a relation-centric notation that aims to cover the complete design space of hardware dataflows and provides accurate performance modeling for various hardware metrics.\",\n  \"Direct Inspiration\": {\n    \"b78\": 1,\n    \"b95\": 1,\n    \"b63\": 1,\n    \"b64\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b67\": 0.9,\n    \"b68\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b61\": 0.8,\n    \"b70\": 0.8,\n    \"b58\": 0.7,\n    \"b81\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the need for a formal notation to represent hardware dataflow in spatial architectures for tensor operations. It introduces TENET, a framework that employs a relation-centric notation to model dataflows, data assignments, and interconnections systematically. The primary challenges include the limited expressiveness and performance modeling capabilities of existing compute-centric and data-centric notations. TENET aims to provide a complete design space and accurate performance metrics for hardware dataflows.\",\n  \"Direct Inspiration\": {\n    \"b63\": 0.9,\n    \"b64\": 0.9,\n    \"b78\": 0.9,\n    \"b95\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b67\": 0.7,\n    \"b56\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b58\": 0.6,\n    \"b81\": 0.6,\n    \"b72\": 0.6\n  }\n}\n```"], "61f2175b5244ab9dcb983a01": ["```json\n{\n    \"Summary\": \"The paper discusses the challenges of optimizing processes in a macroeconomic environment for businesses facing increased competition and informed customers. It proposes using discrete-event simulation to evaluate different scenarios without risking physical changes or large investments. The case study focuses on a mattress production process and involves data gathering, model validation, scenario analysis, and assessment of improvement proposals.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1,\n        \"b3\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.9,\n        \"b6\": 0.8,\n        \"b7\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.7,\n        \"b8\": 0.7,\n        \"b9\": 0.9\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing processes in an uncertain macroeconomic environment. It proposes using discrete-event simulation to evaluate different scenarios without making physical changes or large investments. The primary challenge is to increase demand by up to 20% while dealing with current issues like delivery delays, accumulated inventories, and worker overload. The paper uses a case study of a mattress production process to demonstrate the effectiveness of the simulation model.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing processes in a mattress production company using discrete-event simulation. The primary focus is on reducing delays, managing inventories, and improving worker utilization to cope with increased demand. The algorithm proposed involves using discrete-event simulation to model and validate different production scenarios without making physical changes.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b3\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b5\": 0.75,\n    \"b6\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing manufacturing processes in a competitive and uncertain macroeconomic environment. It introduces discrete-event simulation as a tool for decision support, specifically in a mattress production process, to evaluate different scenarios and improve productivity without significant physical changes or investments.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.75,\n    \"b6\": 0.7,\n    \"b7\": 0.65,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing production processes in a competitive and resource-constrained environment using discrete-event simulation. Key contributions include the identification of inefficiencies and the testing of scenarios to improve productivity in a mattress production process.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5\n  }\n}\n```"], "6006a7e491e0111a1b6a21c9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of long-form text matching, particularly the issues of noise, computational expense, and sentence-level structure in text. The authors propose a hierarchical noise filtering model, Match-Ignition, which utilizes PageRank for both sentence-level and word-level noise filtering. The model aims to distill significant matching signals and improve the efficiency and performance of long-form text matching.\",\n  \n  \"Direct Inspiration\": [\"b2\", \"b4\", \"b9\", \"b35\"],\n  \n  \"Indirect Inspiration\": [\"b5\", \"b15\", \"b13\"],\n  \n  \"Other Inspiration\": [\"b11\", \"b30\", \"b10\", \"b22\", \"b31\", \"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of long-form text matching, particularly the issue of noise affecting performance and efficiency. The proposed solution, Match-Ignition, uses a hierarchical noise filtering model based on the PageRank algorithm to distill significant matching signals. This involves sentence-level and word-level noise filtering processes, leveraging the Transformer architecture for contextual information capture.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b18\": 0.8,\n    \"b12\": 0.85,\n    \"b23\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b9\": 0.75,\n    \"b35\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b34\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of long-form text matching, which differs significantly from short-form text matching due to noise and the need for capturing global semantic meanings. The authors propose a hierarchical noise filtering model called Match-Ignition, utilizing the PageRank algorithm at both sentence and word levels to eliminate noise and improve matching efficiency and effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b15\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b12\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.65,\n    \"b9\": 0.65,\n    \"b35\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the primary challenge of noise in long-form text matching which affects both performance and efficiency. It proposes a novel hierarchical noise filtering model named Match-Ignition that leverages the PageRank algorithm to distill significant matching signals at both the sentence and word levels.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1.0,\n        \"b12\": 0.9,\n        \"b23\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.7,\n        \"b9\": 0.7,\n        \"b35\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.6,\n        \"b5\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the noise in long-form text matching tasks, which affects both performance and efficiency. The authors propose a novel hierarchical noise filtering model, namely Match-Ignition, which uses PageRank for noise reduction at both the sentence and word levels. The model aims to distill significant matching signals and improve the matching process by eliminating less important nodes (noise) from the text.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b4\": 0.9,\n    \"b9\": 0.9,\n    \"b12\": 1,\n    \"b23\": 1,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b6\": 0.7,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b13\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b18\": 0.7,\n    \"b20\": 0.6,\n    \"b22\": 0.7,\n    \"b27\": 0.7,\n    \"b29\": 0.7,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b33\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b3\": 0.5,\n    \"b8\": 0.5,\n    \"b19\": 0.5,\n    \"b24\": 0.5,\n    \"b25\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```"], "619ba3461c45e57ce9780201": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of modeling and forecasting multiphysics and multiscale systems, particularly in the context of the Earth system, using data-driven methods and machine learning. It highlights the limitations of traditional approaches to solving inverse problems and incorporating multi-fidelity data into physical models. The proposed solution involves leveraging machine learning, particularly deep learning and physics-informed learning, to integrate observational data with fundamental physical laws.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b16\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of modeling and forecasting multiphysics and multiscale systems, particularly in the context of the Earth's system. It highlights the limitations of traditional computational methods and introduces machine learning (ML), especially physics-informed neural networks (PINNs), as a transformative approach to integrate observational data and physical models.\",\n  \"Direct Inspiration\": [\n    \"b6\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b1\",\n    \"b2\",\n    \"b3\",\n    \"b4\"\n  ],\n  \"Other Inspiration\": [\n    \"b7\",\n    \"b8\",\n    \"b11\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of modelling and forecasting dynamics in multiphysics and multiscale systems, particularly emphasizing the integration of machine learning (ML) with traditional physical models to overcome these challenges. It introduces physics-informed neural networks (PINNs) as a novel approach to embed physical laws into ML models, aiming to improve interpretability and robustness.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b2\": 0.7,\n        \"b3\": 0.8,\n        \"b4\": 0.8,\n        \"b16\": 0.6,\n        \"b18\": 0.7,\n        \"b19\": 0.7,\n        \"b36\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.5,\n        \"b7\": 0.6,\n        \"b8\": 0.6,\n        \"b9\": 0.5,\n        \"b10\": 0.5,\n        \"b20\": 0.6,\n        \"b21\": 0.6,\n        \"b22\": 0.6,\n        \"b23\": 0.6,\n        \"b24\": 0.6,\n        \"b25\": 0.6,\n        \"b26\": 0.6,\n        \"b27\": 0.5,\n        \"b28\": 0.5,\n        \"b29\": 0.5,\n        \"b30\": 0.5,\n        \"b31\": 0.5,\n        \"b32\": 0.5,\n        \"b33\": 0.5,\n        \"b34\": 0.5,\n        \"b35\": 0.5,\n        \"b37\": 0.5,\n        \"b38\": 0.5,\n        \"b39\": 0.5,\n        \"b40\": 0.5,\n        \"b41\": 0.5,\n        \"b42\": 0.5,\n        \"b43\": 0.5,\n        \"b44\": 0.5,\n        \"b45\": 0.5,\n        \"b46\": 0.5,\n        \"b47\": 0.5,\n        \"b49\": 0.5,\n        \"b50\": 0.5,\n        \"b51\": 0.5,\n        \"b52\": 0.5,\n        \"b53\": 0.5,\n        \"b54\": 0.5,\n        \"b56\": 0.5,\n        \"b75\": 0.5,\n        \"b102\": 0.5,\n        \"b153\": 0.5,\n        \"b172\": 0.5,\n        \"b195\": 0.5,\n        \"b203\": 0.5,\n        \"b204\": 0.5,\n        \"b205\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of modelling and forecasting the dynamics of multiphysics and multiscale systems, particularly focusing on integrating machine learning (ML) and physics-informed learning for predictive modelling. The primary challenges include handling nonlinear multiscale systems, dealing with missing or noisy boundary conditions, and incorporating multi-fidelity observational data into physical models. The proposed solution involves leveraging deep learning approaches, particularly physics-informed neural networks (PINNs), to integrate data and mathematical models, ensuring physically consistent predictions even with imperfect data.\",\n  \"Direct Inspiration\": {\n    \"0\": 0.9,\n    \"1\": 0.95,\n    \"2\": 0.85,\n    \"3\": 0.8,\n    \"4\": 0.75,\n    \"5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"6\": 0.7,\n    \"7\": 0.65,\n    \"8\": 0.7,\n    \"9\": 0.6,\n    \"10\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"11\": 0.55,\n    \"12\": 0.6,\n    \"13\": 0.55,\n    \"14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of modeling and forecasting dynamics in multiphysics and multiscale systems, emphasizing the integration of machine learning with physical laws to overcome limitations of traditional methods. The primary goal is to develop physics-informed learning algorithms that can provide interpretable and physically consistent predictions, even with imperfect data.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b30\": 0.9,\n    \"b47\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b49\": 0.6,\n    \"b51\": 0.6\n  }\n}\n```"], "61bff4275244ab9dcb79c78b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating image compression and image analysis, specifically targeting both human and machine vision. It leverages a novel paradigm combining learning-based image compression with Transformer-based image analysis to create a synergy between these tasks. The proposed method redesigns the Vision Transformer (ViT) model to perform image classification directly from compressed features, bypassing image reconstruction, and enhances compression performance using long-term information from the Transformer.\",\n  \"Direct Inspiration\": [\"b36\"],\n  \"Indirect Inspiration\": [\"b34\", \"b3\", \"b0\", \"b12\", \"b37\"],\n  \"Other Inspiration\": [\"b42\", \"b44\", \"b47\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper include the high computational cost of Transformer-based models, the integration of image compression with image analysis, and the difficulty in optimizing both tasks for machine vision and human vision.\",\n    \"inspirations\": \"The paper is inspired by previous work on learning-based image compression and Transformer-based image analysis, particularly the integration and joint optimization of these tasks.\"\n  },\n  \"Direct Inspiration\": [\"b36\"],\n  \"Indirect Inspiration\": [\"b34\", \"b3\", \"b0\"],\n  \"Other Inspiration\": [\"b12\", \"b37\", \"b42\", \"b44\", \"b47\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of integrating image compression and analysis for both human and machine vision using a novel Transformer-based approach. It proposes an end-to-end model that performs image classification directly from compressed features, optimizing for both rate-distortion and rate-accuracy using a variational auto-encoder and information bottleneck framework.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1,\n    \"b34\": 0.9,\n    \"b3\": 0.9,\n    \"b0\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b37\": 0.8,\n    \"b47\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b6\": 0.6,\n    \"b4\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating image compression with image analysis to improve both human and machine vision. The authors propose a novel paradigm that combines learning-based image compression with Transformer-based image analysis, which enhances the performance of both tasks. Key contributions include the end-to-end model for image compression and analysis, rate-distortion-accuracy optimization based on VAE and IB, and redesigning the ViT model with a CNN-based image encoder.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b3\": 0.85,\n    \"b34\": 0.8,\n    \"b0\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.7,\n    \"b24\": 0.65,\n    \"b47\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating image compression with image analysis for both human and machine vision, using a novel end-to-end model combining learning-based image compression and Transformer-based image analysis. The proposed model redesigns the Vision Transformer (ViT) to perform image classification from compressed features and introduces a feature aggregation module for improved image reconstruction. The key contributions include the development of a rate-distortion-accuracy optimization framework based on variational auto-encoder (VAE) and information bottleneck (IB).\",\n  \"Direct Inspiration\": [\"b36\"],\n  \"Indirect Inspiration\": [\"b34\", \"b3\", \"b0\"],\n  \"Other Inspiration\": [\"b12\", \"b42\", \"b44\", \"b47\"]\n}\n```"], "61b022bc5244ab9dcb5c4d5a": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper revolve around effectively capturing and aggregating temporal and proposal-level context in video data. The difficulties include handling the boundary-level context for different action speeds and managing the overlap and semantic density issues in proposal-level context aggregation.\",\n    \"Inspirations\": \"The paper introduces a novel Dual Context Aggregation Network (DCAN) to address these challenges, inspired by existing methods like BMN for proposal generation and context aggregation.\"\n  },\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b6\": 0.6,\n    \"b2\": 0.6,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of temporal action detection in untrimmed videos by proposing a novel Dual Context Aggregation Network (DCAN). The primary challenges include capturing complex semantic changes between frames and efficiently aggregating context at both boundary and proposal levels. The proposed solution, DCAN, includes the Multi-Path Temporal Context Aggregation (MTCA) for boundary-level context aggregation and Coarse-to-fine Matching (CFM) for proposal-level context aggregation.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b6\": 0.6,\n    \"b2\": 0.6,\n    \"b24\": 0.6,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of temporal action detection in videos, specifically focusing on boundary-level and proposal-level context aggregation. The proposed solution, Dual Context Aggregation Network (DCAN), introduces novel methods: Multi-Path Temporal Context Aggregation (MTCA) for boundary-level context aggregation and Coarse-to-fine Matching (CFM) for proposal-level context aggregation.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b24\": 0.5,\n    \"b32\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the difficulties in boundary-level and proposal-level context aggregation in temporal action detection. The novel method introduced to tackle these challenges is the Dual Context Aggregation Network (DCAN), which includes the Multi-Path Temporal Context Aggregation (MTCA) and Coarse-to-fine Matching (CFM) modules.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b17\": 0.75,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b2\": 0.65,\n    \"b0\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in temporal action detection in videos, particularly focusing on context aggregation at boundary and proposal levels. The authors propose a Dual Context Aggregation Network (DCAN) with Multi-Path Temporal Context Aggregation (MTCA) and Coarse-to-fine Matching (CFM) to improve temporal proposal generation and enhance the expressiveness and robustness of matching contexts.\",\n  \"Direct Inspiration\": [\"b13\"],\n  \"Indirect Inspiration\": [\"b17\", \"b24\"],\n  \"Other Inspiration\": [\"b0\", \"b7\", \"b6\"]\n}\n```"], "60782ec791e011f5ecc9dc26": ["```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in Graph Neural Networks (GNNs): the architecture challenge and the computational challenge. The architecture challenge is the difficulty in finding a single model that performs best across various tasks and datasets. The computational challenge is the high cost of the neural architecture search (NAS) process due to the trial-and-error nature of training multiple candidate architectures. To tackle these, the authors propose the SANE framework, which defines a novel search space for GNNs and employs a differentiable search algorithm to reduce computational costs.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b22\", \"b23\", \"b26\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b18\", \"b19\", \"b24\", \"b25\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b20\", \"b21\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in Graph Neural Networks (GNNs): architecture and computational challenges. It proposes a novel NAS framework called SANE, which defines a novel search space to emulate more human-designed GNN architectures and adopts a differentiable search algorithm to reduce computational cost. The effectiveness and efficiency of SANE are demonstrated through extensive experiments.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b23\": 1.0,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses two primary challenges in Graph Neural Networks (GNNs): the architecture challenge and the computational challenge. It introduces the SANE framework that uses Neural Architecture Search (NAS) to automatically design efficient GNN architectures. The key innovations include a novel search space that emulates more human-designed GNN architectures, and a differentiable architecture search algorithm that reduces computational cost.\",\n    \"Direct Inspiration\": {\n        \"b22\": 1.0,\n        \"b23\": 1.0,\n        \"b26\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.8,\n        \"b19\": 0.8,\n        \"b24\": 0.8,\n        \"b25\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.7,\n        \"b16\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in Graph Neural Networks (GNNs): the architecture challenge and the computational challenge. The architecture challenge involves the difficulty of finding a single GNN model that performs best across various tasks and datasets. The computational challenge involves the high computational cost of existing Neural Architecture Search (NAS) approaches for GNNs due to their trial-and-error nature. The proposed framework, SANE, uses a novel and expressive search space and a differentiable search algorithm to mitigate these challenges.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b23\": 1,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b16\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\"architecture challenge\", \"computational challenge\"],\n    \"proposed_algorithm\": \"SANE framework based on NAS with a novel and expressive search space and a differentiable search algorithm\"\n  },\n  \"Direct Inspiration\": [\"b22\", \"b23\", \"b26\"],\n  \"Indirect Inspiration\": [\"b18\", \"b24\"],\n  \"Other Inspiration\": [\"b16\", \"b15\"]\n}\n```"], "60641d939e795e72406b6661": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in predicting customer value by incorporating social network information. The primary challenges include unclear predictive signals from network structures, limited knowledge of higher-order information, and complex effects of customer attributes and social influence. The proposed solution, MAG, uses motif-based multi-view graph attention networks and gated fusion to jointly model customer attributes and friends' influence.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b47\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.75,\n    \"b61\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b16\": 0.65,\n    \"b56\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in predicting customer value by incorporating social network information, focusing on the unclear predictive signals of network structures, limited knowledge of higher-order information, and complex effects of customer attributes and social influence. The proposed algorithm, Motif-based Multi-view Graph Attention Networks with Gated Fusion (MAG), uses motifs to capture higher-order interactions in social networks and employs gated fusion to model the interplay between customer attributes and social influence.\",\n    \"Direct Inspiration\": {\n        \"b33\": 1.0,\n        \"b47\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.8,\n        \"b56\": 0.8,\n        \"b37\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.7,\n        \"b49\": 0.7,\n        \"b39\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting customer value by incorporating social network information into the prediction models. The authors propose a novel framework called Motif-based Multi-view Graph Attention Networks with Gated Fusion (MAG), which jointly models the effects of customer attributes and social influence on customer value. The key challenges include extracting useful signals from social network structures, capturing higher-order information, and modeling the complex effects of customer attributes and social influence. The proposed framework leverages motifs to capture higher-order structures and uses attention mechanisms and gated fusion to enhance prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b47\": 0.8,\n    \"b61\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b40\": 0.6,\n    \"b7\": 0.5,\n    \"b38\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of predicting customer value by incorporating social network information, specifically higher-order structures, into customer value prediction models. The proposed solution is a framework called Motif-based Multi-view Graph Attention Networks with Gated Fusion (MAG), which jointly models customer attributes and social influence to improve prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b16\": 0.7,\n    \"b56\": 0.7,\n    \"b47\": 0.9,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenges of predicting customer value by incorporating social network information into customer value prediction models. The primary challenges include extracting useful signals from social network structures, capturing higher-order information, and modeling the complex effects of customer attributes and social influence. The proposed solution is a novel framework, MAG (Motif-based Multi-view Graph Attention Networks with Gated Fusion), which leverages motif-based multi-view graphs and attention mechanisms to fuse multi-view graph embeddings and uses gated fusion to model the effects of customer attributes and social influence.\",\n  \"Direct Inspiration\": [\n    \"b33\",\n    \"b47\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b32\",\n    \"b6\",\n    \"b37\",\n    \"b56\"\n  ],\n  \"Other Inspiration\": [\n    \"b1\",\n    \"b61\"\n  ]\n}\n```"], "5ff6842ad4150a363cbd014e": ["```json\n{\n  \"Summary\": \"This paper addresses the out-of-knowledge-base (OOKB) entity problem in knowledge base completion (KBC) tasks, proposing a parameter-efficient embedding model that combines graph neural network (GNN) and convolutional neural network (CNN) structures. The model replaces the transition weight matrix in GNN with a multilayer convolutional network to compute embeddings for OOKB entities while maintaining parameter efficiency.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.9,\n    \"b24\": 0.9,\n    \"b25\": 0.9,\n    \"b26\": 0.9,\n    \"b27\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of knowledge base completion (KBC) with out-of-knowledge-base (OOKB) entities by proposing a parameter-efficient embedding model that combines the benefits of Graph Neural Networks (GNN) and Convolutional Neural Networks (CNN). The algorithm aims to handle the OOKB entity problem by replacing the transition weight matrix in GNN with a multilayer convolutional network, thereby improving parameter efficiency and maintaining high performance. The paper's main contributions include an end-to-end model that combines GNN and CNN structures, a novel method for knowledge transfer for OOKB entities, and verification of the model's effectiveness on OOKB datasets.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b26\": 0.9,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.6,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of knowledge base completion (KBC) with out-of-knowledge-base (OOKB) entities by proposing a parameter-efficient embedding model that combines the benefits of Graph Neural Networks (GNN) and Convolutional Neural Networks (CNN). The novel approach involves replacing the transition weight matrix in GNNs with a multilayer convolutional network to handle OOKB entities efficiently.\",\n  \"Direct Inspiration\": [\"b28\"],\n  \"Indirect Inspiration\": [\"b23\", \"b25\", \"b26\", \"b27\"],\n  \"Other Inspiration\": [\"b6\", \"b24\", \"b33\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the Knowledge Base Completion (KBC) task, specifically focusing on the out-of-knowledge-base (OOKB) entity problem. The authors propose a parameter-efficient embedding model combining the benefits of Graph Neural Networks (GNN) and Convolutional Neural Networks (CNN) to improve performance and parameter efficiency. The primary challenges include handling OOKB entities, reducing memory requirements, and maintaining high accuracy and parameter efficiency.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b24\": 0.8,\n    \"b25\": 0.8,\n    \"b26\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the out-of-knowledge-base (OOKB) entity problem in knowledge base completion (KBC). The authors propose a parameter-efficient embedding model that combines the benefits of Graph Neural Networks (GNN) and Convolutional Neural Networks (CNN) by replacing the transition weight matrix in GNN with a multilayer convolutional network. This novel approach is designed to handle OOKB entities more effectively while maintaining computational efficiency.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b24\": 0.8,\n    \"b25\": 0.8,\n    \"b26\": 0.8,\n    \"b27\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7\n  }\n}\n```"], "61e14c9f5244ab9dcb008bd6": ["```json\n{\n  \"Summary\": \"The paper primarily addresses the performance overheads caused by deoptimization checks in the V8 JavaScript engine during JIT compilation. It proposes an analysis methodology and explores potential HW/SW codesign optimizations to reduce these overheads.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b3\": 1.0,\n    \"b4\": 0.9,\n    \"b6\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b14\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the runtime overheads and micro-architectural implications of deoptimization checks in the V8 JavaScript engine, particularly in the context of JIT compilation. The paper proposes novel methods to analyze and optimize these checks, leveraging both statistical sampling and direct modification of the V8 compiler to measure and reduce overheads. The extended JetStream2 benchmark suite is used to validate their findings.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b3\": 0.9,\n    \"b6\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b10\": 0.6,\n    \"b11\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.4,\n    \"b9\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the complexity of JavaScript's Just-In-Time (JIT) compilation and the overhead caused by deoptimization checks in the V8 engine. It proposes methods to analyze and reduce this overhead, focusing on hardware/software co-design optimizations and statistical inference techniques.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b2\": 0.85,\n    \"b4\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.65,\n    \"b11\": 0.6,\n    \"b13\": 0.55\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are the complex runtime and performance overheads associated with deoptimization checks in the Just-In-Time (JIT) compilation process of the V8 JavaScript engine. The paper proposes an in-depth analysis of these overheads, categorizes different types of deoptimization checks, and evaluates their impact on performance using various benchmarks. The authors also explore solutions like HW/SW codesign to optimize these checks.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b3\": 0.85,\n        \"b4\": 0.75,\n        \"b6\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.65,\n        \"b9\": 0.6,\n        \"b10\": 0.6,\n        \"b11\": 0.55\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the performance overhead caused by deoptimization checks in the V8 JavaScript engine, especially focusing on the Just-In-Time (JIT) compilation process and its impact on modern CPU architectures. The authors propose a detailed categorization of these checks, measure their runtime costs, and propose HW/SW codesign optimizations to reduce overhead.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.8,\n    \"b6\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b9\": 0.65,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b5\": 0.4\n  }\n}\n```"], "5ff880e391e011c83266b1a3": ["```json\n{\n  \"Summary\": \"This paper addresses the challenges of characterizing and understanding I/O behaviors in cloud block storage systems to enable performance optimizations and efficient resource provisioning. The paper introduces a novel trace analysis methodology to tackle these challenges and uses three sets of block-level I/O traces from different production environments (AliCloud, TencentCloud, and MSRC). The key contributions include identifying read/write patterns, load balancing issues, cache efficiency, and storage cluster management.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1,\n    \"b20\": 0.9,\n    \"b45\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b36\": 0.7,\n    \"b47\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper include characterizing and understanding the I/O behaviors of applications in cloud block storage systems, addressing load balancing, cache efficiency, and storage cluster management, and dealing with the limitations of existing I/O traces such as MSRC which may not reflect the I/O behaviors of modern cloud environments.\",\n    \"inspirations\": \"The paper is inspired by various previous works on I/O trace analysis, storage system designs, and optimizations in cloud environments.\"\n  },\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b32\": 1,\n    \"b52\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b9\": 0.8,\n    \"b11\": 0.8,\n    \"b22\": 0.8,\n    \"b35\": 0.8,\n    \"b36\": 0.8,\n    \"b45\": 0.8,\n    \"b47\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b24\": 0.6,\n    \"b26\": 0.6,\n    \"b31\": 0.6,\n    \"b39\": 0.6,\n    \"b41\": 0.6,\n    \"b53\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Characterizing and understanding I/O behaviors of applications in production environments for performance optimizations and efficient resource provisioning in cloud block storage systems.\",\n    \"inspirations\": \"The need to address differences in I/O characteristics between traditional enterprise data centers and modern cloud environments, considering diverse application behaviors and write-dominant workloads.\"\n  },\n  \"Direct Inspiration\": [\n    \"b32\",\n    \"b31\",\n    \"b24\",\n    \"b45\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b9\",\n    \"b22\",\n    \"b23\",\n    \"b36\",\n    \"b20\",\n    \"b11\",\n    \"b52\"\n  ],\n  \"Other Inspiration\": [\n    \"b14\",\n    \"b26\",\n    \"b47\",\n    \"b48\",\n    \"b4\",\n    \"b43\",\n    \"b16\",\n    \"b30\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of characterizing and understanding I/O behaviors in cloud block storage systems for performance optimizations and efficient resource provisioning. It introduces a trace analysis methodology to address design considerations such as load balancing, cache efficiency, and storage cluster management. The study is inspired by the limitations of existing block-level I/O traces, particularly the MSRC traces, and aims to provide more relevant insights into modern cloud environments using new traces from AliCloud and TencentCloud.\",\n  \"Direct Inspiration\": [\n    \"b32\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b31\",\n    \"b45\",\n    \"b18\"\n  ],\n  \"Other Inspiration\": [\n    \"b0\",\n    \"b20\",\n    \"b36\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in this paper involve understanding and optimizing I/O behaviors in modern cloud block storage systems, which are distinct from traditional enterprise data centers due to more diverse and write-dominant workloads. The paper proposes an in-depth trace analysis methodology to address load balancing, cache efficiency, and storage cluster management in cloud environments. The study leverages three sets of block-level I/O traces from AliCloud, TencentCloud, and MSRC to compare and contrast I/O characteristics.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b32\": 1,\n    \"b31\": 0.9,\n    \"b45\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b11\": 0.7,\n    \"b47\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5,\n    \"b36\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```"], "61fb47e05aee126c0f8739a2": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of hardware-software co-design for hardware accelerators, proposing a flexible and rapid simulation framework integrated with MLIR that allows for simulation at multiple levels of detail through the EQueue dialect. This approach aims to improve the design and prototyping process for hardware accelerators by leveraging a compiler infrastructure.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the need for rapid and flexible simulation of hardware accelerators during their co-design with accompanying software stacks. The novel contribution is a general framework for implementing high-level simulators using the MLIR infrastructure, featuring the EQueue dialect which represents hardware accelerators at various levels of detail and enables efficient simulation.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Rapid iteration of hardware-software co-design\",\n      \"Flexibility and accuracy in hardware simulators\",\n      \"Integration with compiler stacks for performance measurement\"\n    ],\n    \"Inspirations\": [\n      \"Combining hardware simulation with MLIR to leverage existing transformations and analyses\",\n      \"Event-based control for representing memory allocation, data movement, and parallel execution\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b19\",\n    \"b31\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b27\",\n    \"b36\"\n  ],\n  \"Other Inspiration\": [\n    \"b3\",\n    \"b14\",\n    \"b15\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of rapidly simulating proposed hardware accelerators for efficient hardware-software co-design. It introduces a general framework using the EQueue dialect within the MLIR infrastructure to provide flexible, accurate, and low-effort high-level simulation of hardware accelerators. This approach allows simulation at multiple abstraction levels and decouples architecture changes from simulation logic.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b26\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the difficulty of rapidly simulating proposed hardware accelerators in early design phases, the need for flexibility in custom high-level simulators, and the lack of integration of traditional simulators with compiler stacks. The authors propose a novel framework for high-level simulation of hardware accelerators using the MLIR infrastructure, focusing on the EQueue dialect for representing accelerators at various detail levels and a generic simulation engine for execution.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b27\": 0.8,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```"], "613586d45244ab9dcbd3a2fa": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving zero-shot learning performance in large language models by introducing a method called instruction tuning. The proposed solution involves finetuning a pretrained language model on a mixture of NLP tasks described using natural language instructions. This method, termed FLAN, significantly enhances the zero-shot capabilities of the base model, outperforming GPT-3 on various tasks.\",\n  \"Direct Inspiration\": [\"b11\"],\n  \"Indirect Inspiration\": [\"b33\", \"b57\", \"b65\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is improving the zero-shot performance of large language models, which typically perform poorly without few-shot exemplars. The proposed solution is instruction tuning, where a pretrained language model is finetuned on a mixture of more than 60 NLP tasks expressed via natural language instructions. This approach, termed FLAN, demonstrated substantial improvements in zero-shot performance across a variety of tasks.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b57\": 0.9,\n    \"b65\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is improving the zero-shot learning performance of large language models. The proposed algorithm involves instruction tuning, where a pretrained language model is finetuned on a mixture of NLP tasks expressed via natural language instructions. This method aims to enhance the model's ability to follow instructions and perform well on unseen tasks.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7,\n    \"b57\": 0.7,\n    \"b65\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of improving zero-shot learning performance in large language models by leveraging natural language instructions. The proposed method, FLAN, involves instruction tuning a pretrained language model on a mixture of over 60 NLP tasks formatted as natural language instructions. The results show significant improvements in zero-shot performance compared to the base model and GPT-3.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b57\": 0.9,\n    \"b65\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the improvement of zero-shot learning performance in large language models, such as GPT-3, which currently excel in few-shot learning but underperform in zero-shot tasks. The proposed solution, referred to as Finetuned Language Net (FLAN), involves instruction tuning\u2014fine-tuning a pretrained language model on a variety of NLP tasks expressed via natural language instructions. This approach aims to enhance the model's ability to follow instructions and perform well on unseen tasks.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b57\": 0.8,\n    \"b65\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7\n  }\n}\n```"], "600ff2eb91e011256c9560c7": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the optimization of convolutional neural networks (CNNs) for deep neural network (DNN) pipelines, specifically focusing on the reduction of data movement costs associated with tiling loop permutations. The proposed algorithm introduces a novel analytical modeling approach to explore the entire search space of permutations and tile sizes, dramatically pruning the space of possibilities and solving the optimization problem using off-the-shelf solvers. This approach aims to improve performance across various hardware platforms, including multicore CPUs, GPUs, TPUs, FPGAs, and accelerator arrays.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b24\": 0.6,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.8,\n    \"b37\": 0.7,\n    \"b12\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge outlined in the paper is the optimization of convolutional neural network (CNN) stages in deep neural network (DNN) pipelines, specifically targeting the reduction of data movement cost and the vast search space of possible tiled loop variants.\",\n    \"inspirations\": \"The paper develops a novel analytical modeling approach for data movement in multi-level memory hierarchies to efficiently prune the search space of permutations and tile sizes. It differentiates itself from previous efforts by leveraging algebraic properties for optimization.\"\n  },\n  \"Direct Inspiration\": [\"b6\"],\n  \"Indirect Inspiration\": [\"b4\", \"b5\", \"b8\", \"b9\", \"b12\", \"b17\", \"b19\", \"b21\", \"b24\", \"b29\", \"b33\", \"b35\", \"b36\", \"b37\", \"b38\"],\n  \"Other Inspiration\": [\"b13\", \"b14\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the optimization of Convolutional Neural Networks (CNNs) by developing an analytical model for data movement in multi-level memory hierarchies. The main challenge is the large search space for tile-loop permutations and tile sizes in CNN computations. The proposed method uses algebraic properties to prune the search space and solve non-linear optimization problems for optimal configurations. The approach is demonstrated to achieve high performance on multicore CPUs, outperforming state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b35\": 0.9,\n    \"b21\": 0.9,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b8\": 0.5,\n    \"b37\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing data movement in Convolutional Neural Networks (CNNs) due to the high computational demands and large search space for loop tiling. The authors propose an analytical modeling approach to minimize data movement, which significantly prunes the search space and improves performance compared to state-of-the-art methods.\",\n  \"Direct Inspiration\": [\"b6\", \"b14\", \"b13\", \"b29\"],\n  \"Indirect Inspiration\": [\"b4\", \"b5\", \"b8\", \"b37\"],\n  \"Other Inspiration\": [\"b12\", \"b25\", \"b26\", \"b38\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing the computationally intensive stages of Convolutional Neural Networks (CNNs) by developing a comprehensive analytical model for data movement within a multi-level memory hierarchy. This approach enables a dramatic reduction in the search space for tile-loop permutations and sizes, achieving high performance across various CNN instances using multicore CPUs.\",\n  \"Direct Inspiration\": [\"b6\"],\n  \"Indirect Inspiration\": [\"b4\", \"b5\", \"b8\", \"b11\", \"b19\", \"b29\", \"b33\", \"b36\", \"b37\"],\n  \"Other Inspiration\": [\"b13\", \"b14\", \"b21\", \"b24\", \"b26\", \"b35\"]\n}\n```"], "60cd8ed091e011329faa2405": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving the relevance of e-commerce product retrieval systems by developing a Multi-Grained Deep Semantic Product Retrieval (MGDSPR) model. The key issues are the semantic gap in lexical matching engines, the low controllability of embedding-based retrieval (EBR) systems, and the discrepancy between training and inference stages in current methods. The proposed MGDSPR model uses softmax cross-entropy loss for global comparison and introduces methods to smooth relevance noise and generate relevance-improving hard negative samples.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b33\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.8,\n        \"b20\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.6,\n        \"b30\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving product relevance in e-commerce search engines, particularly during the retrieval phase. It proposes the Multi-Grained Deep Semantic Product Retrieval (MGDSPR) model, which aims to enhance the relevance and efficiency of retrieved product lists by capturing the relationship between query semantics and historical user behaviors. The model uses softmax cross-entropy loss for global comparison, smoothing relevance noise with a temperature parameter, and generating relevance-improving hard negative samples.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b20\": 0.95,\n    \"b33\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.85,\n    \"b28\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.75,\n    \"b17\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in e-commerce search retrieval, particularly the semantic gap and low controllability of relevance. The proposed Multi-Grained Deep Semantic Product Retrieval (MGDSPR) model aims to retrieve more relevant products by incorporating user behavior data and enhancing the embedding-based retrieval (EBR) system. Key methods include using softmax cross-entropy loss for global comparison, smoothing relevance noise with a temperature parameter, and generating hard negative samples.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b20\": 1.0,\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b13\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed by the paper is improving the relevance and efficiency of product retrieval in e-commerce search systems. The proposed solution, Multi-Grained Deep Semantic Product Retrieval (MGDSPR), aims to dynamically capture the relationship between query semantics and personalized user behaviors. The paper introduces several novel methods, including the use of softmax cross-entropy loss for global comparison ability, relevance noise smoothing, and hybrid negative sample generation, to enhance the retrieval process and ensure more relevant product candidates are fed into the ranking stage.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b13\": 0.8,\n        \"b20\": 0.85,\n        \"b33\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.75,\n        \"b28\": 0.7,\n        \"b30\": 0.7,\n        \"b34\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.6,\n        \"b11\": 0.55,\n        \"b24\": 0.5,\n        \"b26\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in e-commerce product search, particularly the need to improve the relevance of retrieved products during the embedding-based retrieval (EBR) phase. The authors propose the Multi-Grained Deep Semantic Product Retrieval (MGDSPR) model, which employs the softmax cross-entropy loss to ensure global comparison ability and introduces methods to smooth relevance noise and generate hard negative samples. The aim is to enhance the controllability of search relevance and increase the number of relevant products in the ranking stage.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b30\": 0.9,\n    \"b1\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b20\": 0.8,\n    \"b33\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.65,\n    \"b17\": 0.6,\n    \"b26\": 0.55\n  }\n}\n```"], "60f15cc25244ab9dcbfdb2c8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of using wireless sensing for freehand exercise monitoring, specifically focusing on the limitations of existing systems like camera-based, wearable devices, and RFIDs. The proposed system, MobiFit, utilizes cellular signals from widely distributed base stations to trace freehand exercises, overcoming issues like the need for exclusive sensing channels, specific exercise areas, and interference among multiple users. The novel contributions include an analytic model for signal propagation, a real-time segmentation scheme, and low-frequency feature extraction for exercise recognition.\",\n  \"Direct Inspiration\": [\"b10\", \"b13\"],\n  \"Indirect Inspiration\": [\"b1\", \"b2\", \"b3\", \"b5\", \"b7\", \"b8\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing fitness assistants and proposes a novel system, MobiFit, which uses cellular signals for tracking and assessing freehand exercises. The primary challenges include managing uncontrollable cellular signals and dealing with time-variant propagation paths. The proposed solution involves focusing on the low-frequency components of received signals and using signal processing techniques like FFT, DWT, and SVM for exercise repetition segmentation and type recognition.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b10\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the limitations of existing fitness assistants by proposing a novel cellular-based exercise assistant system called MobiFit. The primary challenges include dealing with uncontrollable cellular signals and time-variant propagation paths. MobiFit utilizes a spectrogram analysis, Fast Fourier Transform (FFT), Discrete Wavelet Transform (DWT), and Support Vector Machine (SVM) to track exercises and recognize exercise types in real-time, without needing a dedicated wireless channel for sensing.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b11\": 0.9,\n    \"b12\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b21\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes MobiFit, a cellular-based freehand exercise assistant system. It addresses limitations in existing fitness assistants by leveraging cellular signals for passive exercise tracking. MobiFit faces challenges related to uncontrollable cellular signals and time-variant propagation paths. It introduces an analytic model for signal fluctuations, applies spectrogram analysis, FFT, DWT, and SVM for exercise recognition, and demonstrates high accuracy in experiments.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b11\": 0.7,\n    \"b6\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing fitness assistants for freehand exercises, such as those based on cameras, wearable devices, and RFIDs. It proposes a novel cellular-based system, MobiFit, which uses cellular signal fluctuations to monitor and assess freehand exercises. The main challenges include dealing with uncontrollable cellular signals and long signal propagation paths. MobiFit introduces an analytic model for signal analysis and applies FFT, DWT, and SVM for exercise segmentation and type recognition.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b10\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b19\": 0.6,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b25\": 0.5,\n    \"b31\": 0.5,\n    \"b33\": 0.5\n  }\n}\n```"], "619799c291e011c822372aab": ["```json\n{\n    \"Summary\": \"The paper addresses the limitations of WiFi-based wireless sensing, particularly its limited sensing coverage, impact on data communication, and sensitivity to target location and orientation. The authors propose LTE-Track, a new sensing modality leveraging LTE signals, which provides pervasive coverage, frequency diversity, spatial diversity, and high-rate uniform frames. This approach aims to achieve reliable sensing without compromising data communication. The proposed system is validated through applications in indoor respiration sensing and outdoor traffic monitoring, demonstrating significant improvements in sensing performance.\",\n    \"Direct Inspiration\": {\n        \"b48\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8,\n        \"b18\": 0.8,\n        \"b33\": 0.8,\n        \"b35\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.7,\n        \"b15\": 0.7,\n        \"b23\": 0.6,\n        \"b22\": 0.6,\n        \"b34\": 0.6,\n        \"b38\": 0.6,\n        \"b45\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the limited coverage, interference with data communication, and performance sensitivity to target location and orientation in WiFi-based sensing systems. The proposed solution is LTE-Track, which leverages LTE signals' unique features such as pervasive coverage, frequency and spatial diversity, and high-rate uniform frames to achieve pervasive and reliable sensing without compromising data communication. The paper demonstrates the capability of LTE-Track through applications like indoor respiration sensing and outdoor traffic monitoring.\",\n  \"Direct Inspiration\": {\n    \"b48\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.6,\n    \"b45\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include limited sensing coverage in WiFi-based sensing, the impact of WiFi sensing on data communication, and the sensitivity of WiFi sensing to target location and orientation. The proposed solution is to exploit LTE signals for sensing due to their pervasive coverage, frequency and spatial diversity, and high-rate uniform frames.\",\n  \"Direct Inspiration\": {\n    \"b48\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b33\": 0.8,\n    \"b18\": 0.7,\n    \"b35\": 0.7,\n    \"b10\": 0.9,\n    \"b15\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b13\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the limitations of WiFi-based wireless sensing and proposes a novel approach using LTE signals to achieve pervasive and reliable sensing without compromising data communication. The challenges addressed by the paper include limited sensing coverage, interference with WiFi communication, and sensitivity to target location and orientation. The proposed algorithm leverages unique features of LTE such as pervasive coverage, frequency and spatial diversity, and high-rate uniform frames. The authors introduce LTE-Track for applications like indoor respiration sensing and outdoor traffic monitoring.\",\n  \"Direct Inspiration\": {\n    \"b48\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b33\": 0.8,\n    \"b18\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.7,\n    \"b45\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in WiFi-based wireless sensing, such as limited coverage, interference with communication, and sensitivity to target location and orientation. It proposes using LTE signals for pervasive and reliable sensing without compromising data communication. The proposed system, LTE-Track, leverages LTE's pervasive coverage, frequency and spatial diversity, and high-rate uniform frames to achieve improved sensing performance. The system is demonstrated through applications in indoor respiration sensing and outdoor traffic monitoring.\",\n  \"Direct Inspiration\": {\n    \"b48\": 1,\n    \"b1\": 0.9,\n    \"b33\": 0.9,\n    \"b18\": 0.9,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b16\": 0.7,\n    \"b10\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b31\": 0.6,\n    \"b45\": 0.6,\n    \"b43\": 0.5,\n    \"b40\": 0.5,\n    \"b41\": 0.5,\n    \"b46\": 0.5,\n    \"b47\": 0.5,\n    \"b22\": 0.5,\n    \"b50\": 0.5,\n    \"b34\": 0.5,\n    \"b38\": 0.5,\n    \"b11\": 0.6\n  }\n}\n```"], "616e37435244ab9dcbd1a7bc": ["```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include mitigating various social biases (gender, racial, and religious) in large pre-trained language models while maintaining their language modeling ability and performance on downstream natural language understanding (NLU) tasks. The paper evaluates the effectiveness of five debiasing techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection (INLP), Self-Debias, and SentenceDebias. The authors investigate how these techniques impact bias mitigation, language modeling ability, and performance on downstream NLU tasks.\",\n    \"Direct Inspiration\": {\n        \"b31\": 1.0,\n        \"b26\": 0.9,\n        \"b28\": 0.95,\n        \"b17\": 0.9,\n        \"b35\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b20\": 0.8,\n        \"b22\": 0.75,\n        \"b23\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.7,\n        \"b21\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper investigates the effectiveness of five debiasing techniques for pre-trained language models in mitigating gender, racial, and religious biases. It evaluates these techniques using intrinsic bias benchmarks and explores their impact on language modeling ability and performance on downstream NLU tasks.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0,\n    \"b31\": 0.9,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of mitigating social biases in large pre-trained language models, such as gender, racial, and religious biases. It evaluates five debiasing techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection (INLP), Self-Debias, and SentenceDebias across various models (BERT, ALBERT, RoBERTa, GPT-2). The study investigates the effectiveness of these techniques in reducing biases without significantly impacting language modeling performance or downstream natural language understanding tasks.\",\n  \"Direct Inspiration\": [\"b31\", \"b28\", \"b17\", \"b26\", \"b35\"],\n  \"Indirect Inspiration\": [\"b20\", \"b22\", \"b23\"],\n  \"Other Inspiration\": [\"b3\", \"b21\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of mitigating social biases in large pre-trained language models. It evaluates the effectiveness of five debiasing techniques (Counterfactual Data Augmentation, Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias) in reducing gender, racial, and religious biases, and explores their impact on language modeling and downstream natural language understanding tasks.\",\n    \"Direct Inspiration\": {\n        \"b20\": 1,\n        \"b31\": 1,\n        \"b22\": 0.9,\n        \"b23\": 0.9,\n        \"b28\": 0.8,\n        \"b17\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b26\": 0.7,\n        \"b35\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.6,\n        \"b21\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the performance and biases in large pre-trained language models. It evaluates five debiasing techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection (INLP), Self-Debias, and SentenceDebias, focusing on gender, racial, and religious biases in BERT, ALBERT, RoBERTa, and GPT-2 models. The work aims to answer which technique is most effective in mitigating bias, if these techniques worsen language modeling ability, and their impact on downstream NLU tasks.\",\n    \"Direct Inspiration\": {\n        \"b20\": 1,\n        \"b31\": 1,\n        \"b28\": 1,\n        \"b17\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b26\": 0.9,\n        \"b22\": 0.9,\n        \"b23\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b35\": 0.8,\n        \"b21\": 0.8,\n        \"b3\": 0.8\n    }\n}\n```"], "600832d79e795ed227f5313b": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inefficiency and high computational cost of manually designing neural architectures and task-specific Neural Architecture Search (NAS) methods. Inspired by this, the authors propose the Meta Dataset-to-Architecture (MetaD2A) framework, which learns to generate neural architectures from datasets using amortized meta-learning. The framework consists of a set encoder and a graph decoder to learn a cross-modal latent space, enabling the rapid generation of neural architectures for unseen datasets.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.95,\n    \"b11\": 0.90,\n    \"b31\": 0.90,\n    \"b44\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.85,\n    \"b4\": 0.80,\n    \"b41\": 0.80,\n    \"b50\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.75,\n    \"b53\": 0.75,\n    \"b9\": 0.70\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency and high computational cost of Neural Architecture Search (NAS) methods for new datasets. The proposed MetaD2A framework aims to generalize across multiple datasets using meta-learning to rapidly generate dataset-specific neural architectures by learning a cross-modal latent space. This approach leverages a set encoder and a graph decoder to map datasets to architectures, significantly reducing search time and computational cost.\",\n  \"Direct Inspiration\": [\"b36\", \"b43\", \"b31\"],\n  \"Indirect Inspiration\": [\"b11\", \"b44\", \"b9\", \"b50\"],\n  \"Other Inspiration\": [\"b2\", \"b55\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiency of manual neural architecture design by proposing an efficient Neural Architecture Search (NAS) framework called MetaD2A. MetaD2A leverages a database of datasets and neural architectures to generalize over unseen tasks, thus rapidly generating a high-performing neural architecture for a new dataset. The core contributions include a set encoder and graph decoder for learning a cross-modal latent space of datasets and architectures, and a meta-performance predictor for selecting the best architecture based on predicted performance.\",\n  \"Direct Inspiration\": [\"b36\", \"b31\", \"b11\", \"b44\"],\n  \"Indirect Inspiration\": [\"b9\", \"b50\", \"b43\", \"b53\"],\n  \"Other Inspiration\": [\"b25\", \"b6\", \"b15\", \"b46\", \"b47\", \"b54\", \"b2\", \"b55\", \"b20\", \"b33\", \"b41\", \"b34\", \"b5\", \"b51\", \"b10\"]\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"The primary challenge is the inefficiency and high computational cost of manually designing neural architectures for new tasks, which is exacerbated by the need to train models from scratch repeatedly for each new dataset.\",\n        \"algorithm\": \"The proposed MetaD2A framework uses amortized meta-learning to generalize neural architecture search (NAS) across multiple datasets by learning a cross-modal latent space for datasets and architectures. It employs a set encoder and graph decoder to generate dataset-specific architectures and a meta-performance predictor to select the best architecture based on predicted performance.\"\n    },\n    \"Direct Inspiration\": {\n        \"b36\": 0.9,\n        \"b31\": 0.85,\n        \"b11\": 0.85,\n        \"b44\": 0.85,\n        \"b43\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.7,\n        \"b53\": 0.75,\n        \"b9\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b50\": 0.6,\n        \"b5\": 0.6,\n        \"b4\": 0.6,\n        \"b41\": 0.6,\n        \"b34\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency of conventional Neural Architecture Search (NAS) methods that require extensive computational resources to design optimal neural architectures for new datasets. The proposed MetaD2A framework aims to overcome this challenge by leveraging meta-learning to generate neural architectures quickly for multiple unseen datasets. This is achieved through the use of a set encoder, graph decoder, and a meta-performance predictor that together learn a cross-modal latent space for datasets and architectures.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b11\": 0.85,\n    \"b44\": 0.88\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.75,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b50\": 0.6,\n    \"b9\": 0.55,\n    \"b41\": 0.5\n  }\n}\n```"], "600831f09e795ed227f530d8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of social bias in pretrained sentence encoders and proposes a novel neural debiasing method called FairFil. This method aims to reduce social bias in sentence embeddings while preserving semantic meaning, utilizing a multi-view contrastive learning approach and a debiasing regularizer.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b26\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b32\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b41\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of social bias in pretrained sentence encoders and proposes a neural debiasing method called FairFil to mitigate this bias. The method utilizes multi-view contrastive learning and a debiasing regularizer to ensure that the debiased embeddings maintain the semantic meaning of the input text while reducing bias.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1.0,\n        \"b26\": 1.0,\n        \"b32\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b41\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b13\": 0.6,\n        \"b52\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of eliminating social bias in pretrained sentence encoders without retraining large-scale models. It introduces a neural debiasing method called FairFil, which uses a multi-view contrastive learning framework and a debiasing regularizer to produce debiased embeddings while preserving semantic information.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b26\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.85,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.75,\n    \"b13\": 0.7,\n    \"b41\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of debiasing pretrained sentence encoders to ensure fairness, specifically targeting social biases related to gender, race, and religion. The proposed solution is a neural debiasing method called FairFil, which employs multi-view contrastive learning and a debiasing regularizer to reduce biases while preserving semantic information.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b26\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b32\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the social bias in pretrained sentence encoders. The paper proposes a novel neural debiasing method called FairFil, which employs a multi-view contrastive learning approach and a debiasing regularizer to reduce bias while preserving the semantic information of the input text.\",\n  \"Direct Inspiration\": [\"b8\", \"b10\", \"b26\"],\n  \"Indirect Inspiration\": [\"b6\", \"b32\"],\n  \"Other Inspiration\": [\"b13\", \"b41\"]\n}\n```"], "600fe67691e011256c955faa": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of debiasing contextualised word embeddings, which is more complex compared to static word embeddings due to the dynamic and context-dependent nature of the former. The proposed method involves fine-tuning pre-trained contextualised word embeddings to remove gender biases while preserving essential semantic information. The method is architecture-agnostic and can be applied to various models like BERT, RoBERTa, ALBERT, DistilBERT, and ELECTRA. The effectiveness of the method is evaluated on tasks like SEAT and MNLI, showing promising results in debiasing without significant loss of semantic information.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b3\", \"b9\", \"b27\", \"b40\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b26\", \"b7\", \"b22\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b19\", \"b42\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of debiasing contextualised word embeddings, which is more complex than debiasing static word embeddings due to the dynamic nature of contextual embeddings, their large number of parameters, and the need for fine-tuning rather than retraining. The proposed method fine-tunes pre-trained contextualised embeddings to remove gender bias while preserving semantic information, and it is adaptable to different models like BERT, RoBERTa, ALBERT, DistilBERT, and ELECTRA. The effectiveness of the method is demonstrated through various tasks, showing it not only debiases the embeddings but also retains useful semantic information.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b9\": 0.85,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b22\": 0.7,\n    \"b18\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of debiasing contextualized word embeddings, which are known to encode various types of unfair biases such as gender, racial, and religious biases. The proposed method aims to fine-tune pre-trained contextualized word embeddings to remove these biases while retaining useful semantic information. The approach is agnostic to the internal architecture of the embeddings and can be applied at different granularities and layers.\",\n  \"Direct Inspiration\": [\"b22\", \"b27\", \"b40\"],\n  \"Indirect Inspiration\": [\"b9\", \"b3\", \"b19\"],\n  \"Other Inspiration\": [\"b11\", \"b26\", \"b7\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of debiasing contextualised word embeddings, which encode unfair biases like gender, racial, and religious biases. The proposed method fine-tunes pre-trained contextualised word embeddings to remove biases while retaining semantic information. It can be applied to different embedding models (BERT, RoBERTa, ALBERT, DistilBERT, ELECTRA) and evaluated on tasks like SEAT and MNLI.\",\n  \"Direct Inspiration\": [\"b27\", \"b9\"],\n  \"Indirect Inspiration\": [\"b40\", \"b22\", \"b3\"],\n  \"Other Inspiration\": [\"b26\", \"b23\", \"b7\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in debiasing contextualized word embeddings, which are more complex compared to static embeddings due to their dynamic nature and extensive parameters. The proposed method aims to fine-tune pre-trained contextualized embeddings to remove gender biases while preserving semantic information. The method is independent of the model architecture and can be applied at different granularities. Experimental results show that the method effectively debiases various models while maintaining their performance on downstream tasks.\",\n  \"Direct Inspiration\": [\"b27\", \"b40\"],\n  \"Indirect Inspiration\": [\"b9\", \"b3\"],\n  \"Other Inspiration\": [\"b11\", \"b26\", \"b7\"]\n}\n```"], "6103d7ba91e01159791b20cf": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of mitigating biases in natural language processing models, specifically during the training and generation phases of BERT. It introduces DEBIASBERT and DEBIASGEN, which incorporate bias mitigation losses during model training and language decoding to reduce gender and racial biases.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b30\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b7\": 0.75,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.65,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses representation bias in NLP models, specifically focusing on mitigating biases in language generation tasks such as summarization. The proposed method involves further pre-training BERT with bias mitigation losses and introducing bias penalizing losses during the decoding stage.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b30\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b26\": 0.75,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.65,\n    \"b31\": 0.6,\n    \"b32\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are mitigating representation biases in language models, specifically in large contextual models such as BERT and GPT. The authors propose methods for bias mitigation during model training and language decoding stages. They introduce DEBIASBERT, which includes equalizing losses and a novel declustering loss to reduce biases in generated language. Additionally, they present DEBIASGEN, a bias-mitigating summarization model.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.95,\n    \"b13\": 0.90,\n    \"b21\": 0.88,\n    \"b23\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b26\": 0.70,\n    \"b32\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.60,\n    \"b31\": 0.55,\n    \"b35\": 0.50\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of mitigating representation bias in language models, specifically BERT. It introduces novel methods: bias mitigation during model training using pre-training with bias mitigation losses, and bias mitigation during language decoding in generation tasks. The main contributions include the introduction of DEBIASBERT and DEBIASGEN, demonstrating reduced biases in generated text.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b23\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b13\": 0.6,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses representation bias in language models, specifically focusing on BERT, and proposes methods to mitigate these biases during training and language decoding stages. The main contributions include introducing bias mitigation losses during BERT training and summarization tasks to reduce biases against different demographic groups.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b30\": 0.85,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.75,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b26\": 0.6,\n    \"b32\": 0.55\n  }\n}\n```"], "60c0940491e0116059b15cb8": ["```json\n{\n  \"Summary\": \"The paper tackles the challenge of designing a system without relying on a CPU for coordination or configuration of devices. It proposes decentralizing control by using self-managing devices and a system management bus to replace traditional OS functions, thereby removing the CPU entirely.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b38\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b39\": 0.7,\n    \"b42\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b27\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing a high-performance system without reliance on a CPU by proposing a system composed of self-managing devices coordinated by a system management bus. It focuses on decentralizing control and distributing OS functions among hardware components to enhance performance, security, and scalability.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b27\": 0.6,\n    \"b42\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inefficiency and security issues associated with using a CPU as a central component in high-performance systems. The paper proposes a decentralized system design where self-managing hardware devices communicate via a system management bus, thereby eliminating the need for a CPU. The novel methods introduced include self-managing devices, a system bus for inter-device communication, and the distribution of traditional OS functions among hardware components.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b7\": 1.0,\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b39\": 0.8,\n    \"b42\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of decentralized control in systems without relying on a CPU. It proposes a system where devices are self-managed and communicate autonomously through a system bus, effectively removing the CPU from the system. The key contributions include understanding the role of the operating system in a CPU-less system, and the division of responsibilities among hardware components.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1.0,\n        \"b38\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b4\": 0.8,\n        \"b39\": 0.7,\n        \"b42\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b27\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of decentralizing control in computing systems by removing the CPU and leveraging self-managing devices and a system management bus for coordination. The proposed system redistributes operating system functions among hardware components, aiming to enhance performance, simplify system design, and improve security.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1,\n    \"b2\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.85,\n    \"b39\": 0.8,\n    \"b4\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b30\": 0.7,\n    \"b42\": 0.65\n  }\n}\n```"], "616e37435244ab9dcbd1a6d3": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of running neural networks on microcontroller-powered IoT devices, which have limited compute power, memory, and storage. The proposed solution is a budgeted differentiable network pruning method that considers model size, peak memory usage, and latency constraints as differentiable objectives. This method is designed to be more efficient and targeted towards microcontroller-specific bottlenecks.\",\n    \"Direct Inspiration\": {\n        \"b7\": 1,\n        \"b20\": 0.9,\n        \"b11\": 0.8,\n        \"b24\": 0.8,\n        \"b3\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b22\": 0.8,\n        \"b30\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b41\": 0.7,\n        \"b6\": 0.7,\n        \"b19\": 0.7,\n        \"b8\": 0.7,\n        \"b33\": 0.7,\n        \"b13\": 0.6,\n        \"b28\": 0.6,\n        \"b14\": 0.6,\n        \"b26\": 0.6,\n        \"b40\": 0.6,\n        \"b27\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the limited compute power and memory resources of microcontroller units (MCUs) when deploying neural networks. The proposed algorithm employs budgeted differentiable network pruning to optimize neural networks for MCUs, focusing on model size, peak memory usage, and latency constraints.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b20\": 0.9,\n    \"b11\": 0.8,\n    \"b24\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.7,\n    \"b22\": 0.6,\n    \"b30\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.5,\n    \"b6\": 0.5,\n    \"b19\": 0.5,\n    \"b8\": 0.5,\n    \"b33\": 0.5,\n    \"b13\": 0.4,\n    \"b28\": 0.4,\n    \"b14\": 0.4,\n    \"b26\": 0.4,\n    \"b40\": 0.4,\n    \"b27\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The paper addresses the challenge of running neural networks on microcontroller-powered IoT devices, which have limited compute power, memory, and storage.\",\n    \"Inspirations\": \"The paper proposes a budgeted differentiable network pruning method to address these challenges, using bi-level gradient descent optimization to learn the sizes of each layer in the network.\"\n  },\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b23\": 0.9,\n    \"b20\": 0.8,\n    \"b11\": 0.9,\n    \"b24\": 0.8,\n    \"b3\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.7,\n    \"b6\": 0.6,\n    \"b19\": 0.6,\n    \"b8\": 0.6,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b28\": 0.5,\n    \"b22\": 0.6,\n    \"b30\": 0.8,\n    \"b14\": 0.6,\n    \"b26\": 0.5,\n    \"b40\": 0.4,\n    \"b27\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling deep learning on microcontroller units (MCUs) by proposing a budgeted differentiable network pruning algorithm. The unique constraints of MCUs, such as limited SRAM and slow processors, necessitate specialized methodologies different from those used for GPU/mobile-scale neural networks. The proposed method employs bi-level gradient descent optimization to iteratively compress neural networks while adhering to resource budget requirements, achieving faster compression and better resource allocation compared to existing methods.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b23\": 0.9,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b24\": 0.7,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.9,\n    \"b22\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of deploying neural networks on microcontroller-powered IoT devices, which have significant limitations in terms of compute power, memory, and storage. The proposed solution involves budgeted differentiable network pruning, using bi-level gradient descent optimization to learn layer sizes and meet resource constraints. This method is tailored for microcontroller-specific requirements and claims to achieve faster and more effective compression compared to existing methods.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b11\": 0.8,\n    \"b24\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.7,\n    \"b13\": 0.7,\n    \"b22\": 0.7\n  }\n}\n```"], "60cafb5291e011b3293742ba": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of training semi-supervised learning (SSL) models efficiently and robustly, particularly in the presence of out-of-distribution (OOD) data and class imbalance. The authors propose RETRIEVE, a coreset selection framework designed to speed up SSL training and improve robustness while maintaining or improving accuracy.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.95,\n    \"b52\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.80,\n    \"b41\": 0.75,\n    \"b55\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.60,\n    \"b23\": 0.55,\n    \"b38\": 0.50\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses two primary challenges in semi-supervised learning (SSL): (1) Efficiently training SSL models on coresets of unlabeled data to achieve faster convergence and reduced training time, and (2) Robustly training SSL models on coresets of unlabeled data in the presence of out-of-distribution (OOD) data or class imbalance. The proposed method, RETRIEVE, is a coreset selection framework that enables faster and robust training of SSL algorithms by selecting data instances from the unlabeled set whose gradients are aligned with the labeled set gradients.\",\n    \"Direct Inspiration\": {\n        \"b16\": 0.95,\n        \"b41\": 0.9,\n        \"b52\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b55\": 0.8,\n        \"b7\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b43\": 0.7,\n        \"b22\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges: (1) reducing training times and computational costs of semi-supervised learning (SSL) algorithms while maintaining accuracy, and (2) improving the robustness of SSL algorithms in the presence of out-of-distribution (OOD) data or class imbalance. The proposed algorithm, RETRIEVE, aims to select a coreset of unlabeled data that minimizes labeled set loss, thus enabling faster convergence and robust training.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b41\": 0.9,\n    \"b52\": 0.9,\n    \"b55\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b43\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.4,\n    \"b22\": 0.4,\n    \"b23\": 0.4,\n    \"b48\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in semi-supervised learning (SSL): reducing training time and energy consumption, and improving performance in the presence of out-of-distribution (OOD) data or class imbalance. The authors propose the RETRIEVE framework, which uses a coreset selection algorithm for efficient and robust SSL, achieving significant speedups while maintaining or improving accuracy.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.95,\n    \"b52\": 0.85,\n    \"b41\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b55\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.65,\n    \"b43\": 0.60,\n    \"b22\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in semi-supervised learning (SSL): 1) The inefficiency and high computational cost of current SSL algorithms, and 2) The poor performance of SSL algorithms when out-of-distribution (OOD) data or class imbalance is present. The proposed solution is the RETRIEVE framework, which selects coresets of unlabeled data to achieve faster convergence and robust training.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.8,\n    \"b52\": 0.8,\n    \"b3\": 0.7,\n    \"b55\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```"], "60e42e76dfae54001623be83": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the lack of node attributes in many real-world graphs, which deteriorates the performance of Graph Neural Networks (GNNs). The paper proposes a categorization of common artificial node features into positional and structural ones and studies their utility across different types of graph mining tasks. Additionally, the paper introduces a novel degree-based node feature initialization method called degree bucket range, which achieves state-of-the-art performance on structural node classification.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b31\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b12\": 0.6,\n    \"b46\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.4,\n    \"b23\": 0.3,\n    \"b36\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively initializing node features in graphs that lack natural node attributes, which is a common issue in many real-world datasets. The authors categorize various artificial node feature initialization methods into positional and structural features and propose a novel degree-based method, 'degree bucket range,' which achieves state-of-the-art performance in structural node classification tasks.\",\n  \"Direct Inspiration\": [\"b28\", \"b31\", \"b12\"],\n  \"Indirect Inspiration\": [\"b10\", \"b21\", \"b34\"],\n  \"Other Inspiration\": [\"b5\", \"b18\", \"b52\", \"b45\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of initializing node features for Graph Neural Networks (GNNs) in non-attributed graphs. It categorizes artificial node features into positional and structural types and proposes a novel degree-based initialization method called degree+. Extensive experiments validate that positional features are better for positional node classification, while structural features are more suitable for structural node classification and graph classification tasks. The degree+ method achieves state-of-the-art performance for structural node classification.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.95,\n    \"b28\": 0.9,\n    \"b12\": 0.85,\n    \"b45\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b24\": 0.7,\n    \"b23\": 0.65,\n    \"b46\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b34\": 0.55,\n    \"b5\": 0.5,\n    \"b18\": 0.5,\n    \"b52\": 0.5,\n    \"b3\": 0.5,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of initializing node features for non-attributed graphs in Graph Neural Networks (GNNs). It categorizes common artificial node features into positional and structural features and studies their utility on various graph mining tasks. The paper proposes a novel degree-based node feature initialization method, degree bucket range, achieving state-of-the-art performance on structural node classification.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b31\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.75,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b23\": 0.55,\n    \"b46\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of initializing node features in non-attributed graphs for Graph Neural Networks (GNNs). It categorizes artificial node features into positional and structural types and evaluates their utility across various graph mining tasks. The paper introduces a novel degree-based node feature initialization method called degree+, which achieves state-of-the-art performance in structural node classification.\",\n  \"Direct Inspiration\": [\"b12\", \"b10\", \"b31\"],\n  \"Indirect Inspiration\": [\"b28\", \"b45\"],\n  \"Other Inspiration\": [\"b7\", \"b23\", \"b46\"]\n}\n```"], "6082aaa4e4510cd7c85b2d71": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving memory latency for sparse irregular algorithms by proposing a hardware-software co-design. The key innovation is the Data Indirection Graph (DIG), which efficiently communicates workload attributes from software to hardware for better prefetching. This approach aims to prefetch data structures with irregular memory accesses, using dynamic run-time information to ensure timeliness and low-cost hardware mechanisms. The system, named ProDIGy, outperforms existing prefetchers and offers significant performance and energy efficiency improvements.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b14\": 0.9,\n    \"b98\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b76\": 0.7,\n    \"b6\": 0.7,\n    \"b10\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b30\": 0.6,\n    \"b42\": 0.6,\n    \"b49\": 0.6,\n    \"b78\": 0.6,\n    \"b97\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of sub-optimal performance in sparse irregular algorithms due to irregular memory access patterns. The proposed solution, ProDIGy, introduces a hardware-software co-design that uses a compact Data Indirection Graph (DIG) to improve memory latency by prefetching data based on irregular algorithm memory traversal patterns. The DIG representation encodes program semantics and is used to program a low-cost hardware prefetcher.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b14\": 0.9,\n    \"b98\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b76\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.8,\n    \"b58\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of sub-optimal performance in sparse irregular algorithms due to irregular memory access patterns. It proposes a hardware-software co-design, introducing a Data Indirection Graph (DIG) representation to improve memory latency for irregular workloads. The system, Prodigy, prefetches data based on dynamic run-time information and uses a low-cost hardware prefetching mechanism.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b14\": 0.8,\n    \"b98\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b6\": 0.7,\n    \"b76\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b58\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper proposes a hardware-software co-design named Prodigy to improve memory latency for irregular workloads. It introduces the Data Indirection Graph (DIG) for encoding program semantics and designing a low-cost hardware prefetcher.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1,\n        \"b5\": 1,\n        \"b14\": 1,\n        \"b98\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b7\": 0.8,\n        \"b76\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.7,\n        \"b26\": 0.7,\n        \"b58\": 0.7,\n        \"b28\": 0.6,\n        \"b11\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving memory latency for sparse irregular algorithms, which exhibit complex memory access patterns. The authors propose a hardware-software co-design utilizing a novel Data Indirection Graph (DIG) representation to efficiently prefetch data. This approach aims to prefetch all key data structures, leverage dynamic run-time information, and maintain low hardware costs.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b14\": 0.9,\n    \"b98\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b76\": 0.7,\n    \"b88\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b22\": 0.6,\n    \"b25\": 0.6,\n    \"b30\": 0.6,\n    \"b42\": 0.6,\n    \"b49\": 0.6,\n    \"b78\": 0.6,\n    \"b97\": 0.6\n  }\n}\n```"], "619bbb131c45e57ce977694c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of applying Process-in-Memory (PIM) architecture to Graph Pattern Mining (GPMI) applications. The primary challenges include the complex iterations causing load imbalance and the memory-intensive nature of intersection and subtraction (I/S) operations. The proposed algorithm evaluates different scheduling schemes to reduce load imbalance and enhance performance.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b18\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b15\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b17\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accelerating graph pattern mining (GPMI) applications using Process-in-Memory (PIM) architecture, focusing on memory-intensive intersection and subtraction (I/S) operations. The authors explore the memory access characteristics of these operations, compare performance on CPU and PIM, and propose scheduling schemes to mitigate load imbalance issues.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b18\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b15\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of using Process-in-Memory (PIM) architecture to accelerate Graph Pattern Mining (GPMI) applications. It identifies memory-intensive I/S operations as suitable for PIM and evaluates various scheduling schemes to mitigate load imbalance in PIM cores.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b12\": 1.0,\n    \"b18\": 1.0,\n    \"b19\": 1.0,\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b17\": 0.8,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of applying Process-in-Memory (PIM) architecture to Graph Pattern Mining (GPMI) applications, focusing on the memory-intensive nature of intersection and subtraction (I/S) operations and the load imbalance issue. The paper evaluates various scheduling schemes to optimize load distribution across PIM cores.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b18\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b15\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Memory access intensity and load imbalance in GPMI applications\",\n      \"Scheduling and balancing workloads on PIM architecture to optimize performance\"\n    ],\n    \"inspirations\": [\n      \"Exploration of memory access characteristics in graph matching algorithms\",\n      \"Evaluation of I/S operations on CPU and PIM to identify performance gains\",\n      \"Scheduling methods to reduce load imbalance in PIM architecture\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b12\", \"b18\", \"b19\", \"b20\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b10\", \"b15\", \"b1\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b14\", \"b11\"]\n  }\n}\n```"], "61c145ba5244ab9dcb84f2bc": ["```json\n{\n  \"Summary\": \"The paper proposes G2 Miner, a novel GPU-based Graph Pattern Mining (GPM) system designed to handle the challenges of memory capacity, parallelism, and workload balance in GPUs. It introduces automated CUDA code generation for arbitrary patterns, supports both BFS and DFS search orders, and scales to multiple GPUs. The system aims to improve performance and programmability in GPM tasks.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b52\", \"b25\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b24\", \"b32\", \"b73\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b41\", \"b42\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of implementing Graph Pattern Mining (GPM) on GPUs, which include architecture awareness, pattern awareness, and efficient task scheduling. The proposed solution, G2 Miner, is the first pattern-aware, input data-graph-aware, and architecture-aware framework for GPM on GPUs. It automates CUDA code generation and supports both BFS and DFS search orders. G2 Miner is also the first multi-GPU framework for GPM, significantly improving performance over existing systems.\",\n  \"Direct Inspiration\": {\n    \"b52\": 1,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b24\": 0.7,\n    \"b73\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.6,\n    \"b41\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges of implementing Graph Pattern Mining (GPM) on GPUs due to memory constraints, thread divergence, and load imbalance. It introduces G2 Miner, a novel multi-GPU framework that automates CUDA code generation and balances workload among GPUs for efficient GPM.\",\n  \"Direct Inspiration\": [\"b52\", \"b25\"],\n  \"Indirect Inspiration\": [\"b17\", \"b86\", \"b23\"],\n  \"Other Inspiration\": [\"b24\", \"b32\", \"b73\", \"b41\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include efficiently implementing Graph Pattern Mining (GPM) on GPUs due to GPU-specific constraints like smaller memory capacity, the need for more fine-grained data parallelism, thread divergence, and load imbalance. The proposed solution, G2 Miner, addresses these challenges by being architecture-aware, pattern-aware, and input data-graph-aware. It is the first GPM system to automate CUDA code generation for arbitrary patterns and support both BFS and DFS search orders on GPUs while balancing workload among multiple GPUs. G2 Miner significantly outperforms existing GPU-based and CPU-based GPM systems.\",\n    \"Direct Inspiration\": {\n        \"b52\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b17\": 0.8,\n        \"b25\": 0.75,\n        \"b24\": 0.7,\n        \"b41\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b32\": 0.6,\n        \"b73\": 0.6,\n        \"b42\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the challenges of efficiently implementing Graph Pattern Mining (GPM) on GPUs, particularly focusing on memory capacity, thread divergence, and workload imbalance. The paper introduces G2 Miner, a novel system that automates CUDA code generation, supports both BFS and DFS, and scales to multiple GPUs, significantly outperforming existing systems.\",\n  \"Direct Inspiration\": [\"b52\", \"b25\"],\n  \"Indirect Inspiration\": [\"b17\", \"b24\", \"b26\", \"b32\", \"b41\", \"b42\", \"b73\"],\n  \"Other Inspiration\": [\"b6\", \"b20\", \"b21\", \"b58\", \"b70\", \"b72\", \"b92\"]\n}\n```"], "605076fb91e0111e1cd4695d": ["```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are the difficulty in accurately determining which author mentions in a bibliographic database refer to the same real-world person, due to name ambiguities and dataset biases. The paper introduces S2AND, a new author disambiguation dataset that combines eight previous datasets into a single resource with a uniform format and a rich feature set. The proposed algorithm uses Gradient Boosted Trees (GBT) for pairwise similarity estimation and hierarchical agglomerative clustering (HAC) for clustering, showing that training on S2AND improves AND accuracy and generalization.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b2\": 0.85,\n        \"b3\": 0.8,\n        \"b5\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.75,\n        \"b7\": 0.7,\n        \"b10\": 0.7,\n        \"b11\": 0.7,\n        \"b14\": 0.7,\n        \"b15\": 0.7,\n        \"b17\": 0.7,\n        \"b19\": 0.7,\n        \"b20\": 0.65,\n        \"b21\": 0.65,\n        \"b22\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b23\": 0.6,\n        \"b24\": 0.6,\n        \"b25\": 0.6,\n        \"b26\": 0.6,\n        \"b27\": 0.6,\n        \"b28\": 0.6,\n        \"b29\": 0.6,\n        \"b30\": 0.6,\n        \"b31\": 0.6,\n        \"b32\": 0.6,\n        \"b33\": 0.6,\n        \"b34\": 0.6,\n        \"b35\": 0.6,\n        \"b36\": 0.6,\n        \"b37\": 0.6,\n        \"b38\": 0.6,\n        \"b39\": 0.6,\n        \"b40\": 0.6,\n        \"b41\": 0.6,\n        \"b42\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is Author Name Disambiguation (AND), which involves determining which author mentions in large bibliographic databases refer to the same real-world person. The authors propose S2AND, a new author disambiguation dataset that combines eight previous datasets into a single resource with a uniform format and a consistent, rich feature set. This dataset is used to train a new AND model that improves accuracy and generalization compared to single-dataset approaches.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.85,\n    \"b5\": 0.8,\n    \"b19\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b7\": 0.65,\n    \"b11\": 0.6,\n    \"b17\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b10\": 0.45,\n    \"b12\": 0.4,\n    \"b14\": 0.35\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately determining author identity in bibliographic databases, a task known as Author Name Disambiguation (AND). It presents a new dataset, S2AND, combining eight previous datasets into a single, consistent resource. The paper demonstrates that training AND models on S2AND improves both in-domain and out-of-domain performance and introduces new features and a reference AND implementation.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.9,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b4\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents S2AND, a new author disambiguation dataset that combines eight previous datasets into a single resource with a uniform format and a consistent, rich feature set. Key contributions include a new training dataset, an open-sourced reference AND algorithm, experiments showing improved generalization, and a comparison against the existing Semantic Scholar system.\",\n  \"Direct Inspiration\": [\"b1\"],\n  \"Indirect Inspiration\": [\"b2\", \"b5\", \"b7\", \"b17\"],\n  \"Other Inspiration\": [\"b6\", \"b10\", \"b11\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of determining which author mentions in large bibliographic databases refer to the same real-world person, a task known as Author Name Disambiguation (AND). The paper introduces S2AND, a new AND dataset that unifies eight previous datasets, and demonstrates that training on S2AND improves the generalization and accuracy of AND systems compared to training on individual datasets alone. The paper also evaluates the fairness of AND systems across different demographic groups and showcases a new state-of-the-art AND model using gradient-boosted trees and hierarchical agglomerative clustering.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.9,\n    \"b3\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b0\": 0.8,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b26\": 0.7,\n    \"b27\": 0.7,\n    \"b28\": 0.7,\n    \"b29\": 0.7,\n    \"b30\": 0.7,\n    \"b31\": 0.7,\n    \"b32\": 0.7,\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7,\n    \"b36\": 0.7,\n    \"b37\": 0.7,\n    \"b38\": 0.7,\n    \"b39\": 0.7,\n    \"b40\": 0.7,\n    \"b41\": 0.7,\n    \"b42\": 0.7\n  }\n}\n```"], "6204827f5aee126c0f77da66": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of inferring complex network dynamics from time-series data of node activities and network topology, proposing a two-phase inference approach. This includes a global crude-regression phase to narrow down the search space of dynamic functions and a local fine-tuning phase to determine the precise coefficients. The approach is validated on various networked dynamical models and real-world data, demonstrating robustness against noise and incomplete data.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b18\": 0.9,\n    \"b34\": 0.9,\n    \"b35\": 0.9,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.4,\n    \"b32\": 0.4,\n    \"b33\": 0.4,\n    \"b39\": 0.4,\n    \"b40\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inferring the governing equations of complex network dynamics from observed data, without a priori knowledge of the system's internal mechanisms. The authors propose a novel two-phase inference approach which includes global crude-regression and local fine-tuning to accurately infer the network dynamics by narrowing down the search space and fine-tuning the coefficients of the elementary functions that constitute the true dynamics.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b30\": 0.6,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"Challenges\": [\n            \"Inferring the structure of self-regulation and interaction dynamics in complex networks\",\n            \"Handling noisy and incomplete data\",\n            \"Addressing synchronization and dynamical heterogeneity in network dynamics\"\n        ],\n        \"Inspirations\": [\n            \"Developing a two-phase inference approach combining global regression and local fine-tuning\",\n            \"Building comprehensive libraries of elementary functions for capturing diverse network dynamics\"\n        ]\n    },\n    \"Direct Inspiration\": {\n        \"b8\": 1.0,\n        \"b22\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b21\": 0.9,\n        \"b29\": 0.8,\n        \"b30\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.7,\n        \"b35\": 0.7,\n        \"b40\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper include the inference of complex network dynamics from noisy and incomplete data, the lack of a priori knowledge of the internal mechanisms (F and G functions), and dealing with high-dimensional nonlinear systems. The proposed algorithm involves a two-phase inference approach: (1) a global crude-regression to narrow down the search space for dynamic functions, and (2) local fine-tuning to obtain precise and compact forms of the governing equations.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b21\": 0.95,\n    \"b22\": 0.95,\n    \"b31\": 0.9,\n    \"b32\": 0.9,\n    \"b33\": 0.9,\n    \"b34\": 0.85,\n    \"b35\": 0.85\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b18\": 0.75,\n    \"b19\": 0.7,\n    \"b20\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b26\": 0.7,\n    \"b27\": 0.7,\n    \"b28\": 0.7,\n    \"b29\": 0.65,\n    \"b30\": 0.65,\n    \"b37\": 0.6,\n    \"b38\": 0.55,\n    \"b39\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inferring the governing equations of complex network dynamics from observed data of nodes' activities and network topology. The proposed two-phase inference approach involves a global regression to narrow down potential dynamic functions and a local fine-tuning to accurately infer the specific coefficients and structures of these functions.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b34\": 0.9,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b22\": 0.85,\n    \"b29\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.7,\n    \"b32\": 0.7,\n    \"b33\": 0.7,\n    \"b39\": 0.75,\n    \"b40\": 0.75\n  }\n}\n```"], "602501dc91e011db1800e568": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge is deploying deep neural networks (DNNs) on resource-constrained microcontroller units (MCUs) with limited RAM and storage. Existing compression techniques either cause significant accuracy loss or are infeasible for MCUs.\",\n    \"inspirations\": \"The paper is inspired by the need to optimize DNN deployment on MCUs without sacrificing accuracy, leveraging dynamic pruning and other compression techniques.\"\n  },\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.85,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b11\": 0.7,\n    \"b15\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b17\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of deploying deep neural networks (DNNs) on resource-constrained microcontroller units (MCUs) in IoT devices. The primary challenge is the limited RAM and storage capacity of MCUs which makes it difficult to run DNNs that typically require large working memory. The proposed solution is a lightweight run-time working memory compression algorithm that dynamically prunes intermediate output feature maps based on the current input, ensuring that the DNN fits into the RAM without significant accuracy loss.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b2\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b10\": 0.75,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.65,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of deploying deep neural networks (DNNs) on resource-constrained microcontroller units (MCUs) by proposing a novel run-time working memory compression algorithm. The goal is to dynamically prune intermediate output feature maps to fit into limited RAM, thus maintaining the complete architecture of the DNN and minimizing accuracy degradation.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b10\": 0.85,\n    \"b11\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.65,\n    \"b14\": 0.6,\n    \"b17\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of deploying Deep Neural Networks (DNNs) on resource-constrained Microcontroller Units (MCUs) by proposing a novel run-time working memory compression algorithm. This method dynamically prunes intermediate output feature maps to fit into the limited RAM of MCUs without changing the architecture of the deployed DNNs, thus minimizing accuracy degradation and maintaining satisfactory performance with low overhead on memory and run-time latency.\",\n  \"Direct Inspiration\": [\"b10\", \"b2\", \"b3\"],\n  \"Indirect Inspiration\": [\"b4\", \"b11\", \"b15\"],\n  \"Other Inspiration\": [\"b8\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of deploying deep neural networks (DNNs) on resource-constrained microcontroller units (MCUs) by proposing a lightweight run-time working memory compression algorithm. This algorithm dynamically prunes intermediate output feature maps to fit them into the limited RAM of MCUs, maintaining accuracy and avoiding significant performance degradation.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```"], "60532ac89e795e4033f71a0e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of re-running regions of interest in workload characterization, proposing the novel creation of standalone executables called ELFies from program captures (pinballs) using the PinPlay toolkit. ELFies allow deterministic replay and performance analysis without dependency on Pin-based tools.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b11\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the difficulty in re-running regions of interest in program execution due to variations and dependencies on specific tools like Pin. The proposed solution involves creating standalone executables called ELFies that can capture and replay regions of interest without relying on Pin, addressing challenges in system call handling and ensuring graceful exit.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b10\": 0.7,\n    \"b12\": 0.7,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges highlighted in the paper are re-running regions of interest, dealing with run-to-run variation in program execution, handling system calls, and ensuring graceful exit for ELFies. The paper introduces a novel tool-chain involving PinPlay and pinball2elf to create standalone executables (ELFies) for targeted analysis of regions of interest from long-running applications.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b12\": 1,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b7\": 0.85,\n    \"b8\": 0.75,\n    \"b9\": 0.75,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b10\": 0.7,\n    \"b15\": 0.65,\n    \"b16\": 0.65,\n    \"b17\": 0.65,\n    \"b18\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"Challenges\": [\n            \"Rerunning regions of interest in native execution while maintaining performance accuracy.\",\n            \"Handling system calls and ensuring graceful exits during ELFie execution.\"\n        ],\n        \"Inspirations\": [\n            \"The need for a platform-independent sharing mechanism for regions of interest.\",\n            \"Improving upon tools like PinPlay by providing a more flexible and easily shareable format.\"\n        ]\n    },\n    \"Direct Inspiration\": {\n        \"b7\": 1,\n        \"b12\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.8,\n        \"b9\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.7,\n        \"b13\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of capturing and sharing regions of interest in program execution without relying on the Pin framework. The proposed solution, ELFies, are standalone executables that facilitate easier sharing and analysis. The authors developed the pinball2elf tool to convert PinPlay-generated pinballs into ELF binaries, overcoming challenges related to system call handling and graceful exits.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b11\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b13\": 0.7,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```"], "60da8fc20abde95dc965f7cd": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of training Graph Neural Networks (GNNs) on large-scale graphs due to the high computation and memory costs. It proposes a novel node-wise sampling strategy called blocking-based neighbor sampling (BNS) to reduce these costs. The proposed approach includes a blocking mechanism to limit the expansion of neighboring nodes and a reweighted policy to adjust the contributions of blocked and non-blocked nodes. The paper also provides theoretical proof of the unbiased estimation of the original graph convolution operation and presents extensive experiments showing the efficiency of BNS.\",\n    \"Direct Inspiration\": {\n        \"b8\": 0.9,\n        \"b6\": 0.85,\n        \"b5\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.7,\n        \"Chen et al., 2020c\": 0.65,\n        \"Chen et al., 2018a\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.55,\n        \"Zeng et al., 2020\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently training graph neural networks (GNNs) on large-scale graphs by proposing a novel node-wise sampling strategy called blocking-based neighbor sampling (BNS). The main contributions include a blocking mechanism to reduce computation and memory complexity, a reweighted policy for adjusting contributions of neighbors, and theoretical proof of unbiased estimation. Extensive experiments demonstrate that BNS is significantly faster than existing state-of-the-art methods while maintaining accuracy.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b12\": 0.9,\n    \"b6\": 0.8,\n    \"b18\": 0.7,\n    \"b5\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.6,\n    \"b15\": 0.5,\n    \"b10\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.4,\n    \"b9\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Graph Neural Networks (GNNs) on large-scale graphs, which suffer from significant computation and memory costs due to the iteratively dependent nature of nodes. The proposed solution is a novel node-wise sampling strategy called blocking-based neighbor sampling (BNS), which introduces a blocking mechanism to reduce complexity and a reweighted policy to adjust the contributions of blocked and non-blocked nodes.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b8\": 0.9,\n    \"b6\": 0.8,\n    \"b5\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.6,\n    \"b15\": 0.6,\n    \"Chen et al., 2020c\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"Chen et al., 2020b\": 0.5,\n    \"Chen et al., 2018a\": 0.5,\n    \"Chen et al., 2018b\": 0.5,\n    \"Zeng et al., 2020\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges addressed in the paper include the computational and memory complexity of training Graph Neural Networks (GNNs) on large-scale graphs. The proposed algorithm, Blocking-based Neighbor Sampling (BNS), introduces a novel node-wise sampling strategy that includes a blocking mechanism to stochastically block the expansion of neighboring nodes, thus reducing complexity. A reweighted policy adjusts the contribution of blocked and non-blocked neighbors to central nodes, providing unbiased estimation for graph convolution operations.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1.0,\n        \"b12\": 1.0,\n        \"Zeng et al., 2020\": 1.0,\n        \"Chen et al., 2018a\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b10\": 0.8,\n        \"b18\": 0.8,\n        \"Chen et al., 2020c\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"Li et al., 2019\": 0.6,\n        \"Chen et al., 2020a\": 0.6,\n        \"Chen et al., 2018b\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the high computational and memory complexity of training Graph Neural Networks (GNNs) on large-scale graphs. The authors propose a novel blocking-based neighbor sampling (BNS) strategy to address this challenge. Key contributions include a blocking mechanism to reduce complexity, a reweighted policy for adjusting node contributions, and theoretical proof of unbiased estimation.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b8\": 0.85,\n    \"b15\": 0.8,\n    \"b18\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b5\": 0.65,\n    \"b10\": 0.6,\n    \"b6\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b7\": 0.45\n  }\n}\n```"], "6201df495aee126c0f64dbb6": ["```json\n{\n    \"Summary\": \"The paper addresses the limitations of using a static graph structure in Graph Neural Networks (GNNs) for message passing. It proposes the MarkovGNN method, which uses a series of graphs generated by a Markov diffusion process to capture community patterns and improve the expressivity and performance of GNNs.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b1\"],\n        \"confidence_score\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b2\", \"b3\", \"b4\", \"b6\", \"b7\", \"b8\", \"b9\"],\n        \"confidence_score\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b10\", \"b11\", \"b26\", \"b27\", \"b30\"],\n        \"confidence_score\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitation of using a static graph structure in Graph Neural Networks (GNNs) by proposing MarkovGNN, which utilizes a series of graphs generated through a Markov diffusion process. The key contributions include the use of community-aware sparse adjacency matrices to improve GNN performance and prevent issues like over-smoothing and neighbor explosion.\",\n  \"Direct Inspiration\": [\"b1\", \"b9\", \"b2\"],\n  \"Indirect Inspiration\": [\"b3\", \"b6\", \"b7\", \"b8\"],\n  \"Other Inspiration\": [\"b4\", \"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is the limitation of Graph Neural Networks (GNNs) that use a static graph structure at every layer, which restricts their expressive power. The proposed solution, MarkovGNN, employs a Markov diffusion process to generate a series of graphs that are used at different layers of the GNN, improving its performance by better capturing community structures and preventing issues like over-smoothing and neighbor explosion.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitation of using a static graph structure in Graph Neural Networks (GNNs) by proposing the MarkovGNN algorithm. It uses a Markov diffusion process to generate a series of modified graphs, which are then applied in different layers of a GNN to enhance its performance. The proposed method aims to prevent over-smoothing and neighbor explosion problems while improving expressivity by capturing community structures in the graph.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b9\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of GNNs using a static graph structure at every layer, which restricts expressive power and leads to issues like over-smoothing. The proposed solution, MarkovGNN, uses a Markov diffusion process to generate a series of graphs, improving the performance of existing GNNs by promoting direct messaging among nodes and capturing community patterns.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b3\": 0.6,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```"], "608fc0ea91e01142b70f24a7": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are optimizing deep neural networks (DNN) for various target hardware with constraints on compilation time and cross-compilation. The proposed system, Tuna, aims to address these challenges by utilizing static analysis and analytical cost modeling instead of relying on the traditional auto-tuning or vendor-supplied kernel libraries. Tuna offers advantages such as not requiring real hardware for optimization, supporting a large set of DNN kernels, and reducing compilation time while achieving better performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of optimizing deep neural networks (DNN) for a wide range of target hardware, focusing on reducing compilation time and supporting cross-compilation. The proposed system, Tuna, utilizes static analysis and analytical cost modeling to optimize DNN without requiring real hardware. Key components include a hardware-related cost model and a multi-threaded search algorithm. Tuna aims to provide a uniform compilation stack, supporting various hardware architectures (CPU, GPU) and achieving better performance with reduced compilation time compared to existing methods like auto-tuning and vendor-supplied kernel libraries.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b4\": 1.0,\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b0\": 0.8,\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b6\": 0.7,\n    \"b7\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of optimizing deep neural networks (DNNs) for various target hardware, focusing on reducing compilation time and cross-compilation constraints. It proposes Tuna, a system that uses static analysis and analytical cost modeling to optimize DNNs without requiring real hardware for performance measurement. This approach supports various target hardware, including CPUs and GPUs, and significantly reduces compilation time while achieving comparable or better performance than traditional methods.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b4\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b3\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b5\": 0.60,\n    \"b6\": 0.55,\n    \"b7\": 0.50\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of optimizing compilation time and cross-compilation for DNN models across various hardware architectures. The proposed system, Tuna, utilizes static analysis and analytical cost modeling to optimize DNN, offering advantages over auto-tuning and vendor kernel libraries by reducing compilation time and supporting a wider range of hardware.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.7,\n    \"b3\": 0.6,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b6\": 0.4,\n    \"b7\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are optimizing DNN models for various target hardware with constraints of compilation time and cross-compilation mechanism. The paper proposes a system, Tuna, which utilizes static analysis and a combination of analytical cost modeling to optimize DNN. Tuna offers several advantages over existing methods, including no need for real hardware, transferable cost model across different micro architectures, and support for a large set of DNN kernels.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.9,\n    \"b2\": 0.9,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7\n  }\n}\n```"], "61f753205aee126c0f9c2174": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficiently parallelizing large-scale deep learning models across distributed computing clusters. It proposes a novel hierarchical approach that categorizes parallelization techniques into intra-operator and inter-operator parallelisms, and introduces Alpa, a compiler system that automatically generates parallel execution plans. The paper emphasizes the importance of optimizing both intra-operator and inter-operator parallelisms to achieve high performance and scalability in training large models.\",\n    \"Direct Inspiration\": {\n        \"b12\": 1,\n        \"b33\": 1,\n        \"b49\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b35\": 0.8,\n        \"b44\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.6,\n        \"b26\": 0.6,\n        \"b28\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automating the parallelization of large-scale deep learning models, which traditionally requires significant manual tuning and strong ML and system expertise. The proposed solution, Alpa, organizes different parallelization techniques into a hierarchical space and maps these techniques to the hierarchical structure of the compute cluster. Alpa automatically generates parallel execution plans covering all data, operator, and pipeline parallelisms, optimizing the training process and making it more efficient and accessible.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b33\": 0.85,\n    \"b49\": 0.85,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b26\": 0.8,\n    \"b40\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b28\": 0.7,\n    \"b44\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automating the parallelization of large-scale deep learning models, which currently requires significant manual tuning and engineering efforts. It introduces a novel hierarchical approach to organize and optimize parallelization techniques into intra-operator and inter-operator parallelisms, leveraging the structural properties of compute clusters. The proposed system, Alpa, automates the generation of parallel execution plans, leading to substantial performance improvements in training large models.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.95,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b49\": 0.75,\n    \"b44\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.65,\n    \"b20\": 0.6,\n    \"b28\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in this paper is the complexity of training large deep learning models on distributed systems, which involves tuning various parallelism strategies. The proposed solution, Alpa, introduces a hierarchical approach to parallel model training, categorizing parallelism into intra-operator and inter-operator levels, and automatically generating execution plans through optimization algorithms.\",\n  \"Direct Inspiration\": [\"b12\", \"b33\", \"b49\"],\n  \"Indirect Inspiration\": [\"b6\", \"b35\", \"b44\", \"b26\"],\n  \"Other Inspiration\": [\"b20\", \"b28\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of efficiently parallelizing the training of large-scale deep learning models on distributed clusters. The proposed solution, Alpa, is a compiler that automatically generates parallel execution plans by organizing parallelization techniques into a hierarchical space and mapping them to the compute cluster's structure. The hierarchical design allows for optimizing parallelization at different granularities, leading to improved performance and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b33\": 1.0,\n    \"b49\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.9,\n    \"b44\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b28\": 0.7\n  }\n}\n```"], "60128cb591e011e87ae80f5e": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the imbalance in the skill market of smart speakers due to repeated simple skills and insufficient user knowledge. The proposed algorithm aims to extract user needs and points of interest (POIs) from unstructured data such as user reviews, using methods like BERT multi-label classification model and RAKE algorithm. The goal is to analyze these needs and POIs to provide better guidance for skill development.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b9\": 0.95,\n    \"b10\": 0.88,\n    \"b12\": 0.87\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b11\": 0.75\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of analyzing user needs and POIs from unstructured data in the skill market for smart speakers. The authors propose a framework that uses the BERT multi-label classification model for extracting user needs and the RAKE algorithm for extracting POIs from user reviews. The framework aims to provide insights for skill developers to improve and create new skills based on user feedback.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b10\": 0.85,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b8\": 0.7,\n    \"b6\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b5\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of extracting and analyzing user needs and points of interest (POIs) from unstructured data in the skill market for smart speakers. The primary methods involve using BERT for multi-label classification of user needs and the RAKE algorithm for extracting POIs from user reviews. The extracted data aims to provide insights for skill developers regarding popular user needs and areas for improvement.\",\n  \"Direct Inspiration\": [\"b9\", \"b10\"],\n  \"Indirect Inspiration\": [\"b5\", \"b7\", \"b8\"],\n  \"Other Inspiration\": [\"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in the development and distribution of skills for smart speakers, such as Alexa and Google Home. The main challenge is the imbalance in the skill market, where a small percentage of skills dominate user interactions, while the majority are underutilized. The proposed algorithm involves extracting and analyzing user needs and POIs (Points of Interest) from unstructured data in user reviews to improve skill development and market distribution. Key methods include using BERT for multi-label classification to identify user needs and RAKE for keyword extraction to determine POIs.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of analyzing user needs and points of interest (POIs) in the skill market for smart speakers by leveraging unstructured data from user reviews. The authors propose using the BERT multi-label classification model for user needs extraction and the RAKE algorithm for keyword extraction from user reviews to identify POIs. They further analyze the hierarchical and related relationships between POIs using the ConceptNet knowledge graph and perform sentiment analysis on user reviews to evaluate emotional tendencies towards different POIs.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1, \n    \"b8\": 1, \n    \"b9\": 1,\n    \"b10\": 1,\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b5\": 0.8\n  }\n}\n```"], "61792de35244ab9dcb0a5c1e": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to explore the vast chemical space and generate novel drug-like molecules with desirable properties. The proposed algorithm is MolGPT, a Transformer-Decoder model, which utilizes masked self-attention mechanisms to learn long-range dependencies in SMILES strings, enabling the generation of valid, unique, and novel molecules. The model also demonstrates the ability to control molecular properties and scaffolds during generation.\",\n  \"Direct Inspiration\": [\"b43\"],\n  \"Indirect Inspiration\": [\"b9\", \"b30\", \"b33\"],\n  \"Other Inspiration\": [\"b2\", \"b3\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating novel drug-like molecules efficiently by leveraging deep generative models. It introduces MolGPT, a Transformer-Decoder model, for molecular generation based on SMILES strings, incorporating masked self-attention mechanisms to handle long-range dependencies and complex molecular properties.\",\n  \"Direct Inspiration\": {\n    \"b43\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b19\": 0.6,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the difficulty in screening an almost infinite chemical space of potential drug-like molecules and the need for generating molecules with desirable properties using deep generative models. The proposed algorithm, MolGPT, utilizes the Generative Pre-Training Transformer (GPT) architecture to predict sequences of SMILES tokens for molecular generation. This model aims to improve the validity, uniqueness, and property control of generated molecules while maintaining the structural scaffold.\",\n  \"Direct Inspiration\": [\"b43\"],\n  \"Indirect Inspiration\": [\"b9\", \"b30\", \"b33\", \"b8\"],\n  \"Other Inspiration\": [\"b12\", \"b19\", \"b24\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge in this paper is the exploration of the vast chemical space to generate drug-like molecules with desirable properties using generative models. The authors propose a novel approach called MolGPT, which leverages the GPT architecture to generate molecular SMILES strings while controlling specific molecular properties and scaffolds.\",\n  \"Direct Inspiration\": {\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b28\": 0.7,\n    \"b30\": 0.7,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.5,\n    \"b47\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating novel drug-like molecules from an immense chemical space using a deep learning model named MolGPT. Inspired by the Generative Pre-Training Transformer (GPT) model, MolGPT leverages masked self-attention mechanisms to predict sequences of SMILES tokens for molecular generation. It aims to achieve high validity, uniqueness, and novelty in generated molecules, with capabilities of conditional generation based on molecular properties and scaffold structures.\",\n    \"Direct Inspiration\": {\n        \"b43\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.9,\n        \"b30\": 0.8,\n        \"b33\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.7,\n        \"b12\": 0.7,\n        \"b14\": 0.7,\n        \"b19\": 0.7\n    }\n}\n```"], "617b66765244ab9dcbb6a815": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of achieving hardware-native performance in auto-tuning engines for DNN compilers and frameworks. It proposes Bolt, which leverages hardware-native templated search to optimize tensor programs and computational graphs, combining the flexibility of auto-tuners with the performance of vendor libraries.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b32\": 0.9,\n    \"b7\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.6,\n    \"b8\": 0.6,\n    \"b30\": 0.6,\n    \"b17\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.4,\n    \"b1\": 0.4,\n    \"b24\": 0.4,\n    \"b26\": 0.4,\n    \"b15\": 0.4,\n    \"b29\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing DNN compilers and frameworks by bridging the gap between auto-tuners and hardware-native performance. The proposed solution, Bolt, leverages templated vendor libraries to achieve efficient tensor program generation and novel computational graph-level optimizations.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of achieving hardware-native performance using auto-tuners for DNN models and proposes Bolt, a system that leverages templated libraries for efficient optimizations. Bolt integrates graph-level, operator-level, and model-level optimizations to bridge the gap between flexibility and performance, significantly reducing auto-tuning time and improving inference speed.\",\n  \"Direct Inspiration\": [\"b5\", \"b32\"],\n  \"Indirect Inspiration\": [\"b4\", \"b30\", \"b10\"],\n  \"Other Inspiration\": [\"b8\", \"b17\", \"b7\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the inefficiency in auto-tuning due to opaque device models and the performance gap for certain workloads when compared to hardware-native performance. The proposed algorithm, Bolt, leverages templated libraries like NVIDIA CUTLASS to achieve end-to-end DNN model optimizations via hardware-native templated search. Bolt introduces novel computational graph-level optimizations and persistent kernel fusion to enhance performance and reduce tuning time.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.95,\n    \"b32\": 0.95,\n    \"b4\": 0.80,\n    \"b10\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.60,\n    \"b8\": 0.60,\n    \"b7\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.50,\n    \"b17\": 0.50,\n    \"b24\": 0.50\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance gap in DNN compilers and frameworks by proposing Bolt, a solution that combines the flexibility of auto-tuners with the hardware-native performance of vendor libraries. Bolt leverages templated libraries to achieve efficient tensor program generation and introduces novel computational graph-level optimizations and persistent kernel fusion techniques to improve performance.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b8\": 0.6,\n    \"b17\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```"], "5ff68400d4150a363cbc7ef4": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient data prefetching in multi-core systems, specifically focusing on the limitations of pairwise-correlating prefetchers. It proposes a novel method, Runahead MetaData (RMD), to enhance multi-degree prefetching by using separate metadata tables to predict the next but one event. This approach aims to improve accuracy and timeliness while reducing storage overhead and overpredictions.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b8\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.75,\n    \"b18\": 0.7,\n    \"b24\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b28\": 0.55,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges in delta-based pairwise-correlating data prefetching, particularly focusing on the need to reduce storage overhead and improve prefetching accuracy and timeliness. The proposed solution, Runahead MetaData (RMD), aims to harness multi-degree prefetching by using separate metadata tables to predict the next and next-but-one events, thereby addressing the lookahead limitation and improving performance.\",\n  \"Direct Inspiration\": [\"b7\", \"b8\", \"b24\"],\n  \"Indirect Inspiration\": [\"b15\", \"b18\", \"b29\"],\n  \"Other Inspiration\": [\"b3\", \"b4\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficient data prefetching in multi-core systems by proposing a novel method, Runahead MetaData (RMD), for harnessing multi-degree prefetching in the context of pairwise-correlating prefetchers. The key idea is to use separate metadata tables (D1 and D2) to predict the next and the next-but-one events, improving prefetch accuracy and timeliness while reducing storage overhead and memory bandwidth consumption.\",\n    \"Direct Inspiration\": {\n        \"b7\": 1,\n        \"b8\": 1,\n        \"b24\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.8,\n        \"b14\": 0.7,\n        \"b15\": 0.8,\n        \"b18\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b17\": 0.6,\n        \"b19\": 0.5,\n        \"b29\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of optimizing data prefetching in multi-and many-core systems by revisiting delta-based pairwise-correlating data prefetching. The proposed solution, Runahead MetaData (RMD), introduces a novel method to harness multi-degree prefetching by using two separate metadata tables to predict the next and the next but one expected events, improving accuracy and timeliness of prefetching.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b8\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b24\": 0.7,\n        \"b29\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.5,\n        \"b18\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the lack of sufficient lookahead information in pairwise-correlating prefetchers, which makes end-of-the-stream detection difficult and results in poor accuracy with prior multi-degree prefetching mechanisms. The proposed algorithm, Runahead MetaData (RMD), introduces an additional metadata table (D2) to enhance multi-degree prefetching by predicting the next but one expected event, thereby improving timeliness and accuracy.\",\n    \"Direct Inspiration\": {\n        \"b7\": 1.0,\n        \"b8\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b24\": 0.8,\n        \"b15\": 0.7,\n        \"b18\": 0.7,\n        \"b29\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b4\": 0.6\n    }\n}\n```"], "61cacea45244ab9dcb0ca982": ["```json\n{\n  \"Summary\": \"The paper discusses the development of a Spatial-Temporal Spike algorithm to address challenges in high dynamic range (HDR) imaging and efficient data transmission. Key contributions include the use of ON and OFF pathways inspired by biological systems, and the implementation of advanced event-based vision techniques.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.95,\n    \"b16\": 0.9,\n    \"b18\": 0.85,\n    \"b19\": 0.85,\n    \"b20\": 0.85,\n    \"b22\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.7,\n    \"b35\": 0.7,\n    \"b37\": 0.7,\n    \"b38\": 0.7,\n    \"b39\": 0.7,\n    \"b40\": 0.7,\n    \"b41\": 0.7,\n    \"b42\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.6,\n    \"b44\": 0.6,\n    \"b45\": 0.6,\n    \"b46\": 0.6,\n    \"b47\": 0.6,\n    \"b48\": 0.6,\n    \"b49\": 0.6,\n    \"b50\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in high dynamic range (HDR) imaging, ON and OFF event detection, and efficient data transmission using spatial-temporal spike processing inspired by biological systems. The proposed algorithm utilizes event-based asynchronous processing to achieve high-speed and efficient data handling.\",\n    \"Direct Inspiration\": {\n        \"b10\": 1,\n        \"b16\": 0.9,\n        \"b18\": 1,\n        \"b19\": 1,\n        \"b20\": 1,\n        \"b21\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b23\": 0.8,\n        \"b22\": 0.8,\n        \"b37\": 0.7,\n        \"b38\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b39\": 0.6,\n        \"b45\": 0.5,\n        \"b46\": 0.5,\n        \"b51\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include high dynamic range (HDR) image processing, the spatial-temporal spike processing, dealing with ON and OFF signals, and achieving high-speed data transmission. The authors propose novel methods such as using magnocellular and parvocellular pathways, advancements in asynchronous event-based sensors (AER), and integrating various DVS, ATIS, DAVIS, CeleX, and Vidar technologies.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.95,\n    \"b16\": 0.9,\n    \"b18\": 0.85,\n    \"b19\": 0.85,\n    \"b20\": 0.85,\n    \"b23\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.75,\n    \"b35\": 0.75,\n    \"b36\": 0.75,\n    \"b37\": 0.75,\n    \"b38\": 0.75,\n    \"b39\": 0.75,\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b40\": 0.7,\n    \"b41\": 0.7,\n    \"b42\": 0.7,\n    \"b43\": 0.7,\n    \"b44\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.65,\n    \"b46\": 0.65,\n    \"b47\": 0.65,\n    \"b48\": 0.65,\n    \"b49\": 0.65,\n    \"b50\": 0.65,\n    \"b51\": 0.65,\n    \"b52\": 0.65,\n    \"b53\": 0.65,\n    \"b54\": 0.65,\n    \"b55\": 0.65,\n    \"b56\": 0.65,\n    \"b57\": 0.65,\n    \"b58\": 0.65,\n    \"b59\": 0.65,\n    \"b60\": 0.65,\n    \"b61\": 0.65,\n    \"b62\": 0.65,\n    \"b63\": 0.65,\n    \"b64\": 0.65,\n    \"b65\": 0.65,\n    \"b66\": 0.65,\n    \"b68\": 0.65,\n    \"b69\": 0.65,\n    \"b70\": 0.65,\n    \"b71\": 0.65,\n    \"b72\": 0.65,\n    \"b73\": 0.65,\n    \"b75\": 0.65,\n    \"b76\": 0.65,\n    \"b77\": 0.65,\n    \"b78\": 0.65,\n    \"b79\": 0.65,\n    \"b80\": 0.65,\n    \"b81\": 0.65,\n    \"b82\": 0.65,\n    \"b83\": 0.65,\n    \"b84\": 0.65,\n    \"b85\": 0.65,\n    \"b86\": 0.65,\n    \"b87\": 0.65,\n    \"b88\": 0.65,\n    \"b89\": 0.65,\n    \"b90\": 0.65,\n    \"b91\": 0.65,\n    \"b92\": 0.65,\n    \"b93\": 0.65,\n    \"b94\": 0.65,\n    \"b95\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in developing Spatial-Temporal Spike-based systems, including high dynamic range, efficient data processing rates, and the integration of magnocellular and parvocellular pathways. The authors propose a novel approach to solve these challenges using inspiration from prior works on event-based vision sensors and AER (Address Event Representation) systems.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b16\": 0.9,\n    \"b18\": 0.9,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b37\": 0.6,\n    \"b38\": 0.6,\n    \"b39\": 0.6,\n    \"b40\": 0.6,\n    \"b41\": 0.6,\n    \"b42\": 0.6,\n    \"b43\": 0.6,\n    \"b44\": 0.6,\n    \"b45\": 0.5,\n    \"b46\": 0.5,\n    \"b47\": 0.5,\n    \"b48\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in high dynamic range (HDR) imaging, spatial-temporal spike processing, and the development of advanced vision sensors like DVS, ATIS, DAVIS, and CeleX. It leverages architectures and methods inspired by previous works in neuromorphic engineering and AER (Address Event Representation) systems.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.95,\n    \"b16\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b20\": 0.8,\n    \"b22\": 0.7,\n    \"b23\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.7,\n    \"b35\": 0.65,\n    \"b37\": 0.6,\n    \"b38\": 0.6,\n    \"b39\": 0.65\n  }\n}\n```"], "619715f95244ab9dcb1855b0": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in this paper are improving the efficiency of pre-trained language models (PLMs) and building models with fewer parameters and less computation cost while retaining high model capacity. The paper introduces two novel methods: replacing masked language modeling (MLM) in DeBERTa with replaced token detection (RTD) and a new gradient-disentangled embedding sharing (GDES) method.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9,\n    \"b32\": 0.8,\n    \"b12\": 0.8,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b28\": 0.6,\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the efficiency of pre-trained language models (PLMs) by reducing parameters and computational cost while retaining model capacity. The authors propose two methods for enhancing DeBERTa's pre-training efficiency: replacing masked language modeling (MLM) with replaced token detection (RTD) and introducing a gradient-disentangled embedding sharing (GDES) method. These methods are validated through empirical results on various natural language understanding (NLU) benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.75,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.6,\n    \"b31\": 0.6,\n    \"b2\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the efficiency of pre-training DeBERTa models with fewer parameters and less computation cost while retaining high model capacity. It proposes two main methods: replacing MLM with RTD and introducing a new Gradient-Disentangled Embedding Sharing (GDES) method to improve training efficiency and model performance.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in this paper is improving the efficiency of pre-trained language models (PLMs) in terms of parameter count and computation cost while maintaining high performance. The paper introduces two main methods: replacing Masked Language Modeling (MLM) with Replaced Token Detection (RTD) in DeBERTa training, and a Gradient-Disentangled Embedding Sharing (GDES) method to mitigate the inefficiencies caused by embedding sharing between the generator and discriminator.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of improving the efficiency of pre-trained language models (PLMs) while retaining high model capacity. The authors propose two main methods: replacing masked language modeling (MLM) in DeBERTa with replaced token detection (RTD) as used in ELECTRA, and introducing a new gradient-disentangled embedding sharing (GDES) method to balance the training dynamics between the generator and discriminator. The paper demonstrates that these methods significantly enhance pre-training efficiency and model performance on various natural language understanding (NLU) benchmarks.\",\n  \"Direct Inspiration\": [\"b6\"],\n  \"Indirect Inspiration\": [\"b18\", \"b32\"],\n  \"Other Inspiration\": [\"b26\", \"b31\", \"b42\"]\n}\n```"], "618b64306750f8456ca9f190": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of exploring microarchitecture designs for the BOOM core to find a balance between power and performance. The main contributions are a microarchitecture-aware active learning algorithm (MicroAL) and a novel Gaussian process model with deep kernel learning (DKL-GP) for design space exploration and optimization.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b15\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b22\": 0.8,\n    \"b23\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b18\": 0.7,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b30\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Finding a good microarchitecture that balances power and performance in RISC-V BOOM design space. The challenge includes the large design space and the high time cost for acquiring metrics.\",\n    \"Inspirations\": \"The paper proposes BOOM-Explorer, which introduces a microarchitecture-aware active learning methodology and a novel Gaussian process model with deep kernel learning to explore the design space.\"\n  },\n  \"Direct Inspiration\": {\n    \"b15\": 0.95,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.85,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b18\": 0.65,\n    \"b19\": 0.6,\n    \"b20\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of finding a good microarchitecture design that balances power and performance in the RISC-V BOOM core. It introduces BOOM-Explorer, which utilizes a microarchitecture-aware active learning algorithm based on transductive experimental design and a novel Gaussian process model with deep kernel learning functions to explore the design space efficiently.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b15\": 0.95,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b21\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.65,\n    \"b1\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of finding a good microarchitecture design that balances power and performance. Inspired by previous works, the authors propose BOOM-Explorer, a framework that combines MicroAL, DKL-GP, and Bayesian optimization to explore the design space efficiently and accurately.\",\n  \"Direct Inspiration\": [\"b15\", \"b16\"],\n  \"Indirect Inspiration\": [\"b5\", \"b17\", \"b18\", \"b21\", \"b22\", \"b23\"],\n  \"Other Inspiration\": [\"b24\", \"b25\", \"b26\", \"b27\", \"b28\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in exploring the microarchitecture design space for RISC-V BOOM processors, focusing on balancing power and performance. The proposed BOOM-Explorer framework uses a microarchitecture-aware active learning algorithm (MicroAL) and a novel Gaussian process model with deep kernel learning (DKL-GP) to efficiently sample and characterize designs. This approach is aimed at achieving Pareto optimality in power-performance trade-offs.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.95,\n    \"b15\": 0.90,\n    \"b16\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b18\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b24\": 0.7\n  }\n}\n```"], "60becf8391e01184918177fb": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of solving nonconvex-nonconcave minimax problems by proposing a new method called the Fast Extragradient (FEG) method, which achieves an accelerated convergence rate. The FEG method is inspired by existing methods such as EG+ and EAG, and it improves upon their convergence rates by reusing terms in the updates.\",\n  \"Direct Inspiration\": [\"b6\", \"b42\"],\n  \"Indirect Inspiration\": [\"b5\", \"b11\"],\n  \"Other Inspiration\": [\"b9\", \"b26\", \"b13\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of nonconvex-nonconcave minimax optimization problems. It proposes an efficient first-order method, called the Fast Extragradient (FEG) method, which achieves an accelerated convergence rate of O(1/k^2) under the negative comonotonicity condition. The FEG method is also extended to adaptive (FEG-A) and stochastic (S-FEG) variants to handle cases where problem parameters are unknown or when stochastic oracles are used.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b42\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b11\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b24\": 0.7,\n    \"b30\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of solving nonconvex-nonconcave minimax problems by proposing an efficient first-order method named Fast Extragradient (FEG). The FEG method builds upon existing extragradient methods and Halpern-type methods to achieve an accelerated convergence rate under Lipschitz continuity and negative comonotonicity conditions.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b42\": 1,\n    \"b5\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b18\": 0.8,\n    \"b25\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b24\": 0.6,\n    \"b30\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses nonconvex-nonconcave minimax problems and proposes the Fast Extragradient (FEG) method which achieves an accelerated convergence rate of O(1/k^2) on the squared gradient norm. The FEG method is inspired by and builds upon existing methods such as EG+ and EAG. It also introduces an adaptive variant (FEG-A) and a stochastic version (S-FEG) to handle practical scenarios where problem parameters are unavailable or computations are expensive.\",\n    \n    \"Direct Inspiration\": {\n        \"b6\": 1,\n        \"b42\": 1,\n        \"b5\": 0.9\n    },\n    \n    \"Indirect Inspiration\": {\n        \"b11\": 0.8,\n        \"b13\": 0.7\n    },\n    \n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b24\": 0.6,\n        \"b30\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on solving nonconvex-nonconcave minimax problems, particularly under the negative comonotonicity condition. It proposes the Fast Extragradient (FEG) method, which has an accelerated convergence rate of O(1/k^2) on the squared gradient norm. The paper also introduces a backtracking line-search variant (FEG-A) and a stochastic version (S-FEG). The main contributions include the development of FEG, FEG-A, and S-FEG, and their convergence analyses.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b6\": 0.95,\n    \"b42\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.85,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.75,\n    \"b25\": 0.7,\n    \"b41\": 0.7\n  }\n}\n```"], "6087f2ff91e011e25a316d31": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of training a large-scale Chinese Pre-trained Language Model (PLM) named PanGu-\u03b1 with up to 200 billion parameters. The primary challenges include model design, training corpora, and distributed training. The proposed PanGu-\u03b1 model is inspired by the GPT-3 model and incorporates a Transformer-based autoregressive language model with an additional query layer. The authors also develop a large-scale Chinese text corpus and implement a five-dimensional parallel training strategy to handle the computational demands.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b5\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of creating a large-scale Chinese Pre-trained Language Model (PLM) named PanGu-? with up to 200 billion parameters. The primary challenges include model design, training corpora, and distributed training. The model design is inspired by GPT-3 and involves a transformer-based autoregressive language model with an additional query layer. The training data is meticulously curated and cleaned from a wide range of sources to ensure high quality and diversity. Distributed training is managed through a five-dimensional parallel strategy on a cluster of 2048 Ascend AI processors.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b10\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b5\": 0.7,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three primary challenges in developing the PanGu-? model: Model Design, Training Corpora, and Distributed Training. It proposes a Transformer-based autoregressive language model with an additional query layer, a comprehensive data collection and cleaning process for high-quality Chinese text corpora, and a five-dimensional parallel training strategy to efficiently manage memory and computational demands for training a model with up to 200 billion parameters.\",\n  \"Direct Inspiration\": [\"b1\", \"b10\", \"b11\"],\n  \"Indirect Inspiration\": [\"b5\", \"b2\", \"b6\"],\n  \"Other Inspiration\": [\"b14\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the challenges of training a large-scale Chinese PLM named PanGu- with up to 200 billion parameters. These challenges include model design, training corpora, and distributed training. The paper proposes a Transformer-based autoregressive language model with an additional query layer to handle these challenges.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b5\": 0.7,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are related to model design, training corpora, and distributed training. The authors propose a very large-scale Chinese PLM named PanGu-? with up to 200 billion parameters, addressing these challenges with a Transformer-based autoregressive language model, a high-quality 1.1TB Chinese text corpus, and a five-dimensional parallelization strategy.\",\n  \"Direct Inspiration\": [\"b1\", \"b10\"],\n  \"Indirect Inspiration\": [\"b5\", \"b11\"],\n  \"Other Inspiration\": [\"b2\", \"b6\", \"b12\", \"b13\"]\n}\n```"], "5ffec7ab91e011dbe570781e": ["```json\n{\n  \"Summary\": \"The paper addresses the 'Memory Wall' problem caused by unbalanced technological advancements in processor and memory. It proposes the Pure Prefetch Coverage (PPC) metric to evaluate prefetcher performance in concurrent memory accesses and introduces the Adaptive Prefetch Considers Access Concurrency (APAC) framework to dynamically adjust prefetch aggressiveness based on runtime data access patterns.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b8\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b13\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.65,\n    \"b9\": 0.6,\n    \"b2\": 0.55,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the 'Memory Wall' problem, where advancements in processor speeds have outpaced memory speeds, leading to significant performance bottlenecks. The paper introduces a novel metric called Pure Prefetch Coverage (PPC) to evaluate the effectiveness of prefetchers in concurrent memory access environments. The proposed adaptive prefetch framework, APAC, dynamically adjusts prefetch aggressiveness based on runtime measurements of PPC, prefetch accuracy (PA), and pure miss rate (pMR).\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b20\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the 'Memory Wall' problem by introducing a new metric, Pure Prefetch Coverage (PPC), which extends the traditional Prefetch Coverage (PC) metric to consider concurrent memory accesses. The proposed adaptive prefetch framework, APAC, dynamically adjusts prefetch aggressiveness based on PPC, prefetch accuracy (PA), and pure miss rate (pMR). The framework aims to optimize the performance by reducing pure misses in concurrent data access environments.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b2\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b10\": 0.6,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the 'Memory Wall' problem by introducing Pure Prefetch Coverage (PPC) and an adaptive prefetching framework called APAC. The PPC metric is designed to evaluate prefetcher effectiveness considering concurrent memory accesses, improving upon existing metrics like Prefetch Coverage (PC). APAC dynamically adjusts prefetcher aggressiveness based on PPC and other metrics to enhance performance in different memory access phases.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b20\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b6\": 0.6,\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the 'Memory Wall' problem due to technological advancements in processors outpacing memory improvements. It introduces the Pure Prefetch Coverage (PPC) metric, which extends traditional prefetch coverage (PC) to consider concurrent memory accesses. This new metric allows for more accurate evaluation of prefetchers by focusing on the reduction of pure misses. The paper also proposes the Adaptive Prefetcher that Considers Access Concurrency (APAC), which dynamically adjusts prefetch aggressiveness based on PPC, prefetch accuracy (PA), and pure miss rate (pMR). The effectiveness of PPC and APAC is demonstrated through correlation analysis and experimental results, showing improvements over state-of-the-art frameworks like FDP and NST.\",\n    \"Direct Inspiration\": {\n        \"b4\": 0.9,\n        \"b8\": 0.8,\n        \"b17\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.7,\n        \"b20\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b6\": 0.65,\n        \"b9\": 0.55\n    }\n}\n```"], "600e989e91e0118e94bbcaab": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of Chinese procedure terminology normalization in Electronic Health Records (EHR), focusing on issues such as multi-implication, short text, keyword sensitivity, and efficiency. The proposed solution is a novel 'combined recall and rank' framework incorporating a multi-task candidate generator (MTCG), a keywords attentive ranker (KAR), and a fusion block (FB).\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b8\": 0.6,\n    \"b10\": 0.6,\n    \"b12\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in Chinese procedure terminology normalization from Electronic Health Records (EHR). The main challenges include multi-implication, short text nature, keyword sensitivity, and efficiency. To tackle these, the authors propose a 'combined recall and rank' framework with a multi-task candidate generator (MTCG), a keywords attentive ranker (KAR), and a fusion block (FB). The novel contributions are the combined recall and rank framework, an online negative sampling strategy, and the keywords attentive ranker.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.8,\n        \"b14\": 0.8,\n        \"b15\": 0.8,\n        \"b16\": 0.8,\n        \"b17\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.6,\n        \"b12\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of Chinese procedure terminology normalization in EHRs, including multi-implication, short text, keyword sensitivity, and efficiency. The proposed solution is a 'combined recall and rank' framework with a multi-task candidate generator (MTCG), a keywords attentive ranker (KAR), and a fusion block (FB). The novel contributions include a fusion block to merge recall and rank results, an effective online negative sampling strategy, and a keywords attentive ranker focusing on procedure site and type.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b4\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b13\", \"b14\", \"b15\", \"b16\", \"b17\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b11\", \"b12\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in Chinese procedure terminology normalization within Electronic Health Records (EHR). It identifies four main challenges: multi-implication, short text, keyword sensitivity, and high efficiency. The proposed solution is a novel 'combined recall and rank' framework utilizing a multi-task candidate generator (MTCG), a keywords attentive ranker (KAR), and a fusion block (FB). Key innovations include an effective online negative sampling strategy and the use of keywords to improve ranking accuracy.\",\n  \"Direct Inspiration\": {\n    \"reference_numbers\": [\"b4\", \"b14\", \"b15\", \"b16\", \"b17\"]\n  },\n  \"Indirect Inspiration\": {\n    \"reference_numbers\": [\"b5\", \"b6\", \"b8\", \"b10\", \"b12\", \"b18\", \"b19\", \"b20\", \"b21\", \"b22\", \"b23\", \"b24\", \"b25\", \"b26\", \"b27\", \"b28\", \"b29\", \"b30\", \"b31\", \"b32\", \"b33\"]\n  },\n  \"Other Inspiration\": {\n    \"reference_numbers\": [\"b11\", \"b13\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of normalizing Chinese procedure terminologies in electronic health records (EHR). The primary challenges include multi-implication, short text, keyword sensitivity, and efficiency. The proposed solution is a novel 'combined recall and rank' framework consisting of a multi-task candidate generator (MTCG), a keywords attentive ranker (KAR), and a fusion block (FB). The approach aims to improve recall rate, utilize keyword information better, and handle multi-implication problems effectively and efficiently.\",\n  \"Direct Inspiration\": [\n    \"b4\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b5\",\n    \"b6\",\n    \"b8\",\n    \"b9\",\n    \"b10\",\n    \"b11\",\n    \"b12\",\n    \"b14\",\n    \"b15\",\n    \"b16\",\n    \"b17\",\n    \"b18\",\n    \"b19\",\n    \"b20\"\n  ],\n  \"Other Inspiration\": []\n}\n```"], "600832489e795ed227f530f8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of oversmoothing in Graph Convolutional Networks (GCNs) and proposes a Simple Spectral Graph Convolution (S2GC) network. The proposed S2GC network aims to improve node clustering, node classification, and graph classification by aggregating k-step diffusion matrices and using a linear model to balance global and local context aggregation.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b13\": 0.6,\n    \"b21\": 0.5,\n    \"b7\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of oversmoothing in deep Graph Convolutional Networks (GCNs) and proposes a Simple Spectral Graph Convolution (S2GC) network for node clustering, classification, and graph classification. The approach leverages Markov Diffusion Kernel to aggregate k-step diffusion matrices and incorporates larger neighborhoods while balancing local and global contexts.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b15\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b9\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a Simple Spectral Graph Convolution (S2GC) network to address the challenges of oversmoothing and computational inefficiency in Graph Convolutional Networks (GCNs). The approach leverages the Markov Diffusion Kernel to aggregate k-step diffusion matrices, balancing the global and local contexts of each node. S2GC shows improved performance in node clustering, classification, and property prediction tasks.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b15\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b21\": 0.7,\n    \"b35\": 0.7,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b8\": 0.5,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of oversmoothing in Graph Convolutional Networks (GCNs) and proposes a Simple Spectral Graph Convolution (S2GC) network to improve node clustering, classification, and graph classification. It builds on previous work by utilizing a Markov Diffusion Kernel and aims to balance neighborhood aggregation to mitigate oversmoothing while preserving both local and global node contexts.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b15\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of oversmoothing in Graph Convolutional Networks (GCNs) by proposing the Simple Spectral Graph Convolution (S2GC) network. The method aggregates multi-step diffusion matrices to capture broader neighborhoods while balancing local and global contexts effectively, preventing oversmoothing and improving performance in node clustering, node classification, and graph classification tasks.\",\n  \"Direct Inspiration\": [\"b14\", \"b15\"],\n  \"Indirect Inspiration\": [\"b30\", \"b13\"],\n  \"Other Inspiration\": [\"b17\", \"b21\"]\n}\n```"], "60efb1495244ab9dcbce0fb8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of drug-target interaction (DTI) prediction using computational methods. It introduces a novel deep neural network architecture called DTI-GAT (Drug-Target Interaction prediction with Graph Attention networks), which utilizes graph-structured data and an attention mechanism to predict DTIs by constructing a heterogeneous graph and applying graph attention networks to generate embeddings for proteins and drugs. The main contributions include transforming feature representations into a protein-drug interaction graph, using an attention mechanism to extract high-level relationships, and providing better interpretability of learned attention weights.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b37\": 0.9,\n    \"b21\": 0.9,\n    \"b24\": 0.9,\n    \"b43\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.7,\n    \"b4\": 0.7,\n    \"b15\": 0.6,\n    \"b36\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b3\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reliably predicting drug-target interactions (DTIs) using computational methods to accelerate drug development and reduce costs. The authors propose a novel deep learning approach called DTI-GAT, which leverages Graph Attention Networks (GAT) to handle graph-structured data and apply an attention mechanism to better capture interaction patterns.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b24\": 0.9,\n    \"b21\": 0.9,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.8,\n    \"b4\": 0.7,\n    \"b15\": 0.7,\n    \"b41\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in predicting drug-target interactions (DTIs) using computational methods, leveraging deep learning and graph-based techniques to enhance prediction accuracy and interpretability. The proposed method, DTI-GAT, introduces a graph attention network to handle heterogeneous data and improve feature extraction and interaction prediction through attention mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.95,\n    \"b33\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.85,\n    \"b21\": 0.80,\n    \"b37\": 0.75,\n    \"b43\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include effective filtering and aggregation of local features of proteins and drugs, interpretation issues of deep neural architectures, and the need for scalability to large data. The paper introduces DTI-GAT, a deep neural network architecture using graph attention networks (GAT) to predict drug-target interactions (DTIs). Key contributions include transforming feature representations into a protein-drug interaction graph, utilizing attention mechanisms to extract high-level relationships, and providing better interpretability of learned attention weights.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.9,\n    \"b43\": 0.85,\n    \"b24\": 0.85,\n    \"b21\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.8,\n    \"b16\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in drug-target interaction (DTI) prediction by proposing a novel deep learning architecture called DTI-GAT (Drug-Target Interaction with Graph ATtention networks). The primary challenges include effective filtering and aggregation of local features of proteins and drugs while preserving significant contextual and sequential information, the interpretation issue in deep neural architectures without biological insights, and apprehending the mutual influence of protein-drug pairs. DTI-GAT combines similarity-based and feature-based methods, using graph attention networks to generate embeddings for proteins and drugs and predict interactions with a final decoder architecture.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b21\": 1.0,\n    \"b37\": 1.0,\n    \"b43\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```"], "618ba7765244ab9dcbbf4797": ["```json\n{\n  \"Summary\": \"The primary challenges in the paper include the efficient construction of a high-quality process knowledge base for automated process design in manufacturing. The paper introduces a framework for automatically constructing a knowledge base in the machining field, leveraging the function-behavior-states (FBS) design framework, fuzzy comprehensive evaluation algorithm, and a knowledge extraction system based on the browser/server (B/S) architecture. This system aims to automate the extraction of manufacturing knowledge from simulation models and process documents.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b5\", \"b7\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b49\", \"b39\", \"b46\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b41\", \"b42\", \"b43\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in constructing a high-quality process knowledge base for intelligent process design in the manufacturing field. Major challenges include the time-consuming and knowledge-intensive nature of product manufacturing, the diversity and heterogeneity of manufacturing-related knowledge, and the inefficiencies in traditional manual methods. The proposed framework includes an architecture for the automatic extraction of manufacturing knowledge, a knowledge representation method using the FBS design framework and fuzzy comprehensive evaluation algorithm, and a knowledge extraction system based on a browser/server architecture.\",\n  \"Direct Inspiration\": {\n    \"b49\": 0.9,\n    \"b39\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b2\": 0.75,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.65,\n    \"b42\": 0.6,\n    \"b46\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of constructing a high-quality process knowledge base in a shorter time frame for modern manufacturing models. It proposes a framework for an automated construction of a knowledge base in the machining field, involving automated extraction of manufacturing knowledge from existing simulation models and process documents, and using a fuzzy comprehensive evaluation algorithm for knowledge representation. The framework aims to improve efficiency in product manufacturing by reducing manual efforts and enhancing knowledge sharing and reuse.\",\n  \"Direct Inspiration\": {\n    \"b49\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.6,\n    \"b7\": 0.7,\n    \"b41\": 0.6,\n    \"b42\": 0.6,\n    \"b43\": 0.6,\n    \"b44\": 0.6,\n    \"b45\": 0.6,\n    \"b46\": 0.6,\n    \"b47\": 0.6,\n    \"b48\": 0.6,\n    \"b50\": 0.6,\n    \"b51\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5,\n    \"b33\": 0.5,\n    \"b34\": 0.5,\n    \"b35\": 0.5,\n    \"b36\": 0.5,\n    \"b37\": 0.5,\n    \"b38\": 0.5,\n    \"b39\": 0.5,\n    \"b40\": 0.5,\n    \"b41\": 0.5,\n    \"b42\": 0.5,\n    \"b43\": 0.5,\n    \"b44\": 0.5,\n    \"b45\": 0.5,\n    \"b46\": 0.5,\n    \"b47\": 0.5,\n    \"b48\": 0.5,\n    \"b49\": 0.5,\n    \"b50\": 0.5,\n    \"b51\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the time-consuming and knowledge-intensive nature of product manufacturing, the difficulty in sharing and reusing diverse and heterogeneous manufacturing-related knowledge, and the lack of formal representations of existing design knowledge. The proposed solution includes a framework for the knowledge-based engineering system that supports the automated construction of a knowledge base. The core contributions are an architecture for automatically constructing a knowledge base, using the FBS (Function-Behavior-State) design framework and a knowledge extraction system based on the browser/server architecture.\",\n  \"Direct Inspiration\": {\n    \"b49\": 0.9,\n    \"b41\": 0.85,\n    \"b42\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.7,\n    \"b47\": 0.7,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.6,\n    \"b46\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in constructing a high-quality process knowledge base for intelligent process design in manufacturing. The main contributions include developing an architecture for automatic knowledge base construction, summarizing knowledge types using the FBS design framework, and creating a knowledge extraction system based on a browser/server architecture.\",\n  \"Direct Inspiration\": {\n    \"b49\": 0.95,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.85,\n    \"b42\": 0.85,\n    \"b43\": 0.8,\n    \"b44\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b51\": 0.75\n  }\n}\n```"], "61179e3391e011ffc72e1111": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include reducing the latency of application switch times under memory pressure in mobile devices. The algorithm proposed, ASAP, aims to minimize this latency by utilizing adaptive prepaging that takes advantage of underutilized hardware resources during application switches. Key observations include the underutilization of CPU and disk bandwidth during switches and the difference in access patterns between file-backed and anonymous pages.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.6,\n    \"b8\": 0.6,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the latency during application switches on mobile devices under memory pressure, which negatively impacts user experience. The proposed algorithm, ASAP (Adaptive Switch-Aware Prepaging), aims to reduce this latency by effectively utilizing available hardware resources through adaptive prepaging of memory pages.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b20\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b6\": 0.7,\n    \"b21\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the high latency of application switch times under memory pressure in mobile devices. The proposed solution, ASAP, is an adaptive prepaging mechanism designed to reduce switch times by utilizing available CPU and disk resources more effectively and by accurately predicting the switch footprint of accessed pages.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b8\": 0.9,\n    \"b16\": 0.9,\n    \"b20\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b6\": 0.6,\n    \"b13\": 0.6,\n    \"b15\": 0.6,\n    \"b26\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of reducing latency in application switching under memory pressure in mobile devices. It proposes ASAP, an adaptive prepaging mechanism that utilizes empirical observations about hardware resource usage and differing characteristics of file-backed and anonymous pages to minimize demand-paging related slowdowns.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b26\": 0.7,\n    \"b16\": 0.6,\n    \"b21\": 0.6,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing latency during application switches on mobile devices under memory pressure. The proposed solution, ASAP (Adaptive Prepaging), aims to utilize underutilized CPU and disk resources to prepage memory pages, significantly reducing switch time and improving user experience.\",\n  \"Direct Inspiration\": [\"b1\", \"b8\"],\n  \"Indirect Inspiration\": [\"b16\", \"b20\", \"b21\", \"b26\"],\n  \"Other Inspiration\": [\"b2\", \"b6\"]\n}\n```"], "60dafe6c1e8bd4ede2475afe": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve the efficient execution of DNNs on heterogeneous hardware, specifically CPUs and GPUs, due to their different execution characteristics and the complexity of DNN operations. The proposed algorithm, DUET, aims to optimize DNN inference by leveraging both CPU and GPU concurrently through a compiler-aware approach, involving coarse-grained partitioning, runtime profiling, and a greedy-correction subgraph scheduling algorithm.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b25\": 0.8,\n    \"b27\": 0.8,\n    \"b31\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b37\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the efficient inference of deep neural networks (DNNs) on heterogeneous hardware (CPU and GPU) due to their differing characteristics and the complexity of tensor operations. The paper proposes DUET, a DNN inference engine that supports concurrent execution on heterogeneous hardware by employing a coarse-grained partitioning strategy and a greedy-correction subgraph scheduling algorithm. The approach aims to improve scheduling decisions by involving a deep learning compiler in the heterogeneity optimization loop.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b31\": 0.8,\n        \"b34\": 0.8,\n        \"b37\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.7,\n        \"b25\": 0.7,\n        \"b27\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge is optimizing deep neural networks (DNNs) for efficient execution on coupled CPU-GPU architectures. The proposed solution, DUET, introduces a heterogeneity-conscious and compiler-aware DNN inference engine. Key contributions include a coarse-grained partitioning strategy, a greedy-correction subgraph scheduling algorithm, and leveraging the DL compiler for improved scheduling decisions.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b31\": 0.7,\n    \"b34\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b25\": 0.5,\n    \"b27\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges identified in the paper include the inefficiencies of deploying DNNs on coupled CPU-GPU architectures, the lack of support for heterogeneous hardware in existing DL compilers, and the difficulty in leveraging the diverse computation patterns of DNNs. The proposed solution, DUET, introduces a compiler-aware DNN inference engine that partitions DNN computation graphs for concurrent execution on CPUs and GPUs, utilizing a coarse-grained partitioning strategy and a greedy-correction subgraph scheduling algorithm to improve efficiency.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b31\": 0.8,\n    \"b34\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b25\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently performing deep neural network (DNN) inference on coupled CPU-GPU architectures. The primary motivation is to improve the agility of optimizing DNNs by developing an automated framework that leverages both CPU and GPU capabilities. The proposed solution, DUET, introduces a heterogeneity-conscious and compiler-aware DNN inference engine that partitions DNN computation into coarse-grained subgraphs and employs a greedy-correction scheduling algorithm to optimize execution across CPU and GPU.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b31\": 0.8,\n    \"b34\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b25\": 0.6,\n    \"b27\": 0.6,\n    \"b13\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```"], "6034e61091e01122c046f894": ["```json\n{\n  \"Summary\": \"The paper tackles the challenge of errors in Hardware Performance Counters (HPCs) due to multiplexing and proposes a Bayesian model (BayesPerf) to correct these errors. The model leverages microarchitectural relationships to improve the accuracy of HPC measurements, which is crucial for real-time decision-making in machine learning (ML) applications. The paper also introduces an accelerator for Monte Carlo-based training and inference of the model and presents an FPGA-based prototype implementation.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b18\": 1,\n    \"b28\": 1,\n    \"b42\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b3\": 0.8,\n    \"b9\": 0.8,\n    \"b13\": 0.8,\n    \"b16\": 0.8,\n    \"b20\": 0.8,\n    \"b35\": 0.8,\n    \"b36\": 0.8,\n    \"b39\": 0.8,\n    \"b47\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b4\": 0.6,\n    \"b8\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b17\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b33\": 0.6,\n    \"b38\": 0.6,\n    \"b40\": 0.6,\n    \"b41\": 0.6,\n    \"b43\": 0.6,\n    \"b44\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of errors in Hardware Performance Counters (HPCs) due to event multiplexing, and presents BayesPerf, a Bayesian model-based system to correct these errors. The main contributions include a probabilistic ML model to infer true values of HPCs, an accelerator for real-time inference, and a prototype implementation demonstrating significant error reduction and efficiency improvements.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b10\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b18\": 0.7,\n    \"b28\": 0.6,\n    \"b42\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b25\": 0.5,\n    \"b30\": 0.5,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is the error-prone nature of Hardware Performance Counters (HPCs) due to multiplexing, which impacts real-time decision-making applications. The paper introduces BayesPerf, a system using a domain-driven Bayesian model to quantify uncertainty and correct errors in HPC measurements, enhancing decision-making and reducing the need for high-frequency HPC sampling.\",\n  \"Direct Inspiration\": {\n    \"b42\": 1,\n    \"b28\": 0.9,\n    \"b6\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b3\": 0.7,\n    \"b9\": 0.7,\n    \"b16\": 0.7,\n    \"b47\": 0.7,\n    \"b12\": 0.7,\n    \"b35\": 0.7,\n    \"b36\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b25\": 0.6,\n    \"b15\": 0.6,\n    \"b30\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies the challenge of errors in Hardware Performance Counters (HPCs) measurements due to multiplexing and proposes a Bayesian model, BayesPerf, to quantify and correct these errors. The model leverages microarchitectural relationships to reduce errors and improve decision-making in real-time applications. The contributions include the BayesPerf ML model, an accelerator for real-time inference, a prototype implementation, and efficiency improvements in decision-making tasks.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b18\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.8,\n    \"b42\": 0.8,\n    \"b47\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b25\": 0.7,\n    \"b38\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high error rates in Hardware Performance Counters (HPC) measurements due to multiplexing and proposes a Bayesian model-based system, BayesPerf, to correct these errors. The system leverages microarchitectural relationships between HPCs to improve decision-making in real-time applications by reducing measurement errors and quantifying uncertainties. Key contributions include the BayesPerf ML model, a hardware accelerator for real-time inference, and an FPGA-based prototype implementation.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b6\", \"b18\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b4\", \"b8\", \"b11\", \"b28\", \"b31\", \"b42\", \"b43\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b2\", \"b10\", \"b20\", \"b25\", \"b29\", \"b30\", \"b33\", \"b38\", \"b41\", \"b44\"]\n  }\n}\n```"], "5ff68a3bd4150a363ccc615a": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of memory constraints in datacenters using a novel approach called Application-Integrated Far Memory (AIFM). This method ties swapping to application-level memory objects instead of virtual memory pages, thereby avoiding I/O amplification and context switches. The algorithm leverages four key ideas: remoteable pointers, a pauseless memory evacuator, runtime APIs for semantic information, and a remote device interface. These innovations collectively enhance memory elasticity, throughput, and overall system performance.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b39\": 0.7,\n    \"b25\": 0.7,\n    \"b12\": 0.6,\n    \"b69\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b23\": 0.5,\n    \"b72\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently using far memory in datacenters due to the inelastic nature of memory and the substantial overheads imposed by existing memory management mechanisms, such as OS swapping. The authors propose AIFM (Application-Integrated Far Memory), which allows developers to write remoteable data structures whose memory can be local or remote without affecting performance. The proposed system ties swapping to individual application-level memory objects rather than the virtual memory abstraction of pages, thus avoiding I/O amplification and high context switch costs. AIFM features a fast, low-overhead remoteable pointer abstraction, a pauseless memory evacuator, semantic-aware runtime APIs, and a remote device interface for offloading computations to remote memory.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b69\": 1.0,\n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.8,\n    \"b44\": 0.8,\n    \"b70\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b72\": 0.6,\n    \"b0\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of memory limitations in datacenters by proposing Application-Integrated Far Memory (AIFM), which ties swapping to individual application-level memory objects rather than page-level virtual memory. AIFM uses several novel techniques including a fast remoteable pointer abstraction, a pauseless memory evacuator, and efficient runtime APIs to ensure high throughput and low latency in memory access.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b25\": 0.85,\n    \"b39\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.75,\n    \"b70\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b23\": 0.6,\n    \"b69\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of memory elasticity in datacenters, specifically focusing on the inefficiencies of existing OS swap mechanisms that operate at page granularity. The proposed solution, Application-Integrated Far Memory (AIFM), integrates far memory at the application level, allowing for object-level swapping, fast context switches, and custom prefetching policies, thus improving throughput and reducing latency.\",\n  \"Direct Inspiration\": [\"b5\", \"b25\", \"b72\"],\n  \"Indirect Inspiration\": [\"b31\", \"b49\", \"b73\"],\n  \"Other Inspiration\": [\"b10\", \"b47\", \"b76\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of memory constraints in datacenters, proposing Application-Integrated Far Memory (AIFM) to increase memory elasticity and avoid the overheads of kernel-based paging mechanisms. AIFM operates at the object level rather than the page level, incorporating low-overhead remoteable pointers, a pauseless memory evacuator, and APIs that allow application and data structure developers to utilize far memory efficiently.\",\n    \"Direct Inspiration\": {\n        \"b5\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b39\": 0.8,\n        \"b70\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b25\": 0.7,\n        \"b72\": 0.7,\n        \"b0\": 0.7,\n        \"b19\": 0.7\n    }\n}\n```"], "617147af5244ab9dcbe8357d": ["```json\n{\n  \"Summary\": \"The paper addresses the urgent need for novel antiviral agents targeting SARS-CoV-2, specifically focusing on inhibiting the viral cysteine protease PLpro. The primary challenge is the design of potent and specific PLpro inhibitors, given the lack of druggable pockets at the active site. The authors propose a strategy leveraging binding cooperativity at multiple shallow binding sites, including a novel 'BL2 groove'. The paper highlights the development of 2-phenylthiophene-based PLpro inhibitors with enhanced potency and slow dissociation rates.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.9,\n    \"b26\": 0.85,\n    \"b31\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.75,\n    \"b29\": 0.7,\n    \"b28\": 0.65,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.55,\n    \"b18\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"**Summary of the challenges and inspirations of the paper**: The primary challenge outlined in the paper is the urgent need to develop potent small molecule inhibitors targeting SARS-CoV-2 papain-like protease (PLpro) to address COVID-19 and potential future outbreaks. The authors propose a novel design strategy leveraging binding cooperativity to exploit multiple shallow binding sites on the PLpro surface, including a newly identified 'BL2 groove,' to develop potent noncovalent PLpro inhibitors.\",\n  \"Direct Inspiration\": [\n    \"b25\",\n    \"b26\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b19\",\n    \"b31\",\n    \"b37\"\n  ],\n  \"Other Inspiration\": [\n    \"b17\",\n    \"b35\",\n    \"b45\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the urgent need for small molecule antiviral agents to treat COVID-19 and other zoonotic viruses. It focuses on designing and optimizing inhibitors for the SARS-CoV-2 papain-like protease (PLpro), which is crucial for viral replication and disruption of the host immune response. The authors propose that targeting multiple binding sites, including a novel 'BL2 groove,' can lead to potent inhibitors with improved efficacy over existing compounds.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b26\": 0.8,\n    \"b27\": 0.7,\n    \"b28\": 0.7,\n    \"b46\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b24\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the urgent need for effective antiviral agents against COVID-19, specifically targeting the SARS-CoV-2 papain-like protease (PLpro). It proposes the design and optimization of novel PLpro inhibitors based on structure-based drug design, focusing on binding cooperativity and the identification of a novel binding site, the BL2 groove. The paper demonstrates the efficacy of these inhibitors in biochemical and cellular assays, highlighting their potential for further development.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b26\": 0.9,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b31\": 0.8,\n    \"b36\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b7\": 0.5,\n    \"b8\": 0.5,\n    \"b9\": 0.5,\n    \"b10\": 0.5,\n    \"b11\": 0.5,\n    \"b12\": 0.5,\n    \"b13\": 0.5,\n    \"b14\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b24\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5,\n    \"b29\": 0.5,\n    \"b30\": 0.5,\n    \"b33\": 0.5,\n    \"b34\": 0.5,\n    \"b35\": 0.5,\n    \"b38\": 0.5,\n    \"b39\": 0.5,\n    \"b40\": 0.5,\n    \"b41\": 0.5,\n    \"b42\": 0.5,\n    \"b43\": 0.5,\n    \"b44\": 0.5,\n    \"b45\": 0.5,\n    \"b46\": 0.5,\n    \"b47\": 0.5,\n    \"b48\": 0.5,\n    \"b49\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the urgent need to develop small molecule antiviral agents against SARS-CoV-2, specifically targeting the viral protease PLpro. It outlines the challenges in designing potent PLpro inhibitors due to the restricted binding pockets and the featureless P1 and P2 sites. The study is inspired by previous work on the SARS-CoV PLpro inhibitor GRL0617 and aims to improve potency through structure-based design leveraging binding cooperativity at multiple shallow binding sites, including a novel BL2 groove.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b26\": 0.9,\n    \"b27\": 0.8,\n    \"b31\": 0.85,\n    \"b37\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.6,\n    \"b7\": 0.65,\n    \"b8\": 0.7,\n    \"b10\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.55,\n    \"b23\": 0.5,\n    \"b29\": 0.8,\n    \"b46\": 0.85\n  }\n}\n```"], "60d4314191e0112ca5d189e0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting general event boundaries in long-term videos, a task more subjective in nature compared to other temporal detection tasks. The proposed method uses a Temporal Self-Similarity Matrix (TSM) to identify variations in video snippet features near event boundaries and employs contrastive learning to enhance the TSM representation. The architecture consists of two passes: one using TSM and another direct approach, with the final output being a combination of both predictions.\",\n  \"Direct Inspiration\": [\"b6\", \"b8\"],\n  \"Indirect Inspiration\": [\"b10\", \"b5\", \"b4\", \"b11\", \"b12\", \"b18\"],\n  \"Other Inspiration\": [\"b2\", \"b17\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of detecting class-agnostic event boundaries in long-term videos, a task that is subjective and more complex than traditional temporal detection tasks. The authors propose a novel network that uses a temporal self-similarity matrix (TSM) and employs contrastive learning to improve gradient signals, resulting in better feature representation for event boundary detection. The final model combines predictions from both TSM-based and direct approaches to enhance performance.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.8,\n        \"b11\": 0.7,\n        \"b12\": 0.7,\n        \"b18\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.9\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting general event boundaries in videos, which is subjective and more complex compared to other temporal detection tasks. The proposed method employs a novel network using a Temporal Self-Similarity Matrix (TSM) and contrastive learning to enhance performance. The model consists of two pathways: one through TSM and another direct approach, both sharing an encoder.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b8\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b2\": 0.55,\n    \"b17\": 0.55\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of detecting class-agnostic event boundaries in videos, which is subjective in nature and more complex than other temporal detection tasks. The authors propose a novel network using a temporal self-similarity matrix (TSM) as an intermediate representation, combined with contrastive learning to enhance gradient signals for better event boundary detection.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.9,\n        \"b2\": 0.7,\n        \"b17\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of detecting class-agnostic event boundaries in videos, leveraging a novel network using temporal self-similarity matrix (TSM) and contrastive learning. The model combines TSM-based and direct prediction passes to improve performance.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1.0,\n        \"b8\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.8,\n        \"b12\": 0.8,\n        \"b18\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.7,\n        \"b17\": 0.7\n    }\n}\n```"], "61a88bc06750f8304711e96a": ["```json\n{\n  \"Summary\": \"The paper addresses the decentralized policy evaluation (DPE) problem for multi-agent reinforcement learning (MARL) with nonlinear function approximations. Key contributions include reformulating the DPE problem as a decentralized non-convex-strongly-concave minimax saddle point problem, proposing the GT-GDA, GT-SRVR, and GT-SRVRI algorithms, and relaxing commonly-used compactness conditions in their theoretical analysis.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b48\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.75,\n    \"b50\": 0.7,\n    \"b53\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.65,\n    \"b45\": 0.6,\n    \"b49\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of decentralized policy evaluation (DPE) in multi-agent reinforcement learning (MARL) with nonlinear function approximations. The primary contributions include the reformulation of the DPE problem as a decentralized non-convex-strongly-concave minimax problem and the development of novel algorithms (GT-GDA, GT-SRVR, GT-SRVRI) to solve it efficiently.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b48\": 0.85,\n    \"b40\": 0.8,\n    \"b12\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b50\": 0.65,\n    \"b29\": 0.6,\n    \"b30\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.55,\n    \"b53\": 0.5,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of decentralized policy evaluation (DPE) in multi-agent reinforcement learning (MARL) with nonlinear function approximations. It proposes novel algorithms (GT-GDA, GT-SRVR, and GT-SRVRI) to solve the DPE problem by reformulating it as a decentralized non-convex-strongly-concave minimax problem. The proposed methods aim to improve sample and communication efficiency while ensuring convergence.\",\n    \"Direct Inspiration\": [\"b12\", \"b48\"],\n    \"Indirect Inspiration\": [\"b11\", \"b50\"],\n    \"Other Inspiration\": [\"b49\", \"b40\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in decentralized policy evaluation (DPE) in multi-agent reinforcement learning (MARL) with nonlinear function approximations. It proposes gradient-tracking-based gradient descent-ascent (GT-GDA) and variance-reduced algorithms (GT-SRVR, GT-SRVRI) to solve the decentralized non-convex-strongly-concave minimax problem, achieving significant improvements in sample and communication complexities.\",\n  \"Direct Inspiration\": [\"b12\", \"b48\", \"b40\"],\n  \"Indirect Inspiration\": [\"b24\", \"b11\", \"b3\"],\n  \"Other Inspiration\": [\"b49\", \"b50\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The paper addresses the problem of decentralized policy evaluation (DPE) in multi-agent reinforcement learning (MARL) with nonlinear function approximations. The primary challenges include the non-observability of global accumulative reward, the complexity of MARL tasks, and the computational difficulty associated with using highly nonlinear parametric models.\",\n    \"inspirations\": \"The authors propose novel algorithms such as GT-GDA, GT-SRVR, and GT-SRVRI to solve the DPE problem efficiently. The inspirations for these algorithms are drawn from existing work in MARL, decentralized optimization, and variance reduction techniques.\"\n  },\n  \"Direct Inspiration\": {\n    \"b36\": 0.9,\n    \"b48\": 0.85,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b59\": 0.7,\n    \"b19\": 0.7,\n    \"b58\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```"], "61a887c26750f87bf870223a": ["```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is the high training cost of Transformer language models. The authors propose a novel model called Primer, which introduces modifications such as squaring ReLU activations and adding depthwise convolution layers to achieve significant reductions in training and inference costs. The paper heavily relies on evolutionary search methods and TensorFlow programs for the optimization of the Transformer architecture.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b29\": 0.7,\n    \"b48\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the challenge of reducing the training costs of Transformer language models. The authors propose modifying the TensorFlow computation graph and utilizing an evolutionary search to discover more efficient model architectures, specifically focusing on decoder-only auto-regressive language modeling. The main contributions include the introduction of Primer, a model that achieves significant performance improvements through modifications like squaring ReLU activations and adding depthwise convolution layers.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b6\": 0.9,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b17\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing the training costs of Transformer language models by proposing modifications to the TensorFlow computation graph and using evolutionary search to discover more efficient alternatives. The main contributions include the introduction of the Primer model, which achieves significant performance improvements over common Transformer variants by incorporating squared ReLU activations and depthwise convolution layers.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b24\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b1\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is reducing the training costs of Transformer language models. The authors propose an evolutionary search method using TensorFlow programs to find more efficient model architectures, resulting in the Primer model. Key innovations include squaring ReLU activations and adding depthwise convolution layers, which significantly improve training and inference efficiency.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b6\": 0.85,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b7\": 0.65,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.55,\n    \"b2\": 0.5,\n    \"b10\": 0.5,\n    \"b11\": 0.5,\n    \"b13\": 0.5,\n    \"b14\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b18\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of reducing the training costs of Transformer language models. It proposes modifications to TensorFlow computation graphs and employs evolutionary search to find more efficient architectures. The resulting model, Primer, showcases significant performance improvements, including reduced training and inference costs while maintaining or improving quality. Key novel methods include squaring ReLU activations and adding depthwise convolution layers.\",\n    \"Direct Inspiration\": {\n        \"b4\": 0.9,\n        \"b6\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.7,\n        \"b24\": 0.6,\n        \"b48\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.5,\n        \"b37\": 0.5,\n        \"b39\": 0.5\n    }\n}\n```"], "61ca80355244ab9dcba69a32": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting Graph Convolutional Networks (GCNs) to networks with heterophily (low homophily), where current GCN-based methods perform poorly. It proposes a novel Homophily-Guided Graph Convolutional Network (HOG-GCN) that introduces a homophily degree matrix to adaptively change the feature propagation process based on the homophily or heterophily between node pairs.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b27\": 0.9,\n    \"b2\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting Graph Convolutional Networks (GCNs) to networks with heterophily or low homophily. Existing GCN-based methods are ineffective for these types of networks as they assume homophily. The proposed solution is HOG-GCN, which introduces a homophily degree matrix to adaptively change the feature propagation process, thereby improving performance on heterophilic networks.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b27\": 0.9,\n    \"b2\": 0.9,\n    \"b24\": 0.9,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting Graph Convolutional Networks (GCNs) to networks with heterophily or low homophily. It introduces a new method, HOG-GCN, which incorporates a homophily degree matrix into the graph convolution framework to adaptively change the feature propagation process. This matrix is learned from both attribute and topology information to improve performance on networks with varying levels of homophily.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b27\": 0.9,\n    \"b2\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting Graph Convolutional Networks (GCNs) to handle heterophilic networks, where connected nodes often belong to different classes, contrasting with the homophilic assumption in traditional GCNs. The authors propose HOG-GCN, a homophily-guided graph convolutional network that introduces a learnable homophily degree matrix to adapt the propagation process according to the homophily degree between node pairs.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b27\": 0.85,\n    \"b2\": 0.8,\n    \"b26\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b24\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b20\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting Graph Convolutional Networks (GCNs) to networks with heterophily (low homophily). The authors propose a new method called HOG-GCN, which incorporates a homophily degree matrix to model the homophily and heterophily of networks. This matrix is learned from both attribute and topology information and is used to adaptively change the propagation process between neighborhoods. The framework aims to improve node classification by better handling both homophilic and heterophilic networks.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b14\": 0.9,\n    \"b26\": 0.9,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b20\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b17\": 0.6,\n    \"b19\": 0.6,\n    \"b4\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```"]}