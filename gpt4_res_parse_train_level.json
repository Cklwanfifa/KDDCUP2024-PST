{"5db80dc83a55acd5c14a24b9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of name disambiguation in academic profiles, proposing a novel algorithm called CONNA to improve matching accuracy by capturing both exact and semantic matches. It highlights the limitations of traditional feature-engineering methods and representation-based models, and introduces an interaction-based model to better handle the nuances of name disambiguation. The paper also integrates a self-correcting mechanism through reinforcement learning.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b12\": 0.85,\n    \"b52\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b56\": 0.7,\n    \"b57\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of accurately matching academic papers with their respective authors, focusing on capturing both exact and soft semantic matches. The proposed algorithm, CONNA, integrates interaction-based models and reinforcement learning to improve matching and decision accuracy.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b4\": 0.8,\n        \"b12\": 0.85,\n        \"b56\": 0.9,\n        \"b52\": 0.75\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.65,\n        \"b14\": 0.7,\n        \"b18\": 0.6,\n        \"b23\": 0.6,\n        \"b31\": 0.5,\n        \"b45\": 0.5,\n        \"b57\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.4,\n        \"b27\": 0.4,\n        \"b34\": 0.4,\n        \"b37\": 0.4,\n        \"b43\": 0.3,\n        \"b9\": 0.3,\n        \"b15\": 0.3,\n        \"b24\": 0.3,\n        \"b29\": 0.3,\n        \"b30\": 0.3,\n        \"b38\": 0.3,\n        \"b54\": 0.3,\n        \"b0\": 0.25\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurate name disambiguation in academic datasets, proposing an algorithm that combines traditional feature-engineering methods with representation-based models to capture both exact and semantic matches. The proposed CONNA framework integrates a matching component and a decision component, fine-tuning them through reinforcement learning to self-correct errors.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b56\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b12\": 0.8,\n    \"b52\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b34\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately matching target papers to the correct authors in academic datasets, considering both exact and semantic matches. The proposed algorithm, CONNA, integrates feature-engineering and representation-based methods, leveraging reinforcement learning to fine-tune the matching and decision components.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b12\": 0.85,\n    \"b52\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b56\": 0.7,\n    \"b57\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b18\": 0.55,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of name disambiguation in academic datasets by proposing an end-to-end framework named CONNA. The primary challenge is to capture both exact and soft matches between candidate names and target pairs while accounting for semantic drift and historical errors. The proposed algorithm uses interaction-based models and reinforcement learning to jointly fine-tune matching and decision components.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b56\": 0.95,\n    \"b4\": 0.90,\n    \"b12\": 0.90,\n    \"b52\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.80,\n    \"b31\": 0.75,\n    \"b6\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.70,\n    \"b23\": 0.70,\n    \"b25\": 0.70,\n    \"b45\": 0.70,\n    \"b57\": 0.70\n  }\n}\n```"], "5eede0b091e0116a23aafbd3": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the limited transferability of graph representation learning models across different graphs. The authors propose the Graph Contrastive Coding (GCC) model, which leverages contrastive learning to pre-train structural representations that can be transferred across various graph learning tasks. The model aims to capture universal structural patterns using a self-supervised instance discrimination task.\",\n  \n  \"Direct Inspiration\": {\n    \"b9\": 0.95,\n    \"b58\": 0.9,\n    \"b16\": 0.85\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b29\": 0.8,\n    \"b18\": 0.75,\n    \"b19\": 0.7\n  },\n  \n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b31\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning transferable graph embeddings from diverse networks, inspired by universal structural patterns and pre-training methods in other domains. The proposed Graph Contrastive Coding (GCC) model uses contrastive learning to pre-train a graph neural network for capturing structural similarities across graphs, allowing for application on various downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b58\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.7,\n    \"b24\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning transferable structural graph representations using a pre-training approach. Inspired by the success of pre-training in other domains like NLP and computer vision, the authors propose the Graph Contrastive Coding (GCC) model, which leverages contrastive learning to pre-train a graph neural network encoder. The main novelty is the use of instance discrimination to capture universal structural patterns across different graphs, enabling the model to be applied to unseen graphs for various downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b58\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b29\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b42\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning transferable graph embeddings that can be applied across different networks. The proposed method, Graph Contrastive Coding (GCC), uses contrastive learning to pre-train a representation learning model on multiple graphs. This model learns to encode structural patterns that are universal and transferable, allowing it to perform well on various graph learning tasks on unseen graphs.\",\n  \"Direct Inspiration\": [\"b9\", \"b58\"],\n  \"Indirect Inspiration\": [\"b18\", \"b19\", \"b24\"],\n  \"Other Inspiration\": [\"b3\", \"b29\", \"b38\", \"b59\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning transferable graph representations across different networks by proposing the Graph Contrastive Coding (GCC) model. Inspired by the success of contrastive learning in other domains, the authors design a pre-training task using instance discrimination to capture universal structural patterns in graphs.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b58\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b34\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.65,\n    \"b19\": 0.6\n  }\n}\n```"], "5c2348ceda562935fc1d57a4": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of handling biased and incomplete user click feedback in contextual bandit algorithms. The authors propose a probabilistic model that incorporates user result examination as a latent variable, using a variational Bayesian approach to approximate the posterior distribution of model parameters. The proposed method is tested on a MOOC platform for personalized education, showing improvement in click-through rates compared to standard contextual bandits.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b12\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b24\": 0.8,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"This paper addresses the challenge of biased and incomplete user feedback in contextual bandit algorithms, specifically implicit feedback such as clicks. The authors propose a probabilistic model incorporating a latent variable to represent user examination behavior, combined with a variational Bayesian approach for parameter estimation and a Thompson sampling strategy for arm selection.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b12\", \"b7\", \"b19\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b16\", \"b17\", \"b24\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b14\", \"b22\"]\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is learning contextual bandits with implicit user click feedback, which is biased and incomplete. The proposed algorithm models click feedback as a composition of user result examination and relevance judgment, employing a variational Bayesian approach for parameter estimation and Thompson sampling for arm selection.\",\n    \"Direct Inspiration\": [\n        \"b7\",\n        \"b12\",\n        \"b19\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b15\",\n        \"b24\"\n    ],\n    \"Other Inspiration\": [\n        \"b5\",\n        \"b22\"\n    ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of learning contextual bandits with user click feedback, specifically modeling implicit feedback as a combination of user result examination and relevance judgment. The key algorithmic contributions include using variational Bayesian approaches to model examination as a latent variable and develop a Thompson sampling strategy for bandit learning.\",\n  \"Direct Inspiration\": [\"b7\", \"b12\"],\n  \"Indirect Inspiration\": [\"b19\", \"b24\", \"b16\", \"b17\", \"b14\"],\n  \"Other Inspiration\": [\"b6\", \"b9\", \"b26\", \"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the bias and incompleteness of implicit user feedback, such as clicks, in contextual bandit algorithms. The proposed solution models user click feedback as a composition of result examination and relevance judgment, introducing a probabilistic model with latent variables for examination. The algorithm employs a variational Bayesian approach for parameter estimation and uses Thompson sampling for arm selection, aiming to minimize Bayesian regret.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b7\": 1,\n    \"b19\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b22\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b2\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```"], "5b67b45517c44aac1c860884": ["```json\n{\n  \"Summary\": \"The paper tackles the challenge of predicting user-level social influence by leveraging deep learning techniques to model social influence dynamics and network structures. The proposed model, DeepInf, integrates network embedding, graph convolution, and graph attention mechanisms into a unified framework to automatically discover predictive signals in social networks. The effectiveness of DeepInf is demonstrated on four diverse social and information networks.\",\n  \"Direct Inspiration\": [\"b24\", \"b48\"],\n  \"Indirect Inspiration\": [\"b1\", \"b45\"],\n  \"Other Inspiration\": [\"b26\", \"b52\", \"b53\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting user-level social influence in social networks, proposing a deep learning-based framework called DeepInf. The framework leverages network embedding, graph convolutional networks (GCN), and graph attention mechanisms (GAT) to predict user actions based on their neighbors' actions and local network structure.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b24\": 0.95,\n    \"b36\": 0.85,\n    \"b48\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b52\": 0.8,\n    \"b53\": 0.8,\n    \"b44\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting user-level social influence in online social networks. It proposes a deep learning-based framework called DeepInf that uses network embedding, graph convolution, and graph attention mechanisms to automatically detect the dynamics of social influence. The framework aims to predict a user's action status based on the observed actions of their near neighbors and local network structure.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b24\": 1,\n    \"b36\": 1,\n    \"b45\": 1,\n    \"b48\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b46\": 0.7,\n    \"b50\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b44\": 0.6,\n    \"b47\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting user-level social influence in social networks, aiming to predict a user\u2019s action status given the action statuses of near neighbors and the user's local network structure. The proposed algorithm, DeepInf, uses a deep learning framework incorporating network embedding, graph convolution, and graph attention mechanisms to automatically discover predictive signals of social influence.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b45\": 0.85,\n    \"b24\": 0.8,\n    \"b48\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b52\": 0.75,\n    \"b53\": 0.75,\n    \"b31\": 0.7,\n    \"b25\": 0.7,\n    \"b44\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.65,\n    \"b22\": 0.65,\n    \"b11\": 0.65,\n    \"b14\": 0.65,\n    \"b43\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of predicting user-level social influence by leveraging deep learning methodologies. The proposed model, DeepInf, integrates network embedding, graph convolution, and graph attention mechanisms to achieve improved prediction performance. The paper is inspired by the success of neural networks in representation learning and aims to move beyond hand-crafted features to a more automated approach.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b36\": 0.9,\n    \"b48\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b45\": 0.8,\n    \"b52\": 0.8,\n    \"b53\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b44\": 0.6,\n    \"b46\": 0.6\n  }\n}\n```"], "5843777eac44360f108419f2": ["```json\n{\n  \"Summary\": \"The main challenges addressed in the paper include error propagation and data redundancy in web user profiling, especially when extracting profile attributes from the web. The proposed solution is a unified approach framework that processes all extraction subtasks together in one step. The authors utilize redundancy in big web data to improve extraction accuracy and propose a markov logic factor graph (MagicFG) model to formalize human knowledge as first-order logics in the model.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Web user profiling is challenged by error propagation, data redundancy, and the growing volume of noisy and redundant web data.\",\n    \"Inspirations\": \"The paper proposes a unified approach to process all extraction tasks together and leverages redundant information using a markov logic factor graph (MagicFG) model.\"\n  },\n  \"Direct Inspiration\": {\n    \"References\": [\"b4\", \"b8\"]\n  },\n  \"Indirect Inspiration\": {\n    \"References\": [\"b6\", \"b7\", \"b9\", \"b13\"]\n  },\n  \"Other Inspiration\": {\n    \"References\": [\"b12\", \"b14\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of web user profiling, focusing on extracting profile attributes from unstructured web data. It proposes a unified framework to avoid error propagation and leverages data redundancy using a Markov logic factor graph (MagicFG) model.\",\n  \"Direct Inspiration\": [\"b4\"],\n  \"Indirect Inspiration\": [\"b8\", \"b10\"],\n  \"Other Inspiration\": [\"b12\", \"b13\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of web user profiling by introducing a unified approach to extract profile attributes from unstructured web data using a Markov logic factor graph (MagicFG) model. The primary challenge is error propagation and data redundancy in traditional methods. The proposed method leverages redundant information to improve accuracy and avoids error propagation by processing extraction tasks in one step.\",\n\n  \"Direct Inspiration\": {\n    \"1\": \"b4\"\n  },\n\n  \"Indirect Inspiration\": {\n    \"1\": \"b8\",\n    \"2\": \"b10\"\n  },\n\n  \"Other Inspiration\": {\n    \"1\": \"b11\",\n    \"2\": \"b12\",\n    \"3\": \"b13\"\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of web user profiling, focusing on extracting accurate user profile attributes from big web data while tackling issues of error propagation and data redundancy. The authors propose a unified approach using a Markov logic factor graph (MagicFG) model to leverage redundant information for improved extraction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b16\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b11\": 0.4,\n    \"b14\": 0.3,\n    \"b19\": 0.2\n  }\n}\n```"], "5ce2d0b2ced107d4c63a8bff": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of personalized course recommendation in MOOCs by proposing a hierarchical reinforcement learning algorithm to revise user profiles by removing noisy courses. The proposed model jointly trains a profile reviser and a basic recommendation model to improve recommendation accuracy.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b13\": 0.9,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b12\": 0.4,\n    \"b10\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of personalized course recommendation in MOOCs platforms, focusing on removing noisy historical courses that dilute the effectiveness of recommendation algorithms. The authors propose a hierarchical reinforcement learning algorithm combined with a profile reviser and a basic recommendation model (HRL+NAIS or HRL+NASR) to improve recommendation accuracy.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.8,\n    \"b18\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The paper outlines challenges in personalized course recommendation due to the dilution of contributing courses by irrelevant ones in user profiles, and the rigid assignment of attention coefficients in existing models.\",\n    \"inspirations\": \"The paper proposes a hierarchical reinforcement learning algorithm to revise user profiles by removing noisy courses, inspired by the theory of hierarchical abstract machines.\"\n  },\n  \"Direct Inspiration\": [\"b15\"],\n  \"Indirect Inspiration\": [\"b8\", \"b9\", \"b13\"],\n  \"Other Inspiration\": [\"b18\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Noisy historical courses diluting the effects of relevant courses\",\n      \"Lack of explicit supervision to identify noisy courses\"\n    ],\n    \"algorithm\": \"Hierarchical reinforcement learning to revise user profiles by removing noisy courses\"\n  },\n  \"Direct Inspiration\": [\n    \"b8\",\n    \"b13\",\n    \"b15\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b9\",\n    \"b18\"\n  ],\n  \"Other Inspiration\": [\n    \"b12\",\n    \"b17\",\n    \"b19\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": [\n            \"Diluted effects of contributing courses due to irrelevant ones.\",\n            \"Lack of explicit information about noisy courses.\",\n            \"Need for an effective way to revise user profiles.\"\n        ],\n        \"inspirations\": [\n            \"Inspired by hierarchical reinforcement learning and hierarchical abstract machines.\"\n        ],\n        \"algorithm\": \"Hierarchical reinforcement learning algorithm with profile reviser and basic recommendation model.\"\n    },\n    \"Direct Inspiration\": {\n        \"b15\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.8,\n        \"b13\": 0.7,\n        \"b10\": 0.6,\n        \"b18\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.5,\n        \"b12\": 0.4,\n        \"b17\": 0.4,\n        \"b19\": 0.4,\n        \"b21\": 0.4,\n        \"b23\": 0.4\n    }\n}\n```"], "5e5f7c4791e011df604ecbed": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of modeling heterogeneous graphs, particularly focusing on node-and edge-type dependent representations, capturing network dynamics, avoiding customized meta paths, and scalability to Web-scale graphs. The proposed Heterogeneous Graph Transformer (HGT) introduces several novel methods such as node-and edge-type dependent attention mechanism, relative temporal encoding (RTE) strategy, and a heterogeneous sub-graph sampling algorithm (HGSampling) to tackle these challenges.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b6\": 1.0,\n    \"b13\": 1.0,\n    \"b21\": 1.0,\n    \"b22\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b3\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of modeling heterogeneous graphs, particularly focusing on the issues of meta path design, feature distribution, graph dynamics, and scalability. It proposes the Heterogeneous Graph Transformer (HGT) with a node-and edge-type dependent attention mechanism, relative temporal encoding (RTE) strategy, and HGSampling algorithm to handle these challenges effectively.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b6\": 1,\n    \"b21\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b22\": 0.9,\n    \"b25\": 0.8,\n    \"b26\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b17\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper include handling the heterogeneity of graph nodes and edges, capturing the dynamic nature of graphs, avoiding the need for manually designed meta paths, and ensuring scalability for Web-scale graphs. The proposed algorithm, Heterogeneous Graph Transformer (HGT), introduces a node-and edge-type dependent attention mechanism, relative temporal encoding (RTE) for graph dynamics, and HGSampling for scalable training.\",\n  \"Direct Inspiration\": [\"b13\", \"b22\", \"b26\"],\n  \"Indirect Inspiration\": [\"b16\", \"b17\", \"b21\"],\n  \"Other Inspiration\": [\"b2\", \"b6\", \"b8\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents the Heterogeneous Graph Transformer (HGT) to address challenges in heterogeneous graph neural networks (GNNs) such as the need for domain-specific meta paths, inability to handle dynamic and web-scale graphs, and the requirement to maintain node-and edge-type dependent representations. HGT introduces a node-and edge-type dependent attention mechanism, a relative temporal encoding (RTE) strategy, and a heterogeneous sub-graph sampling algorithm (HGSampling) to overcome these challenges.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b17\": 1.0,\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b22\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include handling graph heterogeneity, capturing network dynamics, avoiding customized meta paths, and ensuring scalability to Web-scale graphs. The proposed algorithm, Heterogeneous Graph Transformer (HGT), introduces a node-and edge-type dependent attention mechanism, a relative temporal encoding strategy, and a heterogeneous sub-graph sampling algorithm (HGSampling) to address these issues.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b6\": 0.8,\n    \"b13\": 0.8,\n    \"b22\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b17\": 0.7\n  }\n}\n```"], "59ae3c262bbe271c4c71f4a2": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning representations for heterogeneous networks, which contain multiple types of nodes and relationships. The proposed algorithms, metapath2vec and metapath2vec++, aim to preserve both structural and semantic correlations in such networks. These methods extend traditional network embedding models like skip-gram and introduce meta-path-based random walks and heterogeneous negative sampling to better capture network heterogeneity.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b7\": 0.9,\n    \"b17\": 0.9,\n    \"b21\": 0.9,\n    \"b24\": 1,\n    \"b25\": 1,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b28\": 0.7,\n    \"b26\": 0.6,\n    \"b27\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of heterogeneous network representation learning, proposing the metapath2vec and metapath2vec++ frameworks. These models aim to preserve both structural and semantic correlations in heterogeneous networks, overcoming limitations of conventional homogeneous network embedding models.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b21\": 0.9,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b25\": 0.6,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of heterogeneous network representation learning by proposing the metapath2vec and metapath2vec++ frameworks. These frameworks aim to preserve both structural and semantic correlations in heterogeneous networks, improving tasks such as node classification, clustering, and similarity search. The paper draws inspiration from existing homogeneous network embedding methods like DeepWalk, node2vec, and LINE, and adapts these methods to handle heterogeneous networks using meta-path-based random walks.\",\n  \"Direct Inspiration\": [\"b7\", \"b16\", \"b17\", \"b21\", \"b24\", \"b25\", \"b29\"],\n  \"Indirect Inspiration\": [\"b28\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of heterogeneous network representation learning and proposes the metapath2vec and metapath2vec++ frameworks. These models aim to preserve the structural and semantic correlations of heterogeneous networks by utilizing meta-path-based random walks and heterogeneous negative sampling techniques.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b21\": 0.9,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b25\": 0.6,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of representing heterogeneous networks with multiple types of nodes and relationships using the metapath2vec and metapath2vec++ frameworks. These methods aim to preserve structural and semantic correlations in heterogeneous networks, overcoming the limitations of homogeneous network embedding models.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b7\": 1,\n    \"b17\": 1,\n    \"b21\": 1,\n    \"b24\": 1,\n    \"b25\": 1,\n    \"b28\": 1,\n    \"b29\": 1\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```"], "599c7988601a182cd2648a09": ["```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is generating low-dimensional vector embeddings for nodes in large graphs that can generalize to unseen nodes and new subgraphs, which is critical for high-throughput production machine learning systems. The proposed algorithm, GraphSAGE (SAmple and aggreGatE), leverages node features to learn an embedding function that generalizes to unseen nodes by training aggregator functions to aggregate feature information from a node's local neighborhood.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b10\": 0.8,\n    \"b22\": 0.7,\n    \"b34\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inductive node embedding for unseen nodes in evolving graphs. It proposes the GraphSAGE framework, which uses trainable aggregation functions to generate node embeddings based on node features and local neighborhood structure. The method is evaluated on multiple datasets, showing significant improvements over baseline approaches.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b35\": 0.7,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a framework called GraphSAGE for inductive node embedding that leverages node features to learn an embedding function generalizable to unseen nodes. This is achieved by training a set of aggregator functions that aggregate feature information from local neighborhoods, rather than training distinct embedding vectors for each node. The framework addresses the challenge of generating embeddings for evolving graphs and unseen nodes, which is essential for high-throughput machine learning systems.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b9\": 0.6,\n    \"b20\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of generating inductive node embeddings for unseen nodes or new subgraphs in evolving graphs. It proposes the GraphSAGE framework, which uses trainable aggregation functions to leverage node features for embedding generation. This approach allows for effective generalization across different graphs and improves performance on node classification tasks compared to traditional transductive methods.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.95,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b34\": 0.75,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.65,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating inductive node embeddings for unseen nodes in evolving graphs, proposing the GraphSAGE framework which uses trainable aggregation functions to leverage node features and learn embeddings, demonstrating significant performance improvements over existing methods in various tasks.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b10\": 0.8,\n    \"b34\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```"], "53e9b253b7602d9703cf4028": ["```json\n{\n  \"Summary\": \"The paper introduces DeepWalk, an algorithm that learns social representations of graph vertices by modeling short random walks. The primary challenges addressed include handling the sparsity of network representations and improving classification performance in multilabel network classification problems, especially under conditions of label sparsity. DeepWalk leverages techniques from language modeling to capture community structure and neighborhood similarity in a low-dimensional, continuous vector space.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0,\n    \"b29\": 1.0,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.8,\n    \"b33\": 0.7,\n    \"b12\": 0.7,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b40\": 0.6,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning latent space representations of social interactions in networks, particularly focusing on network sparsity and its impact on statistical learning and classification tasks. The proposed method, DeepWalk, introduces deep learning techniques into network analysis by modeling a stream of short random walks. This approach captures neighborhood similarity and community membership, producing low-dimensional latent features suitable for various classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b28\": 0.85,\n    \"b29\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.8,\n    \"b33\": 0.75,\n    \"b38\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning a latent space representation of social interactions in networks, specifically focusing on sparsity in network representations which can hinder generalization in statistical learning. The proposed method, DeepWalk, combines random walks with language modeling techniques to generate low-dimensional, continuous vector space representations that encode community structure and neighborhood similarity. This approach is demonstrated to improve classification performance in social networks, especially under conditions of label sparsity.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b28\": 0.9,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b12\": 0.7,\n    \"b30\": 0.7,\n    \"b33\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b20\": 0.6,\n    \"b24\": 0.6,\n    \"b35\": 0.6,\n    \"b38\": 0.6,\n    \"b40\": 0.6,\n    \"b42\": 0.6,\n    \"b45\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of learning latent space representations of social interactions in graphs, particularly focusing on overcoming the sparsity of network representation. The proposed method, DeepWalk, utilizes random walks and language modeling techniques to capture community structures and enhances classification performance in social networks. The method is scalable, adaptable, and capable of handling label sparsity.\",\n    \"Direct Inspiration\": {\n        \"b28\": 1,\n        \"b29\": 1,\n        \"b33\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b30\": 0.8,\n        \"b38\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b6\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning latent space representations of social interactions in networks using a novel algorithm called DeepWalk. The primary motivation is to handle the sparsity of network representations and improve classification in social networks by leveraging unsupervised feature learning techniques from natural language processing. The algorithm models random walks on graphs to learn social representations that capture neighborhood similarity and community membership.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b12\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b33\": 0.6,\n    \"b38\": 0.6,\n    \"b40\": 0.5,\n    \"b42\": 0.5\n  }\n}\n```"], "5736973b6e3b12023e62b0a8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning effective text representations that leverage both labeled and unlabeled data, aiming to improve predictive power for specific tasks while maintaining efficiency and scalability. The proposed Predictive Text Embedding (PTE) model integrates supervised and unsupervised techniques using heterogeneous text networks.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b17\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning effective text representations that leverage both labeled and unlabeled data, optimizing for specific text classification tasks. It proposes the Predictive Text Embedding (PTE) method, which integrates the benefits of unsupervised text embeddings and supervised deep learning approaches to achieve high efficiency and strong predictive power.\",\n  \"Direct Inspiration\": [\"b26\", \"b17\", \"b9\"],\n  \"Indirect Inspiration\": [\"b4\", \"b7\", \"b22\", \"b5\"],\n  \"Other Inspiration\": [\"b19\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the limitations of traditional text representation methods, the inefficiency of unsupervised text embeddings in specific tasks, and the high computational cost of deep neural networks. The proposed Predictive Text Embedding (PTE) algorithm aims to combine the advantages of unsupervised embeddings with the predictive power of supervised learning by utilizing labeled information in the representation learning phase. The approach leverages heterogeneous text networks and bipartite network embedding techniques to achieve this.\",\n  \"Direct Inspiration\": [\"b26\"],\n  \"Indirect Inspiration\": [\"b17\", \"b9\"],\n  \"Other Inspiration\": [\"b7\", \"b4\", \"b22\", \"b5\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning effective text representations that incorporate both labeled and unlabeled data, bridging the gap between efficient unsupervised methods and powerful supervised deep learning methods. The proposed Predictive Text Embedding (PTE) method leverages heterogeneous text networks to learn low-dimensional representations optimized for specific tasks such as text classification.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b9\": 1,\n    \"b26\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b19\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning meaningful and effective text representations optimized for specific tasks, balancing the advantages of unsupervised text embeddings and supervised deep learning approaches. It proposes the Predictive Text Embedding (PTE) method, which learns low-dimensional representations from both labeled and unlabeled data using a heterogeneous text network.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.95,\n    \"b17\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b4\": 0.75,\n    \"b7\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.65,\n    \"b14\": 0.6\n  }\n}\n```"], "5d08be648607575390f908ca": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of guaranteeing performance for high-priority workloads on SMT processors while achieving reasonable overall throughput. It introduces QoSMT, a hardware mechanism that dynamically adjusts resource allocation based on real-time performance measurements to mitigate SMT-induced performance degradation.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b22\": 0.5,\n    \"b25\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of unpredictable performance variations caused by simultaneous multithreading (SMT) in processors. It proposes a novel hardware mechanism, QoSMT, which guarantees real-time performance requirements for high-priority workloads under SMT environments without the need for prior profiling. The key contributions include a thorough analysis of SMT-induced interference, a shadow solo-cycle accounting methodology, and a closed-loop controlling algorithm for dynamic resource adjustment.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b25\": 0.9,\n    \"b22\": 0.85,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenge of unpredictable performance variations in simultaneous multithreading (SMT) environments. It proposes QoSMT, a hardware mechanism to guarantee performance for high-priority workloads by dynamically adjusting shared resources without prior profiling. The novel approach includes identifying critical resources causing performance loss, quantifying performance loss using shadow solo-cycle accounting, and performing timely resource adjustments through hardware-software codesign.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b18\", \"b19\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b22\", \"b25\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b2\", \"b8\", \"b10\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the unpredictable performance variations caused by simultaneous multithreading (SMT) in processors, which can lead to significant performance degradation for high-priority workloads. The proposed solution, QoSMT, introduces a novel hardware mechanism to guarantee real-time performance requirements for higher priority workloads without requiring offline profiling. The methodology involves quantitatively measuring SMT-induced performance degradation at runtime, identifying critical interference, and dynamically adjusting resource allocation to meet performance targets.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b25\": 0.85,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b10\": 0.7,\n    \"b18\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b30\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unpredictable performance variations in simultaneous multithreading (SMT) technology. It introduces QoSMT, a novel hardware mechanism designed to guarantee real-time performance requirements for high-priority workloads without profiling in advance. The key contributions include a thorough analysis of SMT-induced interference, a shadow solo-cycle accounting methodology to quantify performance loss, and a closed-loop controlling algorithm for dynamic resource adjustment.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b8\", \"b10\", \"b18\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b19\", \"b25\", \"b26\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b22\", \"b30\"]\n  }\n}\n```"], "5bdc316717c44a1f58a071ff": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Leveraging specific hardware features and abstractions for DL workloads\",\n      \"Large search space for optimization without manual tuning\",\n      \"Operator fusion and data layout transformation for computational graphs\",\n      \"Generating tensor operations efficiently for diverse hardware back-ends\"\n    ],\n    \"inspirations\": [\n      \"Decoupling descriptions from computation rules (or schedule optimizations) inspired by Halide\",\n      \"Tensor expression language and schedule space inspired by Halide, Darkroom, and TACO\",\n      \"Latent memory access latency hiding techniques\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b31\", \"b16\", \"b22\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b2\", \"b3\", \"b6\", \"b8\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b0\", \"b11\", \"b20\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the primary challenges of optimizing deep learning workloads across diverse hardware back-ends and introduces TVM, a compiler that generates low-level optimized code from high-level deep learning models. The key modules include a tensor expression language, an automated program optimization framework guided by an ML-based cost model, and a graph rewriter for joint high- and low-level optimizations.\",\n    \"Direct Inspiration\": {\n        \"b31\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.8,\n        \"b20\": 0.8,\n        \"b11\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b3\": 0.6,\n        \"b8\": 0.6,\n        \"b7\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of optimizing deep learning workloads across diverse hardware back-ends. It proposes TVM, a compiler that takes high-level specifications from existing DL frameworks and generates low-level optimized code for various back-ends. Key challenges include leveraging specific hardware features, managing a large optimization search space, and automating operator tuning. The paper introduces a tensor expression language, an automated optimization framework guided by an ML-based cost model, and a graph rewriter to combine high-and low-level optimizations.\",\n  \"Direct Inspiration\": [\"b0\", \"b20\", \"b31\"],\n  \"Indirect Inspiration\": [\"b11\", \"b22\"],\n  \"Other Inspiration\": [\"b1\", \"b2\", \"b3\", \"b8\", \"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of deploying deep learning workloads across diverse hardware back-ends, which include leveraging specific hardware features, handling large optimization search spaces, and providing performance portability. The authors propose TVM, a compiler that generates optimized code for various hardware back-ends by introducing a tensor expression language, an automated program optimization framework, and a graph rewriter for high-level and operator-level optimizations.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b3\": 0.5,\n    \"b8\": 0.5,\n    \"b0\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces TVM, a compiler that generates optimized code for deep learning programs across diverse hardware back-ends. The primary challenges addressed include leveraging specific hardware features, managing a large search space for optimization, and providing performance portability. Key innovations include a tensor expression language, automated program optimization using an ML-based cost model, and new schedule primitives for different hardware back-ends.\",\n    \"Direct Inspiration\": [\"b31\"],\n    \"Indirect Inspiration\": [\"b2\", \"b3\", \"b8\", \"b11\", \"b20\"],\n    \"Other Inspiration\": [\"b0\", \"b13\", \"b19\", \"b22\", \"b34\", \"b45\"]\n}\n```"], "53e9bd8cb7602d9704a3405d": ["```json\n{\n  \"Summary\": \"The paper discusses improvements to the TAGE branch predictor, emphasizing reducing the number of predictor table accesses and enhancing prediction accuracy by combining TAGE with other predictors such as a local history predictor, the Immediate Update Mimicker (IUM), and the Statistical Corrector predictor. The goal is to achieve state-of-the-art prediction accuracy with cost-effective implementation.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b25\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.85,\n    \"b10\": 0.8,\n    \"b7\": 0.75,\n    \"b4\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b22\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on enhancing the TAGE predictor by reducing the number of accesses to predictor tables and incorporating a local history predictor for higher accuracy. Key challenges include managing prediction accuracy with delayed updates and optimizing hardware implementation.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b10\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in branch prediction, specifically focusing on the TAGE predictor. The proposed solutions include reducing the number of accesses to predictor tables and combining TAGE with side predictors (IUM, loop predictor, Statistical Corrector). The paper introduces cost-effective implementations and aims to achieve state-of-the-art prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b20\": 0.7,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Reduce the number of accesses to predictor tables for simpler and cost-effective implementation.\",\n      \"Combine TAGE with local history predictors for higher accuracy within the same storage budget.\",\n      \"Mitigate accuracy loss due to delayed updates in hardware predictor tables.\"\n    ],\n    \"inspirations\": [\n      \"Inspired by previous works in TAGE predictors and side predictors from the Championship Branch Prediction.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving branch prediction accuracy and efficiency. It reinforces the TAGE predictor by showing its reduced access requirements and combining it with a small local history predictor for higher accuracy. The paper introduces novel methods such as Immediate Update Mimicker (IUM) to handle delayed updates, a loop predictor for better loop exit predictions, and a Statistical Corrector predictor for branches with statistical bias. The proposed TAGE-LSC predictor combines these techniques to achieve state-of-the-art accuracy with cost-effective implementation.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b25\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b8\": 0.7,\n    \"b4\": 0.7\n  }\n}\n```"], "5dce788a3a55ac9580a162f8": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of integrating factual world knowledge into pre-trained language models (PLMs) effectively. The proposed algorithm, KE-PLER, unifies knowledge embedding and language representation into the same semantic space to avoid the gap between language representations and fixed entity embeddings, eliminate the need for an entity linker, and enable inductive knowledge embedding. Additionally, the authors introduce a new large-scale knowledge graph dataset, Wikidata5m.\",\n    \"Direct Inspiration\": {\n        \"b10\": 0.9,\n        \"b2\": 0.85,\n        \"b38\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b29\": 0.7,\n        \"b9\": 0.75,\n        \"b46\": 0.75,\n        \"b19\": 0.7,\n        \"b20\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b42\": 0.65,\n        \"b23\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of integrating factual world knowledge into pre-trained language models (PLMs) by proposing KE-PLER, a unified model that encodes entities and text into the same semantic space. This approach avoids the issues of fixed entity embeddings, error propagation due to entity linkers, and additional inference overhead. The authors construct a new large-scale knowledge graph dataset, Wikidata5m, to support the model's training and evaluation.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b2\": 1.0,\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b46\": 0.8,\n    \"b44\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the integration of world knowledge into pre-trained language models (PLMs) to improve their performance on NLP tasks. The proposed KE-PLER model unifies knowledge embedding and language representation into the same semantic space, overcoming issues like error propagation and additional inference overhead associated with previous methods. The paper also introduces a new large-scale knowledge graph dataset, Wikidata5m, with aligned text descriptions for each entity.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b2\": 0.9,\n    \"b32\": 0.8,\n    \"b42\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b20\": 0.6,\n    \"b28\": 0.7,\n    \"b49\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b29\": 0.5,\n    \"b46\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges outlined in the paper include the alignment of language representations with knowledge embeddings, the error propagation problem in entity linking, and the additional inference overhead in existing methods. The proposed algorithm, KE-PLER, aims to integrate world knowledge into PLMs by encoding entities and text into the same semantic space, avoiding these issues. KE-PLER uses textual descriptions for entity embeddings and introduces a new large-scale dataset, Wikidata5m, to facilitate further research.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b2\": 0.9,\n    \"b38\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b44\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.7,\n    \"b46\": 0.65,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating world knowledge into pre-trained language models (PLMs) by proposing KEPLER, a unified model for knowledge embedding and pre-trained language representation. The main contributions include encoding entities and texts into the same space to avoid error propagation and additional overhead, improving knowledge embedding with textual information, and introducing a new large-scale knowledge graph dataset, Wikidata5m.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b10\", \"b2\", \"b38\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b9\", \"b46\", \"b20\", \"b19\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b29\", \"b32\", \"b21\", \"b28\", \"b19\", \"b22\", \"b3\", \"b4\"]\n  }\n}\n```"], "5cede0edda562983788cb3c2": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges are the limitations of state-of-the-art NLP methodologies when directly applied to biomedical text mining due to differences in word distributions between general and biomedical corpora.\",\n    \"inspirations\": \"The paper is inspired by the need to adapt BERT for the biomedical domain to improve performance on biomedical text mining tasks such as NER, RE, and QA.\"\n  },\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b7\": 0.9,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b12\": 0.7,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b36\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively applying state-of-the-art NLP methodologies to biomedical text mining due to differences in word distributions between general and biomedical corpora. The authors propose BioBERT, a domain-specific model pre-trained on biomedical corpora, and demonstrate its improved performance on multiple biomedical text mining tasks (NER, RE, and QA).\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b22\": 0.9,\n    \"b33\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b7\": 0.75,\n    \"b12\": 0.8,\n    \"b32\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.65,\n    \"b17\": 0.65,\n    \"b21\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces BioBERT, a pre-trained language representation model specifically for the biomedical domain, addressing the challenge of applying general NLP models to biomedical texts. The authors highlight the limitations of existing word representation models like Word2Vec, ELMo, and general BERT and propose pre-training BERT on biomedical corpora to improve performance in tasks such as NER, RE, and QA.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b7\": 0.7,\n    \"b12\": 0.7,\n    \"b32\": 0.7,\n    \"b33\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively applying state-of-the-art NLP methodologies to biomedical text mining due to domain-specific vocabulary differences. The authors propose BioBERT, a pre-trained language representation model tailored for the biomedical domain, which demonstrates superior performance in biomedical NER, RE, and QA tasks compared to existing models.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b21\": 0.8,\n    \"b33\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b12\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the difficulty of applying state-of-the-art NLP methodologies directly to biomedical text mining due to differences in word distributions between general and biomedical corpora. The paper introduces BioBERT, a pre-trained language representation model specifically for the biomedical domain, which is initialized with weights from BERT and then pre-trained on biomedical corpora. BioBERT is fine-tuned and evaluated on three popular biomedical text mining tasks: NER, RE, and QA, showing significant improvements over existing models.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b12\": 0.7,\n    \"b22\": 0.7,\n    \"b36\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```"], "5db9299b47c8f766461f993e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving performance on NLP tasks in scientific domains where annotated data is scarce and expensive to obtain. It introduces SCIBERT, a pretrained language model based on BERT, but trained on a large corpus of scientific text. SCIBERT is shown to outperform BERT on various scientific tasks, achieving state-of-the-art results in many cases.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b24\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b18\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving NLP performance in the scientific domain, particularly given the difficulty and expense of obtaining annotated data in this field. The primary contribution is the release of SCIBERT, a pretrained language model based on BERT, but trained on a large corpus of scientific text. This model is evaluated on various NLP tasks and demonstrates state-of-the-art performance in many cases.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b7\": 0.7,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b23\": 0.5,\n    \"b2\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving NLP performance in scientific domains, where annotated data is scarce and expensive to collect. It introduces SCIBERT, a BERT-based pretrained language model trained on scientific texts. The paper investigates the effects of finetuning versus task-specific architectures and evaluates SCIBERT on various NLP tasks in the scientific domain, achieving state-of-the-art results.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b24\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b7\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of applying NLP models to the scientific domain, where acquiring annotated data is difficult and expensive. The authors propose SCIBERT, a pretrained language model based on BERT but trained on a large corpus of scientific text. The paper details the construction of a new vocabulary (SCIVOCAB), extensive experimentation on fine-tuning versus task-specific architectures, and evaluation on various scientific NLP tasks, achieving state-of-the-art results in many cases.\",\n    \"Direct Inspiration\": {\n        \"b5\": 1,\n        \"b20\": 0.9,\n        \"b18\": 0.8,\n        \"b23\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.6,\n        \"b8\": 0.5,\n        \"b24\": 0.5,\n        \"b25\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.4,\n        \"b9\": 0.3,\n        \"b21\": 0.3,\n        \"b16\": 0.2\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a pretrained language model specifically for scientific text, as existing models like BERT, ELMo, and GPT are trained on general domain corpora. The authors introduce SCIBERT, a BERT-based model trained on a large corpus of scientific text, and demonstrate its effectiveness on various scientific NLP tasks, achieving new state-of-the-art results on many of them. The primary contributions include the release of SCIBERT, extensive experimentation on finetuning versus task-specific architectures, and evaluation of SCIBERT across multiple tasks in the scientific domain.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b18\": 0.9,\n    \"b24\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b20\": 0.7,\n    \"b26\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b2\": 0.5,\n    \"b13\": 0.5,\n    \"b21\": 0.5,\n    \"b23\": 0.5,\n    \"b28\": 0.5,\n    \"b29\": 0.5\n  }\n}\n```"], "5d9b0cb93a55acb0384964ef": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of anomaly detection in attributed networks, which include network sparsity, data nonlinearity, and complex modality interactions. The authors propose a novel graph convolutional autoencoder framework called Dominant, which combines graph convolutional networks (GCNs) for node embedding representation and autoencoders for anomaly detection.\",\n  \"Direct Inspiration\": [\"b15\"],\n  \"Indirect Inspiration\": [\"b16\", \"b22\", \"b10\"],\n  \"Other Inspiration\": [\"b23\", \"b9\", \"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses anomaly detection in attributed networks, which pose challenges due to network sparsity, data nonlinearity, and complex modality interactions. The authors propose a novel graph convolutional autoencoder framework called Dominant to tackle these issues by learning node embeddings and reconstructing both the topological structure and nodal attributes to identify anomalies.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b16\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b27\": 0.5,\n    \"b26\": 0.5,\n    \"b20\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include network sparsity, data nonlinearity, and complex modality interactions in attributed networks. The proposed algorithm is a novel graph convolutional autoencoder framework called Dominant, which uses graph convolutional networks (GCN) to learn node embeddings and reconstruct both the topological structure and nodal attributes to detect anomalies.\",\n    \"Direct Inspiration\": {\n        \"b15\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b16\": 0.75,\n        \"b31\": 0.7,\n        \"b36\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b23\": 0.6,\n        \"b9\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of anomaly detection on attributed networks, including network sparsity, data nonlinearity, and complex modality interactions. It proposes a novel graph convolutional autoencoder framework called Dominant, which compresses the input attributed network to low-dimensional embeddings and reconstructs both the topological structure and nodal attributes to detect anomalies based on reconstruction errors.\",\n  \"Direct Inspiration\": [\"b15\"],\n  \"Indirect Inspiration\": [\"b16\", \"b10\"],\n  \"Other Inspiration\": [\"b23\", \"b9\", \"b27\", \"b26\", \"b20\", \"b24\", \"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of anomaly detection on attributed networks, including network sparsity, data nonlinearity, and complex modality interactions. It proposes a novel graph convolutional autoencoder framework called Dominant to detect anomalies by leveraging reconstruction errors of both network structure and nodal attributes.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.9,\n    \"b22\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b23\": 0.6,\n    \"b27\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```"], "5f7d8a8391e011346ad27d2b": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of enhancing active sequence labeling through data augmentation, specifically using a novel method called SeqMix. This method generates sub-sequences along with their labels based on mixup, aiming to introduce data diversity and improve model generalization in low-resource settings.\",\n    \"Direct Inspiration\": {\n        \"b46\": 1.0,\n        \"b28\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b32\": 0.8,\n        \"b9\": 0.8,\n        \"b8\": 0.7,\n        \"b33\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.6,\n        \"b41\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is enhancing active sequence labeling via data augmentation in low-resource settings. The authors propose SeqMix, a novel data augmentation method for generating sub-sequences along with their labels based on mixup, to introduce more data diversity and improve model generalization. The method involves searching for pairs of eligible sequences, mixing them both in feature and label spaces, and using a discriminator to select plausible sequences.\",\n  \"Direct Inspiration\": [\"b46\"],\n  \"Indirect Inspiration\": [\"b7\", \"b32\", \"b8\"],\n  \"Other Inspiration\": [\"b49\", \"b2\", \"b20\", \"b41\", \"b5\", \"b43\", \"b28\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of data scarcity in sequence labeling tasks by proposing SeqMix, a data augmentation technique that generates pseudo-labeled sequences through mixup strategies. This approach aims to enhance the diversity and generalization of the model in low-resource settings.\",\n  \"Direct Inspiration\": {\n    \"b46\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.9,\n    \"b7\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b50\": 0.7,\n    \"b44\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of data scarcity in sequence labeling tasks like NER and event extraction. It proposes SeqMix, a novel data augmentation method that generates pseudo-labeled sequences using mixup strategies in the feature and label spaces. The primary goal is to enhance active learning by improving data diversity and model generalization.\",\n  \"Direct Inspiration\": {\n    \"b46\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b39\": 0.8,\n    \"b15\": 0.8,\n    \"b11\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing active sequence labeling via data augmentation, specifically through a method called SeqMix. SeqMix aims to generate augmented labeled sequences for queried samples in each iteration, increasing data diversity and improving model generalization. The main contributions include a mixup-based data augmentation technique that combines sequences in feature and label spaces, and a discriminator to ensure the plausibility of generated sequences.\",\n  \"Direct Inspiration\": {\n    \"b46\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b25\": 0.8,\n    \"b15\": 0.7,\n    \"b38\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b49\": 0.6\n  }\n}\n```"], "58d82fcbd649053542fd669e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning an encoding in deep networks that is maximally informative about a target while being constrained in complexity. It proposes a novel method called Variational Information Bottleneck (VIB), leveraging variational inference and the reparameterization trick to handle high-dimensional, continuous data, enhancing robustness to overfitting and adversarial inputs.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1,\n    \"b17\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b32\": 0.7,\n    \"b14\": 0.6,\n    \"b36\": 0.6,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b20\": 0.5,\n    \"b34\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in learning a representation that is maximally informative about the target while being compressive about the input data, proposing a variational information bottleneck (VIB) method. It is inspired by the information bottleneck (IB) principle and uses variational inference to optimize the IB objective, making it feasible to apply to deep neural networks and handle high-dimensional, continuous data. The VIB method is shown to be robust to overfitting and adversarial attacks.\",\n  \"Direct Inspiration\": [\n    \"b33\",\n    \"b17\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b1\",\n    \"b20\",\n    \"b26\"\n  ],\n  \"Other Inspiration\": [\n    \"b32\",\n    \"b8\",\n    \"b14\",\n    \"b36\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": \"The paper proposes a method called Variational Information Bottleneck (VIB) to address the challenges of learning robust and informative representations in deep networks. The main challenge is to balance the informativeness of the representation about the target while compressing the information from the input. The method uses variational inference to approximate the IB objective, leveraging the reparameterization trick for optimization using stochastic gradient descent. Key inspirations include the Information Bottleneck (IB) principle and variational autoencoders (VAEs).\",\n    \"Direct Inspiration\": [\"b33\", \"b17\"],\n    \"Indirect Inspiration\": [\"b26\", \"b1\", \"b20\"],\n    \"Other Inspiration\": [\"b8\", \"b32\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning an encoding in deep networks that balances informativeness about the target variable with compression of the input data, using the variational information bottleneck (VIB) method. The proposed method builds on the information bottleneck principle and variational inference, leveraging the reparameterization trick to enable optimization with stochastic gradient descent.\",\n  \"Direct Inspiration\": [\"b33\", \"b17\", \"b26\"],\n  \"Indirect Inspiration\": [\"b1\", \"b20\", \"b31\", \"b7\"],\n  \"Other Inspiration\": [\"b8\", \"b36\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning an encoding that balances being maximally informative about the target while maintaining minimal complexity. The authors propose a variational information bottleneck (VIB) method to construct a lower bound on the IB objective, leveraging variational inference and the reparameterization trick to optimize the objective using stochastic gradient descent, allowing the handling of high-dimensional, continuous data.\",\n  \"Direct Inspiration\": [\"b33\", \"b17\"],\n  \"Indirect Inspiration\": [\"b1\", \"b20\"],\n  \"Other Inspiration\": [\"b8\", \"b26\"]\n}\n```"], "53e9a710b7602d970304482e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of capturing fine-grained contextual structures in information retrieval (IR) by introducing the Convolutional Deep Structured Semantic Models (C-DSSM). The key contributions include the use of a convolutional layer for local contextual feature extraction and a max-pooling layer to form global feature vectors, which are then used for semantic relevance scoring.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of capturing fine-grained contextual structures in information retrieval, which traditional latent semantic models and other approaches fail to achieve effectively. The authors propose a new Convolutional Deep Structured Semantic Model (C-DSSM) based on a convolutional neural network. This model includes a word hashing layer, a convolutional layer to extract local contextual features, a max-pooling layer to form a global feature vector, and a final semantic layer to represent the high-level semantic feature vector of the input word sequence.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.95,\n    \"b4\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.75,\n    \"b6\": 0.70\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively capturing fine-grained contextual structures in information retrieval (IR) by proposing a new model called Convolutional Deep Structured Semantic Models (C-DSSM). This model integrates a convolutional layer to project words within a context window to local contextual feature vectors and uses a max-pooling layer to form a global feature vector, which is then processed by feedforward neural network layers to extract highly non-linear and effective features.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b6\": 0.5,\n    \"b9\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the effective modeling of fine-grained contextual structures for information retrieval (IR). Traditional latent semantic models, such as LSA, TF-IDF, and BM25, fail to capture these structures effectively. The paper proposes a novel algorithm, the Convolutional Deep Structured Semantic Models (C-DSSM), to address this challenge. The C-DSSM incorporates a convolutional layer to project words within a context window to local contextual feature vectors, and a max pooling layer to extract the most salient features, forming a fixed-length global feature vector. This method improves the semantic representation and effectiveness of IR tasks.\",\n    \"Direct Inspiration\": {\n        \"b5\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.7,\n        \"b8\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.5,\n        \"b9\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of capturing fine-grained contextual structures for information retrieval (IR). It proposes a Convolutional Deep Structured Semantic Model (C-DSSM) that enhances the Deep Structured Semantic Models (DSSM) by integrating a convolutional layer and a max-pooling layer to improve semantic representation and relevance scoring for queries and documents.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b8\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b6\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```"], "5a9cb60d17c44a376ffb3c4c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of modeling n-gram soft-matches in neural information retrieval (IR) by introducing the Convolutional Kernel-based Neural Ranking Model (Conv-KNRM). This model uses convolutional neural networks (CNNs) to compose n-gram embeddings and kernel pooling techniques to combine n-gram soft-matches into a final ranking score. The method is designed to overcome data sparsity and parameter explosion issues that arise from treating n-grams as discrete terms. Additionally, the paper proposes a domain adaptation strategy to apply Conv-KNRM to search domains with limited training data.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b22\": 0.9,\n    \"b31\": 0.7,\n    \"b33\": 0.7,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are effectively modeling n-gram soft matches in neural information retrieval (IR) systems and addressing the data sparsity and parameter explosion issues associated with n-grams. The proposed algorithm, Conv-KNRM, uses Convolutional Neural Networks (CNN) to compose n-gram embeddings from word embeddings and employs kernel pooling and learning-to-rank techniques to achieve a final ranking score.\",\n    \"Direct Inspiration\": {\n        \"b39\": 1.0,\n        \"b22\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b31\": 0.7,\n        \"b28\": 0.7,\n        \"b20\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b23\": 0.5,\n        \"b33\": 0.5,\n        \"b38\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively modeling n-gram soft matches in neural IR systems. It proposes a new Convolutional Kernel-based Neural Ranking Model (Conv-KNRM) that uses CNNs to compose n-gram embeddings and kernel-pooling techniques to combine these embeddings into ranking signals. The model aims to overcome issues of data sparsity and parameter explosion associated with treating n-grams as discrete terms.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.9,\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7,\n    \"b28\": 0.6,\n    \"b38\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces Conv-KNRM, a Convolutional Kernel-based Neural Ranking Model, which addresses the challenge of effectively modeling n-gram soft-matches in neural information retrieval (IR). The algorithm embeds words into continuous vectors using CNNs, composes n-grams, and then uses kernel pooling and learning-to-rank techniques to combine n-gram soft-matches into a final ranking score.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.9,\n    \"b31\": 0.8,\n    \"b28\": 0.7,\n    \"b20\": 0.7,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are effectively modeling n-gram soft matches in neural information retrieval (neural IR) and dealing with parameter space explosion and data sparsity when treating n-grams atomically. The proposed solution is the Convolutional Kernel-based Neural Ranking Model (Conv-KNRM), which uses Convolutional Neural Networks (CNN) to compose n-gram embeddings from word embeddings and applies kernel pooling with learning-to-rank techniques to optimize relevance ranking.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b39\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b31\": 0.7,\n    \"b33\": 0.7,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.6,\n    \"b38\": 0.6,\n    \"b30\": 0.5\n  }\n}\n```"], "5d9ed30647c8f76646f7f04c": ["```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is the noisy data generated by Distant Supervision (DS) in Relation Classification (RC). The proposed algorithm, ARNOR, aims to reduce this noise through a novel attention regularization method. ARNOR trains a neural model to focus on relation patterns and selects trustable instances based on the clarity with which the model can explain these patterns.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b5\": 0.9,\n    \"b3\": 0.9,\n    \"b2\": 0.8,\n    \"b10\": 0.8,\n    \"b4\": 0.8,\n    \"b18\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b8\": 0.7,\n    \"b16\": 0.7,\n    \"b20\": 0.7,\n    \"b17\": 0.7,\n    \"b13\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of noisy data in Distant Supervision (DS)-based Relation Classification (RC) and proposes a novel Attention Regularization (AR) framework called ARNOR to reduce noise and improve interpretability. The ARNOR framework iteratively learns an interpretable model and selects trustable instances based on attention regularization and instance selection.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b9\": 0.85,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b11\": 0.7,\n    \"b21\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of reducing noise in Distant Supervision (DS) for Relation Classification (RC) by proposing a novel Attention Regularization-based framework called ARNOR. ARNOR iteratively learns an interpretable model and selects trustable instances based on their relation patterns. The framework seeks to improve RC performance and noise reduction effects by training a neural model to focus on relation patterns and selecting instances that can be clearly explained by the model.\",\n    \"Direct Inspiration\": {\n        \"b9\": 1.0,\n        \"b2\": 0.9,\n        \"b11\": 0.9,\n        \"b8\": 0.8,\n        \"b10\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b3\": 0.7,\n        \"b20\": 0.6,\n        \"b22\": 0.6,\n        \"b23\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.5,\n        \"b13\": 0.5,\n        \"b12\": 0.5,\n        \"b17\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of noise in Distant Supervision (DS) for Relation Classification (RC) and proposes a novel framework called ARNOR. ARNOR uses attention regularization to focus on relation patterns and iteratively refines the model by selecting trustable instances based on their interpretability. The framework aims to improve both RC performance and noise reduction.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b3\": 0.9,\n    \"b2\": 0.8,\n    \"b10\": 0.8,\n    \"b4\": 0.8,\n    \"b18\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b20\": 0.7,\n    \"b17\": 0.6,\n    \"b13\": 0.6,\n    \"b12\": 0.6,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge addressed in the paper is the noise in distant supervision (DS) data for relation classification (RC), which negatively impacts the performance of RC models. The authors propose a novel attention regularization-based framework, ARNOR, to reduce this noise by making the model focus on relation patterns and selecting trustable instances based on clear explanations provided by the model.\",\n    \"Inspirations\": \"The paper draws inspiration from various methods for dealing with noisy DS data, including multi-instance learning, reinforcement learning, adversarial training, and pattern-based extraction.\"\n  },\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b5\": 1,\n    \"b3\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b8\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.8,\n    \"b23\": 0.8\n  }\n}\n```"], "58d82fc8d649053542fd5862": ["```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is recognizing objects at vastly different scales in computer vision. The proposed method, Feature Pyramid Network (FPN), leverages the pyramidal feature hierarchy of ConvNets to create feature pyramids with strong semantics at all levels. This architecture combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections.\",\n    \"Direct Inspiration\": {\n        \"b21\": 1.0,\n        \"b28\": 1.0,\n        \"b10\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.8,\n        \"b27\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.6,\n        \"b16\": 0.6,\n        \"b25\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the recognition of objects at vastly different scales in computer vision, and the goal is to leverage a ConvNet's pyramidal feature hierarchy to create a feature pyramid with strong semantics at all scales. The proposed solution is the Feature Pyramid Network (FPN), which combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b28\": 1.0,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b16\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of recognizing objects at different scales in computer vision by proposing a Feature Pyramid Network (FPN) that leverages a ConvNet's pyramidal feature hierarchy to create a feature pyramid with strong semantics across all levels. This new method aims to replace traditional featurized image pyramids, reducing inference time and memory usage while maintaining accuracy.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b16\": 0.7,\n    \"b25\": 0.7,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.8,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of recognizing objects at different scales in computer vision by proposing a Feature Pyramid Network (FPN). It leverages the inherent pyramidal feature hierarchy of deep convolutional networks (ConvNets) to build a feature pyramid with strong semantics at all levels. This approach aims to replace traditional featurized image pyramids, offering improvements in speed, memory efficiency, and accuracy without sacrificing representational power.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.9,\n    \"b20\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b25\": 0.7,\n    \"b27\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is recognizing objects at vastly different scales in computer vision. The proposed solution, Feature Pyramid Networks (FPN), leverages a ConvNet's pyramidal feature hierarchy to create feature pyramids with high-level semantics at all scales. The architecture combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections.\",\n  \"Direct Inspiration\": [\n    \"b21\",\n    \"b27\",\n    \"b16\",\n    \"b7\",\n    \"b25\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b10\",\n    \"b28\",\n    \"b15\"\n  ],\n  \"Other Inspiration\": []\n}\n```"], "5b3d98cc17c44a510f802054": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include dealing with general nonconvex optimization problems in deep neural networks, the computational expense of model ensembles, and improving model accuracy without impacting inference complexity. The proposed algorithm, collaborative learning, tackles these issues by simultaneously training multiple classifier heads within the same network, leveraging techniques like ILR sharing and backpropagation rescaling to enhance performance and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b22\": 0.9,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b2\": 0.8,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b8\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper include the nonconvex optimization problems typical in deep neural networks, lack of guaranteed convergence to a global minimum with local gradient descent methods, and the computational expense of model ensembles during inference.\",\n    \"Inspirations\": \"The paper proposes a framework for collaborative learning inspired by techniques such as auxiliary training, multi-task learning, and knowledge distillation. It aims to improve accuracy without increasing inference costs by incorporating multiple classifier heads within the same network during training.\"\n  },\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b22\": 1.0,\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b3\": 0.8,\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b13\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of nonconvex optimization in deep neural network training and proposes a collaborative learning framework that trains multiple classifier heads simultaneously to improve accuracy without extra inference cost. The method leverages strategies from auxiliary training, multi-task learning, and knowledge distillation, incorporating ILR sharing and backpropagation rescaling to enhance training efficiency and robustness to label noise.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b22\": 0.8,\n    \"b18\": 0.7,\n    \"b3\": 0.6,\n    \"b2\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b19\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the optimization of nonconvex problems in deep neural networks and the computational expense of model ensembles during inference. The proposed algorithm, collaborative learning, addresses these challenges by training multiple classifier heads of the same network simultaneously. This method leverages the advantages of auxiliary training, multi-task learning, and knowledge distillation while maintaining the same inference complexity. Key benefits include improved accuracy, enhanced training time/memory efficiency through ILR sharing, and robustness to label noise.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b9\": 0.85,\n    \"b18\": 0.7,\n    \"b22\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.6,\n    \"b2\": 0.6,\n    \"b19\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b4\": 0.4,\n    \"b10\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the primary challenges of general nonconvex optimization in training deep neural networks, particularly the convergence issues of local gradient descent methods. It proposes a novel collaborative learning framework to improve accuracy without increasing inference complexity, inspired by auxiliary training, multi-task learning, and knowledge distillation. The key mechanisms involve consensus from multiple classifier heads and backpropagation rescaling to enhance performance and stability.\",\n    \"Direct Inspiration\": [\"b18\", \"b3\", \"b2\", \"b9\", \"b22\", \"b1\"],\n    \"Indirect Inspiration\": [\"b19\", \"b20\"],\n    \"Other Inspiration\": [\"b13\", \"b4\", \"b6\"]\n}\n```"], "58437722ac44360f1082f15c": ["```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is improving document ranking in information retrieval (IR) by using a combination of local and distributed text representations. The proposed solution is a novel duet architecture that integrates two deep neural networks (DNNs) for both local and distributed representations, enhancing performance over traditional IR approaches and recent DNN models.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b29\": 0.9,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b35\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.7,\n    \"b30\": 0.7,\n    \"b3\": 0.7,\n    \"b21\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed in the paper is the effective ranking of long body text for document retrieval in information retrieval (IR) systems. The proposed solution is a novel duet architecture that combines local and distributed representations using two deep neural networks (DNNs) to outperform traditional and recent neural models.\",\n    \"inspirations\": \"The work is inspired by the need to combine exact term matching and distributed representations to improve the performance of document ranking models.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b26\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b15\", \"b35\", \"b36\", \"b29\", \"b33\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b32\", \"b30\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively matching queries with long body text in document ranking for information retrieval (IR). It proposes a novel duet architecture combining two deep neural networks (DNNs) that use local and distributed text representations respectively. The local model captures exact term matches and their positions, while the distributed model captures inexact matches and semantic relevance. The duet architecture aims to outperform both traditional and recent neural models by leveraging the strengths of both local and distributed representations.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b35\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b33\": 0.6,\n    \"b32\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving document ranking in information retrieval (IR) by proposing a novel duet architecture that jointly learns local and distributed representations of text. The key contributions include outperforming state-of-the-art neural and traditional baselines and demonstrating the effectiveness of training with human-judged negative examples.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b29\": 0.6,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.8,\n    \"b35\": 0.8,\n    \"b36\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving document ranking in information retrieval (IR) by leveraging both local and distributed text representations. The proposed duet architecture consists of two deep neural networks (DNNs) that are jointly trained to capture exact term matches and semantic similarities, respectively. This approach aims to outperform traditional term-based methods and other neural embedding models by combining the strengths of exact matching and latent semantic analysis.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b11\": 0.9,\n    \"b15\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.7,\n    \"b33\": 0.7,\n    \"b32\": 0.6,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b13\": 0.5,\n    \"b1\": 0.4,\n    \"b38\": 0.4\n  }\n}\n```"], "5ce2d0feced107d4c63dd498": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving the generalization performance of the Adam optimizer compared to SGD with momentum, particularly by decoupling weight decay from gradient-based updates. The primary contributions include the introduction of AdamW (Adam with decoupled weight decay) and empirical validation of its effectiveness across various datasets and training scenarios.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1.0,\n        \"b6\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b5\": 0.9,\n        \"b11\": 0.85,\n        \"b20\": 0.85\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.7,\n        \"b15\": 0.7,\n        \"b13\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the generalization performance of the Adam optimizer by decoupling weight decay from the gradient-based update. The authors propose a new variant called AdamW, which achieves better generalization and simplifies hyperparameter tuning compared to traditional L2 regularization.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b9\": 0.8,\n    \"b15\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of poor generalization of the Adam optimizer compared to SGD with momentum, particularly due to the ineffectiveness of L2 regularization in Adam. The authors propose decoupling weight decay from the gradient-based update to improve Adam's performance.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b0\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b11\": 0.9,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b9\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of poor generalization in adaptive gradient methods, particularly Adam, compared to SGD with momentum. The authors propose decoupling weight decay from the gradient-based update in Adam to improve its generalization performance. This novel method, termed AdamW, is shown to achieve better results than traditional Adam with L2 regularization across various datasets and training configurations.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b0\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.7,\n    \"b9\": 0.8,\n    \"b11\": 0.7,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the generalization of the Adam optimizer compared to SGD with momentum by decoupling weight decay from gradient-based updates. It proposes a variant of Adam, called AdamW, which separates weight decay from the gradient update process, leading to better generalization performance and easier hyperparameter optimization.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b11\": 0.7,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b9\": 0.5,\n    \"b15\": 0.5\n  }\n}\n```"], "5e09a83ddf1a9c0c41685fc3": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is bridging the gap between relevance matching (RM) and semantic matching (SM) in neural networks for tasks such as information retrieval (IR) and natural language processing (NLP). The paper proposes a novel model, HCAN (Hybrid Co-Attention Network), to jointly model both RM and SM processes. The model includes a hybrid encoder module, a relevance matching module, and a semantic matching module, integrating various signals for improved performance across multiple IR and NLP tasks.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b2\": 0.85,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b25\": 0.7,\n    \"b31\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b13\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating relevance matching and semantic matching in information retrieval (IR) and natural language processing (NLP) tasks. It proposes a novel neural ranking approach called HCAN (Hybrid Co-Attention Network) to jointly model both relevance and semantic matching processes. The paper introduces three types of encoders (deep, wide, and contextual) and integrates relevance and semantic matching signals using a fully-connected layer for final classification.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b13\": 1.0,\n    \"b26\": 1.0,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b25\": 0.8,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in this paper involve the differences between relevance matching and semantic matching in NLP tasks. The proposed solution, HCAN (Hybrid Co-Attention Network), aims to combine methods from both relevance and semantic matching to improve performance in tasks like answer selection, paraphrase identification, and tweet search. The key components include hybrid encoders, a relevance matching module, and a semantic matching module. The approach integrates these signals using a fully-connected layer for final classification.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b2\": 0.90,\n    \"b26\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.80,\n    \"b9\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.70,\n    \"b10\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of semantic matching (SM) and relevance matching (RM) in NLP and IR tasks. The proposed HCAN (Hybrid Co-Attention Network) model combines relevance and semantic matching processes using a hybrid encoder, relevance matching module, and semantic matching module to improve performance on tasks like answer selection, paraphrase identification, semantic similarity measurement, and tweet search.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b2\": 0.9,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b25\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the differences between semantic matching and relevance matching in NLP and IR tasks, and the need to develop a model that can effectively handle both. The proposed HCAN (Hybrid Co-Attention Network) model integrates relevance and semantic matching signals using a hybrid encoder module, relevance matching module, and semantic matching module.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b13\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```"], "5f75aa6a9fced0a24b64599d": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is to achieve higher Instruction-Level Parallelism (ILP) and Memory-Hierarchy Parallelism (MHP) with lower hardware complexity compared to traditional superscalar out-of-order (OoO) and slice-out-of-order (sOoO) cores. The proposed Forward Slice Core (FSC) microarchitecture introduces a novel approach by focusing on forward slices of loads rather than backward slices, simplifying hardware requirements and improving performance metrics.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces the Forward Slice Core (FSC) microarchitecture to address the limitations in Instruction-Level Parallelism (ILP), Memory-Hierarchy Parallelism (MHP), and hardware complexity seen in previous slice-out-of-order (sOoO) cores like Load Slice Core (LSC) and Freeway. The FSC focuses on forward slices rather than backward slices, simplifies hardware, and improves performance by steering instructions into separate queues based on dependencies and using novel techniques like Store-Address Replication (SAR).\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving instruction-level parallelism (ILP) and memory-hierarchy parallelism (MHP) in processor design while minimizing hardware complexity. The proposed Forward Slice Core (FSC) microarchitecture focuses on forward slices of load instructions to simplify hardware and enhance performance, distinguishing itself from prior slice-out-of-order (sOoO) cores like Load Slice Core (LSC) and Freeway.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of achieving high performance and energy efficiency in processor design while minimizing hardware complexity. The proposed Forward Slice Core (FSC) microarchitecture improves instruction-level parallelism (ILP) and memory-hierarchy parallelism (MHP) by focusing on forward slices of load instructions and employing techniques such as the Main Lane (ML), Dependent Load Lane (DLL), Dependent Execute Lane (DEL), Holding Lane (HL), and Store-Address Replication (SAR). This approach offers a significant improvement over previous slice-out-of-order (sOoO) cores like Load Slice Core (LSC) and Freeway by reducing hardware overhead and achieving better performance.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is achieving high performance and high memory-hierarchy parallelism (MHP) with lower hardware complexity compared to prior slice-out-of-order (sOoO) cores. The proposed algorithm, Forward Slice Core (FSC), leverages forward slices of load instructions to simplify hardware requirements and improve instruction-level parallelism (ILP) and MHP. FSC introduces novel mechanisms such as Steering Bit Vector (SBV), multiple in-order FIFO queues, Holding Lane (HL), and Store-Address Replication (SAR) to tackle the limitations and hardware complexities of prior sOoO cores like Load Slice Core (LSC) and Freeway.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b9\": 1\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```"], "5da1a6d447c8f7664606888d": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of speeding up computer architecture evaluation, specifically focusing on cache warming methodologies. It introduces DeLorean, which combines Directed Statistical Warming (DSW) to reduce the number of reuse distances collected and Time Traveling (TT) to iteratively gather necessary data for accurate simulation. DeLorean is implemented on the gem5 simulator using KVM, achieving significant speed and accuracy improvements over existing methods like Randomized Statistical Warming (RSW) and Functional Warming (FW).\",\n    \"Direct Inspiration\": {\n        \"b22\": 1.0,\n        \"b33\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b10\": 0.8,\n        \"b25\": 0.7,\n        \"b31\": 0.6,\n        \"b32\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.5,\n        \"b12\": 0.5,\n        \"b19\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of speeding up computer architecture evaluation, specifically focusing on fast and accurate cache warming for sampled evaluations. The proposed solution, DeLorean, introduces Directed Statistical Warming (DSW) and Time Traveling (TT) to efficiently and accurately warm up cache states, reducing the overhead of traditional methods and improving simulation speed and accuracy.\",\n    \"Direct Inspiration\": {\n        \"b22\": 0.9,\n        \"b33\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b31\": 0.75,\n        \"b32\": 0.75,\n        \"b25\": 0.7,\n        \"b10\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.6,\n        \"b19\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of speeding up computer architecture evaluations, particularly focusing on cache warming in sampled evaluations. The proposed 'DeLorean' methodology builds on Directed Statistical Warming (DSW) and Time Traveling (TT) to significantly reduce warm-up overhead and improve simulation speed and accuracy.\",\n    \"Direct Inspiration\": {\n        \"b10\": 1.0,\n        \"b22\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.9,\n        \"b33\": 0.85\n    },\n    \"Other Inspiration\": {\n        \"b31\": 0.8,\n        \"b25\": 0.75\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of speeding up computer architecture evaluation by proposing DeLorean, a methodology that combines Directed Statistical Warming (DSW) and Time Traveling (TT). DSW focuses on reducing the number of reuse distances needed for accurate cache state prediction, while TT allows for efficient collection of these reuse distances by iteratively moving forward and backward in time.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b31\": 0.7,\n    \"b32\": 0.7,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of speeding up computer architecture evaluation by proposing DeLorean, which combines Directed Statistical Warming (DSW) and Time Traveling (TT). DSW reduces the number of reuse distances needed for accurate cache state prediction by focusing on key reuse distances, while TT enables the collection of these key reuse distances by iteratively rolling back and forth in the simulation process.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b31\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.6,\n    \"b4\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```"], "5e09a76bdf1a9c0c41677a7b": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in personalized article recommendation, especially for new users with minimal feedback, by proposing a framework (POLAR++) that combines Bayesian neural networks and active learning with one-shot learning. The aim is to efficiently and effectively capture user preferences through minimal interactions and provide accurate recommendations.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b6\": 1,\n    \"b7\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.9,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in personalized article recommendation for new users, particularly those facing the cold-start problem. It proposes a framework, POLAR++, which combines active learning with one-shot deep learning to effectively learn user preferences through minimal user interactions. The framework utilizes Bayesian neural networks to capture the uncertainty of user preferences and integrates density-weighted Expected Loss Optimization to enhance performance.\",\n  \"Direct Inspiration\": [\n    \"b5\",\n    \"b6\",\n    \"b7\",\n    \"b15\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b16\",\n    \"b8\",\n    \"b9\",\n    \"b10\",\n    \"b11\",\n    \"b12\",\n    \"b13\",\n    \"b14\"\n  ],\n  \"Other Inspiration\": [\n    \"b1\",\n    \"b2\",\n    \"b3\",\n    \"b4\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in personalized article recommendation, particularly for new users with cold-start problems and limited implicit feedback. The proposed solution, POLAR++, combines Bayesian neural networks with active learning and one-shot learning to effectively capture user preferences and provide personalized recommendations.\",\n  \"Direct Inspiration\": [\"b5\", \"b6\", \"b7\", \"b15\", \"b16\"],\n  \"Indirect Inspiration\": [\"b3\", \"b4\", \"b8\", \"b9\", \"b10\", \"b11\", \"b12\", \"b13\", \"b14\"],\n  \"Other Inspiration\": [\"b17\", \"b18\", \"b19\", \"b20\", \"b21\", \"b22\", \"b23\", \"b24\", \"b25\", \"b26\", \"b27\", \"b28\", \"b29\", \"b30\", \"b31\", \"b32\", \"b33\", \"b34\", \"b35\", \"b36\", \"b37\", \"b38\", \"b39\", \"b40\", \"b41\", \"b42\", \"b43\", \"b44\", \"b45\", \"b46\", \"b47\", \"b48\", \"b49\", \"b50\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of personalized article recommendation, particularly for new users with limited feedback (cold-start problem). It proposes a novel approach combining Bayesian neural networks and active learning to capture user preferences efficiently through minimal interactions.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b6\": 1,\n    \"b7\": 1,\n    \"b15\": 1,\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.5,\n    \"b40\": 0.5,\n    \"b41\": 0.5,\n    \"b42\": 0.5,\n    \"b43\": 0.5,\n    \"b44\": 0.5,\n    \"b46\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of personalized article recommendation, particularly for new users with the cold-start problem, by proposing a one-shot deep learning model combined with active learning strategies. The novel method, POLAR++, uses Bayesian neural networks to capture user preference uncertainty and employs density-weighted Expected Loss Optimization to enhance recommendation performance based on limited feedback.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b6\": 1.0,\n    \"b7\": 1.0,\n    \"b15\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b9\": 0.7,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b4\": 0.5,\n    \"b17\": 0.4,\n    \"b24\": 0.4,\n    \"b25\": 0.4,\n    \"b26\": 0.4,\n    \"b27\": 0.4\n  }\n}\n```"], "5ec49a639fced0a24b4de7ed": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of information overload in news recommendation systems by proposing a novel Graph Neural News Recommendation Model with Unsupervised preference Disentanglement (GNUD). The model captures high-order connectivities and disentangles latent preference factors in user-news interactions to improve recommendation performance.\",\n    \"Direct Inspiration\": {\n        \"b24\": 0.9,\n        \"b18\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b31\": 0.8,\n        \"b25\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.7,\n        \"b27\": 0.65,\n        \"b9\": 0.6,\n        \"b30\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of information overload in news recommendation by proposing a novel Graph Neural News Recommendation Model with Unsupervised preference Disentanglement (GNUD). The core contributions include modeling user-news interactions as a bipartite graph to capture high-order connectivities, and using a neighborhood routing mechanism to disentangle latent preference factors. A novel preference regularizer is introduced to enhance the quality of disentangled representations.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b31\": 0.6,\n    \"b27\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.5,\n    \"b9\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"The primary challenges outlined in the paper are: 1) the need to learn better representations of users and news for recommendations, 2) the necessity to capture high-order connectivity among users and news to exploit structure characteristics and alleviate sparsity, and 3) the need to uncover and disentangle latent preference factors to enhance expressiveness and interpretability.\",\n        \"algorithm\": \"The paper proposes a novel Graph Neural News Recommendation Model with Unsupervised preference Disentanglement (GNUD). The model captures high-order connectivities by propagating user and news representations along a bipartite graph and uses a neighborhood routing mechanism for preference disentanglement. A novel preference regularizer is also introduced to maximize mutual information between preference factors and disentangled embeddings.\"\n    },\n    \"Direct Inspiration\": {\n        \"b18\": 1.0,\n        \"b30\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b25\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b24\": 0.7,\n        \"b27\": 0.7,\n        \"b31\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of information overload in news recommendation by proposing a Graph Neural News Recommendation Model with Unsupervised preference Disentanglement (GNUD). The model captures high-order connectivities in user-news interactions and disentangles latent preference factors to improve representation quality and interpretability.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b31\": 0.85,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.75,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.65,\n    \"b7\": 0.6,\n    \"b9\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of information overload in news platforms by improving the accuracy of news recommendations. It proposes a novel Graph Neural News Recommendation Model with Unsupervised preference Disentanglement (GNUD) which captures high-order connectivities in user-news interactions and disentangles latent preference factors. Key contributions include modeling user-news interactions as a bipartite graph, using a preference regularizer, and demonstrating superior performance over state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b25\": 1.0,\n    \"b18\": 1.0,\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b28\": 0.8,\n    \"b9\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b6\": 0.7,\n    \"b22\": 0.7,\n    \"b1\": 0.7,\n    \"b12\": 0.7,\n    \"b3\": 0.7,\n    \"b8\": 0.7,\n    \"b7\": 0.7,\n    \"b15\": 0.7,\n    \"b10\": 0.7,\n    \"b26\": 0.7,\n    \"b0\": 0.7\n  }\n}\n```"], "5b1643ba8fbcbf6e5a9bc5b5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in machine reading comprehension and automated question answering, particularly focusing on improving model speed and efficiency by using a recurrency-free architecture based on convolutions and self-attention. Key contributions include a novel model called QANet and a data augmentation technique using backtranslation.\",\n  \"Direct Inspiration\": [\"b31\", \"b2\", \"b43\"],\n  \"Indirect Inspiration\": [\"b28\", \"b35\"],\n  \"Other Inspiration\": [\"b14\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of slow training and inference times in machine reading comprehension models that use recurrent neural networks (RNNs). The proposed solution, QANet, replaces the recurrent components with convolutions and self-attentions to improve speed while maintaining accuracy. The model is tested on the SQuAD dataset and shows significant improvements in both training and inference times. Additionally, a data augmentation technique using backtranslation is introduced to enhance training data and improve model performance.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.9,\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.7,\n    \"b14\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of slow training and inference in machine reading comprehension models due to their recurrent nature. The proposed QANet model eliminates recurrence, using convolutions and self-attentions to enhance speed while maintaining accuracy. The model also incorporates a novel data augmentation technique through backtranslation to further improve performance.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.8,\n    \"b35\": 0.8,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies in machine reading comprehension models, particularly their slow training and inference speeds due to recurrent components. The proposed QANet model utilizes convolutions and self-attention mechanisms instead of RNNs to enhance speed while maintaining or improving accuracy. Additionally, a data augmentation technique using backtranslation is introduced to further boost performance.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b35\": 0.8,\n    \"b2\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.6,\n    \"b43\": 0.6,\n    \"b15\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.4,\n    \"b28\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of slow training and inference times in machine reading comprehension models, particularly those using recurrent neural networks (RNNs). The proposed solution, QANet, eliminates the recurrent nature by using convolutions and self-attentions, resulting in significant speed improvements while maintaining accuracy. The authors also introduce a data augmentation technique through backtranslation to further enhance the model's performance.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b2\": 0.9,\n    \"b43\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.7,\n    \"b15\": 0.6,\n    \"b25\": 0.5,\n    \"b14\": 0.4\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.3,\n    \"b40\": 0.2,\n    \"b3\": 0.2,\n    \"b42\": 0.1\n  }\n}\n```"], "5e66148a93d709897c385ba2": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of training deep neural networks (DNNs) on datasets with corrupted labels, proposing a collaborative learning (co-learning) algorithm that iteratively refines labels to leverage both correctly labeled and mislabeled data. The inspiration comes from self-training algorithms but introduces innovations to prevent overfitting and amplifying errors.\",\n  \"Direct Inspiration\": {\n    \"b52\": 1,\n    \"b22\": 1,\n    \"b34\": 1,\n    \"b33\": 1,\n    \"b0\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.9,\n    \"b41\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b48\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training deep neural networks (DNNs) reliably on datasets with corrupted labels. The proposed solution is a collaborative learning (co-learning) algorithm inspired by the self-training algorithm. The novelty lies in the early relabelling mechanism, the use of a two-branch network to avoid amplifying mistakes, and a dynamic loss function.\",\n  \"Direct Inspiration\": [\"b52\", \"b22\", \"b34\", \"b33\", \"b0\"],\n  \"Indirect Inspiration\": [\"b19\", \"b41\", \"b12\", \"b9\", \"b48\"],\n  \"Other Inspiration\": [\"b3\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training deep neural networks (DNNs) on datasets with corrupted labels by proposing a collaborative learning (co-learning) algorithm. This algorithm iteratively refines labels and leverages both correctly labeled and mislabeled data to improve model performance. The co-learning approach is inspired by the self-training algorithm but introduces several innovations to prevent overfitting and error amplification, such as early-stage relabelling, a dual-branch network architecture, and a dynamic loss function.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b52\", \"b22\", \"b34\", \"b33\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b0\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b19\", \"b41\", \"b12\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of training deep neural networks (DNNs) on datasets with corrupted labels, which can lead to poor generalization performance. The proposed solution is a collaborative learning (co-learning) algorithm that refines labels iteratively, allowing the model to leverage both correctly labeled and mislabeled data effectively. Key innovations include early relabeling during training, a dual-branch network for mutual supervision, and a dynamic loss function that transitions from original to pseudo labels.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1,\n        \"b19\": 0.9,\n        \"b41\": 0.9,\n        \"b12\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b52\": 0.8,\n        \"b22\": 0.8,\n        \"b34\": 0.8,\n        \"b33\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.7,\n        \"b48\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Training deep neural networks reliably on datasets with corrupted labels remains challenging due to the tendency of DNNs to overfit label noise.\",\n    \"Inspirations\": \"The proposed co-learning algorithm is motivated by self-training and the phenomenon that DNNs learn meaningful patterns before overfitting to noise. It introduces a dynamic loss function and a dual-branch architecture to iteratively refine labels and improve robustness.\"\n  },\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b52\": 0.9,\n    \"b22\": 0.9,\n    \"b34\": 0.9,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b41\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b48\": 0.7\n  }\n}\n```"], "59ae3c262bbe271c4c71ea21": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of natural language inference (NLI), specifically improving the performance of neural network models on the Stanford Natural Language Inference (SNLI) dataset. The proposed solution enhances sequential inference models and incorporates syntactic parsing information, achieving a new state-of-the-art accuracy.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b22\": 1.0,\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.9,\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b27\": 0.7,\n    \"b21\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of modeling natural language inference (NLI) using neural networks. It proposes an Enhanced Sequential Inference Model (ESIM) and a Hybrid Inference Model (HIM) that incorporate syntactic parsing information to achieve state-of-the-art performance.\",\n  \"Direct Inspiration\": [\"b1\", \"b22\", \"b20\"],\n  \"Indirect Inspiration\": [\"b2\", \"b27\", \"b21\"],\n  \"Other Inspiration\": [\"b19\", \"b16\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Modeling natural language inference (NLI) effectively and leveraging syntactic information within neural network frameworks.\",\n    \"inspirations\": \"Enhancing sequential inference models and incorporating syntactic parsing information to achieve state-of-the-art performance.\"\n  },\n  \"Direct Inspiration\": [\"b1\", \"b22\", \"b20\"],\n  \"Indirect Inspiration\": [\"b25\", \"b19\", \"b6\"],\n  \"Other Inspiration\": [\"b2\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on enhancing sequential inference models for natural language inference (NLI) and incorporating syntactic parsing information to improve performance. The main challenges addressed include effective modeling of inference in human language and leveraging syntactic information within neural network frameworks.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b20\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b19\": 0.7,\n    \"b27\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of modeling natural language inference (NLI) by enhancing sequential inference models based on chain networks and incorporating syntactic parsing information with tree-LSTMs. The proposed models achieve state-of-the-art accuracy on the SNLI dataset, demonstrating the effectiveness of combining sequential and syntactic approaches.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b22\": 1.0,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```"], "5f8d6be69fced0a24bbab01e": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of sparsity and cold start problems in collaborative filtering (CF) based recommendation systems. It proposes a novel Disentangled Heterogeneous Graph Attention Network (DisenHAN) to explicitly model various influential factors from different aspects and their corresponding influence from semantic relations. This approach aims to improve representation robustness and interpretability in heterogeneous information networks (HINs) for recommendation tasks.\",\n    \"Direct Inspiration\": [\"b17\", \"b6\", \"b27\", \"b7\", \"b29\", \"b33\", \"b34\", \"b35\", \"b36\"],\n    \"Indirect Inspiration\": [\"b31\", \"b20\", \"b32\", \"b38\"],\n    \"Other Inspiration\": [\"b25\", \"b26\", \"b39\", \"b42\", \"b24\", \"b13\", \"b9\", \"b0\", \"b3\", \"b41\", \"b30\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in recommendation systems by tackling the issues of non-robustness and low interpretability in previous methods. It introduces a novel Disentangled Heterogeneous Graph Attention Network (DisenHAN) that learns disentangled representations with different aspects in HIN for recommendation.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b23\": 1,\n    \"b31\": 1,\n    \"b32\": 1,\n    \"b38\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b27\": 0.8,\n    \"b7\": 0.7,\n    \"b29\": 0.7,\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7,\n    \"b36\": 0.7,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b12\": 0.6,\n    \"b15\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper include the inability to disentangle latent factors in HIN for recommendation, oversimplification of meta paths, and non-robustness and low interpretability of current methods.\",\n    \"Algorithm\": \"The proposed algorithm is a Disentangled Heterogeneous Graph Attention Network (DisenHAN) that explicitly models factors from different aspects and corresponding influence from semantic relations, using a HIN structure and attention mechanisms to generate meta paths and capture semantic information in high-order connectivity.\"\n  },\n  \"Direct Inspiration\": {\n    \"References\": [\"b20\", \"b32\", \"b38\"]\n  },\n  \"Indirect Inspiration\": {\n    \"References\": [\"b18\", \"b30\"]\n  },\n  \"Other Inspiration\": {\n    \"References\": [\"b1\", \"b2\", \"b31\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Disentangling latent factors in HIN\",\n      \"Non-robustness and low interpretability of current methods\",\n      \"Manual design of meta paths\"\n    ],\n    \"Inspirations\": [\n      \"Using disentangled representations for better user-item interaction modeling\",\n      \"Incorporating attention mechanisms for semantic embedding propagation\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b23\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b25\": 0.8,\n    \"b26\": 0.8,\n    \"b9\": 0.8,\n    \"b7\": 0.75,\n    \"b33\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the sparsity of user-item interactions and the cold start problem in collaborative filtering-based recommender systems. The paper proposes a Disentangled Heterogeneous Graph Attention Network (DisenHAN) to explicitly model factors from different aspects and their corresponding influence from semantic relations. This approach leverages heterogeneous information networks (HIN) and utilizes attention mechanisms to generate meta-paths automatically, enhancing the robustness and interpretability of recommendations.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b29\": 0.9,\n    \"b33\": 0.9,\n    \"b34\": 0.9,\n    \"b35\": 0.9,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b27\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```"], "5c0495ae17c44a2c747018e6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a neural network architecture that is efficient for mobile and resource-constrained environments, while maintaining high accuracy. The proposed solution is a novel layer module called the inverted residual with linear bottleneck, which significantly reduces computational and memory requirements without sacrificing accuracy.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b26\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b19\", \"b21\", \"b7\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b22\", \"b23\", \"b24\", \"b25\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is the high computational resource requirement of modern neural networks, which limits their applicability in mobile and embedded environments. The paper proposes a new neural network architecture tailored for resource-constrained environments, introducing a novel layer module called the inverted residual with linear bottleneck, which significantly reduces the number of operations and memory needed while retaining accuracy.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b19\": 0.85,\n    \"b26\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.75,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.65,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a new neural network architecture tailored for mobile and resource-constrained environments, focusing on reducing the number of operations and memory needed while retaining accuracy. The main contribution is the inverted residual with linear bottleneck module, which efficiently uses standard operations to achieve state-of-the-art performance in mobile designs by reducing memory footprint during inference.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b21\": 0.85,\n    \"b26\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is developing a neural network architecture that achieves high accuracy while being computationally efficient for mobile and embedded applications. The main contribution is the introduction of the inverted residual with linear bottleneck module, which reduces computational complexity and memory usage without sacrificing accuracy. Key inspirations include MobileNetV1 and depthwise separable convolutions.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b21\": 0.7,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b28\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of developing a neural network architecture that is efficient for mobile and resource-constrained environments without sacrificing accuracy. The main contribution is a novel layer module called the inverted residual with linear bottleneck, which reduces the number of operations and memory required. The paper draws on several works to achieve this, primarily focusing on improving the balance between accuracy and performance in neural networks.\",\n    \"Direct Inspiration\": {\n        \"b26\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.9,\n        \"b21\": 0.8,\n        \"b22\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.6,\n        \"b28\": 0.5,\n        \"b29\": 0.4\n    }\n}\n```"], "5e3be3c33a55ac29c4ae7e18": ["```json\n{\n  \"Summary\": \"The paper addresses the trade-off between efficiency and effectiveness in neural network-based re-ranking models for ad-hoc retrieval in time-constrained environments. It introduces the Transformer-Kernel (TK) model, which combines lightweight Transformer layers for contextualization and kernel-pooling for explainability.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b35\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b14\": 0.7,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b19\": 0.4,\n    \"b11\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents TK, an interpretable neural re-ranking model designed for efficient and effective ad-hoc retrieval in time-constrained environments. TK's primary contributions are its efficiency, utilizing small and lightweight Transformer layers for contextualization, and its explainability, achieved through a transparent network structure involving a single interaction match matrix and kernel-pooling. The paper introduces time-budget aware evaluation and demonstrates the interpretability of TK through query-level analysis and visualization of word-level similarities.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b35\": 0.9,\n    \"b4\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b23\": 0.7,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b18\": 0.5,\n    \"b22\": 0.4,\n    \"b30\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of balancing efficiency and effectiveness in neural re-ranking models for ad-hoc retrieval under time constraints. It introduces the Transformer-Kernel (TK) model, which uses lightweight Transformer layers for contextualization and kernel-pooling for interaction scoring. TK is designed to be both efficient and interpretable.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1,\n    \"b31\": 0.9,\n    \"b4\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b5\": 0.6,\n    \"b23\": 0.6,\n    \"b19\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.4,\n    \"b21\": 0.4,\n    \"b15\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of balancing efficiency and effectiveness in neural re-ranking models for ad-hoc retrieval under time constraints. It proposes the TK (Transformer-Kernel) model, which uses a small number of lightweight Transformer layers for contextualization and a kernel-pooling technique for interaction scoring. The model is designed to be both efficient and interpretable, with experiments demonstrating its superiority over BERT in time-constrained scenarios.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b35\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b14\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the efficiency and effectiveness of neural re-ranking models in time-constrained environments. The authors propose the TK model, which combines a small number of lightweight Transformer layers with kernel-pooling to achieve a good balance between efficiency and explainability.\",\n  \"Direct Inspiration\": [\"b31\", \"b35\"],\n  \"Indirect Inspiration\": [\"b4\", \"b23\", \"b24\", \"b19\"],\n  \"Other Inspiration\": [\"b10\", \"b21\", \"b25\"]\n}\n```"], "5edf5dd891e011bc656ded70": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of noisy implicit feedback in recommender systems, which negatively impacts the quality of recommendations by misleading the learning process. The authors propose Adaptive Denoising Training (ADT) strategies that dynamically prune or reweigh interactions with large loss values during training, without requiring additional data.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b12\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b32\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b39\": 0.6,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of denoising implicit feedback in recommender systems to improve recommendation quality. It identifies the negative effect of false-positive interactions on recommender training and proposes Adaptive Denoising Training (ADT) strategies to dynamically prune or reweight large-loss interactions without requiring additional data.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b19\": 0.9,\n    \"b1\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b38\": 0.6,\n    \"b6\": 0.6,\n    \"b20\": 0.6,\n    \"b40\": 0.6,\n    \"b42\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.5,\n    \"b39\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of noisy implicit feedback in recommender systems, particularly the detrimental effects of false-positive interactions on recommendation quality. It proposes an Adaptive Denoising Training (ADT) strategy to dynamically prune or reweight such interactions without relying on additional data, thereby improving the recommendation performance.\",\n  \"Direct Inspiration\": [\"b1\", \"b12\", \"b19\"],\n  \"Indirect Inspiration\": [\"b17\", \"b20\", \"b32\", \"b38\"],\n  \"Other Inspiration\": [\"b15\", \"b39\", \"b42\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of denoising implicit feedback in recommender systems, focusing on reducing the influence of false-positive interactions without using additional data. The proposed Adaptive Denoising Training (ADT) strategies dynamically prune or reweight large-loss interactions to improve the quality of recommendations.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b12\": 0.9,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b32\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b20\": 0.5,\n    \"b40\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of noisy implicit feedback in recommender systems, specifically focusing on false-positive interactions that mislead user preference learning. The authors propose Adaptive Denoising Training (ADT) strategies that dynamically prune or reweight large-loss interactions during training to improve recommendation quality without relying on additional data.\",\n    \"Direct Inspiration\": {\n        \"b12\": 0.9,\n        \"b19\": 0.9,\n        \"b1\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b32\": 0.7,\n        \"b38\": 0.7,\n        \"b20\": 0.65,\n        \"b40\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.6,\n        \"b39\": 0.6\n    }\n}\n```"], "5da052ba3a55acfef148243e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of aggregating multi-source imperfect annotations for sequence labeling in NLP tasks. The proposed solution is a framework named Consensus Network (CONNET), which models annotator-specific biases and dynamically assigns the best sources for unseen sentences using a context-aware attention mechanism.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b39\": 0.9,\n    \"b29\": 0.85,\n    \"b35\": 0.85,\n    \"b38\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b22\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.6,\n    \"b42\": 0.6,\n    \"b25\": 0.55,\n    \"b27\": 0.55,\n    \"b37\": 0.5,\n    \"b36\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include aggregating multi-source imperfect annotations without knowing the underlying ground truth label sequences in the target domain. The proposed algorithm, CONNET, aims to dynamically assign the best sources for each unseen sentence by decoupling annotator-specific biases and using an attention network to achieve consensus among multiple sources.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b39\": 0.8,\n    \"b4\": 0.85,\n    \"b6\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b7\": 0.7,\n    \"b22\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b23\": 0.6,\n    \"b28\": 0.6,\n    \"b32\": 0.65,\n    \"b11\": 0.65,\n    \"b24\": 0.65,\n    \"b13\": 0.65,\n    \"b9\": 0.65,\n    \"b18\": 0.65,\n    \"b14\": 0.6,\n    \"b2\": 0.6,\n    \"b42\": 0.6,\n    \"b29\": 0.6,\n    \"b35\": 0.6,\n    \"b38\": 0.6,\n    \"b43\": 0.6,\n    \"b16\": 0.65,\n    \"b37\": 0.6,\n    \"b36\": 0.6,\n    \"b44\": 0.6,\n    \"b25\": 0.6,\n    \"b27\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of aggregating multi-source imperfect annotations for sequence labeling in NLP. The proposed solution, Consensus Network (CONNET), dynamically assigns the best sources of annotations for each sentence, considering the unique strengths of different sources. The framework decouples annotator-specific biases and integrates them through an attention mechanism to achieve consensus among sources.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b39\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b6\": 0.75,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.65,\n    \"b37\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the aggregation of multi-source imperfect annotations for sequence labeling tasks, specifically in scenarios involving crowd annotations and unsupervised cross-domain model adaptation. The proposed algorithm, Consensus Network (CONNET), addresses these challenges by modeling annotator-specific biases and dynamically selecting the most suitable sources for each sentence using an attention mechanism.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b32\": 0.85,\n    \"b39\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b24\": 0.7,\n    \"b37\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b8\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of aggregating multi-source imperfect annotations for sequence labeling tasks in NLP, particularly focusing on learning with crowd annotations and unsupervised cross-domain model adaptation. The proposed algorithm, Consensus Network (CONNET), dynamically assigns the best sources for each sentence through a decoupled representation of annotator-specific biases and a context-aware attention mechanism.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b39\": 0.85,\n    \"b4\": 0.8,\n    \"b6\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b2\": 0.7,\n    \"b42\": 0.7,\n    \"b29\": 0.75,\n    \"b35\": 0.75,\n    \"b38\": 0.75,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b27\": 0.6,\n    \"b7\": 0.65,\n    \"b8\": 0.65,\n    \"b16\": 0.65,\n    \"b22\": 0.65,\n    \"b26\": 0.65,\n    \"b37\": 0.65,\n    \"b36\": 0.65,\n    \"b44\": 0.65,\n    \"b18\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```"], "5ebe685391e0117693a52241": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of sequence labeling on noisy input, such as text from OCR or ASR systems, which often contains errors. The proposed solution includes Noise-Aware Training (NAT) objectives to improve robustness without compromising performance on clean data. Key contributions include a model to estimate error distribution, a novel noise induction procedure, a data augmentation algorithm, and an adapted stability training method.\",\n  \"Direct Inspiration\": {\n    \"b51\": 1.0,\n    \"b12\": 0.9,\n    \"b45\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.7,\n    \"b46\": 0.7,\n    \"b32\": 0.6,\n    \"b50\": 0.6,\n    \"b5\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.5,\n    \"b25\": 0.5,\n    \"b37\": 0.5,\n    \"b2\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of sequence labeling in the presence of noisy input, such as OCR and ASR errors or user-generated text with typos. The authors propose Noise-Aware Training (NAT) objectives to improve the robustness of sequence labeling models to such noise without sacrificing performance on clean data. Key contributions include a new noise induction procedure, a data augmentation algorithm, and a stability training method.\",\n  \"Direct Inspiration\": [\"b12\", \"b45\", \"b51\"],\n  \"Indirect Inspiration\": [\"b27\", \"b46\"],\n  \"Other Inspiration\": [\"b3\", \"b15\", \"b38\", \"b25\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving sequence labeling performance on noisy input data, such as text generated from OCR or ASR systems, or user-generated text with typos. It proposes two Noise-Aware Training (NAT) objectives to enhance robustness while maintaining efficiency on clean data. The methods involve data augmentation and stability training to handle input perturbations effectively.\",\n  \"Direct Inspiration\": [\"b51\", \"b12\", \"b45\"],\n  \"Indirect Inspiration\": [\"b6\", \"b38\", \"b25\", \"b27\", \"b46\"],\n  \"Other Inspiration\": [\"b37\", \"b2\", \"b16\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance degradation in sequence labeling systems when handling noisy text, such as errors introduced by OCR or ASR. The authors propose two Noise-Aware Training (NAT) objectives to improve the robustness of sequence labeling on noisy input while maintaining performance on clean data. The novel methods introduced include a noise induction procedure, a data augmentation algorithm, and a stability training method.\",\n  \"Direct Inspiration\": {\n    \"b51\": 1,\n    \"b12\": 0.9,\n    \"b45\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.7,\n    \"b2\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b46\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of sequence labeling on noisy text, which is often encountered in real-world scenarios where text data may have errors from upstream processes like OCR and ASR. The proposed solution includes Noise-Aware Training (NAT) objectives to improve robustness against such noisy inputs without compromising performance on clean data. Key methods include a noise induction procedure, a data augmentation algorithm, and a stability training method.\",\n  \"Direct Inspiration\": [\"b51\", \"b12\", \"b45\"],\n  \"Indirect Inspiration\": [\"b6\", \"b38\", \"b25\", \"b37\", \"b2\", \"b16\"],\n  \"Other Inspiration\": [\"b3\", \"b15\", \"b30\", \"b39\"]\n}\n```"], "5ee9f15b91e01152af022ce0": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper include the noisy and incorrect textual attribute values in eCommerce product catalogs, the inability of traditional anomaly detection methods to handle self-reported attribute values, and the necessity of validating these values against product descriptions.\",\n    \"Inspirations\": \"The author proposes a novel meta-learning latent variable approach named MetaBridge to address these challenges by leveraging limited labeled data and abundant unlabeled data.\"\n  },\n  \"Direct Inspiration\": {\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b23\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b25\": 0.65,\n    \"b12\": 0.6,\n    \"b3\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge is validating the correctness of textual attribute values in product catalogs, which are often noisy due to contributions from individual retailers. Traditional anomaly detection methods are not suitable due to the variability in attribute values across different product categories.\",\n    \"Inspirations\": \"The paper is inspired by the need for automatic validation algorithms and the potential application of natural language inference (NLI) in this context.\"\n  },\n  \"Direct Inspiration\": [\"b11\"],\n  \"Indirect Inspiration\": [\"b10\", \"b18\", \"b23\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of validating textual attribute values in eCommerce product catalogs, proposing a novel meta-learning latent variable approach named MetaBridge. This method leverages both limited labeled data and vast amounts of unlabeled data to make accurate category-specific inferences, preventing overfitting and capturing category uncertainty.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b23\": 0.6,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.5,\n    \"b3\": 0.4,\n    \"b9\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the validation of textual attribute values in product catalogs, which are often noisy and error-prone due to self-reporting by individual retailers. The proposed solution is MetaBridge, a meta-learning latent variable approach that leverages both labeled and unlabeled data to effectively validate these attributes. This approach aims to generalize across various product categories and prevent overfitting.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of validating textual attributes in product catalogs on eCommerce websites. Due to noisy data contributed by individual retailers, traditional anomaly detection methods are insufficient. The paper proposes a novel meta-learning latent variable approach, MetaBridge, which leverages limited labeled data and extensive unlabeled data to effectively validate attribute correctness.\",\n  \"Direct Inspiration\": [\"b10\", \"b11\", \"b12\", \"b18\"],\n  \"Indirect Inspiration\": [\"b23\"],\n  \"Other Inspiration\": [\"b25\"]\n}\n```"], "5f896fa591e01149071e45df": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of few-shot learning in novel domains where large labeled datasets are unavailable. It proposes STARTUP, a method that adapts feature representations from a source domain to a target domain using a combination of labeled and unlabeled data. This approach aims to bridge the gap between self-supervised learning and few-shot learning by leveraging pre-trained classifiers to induce useful groupings in the target domain.\",\n    \"Direct Inspiration\": {\n        \"b12\": 1,\n        \"b1\": 0.9,\n        \"b44\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b35\": 0.7,\n        \"b45\": 0.7,\n        \"b50\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b40\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot learning in novel domains where labeled data is scarce and large base datasets are from different domains. The proposed algorithm, STARTUP, adapts feature representations from a source task to target domains using a mix of self-training and self-supervised learning techniques to improve few-shot performance.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.95,\n    \"b1\": 0.92,\n    \"b50\": 0.88\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.80,\n    \"b44\": 0.75,\n    \"b35\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.60,\n    \"b51\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training recognition systems for new classes in novel domains with limited labeled data. The proposed algorithm, STARTUP, adapts feature representations trained on source tasks to different target domains using self-supervised learning techniques. This approach allows for rapid learning of novel classes with minimal labeled data, bridging the gap between few-shot learning and self-supervised learning.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b1\": 0.9,\n    \"b50\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.8,\n    \"b45\": 0.8,\n    \"b44\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b51\": 0.7,\n    \"b32\": 0.65,\n    \"b22\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in few-shot learning (FSL) where there is a significant domain gap between the base dataset and the target dataset. The proposed solution, STARTUP, adapts feature representations trained on source tasks to extremely different target domains by leveraging both labeled data from the source domain and unlabeled data from the target domain. This approach aims to create a more effective representation for few-shot tasks in novel domains.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b12\": 0.9,\n    \"b45\": 0.8,\n    \"b50\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b44\": 0.7,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot learning in novel domains with large domain gaps by proposing STARTUP, a self-training approach to adapt feature representations from a source domain to a target domain using both labeled base data and unlabeled target data.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b50\": 0.9,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.8,\n    \"b45\": 0.8,\n    \"b44\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b51\": 0.6\n  }\n}\n```"], "5f03f3b611dc830562232090": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying numerical inconsistency errors in financial document tables due to the diverse tabular presentation and the large volume of data. It proposes an end-to-end solution using a binary classification model to judge the semantic equivalence of table cells without explicit semantic parsing, and introduces practical techniques to handle the large number of table cell pairs and the extreme classification imbalance among them.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b3\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of numerical inconsistency errors in financial documents, specifically in tabular forms. It proposes an automated system, ANCOT, to identify semantically equivalent table cells and detect inconsistencies using an end-to-end binary classification model. The system significantly reduces the time and effort required for manual cross-checking, achieving high accuracy and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b7\": 0.5,\n    \"b8\": 0.4,\n    \"b11\": 0.6,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting numerical inconsistencies in financial document tables due to the tabular presentation diversity, huge number of table cell pairs, and extreme classification imbalance. The authors propose an end-to-end solution using a cell embedding network and pair classification network to identify semantically equivalent table cells. They also introduce filtering methods to reduce the number of pairs for classification, achieving high precision and recall in detecting inconsistencies.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b3\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Tabular presentation diversity\",\n      \"Huge number of table cell pairs\",\n      \"Extreme classification imbalance\"\n    ],\n    \"algorithm\": \"End-to-end model for binary classification to judge semantic equivalence of table cells\"\n  },\n  \"Direct Inspiration\": {\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b3\": 0.5,\n    \"b7\": 0.5,\n    \"b8\": 0.4,\n    \"b12\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of detecting numerical inconsistency errors in financial tables, which can harm a firm's reputation and lead to economic losses. It proposes an end-to-end solution for automatic numerical cross-checking over tables (ANCOT) that identifies semantically equivalent table cells to detect inconsistencies. Key challenges include tabular presentation diversity, the huge number of table cell pairs, and extreme classification imbalance. The proposed method employs a binary classification model along with practical techniques for efficient processing.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b11\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.7,\n        \"b3\": 0.7,\n        \"b7\": 0.6,\n        \"b8\": 0.6\n    }\n}\n```"], "5e5e190b93d709897ce4997e": ["```json\n{\n  \"Summary\": \"The paper primarily addresses the challenges of building efficient, interpretable, and scalable neuro-symbolic systems that can reason over large knowledge bases (KBs) and natural language texts. The proposed approach, GNTPs, introduces two main contributions: efficiency improvements for NTPs and an extension to jointly embed predicates and textual surface patterns in a shared space.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1,\n    \"b18\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b47\": 0.9,\n    \"b14\": 0.8,\n    \"b25\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Natural Language Understanding (NLU) and Machine Reading (MR) by proposing two efficiency improvements for Neural Theorem Provers (NTPs) and extending them to handle natural language. The novel contributions include reducing time and space complexity of NTPs and embedding both predicates and textual patterns in a shared space for end-to-end differentiable reasoning.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b18\": 0.8,\n    \"b47\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.6,\n    \"b33\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.5,\n    \"b41\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges in Natural Language Understanding (NLU) and Machine Reading (MR) by proposing two main contributions: efficiency improvements for Neural Theorem Provers (NTPs) and an extension of NTPs to handle natural language text. The proposed methods aim to reduce the time and space complexity of NTPs and integrate textual patterns with knowledge base reasoning using end-to-end differentiable models.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b18\": 0.9,\n    \"b47\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.8,\n    \"b41\": 0.8,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of Neural Theorem Provers (NTPs) in handling large Knowledge Bases (KBs) and integrating natural language data. It proposes two main improvements: efficiency enhancements for NTPs and an extension to jointly embed predicates and textual patterns for end-to-end differentiable reasoning. The challenges include the computational complexity of NTPs, the need for reasoning over natural language, and the integration of symbolic and neural methods.\",\n  \"Direct Inspiration\": [\n    \"b39\",\n    \"b18\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b47\",\n    \"b33\",\n    \"b41\"\n  ],\n  \"Other Inspiration\": [\n    \"b38\",\n    \"b15\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scaling Neural Theorem Provers (NTPs) to large Knowledge Bases (KBs) and integrating natural language for reasoning. It proposes two efficiency improvements: reducing time and space complexity of NTPs and extending NTPs to handle natural language by jointly embedding predicates and textual surface patterns.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b47\": 0.8,\n    \"b38\": 0.7,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.5,\n    \"b45\": 0.4,\n    \"b30\": 0.3\n  }\n}\n```"], "5ce3ad3fced107d4c65b6bd9": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is dealing with noisy labels in image classification tasks using Convolutional Neural Networks (CNNs). The authors propose an unsupervised model of label noise based on a Beta Mixture Model (BMM) to correct the loss for each sample dynamically. This approach is combined with mixup data augmentation to achieve robustness against label noise and improve classification accuracy.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b36\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Convolutional Neural Networks (CNNs) in the presence of noisy labels. It proposes a robust training procedure that avoids overfitting to noisy labels by unsupervised modeling of label noise based on the loss of each sample. This is achieved through a two-component beta mixture model (BMM) and a dynamic bootstrapping loss combined with mixup data augmentation.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b22\", \"b34\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b23\", \"b11\", \"b36\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b9\", \"b25\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Convolutional Neural Networks (CNNs) with noisy labels, a common scenario in computer vision tasks. The proposed solution is an unsupervised noise label modeling approach based on the loss of each sample, aiming to prevent overfitting to noisy labels. Key contributions include a two-component beta mixture model (BMM) for label noise, dynamic bootstrapping, and the integration of mixup data augmentation to enhance robustness.\",\n  \"Direct Inspiration\": [\"b22\", \"b34\"],\n  \"Indirect Inspiration\": [\"b36\", \"b33\"],\n  \"Other Inspiration\": [\"b11\", \"b23\", \"b9\", \"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Convolutional Neural Networks (CNNs) with noisy labels, which can lead to overfitting and poor generalization. The authors propose an unsupervised model of label noise based on a two-component beta mixture model (BMM) on the loss values of each sample. This model is used to implement a dynamically weighted bootstrapping loss, which avoids fitting noisy labels while utilizing noisy samples for learning visual representations. Furthermore, the approach is combined with mixup data augmentation to achieve robustness against label noise and high classification accuracy.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b34\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.9,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7,\n    \"b19\": 0.6,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Convolutional Neural Networks (CNNs) with noisy labels, which degrade the generalization capabilities of the model. The authors propose an unsupervised noise label modeling approach using a two-component beta mixture model (BMM) on the loss values. This model is used to implement a dynamically weighted bootstrapping loss, combined with mixup data augmentation, to robustly deal with noisy samples without discarding them. The main contributions include a novel unsupervised noise label modeling technique, a loss correction approach, and an enhanced method combining mixup data augmentation.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b34\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.9,\n    \"b33\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b11\": 0.5\n  }\n}\n```"], "5f7ee07491e011a5faf0feb2": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the combination of predictions from multiple KG embeddings in a multilingual setting, addressing issues of inconsistent information across different language-specific KGs, and improving the performance of fact prediction. The novel method proposed is KEnS, an embedding-based ensemble inference framework that combines multiple KG embeddings to enhance fact prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b9\": 0.9,\n    \"b30\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b26\": 0.75,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.65,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of KG completion in a multilingual setting by proposing KEnS, an ensemble framework for combining predictions from multiple KG embeddings. The primary motivations include leveraging complementary knowledge from different language-specific KGs, improving fact prediction accuracy, and addressing inconsistencies and alignment issues between KGs.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b5\", \"b9\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b30\", \"b38\", \"b31\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b0\", \"b27\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes KEnS, an ensemble framework of KG embedding models for multilingual KG completion. The primary challenges addressed include combining predictions from multiple KGs, handling inconsistencies in multilingual KGs, and improving fact prediction accuracy. The algorithm leverages boosting for entity-specific weight learning to enhance prediction reliability across different KGs.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b9\": 0.9,\n    \"b27\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b30\": 0.7,\n    \"b38\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b24\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the difficulty in combining predictions from multiple KG embeddings due to the lack of reliable alignment information and inconsistencies in how facts are described across different language-specific KGs. The proposed algorithm, KEnS (Knowledge Ensemble), addresses this by combining predictions from embedding models of multiple KGs and using boosting-based techniques to weight the confidence of predictions from different sources. This method aims to improve KG completion in a multilingual setting.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b27\": 0.9,\n    \"b9\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.75,\n    \"b4\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.65,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Combining predictions from multiple KG embeddings due to lack of reliable alignment information and inconsistent descriptions across different KGs.\"\n    ],\n    \"Inspirations\": [\n      \"Proposing KEnS, an ensemble framework for KG embeddings, which leverages multilingual KGs to improve fact prediction.\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b5\",\n    \"b9\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b0\",\n    \"b30\",\n    \"b38\"\n  ],\n  \"Other Inspiration\": [\n    \"b26\",\n    \"b27\"\n  ]\n}\n```"], "5c04967517c44a2c74708f29": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of embedding uncertain Knowledge Graphs (KGs) by developing a novel model called UKGE. The main problems include capturing uncertainty in KGs, accurately estimating the uncertainty of unseen relation facts, and outperforming existing deterministic KG embedding models. UKGE incorporates probabilistic soft logic to infer confidence scores for unseen relations, enhancing the precision of embeddings.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b14\": 0.85,\n    \"b22\": 0.8,\n    \"b21\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b19\": 0.65,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.55,\n    \"b11\": 0.5,\n    \"b17\": 0.45,\n    \"b23\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of embedding uncertain knowledge graphs (KGs) by proposing a novel model called UKGE (Uncertain Knowledge Graph Embeddings). The primary challenges include accurately capturing uncertainty information in KGs and estimating the uncertainty of unseen relation facts. The paper introduces two variants of UKGE that incorporate probabilistic soft logic to enhance the precision of embedding uncertain KGs.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b22\": 1.0,\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.9,\n    \"b19\": 0.8,\n    \"b8\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7,\n    \"b23\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating uncertainty in Knowledge Graph (KG) embeddings, which traditional deterministic models fail to capture. It proposes a new model, UKGE, which learns embeddings while preserving both structural and uncertainty information. The paper introduces probabilistic soft logic (PSL) to enhance the estimation of confidence scores for unseen relation facts. The model is validated through experiments on three real-world uncertain KGs, demonstrating its superior performance in tasks such as confidence prediction, relation fact ranking, and relation fact classification.\",\n  \"Direct Inspiration\": [\"b1\", \"b22\", \"b14\"],\n  \"Indirect Inspiration\": [\"b21\", \"b20\"],\n  \"Other Inspiration\": [\"b8\", \"b23\", \"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of embedding uncertain Knowledge Graphs (KGs) with confidence scores into machine learning models, proposing the UKGE model, which preserves both structural and uncertainty information. The authors highlight the limitations of deterministic KG embeddings and propose using probabilistic soft logic for better estimation of uncertainty in unseen relation facts.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b14\": 1.0,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b20\": 0.75,\n    \"b19\": 0.7,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b17\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of incorporating uncertainty into Knowledge Graph (KG) embeddings. The proposed model, UKGE (Uncertain Knowledge Graph Embeddings), aims to preserve both structural and uncertainty information of relation facts in the embedding space. UKGE uses probabilistic soft logic to infer confidence scores for unseen relation facts, enhancing the embedding precision. The paper evaluates UKGE on three tasks: confidence prediction, relation fact ranking, and relation fact classification, and shows that it consistently outperforms baseline models.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b22\": 0.8,\n        \"b14\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.75,\n        \"b8\": 0.7,\n        \"b21\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.6,\n        \"b23\": 0.65\n    }\n}\n```"], "5edf5dd891e011bc656deed4": ["```json\n{\n  \"Summary\": \"The paper tackles the challenge of noise in large-scale Knowledge Graphs (KGs) by proposing a combined approach using ontologies and embeddings for KG refinement. The framework, IterefinE, integrates Probabilistic Soft Logic (PSL) with KG embeddings like ComplEx and ConvE to improve the quality of KG by reducing noise and enhancing type predictions.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b3\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b19\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing noise in large-scale Knowledge Graphs (KGs) by combining ontologies and embeddings for KG refinement. The proposed framework, IterefinE, integrates Probabilistic Soft Logic (PSL) with KG embeddings like ConvE and ComplEx to provide explicit type supervision and iteratively improve KG refinement performance.\",\n  \"Direct Inspiration\": [\"b18\", \"b23\", \"b7\"],\n  \"Indirect Inspiration\": [\"b3\", \"b25\", \"b19\"],\n  \"Other Inspiration\": [\"b16\", \"b5\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of noise in large-scale Knowledge Graphs (KGs) and proposes a combined use of ontologies and embeddings for KG refinement. The proposed framework, IterefinE, integrates Probabilistic Soft Logic (PSL) with KG embeddings (ConvE and ComplEx) to improve type supervision and eliminate noisy facts.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b18\": 0.95\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b23\": 0.85\n    },\n    \"Other Inspiration\": {\n        \"b25\": 0.75,\n        \"b19\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of knowledge graph (KG) refinement, particularly in reducing noise and improving entity type and fact predictions. It proposes a combined approach using ontologies and embeddings, specifically leveraging Probabilistic Soft Logic (PSL) and state-of-the-art KG embedding methods ConvE and ComplEx. The resulting framework, IterefinE, iteratively refines the KG by transferring predictions between PSL-KGI and the embeddings-based methods, achieving significant improvements in accuracy and noise reduction.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b23\": 1.0,\n    \"b7\": 1.0,\n    \"b18\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.9,\n    \"b12\": 0.9,\n    \"b5\": 0.9,\n    \"b8\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.8,\n    \"b11\": 0.8,\n    \"b14\": 0.8,\n    \"b16\": 0.8,\n    \"b19\": 0.8,\n    \"b25\": 0.8\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of noise reduction in large-scale Knowledge Graphs (KGs) by combining ontological rules and KG embeddings. The proposed algorithm, IterefinE, utilizes Probabilistic Soft Logic (PSL) and state-of-the-art KG embeddings (ConvE and ComplEx) to iteratively refine KGs by eliminating incorrect facts and predicting additional links and entity types. The algorithm leverages explicit type supervision from ontological information to improve the performance of KG embeddings.\",\n    \"Direct Inspiration\": {\n        \"b3\": 1,\n        \"b7\": 1,\n        \"b18\": 1,\n        \"b23\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.8,\n        \"b25\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b5\": 0.6,\n        \"b26\": 0.6\n    }\n}\n```"], "5fdb2e1691e0118a02c4f566": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the extraction of temporal relations from clinical case reports. The authors propose using Probabilistic Soft Logic (PSL) rules to model temporal dependencies and introduce a novel framework, CTRL-PG, for this purpose.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b12\": 1.0,\n    \"b28\": 1.0,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b40\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b30\": 0.7,\n    \"b39\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of temporal relation extraction from clinical case reports. The authors propose leveraging Probabilistic Soft Logic (PSL) rules to model the temporal dependencies more flexibly and efficiently. The main contributions include formulating PSL rules for temporal dependencies and developing a global temporal inference algorithm. The proposed method aims to overcome the limitations of previous approaches that used Integer Linear Programming (ILP) and suffered from high complexity and low scalability.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b12\": 1,\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b40\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.7,\n    \"b31\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of temporal relation extraction in clinical case reports (CCRs). The authors propose leveraging Probabilistic Soft Logic (PSL) rules to model temporal dependencies and penalize violations during training. They introduce a novel time-anchored global temporal inference algorithm to classify relations at the document level. The main contributions include formulating the PSL rules as a regularization term, demonstrating the efficacy of global inference, and releasing the code for further research.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b12\": 1.0,\n    \"b28\": 1.0,\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b40\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.6,\n    \"b31\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of temporal relation extraction in clinical case reports. It proposes a novel method using Probabilistic Soft Logic (PSL) rules to model temporal dependencies. The method aims to improve the accuracy and efficiency of temporal relation extraction by summarizing common transitivity and symmetry patterns as PSL rules and penalizing training instances that violate these rules. A time-anchored global temporal inference algorithm is also introduced to classify relations at the document level.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b12\": 1,\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.9,\n    \"b40\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of temporal relation extraction in clinical case reports (CCRs). The authors propose using Probabilistic Soft Logic (PSL) rules to model temporal dependencies in a more flexible and efficient way compared to previous methods like Integer Linear Programming (ILP). The proposed algorithm, CTRL-PG, incorporates PSL rules as a regularization term and includes a time-anchored global temporal inference module.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b12\": 0.9,\n    \"b28\": 0.9,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b40\": 0.7,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.5\n  }\n}\n```"], "5ee7495191e01198a507f6a4": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of learning disentangled representations in an unsupervised manner, particularly questioning the assumption that real-world observations are generated by independent factors. The authors propose a novel framework using Linear Structural Equation Models (SEM) integrated into a Variational Autoencoder (VAE) to recover dependent factors and ensure identifiability of the generative model. They also introduce a loss function that penalizes deviation from a Directed Acyclic Graph (DAG) structure to enforce the learned causal representation.\",\n    \"Direct Inspiration\": {\n        \"b4\": 0.9,\n        \"b14\": 0.85,\n        \"b11\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.75,\n        \"b2\": 0.7,\n        \"b5\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.6,\n        \"b20\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning disentangled representations in an unsupervised manner by introducing a causal disentangled representation learning framework. The key innovation is integrating Linear Structural Equation Models (SEM) into the variational autoencoder framework to capture dependent latent factors. This approach challenges the traditional assumption that meaningful factors are independent and emphasizes the necessity of learning causal representations for better model interpretability and intervention.\",\n  \"Direct Inspiration\": [\"b4\", \"b11\", \"b14\", \"b20\"],\n  \"Indirect Inspiration\": [\"b2\", \"b5\", \"b19\", \"b25\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in unsupervised disentangled representation learning, particularly focusing on the limitations of existing methods that assume independent factors in real-world data. It proposes a novel causal disentangled representation learning framework that incorporates Linear Structural Equation Models (SEM) into the variational autoencoder framework to recover dependent factors. The paper introduces a loss function to enforce a Directed Acyclic Graph (DAG) structure on the learned latent code and analyzes the identifiability of the proposed generative model.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.9,\n    \"b2\": 0.8,\n    \"b19\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b23\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in unsupervised disentangled representation learning, particularly the assumption that real-world observations are generated by independent factors. It proposes a novel framework combining Linear Structural Equation Models (SEM) with Variational Autoencoders (VAE) to learn causal representations. This approach aims to improve the identifiability and practical applicability of disentangled representations by introducing causal relationships among latent factors.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b14\": 0.9,\n    \"b4\": 0.8,\n    \"b2\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b19\": 0.65,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.55,\n    \"b25\": 0.5,\n    \"b35\": 0.5,\n    \"b18\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in unsupervised disentangled representation learning, particularly the assumption that real-world observations are generated by independent factors. Instead, the authors propose a novel framework that incorporates causal relationships using Linear Structural Equation Models (SEM) into the Variational Autoencoder (VAE) framework. They aim to learn causal representations that can be intervened upon to generate artificial data. The paper highlights contributions such as a new framework for causal disentanglement learning, theoretical analysis on model identifiability, and empirical validation through experiments.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b14\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b2\": 0.8,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b5\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```"], "5e2d653a3a55acc8374367fd": ["```json\n{\n  \"Summary\": \"The paper addresses the simultaneous recognition of person names and publications from academic homepages by leveraging the correlation and position patterns between these two tasks. It proposes a novel Position-aware Alternating Memory (PAM) network, which consists of an alternatingly updated memory module and a position-aware memory module.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b12\": 0.9,\n    \"b16\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b30\": 0.7,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.4,\n    \"b26\": 0.4,\n    \"b18\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges are recognizing person names and publications from academic homepages simultaneously, leveraging their correlation and position patterns in the text.\",\n    \"proposed_algorithm\": \"The paper proposes a Position-aware Alternating Memory (PAM) network, which includes an alternatingly updated memory module to capture correlations between tasks and a position-aware memory module to utilize position patterns in the text.\"\n  },\n  \"Direct Inspiration\": [\"b5\", \"b12\"],\n  \"Indirect Inspiration\": [\"b0\", \"b30\"],\n  \"Other Inspiration\": [\"b16\", \"b25\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the simultaneous recognition of person names and publications from academic homepages by leveraging their correlation and position patterns. It introduces a novel Position-aware Alternating Memory (PAM) network, which includes two main modules: an alternatingly updated memory (AM) module to enhance intermediate interaction and a position-aware memory (PM) module to capture position patterns. The proposed method outperforms state-of-the-art models in both tasks.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b12\": 0.9,\n    \"b30\": 0.8,\n    \"b0\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b21\": 0.6,\n    \"b26\": 0.6,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b11\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the simultaneous recognition of person names and publications from academic homepages by proposing a novel Position-aware Alternating Memory (PAM) network. The primary challenges include capturing the strong correlation between person names and publications and utilizing the position patterns of names and publications in academic homepages. The PAM network consists of an alternatingly updated memory (AM) module to enhance intermediate interaction and a position-aware memory (PM) module to capture position patterns.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the task of recognizing person names and publications from academic homepages by leveraging their correlation and position patterns. It proposes a novel Position-aware Alternating Memory (PAM) network that consists of two main modules: an alternatingly updated memory module and a position-aware memory module. The experimental results demonstrate that PAM outperforms the state-of-the-art models on both tasks.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.95,\n    \"b5\": 0.9,\n    \"b12\": 0.9,\n    \"b30\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b16\": 0.8,\n    \"b21\": 0.75,\n    \"b25\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b10\": 0.7,\n    \"b18\": 0.65,\n    \"b20\": 0.65,\n    \"b26\": 0.75,\n    \"b31\": 0.6\n  }\n}\n```"], "5d3ed25a275ded87f97dea9b": ["```json\n{\n    \"Summary\": \"The paper addresses three primary challenges for mining algorithm roadmaps in scientific literature: label scarcity, entity recognition, and relation extraction. It proposes a framework that includes extracting abbreviations as algorithm candidates, leveraging weak supervision to create training data, and using a Cross-sentence Attention Network for comparative relation extraction. The aim is to construct a graph that shows the development and comparative relations of algorithms, which can help analysts and researchers understand the evolution and dynamics of different algorithms.\",\n    \"Direct Inspiration\": [\"b17\", \"b18\", \"b24\", \"b36\"],\n    \"Indirect Inspiration\": [\"b10\", \"b13\", \"b31\", \"b41\"],\n    \"Other Inspiration\": [\"b0\", \"b3\", \"b7\", \"b21\", \"b22\", \"b25\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of mining algorithm roadmaps from scientific publications, focusing on label scarcity, entity recognition, and relation extraction. It proposes a framework utilizing weak supervision and a novel method called CAN-TOR for cross-sentence comparative relation extraction.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b13\": 0.7,\n    \"b31\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses three challenges: label scarcity, entity recognition, and relation extraction for mining algorithm roadmaps in scientific literature. The proposed framework, which includes the Cross-sentence Attention NeTwork for cOmparative Relation (CAN-TOR), extracts comparative algorithms, handles abbreviation ambiguity, and constructs algorithm roadmaps with time and frequency information.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b31\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b21\": 0.6,\n    \"b24\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three primary challenges: label scarcity, entity ambiguity, and relation extraction in mining algorithm roadmaps from scientific publications. The proposed framework leverages weak supervision, a novel relation extraction method (CAN-TOR), and the construction of directed acyclic graphs to tackle these challenges.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b36\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three main challenges in mining algorithm roadmaps from scientific literature: label scarcity, entity ambiguity, and relation extraction across sentences. To tackle these, the authors propose a framework that extracts algorithm abbreviations, leverages weak supervision to create training data, and employs a Cross-sentence Attention NeTwork for cOmparative Relation (CAN-TOR) to predict relationships and construct a directed graph of algorithms.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b18\": 0.9,\n    \"b24\": 0.9,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b13\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b25\": 0.7,\n    \"b31\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b26\": 0.5,\n    \"b29\": 0.5,\n    \"b30\": 0.5,\n    \"b35\": 0.5,\n    \"b39\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```"], "5ec49a639fced0a24b4de8d2": ["```json\n{\n  \"Summary\": \"The paper introduces S2ORC, a large, publicly-available collection of 81.1M academic papers with structured, machine-readable full text. The primary challenges addressed include metadata aggregation, identifying open access publications, clustering papers, and extracting and cleaning full text and bibliometric annotations. The proposed methods involve processing PDFs and LATEX sources, selecting canonical metadata, filtering paper clusters, and linking bibliographies to papers. The construction of S2ORC is inspired by previous works, particularly in handling LATEX sources and extracting structured information.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b32\": 0.85,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.75,\n    \"b18\": 0.7,\n    \"b8\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents S2ORC, a large publicly-available collection of 81.1M academic papers with metadata, abstracts, and structured full text. It addresses challenges in paper metadata aggregation, open access identification, and paper clustering. The corpus aims to overcome limitations of existing corpora by providing more structured full-text papers and covering diverse academic disciplines.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b13\": 0.7,\n    \"b26\": 0.7,\n    \"b53\": 0.7,\n    \"b37\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Aggregating paper metadata from diverse sources\",\n      \"Identifying open access publications\",\n      \"Clustering papers effectively\",\n      \"Extracting and cleaning full text and bibliometric annotations\"\n    ],\n    \"Inspirations\": [\n      \"Creating a large, diverse, publicly-available corpus of academic papers\",\n      \"Providing structured, machine-readable full text for open access papers\",\n      \"Improving upon limitations of existing corpora such as AAN, PubMed Central, and RefSeer\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b44\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b45\": 0.8,\n    \"b47\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.7,\n    \"b49\": 0.7,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces the Semantic Scholar Open Research Corpus (S2ORC), addressing challenges in metadata aggregation, identifying open access publications, and clustering papers. The corpus aims to provide a large, diverse, and structured collection of academic papers, overcoming limitations of existing corpora. Key methodologies include PDF and LATEX processing for metadata extraction, filtering paper clusters, and linking bibliographies to papers.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b32\": 0.7,\n    \"b5\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.5,\n    \"b47\": 0.5,\n    \"b21\": 0.4,\n    \"b26\": 0.4,\n    \"b13\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the construction of a large, comprehensive academic paper corpus (S2ORC) that addresses the limitations of existing corpora, such as small size, domain specificity, and lack of usable full text. The proposed algorithm involves processing PDFs and LATEX sources to derive metadata, clean text, inline citations, and references, and then clustering papers while resolving bibliography links. The inspirations for this work come from previous efforts in creating bibliometrically-enhanced academic corpora and techniques used for parsing and metadata extraction.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b49\": 0.7,\n    \"b13\": 0.6,\n    \"b5\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5\n  }\n}\n```"], "5a73cb4d17c44a0b3035672d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of person re-identification (re-id) across non-overlapping camera views. The core contribution is a novel Early Active Learning with Pairwise Constraint (EALPC) algorithm that enhances re-id by considering pairwise relationships and robustness to outliers using the 2,1-norm.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.95,\n    \"b8\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.75,\n    \"b16\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.60,\n    \"b25\": 0.55,\n    \"b29\": 0.50\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Identifying a person from camera shots across pairs of non-overlapping camera views\",\n      \"Variation of appearance changes across camera views\",\n      \"Labor-intensive labeling of training data for re-id\",\n      \"High labeling costs due to the requirement of pairwise labeled data\"\n    ],\n    \"Inspirations\": [\n      \"Active learning methods for sample selection\",\n      \"Early active learning for limited labeled samples\",\n      \"Applying early active learning with pairwise constraint\",\n      \"Introducing 2,1-norm to improve robustness and suppress outliers\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b8\",\n    \"b17\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b6\",\n    \"b19\",\n    \"b26\"\n  ],\n  \"Other Inspiration\": [\n    \"b16\",\n    \"b25\",\n    \"b29\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in person re-identification (re-id), particularly the issues related to appearance variations across non-overlapping camera views and the high labeling costs for training data. The proposed solution is a novel Early Active Learning with Pairwise Constraint (EALPC) algorithm, which aims to enhance learning performance by selecting representative samples for labeling in the early stages of re-id. The paper introduces the 2,1-norm to improve robustness and suppress outliers, and provides an efficient optimization algorithm that guarantees global convergence.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper relate to the difficulty of person re-identification (re-id) across non-overlapping camera views due to variations in appearance such as body pose, view angle, occlusion, and illumination. The proposed algorithm, Early Active Learning with Pairwise Constraint (EALPC), aims to enhance the learning performance in re-id applications by selecting the most representative samples for labeling. Novel contributions include the introduction of a pairwise constraint and the use of the 2,1-norm for robustness.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b16\": 0.7,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of person re-identification (re-id) across non-overlapping camera views, focusing on the difficulty of labeling data for training supervised models. The authors propose an Early Active Learning with Pairwise Constraint (EALPC) algorithm to select the most representative samples for labeling, enhancing the learning performance in re-id applications. They introduce the 2,1-norm to improve robustness and suppress outliers, and propose an optimization algorithm that guarantees global convergence.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b16\": 0.6,\n    \"b25\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```"], "5f64211c9e795e0286c313a2": ["```json\n{\n    \"Summary\": \"This paper addresses the issue of label modification in deep neural networks, specifically targeting the limitations of existing Output Regularisation (OR) and Label Correction (LC) methods. It introduces a novel method, Progressive Self Label Correction (ProSelfLC), which progressively and adaptively modifies the target as training progresses. The main challenges include the naive penalisation of confident outputs in OR methods, the reliance on accurate auxiliary models in Non-self LC, and the fixed trust score in existing Self LC methods. ProSelfLC aims to enhance the similarity structure information over training classes and correct the semantic classes of noisy label distributions.\",\n    \"Direct Inspiration\": {\n        \"b41\": 1,\n        \"b28\": 1,\n        \"b32\": 1,\n        \"b54\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b8\": 0.8,\n        \"b9\": 0.8,\n        \"b13\": 0.8,\n        \"b37\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.7,\n        \"b22\": 0.7,\n        \"b34\": 0.7,\n        \"b42\": 0.7,\n        \"b21\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of label correction in deep neural networks, particularly focusing on self label correction (Self LC) methods. The novel method proposed, Progressive Self Label Correction (ProSelfLC), progressively and adaptively modifies the target as training goes on, leveraging the model's own knowledge.\",\n  \"Direct Inspiration\": {\n    \"b54\": 0.9,\n    \"b34\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.6,\n    \"b32\": 0.6,\n    \"b16\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.4,\n    \"b9\": 0.4,\n    \"b37\": 0.3,\n    \"b13\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies challenges with existing Output Regularisation (OR) and Label Correction (LC) methods, particularly focusing on Self LC. It introduces a novel method called Progressive Self Label Correction (ProSelfLC), which adapts the trust score of a learner dynamically based on training time and prediction confidence. The key contributions include a theoretical study on target modification methods, defense of entropy minimisation, and extensive experiments demonstrating the effectiveness of ProSelfLC.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b8\": 1,\n    \"b9\": 1,\n    \"b13\": 1,\n    \"b21\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.9,\n    \"b28\": 0.9,\n    \"b32\": 0.9,\n    \"b5\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.8,\n    \"b22\": 0.8,\n    \"b34\": 0.8,\n    \"b42\": 0.8,\n    \"b54\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the drawbacks of existing label correction (LC) methods, particularly in self-label correction (Self LC), where the challenge is determining the trustworthiness of a learner's knowledge over time. The proposed Progressive Self Label Correction (ProSelfLC) method adapts the trust score dynamically during training based on the model's learning progress and prediction confidence. The main contributions include a theoretical study on target modification methods and extensive experiments defending the entropy minimization principle while demonstrating the effectiveness of ProSelfLC.\",\n  \"Direct Inspiration\": [\"b54\"],\n  \"Indirect Inspiration\": [\"b22\", \"b34\", \"b42\", \"b16\"],\n  \"Other Inspiration\": [\"b2\", \"b8\", \"b9\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing target modification methods, particularly focusing on Self Label Correction (Self LC). The authors propose a novel approach called Progressive Self Label Correction (ProSelfLC) which adapts and modifies the learning targets progressively based on training time and prediction confidence. The paper theoretically and empirically defends entropy minimization against confidence penalty, and highlights the effectiveness of ProSelfLC in both clean and noisy settings.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b22\": 0.9,\n    \"b34\": 0.9,\n    \"b42\": 0.9,\n    \"b54\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b28\": 0.7,\n    \"b32\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b21\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```"], "5f3f917891e011d38f9242d9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting fraudulent activities on multi-relation graphs using Graph Neural Networks (GNNs), specifically focusing on overcoming feature and relation camouflage by fraudsters. The proposed model, CARE-GNN, introduces three novel neural modules: a label-aware similarity measure, a similarity-aware neighbor selector optimized using reinforcement learning (RL), and a relation-aware neighbor aggregator, to enhance the GNNs' robustness against camouflage behaviors.\",\n  \"Direct Inspiration\": [\"b3\", \"b7\", \"b12\", \"b14\", \"b24\", \"b40\"],\n  \"Indirect Inspiration\": [\"b9\", \"b15\", \"b18\", \"b43\", \"b48\"],\n  \"Other Inspiration\": [\"b19\", \"b22\", \"b23\", \"b33\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting camouflaged fraudsters in graph-based systems, particularly in the context of Graph Neural Networks (GNNs). The authors propose a novel model called CAmouflage REsistant Graph Neural Network (CARE-GNN) to tackle feature and relation camouflage by fraudsters. This involves developing three neural modules: a label-aware similarity measure, a similarity-aware neighbor selector optimized using reinforcement learning, and a relation-aware neighbor aggregator.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.95,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.85,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.75,\n    \"b14\": 0.7,\n    \"b9\": 0.65,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the detection of fraudulent activities in Internet services using Graph Neural Networks (GNNs). The paper proposes a novel model called CARE-GNN, which includes three neural modules (label-aware similarity measure, similarity-aware neighbor selector, and relation-aware neighbor aggregator) to enhance GNNs against camouflaged fraudsters. The model aims to improve GNN performance on graphs with noisy nodes and edges by addressing feature and relation camouflage behaviors of fraudsters.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b12\": 0.85,\n    \"b24\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b9\": 0.7,\n    \"b14\": 0.7,\n    \"b43\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b16\": 0.6,\n    \"b33\": 0.6,\n    \"b19\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting camouflaged fraudsters using GNNs by proposing the CARE-GNN model, which includes three neural modules: a label-aware similarity measure, a similarity-aware neighbor selector optimized with reinforcement learning, and a relation-aware neighbor aggregator. These modules aim to enhance GNN performance by effectively filtering camouflaged neighbors and aggregating relevant neighbor information.\",\n  \"Direct Inspiration\": [\"b3\", \"b24\"],\n  \"Indirect Inspiration\": [\"b7\", \"b14\", \"b40\"],\n  \"Other Inspiration\": [\"b19\", \"b5\", \"b22\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper are detecting camouflaged fraudsters using GNNs. The authors highlight two types of camouflage: feature camouflage and relation camouflage.\",\n    \"Inspirations\": \"The paper introduces three neural modules: label-aware similarity measure, similarity-aware neighbor selector optimized with reinforcement learning, and relation-aware neighbor aggregator.\"\n  },\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b24\": 0.9,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b14\": 0.7,\n    \"b9\": 0.6,\n    \"b43\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b15\": 0.5,\n    \"b48\": 0.5\n  }\n}\n```"], "5d4d46fb3a55acff992fde53": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting anomalous edges in dynamic graphs, particularly within the context of e-commerce sites. The authors propose a novel semi-supervised learning framework, AddGraph, which extends GCN with temporal information using GRU and an attention-based model. This approach aims to overcome limitations in existing methods, such as the inability to capture long-term and short-term patterns and the challenge of insufficient labeled data.\",\n  \"Direct Inspiration\": [\"b1\", \"b2\", \"b3\"],\n  \"Indirect Inspiration\": [\"b4\", \"b7\"],\n  \"Other Inspiration\": [\"Hooi et al., 2016\", \"Sricharan and Das, 2014\", \"Yu et al., 2018\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting anomalous edges in dynamic graphs, particularly in e-commerce and network attack scenarios. It proposes AddGraph, a semi-supervised learning framework leveraging an extended temporal GCN with an attention-based GRU, selective negative sampling, and margin loss to process structural, content, and temporal features.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1,\n    \"b4\": 1,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"Sricharan and Das, 2014\": 0.8,\n    \"Yu et al., 2018\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"Hooi et al., 2016\": 0.7,\n    \"Kipf and Welling, 2017\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting anomalous edges in dynamic graphs, particularly in contexts such as e-commerce sites. The proposed solution, AddGraph, extends the Graph Convolutional Network (GCN) by incorporating temporal information using Gated Recurrent Units (GRU) with a contextual attention-based model. AddGraph also introduces selective negative sampling and margin loss to improve the detection accuracy in the presence of insufficient labeled data.\",\n  \"Direct Inspiration\": [\"b1\", \"b2\"],\n  \"Indirect Inspiration\": [\"b4\", \"b5\", \"b7\"],\n  \"Other Inspiration\": [\"b3\", \"b6\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of detecting anomalous edges in dynamic graphs, such as those found in e-commerce sites, by extending the GCN model to incorporate temporal information using GRU with a contextual attention-based model. The proposed AddGraph framework aims to combine hidden states for long-term behavior patterns and short-term patterns of nodes while addressing the issue of insufficient labeled data through selective negative sampling and margin loss.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1,\n        \"b2\": 0.9,\n        \"Sricharan and Das, 2014\": 0.95\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b7\": 0.85,\n        \"b3\": 0.75,\n        \"Yu et al., 2018\": 0.8,\n        \"Kipf and Welling, 2017\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"Hooi et al., 2016\": 0.7,\n        \"Wang et al., 2014\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting anomalous edges in dynamic graphs, particularly in the context of e-commerce sites. The proposed framework, AddGraph, extends the traditional Graph Convolutional Network (GCN) by incorporating temporal information using a Gated Recurrent Unit (GRU) with contextual attention. The novel approach also includes a selective negative sampling strategy and margin loss for handling insufficient labeled anomaly data.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b6\": 0.5\n  }\n}\n```"], "5550411c45ce0a409eb3897f": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge outlined in the paper is the difficulty of neural machine translation models, specifically encoder-decoder models, to cope with long sentences due to the need to compress all necessary information into a fixed-length vector.\",\n    \"Inspirations\": \"The authors propose an extension to the encoder-decoder model that learns to align and translate jointly, using a sequence of vectors instead of a single fixed-length vector.\"\n  },\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of neural machine translation (NMT) with a focus on tackling the issue of long sentences in the encoder-decoder model. The authors propose a novel architecture that jointly learns to align and translate, introducing a mechanism of attention that allows the model to focus on relevant parts of the input sentence, thereby improving translation performance, especially for long sentences.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b27\": 1,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of neural machine translation, where traditional encoder-decoder models struggle with long sentences due to the fixed-length vector representation. The authors propose a novel architecture that integrates an attention mechanism to dynamically focus on different parts of the input sentence, thereby improving translation performance, especially for longer sentences.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the difficulty of neural machine translation models to handle long sentences due to the fixed-length vector representation in the encoder-decoder approach. The proposed solution is an extension of the encoder-decoder model that learns to align and translate jointly, using a bidirectional RNN as an encoder and a decoder with an attention mechanism. This approach allows the model to better manage long sentences and significantly improves translation performance.\",\n    \"Direct Inspiration\": [\"b6\", \"b27\", \"b7\"],\n    \"Indirect Inspiration\": [\"b14\", \"b17\"],\n    \"Other Inspiration\": [\"b24\", \"b13\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of neural machine translation, specifically the limitations of the basic encoder-decoder approach in handling long sentences. The proposed solution is an extension of the encoder-decoder model that incorporates a mechanism for joint alignment and translation, allowing the model to focus on relevant parts of the source sentence rather than compressing all information into a fixed-length vector.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b17\": 0.8,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b19\": 0.5\n  }\n}\n```"], "5ee8986891e011e66831c452": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is enabling Graph Neural Networks (GNNs) to perform meta-learning, particularly in scenarios where rapid learning from a few labeled nodes or edges is required. The paper introduces G-META, a novel approach that represents nodes with local subgraphs to train GNNs to meta-learn, contrasting with existing methods that use entire graphs. This approach aims to solve problems of scalability, generality, and effective learning in few-shot settings by leveraging local subgraphs for better structure and feature representation.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.95,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.85,\n    \"b45\": 0.8,\n    \"b49\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b51\": 0.65,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the need for rapid learning from only a few labeled nodes or edges in a graph, known as meta-learning. The proposed algorithm, G-META, addresses this by representing every node with a local subgraph and using these subgraphs to train Graph Neural Networks (GNNs) to meta-learn. This approach contrasts with current methods that are trained to meta-learn on entire graphs. G-META theoretically and empirically demonstrates the advantage of focusing on local subgraphs, allowing for effective feature propagation and label smoothing within a GNN.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b49\": 0.8,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b10\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of meta-learning on graphs, specifically focusing on learning from a few labeled nodes or edges and rapidly adapting to new, unseen graphs and labels. The proposed algorithm, G-META, represents nodes with local subgraphs to train GNNs to meta-learn, contrasting with existing methods that utilize entire graphs. This approach is theoretically justified and empirically shown to outperform other methods in few-shot learning settings by preserving local structural information and enabling effective feature propagation and label smoothing.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b45\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b49\": 0.6,\n    \"b51\": 0.5,\n    \"b2\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the development of a scalable and general approach to meta-learning on graphs, particularly for few-shot learning tasks where only a few labeled nodes or edges are available. The proposed algorithm, G-META, represents nodes with local subgraphs and uses these subgraphs to train Graph Neural Networks (GNNs) to meta-learn, enabling rapid adaptation to new graphs and labels. Theoretical analysis and empirical results demonstrate that G-META outperforms existing methods by capturing finer local structures and effectively propagating features and labels within a GNN.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b45\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.75,\n    \"b17\": 0.75,\n    \"b40\": 0.7,\n    \"b49\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of meta-learning on graphs, focusing on rapidly learning from a few labeled nodes or edges. It introduces G-META, which uses local subgraphs to represent nodes and trains GNNs to meta-learn, overcoming limitations of previous methods that rely on entire graphs.\",\n  \"Direct Inspiration\": {\n    \"b45\": 0.9,\n    \"b49\": 0.9,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b42\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b51\": 0.5\n  }\n}\n```"], "5f02f17c91e011ee5e0258c8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling Graph Neural Networks (GNNs) to large graphs prevalent in real-world problems. The primary bottleneck identified is the expensive recursive message-passing procedure. The proposed solution, PPRGo, leverages approximate personalized PageRank to avoid costly power iterations during training and achieves scalability through pre-computation and distributed training.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b15\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b35\": 0.7,\n    \"b53\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces PPRGo, a scalable GNN model based on approximate personalized PageRank, addressing the challenges of scaling GNNs to large graphs by avoiding expensive message-passing and power iterations. The model pre-computes personalized PageRank vectors to maintain the influence of relevant nodes and supports efficient training and inference in distributed environments.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b13\": 1.0,\n    \"b15\": 1.0,\n    \"b26\": 0.9,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b22\": 0.7,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b53\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces PPRGo, a Graph Neural Network (GNN) model that scales to large graphs by using an adapted propagation scheme based on approximate personalized PageRank. This model addresses the scalability bottlenecks of traditional GNNs, especially the expensive recursive message-passing procedure, by pre-computing sparse personalized PageRank vectors and leveraging their strong localization properties.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b32\": 0.85,\n    \"b22\": 0.8,\n    \"b35\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b15\": 0.7,\n    \"b27\": 0.65,\n    \"b26\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b53\": 0.5,\n    \"b3\": 0.45,\n    \"b18\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the scalability of Graph Neural Networks (GNNs) to large-scale graphs. The proposed algorithm, PPRGo, aims to scale GNNs efficiently by using an adapted propagation scheme based on approximate personalized PageRank. This approach eliminates the need for expensive power iterations during training and leverages sparse pre-computed approximations for efficient multi-hop neighborhood aggregation.\",\n  \"Direct Inspiration\": [\"b32\", \"b4\", \"b22\", \"b35\"],\n  \"Indirect Inspiration\": [\"b12\", \"b13\", \"b15\", \"b26\"],\n  \"Other Inspiration\": [\"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the scalability issues of Graph Neural Networks (GNNs) in large-scale graphs. It proposes the PPRGo model, which leverages approximate personalized PageRank to bypass the computationally intensive message-passing steps used in traditional GNNs. The primary challenge tackled is the exponential growth in neighborhood size and associated IO overhead in large graphs. The model is inspired by previous works that utilize personalized PageRank for information propagation without explicit message-passing.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b22\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b26\": 0.7,\n    \"b27\": 0.7\n  }\n}\n```"], "599c797d601a182cd2643e8a": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting missing information in knowledge bases by proposing a novel approach involving relational graph convolutional networks (R-GCNs). The main contributions include applying the GCN framework to relational data for link prediction and entity classification, developing parameter sharing techniques, and showing the benefit of enriching factorization models with an encoder model.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b30\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.65,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting missing information in knowledge bases, focusing on two key tasks: link prediction and entity classification. The proposed algorithm involves a relational graph convolutional network (R-GCN) that processes multi-relational data and applies it to both tasks. It combines an encoder model using R-GCN with a tensor factorization method for improved performance.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b1\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the incompleteness of large knowledge bases and the need for improved methods in link prediction and entity classification within these knowledge bases. The proposed algorithm centers on the use of relational graph convolutional networks (R-GCNs) to encode entities in relational graphs, applying these to both entity classification and link prediction tasks. The paper introduces techniques such as parameter sharing and sparsity constraints to handle multi-relational data, and it demonstrates the effectiveness of combining R-GCNs with factorization models like DistMult.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b15\": 0.7,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5,\n    \"b18\": 0.4,\n    \"b27\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting missing information in knowledge bases through statistical relational learning (SRL). It proposes an encoder model for entities in relational graphs, using relational graph convolutional networks (R-GCNs) for entity classification and link prediction. The novel contributions include the application of the GCN framework to relational data, techniques for parameter sharing and sparsity constraints, and the integration of R-GCNs with the DistMult factorization model for improved performance.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b16\", \"b30\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b8\", \"b10\", \"b15\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b1\", \"b20\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of incomplete knowledge bases by proposing an encoder model for entities in relational graphs, specifically using Relational Graph Convolutional Networks (R-GCNs) for tasks like link prediction and entity classification. The main inspiration comes from the need to improve statistical relational learning (SRL) and the limitations of existing factorization methods.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b30\": 1.0,\n    \"b8\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b15\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```"], "5f92ba1691e011edb3573ba0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying Transformer architectures, which have shown success in NLP, to the field of computer vision, which has traditionally relied on convolutional neural networks (CNNs). The authors propose the Vision Transformer (ViT), which splits images into patches and processes them as tokens in a Transformer model. The approach is shown to achieve state-of-the-art results when trained on large datasets, overcoming the limitations of CNNs and previous attempts at incorporating self-attention in image processing.\",\n  \"Direct Inspiration\": [\"b43\", \"b13\"],\n  \"Indirect Inspiration\": [\"b36\", \"b44\", \"b7\"],\n  \"Other Inspiration\": [\"b22\", \"b51\", \"b38\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying self-attention-based architectures (Transformers) to image recognition tasks, which have traditionally been dominated by convolutional neural networks (CNNs). The authors propose the Vision Transformer (ViT), which applies a standard Transformer architecture directly to images by splitting them into patches and treating these patches as tokens. The main inspiration comes from the success of Transformers in natural language processing (NLP). Key contributions include demonstrating that large-scale training with Transformers can outperform state-of-the-art CNN models on image recognition benchmarks when pre-trained on large datasets.\",\n  \"Direct Inspiration\": [\"b43\", \"b13\", \"b36\", \"b22\", \"b51\"],\n  \"Indirect Inspiration\": [\"b6\", \"b25\", \"b24\", \"b15\", \"b29\", \"b47\", \"b44\", \"b32\"],\n  \"Other Inspiration\": [\"b7\", \"b10\", \"b38\", \"b14\", \"b12\", \"b4\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying Transformers, particularly the Vision Transformer (ViT), to image recognition tasks. Inspired by the success of Transformers in NLP, the authors investigate whether similar scaling benefits can be realized in computer vision. The primary innovation is the adaptation of the Transformer architecture to handle image data by splitting images into patches and treating these patches as tokens. This method is shown to work effectively when pre-trained on large datasets, achieving state-of-the-art results on multiple benchmarks.\",\n  \"Direct Inspiration\": [\"b43\", \"b13\"],\n  \"Indirect Inspiration\": [\"b22\", \"b29\", \"b51\"],\n  \"Other Inspiration\": [\"b36\", \"b44\", \"b6\", \"b47\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying Transformers, which have shown great success in NLP, to the domain of computer vision. The primary inspiration comes from the observed success of Transformer models in NLP tasks. The authors propose the Vision Transformer (ViT) model, which applies a standard Transformer architecture directly to image patches, treating them as sequences of tokens similar to words in NLP. The paper explores the effectiveness of this approach when pre-trained on large datasets and evaluates its performance on various benchmark tasks.\",\n  \"Direct Inspiration\": [\"b43\"],\n  \"Indirect Inspiration\": [\"b13\", \"b6\", \"b36\", \"b44\"],\n  \"Other Inspiration\": [\"b29\", \"b51\", \"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the application of Transformer architectures, traditionally successful in NLP, to the domain of computer vision, specifically for large-scale image recognition. The proposed algorithm, Vision Transformer (ViT), involves splitting images into patches and treating them as sequences of tokens similar to words in NLP. The paper takes inspiration from the success of Transformer models in NLP and aims to achieve state-of-the-art results in image recognition through large-scale training.\",\n  \"Direct Inspiration\": {\n    \"b43\": 1.0,\n    \"b13\": 0.9,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.7,\n    \"b6\": 0.65,\n    \"b44\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.5,\n    \"b51\": 0.45,\n    \"b25\": 0.4\n  }\n}\n```"], "5eccb534e06a4c1b26a838ac": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of manual network design and neural architecture search (NAS). It proposes a new network design paradigm called RegNet, which combines the interpretability of manual design with the efficiency of NAS. The main contributions include designing network design spaces, progressively refining these spaces, and discovering general design principles that apply to model populations.\",\n  \"Direct Inspiration\": [\"b20\"],\n  \"Indirect Inspiration\": [\"b7\", \"b28\", \"b30\"],\n  \"Other Inspiration\": [\"b2\", \"b25\", \"b12\", \"b33\", \"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of manually designing well-optimized neural networks, proposing a new paradigm that combines manual design and neural architecture search (NAS). The authors introduce the concept of design spaces and focus on progressively simplifying these spaces to improve model performance and interpretability. The RegNet design space is presented as a key contribution, offering simpler, more interpretable models that generalize well across settings.\",\n  \"Direct Inspiration\": [\"b20\"],\n  \"Indirect Inspiration\": [\"b7\", \"b30\"],\n  \"Other Inspiration\": [\"b28\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the difficulty of manually designing optimized neural network architectures as the number of design choices increases. The proposed solution is a new network design paradigm called 'design space design,' which combines the advantages of manual design and neural architecture search (NAS). The goal is to design a space containing a population of network models, known as RegNet, that are simpler, more interpretable, and generalize better across different settings. The key inspiration is derived from previous methodologies, particularly the concept of analyzing and refining design spaces introduced by Radosavovic et al. [b20].\",\n  \"Direct Inspiration\": [\"b20\"],\n  \"Indirect Inspiration\": [\"b7\", \"b25\", \"b30\", \"b28\"],\n  \"Other Inspiration\": [\"b2\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing neural network architectures, shifting from individual model optimization to a population-based design space approach. The proposed RegNet design space aims to simplify model structures, enhance interpretability, and generalize across different settings.\",\n  \"Direct Inspiration\": [\"b20\"],\n  \"Indirect Inspiration\": [\"b28\", \"b7\", \"b30\"],\n  \"Other Inspiration\": [\"b2\", \"b12\", \"b25\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of manual network design and the limitations of neural architecture search (NAS) by proposing a new network design paradigm called design space design. The main contribution is the introduction of RegNet, a simplified and interpretable design space for neural networks that generalizes well across different settings.\",\n  \"Direct Inspiration\": [\"b20\"],\n  \"Indirect Inspiration\": [\"b7\", \"b30\", \"b28\"],\n  \"Other Inspiration\": [\"b25\", \"b12\", \"b33\", \"b17\"]\n}\n```"], "5ea2b8bf91e01167f5a89d89": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of scaling deep learning on graphs to web-scale datasets. The authors propose a scalable graph neural network architecture, called SIGN, which generalizes GCN, S-GCN, ChebNet, and related methods. The proposed model combines graph convolutional filters of different sizes, inspired by the inception module, allowing efficient precomputation for fast training and inference. The architecture is validated experimentally, showing comparable performance to state-of-the-art models while being significantly faster.\",\n    \"Direct Inspiration\": {\n        \"b46\": 1.0,\n        \"b40\": 0.9,\n        \"b43\": 0.8,\n        \"b22\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.6,\n        \"b25\": 0.5,\n        \"b24\": 0.4,\n        \"b16\": 0.3,\n        \"b35\": 0.3\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.2,\n        \"b27\": 0.1,\n        \"b28\": 0.1,\n        \"b19\": 0.1\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling graph neural networks (GNNs) to large-scale graphs. It proposes a novel architecture, Scalable Inception Graph Network (SIGN), which combines graph convolutional filters of different sizes and is inspired by the inception module used in classic CNN architectures. The architecture aims to achieve efficient precomputation and fast training/inference, while maintaining performance comparable to state-of-the-art models.\",\n  \"Direct Inspiration\": {\n    \"b46\": 1,\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b25\": 0.6,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are the scalability issues of graph neural networks (GNNs) when applied to large-scale graphs. The proposed algorithm, Scalable Inception Graph Network (SIGN), addresses these challenges by generalizing GCN, S-GCN, ChebNet, and related methods into a scalable architecture that combines graph convolutional filters of different sizes, allowing for efficient precomputation and significantly faster training and inference.\",\n    \"Direct Inspiration\": {\n        \"b46\": 1.0,\n        \"b40\": 0.9,\n        \"b43\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b22\": 0.7,\n        \"b11\": 0.6,\n        \"b25\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.5,\n        \"b30\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling graph neural networks (GNNs) to extremely large graphs, which is critical for their application in industrial settings. The proposed algorithm, SIGN (Scalable Inception Graph Neural Networks), combines graph convolutional filters of different sizes to allow efficient precomputation and significantly faster training and inference. The inspiration comes from S-GCN's efficiency and GCN's low-pass filter learning.\",\n  \"Direct Inspiration\": [\"b46\", \"b40\"],\n  \"Indirect Inspiration\": [\"b22\", \"b43\"],\n  \"Other Inspiration\": [\"b11\", \"b25\", \"b30\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper outlines the primary challenge of scaling graph neural networks (GNNs) to extremely large graphs, which involves significant computational and memory complexity. The proposed algorithm, SIGN (Scalable Inception Graph Network), is a scalable GNN architecture that generalizes existing methods like GCN, S-GCN, and ChebNet, and is analogous to the inception module in CNNs. SIGN combines graph convolutional filters of different sizes to allow efficient precomputation, resulting in significantly faster training and inference.\",\n  \"Direct Inspiration\": [\"b46\", \"b40\"],\n  \"Indirect Inspiration\": [\"b22\", \"b43\"],\n  \"Other Inspiration\": [\"b11\", \"b30\"]\n}\n```"], "5ede0553e06a4c1b26a83f63": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of over-smoothing in Graph Convolutional Networks (GCNs) and proposes the GCNII model, which integrates initial residual connections and identity mapping to extend GCNs to deeper architectures without performance degradation.\",\n  \"Direct Inspiration\": [\"b16\", \"b17\"],\n  \"Indirect Inspiration\": [\"b33\", \"b41\"],\n  \"Other Inspiration\": [\"b13\", \"b43\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the over-smoothing problem in Graph Convolutional Networks (GCNs), which limits their performance when stacking multiple layers. The proposed solution, GCNII, introduces Initial Residual and Identity Mapping techniques to extend the depth of GCNs while preventing over-smoothing and achieving state-of-the-art performance.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b17\": 1.0,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b33\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.6,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of over-smoothing in Graph Convolutional Networks (GCNs) and proposes two techniques, initial residual connections and identity mapping, to extend GCNs to deep models effectively. The proposed methods aim to maintain performance as network depth increases and achieve state-of-the-art results in various tasks.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b17\": 1.0,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.8,\n    \"b43\": 0.8,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the over-smoothing problem in deep Graph Convolutional Networks (GCNs). The algorithm proposed by the authors is the Graph Convolutional Network via Initial Residual and Identity Mapping (GCNII), which addresses over-smoothing by incorporating initial residual connections and identity mapping into the GCN model, allowing it to maintain performance and achieve state-of-the-art results even with increased network depth.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b17\": 0.9,\n    \"b33\": 0.9,\n    \"b41\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b43\": 0.6,\n    \"b18\": 0.5,\n    \"b24\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.4,\n    \"b38\": 0.4,\n    \"b40\": 0.3,\n    \"b29\": 0.3,\n    \"b12\": 0.3,\n    \"b28\": 0.3,\n    \"b30\": 0.3,\n    \"b46\": 0.2,\n    \"b31\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of over-smoothing in deep Graph Convolutional Networks (GCNs) and proposes a new model, GCNII, which incorporates initial residual and identity mapping techniques to overcome this issue and improve the performance of deep GCNs.\",\n  \"Direct Inspiration\": [\"b17\", \"b33\", \"b41\"],\n  \"Indirect Inspiration\": [\"b13\", \"b16\"],\n  \"Other Inspiration\": [\"b38\", \"b43\"]\n}\n```"], "5cf48a34da56291d58296d51": ["```json\n{\n  \"Summary\": \"This paper addresses the challenges in embedding learning for Attributed Multiplex Heterogeneous Networks (AMHENs), which include handling multiplex edges, partial observations, and scalability. The proposed solution, GATNE, aims to capture both rich attributed information and multiplex topological structures from different node types.\",\n  \"Direct Inspiration\": [\"b33\", \"b6\", \"b30\"],\n  \"Indirect Inspiration\": [\"b42\", \"b10\", \"b18\"],\n  \"Other Inspiration\": [\"b21\", \"b29\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of embedding learning for Attributed Multiplex Heterogeneous Networks (AMHENs), which consist of various node types connected by multiple edge types and associated with diverse attributes. The proposed solution, GATNE, aims to capture rich attributed information and utilize multiplex topological structures for both transductive and inductive embeddings, addressing challenges such as multiplex edges, partial observations, and scalability.\",\n  \"Direct Inspiration\": [\"b9\", \"b42\"],\n  \"Indirect Inspiration\": [\"b6\", \"b30\", \"b33\"],\n  \"Other Inspiration\": [\"b15\", \"b14\", \"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of embedding learning for Attributed Multiplex Heterogeneous Networks (AMHENs) which include handling multiplex edges, dealing with partial observations, and achieving scalability. The novel approach proposed is GATNE, which aims to capture both rich attributed information and utilize multiplex topological structures from different node types.\",\n  \"Direct Inspiration\": {\n    \"b42\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.9,\n    \"b34\": 0.9,\n    \"b9\": 0.9,\n    \"b28\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7,\n    \"b6\": 0.7,\n    \"b30\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are handling attributed multiplex heterogeneous networks (AMHENs) with multiple types of nodes and edges and dealing with issues such as multiplex edges, partial observations, and scalability. The proposed algorithm, GATNE, aims to capture rich attributed information and utilize multiplex topological structures from different node types, supporting both transductive and inductive embeddings learning for AMHENs.\",\n  \"Direct Inspiration\": {\n    \"b42\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b34\": 0.8,\n    \"b9\": 0.8,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b30\": 0.7,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of embedding learning for Attributed Multiplex Heterogeneous Networks (AMHENs), which involve multiplex edges, partial observations, and scalability. The proposed GATNE framework aims to capture both rich attributed information and utilize multiplex topological structures from different node types. Key features include supporting both transductive and inductive embedding learning and developing efficient and scalable learning algorithms.\",\n  \"Direct Inspiration\": {\n    \"b42\": 0.9,\n    \"b41\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b34\": 0.75,\n    \"b6\": 0.7,\n    \"b30\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b33\": 0.55\n  }\n}\n```"], "5f7fdd328de39f08283980ba": ["```json\n{\n    \"Summary\": \"The paper addresses challenges in graph representation learning, including the difficulty in handling non-i.i.d. data and optimizing discrete structural information. It proposes the Graph Information Bottleneck (GIB) principle to extract minimal sufficient information from graph-structured data, enhancing robustness and accuracy.\",\n    \"Direct Inspiration\": [\"b17\", \"b18\", \"b4\"],\n    \"Indirect Inspiration\": [\"b20\", \"b21\", \"b22\"],\n    \"Other Inspiration\": [\"b14\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces the Graph Information Bottleneck (GIB) principle, extending the Information Bottleneck (IB) principle to graph representation learning. The primary challenges addressed are the non-i.i.d. nature of graph data and the need to optimize over discrete structural information. The proposed GIB principle aims to extract minimal sufficient information from both node features and graph structure, making the model robust to adversarial attacks and overfitting. The novel methods include variational bounds for GIB and the design of two models, GIB-Cat and GIB-Bern, based on the Graph Attention Networks (GAT).\",\n  \"Direct Inspiration\": [\"b4\"],\n  \"Indirect Inspiration\": [\"b17\", \"b18\", \"b20\", \"b21\", \"b22\"],\n  \"Other Inspiration\": [\"b14\", \"b15\", \"b32\", \"b33\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of graph representation learning, specifically overfitting and vulnerability to adversarial attacks, by proposing the Graph Information Bottleneck (GIB) principle. This principle aims to create robust and efficient representations by minimizing irrelevant information and maximizing relevant information for prediction.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b14\": 1,\n    \"b17\": 1,\n    \"b18\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b20\": 0.8,\n    \"b21\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of representation learning on graph-structured data, particularly focusing on the problems of non-useful neighborhood node features, noise, and adversarial attacks. It proposes the Graph Information Bottleneck (GIB) principle to extract minimal and sufficient information from both graph structure and node features, inspired by the Information Bottleneck (IB) principle. The paper introduces GIB-Cat and GIB-Bern models, applying GIB to Graph Attention Networks (GAT).\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b18\": 1.0,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"GNNs still suffer from problems like non-useful information in neighborhood nodes negatively impacting predictions and vulnerability to noise and adversarial attacks.\",\n    \"inspirations\": \"Inspired by the Information Bottleneck (IB) principle, this paper adapts it to graph-structured data to develop the Graph Information Bottleneck (GIB) principle for learning robust and minimal sufficient representations.\"\n  },\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b18\": 1,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b20\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```"], "5ee8986891e011e66831c3bc": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of training deep Graph Convolutional Networks (GCNs) to improve their performance on large-scale graph datasets. It proposes GENeralized Aggregation Networks (GEN), which utilize generalized message aggregators, pre-activation residual connections, and message normalization layers to enhance GCN training and performance.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b15\": 1,\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b50\": 0.9,\n    \"b14\": 0.8,\n    \"b23\": 0.7,\n    \"b41\": 0.6,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b19\": 0.5,\n    \"b12\": 0.5,\n    \"b8\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training very deep Graph Convolutional Networks (GCNs) by introducing GENeralized Aggregation Networks (GEN) that incorporate generalized message aggregators, pre-activation residual connections, and message normalization layers. The goal is to improve the performance and stability of GCNs on large-scale graph datasets.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b15\": 0.9,\n    \"b50\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b14\": 0.7,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.5,\n    \"b24\": 0.5,\n    \"b57\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training very deep Graph Convolutional Networks (GCNs) and proposes solutions like generalized message aggregators, pre-activation residual connections, and message normalization layers. These methods are evaluated on the Open Graph Benchmark (OGB) datasets.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b15\": 0.9,\n    \"b14\": 0.8,\n    \"b50\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.7,\n    \"b24\": 0.7,\n    \"b57\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b1\": 0.6,\n    \"b12\": 0.6,\n    \"b23\": 0.6,\n    \"b47\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training deeper Graph Convolutional Networks (GCNs) and proposes GENeralized Aggregation Networks (GEN) featuring generalized message aggregators, pre-activation residual connections, and message normalization layers. The paper is inspired by existing works on deep GCN architectures and aims to improve upon limitations related to aggregation functions and over-smoothing.\",\n  \"Direct Inspiration\": [\"b29\", \"b15\", \"b50\"],\n  \"Indirect Inspiration\": [\"b41\", \"b57\", \"b24\"],\n  \"Other Inspiration\": [\"b12\", \"b14\", \"b1\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of training deep Graph Convolutional Networks (GCNs) and proposes novel methods for improving the performance of deep GCN models using generalized message aggregators, pre-activation residual connections, and message normalization layers.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b29\": 1.0,\n    \"b50\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b1\": 0.8,\n    \"b41\": 0.8,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b23\": 0.6,\n    \"b28\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```"], "5cede0fcda562983788dbed8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial attacks on node classification models in graph data. The novel contribution is the development of a meta-learning based algorithm for poisoning attacks that can degrade the overall performance of node classification models. The approach leverages meta-gradients to optimize the graph structure and perform unnoticeable adversarial modifications.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b11\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial attacks on node classification models in graphs, specifically proposing an algorithm for global poisoning attacks using meta-learning techniques.\",\n  \"Direct Inspiration\": [\"b28\", \"b10\"],\n  \"Indirect Inspiration\": [\"b3\", \"b11\", \"b26\", \"b18\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of global poisoning attacks on node classification models in attributed graphs. The primary novel contribution is a meta-learning-based algorithm for training-time attacks that degrade the overall classification performance of deep learning models on graphs by modifying the graph structure. This work is motivated by the limitations of existing targeted attacks and the need for a method that can impact the global performance of node classification models.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b11\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b27\": 0.65,\n    \"b26\": 0.6,\n    \"b19\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial attacks on node classification models using graph convolution, which leverage neighborhood information for improved classification but are vulnerable to such attacks. The authors propose a novel algorithm for global poisoning attacks based on meta learning, which treats the input data (graph) as a hyperparameter to be optimized. The algorithm explicitly tackles the bilevel optimization problem using meta gradients and demonstrates significant degradation in model performance across various datasets.\",\n  \"Direct Inspiration\": [\"b28\", \"b10\", \"b18\"],\n  \"Indirect Inspiration\": [\"b3\", \"b11\"],\n  \"Other Inspiration\": [\"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial attacks on node classification in graphs, proposing a novel algorithm for poisoning attacks that degrade the global performance of node classifiers. The key innovation is the use of meta-learning to optimize the graph structure for these attacks.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b10\": 1,\n    \"b18\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b11\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b22\": 0.7\n  }\n}\n```"], "5c04967517c44a2c7470926f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in semi-supervised node classification in graphs and proposes a standardized evaluation framework to better understand the differences in performance of various GNN architectures. The key challenge is the inconsistency in empirical evaluation procedures, which leads to misleading conclusions about model performance. The paper performs a thorough evaluation of four GNN models (GCN, MoNet, GraphSAGE, GAT) using a unified training and hyperparameter selection procedure across multiple datasets and splits.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b11\": 1.0,\n    \"b14\": 1.0,\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the problematic empirical evaluation procedures in semi-supervised node classification using graph neural networks (GNNs). The authors propose a standardized training and hyperparameter selection procedure to ensure fair comparison and accurate assessment of model generalization. The paper introduces four new datasets and evaluates four prominent GNN architectures (GCN, MoNet, GraphSAGE, GAT) on a more robust experimental setup.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b11\": 1,\n    \"b14\": 1,\n    \"b3\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.9,\n    \"b2\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b0\": 0.7,\n    \"b4\": 0.6,\n    \"b2009\": 0.6,\n    \"b10\": 0.5,\n    \"b8\": 0.5,\n    \"b9\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of evaluating semi-supervised node classification models in graphs due to inconsistent experimental setups. It proposes a standardized evaluation framework for four prominent graph neural network (GNN) architectures (GCN, MoNet, GAT, and GraphSAGE) and introduces new datasets to ensure fairer comparison and better generalization assessment.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b2\": 0.9,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b11\": 0.8,\n    \"b3\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b4\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reliably evaluating semi-supervised node classification models for graphs, highlighting issues with current empirical evaluation practices. It proposes a standardized training and hyperparameter selection procedure to ensure fairer comparisons of different graph neural network (GNN) architectures. The paper performs extensive experiments on several well-known and newly introduced datasets to assess the generalization performance of the models.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b11\": 1,\n    \"b14\": 1,\n    \"b3\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b1\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges addressed in the paper are the problematic aspects of empirical evaluation procedures in semi-supervised node classification tasks using graph neural networks (GNNs). The authors propose a standardized training and hyperparameter selection procedure to ensure fair comparisons between different GNN models. They also introduce four new datasets and perform extensive experiments to assess the generalization performance of various models.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b2\": 0.9,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b11\": 0.75,\n    \"b14\": 0.7,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b1\": 0.55,\n    \"b8\": 0.5\n  }\n}\n```"], "5ca600ae6558b90bfa4d76e9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial robustness in machine learning models, particularly in the context of image classification. It proposes an optimization-based approach using projected gradient descent (PGD) to enhance the robustness of neural networks against adversarial attacks. The paper demonstrates the effectiveness of this method through experiments on MNIST and CIFAR10 datasets, achieving unprecedented levels of robustness.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b10\", \"b28\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b1\", \"b27\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b17\", \"b21\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial robustness in machine learning, particularly in the context of deep neural networks. The authors propose an optimization-based approach, focusing on a saddle point formulation that incorporates adversarial perturbations. Key contributions include the use of projected gradient descent (PGD) as a universal first-order adversary, insights into the impact of network architecture on robustness, and experimental validation on MNIST and CIFAR10 datasets.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b28\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b27\": 0.65,\n    \"b21\": 0.55,\n    \"b8\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in adversarial robustness for deep neural networks, proposing an optimization approach using projected gradient descent (PGD) as a universal first-order adversary. It explores network architecture's impact on robustness and achieves high levels of robustness on MNIST and CIFAR10 datasets. The main contributions include experimental studies on optimization landscapes, training robust models, and demonstrating the tractability of the saddle point problem.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b17\": 1,\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b18\": 0.7,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in making neural networks robust against adversarial attacks. It focuses on solving an optimization problem using projected gradient descent (PGD) and explores the impact of network architecture on adversarial robustness.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b17\": 0.9,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the robustness of machine learning models, particularly deep neural networks, to adversarially crafted inputs. The authors propose the use of a saddle point optimization framework to ensure adversarial robustness and employ projected gradient descent (PGD) as a universal first-order adversary. They explore the impact of network architecture on robustness and demonstrate their approach on MNIST and CIFAR10 datasets achieving significant robustness against various adversarial attacks.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b17\": 0.9,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b27\": 0.7,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b22\": 0.5\n  }\n}\n```"], "5ea16b2b91e011fa08b8f6e3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial attacks on machine learning models, particularly focusing on the weaknesses of single-step adversarial training methods. It proposes a novel single-step adversarial training method with dropout scheduling to enhance robustness against both single-step and multi-step attacks.\",\n  \"Direct Inspiration\": [\"b21\", \"b34\"],\n  \"Indirect Inspiration\": [\"b12\", \"b17\"],\n  \"Other Inspiration\": [\"b33\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial samples in machine learning, especially focusing on single-step adversarial training methods which suffer from gradient masking and over-fitting issues. The authors propose a novel single-step adversarial training method combined with dropout scheduling to improve robustness against both single-step and multi-step attacks.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7,\n    \"b9\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the susceptibility of machine learning models to adversarial samples and the inadequacy of existing single-step adversarial training methods. It highlights challenges such as over-fitting during initial stages of training and proposes a novel single-step adversarial training method with dropout scheduling to enhance robustness against both single-step and multi-step attacks.\",\n  \"Direct Inspiration\": [\"b21\", \"b12\", \"b17\", \"b34\"],\n  \"Indirect Inspiration\": [\"b33\", \"b0\", \"b9\"],\n  \"Other Inspiration\": [\"b32\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Models trained using single-step adversarial training method exhibit gradient masking effect due to over-fitting during initial stages of training.\",\n      \"Existing single-step adversarial training methods fail to provide robustness against both single-step and multi-step attacks.\",\n      \"Training time increases substantially with multi-step adversarial training methods.\"\n    ],\n    \"contributions\": [\n      \"Proposed a novel single-step adversarial training method with dropout scheduling to learn robust models.\",\n      \"Showed that models trained using the proposed method are robust against both single-step and multi-step attacks.\",\n      \"Demonstrated that the proposed method is much faster than multi-step adversarial training methods and achieves comparable results.\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b21\",\n    \"b34\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b12\",\n    \"b17\"\n  ],\n  \"Other Inspiration\": [\n    \"b0\",\n    \"b32\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial attacks on machine learning models, focusing on single-step adversarial training methods. The authors propose a novel single-step adversarial training method with dropout scheduling to enhance the robustness of models against both single-step and multi-step attacks.\",\n  \"Direct Inspiration\": [\"b21\", \"b34\"],\n  \"Indirect Inspiration\": [\"b12\", \"b17\"],\n  \"Other Inspiration\": [\"b33\", \"b32\"]\n}\n```"], "5dde4b463a55ac4c42972afc": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of defending against adversarial examples in deep neural networks by proposing a novel approach involving adversarial transformation. Instead of obfuscating gradients or purging adversarial noise, the paper suggests embracing adversarial noise to strengthen the model's robustness. The method uses a pre-trained external model to transform input images into adversarial examples before classification, making it difficult for attackers to estimate gradients and craft effective adversarial examples.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.9,\n    \"b43\": 0.8,\n    \"b18\": 0.75,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.65,\n    \"b13\": 0.6,\n    \"b38\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of defending against adversarial examples in deep neural networks, which are natural images with imperceptible noise crafted to mislead the network's decision. The authors propose a novel defense mechanism that embraces adversarial examples instead of expelling them, transforming input images through the process of finding adversarial examples on an external model to enhance robustness.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b25\": 0.9,\n    \"b13\": 0.8,\n    \"b38\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b40\": 0.7,\n    \"b12\": 0.7,\n    \"b43\": 0.6,\n    \"b27\": 0.6,\n    \"b7\": 0.6,\n    \"b18\": 0.6,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b6\": 0.5,\n    \"b48\": 0.5,\n    \"b49\": 0.5,\n    \"b32\": 0.5,\n    \"b46\": 0.5,\n    \"b39\": 0.5,\n    \"b35\": 0.5,\n    \"b42\": 0.5,\n    \"b33\": 0.5,\n    \"b44\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge of the paper is the vulnerability of deep neural networks to adversarial examples, which are natural images with imperceptible noise designed to mislead the network's decision. The authors propose a novel defense mechanism by embracing adversarial examples rather than expelling them. They transform input images through the process of finding adversarial examples on an external model before feeding them to a classification model. This approach leverages the fundamental difficulties of estimating gradients to enhance the robustness of the defense model.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.9,\n    \"b43\": 0.8,\n    \"b13\": 0.7,\n    \"b38\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b40\": 0.5,\n    \"b12\": 0.5,\n    \"b27\": 0.5,\n    \"b7\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial examples in deep neural networks, proposing a novel defense mechanism that transforms input images through the process of finding adversarial examples on an external model before classification. The approach embraces adversarial noise to enhance robustness, and the authors demonstrate superior performance against various attacks compared to state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b25\": 0.9,\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b38\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of defending against adversarial examples in deep neural networks, particularly focusing on image classification tasks. The proposed algorithm embraces adversarial examples by transforming input images through adversarial noise to strengthen the robustness of the classification model, instead of trying to expel them. This approach is motivated by the need to obfuscate network gradients, making it difficult for adversaries to craft effective adversarial examples.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b25\": 0.9,\n    \"b43\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b38\": 0.6,\n    \"b13\": 0.6,\n    \"b32\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b40\": 0.5,\n    \"b27\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```"], "5a9cb66717c44a376ffb89eb": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges posed by adversarial examples to neural networks and proposes new techniques to overcome obfuscated gradients, a phenomenon that hinders the effectiveness of iterative optimization-based attacks. The authors identify three types of obfuscated gradients: shattered gradients, stochastic gradients, and vanishing/exploding gradients. They introduce methods such as Backward Pass Differentiable Approximation, Expectation Over Transformation, and reparameterization to tackle these issues.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b6\": 0.9,\n    \"b12\": 0.8,\n    \"b15\": 0.9,\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b26\": 0.6,\n    \"b27\": 0.7,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the problem of neural networks' susceptibility to adversarial examples, particularly focusing on the issue of obfuscated gradients. It proposes new techniques to overcome this problem, including Backward Pass Differentiable Approximation, Expectation Over Transformation, and reparameterization to handle shattered, stochastic, and vanishing/exploding gradients respectively. The paper extensively evaluates these techniques against ICLR 2018 non-certified defenses, demonstrating their effectiveness in overcoming obfuscated gradients and providing a baseline for future research in this area.\",\n    \"Direct Inspiration\": {\n        \"b17\": 0.95,\n        \"b0\": 0.90\n    },\n    \"Indirect Inspiration\": {\n        \"b25\": 0.85,\n        \"b6\": 0.80,\n        \"b15\": 0.80\n    },\n    \"Other Inspiration\": {\n        \"b26\": 0.75,\n        \"b29\": 0.70,\n        \"b10\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the vulnerability of neural networks to adversarial examples. The authors propose new techniques to overcome the phenomenon of obfuscated gradients, which prevent iterative optimization-based attacks from succeeding. These techniques include Backward Pass Differentiable Approximation, Expectation Over Transformation, and reparameterization to handle gradient shattering, stochastic gradients, and vanishing/exploding gradients, respectively.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b6\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b12\": 0.5,\n    \"b25\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the susceptibility of neural networks to adversarial examples, particularly in the white-box setting. The authors identify a common issue called obfuscated gradients, which hinder iterative optimization-based attacks. They propose new techniques to overcome this, such as Backward Pass Differentiable Approximation, Expectation Over Transformation, and reparameterization to handle shattered, stochastic, and vanishing/exploding gradients, respectively.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.95,\n    \"b27\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.85,\n    \"b12\": 0.80,\n    \"b15\": 0.80,\n    \"b6\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.70,\n    \"b10\": 0.70\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the susceptibility of neural networks to adversarial examples and the phenomenon of obfuscated gradients, which can hinder the effectiveness of iterative optimization-based attacks. The authors propose new techniques to overcome obfuscated gradients, including Backward Pass Differentiable Approximation, Expectation Over Transformation, and reparameterization to address shattered, stochastic, and vanishing/exploding gradients, respectively. They evaluate these techniques on various defenses and propose a common baseline of knowledge for future research.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b15\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```"], "5dd7b1be3a55ac97f763dd30": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of leveraging adversarial examples to improve the accuracy of convolutional neural networks (ConvNets), rather than just focusing on defending against them. The proposed solution, AdvProp, introduces a two-batchnorm approach to handle the distribution mismatch between clean and adversarial examples, effectively disentangling the two distributions at normalization layers. This method shows significant improvements in model performance, particularly on distorted images, and stands out as the first to demonstrate such improvements on a large-scale dataset like ImageNet in a fully-supervised setting.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b22\", \"b17\", \"b6\", \"b44\", \"b15\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b40\", \"b32\", \"b8\", \"b9\", \"b5\", \"b25\", \"b29\", \"b42\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b50\", \"b46\", \"b41\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of improving the performance of Convolutional Neural Networks (ConvNets) on clean images by leveraging adversarial examples. The novel algorithm proposed, AdvProp, uses a two-batchnorm technique to disentangle the statistical distributions of clean and adversarial images, thereby mitigating performance degradation traditionally associated with adversarial training. This approach is validated using models like EfficientNet-B7, achieving state-of-the-art results on ImageNet and other datasets.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b6\", \"b15\", \"b17\", \"b22\", \"b44\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b25\", \"b29\", \"b31\", \"b42\", \"b46\", \"b50\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b32\", \"b40\", \"b8\", \"b9\", \"b5\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge is the performance degradation of ConvNets when trained with adversarial examples, especially on clean images.\",\n    \"Inspirations\": \"The paper is inspired by the need to leverage adversarial examples to improve model accuracy rather than just defending against them. The authors propose AdvProp, which uses separate batch normalization statistics for clean and adversarial images to address distribution mismatch.\"\n  },\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b17\": 1.0,\n    \"b22\": 1.0,\n    \"b44\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.9,\n    \"b50\": 0.9,\n    \"b42\": 0.9,\n    \"b46\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.8,\n    \"b29\": 0.8,\n    \"b40\": 0.8,\n    \"b32\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of leveraging adversarial examples to improve the accuracy of Convolutional Neural Networks (ConvNets) rather than just defending against them. The authors propose a novel training scheme named AdvProp, which uses two separate batch normalization statistics for clean and adversarial examples to bridge the distribution mismatch and improve model performance.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b17\": 1.0,\n    \"b22\": 1.0,\n    \"b44\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b25\": 0.8,\n    \"b29\": 0.8,\n    \"b42\": 0.8,\n    \"b50\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b32\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of utilizing adversarial examples to improve the accuracy of convolutional neural networks (ConvNets), particularly on large datasets like ImageNet. The authors propose a new training scheme called AdvProp that uses two separate batch normalization statistics for clean and adversarial images to bridge the distribution mismatch between the two, thereby enhancing model performance without degrading accuracy on clean images.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.95,\n    \"b17\": 0.9,\n    \"b22\": 0.85,\n    \"b44\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.75,\n    \"b25\": 0.7,\n    \"b29\": 0.7,\n    \"b42\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.6,\n    \"b8\": 0.55,\n    \"b9\": 0.55,\n    \"b5\": 0.55\n  }\n}\n```"], "5cede0e1da562983788bfe5f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the robustness of image classification models to both random noise and adversarial attacks. The authors propose that adversarial examples and errors in noisy distributions are closely related and that improving robustness in one will improve the other. They provide empirical evidence and theoretical analysis to support this claim, and they suggest that future defense efforts should focus on reducing test errors in noisy distributions.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b26\": 0.9,\n    \"b32\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b18\": 0.8,\n    \"b21\": 0.85,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.75,\n    \"b30\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of robustness in state-of-the-art computer vision models against both random and adversarial perturbations. It proposes an empirical investigation to link errors due to Gaussian noise and adversarial examples, suggesting that improving robustness in one scenario could enhance it in the other. The study provides experimental evidence to support this hypothesis using models trained on MNIST, CIFAR-10, and ImageNet datasets.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1,\n    \"b15\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b21\": 0.7,\n    \"b14\": 0.6,\n    \"b18\": 0.6,\n    \"b27\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.5,\n    \"b33\": 0.5,\n    \"b37\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the robustness of computer vision models to both random and adversarial perturbations. It explores the connection between errors caused by random noise and adversarial examples and suggests that improving robustness to one type of error can lead to improvements in the other. The paper provides empirical evidence, theoretical analysis, and experimental results to support this claim, focusing on models trained on MNIST, CIFAR-10, and ImageNet datasets.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1,\n    \"b14\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b27\": 0.6,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is improving the robustness of computer vision models to various forms of image corruptions and adversarial examples. The paper explores the relationship between errors in Gaussian noise and adversarial perturbations, providing empirical evidence and theoretical analysis to suggest that improvements in adversarial robustness can lead to better generalization in noisy conditions.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b21\": 0.8,\n    \"b26\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.75,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b18\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the robustness of computer vision models to both random noise and adversarial examples. The authors propose that adversarial examples and errors in noisy images are closely related, suggesting that training methods improving robustness to one can also improve the other. The empirical and theoretical analysis provided supports this claim, establishing a connection between test error in noise and adversarial robustness.\",\n  \"Direct Inspiration\": [\"b15\", \"b32\", \"b26\"],\n  \"Indirect Inspiration\": [\"b14\", \"b18\", \"b27\"],\n  \"Other Inspiration\": [\"b21\", \"b12\"]\n}\n```"], "5f58a1b491e011e46ee73247": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalability and evaluation in adversarial attacks on graph neural networks. It proposes a novel Simplified Gradient-based Attack (SGA) framework, inspired by Simplified Graph Convolutional Network (SGC) and gradient-based attack methods, to improve time and space efficiency. Additionally, it introduces a new metric, Degree Assortativity Change (DAC), to measure the impact of these attacks.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b12\", \"b14\", \"b16\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": []\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b11\", \"b7\", \"b17\", \"b18\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of adversarial attacks on graph neural networks, specifically focusing on scalability and evaluation. It proposes a novel Simplified Gradient-based Attack (SGA) framework inspired by existing methods to efficiently perform adversarial attacks on large graphs. Additionally, it introduces a new metric, Degree Assortativity Change (DAC), to measure the impact of such attacks.\",\n  \"Direct Inspiration\": [\n    \"b12\",\n    \"b16\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b11\",\n    \"b14\"\n  ],\n  \"Other Inspiration\": [\n    \"b2\",\n    \"b3\",\n    \"b7\",\n    \"b18\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of deep learning models to adversarial attacks, specifically focusing on graph data. The main challenges identified are scalability and evaluation of adversarial impacts on graph data. The authors propose a Simplified Gradient-based Attack (SGA) framework, inspired by Simplified Graph Convolutional Network (SGC) and previous gradient-based attack methods. They also introduce a Degree Assortativity Change (DAC) metric to measure the impact of adversarial attacks.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b12\": 1.0,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b17\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalability and evaluation in adversarial attacks on graph neural networks. It proposes a novel Simplified Gradient-based Attack (SGA) framework inspired by the Simplified Graph Convolutional Network (SGC) and gradient-based attack methods. Additionally, it introduces a new metric, Degree Assortativity Change (DAC), to measure the impact of adversarial attacks on graph data.\",\n  \"Direct Inspiration\": [\"b12\", \"b14\", \"b16\"],\n  \"Indirect Inspiration\": [\"b11\", \"b17\", \"b18\"],\n  \"Other Inspiration\": [\"b7\", \"b2\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses two primary challenges in adversarial attacks on graph data: scalability and evaluation. The proposed Simplified Gradient-based Attack (SGA) framework and Degree Assortativity Change (DAC) metric aim to efficiently conduct attacks on large graphs and measure the impact of these attacks, respectively. The SGA framework focuses on using a smaller subgraph for attacks, inspired by Simplified Graph Convolutional Network (SGC) and previous gradient-based methods. The DAC metric addresses the lack of practical methods to evaluate the unnoticeability and impact of adversarial attacks on graph data.\",\n    \"Direct Inspiration\": [\"b12\", \"b16\"],\n    \"Indirect Inspiration\": [\"b11\", \"b14\"],\n    \"Other Inspiration\": []\n}\n```"], "5ee8986891e011e66831c556": ["```json\n{\n  \"Summary\": \"The paper introduces GNNGUARD, a defense mechanism designed to protect Graph Neural Networks (GNNs) from adversarial attacks that perturb the graph structure during training. The proposed method focuses on two key components: neighbor importance estimation based on network homophily and layer-wise graph memory to stabilize training. GNNGUARD is shown to improve the robustness of GNNs across various datasets and attack types, outperforming state-of-the-art GNN defenders.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b14\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b16\": 0.6,\n    \"b15\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.4,\n    \"b2\": 0.3,\n    \"b18\": 0.3,\n    \"b19\": 0.3,\n    \"b20\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces GNNGUARD, a defense mechanism designed to enhance the robustness of Graph Neural Networks (GNNs) against adversarial attacks, particularly poisoning attacks. GNNGUARD modifies the neural message-passing operators of existing GNN models to detect and mitigate the impact of adversarial edges, improving defense performance by up to 15.3% over state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b14\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.6,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b2\": 0.5,\n    \"b18\": 0.5,\n    \"b20\": 0.4,\n    \"b26\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the vulnerability of Graph Neural Networks (GNNs) to adversarial attacks that perturb graph structures, which can degrade the performance of even the strongest GNNs. The proposed solution, GNNGUARD, introduces a robust defense mechanism that can be integrated into any GNN model to counteract these adversarial attacks effectively. GNNGUARD achieves this by modifying the neural message passing operators to dynamically adjust the relevance of nodes' local neighborhoods, prune likely fake edges, and stabilize the graph structure through a layer-wise memory mechanism.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b14\": 1.0,\n    \"b15\": 1.0,\n    \"b16\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b6\": 0.8,\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b25\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of defending Graph Neural Networks (GNNs) against adversarial attacks, particularly focusing on poisoning attacks that corrupt the graph structure during training. The proposed method, GNNGUARD, enhances GNN robustness by modifying the neural message passing scheme. Key components include neighbor importance estimation to detect and prune fake edges, and layer-wise graph memory to stabilize graph structure evolution.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b14\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b20\": 0.5,\n    \"b18\": 0.5,\n    \"b25\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces GNNGUARD, an approach designed to defend Graph Neural Networks (GNNs) against adversarial attacks, specifically targeting attacks that perturb graph structures during training. The key components of GNNGUARD include neighbor importance estimation and layer-wise graph memory, which jointly enhance the robustness of GNNs by detecting and mitigating the effects of fake edges. This approach is evaluated against state-of-the-art GNN defenders and demonstrates significant improvements in defense performance across various datasets and attack types.\",\n    \"Direct Inspiration\": {\n        \"b13\": 0.9,\n        \"b14\": 0.8,\n        \"b18\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.6,\n        \"b7\": 0.6,\n        \"b25\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5,\n        \"b6\": 0.5,\n        \"b16\": 0.5\n    }\n}\n```"], "5cede0f9da562983788d862a": ["```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of deep neural networks to adversarial examples, especially in security-sensitive applications. It proposes a translation-invariant attack method to generate adversarial examples that are less sensitive to the discriminative regions of the white-box model, improving the transferability and success rate of black-box attacks against defense models.\",\n  \"Direct Inspiration\": [\n    \"b3\",\n    \"b11\",\n    \"b16\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b8\",\n    \"b19\",\n    \"b37\",\n    \"b12\",\n    \"b34\"\n  ],\n  \"Other Inspiration\": [\n    \"b20\",\n    \"b6\",\n    \"b38\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of deep neural networks to adversarial examples, particularly in the context of black-box attacks. The primary challenge is the transferability of adversarial examples across different models, especially defense models. To tackle this, the authors propose a translation-invariant attack method that generates adversarial examples less sensitive to the discriminative regions of the attacked white-box model.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b19\": 1.0,\n    \"b34\": 1.0,\n    \"b37\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b16\": 0.8,\n    \"b3\": 0.8,\n    \"b38\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b21\": 0.6,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating transferable adversarial examples that can fool defense models in black-box attacks by proposing a translation-invariant attack method. The method optimizes adversarial examples using translated images to enhance transferability across different models.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b8\": 0.8,\n    \"b16\": 0.7,\n    \"b3\": 0.6\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b37\": 0.7,\n    \"b12\": 0.7,\n    \"b34\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of deep neural networks to adversarial examples and proposes a novel translation-invariant attack method to generate more transferable adversarial examples. The primary challenge is to create adversarial examples that can evade defense models by being less sensitive to the discriminative regions used by these models. The proposed method involves generating adversarial examples for an ensemble of images, including translated versions of a legitimate one, to enhance transferability across models.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b16\": 0.8,\n    \"b8\": 0.85,\n    \"b19\": 0.7,\n    \"b37\": 0.7,\n    \"b12\": 0.7,\n    \"b34\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.65,\n    \"b38\": 0.65,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of adversarial examples in deep neural networks and proposes a translation-invariant attack method to improve the transferability of adversarial examples against defense models. The method generates adversarial examples for an ensemble of images to mitigate the effect of different discriminative regions between models.\",\n    \"Direct Inspiration\": {\n        \"b11\": 0.9,\n        \"b16\": 0.8,\n        \"b19\": 0.8,\n        \"b37\": 0.8,\n        \"b12\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.7,\n        \"b38\": 0.7,\n        \"b34\": 0.7,\n        \"b32\": 0.6,\n        \"b31\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b33\": 0.5,\n        \"b6\": 0.5,\n        \"b20\": 0.5,\n        \"b3\": 0.4\n    }\n}\n```"], "5c0495ae17c44a2c747019af": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating effective adversarial examples for deep neural networks in both white-box and black-box settings. It introduces a momentum iterative gradient-based method to improve the success rates of adversarial attacks, enhancing both attack ability and transferability. The method incorporates momentum to stabilize update directions and escape poor local maxima. Additionally, the paper explores attacking an ensemble of models to further improve transferability.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b8\": 0.9,\n    \"b11\": 0.8,\n    \"b23\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b15\": 0.6,\n    \"b21\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.4,\n    \"b13\": 0.4,\n    \"b18\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the vulnerability of deep neural networks (DNNs) to adversarial examples and the difficulty of effectively performing black-box attacks. The authors propose a novel algorithm called the momentum iterative fast gradient sign method (MI-FGSM) to address these challenges. This method accumulates gradients over iterations to stabilize updates and escape poor local maxima, thus improving both white-box and black-box attack success rates. Additionally, the authors explore ensemble approaches to further enhance the transferability of adversarial examples.\",\n  \"Direct Inspiration\": [\"b4\", \"b8\", \"b11\", \"b23\"],\n  \"Indirect Inspiration\": [\"b22\", \"b15\"],\n  \"Other Inspiration\": [\"b5\", \"b7\", \"b1\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating adversarial examples for Deep Neural Networks (DNNs) that are effective in both white-box and black-box settings. The core contribution is the introduction of momentum iterative gradient-based methods to stabilize optimization and enhance the transferability of adversarial examples. The paper also explores ensemble approaches to further improve adversarial example transferability.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b8\": 0.9,\n    \"b11\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating effective adversarial examples for deep neural networks (DNNs) that can successfully attack both white-box and black-box models. The proposed solution is a class of momentum iterative gradient-based methods, which enhance the transferability and success rate of adversarial examples by incorporating momentum into the iterative process. The paper also explores attacking an ensemble of models to improve the robustness of adversarial attacks.\",\n  \"Direct Inspiration\": [\"b4\", \"b8\", \"b11\", \"b23\"],\n  \"Indirect Inspiration\": [\"b15\", \"b18\"],\n  \"Other Inspiration\": [\"b22\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating effective adversarial examples for deep neural networks (DNNs), particularly for black-box attacks. It introduces momentum iterative gradient-based methods (MI-FGSM) to stabilize updates and improve success rates in both white-box and black-box attacks. The approach also includes attacking ensembles of models to enhance transferability. Key contributions include the introduction of momentum-based methods and ensemble attack strategies.\",\n    \"Direct Inspiration\": [\"b4\", \"b8\", \"b11\", \"b23\"],\n    \"Indirect Inspiration\": [\"b15\", \"b22\"],\n    \"Other Inspiration\": []\n}\n```"], "57a4e91aac44365e35c975d0": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of adversarial sample transferability within and across different machine learning classifiers and proposes a generalized algorithm for black-box attacks exploiting this transferability. The primary contributions include introducing adversarial sample crafting techniques for non-differentiable models, characterizing transferability across various machine learning models, generalizing substitute model learning, and validating these concepts through black-box attacks on commercial classifiers.\",\n    \"Direct Inspiration\": [\"b18\", \"b19\", \"b21\"],\n    \"Indirect Inspiration\": [\"b11\"],\n    \"Other Inspiration\": [\"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of black-box attacks on machine learning models by exploiting adversarial sample transferability. It proposes a generalized algorithm for these attacks across various machine learning techniques beyond deep neural networks (DNNs), including logistic regression (LR), support vector machines (SVM), decision trees (DT), nearest neighbors (kNN), and ensembles. The key contributions include new adversarial sample crafting techniques for non-differentiable models, enhanced substitute learning methods using periodic step size and reservoir sampling, and empirical validation of black-box attacks on commercial classifiers like those hosted by Amazon and Google.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b21\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of adversarial samples in machine learning, focusing on their transferability across various models and techniques. The authors propose a generalized algorithm for black-box attacks exploiting adversarial sample transferability, particularly highlighting the vulnerability of non-deep learning models to such attacks. They introduce novel techniques for crafting adversarial samples for non-differentiable models like SVMs and decision trees and demonstrate the effectiveness of these techniques in practical settings, including commercial classifiers hosted by Amazon and Google.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.9,\n    \"b11\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of machine learning algorithms to adversarial samples, specifically focusing on black-box attacks leveraging adversarial sample transferability. It proposes a generalized algorithm for black-box attacks that is applicable across various machine learning classifiers including DNNs, LR, SVMs, DTs, kNNs, and ensembles. Key contributions include the introduction of adversarial sample crafting techniques for SVMs and DTs, a study on adversarial sample transferability across different models, and improvements in learning substitute models for black-box attacks.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.85,\n    \"b21\": 0.80,\n    \"b11\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.65,\n    \"b22\": 0.60,\n    \"b15\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of black box attacks on machine learning classifiers by leveraging the transferability of adversarial samples. It generalizes the attack to various machine learning techniques beyond deep neural networks and introduces new methods for crafting adversarial samples that are effective across different models. The authors demonstrate the effectiveness of these attacks on commercial classifiers and propose improvements to substitute learning techniques to enhance attack accuracy and reduce computational cost.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b18\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b15\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.4,\n    \"b1\": 0.4,\n    \"b22\": 0.4\n  }\n}\n```"], "573696076e3b12023e51a63f": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge addressed in the paper is the creation of adversarial samples that mislead deep neural networks (DNNs) into misclassifying inputs. The paper proposes a new class of algorithms that leverage forward derivatives and adversarial saliency maps to generate these adversarial samples with minimal perturbation to input features.\",\n    \"Inspirations\": \"The paper is inspired by previous works that explored adversarial attacks on DNNs, particularly those that used gradient-based techniques to create adversarial samples.\"\n  },\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b28\": 1.0,\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating adversarial samples to mislead deep neural networks (DNNs). It introduces a new class of algorithms that use forward derivatives to craft adversarial samples with minimal perturbation. The primary contributions include formalizing the adversarial threat model, introducing algorithms that exploit forward derivatives and adversarial saliency maps, and validating these algorithms on the LeNet architecture and MNIST dataset.\",\n  \"Direct Inspiration\": [\"b17\", \"b34\"],\n  \"Indirect Inspiration\": [\"b3\", \"b28\"],\n  \"Other Inspiration\": [\"b24\", \"b29\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the creation of adversarial samples that can mislead deep neural networks (DNNs) into misclassifying inputs, and the formalization of threat models in deep learning. The paper proposes a novel class of algorithms for generating adversarial samples by computing a direct mapping from input to output, utilizing forward derivatives and adversarial saliency maps to efficiently perturb input features.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b34\": 0.9,\n    \"b28\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.75,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of crafting adversarial samples that can mislead deep neural networks (DNNs). The authors propose a new class of algorithms based on forward derivatives and adversarial saliency maps to generate adversarial samples with minimal perturbations to the input features. The primary focus is on enhancing the understanding of adversary capabilities and developing efficient methods for adversary sample generation.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b34\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b25\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of adversarial sample creation, proposing a new class of algorithms for crafting adversarial samples against feedforward DNNs. The approach involves computing a direct mapping from input to output and using forward derivatives to generate adversarial saliency maps, providing a more efficient and effective method than previous gradient-based techniques.\",\n  \"Direct Inspiration\": [\"b17\", \"b28\", \"b34\"],\n  \"Indirect Inspiration\": [\"b3\", \"b24\"],\n  \"Other Inspiration\": [\"b29\"]\n}\n```"], "5e5e189993d709897ce1ddbc": ["```json\n{\n  \"Summary\": \"The paper addresses the significant memory and computational challenges associated with large Transformer models, especially when processing long sequences. The proposed Reformer model introduces three main techniques: reversible layers, chunking in feed-forward layers, and locality-sensitive hashing (LSH) for attention. These methods collectively aim to reduce memory usage and computational complexity, enabling the training and fine-tuning of large models on single machines.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b23\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b11\": 0.6,\n    \"b9\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the excessive memory and computational requirements of large-scale Transformer models. The proposed solution, the Reformer model, introduces reversible layers, chunking in feed-forward layers, and locality-sensitive hashing (LSH) for attention to significantly reduce memory usage and computational complexity while maintaining performance.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b11\": 0.6,\n    \"b9\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the significant challenge of the high memory and computational requirements of large Transformer models, especially when dealing with long sequences. It introduces the Reformer model, which employs techniques such as reversible layers, chunking, and locality-sensitive hashing (LSH) attention to reduce memory usage and computational complexity while maintaining performance.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b1\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces the Reformer model to address the excessive memory consumption in large-scale Transformer models. It proposes solutions like reversible layers, chunking feed-forward activations, and locality-sensitive hashing (LSH) to reduce memory usage while maintaining performance.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b9\": 0.6,\n    \"b13\": 0.6,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper include the high memory and computational requirements of large-scale Transformer models, particularly when dealing with long sequences. The paper addresses issues such as the need for storing activations for back-propagation, the memory requirements of intermediate feed-forward layers, and the computational complexity of attention mechanisms.\",\n    \"Algorithm\": \"The Reformer model is proposed to tackle these challenges using techniques such as reversible layers, splitting activations inside feed-forward layers, and approximate attention computation based on locality-sensitive hashing.\"\n  },\n  \"Direct Inspiration\": [\n    \"b7\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b1\",\n    \"b23\"\n  ],\n  \"Other Inspiration\": [\n    \"b11\",\n    \"b9\",\n    \"b13\",\n    \"b18\"\n  ]\n}\n```"], "5db9298647c8f766461f8ecd": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is evaluating the reliability of attention mechanisms in neural networks as a means of model explanation. The authors propose several novel methods for assessing the utility of attention distributions, including freezing attention weights, initializing multiple training sequences with different random seeds, and using a diagnostic tool that tests attention distributions by employing them as frozen weights in a non-contextual MLP architecture. The paper also introduces a model-consistent training protocol for finding adversarial attention weights.\",\n  \"Direct Inspiration\": {\n    \"reference_number\": \"b3\",\n    \"confidence_score\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"reference_number\": \"b2\",\n    \"confidence_score\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"reference_number\": \"b0\",\n    \"confidence_score\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of using attention mechanisms in NLP models as explanations for model predictions. It critiques the work of Jain and Wallace, which argues that attention scores are not reliable explanations because alternative distributions can produce similar model outcomes. The authors propose a more model-driven approach to test the utility of attention distributions, introduce diagnostic tools, and develop a model-consistent training protocol for finding adversarial attention weights.\",\n  \"Direct Inspiration\": [\"b3\"],\n  \"Indirect Inspiration\": [\"b0\", \"b2\"],\n  \"Other Inspiration\": [\"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of using attention mechanisms as explanations for model predictions in NLP. It critiques the methodology of Jain and Wallace (2019) and proposes a more model-driven approach to test the utility of attention distributions. Key contributions include testing frozen attention weights, examining variance in attention weights, introducing a diagnostic tool using MLP architecture, and developing a model-consistent training protocol for adversarial attention distributions.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b8\": 0.6,\n    \"b11\": 0.6,\n    \"b14\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of using attention mechanisms as explanations for model predictions in NLP. It critiques the methodology of Jain and Wallace (2019) and proposes a more model-driven approach to test the utility of attention distributions. The authors introduce several experiments and a diagnostic tool to evaluate attention scores' explainability. They also develop a model-consistent training protocol for adversarial attention distributions.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b15\": 0.7,\n    \"b0\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.5,\n    \"b11\": 0.4,\n    \"b8\": 0.4,\n    \"b16\": 0.4,\n    \"b14\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges related to using attention mechanisms in NLP models as explainable tools. It critiques the experimental design of previous work by Jain and Wallace, which argues that attention scores cannot reliably explain model predictions. The authors propose a more controlled and model-driven approach to evaluate the utility of attention distributions.\",\n\n  \"Direct Inspiration\": {\n    \"b3\": 1\n  },\n\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9\n  },\n\n  \"Other Inspiration\": {\n    \"b0\": 0.8,\n    \"b15\": 0.7\n  }\n}\n```"], "5ce3afafced107d4c65f7c4c": ["```json\n{\n  \"Summary\": \"The paper discusses the creation and features of SuperGLUE, a new benchmark designed to provide a more rigorous test of language understanding than the original GLUE benchmark. The primary challenges include the need for more challenging tasks, diverse task formats, comprehensive human baselines, improved code support, and refined usage rules. The algorithm and methods proposed include sample-efficient learning, transfer learning, multitask learning, and unsupervised or self-supervised learning.\",\n  \"Direct Inspiration\": {\n    \"b62\": 1.0,\n    \"b16\": 0.9,\n    \"b52\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b67\": 0.8,\n    \"b37\": 0.8,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b46\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents SuperGLUE, a new benchmark designed to pose a more rigorous test of language understanding, improving upon GLUE with more challenging tasks and diverse task formats. The authors anticipate significant progress on SuperGLUE will require innovations in sample-efficient, transfer, multitask, and unsupervised or self-supervised learning.\",\n  \"Direct Inspiration\": {\n    \"b62\": 1.0,\n    \"b16\": 0.9,\n    \"b52\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b48\": 0.8,\n    \"b37\": 0.7,\n    \"b49\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b67\": 0.6,\n    \"b45\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the need for a more rigorous benchmark for language understanding tasks, as the original GLUE benchmark has seen rapid progress and is no longer a suitable metric for current advancements. The proposed algorithm, SuperGLUE, aims to address this by providing a harder set of tasks, more diverse task formats, comprehensive human baselines, and improved code support. The tasks are designed to test various core areas of machine learning, including sample-efficient, transfer, multitask, and unsupervised or self-supervised learning.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b37\": 1.0,\n    \"b67\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b48\": 0.8,\n    \"b52\": 0.8,\n    \"b62\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b49\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces SuperGLUE, a new benchmark designed to improve upon the GLUE benchmark for evaluating general-purpose language understanding systems. SuperGLUE aims to provide a more rigorous test of language understanding through a collection of more challenging and diverse tasks. The paper highlights the necessity of innovations in sample-efficient, transfer, multitask, and unsupervised/self-supervised learning to achieve significant progress on SuperGLUE.\",\n  \"Direct Inspiration\": {\n    \"b62\": 1.0,\n    \"b16\": 0.9,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b48\": 0.7,\n    \"b52\": 0.7,\n    \"b45\": 0.6,\n    \"b67\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b49\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5,\n    \"b60\": 0.4,\n    \"b26\": 0.4,\n    \"b70\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the need for a more rigorous benchmark for evaluating general-purpose language understanding systems, as recent models have surpassed the original GLUE benchmark. The authors introduce SuperGLUE, which includes more challenging tasks, diverse task formats, comprehensive human baselines, improved code support, and refined usage rules. The goal of SuperGLUE is to provide a robust evaluation metric that requires substantive innovations in machine learning, including sample-efficient, transfer, multitask, and unsupervised or self-supervised learning.\",\n  \"Direct Inspiration\": {\n    \"b62\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b48\": 0.7,\n    \"b67\": 0.6,\n    \"b37\": 0.6,\n    \"b45\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b49\": 0.4,\n    \"b31\": 0.3,\n    \"b70\": 0.3\n  }\n}\n```"], "5efb0d5691e011063336d27d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of interpreting Transformer models applied to protein sequences, focusing on how attention mechanisms capture structural and functional characteristics of proteins. Key contributions include analyzing the alignment of attention with protein contact maps, binding sites, and post-translational modifications, as well as presenting methods for visualizing attention within protein structures.\",\n  \"Direct Inspiration\": {\n    \"b61\": 0.9,\n    \"b49\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.8,\n    \"b14\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b50\": 0.6,\n    \"b40\": 0.6,\n    \"b58\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of interpreting Transformer models applied to protein sequences, aiming to reveal how attention mechanisms capture structural and functional properties of proteins. Inspired by NLP techniques, the authors adapt interpretability methods to understand protein structure, binding sites, and post-translational modifications.\",\n  \"Direct Inspiration\": {\n    \"b61\": 0.95, \n    \"b45\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b10\": 0.65,\n    \"b0\": 0.65,\n    \"b49\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.5,\n    \"b58\": 0.5,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper include the lack of interpretability in Transformer models for protein sequences, model bias, and spurious correlations.\",\n    \"inspirations\": \"The paper is inspired by the need to extend interpretability research from NLP to protein sequences, analyzing Transformer models through the lens of attention to capture functional and structural characteristics of proteins.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b11\", \"b45\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b61\", \"b50\", \"b40\", \"b58\", \"b29\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b12\", \"b13\", \"b14\", \"b57\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"The primary challenges are the lack of interpretability in Transformer models applied to protein sequences, which can hide underlying problems such as model bias and spurious correlations.\",\n        \"inspirations\": \"Inspired by interpretability research in NLP, specifically 'BERTology,' the authors adapt these methods to analyze Transformer protein models.\"\n    },\n    \"Direct Inspiration\": {\n        \"b11\": 0.9,\n        \"b49\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.7,\n        \"b45\": 0.7,\n        \"b57\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.5,\n        \"b61\": 0.5,\n        \"b69\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the lack of interpretability in Transformer models applied to protein sequences, which can hide underlying problems such as model bias and spurious correlations. The paper proposes a set of interpretability methods to analyze attention in Transformer models for protein sequences, capturing unique functional and structural characteristics of proteins. The methodologies include attention analysis, probing tasks, and visualization techniques.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b45\": 1.0,\n    \"b49\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.8,\n    \"b58\": 0.8,\n    \"b29\": 0.8,\n    \"b50\": 0.7,\n    \"b61\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b7\": 0.5,\n    \"b20\": 0.5,\n    \"b52\": 0.5,\n    \"b51\": 0.5,\n    \"b39\": 0.5\n  }\n}\n```"], "5f0d8b6891e011047aff993b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of closing the sequence-structure and sequence-function gaps in protein research using advanced language models (LMs) inspired by natural language processing (NLP). The authors propose a method to up-scale language models trained on proteins and compare auto-regressive and auto-encoding pre-training approaches to existing evolutionary-based techniques.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b12\": 0.85,\n    \"b47\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b16\": 0.75,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b23\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of predicting protein structures and functions using advanced language models (LMs) that have been successful in NLP. The approach involves treating protein sequences as sentences and amino acids as words, leveraging the sequential nature of proteins to map NLP algorithms onto protein sequences. The paper explores the scalability of LMs on large protein databases and compares the effects of different pre-training strategies on the success of subsequent supervised training. The methods are validated against state-of-the-art solutions using evolutionary information.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b12\": 1.0,\n    \"b13\": 0.9,\n    \"b15\": 0.9,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b17\": 0.8,\n    \"b18\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7,\n    \"b36\": 0.7,\n    \"b37\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of predicting the sequence-structure and sequence-function gaps in protein research using advanced language models (LMs) inspired by Natural Language Processing (NLP) techniques. The authors propose up-scaling LMs trained on proteins and compare auto-regressive and auto-encoding pre-training methods, leveraging HPC advancements.\",\n    \"Direct Inspiration\": {\n        \"b11\": 0.9,\n        \"b12\": 0.9,\n        \"b16\": 0.9,\n        \"b20\": 0.9,\n        \"b23\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.7,\n        \"b21\": 0.7,\n        \"b15\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b33\": 0.6,\n        \"b9\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of leveraging high-performance computing (HPC) to advance deep learning (DL) models for protein research, particularly to bridge the 'sequence-structure' and 'sequence-function' gaps in protein bioinformatics. It introduces ProtTrans, which scales language models (LMs) trained on protein sequences and compares auto-regressive and auto-encoding pre-training. The study aims to improve the efficiency and effectiveness of protein sequence analysis using advanced LMs inspired by natural language processing (NLP) techniques.\",\n  \"Direct Inspiration\": [\"b11\", \"b12\"],\n  \"Indirect Inspiration\": [\"b15\", \"b16\", \"b17\", \"b18\", \"b19\"],\n  \"Other Inspiration\": [\"b5\", \"b33\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper explores the use of advanced Language Models (LMs) for protein sequence analysis, inspired by breakthroughs in Natural Language Processing (NLP). The main challenges include leveraging large-scale unlabelled protein data, surpassing the limitations of evolutionary information-based methods, and improving protein structure and function predictions.\",\n    \"Direct Inspiration\": {\n        \"b20\": 1,\n        \"b23\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.9,\n        \"b12\": 0.9,\n        \"b16\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.8,\n        \"b24\": 0.8,\n        \"b25\": 0.8,\n        \"b33\": 0.8\n    }\n}\n```"], "5a260c8117c44a4ba8a30f54": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying convolutional neural networks (CNNs) to non-grid-like structured data, such as graphs. It proposes a novel attention-based architecture for node classification in graph-structured data, leveraging self-attention mechanisms to compute hidden representations of nodes by attending to their neighbors. The approach offers advantages like parallelizability, applicability to nodes with varying degrees, and suitability for inductive learning tasks.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b16\": 0.7,\n    \"b1\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b8\": 0.5,\n    \"b10\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of node classification on graph-structured data using an attention-based architecture. Inspired by the recent success of attention mechanisms in sequence-based tasks, the authors propose a graph attention network (GAT) that computes hidden representations of nodes by attending over their neighbors. The key contributions include the application of self-attention to graphs, parallelization of computations, and suitability for inductive learning tasks.\",\n    \"Direct Inspiration\": [\"b2\", \"b38\"],\n    \"Indirect Inspiration\": [\"b28\", \"b16\", \"b23\"],\n    \"Other Inspiration\": [\"b1\", \"b32\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a novel attention-based architecture for node classification in graph-structured data, addressing challenges such as dealing with variable-sized neighborhoods and enabling efficient, parallelizable operations without requiring the entire graph structure upfront. The architecture leverages masked self-attentional layers to compute hidden representations of nodes by attending over their neighbors, achieving state-of-the-art results on several benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b28\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b8\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces an attention-based architecture for node classification in graph-structured data, leveraging self-attention mechanisms. The primary challenges addressed include efficiently handling graphs with different structures, dealing with variable-sized neighborhoods, and applying the model to unseen graphs in inductive learning settings. The proposed method aims to improve computational efficiency and model interpretability while achieving state-of-the-art performance.\",\n  \"Direct Inspiration\": [\"b38\", \"b2\"],\n  \"Indirect Inspiration\": [\"b8\", \"b16\", \"b23\", \"b28\"],\n  \"Other Inspiration\": [\"b1\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying convolutional neural networks (CNNs) to graph-structured data, proposing a novel attention-based architecture for node classification on graphs. The primary inspiration comes from recent advancements in self-attention mechanisms, particularly in sequence-based tasks.\",\n  \"Direct Inspiration\": [\n    \"b38\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b1\",\n    \"b2\",\n    \"b16\",\n    \"b23\",\n    \"b28\"\n  ],\n  \"Other Inspiration\": [\n    \"b4\",\n    \"b8\",\n    \"b10\",\n    \"b11\",\n    \"b32\"\n  ]\n}\n```"], "5e4672c93a55ac14f595d8b5": ["```json\n{\n  \"Summary\": \"The paper introduces SimCLR, a simple framework for contrastive learning of visual representations. The primary challenges addressed include the complexity and computational expense of generative approaches, limitations of heuristic-based pretext tasks in discriminative approaches, and achieving effective representation learning through contrastive methods. Key innovations include the use of multiple data augmentation operations, a learnable nonlinear transformation, normalized embeddings, and optimizing training with larger batch sizes and deeper networks.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b22\": 1.0,\n    \"b41\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b20\": 0.8,\n    \"b51\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b14\": 0.6,\n    \"b21\": 0.6,\n    \"b49\": 0.6,\n    \"b38\": 0.6,\n    \"b25\": 0.6,\n    \"b48\": 0.6,\n    \"b33\": 0.6,\n    \"b8\": 0.6,\n    \"b0\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning effective visual representations without human supervision, specifically through a simple framework for contrastive learning called SimCLR. The key contributions include the importance of strong data augmentation, the introduction of a learnable nonlinear transformation, the benefits of normalized embeddings and an adjusted temperature parameter, and the advantage of larger batch sizes and deeper networks.\",\n  \"Direct Inspiration\": {\n    \"b41\": 0.9,\n    \"b22\": 0.85,\n    \"b51\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.65,\n    \"b46\": 0.6,\n    \"b48\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning effective visual representations without human supervision and proposes a contrastive learning framework called SimCLR. The primary innovations include the use of data augmentation, a learnable nonlinear transformation between the representation and contrastive loss, and the benefits of larger batch sizes and longer training.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b22\": 0.9,\n    \"b41\": 0.85,\n    \"b51\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b20\": 0.7,\n    \"b14\": 0.7,\n    \"b19\": 0.7,\n    \"b38\": 0.65,\n    \"b49\": 0.65,\n    \"b21\": 0.65,\n    \"b46\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b28\": 0.5,\n    \"b17\": 0.5,\n    \"b16\": 0.5,\n    \"b30\": 0.5,\n    \"b43\": 0.5,\n    \"b33\": 0.5,\n    \"b48\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning effective visual representations without human supervision. It introduces SimCLR, a simple framework for contrastive learning of visual representations, which outperforms previous methods by using data augmentation, a nonlinear projection head, and large batch sizes.\",\n  \"Direct Inspiration\": {\n    \"b41\": 0.9,\n    \"b19\": 0.8,\n    \"b1\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b51\": 0.7,\n    \"b22\": 0.7,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.5,\n    \"b30\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of learning effective visual representations without human supervision, focusing on the limitations of generative and discriminative approaches. The authors propose a new contrastive learning framework called SimCLR, which outperforms previous methods and simplifies the architecture by removing the need for specialized components. Key findings include the importance of data augmentation, the benefit of a nonlinear transformation between representation and contrastive loss, and the advantages of larger batch sizes and deeper networks.\",\n    \"Direct Inspiration\": [\n        \"b41\",\n        \"b51\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b1\",\n        \"b22\",\n        \"b20\"\n    ],\n    \"Other Inspiration\": [\n        \"b43\",\n        \"b30\"\n    ]\n}\n```"], "5eede0b091e0116a23aafc15": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is improving the efficiency and effectiveness of semi-supervised learning by leveraging a large amount of unlabeled data, specifically in the domain of computer vision. The authors propose a novel semi-supervised learning framework consisting of three main steps: unsupervised or self-supervised pretraining, supervised fine-tuning, and distillation using unlabeled data. The framework, named SimCLRv2, builds upon the SimCLR approach and introduces several improvements, including the use of larger ResNet models, deeper projection heads, and a memory mechanism inspired by MoCo. The paper's key contributions include demonstrating the label efficiency of bigger models, the importance of the nonlinear transformation after convolutional layers, and the effectiveness of distillation using unlabeled data.\",\n  \"Direct Inspiration\": [\"b0\", \"b10\", \"b11\", \"b18\", \"b23\"],\n  \"Indirect Inspiration\": [\"b15\", \"b16\", \"b19\", \"b21\", \"b22\"],\n  \"Other Inspiration\": [\"b20\", \"b24\", \"b25\", \"b26\", \"b27\", \"b28\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is learning from a few labeled examples while effectively utilizing a large amount of unlabeled data. The proposed algorithm, SimCLRv2, improves upon the 'unsupervised pretrain, supervised fine-tune' paradigm for semi-supervised learning on ImageNet. The method leverages unlabeled data in both task-agnostic and task-specific ways. Key components include the use of larger ResNet models, a deeper projection head, and a distillation phase using unlabeled data to improve and transfer task-specific predictions into a smaller network.\",\n  \"Direct Inspiration\": [\"b18\", \"b19\", \"b10\", \"b23\", \"b11\"],\n  \"Indirect Inspiration\": [\"b0\", \"b21\", \"b22\"],\n  \"Other Inspiration\": [\"b24\", \"b27\", \"b28\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are effectively leveraging a small percentage of labeled data with a large amount of unlabeled data for semi-supervised learning. The proposed algorithm, SimCLRv2, involves three steps: unsupervised or self-supervised pretraining, supervised fine-tuning, and distillation using unlabeled data. The paper emphasizes the importance of model size and design choices in contrastive learning to improve accuracy. Key inspirations include recent advances in self-supervised learning and the use of unlabeled data for distillation and pseudo-labeling.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b11\": 1.0,\n    \"b18\": 1.0,\n    \"b19\": 1.0,\n    \"b21\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b20\": 0.7,\n    \"b24\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b27\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are effectively leveraging both labeled and unlabeled data for semi-supervised learning, particularly in the context of visual representations from images. The proposed algorithm, SimCLRv2, enhances the SimCLR framework to better utilize unlabeled data through a three-step process: unsupervised pretraining, supervised fine-tuning, and distillation using unlabeled data. The approach aims to improve the efficiency of learning from fewer labeled examples by employing larger models and deeper projection heads.\",\n  \"Direct Inspiration\": [\"b0\", \"b10\", \"b11\", \"b18\", \"b19\", \"b21\", \"b23\", \"b24\"],\n  \"Indirect Inspiration\": [\"b25\", \"b26\", \"b27\", \"b28\", \"b29\", \"b30\"],\n  \"Other Inspiration\": [\"b4\", \"b5\", \"b6\", \"b7\", \"b8\", \"b9\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning from a few labeled examples while leveraging a large amount of unlabeled data in the context of computer vision. It proposes a semi-supervised learning framework, SimCLRv2, which improves upon the SimCLR method through unsupervised pretraining, supervised fine-tuning, and distillation using unlabeled data. The main contributions include demonstrating the importance of model size, the benefits of deeper projection heads, and the effectiveness of self-distillation.\",\n  \"Direct Inspiration\": [\"b10\", \"b11\", \"b18\", \"b19\", \"b23\"],\n  \"Indirect Inspiration\": [\"b0\", \"b21\", \"b22\"],\n  \"Other Inspiration\": [\"b15\", \"b16\", \"b20\", \"b24\", \"b26\", \"b27\", \"b29\"]\n}\n```"], "5eede0b091e0116a23aafb82": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of unsupervised visual representation learning, particularly in scaling to large datasets and improving clustering and transformation techniques. It proposes SwAV, a method that combines online clustering with a multi-crop augmentation strategy to improve performance. The key contributions are a scalable clustering loss and a multi-crop strategy that boosts the number of views without increasing computational requirements.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b6\": 1.0,\n    \"b9\": 1.0,\n    \"b23\": 1.0,\n    \"b41\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b55\": 0.8,\n    \"b50\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of unsupervised visual representation learning, specifically focusing on improving self-supervised learning methods. It introduces an online clustering-based method (SwAV) and a multi-crop strategy to enhance performance without increasing computational or memory requirements.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b6\": 0.9,\n        \"b9\": 0.8,\n        \"b23\": 0.8,\n        \"b41\": 0.85,\n        \"b55\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.7,\n        \"b50\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.5,\n        \"b25\": 0.5,\n        \"b38\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in unsupervised visual representation learning or self-supervised learning. It introduces a novel method called SwAV (Swapping Assignments between multiple Views of the same image) which scales to large datasets, works with both small and large batch sizes without a memory bank or momentum encoder, and improves the contrastive loss function and image transformations. The multi-crop strategy is also proposed to increase the number of views without significant computational cost.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b6\": 1.0,\n        \"b23\": 1.0,\n        \"b41\": 1.0,\n        \"b55\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.8,\n        \"b15\": 0.8,\n        \"b22\": 0.8,\n        \"b50\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b7\": 0.6,\n        \"b12\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised visual representation learning, specifically through instance discrimination and contrastive losses. It proposes a new method, SwAV, which involves swapped prediction of cluster assignments between multiple image views to improve scalability and performance. Additionally, it introduces a multi-crop strategy to enhance the number of views without added computational costs, significantly boosting performance on self-supervised benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b6\": 0.9,\n    \"b9\": 0.9,\n    \"b23\": 1.0,\n    \"b55\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b50\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised visual representation learning by proposing an online clustering-based self-supervised method called SwAV, which avoids the need for large memory banks or momentum encoders. The method improves upon the instance discrimination task by using a swapped prediction problem and a multi-crop strategy for image transformations.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b6\": 0.8,\n    \"b9\": 0.8,\n    \"b23\": 0.8,\n    \"b41\": 0.7,\n    \"b55\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b50\": 0.6,\n    \"b61\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b7\": 0.5,\n    \"b18\": 0.5,\n    \"b28\": 0.5,\n    \"b56\": 0.5,\n    \"b59\": 0.5,\n    \"b60\": 0.5,\n    \"b65\": 0.5\n  }\n}\n```"], "5f02f25491e011ee5e0258e0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in attributed graph embedding, specifically the entanglement of filters and weight matrices in GCNs, the suboptimal nature of existing graph convolutional filters for low-pass filtering, and the incompatibility of existing training objectives with real-world applications. The authors propose the Adaptive Graph Encoder (AGE) framework to overcome these issues.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b14\": 0.9,\n    \"b17\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b27\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in attributed graph embedding, specifically focusing on disentangling filters and weight matrices, designing optimal graph convolutional filters, and improving training objectives. The proposed algorithm, Adaptive Graph Encoder (AGE), introduces a Laplacian smoothing filter and adaptive encoder for enhanced node embeddings.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b22\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in attributed graph embedding by proposing the Adaptive Graph Encoder (AGE). The main contributions include disentangling filters and weight matrices to enhance performance, designing a non-parametric Laplacian smoothing filter for better low-pass filtering, and employing adaptive learning to replace traditional reconstruction training objectives.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b17\": 0.9,\n    \"b34\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.6,\n    \"b30\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.5,\n    \"b23\": 0.5,\n    \"b27\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of capturing both structure and feature information in attributed graph embedding. It proposes the Adaptive Graph Encoder (AGE) to overcome the limitations of existing GCN-based methods by disentangling filters and weight matrices, using a non-parametric Laplacian smoothing filter, and employing adaptive learning for better node embeddings.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.9,\n    \"b5\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b30\": 0.7,\n    \"b23\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of attributed graph embedding, particularly the entanglement of filters and weight matrices, the design of graph convolutional filters, and the inadequacies of reconstruction training objectives. The proposed solution is the Adaptive Graph Encoder (AGE), which introduces a non-parametric Laplacian smoothing filter and an adaptive encoder.\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.9,\n    \"b17\": 0.9,\n    \"b22\": 0.8,\n    \"b23\": 0.8,\n    \"b31\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b14\": 0.7,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```"], "5bbacb7417c44aecc4eae150": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of data sparsity and cold start in recommendation systems by leveraging semantic similarity in user review texts. It critiques existing methods that use sentiment analysis and proposes a novel approach incorporating semantic similarity into a Probabilistic Matrix Factorization (PMF) algorithm, enhanced with Bayesian treatment to prevent overfitting.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b6\": 0.9,\n    \"b13\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b27\": 0.7,\n    \"b38\": 0.7,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b17\": 0.6,\n    \"b21\": 0.6,\n    \"b33\": 0.6,\n    \"b36\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the data sparsity and cold start issues in recommendation systems by proposing a novel approach that incorporates semantic similarity of user review texts. The method extends Probabilistic Matrix Factorization (PMF) to include semantic similarity as an additional factor and applies Bayesian treatment to avoid overfitting. The primary contributions are computing semantic similarity between review texts, using this similarity to predict missing ratings, and incorporating the quantified review texts into PMF to enhance rating prediction.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b37\": 1,\n    \"b33\": 1,\n    \"b17\": 1,\n    \"b36\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b6\": 0.9,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b18\": 0.7,\n    \"b38\": 0.7,\n    \"b12\": 0.7,\n    \"b5\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges of data sparsity and cold start in recommendation systems by proposing a novel tensor factorization method based on semantic similarity of user review texts. The algorithm extends Probabilistic Matrix Factorization (PMF) to include semantic similarity, and further incorporates a Bayesian treatment to avoid overfitting.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b4\": 1.0,\n    \"b6\": 1.0,\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b37\": 0.8,\n    \"b38\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b18\": 0.6,\n    \"b30\": 0.6,\n    \"b17\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the data sparsity and cold start issues in recommendation systems by utilizing user review texts. It proposes a new method for computing semantic similarity between user review texts and incorporates this into the Probabilistic Matrix Factorization (PMF) algorithm. The Bayesian Probabilistic Tensor Factorization based on Review Text Semantic Similarity method (RTTF) is introduced to improve the prediction of user preferences for unrated items.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b6\": 0.8,\n    \"b13\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.6,\n    \"b33\": 0.6,\n    \"b17\": 0.6,\n    \"b36\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of data sparsity and cold start in recommendation systems by proposing a novel method that incorporates semantic similarity of user review texts. The algorithm extends Probabilistic Matrix Factorization (PMF) and employs Bayesian treatment to enhance prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.95,\n    \"b6\": 0.9,\n    \"b27\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b3\": 0.55,\n    \"b38\": 0.5\n  }\n}\n```"], "5bdc318017c44a1f58a08780": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of low adaptability and diversity in content-based (CB) e-learning recommender systems. It proposes a recommendation approach based on self-organization theory to improve adaptability and diversity by modeling Learning Objects (LOs) as intelligent entities that can self-organize based on learner interactions.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b14\": 0.85,\n    \"b15\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b18\": 0.65,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the low adaptability and diversity of content-based (CB) recommender systems in e-learning environments. The authors propose a novel recommendation approach based on self-organization theory to address these challenges. The proposed system models Learning Objects (LOs) as intelligent entities capable of self-organizing behaviors to improve adaptability and diversity in recommendations.\",\n  \"Direct Inspiration\": [\n    \"b15\", \n    \"b16\", \n    \"b17\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b7\", \n    \"b18\"\n  ],\n  \"Other Inspiration\": [\n    \"b14\", \n    \"b42\", \n    \"b44\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of adaptability and diversity in content-based (CB) e-learning recommender systems. The proposed approach leverages self-organization theory to model Learning Objects (LOs) as entities that autonomously adapt and organize based on learner interactions. This bottom-up, distributed strategy aims to enhance recommendation diversity and adaptability by considering both learner-LO and LO-LO relationships.\",\n    \"Direct Inspiration\": {\n        \"b14\": 1,\n        \"b2\": 0.9,\n        \"b18\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.8,\n        \"b17\": 0.8,\n        \"b15\": 0.8,\n        \"b7\": 0.8,\n        \"b0\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.7,\n        \"b20\": 0.7,\n        \"b28\": 0.7,\n        \"b24\": 0.7,\n        \"b42\": 0.7,\n        \"b43\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of adaptability and diversity in content-based (CB) e-learning recommender systems. It proposes a novel recommendation approach based on self-organization theory to improve these aspects.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b0\": 0.5,\n    \"b1\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of adaptability and diversity in content-based (CB) e-learning recommender systems. The proposed solution is a recommendation approach based on self-organization theory, which models learning objects (LOs) as intelligent entities capable of self-organizing behaviors to better match learners' dynamic preferences and requirements.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b15\": 0.85,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.65,\n    \"b17\": 0.6\n  }\n}\n```"], "5f3cf98391e011c89f2f178c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of accurately capturing dynamic user behaviors and incorporating contextual information in sequential recommendation systems. The proposed novel method, S3-Rec, utilizes self-supervised learning with mutual information maximization (MIM) to enhance data representations and improve recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.95,\n    \"b9\": 0.90,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b14\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b7\": 0.60,\n    \"b23\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in existing sequential recommendation systems, such as the reliance on item prediction loss and insufficient fusion of context data. To tackle these, the authors propose a novel Self-Supervised learning approach called S3-Rec, leveraging mutual information maximization (MIM) to improve data representation and recommendation performance. They introduce four self-supervised optimization objectives to capture various data correlations.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b9\": 1.0,\n    \"b10\": 1.0,\n    \"b29\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b14\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in capturing the dynamics of sequential user behaviors for recommendation systems, specifically the issues of limited supervision signals and ineffective data representations. The proposed solution, S3-Rec, utilizes self-supervised learning with mutual information maximization (MIM) to improve sequential recommendation tasks. The authors introduce four self-supervised optimization objectives to capture various forms of context information and validate their approach through extensive experiments.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b9\": 1,\n    \"b10\": 1,\n    \"b29\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b7\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b19\": 0.6,\n    \"b20\": 0.7,\n    \"b23\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of effectively capturing the dynamics of sequential user behaviors and incorporating rich contextual information into sequential recommendation systems. The proposed algorithm, S3-Rec, introduces a novel self-supervised learning approach that leverages mutual information maximization (MIM) to enhance data representations and improve recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b9\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b14\": 0.9,\n    \"b10\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.75,\n    \"b23\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving sequential recommendation in online platforms by proposing a novel self-supervised learning approach. The authors identify two primary shortcomings of existing methods: reliance on item prediction loss and insufficient fusion of context data and sequence data. They propose a new method called S3-Rec, which incorporates mutual information maximization (MIM) to enhance data representations through pre-training with self-supervised signals.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b9\": 1,\n    \"b10\": 1,\n    \"b29\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b7\": 0.6,\n    \"b23\": 0.6,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b3\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```"], "5cede10dda562983788ed645": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately characterizing users' dynamic and evolving interests for effective recommendation systems. It proposes a novel bidirectional self-attention model, BERT4Rec, inspired by BERT, to model user behavior sequences. The authors introduce the Cloze task to train the bidirectional model and avoid information leakage, thereby enhancing the representation power of user behavior sequences.\",\n  \"Direct Inspiration\": [\n    \"b5\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b50\",\n    \"b52\"\n  ],\n  \"Other Inspiration\": [\n    \"b6\",\n    \"b14\",\n    \"b58\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately characterizing users' dynamic and evolving interests in recommendation systems by proposing a bidirectional self-attention model (BERT4Rec) that learns user behavior sequences using the Cloze task. This approach incorporates context from both directions in user behavior sequences, overcoming limitations of previous unidirectional models.\",\n  \"Direct Inspiration\": [\"b5\"],\n  \"Indirect Inspiration\": [\"b50\", \"b17\", \"b54\"],\n  \"Other Inspiration\": [\"b6\", \"b13\", \"b14\", \"b56\", \"b58\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately capturing the dynamic and evolving interests of users in recommendation systems. It critiques the limitations of unidirectional models and proposes a bidirectional self-attention model, inspired by BERT, for improved sequential recommendation. The novel approach involves using the Cloze task to avoid information leakage and enhance the learning of bidirectional representations.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b5\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b50\", \"b52\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b6\", \"b13\", \"b14\", \"b56\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inadequacy of unidirectional models in capturing the sequential dynamics and contextual dependencies in user behavior sequences for recommendation systems. The proposed solution is a bidirectional self-attention model, inspired specifically by BERT, to improve representation learning by incorporating context from both directions. This model uses the Cloze task to avoid information leakage and enhance training effectiveness.\",\n  \"Direct Inspiration\": [\"b5\"],\n  \"Indirect Inspiration\": [\"b50\", \"b52\"],\n  \"Other Inspiration\": [\"b6\", \"b13\", \"b14\", \"b56\", \"b58\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately characterizing users' dynamic and evolving interests in recommendation systems. The core contribution is the introduction of a bidirectional self-attention model for sequential recommendation, inspired by the success of BERT in text understanding. The proposed method uses the Cloze task to train the model, avoiding information leakage and leveraging context from both directions.\",\n  \"Direct Inspiration\": [\"b5\"],\n  \"Indirect Inspiration\": [\"b50\", \"b52\"],\n  \"Other Inspiration\": [\"b14\", \"b21\", \"b40\", \"b56\", \"b58\"]\n}\n```"], "5f3e44b791e011c0de1c29bc": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of making effective recommendations by capturing users' preferences from their history, considering the dynamic nature of users' preferences and items' characteristics, as well as the contextual dependency of preferences. The authors propose MEANTIME (MixturE of AtteNTIon mechanisms with Multi-temporal Embeddings) to introduce multiple temporal embeddings and multiple self-attention heads to better encode both absolute and relative positions of user-item interactions.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b18\": 0.8,\n    \"b1\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of capturing users' preferences from their history for sequential recommendations, emphasizing the need to incorporate diverse temporal embeddings to fully capture the patterns in users' behaviors. The proposed algorithm, MEANTIME, introduces multiple temporal embeddings and multiple self-attention heads to better encode both absolute and relative positions of user-item interactions.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b18\": 0.8,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of capturing users' preferences dynamically and contextually for effective sequential recommendation systems. It proposes MEANTIME, a model that utilizes multiple temporal embeddings and self-attention mechanisms to better encode and leverage temporal information in user-item interactions.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b18\": 0.9,\n    \"b9\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b11\": 0.6,\n    \"b26\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b0\": 0.5,\n    \"b8\": 0.5,\n    \"b12\": 0.5,\n    \"b14\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of capturing dynamic and context-dependent user preferences for sequential recommendation systems. It proposes a novel model, MEANTIME, which introduces multiple temporal embeddings and employs multiple self-attention heads to better encode both absolute and relative positions of user-item interactions in a sequence. The authors aim to leverage the diversity of temporal patterns to improve recommendation accuracy.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b18\": 0.8,\n    \"b21\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of capturing dynamic user preferences and item characteristics in sequential recommendation systems. It proposes MEANTIME, which introduces multiple temporal embeddings to better encode user-item interaction sequences and employs multiple self-attention heads to handle each positional embedding separately.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.9,\n        \"b18\": 0.9,\n        \"b21\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b24\": 0.7,\n        \"b36\": 0.7,\n        \"b11\": 0.6,\n        \"b26\": 0.6\n    }\n}\n```"], "5d3ed25a275ded87f97deae9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of spatiotemporal lightning prediction, particularly considering the inconstancy of thunderstorms, deviations in NWP systems, and complex biases in simulations. The authors propose LightNet, a deep neural network-based model that integrates both simulation data and recent weather observation data to improve prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b13\": 0.8,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.75,\n    \"b21\": 0.7,\n    \"b23\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b12\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of spatiotemporal lightning prediction, which is complicated by the inconstancy of thunderstorms, spatial and temporal deviations in NWP systems, and complex error patterns in simulation data. The proposed solution, LightNet, leverages deep neural networks to combine simulation data with recent weather observations for improved prediction accuracy.\",\n  \"Direct Inspiration\": [\"b15\", \"b20\", \"b23\"],\n  \"Indirect Inspiration\": [\"b13\", \"b21\", \"b31\"],\n  \"Other Inspiration\": [\"b10\", \"b12\", \"b25\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of spatiotemporal prediction of lightning by proposing a novel data-driven model called LightNet, which integrates both simulation data and recent weather observation data using deep neural networks. LightNet aims to overcome limitations of existing methods that rely either on simulation data or recent weather observations, resulting in enhanced prediction accuracy without complex feature selection and tuning.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b15\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b21\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of spatiotemporal lightning prediction by proposing LightNet, a deep neural network-based model. The primary challenges include the instability of thunderstorms, biases in numerical weather prediction (NWP) systems, and complex spatiotemporal correspondences. LightNet combines simulation data from NWP systems with recent weather observation data to improve prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b15\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b25\": 0.7,\n    \"b26\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include the inconstancy of thunderstorms, deviations in time and space domain simulations, and varying error patterns across spatiotemporal locations. The proposed algorithm, LightNet, is a data-driven lightning prediction model based on deep neural networks that combines simulation data and recent weather observation data to improve prediction accuracy.\",\n    \"Direct Inspiration\": [\"b15\", \"b23\"],\n    \"Indirect Inspiration\": [\"b13\", \"b20\", \"b21\", \"b31\"],\n    \"Other Inspiration\": []\n}\n```"], "5e09a7e4df1a9c0c4167dacc": ["{\n  \"Summary\": \"The primary challenge of the paper is the accurate forecasting of hourly lightning occurrences over the next 12 hours, leveraging recent lightning observations and numerical weather prediction (NWP) simulations. The proposed solution is an attention-based dual-source spatiotemporal neural network (ADSNet) that incorporates an RNN encoder-decoder structure and a channel-wise attention mechanism to adjust the influences of different simulated parameters during forecasts, improving both accuracy and interpretability.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b21\": 0.85,\n    \"b22\": 0.87,\n    \"b23\": 0.83\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b6\": 0.75,\n    \"b7\": 0.78,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.65,\n    \"b9\": 0.62,\n    \"b10\": 0.6\n  }\n}", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of accurate real-time lightning forecasting by proposing an attention-based dual-source spatiotemporal neural network (ADSNet). This model leverages recent lightning observations and numerical weather prediction (NWP) simulations, utilizing deep learning techniques like RNN, CNN, and attention mechanisms to improve forecasting accuracy.\",\n  \"Direct Inspiration\": [\"b20\", \"b21\", \"b22\", \"b23\", \"b24\"],\n  \"Indirect Inspiration\": [\"b4\", \"b6\", \"b8\", \"b9\", \"b10\"],\n  \"Other Inspiration\": [\"b0\", \"b2\", \"b11\", \"b12\", \"b13\", \"b15\", \"b16\", \"b17\", \"b18\", \"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of accurately forecasting lightning activities, which are crucial due to their destructive potential and the severe weather they often accompany. The authors propose an attention-based dual-source spatiotemporal neural network (ADSNet) that leverages recent lightning observations and numerical weather prediction (NWP) simulations to improve forecast accuracy. Key inspirations include the use of deep learning for spatiotemporal data and attention mechanisms to enhance interpretability and accuracy.\",\n  \"Direct Inspiration\": [\"b20\", \"b21\", \"b22\", \"b23\"],\n  \"Indirect Inspiration\": [\"b4\", \"b6\", \"b7\", \"b24\"],\n  \"Other Inspiration\": [\"b8\", \"b9\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The main challenges addressed in the paper are the rapid changes in thunderstorm development trends that limit the effectiveness of historical trend-based forecasts and the inaccuracies in numerical weather predictions (NWP) due to deviations in partial differential equations. The proposed solution is an attention-based dual-source spatiotemporal neural network (ADSNet) that combines recent lightning observations and NWP simulations to forecast hourly lightning occurrences over the next 12 hours.\",\n    \"Inspirations\": \"The authors draw on the principles of recurrent neural networks (RNN), convolutional neural networks (CNN), and attention mechanisms, particularly channel-wise attention, to improve the accuracy and interpretability of lightning forecasts.\"\n  },\n  \"Direct Inspiration\": [\"b20\", \"b21\", \"b22\", \"b23\", \"b24\"],\n  \"Indirect Inspiration\": [\"b4\", \"b6\", \"b7\", \"b10\", \"b11\", \"b12\"],\n  \"Other Inspiration\": [\"b0\", \"b2\", \"b9\", \"b13\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately forecasting lightning occurrences by proposing an attention-based dual-source spatiotemporal neural network (ADSNet). This model leverages recent lightning observations and numerical weather prediction (NWP) simulations, integrating them through an RNN encoder-decoder structure with an added channel-wise attention mechanism. The goal is to improve the accuracy and interpretability of hourly lightning forecasts over the next 12 hours.\",\n  \"Direct Inspiration\": [\"b20\", \"b21\", \"b22\", \"b23\"],\n  \"Indirect Inspiration\": [\"b0\", \"b2\", \"b4\", \"b6\", \"b7\"],\n  \"Other Inspiration\": [\"b9\", \"b10\", \"b24\"]\n}\n```"], "5e5e18ba93d709897ce2b48e": ["```json\n{\n  \"Summary\": \"The paper tackles two primary challenges in Message-passing neural networks (MPNNs): loss of structural information in node neighborhoods and inability to capture long-range dependencies in disassortative graphs. It proposes a novel geometric aggregation scheme that maps graphs to continuous latent spaces, building structural neighborhoods for aggregation and introducing a bi-level aggregator to update node feature representations.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b14\": 0.9,\n    \"b10\": 0.85,\n    \"b31\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.75,\n    \"b4\": 0.75,\n    \"b18\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.65,\n    \"b11\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses two primary challenges in Message-passing neural networks (MPNNs): the loss of structural information of nodes in neighborhoods due to permutation-invariant aggregation functions and the inability to capture long-range dependencies in disassortative graphs. Inspired by classical neural networks and the concept of network geometry, the authors propose a novel geometric aggregation scheme for graph neural networks, which maps a graph to a continuous latent space and uses geometric relationships to build structural neighborhoods for aggregation. This approach, implemented as Geom-GCN, aims to capture more structural information and aggregate feature representations from distant nodes, achieving state-of-the-art results in transductive learning and node classification on various datasets.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.9,\n        \"b14\": 0.85,\n        \"b18\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.75,\n        \"b21\": 0.7,\n        \"b30\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b28\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Loss of structural information in neighborhoods by existing MPNNs\",\n      \"Inability to capture long-range dependencies in disassortative graphs\"\n    ],\n    \"Inspirations\": [\n      \"Classical neural networks addressing similar limitations using stationarity, locality, and compositionality (Bronstein et al., 2017)\",\n      \"Network geometry bridging the gap between continuous space and graph\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b14\": 1,\n    \"b18\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b21\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper identifies two primary challenges: the loss of structural information in neighborhoods and the inability to capture long-range dependencies in disassortative graphs. The proposed solution is a novel geometric aggregation scheme for graph neural networks (GNNs) that leverages a continuous latent space to build structural neighborhoods and capture long-range dependencies. This scheme is implemented in a model called Geom-GCN, which is validated against state-of-the-art methods on several datasets.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1.0,\n        \"b14\": 1.0,\n        \"b16\": 0.9,\n        \"b18\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8,\n        \"b28\": 0.8,\n        \"b30\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.6,\n        \"b31\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the loss of structural information of nodes in neighborhoods and the inability to capture long-range dependencies in disassortative graphs by existing message-passing neural networks (MPNNs). Inspired by classical neural networks and network geometry, the authors propose a novel geometric aggregation scheme for graph neural networks (Geom-GCN) that maps a graph to a continuous latent space for aggregation. This scheme aims to preserve structural information and capture long-range dependencies effectively.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b14\": 1.0,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b31\": 0.8,\n    \"b21\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b11\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```"], "5cf48a40da56291d582a2f8e": ["```json\n{\n  \"Summary\": \"The paper focuses on understanding why and when graph neural networks (GNNs) work well for vertex classification, proposing a new model named gfNN that emphasizes low-pass filtering of features. The study is inspired by the principles of graph signal processing (GSP) and examines the performance of GNNs under the assumption that input features consist of low-frequency true features and noise.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b19\": 0.7,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve understanding the conditions under which graph neural networks (GNNs) work well for vertex classification, even without training. The paper proposes a new framework, gfNN (graph filter neural network), based on graph signal processing (GSP) to tackle these challenges. The major contributions include verifying Assumption 1 about low-frequency true features and noise, demonstrating that multiplying graph signals with propagation matrices corresponds to low-pass filtering, and proposing gfNN as a faster and more noise-tolerant alternative to GCN and SGC.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b11\": 0.7,\n    \"b18\": 0.7,\n    \"b22\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is understanding the conditions under which graph neural networks (GNNs) perform well for vertex classification, especially in the context of semi-supervised learning. The paper proposes a new baseline framework, gfNN (graph filter neural network), which aims to enhance the efficiency and noise tolerance of GNNs by focusing on low-pass filtering of graph signals. The approach and analysis are inspired by results from graph signal processing.\",\n  \"Direct Inspiration\": [\"b15\", \"b27\", \"b28\"],\n  \"Indirect Inspiration\": [\"b18\", \"b22\", \"b24\"],\n  \"Other Inspiration\": [\"b7\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper are understanding why and when graph neural networks (GNNs) work well for vertex classification and addressing the limitations of baseline models like SGC and GCN under certain conditions, particularly with noisy data and nonlinearly separable features.\",\n    \"inspirations\": \"The paper is inspired by previous work in graph signal processing (GSP) and the need to improve the efficiency and accuracy of GNNs. It introduces a new baseline framework named gfNN and provides theoretical and empirical analyses to support its effectiveness.\"\n  },\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b18\": 0.9,\n    \"b19\": 0.8,\n    \"b22\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b24\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in vertex classification using Graph Neural Networks (GNNs) and proposes a new baseline framework named gfNN (graph filter neural network). The primary inspirations come from the overfitting issues in GCN and the theoretical insights from graph signal processing.\",\n  \"Direct Inspiration\": [\"b15\", \"b27\"],\n  \"Indirect Inspiration\": [\"b18\", \"b22\", \"b28\"],\n  \"Other Inspiration\": [\"b7\", \"b11\"]\n}\n```"], "5e09aa66df1a9c0c416bebf6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of training very deep Graph Convolutional Networks (GCNs) to overcome limitations such as vanishing gradients and limited receptive fields. The authors propose adapting successful concepts from Convolutional Neural Networks (CNNs), including residual connections, dense connections, and dilated convolutions, to GCNs. These adaptations are demonstrated on the task of point cloud semantic segmentation, showing significant improvements in performance.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b12\": 1,\n    \"b50\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.9,\n    \"b18\": 0.8,\n    \"b52\": 0.8,\n    \"b42\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b53\": 0.6,\n    \"b39\": 0.6,\n    \"b23\": 0.6,\n    \"b49\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in this paper is the difficulty in training very deep Graph Convolutional Networks (GCNs) due to issues like the vanishing gradient problem and over-smoothing. The authors propose adapting successful methodologies from Convolutional Neural Networks (CNNs), specifically residual connections, dense connections, and dilated convolutions, to enable the training of deeper GCNs. They showcase their approach by improving state-of-the-art performance on point cloud semantic segmentation tasks.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b12\": 1,\n    \"b50\": 1,\n    \"b41\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b42\": 0.8,\n    \"b52\": 0.8,\n    \"b0\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on addressing the challenges of training deep Graph Convolutional Networks (GCNs) by adapting techniques from Convolutional Neural Networks (CNNs) like residual connections, dense connections, and dilated convolutions. The goal is to overcome issues such as vanishing gradients and limited receptive fields in GCNs, enabling the training of much deeper GCNs which significantly improve performance on tasks like point cloud semantic segmentation.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b12\": 1,\n    \"b50\": 1,\n    \"b41\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b42\": 0.8,\n    \"b52\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b17\": 0.5,\n    \"b25\": 0.5,\n    \"b30\": 0.5,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of training deep Graph Convolutional Networks (GCNs), including vanishing gradients and limited receptive fields. The authors propose adapting successful strategies from Convolutional Neural Networks (CNNs), such as residual connections, dense connections, and dilated convolutions, to GCNs. They demonstrate the effectiveness of these adaptations in the context of point cloud semantic segmentation, achieving state-of-the-art performance.\",\n    \"Direct Inspiration\": {\n        \"b10\": 1,\n        \"b12\": 1,\n        \"b41\": 1,\n        \"b50\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b17\": 0.7,\n        \"b18\": 0.7,\n        \"b42\": 0.7,\n        \"b52\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b26\": 0.5,\n        \"b9\": 0.5,\n        \"b24\": 0.5,\n        \"b14\": 0.5,\n        \"b33\": 0.5,\n        \"b37\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training very deep Graph Convolutional Networks (GCNs) by adapting successful techniques from Convolutional Neural Networks (CNNs) such as residual connections, dense connections, and dilated convolutions. The proposed methods are validated through extensive experiments on point cloud semantic segmentation, achieving a significant performance improvement.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b12\": 1,\n    \"b18\": 1,\n    \"b41\": 1,\n    \"b50\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b52\": 0.8,\n    \"b42\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b53\": 0.5,\n    \"b39\": 0.5,\n    \"b23\": 0.5,\n    \"b49\": 0.5,\n    \"b41\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```"], "5d9edc8347c8f76646042a37": ["```json\n{\n  \"Summary\": \"The paper aims to simplify Graph Convolutional Networks (GCNs) by deriving a simpler linear model called Simple Graph Convolution (SGC). The main challenge addressed is the complexity of GCNs, which can be unnecessary for less demanding applications. The authors propose removing nonlinearities to create a linear transformation that is computationally efficient and interpretable. They demonstrate that SGC achieves comparable or superior performance to GCNs on various tasks.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b10\": 0.7,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b29\": 0.5,\n    \"b48\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the complexity of Graph Convolutional Networks (GCNs) and proposes a simplified linear model called Simple Graph Convolution (SGC). The main challenge is the unnecessary complexity of GCNs for less demanding applications. Inspired by the historic omission of a simpler predecessor, the authors derive SGC by removing nonlinearities between GCN layers and collapsing them into a single linear transformation. The empirical results show that SGC achieves comparable or superior performance to GCNs while being more computationally efficient.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b17\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the complexity of Graph Convolutional Networks (GCNs) and proposes a simplified linear model called Simple Graph Convolution (SGC). The key challenge is the excess complexity of GCNs, which the authors aim to reduce by removing nonlinearities between GCN layers and collapsing the resulting function into a single linear transformation. The paper draws inspiration from historical trends in machine learning and aims to simplify GCNs while retaining their performance.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b7\": 0.5,\n    \"b45\": 0.4,\n    \"b51\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of the complexity of Graph Convolutional Networks (GCNs) by proposing a simplified linear model called Simple Graph Convolution (SGC). The SGC model aims to reduce the complexity by removing nonlinearities between GCN layers and collapsing the resulting function into a single linear transformation, while maintaining or improving performance across various tasks.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Graph Convolutional Networks (GCNs) inherit considerable complexity from their deep learning lineage, which can be burdensome and unnecessary for less demanding applications.\",\n    \"inspirations\": \"The paper is motivated by the need to reduce excess complexity in GCNs by deriving a simpler linear model, referred to as Simple Graph Convolution (SGC), which removes nonlinearities between GCN layers and collapses the resulting function into a single linear transformation.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b23\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b10\", \"b7\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b29\", \"b51\", \"b16\", \"b55\"]\n  }\n}\n```"], "5d9edbfc47c8f7664602eba5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating accurate and comprehensive video captions by incorporating both visual and audio modalities. It introduces three multimodal feature fusion strategies to maximize the benefits of visual-audio resonance information, proposes an audio modality inference module to handle the absence of audio features, and demonstrates improved results using the MSR-VTT and MSVD datasets.\",\n  \"Direct Inspiration\": [\"b10\"],\n  \"Indirect Inspiration\": [\"b7\", \"b20\", \"b22\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating comprehensive video captions by integrating both visual and audio modalities. Traditional methods primarily focus on visual cues, leading to incomplete descriptions. The authors propose three multimodal deep fusion strategies to incorporate audio information, enhancing the accuracy and completeness of video captions. They also introduce a dynamic multimodal feature fusion framework to handle scenarios where the audio modality is absent by inferring audio features from visual data.\",\n  \"Direct Inspiration\": {\n    \"Inspired by [b10]\": 1,\n    \"Inspired by [b7]\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"Memory network [b23]\": 0.8,\n    \"Neural Turing Machine(NTM) [b4]\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"S2VT (sequence to sequence: video to text) model [b20]\": 0.9,\n    \"M3 (multimodal memory modeling) model [b22]\": 0.9\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating accurate video captions by incorporating both visual and audio modalities into the video captioning framework. The authors propose three multimodal deep fusion strategies and introduce an audio modality inference module to handle situations where audio is absent. The primary goal is to utilize resonance information from both modalities to improve the quality of video captions.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b7\": 0.7,\n    \"b20\": 0.9,\n    \"b22\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating accurate video captions by integrating both visual and audio information. The authors propose three multimodal deep fusion strategies to maximize visual-audio resonance information and introduce a dynamic multimodal feature fusion framework to handle cases where audio modality is absent. The primary contributions include multimodal feature fusion strategies and an audio modality inference module, validated through experiments on the MSR-VTT and MSVD datasets.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b20\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b23\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating accurate video captions by incorporating both visual and audio information, as traditional methods relying only on visual cues often result in incomplete sentences. The authors propose three multimodal deep fusion strategies to maximize the benefits of visual-audio resonance information, including exploring cross-modal feature fusion, establishing short-term and long-term dependencies, and a dynamic multimodal feature fusion framework to handle the absence of audio modality. Additionally, an audio modality inference module is introduced to generate audio features when audio is absent.\",\n  \"Direct Inspiration\": [\"b10\"],\n  \"Indirect Inspiration\": [\"b7\", \"b20\", \"b22\"],\n  \"Other Inspiration\": [\"b4\", \"b23\"]\n}\n```"], "5cf48a48da56291d582ab75a": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the insufficiency of existing collaborative filtering (CF) methods in capturing the collaborative signal explicitly in the embedding function. The proposed NGCF model aims to tackle this by incorporating high-order connectivity information from user-item interactions into the embedding process using a neural network-based method inspired by graph neural networks.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b30\": 0.85,\n    \"b36\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b27\": 0.7,\n    \"b38\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b29\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving embeddings in collaborative filtering (CF) by incorporating high-order connectivity information using a graph neural network approach. The proposed method, NGCF, aims to refine embeddings through recursive propagation on an interaction graph, leveraging collaborative signals to enhance recommendation systems.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b36\": 0.9,\n    \"b38\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b25\": 0.7,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving embeddings in collaborative filtering (CF) models by incorporating high-order connectivity information from user-item interactions. The authors propose a Neural Graph Collaborative Filtering (NGCF) method that uses a neural network to propagate embeddings recursively on the interaction graph, inspired by recent developments in graph neural networks (GNNs). The key contributions include refining embeddings with high-order connectivity and demonstrating state-of-the-art performance on public benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b30\": 0.9,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b25\": 0.8,\n    \"b27\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b29\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving embeddings in collaborative filtering (CF) by incorporating high-order connectivity information from user-item interactions. The proposed Neural Graph Collaborative Filtering (NGCF) model uses a neural network to propagate embeddings recursively on a user-item interaction graph. The methodology includes an embedding layer, multiple embedding propagation layers, and a prediction layer, aiming to explicitly inject collaborative signals into the embeddings for better recommendations.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b30\": 0.9,\n        \"b36\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b25\": 0.7,\n        \"b27\": 0.7,\n        \"b13\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.5,\n        \"b38\": 0.5,\n        \"b29\": 0.5,\n        \"b17\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of optimizing embeddings in collaborative filtering (CF) by introducing a neural network method to propagate embeddings using high-order connectivity from user-item interactions. This approach is inspired by recent developments in graph neural networks (GNNs) and aims to improve embedding quality by explicitly incorporating collaborative signals.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b7\", \"b30\", \"b36\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b13\", \"b25\", \"b38\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b19\", \"b27\"]\n    }\n}\n```"], "5b67b45517c44aac1c86078b": ["```json\n{\n  \"Summary\": \"The paper addresses the name disambiguation problem in large-scale online systems like AMiner. The challenges include quantifying similarity between entities, determining the number of persons with the same name, integrating data continuously, and involving human efforts in the loop. The proposed solution includes a global metric and local linkage learning algorithm, an end-to-end model for estimating the number of persons, and leveraging human feedback to improve accuracy.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b41\": 1.0,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.8,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b22\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of name disambiguation in large online systems using the AMiner platform as a basis. It outlines specific challenges such as quantifying similarity between entities, determining the number of persons with the same name, integrating data continuously, and involving human efforts in the loop. The proposed framework includes a global metric and local linkage learning algorithm, an end-to-end model for estimating the number of persons, and methods for incorporating human feedback.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b22\": 0.8,\n    \"b21\": 0.85,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b41\": 0.75,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.65,\n    \"b9\": 0.65,\n    \"b12\": 0.6,\n    \"b33\": 0.55\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of name disambiguation in large-scale online systems like AMiner. The main contributions include a unified framework for similarity quantification using a global metric and local linkage learning algorithm, an end-to-end model for estimating the number of unique persons, and incorporating human feedback for improved disambiguation accuracy. The proposed methods are evaluated on real-world data and show significant improvements over state-of-the-art methods.\",\n    \"Direct Inspiration\": {\n        \"b26\": 1.0,\n        \"b41\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b22\": 0.7,\n        \"b28\": 0.7,\n        \"b31\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b27\": 0.6,\n        \"b23\": 0.5,\n        \"b37\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of name disambiguation in large-scale academic databases, focusing on quantifying similarity between entities, determining the number of individuals sharing the same name, integrating data continuously, and involving human efforts to improve accuracy. The proposed framework combines global metric learning, local linkage learning, and an end-to-end model for cluster size estimation, demonstrating superior performance over state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b31\": 0.85,\n    \"b41\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.75,\n    \"b25\": 0.7,\n    \"b37\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b27\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of name disambiguation in large datasets, particularly in online academic search systems like AMiner. The main challenges include quantifying the similarity between entities from different sources, determining the number of persons with the same name, integrating data continuously, and involving human efforts in the disambiguation process. To tackle these challenges, the authors propose a unified framework that includes a global metric and local linkage learning algorithm, an end-to-end model for estimating the number of persons, and incorporating user feedback to improve accuracy.\",\n  \"Direct Inspiration\": [\"b26\", \"b31\"],\n  \"Indirect Inspiration\": [\"b41\", \"b25\"],\n  \"Other Inspiration\": [\"b10\", \"b22\", \"b23\"]\n}\n```"], "5a260c8117c44a4ba8a30a57": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the lack of theoretical understanding of several popular network embedding methods, such as DeepWalk, LINE, PTE, and node2vec. The paper proposes a unified framework based on matrix factorization to provide theoretical insights into these methods. The authors derive closed-form matrices for each model and introduce a new algorithm, NetMF, to approximate these matrices using SVD, which demonstrates significant performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.9,\n    \"b32\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.85,\n    \"b37\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b19\": 0.7,\n    \"b48\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in understanding and improving network embedding methods such as DeepWalk, LINE, PTE, and node2vec. It proposes a novel algorithm, NetMF, to approximate the implicit matrix factorizations of these methods and demonstrates its superior performance in network embedding tasks.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b37\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b19\": 0.7,\n    \"b48\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding the theoretical mechanisms behind several skip-gram powered network embedding methods, including DeepWalk, LINE, PTE, and node2vec. It proposes a new algorithm, NetMF, to approximate the implicit matrix factorization of these models and demonstrates its improved performance.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1,\n    \"b37\": 0.9,\n    \"b38\": 0.9,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b29\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b19\": 0.6,\n    \"b48\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of network embedding by providing theoretical results for several skip-gram powered network embedding methods (DeepWalk, LINE, PTE, and node2vec), and proposes a new algorithm, NetMF, to approximate the closed form of DeepWalk's implicit matrix, demonstrating its superior performance.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1,\n    \"b38\": 0.9,\n    \"b37\": 0.9,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b19\": 0.7,\n    \"b48\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Lack of theoretical understanding of network embedding models like DeepWalk, LINE, PTE, and node2vec.\",\n      \"Need for efficient computation methods for network embedding.\"\n    ],\n    \"inspirations\": [\n      \"DeepWalk's use of random walks and skip-gram model.\",\n      \"Matrix factorization techniques.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b32\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b38\", \"b37\", \"b17\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b25\"]\n  }\n}\n```"], "5cf48a3cda56291d5829f0d4": ["```json\n{\n  \"Summary\": \"The paper introduces sliced score matching as a scalable and efficient variant of score matching for learning unnormalized statistical models and estimating score functions. The main challenges addressed include the computational difficulty of Hessian trace computation and the inconsistency and variance issues of previous methods. Inspired by the idea of Sliced Wasserstein distance, the authors propose projecting high-dimensional scores onto random directions.\",\n  \"Direct Inspiration\": {\n    \"primary_references\": [\"b12\", \"b19\", \"b22\"]\n  },\n  \"Indirect Inspiration\": {\n    \"secondary_references\": [\"b13\", \"b34\"]\n  },\n  \"Other Inspiration\": {\n    \"additional_references\": [\"b15\", \"b24\", \"b28\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the computational challenges of score matching for learning unnormalized statistical models, proposing a new method called sliced score matching. This new method simplifies the computation by projecting high-dimensional scores onto random directions and matching their projections, making it more scalable and effective for deep and high-dimensional data models.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b19\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.7,\n    \"b35\": 0.6,\n    \"b24\": 0.5,\n    \"b28\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.4,\n    \"b1\": 0.4,\n    \"b22\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the computational difficulty associated with score matching, particularly the computation of the Hessian trace for high-dimensional data and deep neural network models. The proposed solution is sliced score matching, which simplifies the computation by projecting high-dimensional scores onto random directions. The paper also explores its application in density estimation and score estimation for implicit distributions, demonstrating improved performance in variational inference and learning Wasserstein Auto-Encoders.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.9,\n    \"b13\": 0.8,\n    \"b34\": 0.7,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b35\": 0.5,\n    \"b31\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of computational difficulty and intractable partition functions in score matching for unnormalized models. It introduces sliced score matching as a scalable and efficient solution, inspired by the idea of the Sliced Wasserstein distance.\",\n  \"Direct Inspiration\": [\"b22\"],\n  \"Indirect Inspiration\": [\"b12\", \"b19\"],\n  \"Other Inspiration\": [\"b13\", \"b34\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the computational challenges in score matching for unnormalized statistical models, particularly in deep neural networks. It introduces sliced score matching (SSM), which projects high-dimensional scores onto random directions to reduce complexity and improve scalability. The paper also explores applications of SSM in training unnormalized models and estimating score functions, demonstrating improved performance over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b19\": 1,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b1\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```"], "5fc4cfdf91e011abfa2faf94": ["```json\n{\n    \"Summary\": {\n        \"challenges\": \"The paper addresses the practical performance differences between SMLD and DDPM models for score-based generative modeling and the largely unexplored relationship between these approaches.\",\n        \"inspirations\": \"The authors propose a unified framework through the lens of stochastic differential equations (SDEs) to generalize and improve upon existing methods like SMLD and DDPM.\"\n    },\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b40\": 1.0,\n        \"b17\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b34\": 0.7,\n        \"b15\": 0.7,\n        \"b6\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b32\": 0.5,\n        \"b33\": 0.5,\n        \"b42\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in understanding and improving the performance of score-based generative models, specifically SMLD and DDPM. The authors propose a unified framework using stochastic differential equations (SDEs) to generalize and integrate these approaches. They introduce flexible sampling methods, controllable generation techniques, and demonstrate state-of-the-art performance on CIFAR-10 dataset.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b40\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b34\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.7,\n    \"b42\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges in probabilistic generative models, specifically the differences and relationships between Score Matching with Langevin Dynamics (SMLD) and Denoising Diffusion Probabilistic Modeling (DDPM). The authors propose a unified framework through the lens of stochastic differential equations (SDEs) to generalize and improve these approaches. Key contributions include flexible sampling methods, controllable generation, and a unified picture of SMLD and DDPM.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b40\": 1.0,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b13\": 0.6,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the unexplored relationship between Score Matching with Langevin Dynamics (SMLD) and Denoising Diffusion Probabilistic Modeling (DDPM) by unifying and generalizing both approaches through the lens of stochastic differential equations (SDEs). It proposes a new framework that uses a continuum of noise distributions, introduces new sampling methods, and demonstrates improvements in sample quality and flexibility.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b1\", \"b17\", \"b40\"],\n    \"confidence_score\": [0.95, 0.9, 0.9]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b34\", \"b15\"],\n    \"confidence_score\": [0.8, 0.8]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b6\", \"b32\", \"b33\"],\n    \"confidence_score\": [0.7, 0.7, 0.7]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the open questions in score-based generative models by unifying and generalizing both Score Matching with Langevin Dynamics (SMLD) and Denoising Diffusion Probabilistic Models (DDPM) through stochastic differential equations (SDEs). The authors propose novel sampling methods, including Predictor-Corrector (PC) samplers and deterministic samplers based on probability flow ordinary differential equations (ODEs). These innovations enable flexible sampling, controllable generation, and improved performance over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b17\": 1.0,\n    \"b40\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b34\": 0.8,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```"], "5aed14e217c44a4438159868": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in training Neural Machine Translation (NMT) systems, such as determining optimal stopping criteria, handling non-deterministic training processes, and evaluating translation quality reliably. It proposes reporting full learning curves instead of single scores and introduces specific tools and scripts to aid in this evaluation. The paper also discusses preprocessing techniques and hyper-parameter settings that are critical to the success of NMT models.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b18\": 0.9,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b1\": 0.6,\n    \"b22\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.4,\n    \"b7\": 0.3,\n    \"b2\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in neural machine translation (NMT) such as non-deterministic training, lack of clear stopping criteria, and the importance of correct hyper-parameter settings. The authors propose reporting full learning curves rather than single scores to mitigate premature judgments and implemented several scripts to support more reliable evaluations.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.75,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.65,\n    \"b18\": 0.6,\n    \"b1\": 0.55,\n    \"b2\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in neural machine translation (NMT), particularly focusing on the non-deterministic nature of NMT training, the lack of reliable stopping criteria, and the importance of reporting full learning curves instead of single scores. The authors propose the use of full learning curves to allow for a more transparent evaluation of model performance over time.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b18\": 0.8,\n    \"b23\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b22\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in neural machine translation (NMT) related to neural network architectures, training data sensitivity, hyper-parameters, and the non-deterministic nature of training. It proposes reporting full learning curves instead of single scores to evaluate NMT models more reliably.\",\n  \"Direct Inspiration\": [\"b18\", \"b14\", \"b23\"],\n  \"Indirect Inspiration\": [\"b3\", \"b7\"],\n  \"Other Inspiration\": [\"b1\", \"b2\", \"b22\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in neural machine translation (NMT) related to training instability, overfitting, and the determination of stopping criteria. The authors propose reporting full learning curves instead of single scores to provide a more comprehensive evaluation of model performance. They also implement several tools and scripts to improve the evaluation process within Tensor2Tensor (T2T) and focus on preprocessing strategies to optimize training data.\",\n    \"Direct Inspiration\": {\n        \"b14\": 0.9,\n        \"b18\": 0.85,\n        \"b23\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.7,\n        \"b1\": 0.65,\n        \"b2\": 0.6,\n        \"b22\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.5,\n        \"b7\": 0.55\n    }\n}\n```"], "5c2c7a9217c44a4e7cf3189c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of multimodal fusion and data scarcity in automated deception detection (ADD). It proposes a novel face-focused cross-stream network (FFCSN) that incorporates face detection and correlation learning between facial expressions and body motions. Additionally, it introduces meta learning and adversarial learning to tackle the issue of insufficient training data.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1,\n    \"b36\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.9,\n    \"b51\": 0.9,\n    \"b24\": 0.9,\n    \"b10\": 0.8,\n    \"b22\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.7,\n    \"b39\": 0.6,\n    \"b5\": 0.6,\n    \"b34\": 0.6,\n    \"b49\": 0.5,\n    \"b16\": 0.5,\n    \"b0\": 0.5,\n    \"b8\": 0.5,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in automated deception detection (ADD): multimodal fusion and data scarcity. It proposes a novel face-focused cross-stream network (FFCSN) that integrates face detection and correlation learning to address the fusion issue, and employs meta learning and adversarial learning to mitigate the data scarcity problem.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b36\": 1.0,\n    \"b46\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.8,\n    \"b51\": 0.8,\n    \"b10\": 0.75,\n    \"b22\": 0.75,\n    \"b23\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b39\": 0.7,\n    \"b34\": 0.7,\n    \"b24\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in automated deception detection (ADD): multimodal fusion and data scarcity. It proposes a novel face-focused cross-stream network (FFCSN) to effectively fuse facial expressions and body motions, and uses meta learning and adversarial learning to tackle the data scarcity problem.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b36\": 1.0,\n    \"b46\": 0.9,\n    \"b38\": 0.9,\n    \"b51\": 0.9,\n    \"b10\": 0.9,\n    \"b22\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b19\": 0.8,\n    \"b24\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b49\": 0.7,\n    \"b39\": 0.7,\n    \"b5\": 0.7,\n    \"b32\": 0.7,\n    \"b50\": 0.7,\n    \"b30\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses two primary challenges in automated deception detection (ADD): multimodal fusion and data scarcity. It proposes a novel face-focused cross-stream network (FFCSN) to effectively fuse visual cues, specifically facial expressions and body motions, and introduces meta learning and adversarial learning to improve model training with limited data.\",\n    \"Direct Inspiration\": {\n        \"b35\": 1,\n        \"b36\": 1,\n        \"b46\": 0.9,\n        \"b24\": 0.8,\n        \"b38\": 0.8,\n        \"b51\": 0.8,\n        \"b10\": 0.8,\n        \"b22\": 0.8,\n        \"b23\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b19\": 0.7,\n        \"b39\": 0.6,\n        \"b34\": 0.6,\n        \"b8\": 0.6,\n        \"b37\": 0.6,\n        \"b45\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b57\": 0.5,\n        \"b32\": 0.5,\n        \"b50\": 0.5,\n        \"b30\": 0.5,\n        \"b17\": 0.5,\n        \"b53\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in automated deception detection (ADD): the need for effective multimodal fusion and the scarcity of high-quality training data. It proposes a novel face-focused cross-stream network (FFCSN) for joint deep feature learning from facial expressions and body motions, along with meta learning and adversarial learning strategies to cope with data scarcity.\",\n  \"Direct Inspiration\": [\"b35\", \"b36\", \"b38\", \"b51\", \"b24\", \"b10\", \"b22\", \"b23\"],\n  \"Indirect Inspiration\": [\"b46\", \"b57\", \"b32\", \"b50\", \"b30\"],\n  \"Other Inspiration\": [\"b39\", \"b5\", \"b34\", \"b49\"]\n}\n```"], "5f06e5e591e0117f54657c19": ["```json\n{\n    \"Summary\": \"The paper introduces Nouveau VAE (NVAE), a deep hierarchical VAE designed to generate high-quality images. The primary challenges addressed by NVAE include designing expressive neural networks specifically for VAEs, stabilizing training with large hierarchical groups and image sizes, and managing memory requirements. Key innovations include the use of depthwise convolutions, residual parameterization of the approximate posteriors, spectral regularization, and practical solutions for memory reduction.\",\n    \"Direct Inspiration\": {\n        \"b3\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b35\": 0.8,\n        \"b39\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.5,\n        \"b31\": 0.5,\n        \"b44\": 0.6,\n        \"b43\": 0.5,\n        \"b55\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in designing deep hierarchical VAEs: (i) creating expressive neural networks specifically for VAEs, and (ii) scaling up the training to handle large images while maintaining stability. The authors propose NVAE, a novel VAE architecture featuring depthwise convolutions, residual parameterization, and spectral regularization to achieve state-of-the-art results in large image generation.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.8,\n    \"b44\": 0.7,\n    \"b55\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Variational Autoencoders (VAEs) by designing a novel architecture called Nouveau VAE (NVAE). The main challenges include modeling long-range correlations in data, stabilizing training for deep hierarchical VAEs, and reducing the memory burden during training. The proposed NVAE employs depthwise convolutions, novel residual parameterizations, and spectral regularization to achieve state-of-the-art results.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.95,\n    \"b35\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.75,\n    \"b5\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.65,\n    \"b29\": 0.65,\n    \"b43\": 0.60,\n    \"b44\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenges of improving the design of neural network architectures for Variational Autoencoders (VAEs), particularly for generating high-quality large images. The proposed solution, Nouveau VAE (NVAE), incorporates depthwise convolutions, residual parameterization of the approximate posterior, and spectral regularization to enhance performance and stabilize training.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b35\": 0.9,\n    \"b37\": 0.85,\n    \"b44\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b31\": 0.7,\n    \"b39\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b45\": 0.55,\n    \"b46\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in training Variational Autoencoders (VAEs) by focusing on architectural improvements and stabilization techniques. The proposed model, Nouveau VAE (NVAE), incorporates depthwise convolutions, batch normalization, residual parameterization, and spectral regularization to achieve state-of-the-art results on high-resolution images.\",\n  \"Direct Inspiration\": [\n    \"b3\",\n    \"b35\",\n    \"b43\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b18\",\n    \"b31\",\n    \"b44\",\n    \"b50\"\n  ],\n  \"Other Inspiration\": [\n    \"b45\",\n    \"b55\"\n  ]\n}\n```"], "5d4d46fb3a55acff992fdb76": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Natural scarcity of negative signal in implicit feedback data.\",\n      \"Incompleteness of negative preference in exposure data.\",\n      \"Difficulty of optimizing the negative sampler due to non-differentiable objective function.\"\n    ],\n    \"inspirations\": [\n      \"Designing a quality negative sampler for implicit recommender models.\",\n      \"Generating exposure-alike negative instances.\",\n      \"Using reinforcement learning to optimize non-differentiable objectives.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b4\": 0.8,\n    \"b7\": 0.85,\n    \"b13\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.6,\n    \"b10\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of negative sampling in implicit recommender systems, specifically focusing on the scarcity of negative signals and optimizing the generation of negative instances. It proposes a novel Reinforced Negative Sampler (RNS) that combines adversarial training and a feature matching technique to generate hard and real negative instances, leveraging exposure data effectively.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b6\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b7\": 0.6,\n    \"b12\": 0.6,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b10\": 0.5,\n    \"b13\": 0.4,\n    \"b15\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of negative sampling in implicit feedback-based recommender systems, proposing a novel Reinforced Negative Sampler (RNS) that integrates exposure data to generate more reliable negative samples. The key challenges include the incompleteness of negative preference in exposure data and the difficulty of optimizing the negative sampler. The proposed RNS model uses adversarial training and feature matching to generate hard and real negative instances, improving the performance of recommender systems.\",\n  \"Direct Inspiration\": [\"b1\", \"b4\", \"b7\"],\n  \"Indirect Inspiration\": [\"b0\", \"b2\", \"b10\"],\n  \"Other Inspiration\": [\"b12\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating reliable negative samples for implicit feedback-based recommender systems by proposing a novel Reinforced Negative Sampler (RNS). This method leverages adversarial training and exposure-matching techniques to create high-quality negative instances, overcoming issues of negative preference incompleteness and non-differentiable objective functions.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b4\": 0.8,\n    \"b7\": 0.8,\n    \"b10\": 0.7,\n    \"b12\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.6,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating reliable negative samples for implicit feedback-based recommender systems. It proposes a novel Reinforced Negative Sampler (RNS) that generates hard and real negative instances by leveraging adversarial training and feature matching techniques. The sampler improves the quality of negative samples, which in turn enhances the performance of the recommender model.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b4\": 0.85,\n    \"b10\": 0.8,\n    \"b12\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b7\": 0.65,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.55,\n    \"b15\": 0.5\n  }\n}\n```"], "5a9cb60d17c44a376ffb3c6d": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in sequential recommendation systems, particularly the inability of existing models to capture union-level sequential patterns and skip behaviors. It proposes a ConvolutionAl Sequence Embedding Recommendation Model (Caser) that leverages convolutional neural networks (CNN) to capture both point-level and union-level sequential patterns, as well as skip behaviors.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b11\": 1.0,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b18\": 0.7,\n    \"b29\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of previous top-N sequential recommendation methods, specifically their inability to model union-level sequential patterns and skip behaviors. The proposed model, Caser, leverages convolutional neural networks to capture these complex patterns and offers several advantages over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing top-N sequential recommendation systems, particularly their inability to model union-level sequential patterns and skip behaviors. The authors propose a novel model, ConvolutionAl Sequence Embedding Recommendation (Caser), which leverages convolutional filters from Convolutional Neural Networks to capture various types of sequential patterns and integrates both user general preferences and sequential patterns in a unified framework.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b5\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b10\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b18\": 0.6,\n    \"b16\": 0.6,\n    \"b22\": 0.6,\n    \"b35\": 0.6,\n    \"b24\": 0.6,\n    \"b28\": 0.6,\n    \"b25\": 0.5,\n    \"b30\": 0.5,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of previous top-N sequential recommendation models, particularly their inability to capture union-level sequential patterns and skip behaviors. The proposed model, Caser, utilizes convolutional neural networks (CNN) to capture both point-level and union-level sequential patterns, as well as skip behaviors, in a unified framework. The model's design includes embedding look-up, convolutional layers, and fully-connected layers to achieve better recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b20\": 0.8,\n    \"b10\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b29\": 0.6,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating both general user preferences and sequential patterns in recommender systems, highlighting the limitations of existing models such as Markov chains and FPMC. The authors propose Caser, a Convolutional Sequence Embedding Recommendation Model, which uses convolutional neural networks to capture both point-level and union-level sequential patterns, including skip behaviors.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b15\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```"], "5a9cb65d17c44a376ffb83f3": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of open-domain question answering over knowledge bases (KBQA), particularly focusing on multi-relation questions which require reasoning over multiple fact triples. The authors propose a novel Interpretable Reasoning Network (IRN) that provides a traceable and interpretable reasoning process for answering complex questions by predicting relations and entities at each reasoning hop. This approach allows for visualizing a complete reasoning path, facilitating reasoning analysis and failure diagnosis.\",\n    \"Direct Inspiration\": {\n        \"b16\": 0.9,\n        \"b26\": 0.85,\n        \"b1\": 0.8,\n        \"b2\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.75,\n        \"b10\": 0.7,\n        \"b13\": 0.7,\n        \"b29\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.65,\n        \"b18\": 0.6,\n        \"b19\": 0.6,\n        \"b27\": 0.6,\n        \"b32\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Open-domain question answering over knowledge bases (KBQA) is challenging due to the variety and complexity of language and knowledge.\",\n      \"Multi-relation question answering requires reasoning over multiple fact triples, which is yet to be adequately addressed.\"\n    ],\n    \"inspirations\": [\n      \"The proposed Interpretable Reasoning Network (IRN) aims to make multi-relation question answering interpretable and traceable.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\n      \"b5\"\n    ]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\n      \"b7\",\n      \"b13\",\n      \"b29\",\n      \"b16\",\n      \"b26\"\n    ]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\n      \"b1\",\n      \"b2\"\n    ]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is addressing multi-relation question answering (QA) over knowledge bases (KB), which requires reasoning over multiple fact triples. The authors propose an Interpretable Reasoning Network (IRN) that offers a hop-by-hop reasoning process that is interpretable and traceable, enabling visualization of the reasoning path for complex questions.\",\n    \"Direct Inspiration\": {\n        \"b5\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.9,\n        \"b27\": 0.8,\n        \"b18\": 0.8,\n        \"b32\": 0.8,\n        \"b1\": 0.75,\n        \"b2\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.7,\n        \"b28\": 0.7,\n        \"b21\": 0.7,\n        \"b30\": 0.65,\n        \"b31\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the complexity of open-domain question answering over knowledge bases, specifically focusing on multi-relation QA which requires reasoning over multiple fact triples. The proposed algorithm, Interpretable Reasoning Network (IRN), aims to equip QA systems with the reasoning ability to answer multi-relation questions by designing an interpretable reasoning process.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 0.9,\n    \"b16\": 0.95,\n    \"b21\": 0.85,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b13\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.65,\n    \"b19\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of multi-relation question answering over knowledge bases (KBQA), which requires reasoning over multiple fact triples. The authors propose a novel Interpretable Reasoning Network (IRN) designed to provide interpretability in the reasoning process while answering complex questions by predicting relations and entities at each hop, making the process traceable and observable.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b13\": 0.7,\n    \"b29\": 0.7,\n    \"b27\": 0.6,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6\n  }\n}\n```"], "5aed14d617c44a4438158f7c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of automated medical image segmentation, specifically targeting the inefficiencies of cascaded CNN frameworks in handling large inter-patient variations. The proposed solution is the implementation of attention gates (AGs) within standard CNN models, which allows for the automatic focus on target structures, reducing computational overhead and improving model sensitivity and accuracy for dense label predictions.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b10\", \"b31\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b1\", \"b28\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b13\", \"b25\", \"b26\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the need for accurate and efficient automated medical image segmentation, particularly for tasks such as CT pancreas segmentation, which is difficult due to low tissue contrast and large variability in organ shape and size. The authors propose a novel solution involving attention gates (AGs) integrated into a U-Net architecture, which enhances model sensitivity and accuracy without the need for multiple CNN models. The proposed method significantly improves dense label predictions by focusing on relevant features and suppressing irrelevant ones, thereby addressing the computational inefficiencies and redundancy of previous multi-stage cascaded CNN architectures.\",\n  \n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b31\": 0.9,\n    \"b1\": 0.9\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b28\": 0.7,\n    \"b32\": 0.7,\n    \"b35\": 0.7\n  },\n  \n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b21\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies of multi-stage CNN architectures in medical image segmentation, proposing attention gates (AGs) to improve model sensitivity and accuracy without additional computational overhead. This novel approach, integrated into a standard U-Net architecture, is particularly effective for dense predictions and small organ segmentation.\",\n  \"Direct Inspiration\": [\"b10\"],\n  \"Indirect Inspiration\": [\"b0\", \"b1\", \"b31\", \"b32\"],\n  \"Other Inspiration\": [\"b25\", \"b26\", \"b13\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the excessive and redundant use of computational resources and model parameters in multi-stage cascaded CNN frameworks for medical image segmentation. The authors propose a novel solution, Attention Gates (AGs), which can be incorporated into CNN models to automatically focus on target structures without additional supervision, thus improving model sensitivity and accuracy for dense label predictions while reducing computational overhead.\",\n    \"Direct Inspiration\": {\n        \"b10\": 1,\n        \"b28\": 0.9,\n        \"b31\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b32\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.6,\n        \"b13\": 0.6,\n        \"b25\": 0.6,\n        \"b26\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of automated medical image segmentation with a focus on reducing computational redundancy and improving accuracy. The proposed solution is the integration of Attention Gates (AGs) into CNN models, specifically within the U-Net architecture, to enhance focus on relevant features without additional computational overhead. This method is tested on challenging CT pancreas segmentation tasks, demonstrating improved performance over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b0\": 0.9,\n    \"b32\": 0.85,\n    \"b31\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.75,\n    \"b26\": 0.75,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.65,\n    \"b28\": 0.6,\n    \"b35\": 0.55\n  }\n}\n```"], "5b8c9f5317c44af36f8b775c": ["```json\n{\n  \"Summary\": \"The paper addresses the need for more accurate medical image segmentation by proposing UNet++, a new segmentation architecture based on nested and dense skip connections. The key challenges include the high accuracy required for medical images and the inadequacy of existing segmentation models to meet these needs. The proposed model aims to reduce the semantic gap between feature maps from the encoder and decoder sub-networks and employs deep supervision to improve accuracy.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b6\": 1.0,\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurate medical image segmentation, which requires higher precision than natural image segmentation. The authors propose UNet++, a new segmentation architecture that improves on U-Net by incorporating nested and dense skip connections, and deep supervision. These modifications aim to better capture fine-grained details and reduce the semantic gap between feature maps from the encoder and decoder sub-networks, resulting in more accurate segmentation.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b6\": 0.8,\n    \"b1\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.6,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of accurate medical image segmentation, proposing a new architecture called UNet++ that features nested and dense skip connections and deep supervision to improve fine-grained detail recovery in segmentation tasks.\",\n    \"Direct Inspiration\": [\"b4\", \"b6\", \"b1\"],\n    \"Indirect Inspiration\": [\"b2\", \"b3\"],\n    \"Other Inspiration\": [\"b7\", \"b8\"]\n}\n```", "```json\n{\n    \"Summary\": \"The main challenge addressed in the paper is the need for more accurate segmentation of medical images, which requires higher precision than segmenting natural images due to the clinical implications of minor segmentation errors. The proposed solution, UNet++, enhances the U-Net architecture by introducing nested and dense skip connections, which help to reduce the semantic gap between encoder and decoder feature maps, and by incorporating deep supervision, which allows the model to operate in both accurate and fast modes.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0,\n        \"b6\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8,\n        \"b2\": 0.6,\n        \"b3\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.5,\n        \"b7\": 0.9,\n        \"b8\": 0.9\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving high-accuracy segmentation in medical imaging, which is crucial for clinical applications. The proposed solution, UNet++, introduces nested and dense skip connections to enhance the fusion of high-resolution feature maps from the encoder network with semantically rich feature maps from the decoder network. This design aims to bridge the semantic gap and improve the segmentation accuracy over traditional models like U-Net and wide U-Net. The architecture also incorporates deep supervision to further refine segmentation results.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b6\": 0.9,\n    \"b1\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b3\": 0.65,\n    \"b5\": 0.6\n  }\n}\n```"], "5a9cb66717c44a376ffb8afd": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of medical image segmentation, particularly data scarcity and class imbalance. It proposes two novel models, RU-Net and R2U-Net, inspired by deep residual models, RCNN, and U-Net, to enhance segmentation tasks in different medical imaging modalities.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b11\": 1.0,\n    \"b40\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b9\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.6,\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n   \"Summary\":\"The paper addresses challenges in semantic segmentation for medical imaging, specifically focusing on issues like data scarcity and class imbalance. It proposes two new models, RU-Net and R2U-Net, inspired by existing architectures like U-Net, deep residual models, and RCNNs, to improve segmentation performance with fewer network parameters.\",\n   \"Direct Inspiration\":[\n      \"b6\",\n      \"b11\",\n      \"b40\"\n   ],\n   \"Indirect Inspiration\":[\n      \"b9\",\n      \"b12\",\n      \"b13\",\n      \"b42\"\n   ],\n   \"Other Inspiration\":[\n      \"b1\",\n      \"b4\",\n      \"b19\"\n   ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of medical image segmentation, particularly data scarcity and class imbalance. It introduces two novel models, RU-Net and R2U-Net, inspired by deep residual models, RCNN, and U-Net, to improve segmentation performance with efficient parameter usage.\",\n  \"Direct Inspiration\": [\"b6\", \"b11\", \"b40\"],\n  \"Indirect Inspiration\": [\"b12\", \"b13\", \"b42\"],\n  \"Other Inspiration\": [\"b1\", \"b9\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of medical image segmentation, particularly issues such as data scarcity and class imbalance. It proposes two modified segmentation models, RU-Net and R2U-Net, which build on the architectures of U-Net, recurrent convolutional neural networks (RCNN), and deep residual networks to improve performance with fewer network parameters.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b11\": 1.0,\n    \"b40\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include data scarcity, class imbalance, and the need for efficient segmentation models for medical imaging. The paper introduces two novel models, RU-Net and R2U-Net, which utilize recurrent convolutional networks and residual connections to improve segmentation performance while maintaining efficient network parameter usage.\",\n  \"Direct Inspiration\": [\"b6\", \"b11\", \"b40\"],\n  \"Indirect Inspiration\": [\"b1\", \"b12\", \"b13\"],\n  \"Other Inspiration\": [\"b4\", \"b9\", \"b21\"]\n}\n```"], "5ec49a639fced0a24b4de7d4": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatically verifying the integrity of textual content to prevent the spread of false information. The proposed solution, Kernel Graph Attention Network (KGAT), utilizes a combination of kernel-based attentions for evidence propagation and selection in a graph structure to improve fact verification accuracy.\",\n  \"Direct Inspiration\": [\"b34\"],\n  \"Indirect Inspiration\": [\"b9\", \"b13\", \"b24\", \"b27\"],\n  \"Other Inspiration\": [\"b25\", \"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of fact verification to combat false information online. It introduces the Kernel Graph Attention Network (KGAT), which enhances evidence retrieval and reasoning using kernels and graph attention mechanisms. KGAT improves upon previous methods, particularly in scenarios requiring multiple evidence pieces.\",\n  \"Direct Inspiration\": [\"b34\"],\n  \"Indirect Inspiration\": [\"b9\", \"b24\"],\n  \"Other Inspiration\": [\"b25\", \"b27\", \"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge of the paper is to automatically verify the integrity of textual contents to prevent the spread of fake news and other false information. The authors propose Kernel Graph Attention Network (KGAT) which utilizes two sets of kernels for evidence propagation and selection, achieving improved performance in fact verification tasks.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.9,\n    \"b27\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b16\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatically verifying the integrity of textual contents to combat the spread of false information. It introduces the Kernel Graph Attention Network (KGAT), which employs a novel approach using graph-based reasoning with kernel attentions for evidence propagation and selection. The proposed method significantly outperforms previous models in fact verification tasks, especially those requiring reasoning over multiple pieces of evidence.\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.95,\n    \"b27\": 0.85,\n    \"b4\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.75,\n    \"b9\": 0.70,\n    \"b24\": 0.70,\n    \"b13\": 0.65,\n    \"b16\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.60,\n    \"b31\": 0.55,\n    \"b1\": 0.55\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the significant challenge of combating false online information by proposing a Kernel Graph Attention Network (KGAT) for fact verification. KGAT improves upon existing models by constructing an evidence graph and using kernel-based attentions to enhance multi-evidence reasoning and accurate evidence selection.\",\n    \"Direct Inspiration\": [\"b34\", \"b27\"],\n    \"Indirect Inspiration\": [\"b13\", \"b5\", \"b9\"],\n    \"Other Inspiration\": [\"b25\"]\n}\n```"], "5cf48a2cda56291d5828e868": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing the need for labeled data in training deep neural networks, particularly for tasks where obtaining labeled data is expensive or involves private information. The authors propose MixMatch, a semi-supervised learning (SSL) algorithm that unifies entropy minimization, consistency regularization, and traditional regularization into a single loss term. The algorithm leverages data augmentation, label guessing, and MixUp techniques to achieve state-of-the-art results on standard image benchmarks and improve differentially private learning.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b17\": 0.9,\n    \"b27\": 0.9,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b39\": 0.7,\n    \"b43\": 0.7,\n    \"b44\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b29\": 0.5,\n    \"b45\": 0.5,\n    \"b46\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces MixMatch, a semi-supervised learning (SSL) algorithm that unifies the dominant approaches in SSL, such as entropy minimization, consistency regularization, and traditional regularization. The key challenge addressed is the reliance on large labeled datasets, which are expensive and time-consuming to obtain. MixMatch aims to leverage unlabeled data effectively by combining these different SSL methods into a single loss function. The algorithm shows state-of-the-art results on standard image benchmarks and provides improved privacy guarantees in differentially private learning.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1,\n    \"b17\": 1,\n    \"b27\": 1,\n    \"b43\": 1,\n    \"b44\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.7,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces MixMatch, a semi-supervised learning (SSL) algorithm that unifies three dominant approaches: entropy minimization, consistency regularization, and traditional regularization. MixMatch aims to address the challenges of leveraging unlabeled data to improve model generalization and reduce dependency on labeled data. The algorithm demonstrates state-of-the-art results on standard image benchmarks and improves differentially private learning outcomes.\",\n    \"Direct Inspiration\": [\"b17\", \"b27\", \"b30\", \"b43\"],\n    \"Indirect Inspiration\": [\"b5\", \"b24\", \"b39\", \"b46\"],\n    \"Other Inspiration\": [\"b35\", \"b44\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the difficulty and expense of obtaining labeled data for training large, deep neural networks. The proposed algorithm, MixMatch, aims to leverage unlabeled data in semi-supervised learning by introducing a unified loss term that combines entropy minimization, consistency regularization, and traditional regularization techniques.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b27\": 0.8,\n    \"b30\": 0.85,\n    \"b43\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b39\": 0.7,\n    \"b46\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.6,\n    \"b44\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper are the difficulties associated with obtaining labeled data, especially in domains like medical tasks where data collection is expensive and time-consuming. The scarcity of labeled data necessitates efficient semi-supervised learning (SSL) approaches that can leverage unlabeled data effectively.\",\n    \"inspirations\": \"The paper introduces MixMatch, a novel SSL algorithm that unifies three dominant SSL approaches: entropy minimization, consistency regularization, and traditional regularization. The goal is to reduce entropy, maintain consistency, and avoid overfitting while leveraging unlabeled data.\"\n  },\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b17\": 0.9,\n    \"b27\": 0.9,\n    \"b30\": 0.9,\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b44\": 0.7,\n    \"b46\": 0.7\n  }\n}\n```"], "57a4e921ac44365e35c99004": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of gender bias in word embeddings, specifically focusing on the popular word2vec embeddings. The authors propose methods to identify and reduce these biases while maintaining the utility of the embeddings. The inspiration for the work includes previous studies on biases in language, the use of Implicit Association Tests to uncover biases, and prior efforts to improve word embeddings and achieve fairness in machine learning algorithms.\",\n\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b33\": 0.9,\n    \"b13\": 0.8\n  },\n\n  \"Indirect Inspiration\": {\n    \"b26\": 0.7,\n    \"b14\": 0.7,\n    \"b9\": 0.6\n  },\n\n  \"Other Inspiration\": {\n    \"b23\": 0.8,\n    \"b24\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of gender bias in word embeddings, illustrating how such biases are present in widely used embeddings like word2vec. The authors propose geometric methods to identify and quantify direct and indirect gender biases and introduce debiasing algorithms to mitigate these biases while preserving the utility of the embeddings.\",\n  \"Direct Inspiration\": [\"b2\", \"b26\", \"b23\", \"b24\"],\n  \"Indirect Inspiration\": [\"b13\", \"b14\", \"b29\", \"b31\"],\n  \"Other Inspiration\": [\"b9\", \"b12\", \"b27\", \"b35\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper identifies and addresses gender biases present in word embeddings, particularly those in the popular word2vec model. The authors propose methods to quantify these biases and introduce algorithms to debias the embeddings while retaining their useful properties.\",\n    \"Direct Inspiration\": [\"b23\", \"b24\", \"b26\"],\n    \"Indirect Inspiration\": [\"b31\", \"b25\"],\n    \"Other Inspiration\": [\"b2\", \"b13\", \"b14\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of gender bias in word embeddings, particularly focusing on how these biases manifest in the geometry of word embeddings and how they can be quantitatively measured and mitigated. The primary inspiration is to develop methods to debias these word embeddings while maintaining their utility in various applications. The authors use the w2vNEWS embedding and propose a method to identify the gender subspace and reduce bias from gender-neutral words.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b13\": 0.85,\n        \"b14\": 0.8,\n        \"b23\": 0.9,\n        \"b24\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b26\": 0.7,\n        \"b29\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b31\": 0.6,\n        \"b25\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of gender bias in word embeddings, particularly those derived from word2vec trained on Google News corpus. The authors quantitatively demonstrate the presence of biases in these embeddings and propose methods to debias them while maintaining their utility. The paper highlights both direct and indirect gender biases and introduces algorithms to reduce these biases.\",\n    \"Direct Inspiration\": {\n        \"b23\": 0.9,\n        \"b24\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.8,\n        \"b14\": 0.7,\n        \"b26\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5,\n        \"b29\": 0.5,\n        \"b31\": 0.4\n    }\n}\n```"], "5c04967517c44a2c74709354": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of learning-to-rank (LTR) from biased implicit feedback (e.g., clicks) by proposing a new Contextual Position-Based Model (CPBM). The CPBM extends the traditional Position-Based Model (PBM) to take into account arbitrary context vectors through a deep network, making it more accurate in estimating examination bias for different queries and contexts.\",\n    \"Direct Inspiration\": [\"b2\", \"b19\"],\n    \"Indirect Inspiration\": [\"b27\", \"b3\"],\n    \"Other Inspiration\": [\"b9\", \"b26\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of biased implicit feedback in learning-to-rank (LTR) by proposing a Contextual Position-Based Model (CPBM) that models examination bias dependent on arbitrary context vectors using a deep network. This approach overcomes the limitations of existing models restricted to the Position-Based Model (PBM) and treating all queries uniformly.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1,\n        \"b19\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b27\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.7,\n        \"b26\": 0.75,\n        \"b23\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of bias in implicit feedback for learning-to-rank (LTR) systems and proposes a novel Contextual Position-Based Model (CPBM) to improve accuracy by considering query context through a deep network. The method avoids randomized interventions by using implicit interventions from log data.\",\n  \"Direct Inspiration\": [\"b2\", \"b19\", \"b27\"],\n  \"Indirect Inspiration\": [\"b0\", \"b1\", \"b3\", \"b26\"],\n  \"Other Inspiration\": [\"b9\", \"b17\", \"b22\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a novel Contextual Position-Based Model (CPBM) to address the limitations of the Position-Based Model (PBM) in learning-to-rank (LTR) from implicit feedback. The CPBM incorporates context-dependent examination propensity using deep networks, and introduces an AllPairs estimator for learning CPBM models from log data without requiring explicit randomized interventions.\",\n  \"Direct Inspiration\": [\"b2\", \"b19\", \"b27\"],\n  \"Indirect Inspiration\": [\"b3\", \"b26\"],\n  \"Other Inspiration\": [\"b0\", \"b1\", \"b7\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning-to-rank (LTR) from implicit feedback, which is biased. It introduces a Contextual Position-Based Model (CPBM) to overcome limitations of the traditional Position-Based Model (PBM) by incorporating context-dependent features through a deep network. The AllPairs estimator is proposed for learning CPBM models from log data without explicit interventions, improving accuracy and robustness in real-world and semi-synthetic experiments.\",\n  \"Direct Inspiration\": [\"b2\", \"b19\"],\n  \"Indirect Inspiration\": [\"b27\", \"b3\"],\n  \"Other Inspiration\": [\"b9\", \"b7\", \"b0\", \"b1\"]\n}\n```"], "57d063c3ac44367354290545": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting psychological stress from social media data by identifying both stressor events and stressor subjects, and measuring stress levels. It proposes a hybrid model combining multi-task learning and convolutional neural networks, and constructs new dictionaries for stressor events and subjects based on the learned word embeddings from Weibo data. The model is validated using a dataset constructed from Weibo and shows promising results.\",\n  \"Direct Inspiration\": [\"b6\"],\n  \"Indirect Inspiration\": [\"b2\", \"b8\"],\n  \"Other Inspiration\": [\"b1\", \"b4\", \"b7\", \"b9\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting psychological stress from social media data by identifying stressor subjects and events, and measuring stress levels. The proposed solution involves a hybrid model combining multi-task learning and convolutional neural networks, and the creation of stressor event and subject dictionaries. The paper also introduces a new benchmark dataset for stress detection.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b4\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting psychological stress from social media data by identifying stressor subjects and events, and measuring stress levels. The proposed solution involves extracting features from social media posts and using a hybrid model combining multi-task learning with convolutional neural networks to detect stressor subjects and events. The paper also introduces a benchmark dataset for stress detection and validates the proposed scheme through extensive experiments.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b7\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges addressed in the paper include stressor subject identification, stressor event detection, and data representation for stress measurement via social media data. The authors propose a hybrid model combining multi-task learning with convolutional neural networks (CNNs) to detect stressor subjects and events from social media posts. They also construct a benchmark dataset for stress detection and validate their model through extensive experiments.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b4\": 0.7,\n    \"b7\": 0.6,\n    \"b9\": 0.7,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b8\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of detecting stressor subjects and stressor events from social media data, as well as measuring stress levels. The proposed solution involves a hybrid model combining multi-task learning with convolutional neural networks (CNN), and the construction of a benchmark dataset for evaluation.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b9\": 0.7,\n    \"b7\": 0.6\n  }\n}\n```"], "5d9ed2d847c8f76646f797b7": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of fact verification (FV) by proposing a graph-based evidence aggregating and reasoning (GEAR) framework to better integrate and reason over multiple pieces of evidence. The authors aim to improve the verification of claims by encouraging information propagation among evidence in a fully-connected evidence graph and using BERT to better grasp both evidence and claim semantics.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b25\": 0.8,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatically verifying claims using multiple pieces of evidence, which is critical for applications like knowledge graph completion and open domain question answering. The authors propose a Graph-based Evidence Aggregating and Reasoning (GEAR) framework, which builds a fully-connected evidence graph to propagate information among evidence pieces and employs BERT for better semantic understanding. The proposed method aims to overcome limitations of existing methods that fail to sufficiently integrate relational and logical information from multiple evidence pieces.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b25\", \"b10\", \"b6\"],\n    \"details\": [\n      {\n        \"phrase\": \"we adopt an effective pretrained language representation model BERT\",\n        \"reference\": \"b6\"\n      },\n      {\n        \"phrase\": \"we simply follow the method from\",\n        \"reference\": \"b10\"\n      },\n      {\n        \"phrase\": \"on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER)\",\n        \"reference\": \"b25\"\n      }\n    ]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b31\", \"b18\", \"b14\"],\n    \"details\": [\n      {\n        \"phrase\": \"The FEVER shared task challenges participants to develop automatic fact verification systems\",\n        \"reference\": \"b31\"\n      },\n      {\n        \"phrase\": \"achieves the best results in the competition\",\n        \"reference\": \"b18\"\n      },\n      {\n        \"phrase\": \"adopt the decomposable attention model (DAM)\",\n        \"reference\": \"b14\"\n      }\n    ]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b29\", \"b22\"],\n    \"details\": [\n      {\n        \"phrase\": \"several large-scale datasets have been proposed to promote the research in this direction, such as SNLI and Multi-NLI\",\n        \"reference\": \"b29\"\n      },\n      {\n        \"phrase\": \"Pre-trained language representation models such as ELMo and OpenAI GPT\",\n        \"reference\": \"b22\"\n      }\n    ]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of verifying claims by aggregating and reasoning over multiple pieces of evidence. The proposed solution, a graph-based evidence aggregating and reasoning (GEAR) framework, aims to improve the integration and reasoning of evidence. The framework utilizes a fully-connected evidence graph for information propagation and employs BERT for enhanced semantic understanding. The method is validated on the FEVER dataset, showing superior performance compared to state-of-the-art baselines.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b6\": 0.9,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.7,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of fact verification (FV) by integrating and reasoning over multiple pieces of evidence. The authors propose a Graph-based Evidence Aggregating and Reasoning (GEAR) framework, leveraging a fully-connected evidence graph for information propagation and aggregation, and utilizing a pre-trained BERT model for better semantic understanding. The method is validated on the FEVER benchmark dataset, showcasing superior performance compared to state-of-the-art baselines.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b10\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b31\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b30\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of fact verification (FV) by verifying claims with evidence retrieved from plain text. The primary challenge is to integrate and reason over multiple pieces of evidence to verify claims. The authors propose a graph-based evidence aggregating and reasoning (GEAR) framework to address this challenge. This framework builds a fully-connected evidence graph to propagate information among evidence and uses a classifier to decide the support, refutation, or insufficiency of the evidence for the claim. The framework also leverages the pre-trained language representation model BERT to better grasp the semantics of both evidence and claims.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b10\": 0.9,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```"], "555045d745ce0a409eb59fe4": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Massive data on micro-blog platforms making manual labeling infeasible\",\n      \"Tweets containing multiple modalities with incomplete components\",\n      \"Difficulty in modeling user-level stress compared to discrete tweet-level\"\n    ],\n    \"Inspirations\": [\n      \"Using social media for stress detection\",\n      \"Combining low-level and user-scope attributes for better modeling\",\n      \"Employing deep neural networks for cross-media data\"\n    ]\n  },\n  \"Direct Inspiration\": [\"b16\", \"b17\", \"b18\"],\n  \"Indirect Inspiration\": [\"b11\", \"b12\", \"b13\", \"b14\", \"b15\"],\n  \"Other Inspiration\": [\"b8\", \"b9\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting psychological stress from social media data, particularly micro-blog platforms like Twitter and Sina Weibo. It proposes a deep neural network (DNN) model combined with convolutional neural networks (CNN) to automatically and non-invasively detect stress by leveraging cross-media data. The key contributions include building a stressed-twitter-posting database, designing a CNN with cross autoencoders to aggregate low-level content attributes, and proposing a DNN model to incorporate user-scope attributes for stress detection.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.95,\n    \"b17\": 0.90,\n    \"b18\": 0.93\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b10\": 0.80,\n    \"b11\": 0.70,\n    \"b12\": 0.70,\n    \"b13\": 0.75,\n    \"b14\": 0.77,\n    \"b15\": 0.78\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.60,\n    \"b7\": 0.60,\n    \"b9\": 0.65,\n    \"b19\": 0.50\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting psychological stress using micro-blog data by combining low-level content attributes and user-scope statistical attributes through a convolutional neural network (CNN) with cross autoencoders and a deep neural network (DNN) model. The novel method aims to aggregate various modalities of data and handle user-level stress modeling.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b17\": 0.9,\n    \"b18\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.6,\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b1\": 0.5,\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting psychological stress using social media data, particularly from micro-blog platforms. It proposes a deep neural network model that combines low-level content attributes from individual tweets with user-scope statistical attributes to detect user-level stress. The paper is inspired by previous work on stress detection through various modalities and deep learning approaches for cross-media data modeling.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b17\": 0.9,\n    \"b18\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper include the automatic and non-invasive detection of psychological stress using social media data, and the integration of multimodal data (text, images, social interactions) to model user-level stress continuously. The proposed algorithm involves using convolutional neural networks (CNN) and deep neural networks (DNN) to extract and combine low-level content attributes and user-scope statistical attributes for stress detection.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b17\": 0.85,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b9\": 0.75,\n    \"b10\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```"], "5bdc31b817c44a1f58a0c039": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently scaling Graph Convolutional Networks (GCNs) for large graphs by proposing a novel layer-wise sampling method. This method controls the size of sampled neighborhoods to avoid computational and memory issues associated with neighborhood expansion. The paper also introduces an adaptive sampler to minimize variance and a skip connection mechanism to maintain second-order proximity.\",\n  \"Direct Inspiration\": [\"b2\", \"b8\", \"b20\"],\n  \"Indirect Inspiration\": [\"b5\", \"b9\"],\n  \"Other Inspiration\": [\"b12\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the scalability challenge in Graph Convolutional Networks (GCNs) by proposing a novel layer-wise sampling method. The method aims to control the size of sampled neighborhoods to avoid over-expansion and reduce computational costs while maintaining second-order proximity through skip connections.\",\n  \"Direct Inspiration\": [\"b8\", \"b2\", \"b20\"],\n  \"Indirect Inspiration\": [\"b5\", \"b9\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently scaling Graph Convolutional Networks (GCNs) for large graphs. It proposes a novel layer-wise sampling method to control neighborhood size, an adaptive sampler for variance reduction, and a skip connection mechanism for preserving second-order proximities.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b2\": 0.9,\n    \"b20\": 0.9,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b12\": 0.6,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b21\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scalability in Graph Convolutional Networks (GCNs) by introducing a novel layer-wise sampling method that controls the size of sampled neighborhoods to speed up training and reduce memory usage. The proposed method also includes an adaptive sampler for variance reduction and a skip connection mechanism to preserve second-order proximity.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b20\": 0.9,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b9\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Scalability issues in applying current graph networks due to computationally prohibitive convolutions.\",\n      \"Need for efficient message passing across distant nodes.\"\n    ],\n    \"Inspirations\": [\n      \"Graph Convolution Network (GCN) [b8]\",\n      \"GraphSAGE [b2]\",\n      \"FastGCN [b20]\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b2\",\n    \"b8\",\n    \"b20\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b5\",\n    \"b9\"\n  ],\n  \"Other Inspiration\": [\n    \"b6\",\n    \"b12\"\n  ]\n}\n```"], "58437722ac44360f1082f13a": ["```json\n{\n  \"Summary\": \"The paper addresses semi-supervised learning by proposing two novel self-ensembling methods: \u03a0-model and temporal ensembling. These methods leverage dropout regularization and input augmentation to generate ensemble predictions during training, which are used to infer labels for unlabeled data. The main challenges include improving classification accuracy with limited labeled data and achieving robustness against incorrect labels.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b20\": 0.85,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b11\": 0.7,\n    \"b21\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b16\": 0.55\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving semi-supervised learning using self-ensembling methods, specifically the \u03a0-model and temporal ensembling. These methods rely heavily on dropout regularization and versatile input augmentation to leverage ensemble predictions during training, enhancing classification accuracy even with limited labeled data.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1.0,\n        \"b20\": 1.0,\n        \"b21\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.9,\n        \"b11\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.7,\n        \"b23\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of improving semi-supervised learning by leveraging ensemble predictions from a single neural network's outputs over multiple epochs under different regularization and input augmentation conditions. The proposed methods, \u03a0-model and temporal ensembling, aim to enhance classification accuracy and provide tolerance to incorrect labels.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b18\": 1,\n    \"b20\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b11\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving prediction accuracy in semi-supervised learning by using self-ensembling methods. The authors introduce two main approaches, the \u03a0-model and temporal ensembling, which leverage dropout regularization and input augmentation to create ensemble predictions from a single network during different training epochs. These methods improve classification accuracy and provide tolerance against incorrect labels.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b20\": 0.9,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b6\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving semi-supervised learning by leveraging ensemble predictions from a single neural network under different regularization and augmentation conditions. The authors propose two methods, \u03a0-model and temporal ensembling, which surpass prior state-of-the-art results and provide robustness against incorrect labels.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b20\": 1.0,\n    \"b18\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b3\": 0.6,\n    \"b0\": 0.5\n  }\n}\n```"], "5f7aeb7691e011983cc81e80": ["```json\n{\n    \"Summary\": \"The paper addresses several key challenges in entity retrieval from large Knowledge Bases (KBs), such as memory footprint, exact softmax computation, and the inability to handle unseen entities. The proposed solution, GENRE, leverages a sequence-to-sequence model to generate entity names autoregressively, offering significant improvements in context-entity interaction and memory efficiency. The method employs a transformer-based architecture, constrained beam search, and dynamic trie structures to handle large candidate sets effectively.\",\n    \"Direct Inspiration\": {\n        \"b34\": 1,\n        \"b64\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b37\": 0.8,\n        \"b25\": 0.7,\n        \"b45\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b47\": 0.6,\n        \"b52\": 0.5,\n        \"b23\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of entity retrieval from large Knowledge Bases (KBs), such as high memory footprint, inefficiency in handling new entities, and computational expense. The proposed solution, GENRE, uses a sequence-to-sequence architecture to generate entity names in an autoregressive fashion based on context, leveraging a transformer-based model pre-trained with a language modeling objective.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1.0,\n    \"b64\": 0.9,\n    \"b25\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.8,\n    \"b52\": 0.75,\n    \"b47\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b48\": 0.6,\n    \"b37\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes GENRE, a sequence-to-sequence model for entity retrieval that generates entity names in an autoregressive fashion conditioned on context. This approach addresses several shortcomings of traditional classifier-based methods, including memory footprint, computational efficiency, and handling of new entities. GENRE uses BART pre-trained weights and fine-tunes them for entity name generation. The method employs constrained beam search to ensure valid entity names are generated and is evaluated on tasks like entity disambiguation, end-to-end entity linking, and document retrieval, achieving state-of-the-art results.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1.0,\n    \"b64\": 0.9,\n    \"b47\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b30\": 0.7,\n    \"b52\": 0.7,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.5,\n    \"b50\": 0.5,\n    \"b60\": 0.5,\n    \"b65\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of entity retrieval from large Knowledge Bases (KBs) using a novel sequence-to-sequence model named GENRE. The primary challenges include the limitations of traditional classifier-based approaches in handling fine-grained interactions, memory footprint, costly softmax computations, and cold-start problems. GENRE proposes an autoregressive model with a constrained decoding strategy to generate entity names, leveraging a transformer-based architecture pre-trained with BART weights.\",\n  \"Direct Inspiration\": {\n    \"Inspired by\": [\"b34\"],\n    \"motivated by\": [\"b64\"]\n  },\n  \"Indirect Inspiration\": {\n    \"following.. we adopt ... to solve the challenge/problem\": [\"b45\", \"b50\"],\n    \"we use... based on to achieve...\": [\"b52\", \"b47\"]\n  },\n  \"Other Inspiration\": [\"b30\", \"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes GENRE, a sequence-to-sequence model for entity retrieval and disambiguation. It addresses challenges in current entity retrieval systems, such as large memory footprint, expensive computation, and cold-start problems. GENRE uses a transformer-based architecture, specifically BART, and employs constrained beam search to generate entity names efficiently. The model is evaluated on multiple datasets and shows state-of-the-art performance with significant memory savings.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1,\n    \"b64\": 0.95,\n    \"b52\": 0.9,\n    \"b47\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.8,\n    \"b50\": 0.7,\n    \"b30\": 0.65,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.55,\n    \"b15\": 0.5,\n    \"b17\": 0.4,\n    \"b8\": 0.35,\n    \"b19\": 0.3,\n    \"b51\": 0.25,\n    \"b42\": 0.2,\n    \"b29\": 0.2,\n    \"b70\": 0.2,\n    \"b24\": 0.2,\n    \"b12\": 0.2,\n    \"b11\": 0.2,\n    \"b33\": 0.2,\n    \"b10\": 0.2\n  }\n}\n```"], "5550470045ce0a409eb63934": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting psychological stress using cross-media microblog data. The authors propose a novel method that extracts and utilizes low-level features from text, images, and social interactions on microblogs. These features are then used to define stress-related middle-level representations, which are processed by a deep neural network to detect stress and its categories.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b6\": 1.0,\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b3\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting psychological stress using cross-media microblog data, incorporating text, images, and social interactions. The novel contribution is a three-level framework that extracts low-level features, defines middle-level representations based on psychological and art principles, and uses a deep neural network to model stress detection.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b6\": 1.0,\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b8\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting psychological stress through microblog data by proposing a novel method that incorporates text, images, and social interactions from tweets. It introduces a three-level framework that extracts low-level features, defines middle-level representations based on psychological principles and art theories, and employs a deep neural network to learn a stress detection model.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting psychological stress using cross-media microblog data. Traditional methods are labor-intensive and invasive, relying on additional sensors or devices. The novel approach proposed incorporates text, images, and social interactions to enhance stress detection through a three-level framework involving low-level features, middle-level representations, and a deep neural network model.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting psychological stress using cross-media microblog data. The novel method involves extracting low-level features from text, images, and social interactions on microblogs, defining stress-related middle-level representations, and employing a deep neural network for stress detection. The framework aims to offer a non-invasive, efficient alternative to traditional stress detection methods.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b6\": 1.0,\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b2\": 0.5,\n    \"b8\": 0.7\n  }\n}\n```"], "5cede0f2da562983788d0d44": ["```json\n{\n  \"Summary\": \"The paper addresses several deficiencies in previous unsupervised SMT systems by incorporating subword information, applying an unsupervised tuning method, and developing a joint refinement procedure. It also proposes a hybrid approach that uses unsupervised SMT to initialize an NMT system, which is further improved through iterative back-translation. The proposed system significantly outperforms previous state-of-the-art methods in unsupervised machine translation.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b15\": 0.85,\n    \"b14\": 0.8,\n    \"b25\": 0.75,\n    \"b29\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.65,\n    \"b17\": 0.6,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.55,\n    \"b1\": 0.5,\n    \"b4\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of unsupervised machine translation, particularly the reliance on large parallel corpora. The authors propose a more principled approach to unsupervised SMT by incorporating subword information, a novel unsupervised tuning method, and a joint refinement procedure. They also develop a hybrid system that combines SMT with NMT, resulting in significant improvements over previous state-of-the-art methods.\",\n  \"Direct Inspiration\": [\"b2\", \"b15\", \"b25\"],\n  \"Indirect Inspiration\": [\"b3\", \"b14\", \"b9\", \"b26\", \"b10\"],\n  \"Other Inspiration\": [\"b12\", \"b8\", \"b21\", \"b22\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of building unsupervised machine translation (MT) systems that do not rely on parallel corpora. It improves upon previous unsupervised Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) systems by incorporating subword information, a theoretically founded unsupervised tuning method, and a joint refinement procedure. The improved SMT system is used to initialize an unsupervised NMT system, which further enhances performance through on-the-fly back-translation.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1.0,\n        \"b15\": 0.9,\n        \"b3\": 0.8,\n        \"b14\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b25\": 0.6,\n        \"b9\": 0.5,\n        \"b11\": 0.5,\n        \"b29\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.4,\n        \"b12\": 0.4,\n        \"b19\": 0.4,\n        \"b21\": 0.4,\n        \"b28\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised machine translation (SMT) by incorporating subword information, a theoretically well-founded unsupervised tuning method, and a joint refinement procedure. It also proposes a hybrid approach using the improved SMT system to initialize an unsupervised neural machine translation (NMT) system, further enhanced through iterative back-translation.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b15\": 1,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b14\": 0.8,\n    \"b17\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b4\": 0.5,\n    \"b27\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of building unsupervised machine translation (MT) systems without relying on parallel corpora. The authors propose a principled approach to unsupervised Statistical Machine Translation (SMT) that incorporates subword information, a theoretically well-founded unsupervised tuning method, and a joint refinement procedure. Furthermore, they hybridize this improved SMT system with Neural Machine Translation (NMT) to achieve significant performance gains in standard benchmarks.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1.0,\n        \"b15\": 0.9,\n        \"b25\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.7,\n        \"b14\": 0.7,\n        \"b9\": 0.6,\n        \"b1\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.5,\n        \"b19\": 0.5,\n        \"b16\": 0.5,\n        \"b18\": 0.5,\n        \"b20\": 0.5,\n        \"b21\": 0.5,\n        \"b17\": 0.4,\n        \"b12\": 0.4,\n        \"b8\": 0.4,\n        \"b28\": 0.4,\n        \"b22\": 0.4,\n        \"b23\": 0.3,\n        \"b5\": 0.3,\n        \"b6\": 0.3,\n        \"b7\": 0.3,\n        \"b0\": 0.3,\n        \"b4\": 0.3,\n        \"b27\": 0.3,\n        \"b24\": 0.3,\n        \"b29\": 0.3\n    }\n}\n```"], "5f0bde8e9e795ea206ff8ef5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of requiring large amounts of labeled data in deep learning through the use of Semi-Supervised Learning (SSL). The proposed approach, Unsupervised Data Augmentation (UDA), utilizes advanced data augmentation methods typically used in supervised learning to improve performance in SSL. The key contributions include demonstrating the effectiveness of state-of-the-art data augmentations in consistency training, achieving superior results with fewer labeled examples, and combining UDA with transfer learning for enhanced performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b23\": 1.0,\n    \"b34\": 1.0,\n    \"b49\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b32\": 0.8,\n    \"b41\": 0.8,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b8\": 0.6,\n    \"b15\": 0.6,\n    \"b19\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b37\": 0.5,\n    \"b44\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing the need for a large amount of labeled data in deep learning through semi-supervised learning (SSL). It proposes a method called Unsupervised Data Augmentation (UDA) which leverages advanced data augmentation techniques from supervised learning to improve the consistency training framework in SSL. The paper demonstrates the effectiveness of UDA across various language and vision tasks, showing significant improvements over state-of-the-art models.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b23\": 0.9,\n    \"b34\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b32\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.6,\n    \"b49\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the significant amount of labeled data required for deep learning to work well. The proposed algorithm, Unsupervised Data Augmentation (UDA), leverages advanced data augmentation methods to improve the performance of semi-supervised learning (SSL) by substituting traditional noise injection methods with high-quality data augmentations. The method is evaluated on various language and vision tasks, demonstrating significant improvements over state-of-the-art models.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b23\": 1,\n    \"b34\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b6\": 0.8,\n    \"b32\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b12\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b19\": 0.7,\n    \"b25\": 0.7,\n    \"b26\": 0.7,\n    \"b27\": 0.7,\n    \"b37\": 0.7,\n    \"b39\": 0.7,\n    \"b44\": 0.7,\n    \"b49\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined is the high demand for labeled data in deep learning, which the proposed Unsupervised Data Augmentation (UDA) method aims to address by leveraging advanced data augmentation techniques from supervised learning to improve semi-supervised learning. The key contributions include demonstrating the effectiveness of state-of-the-art data augmentations in consistency training and achieving superior performance with fewer labeled examples.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b32\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b39\": 0.7,\n    \"b49\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of deep learning's reliance on large amounts of labeled data by proposing Unsupervised Data Augmentation (UDA) for semi-supervised learning. UDA utilizes advanced data augmentation techniques verified in supervised learning to improve consistency training with unlabeled data, achieving significant performance improvements across various vision and language tasks.\",\n  \"Direct Inspiration\": [\"b1\", \"b23\", \"b34\", \"b49\"],\n  \"Indirect Inspiration\": [\"b4\", \"b6\", \"b8\", \"b15\", \"b32\", \"b39\"],\n  \"Other Inspiration\": [\"b12\", \"b19\", \"b25\", \"b26\", \"b27\", \"b37\", \"b41\", \"b44\"]\n}\n```"], "5d4d46fb3a55acff992fddf3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatically estimating grassland degradation stages by leveraging deep learning techniques, specifically through semantic segmentation of grassland images to detect the coverage of Stellera chamaejasme (SC) as an indicator plant. The authors design a deep neural network and propose a novel Focal-Hinge loss function to tackle class imbalance issues.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b3\": 0.85,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b7\": 0.65,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b4\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately estimating grassland degradation stages automatically using deep learning techniques. The key contributions include leveraging a semantic segmentation algorithm to calculate the coverage of Stellera chamaejasme (SC), designing a new Focal-Hinge loss function to handle class imbalance, and proposing a novel deep neural network architecture for this task.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b3\": 0.75,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b10\": 0.6,\n    \"b12\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of estimating grassland degradation stages accurately, proposing a deep learning-based method to automatically calculate the coverage of Stellera chamaejasme (SC) in grassland images using semantic segmentation. The novel contributions include a deep neural network architecture for semantic segmentation and a new Focal-Hinge loss function to address class imbalance.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b8\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b12\": 0.7,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of automatic grassland degradation estimation using deep learning techniques, specifically through semantic segmentation of grassland images. The key challenges include insufficient public datasets, low-resolution aerial images, class imbalance in grassland scenes, and the limitations of existing semantic segmentation networks. To tackle these challenges, the authors propose a novel deep neural network architecture with refined cross connections and a new Focal-Hinge loss function to handle class imbalance.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b3\": 0.9,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b4\": 0.75,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.65,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately estimating grassland degradation stages without human interaction, by leveraging deep learning techniques for semantic segmentation of grassland images. A novel Focal-Hinge loss function is introduced to handle class imbalance in the dataset. The proposed method involves designing a deep neural network for semantic segmentation to calculate the coverage of Stellera chamaejasme (SC) and determine the degradation stage based on this coverage.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b10\": 0.5,\n    \"b1\": 0.4,\n    \"b4\": 0.4\n  }\n}\n```"], "5ce3ad19ced107d4c65b3dfb": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of data sparsity and cold-start problems in preference prediction methods by proposing a hybrid approach that leverages deep learning to analyze users' review texts. The approach includes using RNN-LSTM architecture for generating item summaries, Doc2Vec for vector embedding, and extending the PMF model to incorporate acquired preference knowledge.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1.0,\n    \"b46\": 1.0,\n    \"b56\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b55\": 0.8,\n    \"b20\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b1\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the issues of data sparsity and cold-start problems in user preference prediction models. The proposed solution involves a hybrid approach using deep learning techniques (specifically RNN-LSTM and Doc2Vec) to analyze review texts and represent user preferences more effectively, thereby enhancing prediction accuracy in the PMF model.\",\n  \"Direct Inspiration\": [\"b34\", \"b56\", \"b46\"],\n  \"Indirect Inspiration\": [\"b20\", \"b41\", \"b55\"],\n  \"Other Inspiration\": [\"b1\", \"b0\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in predicting user preferences, particularly focusing on data sparsity and cold-start problems. The proposed solution involves a hybrid approach using deep learning techniques (RNN-LSTM and Doc2Vec) to summarize and represent users' preference knowledge from review texts, which is then incorporated into the PMF model to improve prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.95,\n    \"b46\": 0.9,\n    \"b56\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b47\": 0.75,\n    \"b52\": 0.7,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b55\": 0.65,\n    \"b20\": 0.6,\n    \"b41\": 0.6,\n    \"b0\": 0.55,\n    \"b1\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of data sparsity and cold-start problems in preference prediction by proposing a hybrid approach that integrates deep learning techniques (RNN-LSTM) and Doc2Vec algorithm to represent users' preference knowledge from review texts. It extends the PMF model by incorporating these representations to improve prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.9,\n    \"b46\": 0.8,\n    \"b56\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.75,\n    \"b47\": 0.7,\n    \"b41\": 0.65,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b55\": 0.5,\n    \"b0\": 0.55,\n    \"b1\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of data sparsity and cold-start issues in predicting users' preferences for items. It proposes a hybrid approach that combines deep learning techniques, specifically RNN-LSTM and Doc2Vec, to learn and represent users' preference knowledge from review texts. This approach aims to generate deep summaries of item reviews, convert these summaries and user reviews into numerical vectors, and use these vectors to improve the predictions of the probabilistic matrix factorization (PMF) model.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b34\": 0.85,\n    \"b46\": 0.85,\n    \"b56\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b47\": 0.75,\n    \"b57\": 0.75,\n    \"b52\": 0.75,\n    \"b51\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.7,\n    \"b25\": 0.7,\n    \"b26\": 0.7,\n    \"b27\": 0.7,\n    \"b55\": 0.7,\n    \"b54\": 0.7,\n    \"b20\": 0.7,\n    \"b41\": 0.7,\n    \"b30\": 0.7,\n    \"b0\": 0.7,\n    \"b1\": 0.7,\n    \"b4\": 0.7,\n    \"b10\": 0.7\n  }\n}\n```"], "5736977f6e3b12023e666356": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding the relationship between Twitter users' profile images and their behaviors. It aims to reveal how users' internal properties influence their choice of profile images and behaviors on Twitter, a topic that has not been extensively studied before. The authors propose categorizing profile images into 13 types and analyzing user behaviors based on these categories.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b6\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b3\": 0.7,\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper investigates the relationship between Twitter users' profile images and their behaviors, exploring how internal properties influence both. It categorizes profile images into 13 types and examines user behaviors such as following and tweeting patterns in each category.\",\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [\n    \"b4\",\n    \"b6\",\n    \"b12\"\n  ],\n  \"Other Inspiration\": [\n    \"b2\",\n    \"b7\",\n    \"b8\",\n    \"b9\",\n    \"b11\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding the relationship between Twitter profile images and user behaviors, which has not been extensively studied. It proposes a novel categorization of profile images and investigates how these categories relate to user behaviors on Twitter.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b6\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding the relationship between Twitter users' profile images and their behaviors, which has not been comprehensively studied before. The study categorizes profile images and investigates how these categories correlate with user behaviors such as the number of followers, followees, and tweets.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b6\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b3\": 0.5,\n    \"b5\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper investigates the relationship between Twitter profile images and user behaviors to understand how internal properties influence these aspects. It categorizes profile images into 13 types and analyzes user behaviors based on these categories.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b6\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6\n  }\n}\n```"], "5736986b6e3b12023e730129": ["```json\n{\n    \"Summary\": \"The paper addresses the computational bottleneck in state-of-the-art object detection systems by proposing a novel Region Proposal Network (RPN) that shares convolutional layers with object detection networks. This approach significantly reduces the cost of proposal computation and improves detection accuracy by efficiently generating region proposals using deep convolutional networks.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0,\n        \"b5\": 0.9,\n        \"b6\": 0.9,\n        \"b17\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.7,\n        \"b19\": 0.7,\n        \"b18\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.6,\n        \"b23\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the computational bottleneck in state-of-the-art object detection systems by introducing Region Proposal Networks (RPNs) that share convolutional layers with object detection networks. This method significantly reduces the cost of computing proposals, resulting in a more efficient and unified detection system.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b18\": 0.8,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b19\": 0.6,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the computational bottleneck in object detection systems, specifically the region proposal step, and introduces Region Proposal Networks (RPNs) to generate region proposals nearly cost-free by sharing convolutional layers with state-of-the-art object detection networks. This results in a unified and efficient object detection system with improved accuracy and speed.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b5\", \"b4\", \"b6\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b13\", \"b17\", \"b18\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b2\", \"b19\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the computational bottleneck caused by region proposal methods in object detection systems. The proposed algorithm, Region Proposal Networks (RPNs), aims to compute proposals using a deep net that shares convolutional layers with state-of-the-art object detection networks, significantly reducing the computational cost.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b6\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b13\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b23\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the computational bottleneck posed by region proposal methods in state-of-the-art object detection systems. The proposed solution, Region Proposal Networks (RPNs), aims to generate region proposals cost-free by sharing convolutional layers with object detection networks, significantly improving both speed and accuracy.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b18\": 0.8,\n    \"b21\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```"], "573696f46e3b12023e5f1198": ["```json\n{\n    \"Summary\": \"The paper addresses the complexity in object detection, particularly the need for accurate localization and efficient processing of numerous candidate object locations. It introduces a single-stage training algorithm that jointly classifies object proposals and refines their spatial locations, significantly improving the speed and accuracy of detection networks compared to R-CNN and SPPnet.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1,\n        \"b10\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.8,\n        \"b14\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b19\": 0.5,\n        \"b5\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in object detection, specifically the complexities arising from the need to process numerous object proposals and refine their localization. The proposed solution is a single-stage training algorithm that jointly learns to classify object proposals and refine their spatial locations, achieving significant improvements in speed and accuracy over previous methods like R-CNN and SPPnet.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.6,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b19\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of object detection, which include processing numerous candidate object locations (proposals) and refining these candidates for precise localization. The proposed algorithm, Fast R-CNN, aims to streamline the training process by jointly learning to classify object proposals and refine their spatial locations in a single-stage training process. This method significantly speeds up training and testing while maintaining high accuracy.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of object detection, specifically the need for accurate localization of objects and the inefficiency of current multi-stage methods. The proposed solution, Fast R-CNN, introduces a single-stage training algorithm that jointly learns to classify object proposals and refine their spatial locations, significantly improving training and detection speed while maintaining high accuracy.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b7\": 0.5,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Complexity in object detection due to the need for accurate localization of objects.\",\n      \"Numerous candidate object locations requiring processing and refinement of rough localizations.\"\n    ],\n    \"proposed_algorithm\": \"A single-stage training algorithm that jointly learns to classify object proposals and refine their spatial locations, leading to a faster and more accurate detection network.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b8\", \"b10\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b18\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b5\", \"b7\"]\n  }\n}\n```"], "599c7968601a182cd2639c4c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of instance segmentation by proposing a new method called Mask R-CNN, which extends the Faster R-CNN framework by adding a branch for predicting segmentation masks on each Region of Interest (RoI). A critical innovation is the RoIAlign layer, which ensures pixel-to-pixel alignment and improves the accuracy of mask predictions. The paper demonstrates significant improvements over previous state-of-the-art methods in instance segmentation and object detection tasks.\",\n  \"Direct Inspiration\": [\"b34\", \"b14\", \"b27\"],\n  \"Indirect Inspiration\": [\"b29\", \"b12\", \"b26\"],\n  \"Other Inspiration\": [\"b20\", \"b28\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of instance segmentation by proposing a new method called Mask R-CNN. This method extends Faster R-CNN by adding a branch for predicting segmentation masks on each Region of Interest (RoI) in parallel with the existing branch for classification and bounding box regression. The main innovation is the introduction of the RoIAlign layer to fix the misalignment issue present in Faster R-CNN.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1,\n    \"b14\": 0.9,\n    \"b27\": 0.9,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b26\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b10\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is instance segmentation, which requires detection and precise segmentation of each object instance in an image. The paper proposes Mask R-CNN, an extension of Faster R-CNN, by adding a branch for predicting segmentation masks. The key innovation includes the introduction of the RoIAlign layer for preserving exact spatial locations, which is critical for good mask predictions.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.9,\n    \"b12\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.7,\n    \"b29\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are related to achieving accurate and efficient instance segmentation, which involves correctly detecting all objects in an image and precisely segmenting each instance. The proposed algorithm, Mask R-CNN, extends Faster R-CNN by adding a branch for predicting segmentation masks on each Region of Interest (RoI). A key innovation is the RoIAlign layer, which addresses misalignment issues in feature extraction.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b34\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.9,\n    \"b29\": 0.8,\n    \"b27\": 0.9,\n    \"b12\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces Mask R-CNN, a framework for instance segmentation that extends Faster R-CNN by adding a branch for predicting segmentation masks on each Region of Interest (RoI). The main challenges addressed include the need for precise detection and segmentation of objects, and the misalignment issue in RoIPool. The proposed solution, RoIAlign, ensures proper alignment, improving performance significantly.\",\n  \"Direct Inspiration\": [\n    \"b34\",\n    \"b14\",\n    \"b27\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b19\",\n    \"b29\"\n  ],\n  \"Other Inspiration\": [\n    \"b12\",\n    \"b26\",\n    \"b23\"\n  ]\n}\n```"], "5a73cbcc17c44a0b3035f34a": ["```json\n{\n    \"Summary\": \"The primary challenge in this paper is to develop high-quality object detectors that minimize close false positives. The authors propose a novel Cascade R-CNN architecture that sequentially trains detectors to handle progressively higher Intersection over Union (IoU) thresholds, thereby improving detection accuracy and reducing false positives.\",\n    \"Direct Inspiration\": {\n        \"b33\": 1.0,\n        \"b8\": 0.9,\n        \"b5\": 0.8,\n        \"b1\": 0.8,\n        \"b34\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.7,\n        \"b25\": 0.7,\n        \"b21\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.5,\n        \"b29\": 0.5,\n        \"b22\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses key challenges in object detection, particularly the issues of recognition and localization which involve distinguishing foreground objects from background and assigning accurate bounding boxes. The proposed solution, Cascade R-CNN, is a multi-stage extension of the R-CNN framework designed to progressively improve detection quality by training sequential stages to be more selective against close false positives.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b29\": 1,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.8,\n    \"b8\": 0.8,\n    \"b6\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of object detection, particularly the issues of distinguishing foreground objects from background and accurately localizing bounding boxes. It proposes a novel Cascade R-CNN architecture that sequentially trains multiple stages of detectors with increasing IoU thresholds to improve detection quality and reduce close false positives.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b1\": 0.9,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b29\": 0.8,\n    \"b12\": 0.8,\n    \"b22\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b25\": 0.7,\n    \"b33\": 0.7,\n    \"b8\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is to improve the quality of object detection by reducing close false positives. The proposed solution is a new detector architecture called Cascade R-CNN, which is a multi-stage extension of the R-CNN framework. This approach leverages sequential training stages, where each stage is progressively more selective against close false positives, and uses resampling to adapt bounding boxes progressively.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b33\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high-quality object detection, specifically focusing on reducing 'close but not correct' bounding boxes and improving detection accuracy by proposing a new multi-stage detector architecture called Cascade R-CNN. The architecture aims to optimize detections at progressively higher IoU thresholds to minimize false positives and improve overall accuracy.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b1\": 0.9,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b33\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```"], "5cd7fa07ced107d4c65bf34f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating Knowledge Graphs (KGs) into Recommender Systems (RS) to capture high-order structure and semantic information, proposing Knowledge Graph Convolutional Networks (KGCN) for this purpose.\",\n  \"Direct Inspiration\": [\"b1\", \"b9\", \"b14\"],\n  \"Indirect Inspiration\": [\"b17\", \"b21\", \"b23\"],\n  \"Other Inspiration\": [\"b3\", \"b8\", \"b18\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is to improve recommender systems by effectively utilizing knowledge graphs (KG) to capture high-order structure and semantic information. The proposed algorithm, Knowledge Graph Convolutional Networks (KGCN), aims to aggregate and incorporate neighborhood information with bias when calculating the representation of a given entity in the KG.\",\n    \"Direct Inspiration\": [\"b1\", \"b9\"],\n    \"Indirect Inspiration\": [\"b0\", \"b17\", \"b21\", \"b23\"],\n    \"Other Inspiration\": [\"b8\", \"b18\", \"b22\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges addressed in the paper are the sparsity of user-item interactions and the cold start problem in recommender systems. The proposed algorithm, Knowledge Graph Convolutional Networks (KGCN), aims to capture both high-order structure and semantic information in a knowledge graph (KG) to improve recommendation performance. The design involves aggregating neighborhood information with bias, using graph convolutional networks (GCN) as inspiration.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1,\n        \"b3\": 0.9,\n        \"b9\": 1,\n        \"b6\": 0.8,\n        \"b14\": 0.8,\n        \"b20\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b17\": 0.75,\n        \"b21\": 0.7,\n        \"b23\": 0.7,\n        \"b22\": 0.65,\n        \"b8\": 0.6,\n        \"b18\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.5,\n        \"b11\": 0.5,\n        \"b15\": 0.4,\n        \"b2\": 0.4,\n        \"b16\": 0.4\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges addressed in the paper include overcoming the sparsity of user-item interactions and the cold start problem in collaborative filtering-based recommender systems. The authors propose Knowledge Graph Convolutional Networks (KGCN) to automatically capture both high-order structure and semantic information in knowledge graphs, thereby improving recommendation results. The core contribution is inspired by graph convolutional networks (GCN) and aims to extend the receptive field of each entity in the knowledge graph to capture users' high-order personalized interests.\",\n    \"Direct Inspiration\": [\"b1\", \"b9\"],\n    \"Indirect Inspiration\": [\"b17\", \"b21\", \"b23\"],\n    \"Other Inspiration\": [\"b8\", \"b18\", \"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of utilizing Knowledge Graphs (KG) in recommender systems to capture both high-order structure and semantic information. It introduces Knowledge Graph Convolutional Networks (KGCN), inspired by graph convolutional networks (GCN), to aggregate and incorporate neighborhood information with bias while calculating the representation of a given entity in the KG.\",\n  \"Direct Inspiration\": [\n    \"b1\",\n    \"b9\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b8\",\n    \"b17\",\n    \"b18\",\n    \"b21\",\n    \"b23\"\n  ],\n  \"Other Inspiration\": [\n    \"b20\",\n    \"b14\"\n  ]\n}\n```"], "5cf48a45da56291d582a8448": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of utilizing Knowledge Graphs (KGs) in recommender systems, focusing on capturing user-specific item-item relatedness and addressing issues like cold-start and scalability. The authors propose Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) to extend GNNs to KGs, incorporating user-specific preferences and addressing overfitting through label smoothness regularization.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.95,\n    \"b34\": 0.9,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b17\": 0.75,\n    \"b23\": 0.75,\n    \"b25\": 0.75,\n    \"b26\": 0.75,\n    \"b32\": 0.75,\n    \"b35\": 0.75,\n    \"b33\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b12\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b28\": 0.5,\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of utilizing Knowledge Graphs (KGs) in recommender systems to capture user-specific item-item relatedness. It proposes Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) to extend GNNs architecture to heterogeneous knowledge graphs. The proposed method incorporates a personalized relation scoring function and label smoothness regularization to achieve better generalization and prevent overfitting.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b27\", \"b34\", \"b37\"],\n        \"phrases\": [\"similar to [b27]\", \"inspired by these methods\", \"develop an approach based on label smoothness [b34] [b37]\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b7\", \"b8\", \"b25\", \"b26\", \"b33\"],\n        \"phrases\": [\"embedding-based methods [b8] [b25] [b26] [b33]\", \"path-based methods [b7]\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b13\", \"b18\", \"b30\", \"b31\"],\n        \"phrases\": [\"recently, several works developed GNNs architecture for recommender systems [b13] [b18] [b27] [b30] [b31]\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of utilizing knowledge graphs (KGs) in recommender systems by developing Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS). It aims to capture semantic relationships between items and user preferences while overcoming issues such as manual feature engineering, end-to-end training, and scalability. The proposed method transforms the KG into a user-specific weighted graph and applies a graph neural network with label smoothness regularization to improve recommendation accuracy and handle cold-start scenarios.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b34\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.7,\n    \"b37\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b18\": 0.65,\n    \"b25\": 0.55,\n    \"b26\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing recommender systems using knowledge graphs (KGs) to capture semantic relationships and user-specific preferences. The proposed algorithm, Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS), extends GNNs to heterogeneous KGs and introduces a personalized relation scoring function and a label smoothness regularization technique to improve recommendation accuracy and generalization.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b34\": 1,\n    \"b37\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b25\": 0.8,\n    \"b26\": 0.8,\n    \"b33\": 0.8,\n    \"b13\": 0.7,\n    \"b18\": 0.7,\n    \"b30\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving recommendation systems by integrating knowledge graphs (KGs) and graph neural networks (GNNs) to capture user-specific item-item relatedness. The authors propose a novel approach, Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS), which extends GNNs to heterogeneous KGs, incorporating a user-specific relation scoring function and a label smoothness regularization to combat overfitting.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b34\": 0.85,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b25\": 0.7,\n    \"b26\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.65,\n    \"b32\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```"], "5b67b45517c44aac1c860876": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of scaling Graph Convolutional Networks (GCNs) to work with massive graphs containing billions of nodes and edges, specifically for recommendation tasks at Pinterest. The authors propose PinSage, a highly scalable GCN framework that introduces several algorithmic innovations and training techniques to improve both scalability and performance. Key features include on-the-fly convolutions, producer-consumer minibatch construction, efficient MapReduce inference, constructing convolutions via random walks, and importance pooling.\",\n    \"Direct Inspiration\": {\n        \"b17\": 0.95,\n        \"b7\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.65,\n        \"b18\": 0.70,\n        \"b20\": 0.60,\n        \"b23\": 0.65,\n        \"b28\": 0.60\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.50,\n        \"b25\": 0.50\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling Graph Convolutional Networks (GCNs) to graphs with billions of nodes and edges, which is critical for real-world production environments like Pinterest. The proposed algorithm, PinSage, introduces several innovations such as localized convolutions, producer-consumer minibatch construction, and efficient MapReduce inference to tackle these challenges.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b18\": 0.8,\n    \"b20\": 0.7,\n    \"b23\": 0.7,\n    \"b28\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of scaling Graph Convolutional Networks (GCNs) for production environments with billions of nodes and edges. The primary contribution is the PinSage framework, which introduces several innovations such as on-the-fly convolutions, producer-consumer minibatch construction, and importance pooling to improve scalability and performance in recommendation tasks.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1,\n        \"b7\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.8,\n        \"b20\": 0.8,\n        \"b23\": 0.8,\n        \"b28\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.7,\n        \"b13\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper focuses on addressing the challenge of scaling Graph Convolutional Networks (GCNs) to handle large-scale graphs with billions of nodes and edges for recommendation systems. The proposed solution, PinSage, introduces several innovations such as on-the-fly convolutions, producer-consumer minibatch construction, efficient MapReduce inference, and importance pooling to improve both scalability and performance in real-world environments.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1.0,\n        \"b7\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.8,\n        \"b5\": 0.7,\n        \"b20\": 0.7,\n        \"b23\": 0.7,\n        \"b28\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.6,\n        \"b22\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of scaling Graph Convolutional Networks (GCNs) for use in large-scale recommendation systems, particularly in a production environment like Pinterest which operates on a graph with billions of nodes and edges. The proposed solution, PinSage, introduces innovations such as on-the-fly convolutions, producer-consumer minibatch construction, and efficient MapReduce inference to improve scalability and performance.\",\n    \"Direct Inspiration\": {\n        \"b5\": 1,\n        \"b18\": 1,\n        \"b20\": 1,\n        \"b23\": 1,\n        \"b28\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b7\": 0.7,\n        \"b9\": 0.6,\n        \"b10\": 0.6,\n        \"b12\": 0.6,\n        \"b17\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.5,\n        \"b16\": 0.5,\n        \"b21\": 0.5,\n        \"b25\": 0.5\n    }\n}\n```"], "5ee8986891e011e66831c3b6": ["```json\n{\n  \"Summary\": \"The paper introduces BYOL (Bootstrap Your Own Latent), a self-supervised learning algorithm for image representation that outperforms state-of-the-art contrastive methods by not using negative pairs. The main challenges addressed are the need for high-quality image representation for downstream tasks, the dependency on image augmentations, and the avoidance of collapsed representations. The novel approach uses two neural networks (online and target) that learn from each other by iteratively refining the representation.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b19\": 0.8,\n    \"b53\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b11\": 0.7,\n    \"b16\": 0.6,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces Bootstrap Your Own Latent (BYOL), a new self-supervised learning algorithm for image representation that achieves high performance without using negative pairs. BYOL uses two neural networks (online and target) that learn from each other through an iterative bootstrapping mechanism, making it more robust to changes in image augmentations and batch sizes compared to contrastive methods.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1,\n        \"b19\": 1,\n        \"b53\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b11\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.7,\n        \"b9\": 0.7,\n        \"b20\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces BYOL (Bootstrap Your Own Latent), a novel self-supervised learning algorithm for image representation that does not rely on negative pairs, unlike state-of-the-art contrastive methods. The primary challenge addressed is learning robust image representations without the need for negative pairs, which are computationally intensive and sensitive to the choice of image augmentations. BYOL utilizes two neural networks, an online and a target network, that iteratively bootstrap each other\u2019s outputs to enhance representations. This method shows improved performance and robustness compared to existing contrastive methods.\",\n  \"Direct Inspiration\": [\"b8\", \"b49\", \"b50\", \"b51\", \"b52\", \"b53\"],\n  \"Indirect Inspiration\": [\"b7\", \"b9\", \"b11\", \"b16\", \"b19\"],\n  \"Other Inspiration\": [\"b20\", \"b21\", \"b31\", \"b33\", \"b62\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge addressed in the paper is learning good image representations for computer vision tasks without relying on negative pairs, as typically required by contrastive methods. The proposed algorithm, BYOL (Bootstrap Your Own Latent), aims to achieve high performance in self-supervised learning by iteratively bootstrapping the outputs of a network to serve as targets for an enhanced representation, making it more robust to the choice of image augmentations.\",\n    \"Inspirations\": \"The paper is inspired by the limitations of existing contrastive methods and aims to overcome these by introducing a new self-supervised learning algorithm that does not rely on negative pairs. The idea of using a slow-moving average target network to produce stable targets for the online network is inspired by deep reinforcement learning techniques.\"\n  },\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b53\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b11\": 0.7,\n    \"b7\": 0.6,\n    \"b49\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.4,\n    \"b48\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is learning good image representations for efficient training on downstream tasks in computer vision. The authors propose a new algorithm, Bootstrap Your Own Latent (BYOL), for self-supervised learning of image representations that does not rely on negative pairs and achieves state-of-the-art performance on ImageNet under the linear evaluation protocol. The algorithm uses two neural networks (online and target networks) that interact and learn from each other, with specific mechanisms to prevent collapse to trivial solutions.\",\n    \"Direct Inspiration\": {\n        \"b8\": 0.9,\n        \"b19\": 0.9,\n        \"b53\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.7,\n        \"b11\": 0.6,\n        \"b16\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.6,\n        \"b10\": 0.5,\n        \"b20\": 0.5\n    }\n}\n```"], "5dcd263a3a55ac58039516c5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving unsupervised visual representation learning. The authors propose the Momentum Contrast (MoCo) algorithm, which builds large and consistent dictionaries using a queue-based mechanism for maintaining encoded keys and a momentum update for consistency. This approach is aimed at overcoming the limitations of existing contrastive loss methods, particularly in terms of dictionary size and consistency.\",\n  \"Direct Inspiration\": {\n    \"b60\": 1.0,\n    \"b62\": 0.9,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.8,\n    \"b45\": 0.8,\n    \"b35\": 0.7,\n    \"b55\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.6,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised visual representation learning, which lags behind supervised methods in computer vision. The proposed algorithm, Momentum Contrast (MoCo), builds large and consistent dictionaries for contrastive learning by maintaining a queue of encoded representations and updating the encoder using a momentum-based moving average. This method shows competitive results in various visual tasks and aims to close the gap between unsupervised and supervised learning.\",\n  \"Direct Inspiration\": [\"b60\", \"b45\", \"b35\"],\n  \"Indirect Inspiration\": [\"b1\", \"b62\"],\n  \"Other Inspiration\": [\"b55\", \"b28\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of unsupervised visual representation learning in computer vision, focusing on building large and consistent dictionaries using contrastive loss. The proposed method, Momentum Contrast (MoCo), maintains a dictionary as a queue to decouple its size from the mini-batch size and employs a momentum update to ensure consistency in the encoded keys.\",\n  \"Direct Inspiration\": {\n    \"b60\": 1.0,\n    \"b45\": 0.9,\n    \"b35\": 0.9,\n    \"b1\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.8,\n    \"b55\": 0.75,\n    \"b62\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised representation learning in computer vision, proposing the Momentum Contrast (MoCo) algorithm to build large and consistent dictionaries for contrastive learning. The main contributions include the use of a queue for the dictionary to decouple its size from the mini-batch size and a momentum update to maintain consistency in the key encoder.\",\n  \"Direct Inspiration\": {\n    \"b60\": 1,\n    \"b1\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.9,\n    \"b55\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.8,\n    \"b35\": 0.75,\n    \"b62\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the gap between unsupervised and supervised representation learning in computer vision. The authors propose the Momentum Contrast (MoCo) method, which builds large and consistent dictionaries for contrastive learning. MoCo maintains these dictionaries using a queue mechanism and a momentum update for the key encoder. The paper demonstrates that MoCo can achieve competitive results in visual representation learning and can serve as an alternative to supervised pre-training in several computer vision tasks.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b60\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.85,\n    \"b35\": 0.8,\n    \"b1\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b55\": 0.7,\n    \"b62\": 0.7\n  }\n}\n```"], "5b3d98cc17c44a510f8018e7": ["```json\n{\n  \"Summary\": \"The paper addresses the susceptibility of deep neural networks to adversarial attacks by proposing a novel defense mechanism that leverages Generative Adversarial Networks (GANs) to diminish the effect of adversarial perturbations. The core contribution lies in 'projecting' input images onto the range of a GAN's generator prior to classification, effectively reducing adversarial noise and improving robustness against both black-box and white-box attacks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b13\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b4\": 0.8,\n    \"b20\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial attacks on deep neural networks and proposes a novel defense mechanism using Generative Adversarial Networks (GANs). The focus is on leveraging GANs to diminish adversarial perturbations by projecting input images onto the range of the GAN's generator before classification, making it effective against both white-box and black-box attacks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b20\": 0.8,\n    \"b21\": 0.75,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b6\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the susceptibility of deep neural networks to adversarial attacks and proposes a novel defense mechanism leveraging GANs to mitigate these attacks. The proposed Defense-GAN aims to diminish adversarial perturbations by projecting input images onto the GAN's generator range before classification.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b4\": 0.7,\n    \"b2\": 0.6,\n    \"b19\": 0.6,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the susceptibility of deep neural networks to adversarial attacks and proposes a novel defense mechanism, Defense-GAN, which leverages Generative Adversarial Networks (GAN) to project input images onto the range of the GAN's generator to diminish the effect of adversarial perturbations. The proposed method is effective against both white-box and black-box attacks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b4\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the susceptibility of deep neural networks to adversarial attacks by proposing a novel defense mechanism using Generative Adversarial Networks (GAN). The primary challenge is to create a defense that is effective against both white-box and black-box attacks. The proposed solution, Defense-GAN, projects input images onto the range of the GAN's generator to diminish adversarial perturbations before classification.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b18\": 0.6,\n    \"b20\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```"], "5d0b00ed8607575390fedd4e": ["```json\n{\n    \"Summary\": \"The paper focuses on addressing the primary challenge of generating adversarial examples in a decision-based attack framework, with an emphasis on query efficiency. It introduces a novel family of algorithms, named HopSkipJumpAttack, optimized for minimum distance with respect to either the L2 or L\u221e distance. The algorithms are iterative, involving gradient direction estimation, step-size search via geometric progression, and boundary search via binary search. The contributions include an optimization framework, a novel gradient-direction estimate at the boundary, and a binary-search procedure to approach the boundary. The work is motivated by the limitations of existing decision-based attacks, particularly the Boundary Attack, which requires a large number of model queries, making it impractical for real-world applications.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b8\": 0.8,\n        \"b9\": 0.8,\n        \"b10\": 0.6,\n        \"b11\": 0.6,\n        \"b12\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.4,\n        \"b2\": 0.4,\n        \"b4\": 0.4,\n        \"b5\": 0.4,\n        \"b6\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of deep neural networks to adversarial examples and proposes a novel family of algorithms called HopSkipJumpAttack to generate such examples under a decision-based attack framework. The key contributions include an iterative algorithm for generating adversarial examples, a novel gradient-direction estimate, and methods to improve query efficiency.\",\n  \"Direct Inspiration\": [\"b13\"],\n  \"Indirect Inspiration\": [\"b7\", \"b8\", \"b9\", \"b17\", \"b18\", \"b19\", \"b20\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating adversarial examples under a decision-based attack scenario with a limited budget of model queries. It proposes a novel family of algorithms called HopSkipJumpAttack, which optimizes for minimum distance with respect to either \\(L_2\\)-distance or \\(L_\\infty\\)-distance. The algorithm is iterative, involving steps for gradient direction estimation, step-size search, and boundary search. Theoretical analysis and experimental results are provided to demonstrate the effectiveness of the proposed method.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b10\": 0.7,\n    \"b11\": 0.6,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b2\": 0.5,\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of deep neural networks to adversarial examples and proposes a novel family of algorithms for decision-based attacks, called HopSkipJumpAttack, optimized for query efficiency. The proposed algorithm involves gradient direction estimation, step-size search via geometric progression, and boundary search via a binary search.\",\n  \"Direct Inspiration\": [\"b13\"],\n  \"Indirect Inspiration\": [\"b7\", \"b8\", \"b9\"],\n  \"Other Inspiration\": [\"b1\", \"b2\", \"b6\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating adversarial examples in decision-based attack settings, focusing on improving query efficiency. It proposes a novel iterative algorithm called HopSkipJumpAttack, which optimizes for minimum distance between adversarial and original examples using an optimization framework. The key contributions include a new gradient-direction estimate and a binary-search procedure for decision-based attacks.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```"], "5eccb534e06a4c1b26a83514": ["```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is the limited transferability of adversarial samples in transfer-based black-box attacks, which are prone to overfitting to the exclusive blind spots of the source model. The proposed algorithm introduces an attention-based regularization term to improve the transferability by guiding the search of adversarial perturbations towards common vulnerable directions of both the source and the target models. This is achieved by exploiting the similarity in attention maps across different models and undermining critical features that diverse models rely on for making predictions.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1.0,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.8,\n    \"b20\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7,\n    \"b8\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of deep neural networks (DNNs) to adversarial samples and aims to enhance the transferability of adversarial attacks in black-box settings. The proposed method focuses on leveraging common attention patterns across different models to guide the generation of adversarial samples, thereby improving their effectiveness against unseen models.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b7\", \"b40\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b38\", \"b20\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b2\", \"b35\", \"b8\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the vulnerability of deep neural networks (DNNs) to adversarial samples, particularly in the context of transfer-based black-box attacks. The paper proposes an algorithm that enhances the transferability of adversarial samples by focusing on critical features shared among different models, using attention-based regularization during the optimization process.\",\n  \"Direct Inspiration\": [\"b7\", \"b40\", \"b20\", \"b38\"],\n  \"Indirect Inspiration\": [\"b35\", \"b2\"],\n  \"Other Inspiration\": [\"b19\", \"b8\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the limited transferability of adversarial samples generated using white-box attack strategies. The proposed algorithm aims to improve the transferability by guiding the search of adversarial images towards common vulnerable features shared across different models. The key innovation is the use of model attention to guide the destruction of critical features, thereby reducing overfitting to specific source models.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b40\": 1,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b38\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b2\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge discussed in the paper is the vulnerability of deep neural networks (DNNs) to adversarial samples, particularly in transfer-based black-box attack scenarios. The proposed algorithm aims to enhance the transferability of adversarial samples by guiding the search towards common vulnerable features across different models. This is achieved by leveraging model attention to direct adversarial perturbations towards critical features shared by diverse architectures.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b29\": 1,\n    \"b40\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```"], "5b1643ba8fbcbf6e5a9bc79b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating adversarial examples for neural network-based image classifiers in black-box settings with various access and resource restrictions. It introduces novel algorithms for query-limited, partial-information, and label-only attack settings, leveraging Natural Evolutionary Strategies (NES) for efficient gradient estimation and proposing new techniques for these specific threat models.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b29\": 0.95,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b16\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating adversarial examples for neural network-based image classifiers under various black-box threat models. The authors propose new methods designed to be more query-efficient and applicable under constraints such as limited access to class probabilities or labels. Key contributions include the application of Natural Evolutionary Strategies (NES) for query-efficient gradient estimation, a new algorithm for the partial-information setting, and an approach for the label-only setting.\",\n    \"Direct Inspiration\": {\n        \"b25\": 1,\n        \"b29\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b4\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.6,\n        \"b16\": 0.6,\n        \"b10\": 0.6,\n        \"b23\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating adversarial examples in black-box settings with additional access and resource restrictions, such as query-limited, partial-information, and label-only settings. It proposes novel algorithms based on Natural Evolutionary Strategies (NES) for query-efficient adversarial example generation, methods for partial-information attacks, and techniques for label-only setting attacks.\",\n    \"Direct Inspiration\": {\n        \"b29\": 1.0,\n        \"b25\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b16\": 0.7,\n        \"b4\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.6,\n        \"b10\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating adversarial examples for neural network-based image classifiers under limited threat models, specifically in black-box settings with additional access and resource restrictions. The proposed techniques include query-efficient generation of adversarial examples using Natural Evolutionary Strategies (NES), targeted attacks in partial-information settings, and attacks in label-only settings.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.9,\n        \"b4\": 0.8,\n        \"b25\": 1.0,\n        \"b29\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b22\": 0.7,\n        \"b19\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.7,\n        \"b10\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of generating adversarial examples for neural network-based image classifiers in black-box settings with various constraints. The authors propose new algorithms for generating adversarial examples in query-limited, partial-information, and label-only settings. They introduce the use of Natural Evolutionary Strategies (NES) for efficient gradient estimation and develop techniques for attacking classifiers with limited feedback.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b16\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b20\": 0.6,\n    \"b22\": 0.5\n  }\n}\n```"], "5d1eb9beda562961f0af981f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the robustness of deep learning computer vision models to distributional shifts and image corruptions. The authors propose an analysis of the frequency information of different corruptions and suggest that data augmentation procedures can bias models towards utilizing specific frequency information. They demonstrate the effectiveness of AutoAugment in achieving state-of-the-art results in robustness benchmarks.\",\n  \"Direct Inspiration\": [\"b5\", \"b0\"],\n  \"Indirect Inspiration\": [\"b10\", \"b16\"],\n  \"Other Inspiration\": [\"b7\", \"b9\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the robustness of deep learning models against distributional shifts, specifically image corruptions. The authors propose using frequency domain analysis to understand the biases introduced by different data augmentation techniques and demonstrate that more diverse data augmentation procedures could mitigate observed trade-offs. They highlight the effectiveness of the AutoAugment data augmentation policy in achieving state-of-the-art results on CIFAR-10-C and ImageNet-C benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b10\": 0.9,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b7\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the robustness of deep learning computer vision models to distributional shifts and common image corruptions. It proposes analyzing the frequency information of different corruptions to understand the trade-offs observed with various data augmentation strategies and suggests that more diverse data augmentation could mitigate these trade-offs. The paper finds that AutoAugment achieves state-of-the-art results on robustness benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b12\": 0.7,\n    \"b9\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b17\": 0.5,\n    \"b13\": 0.5,\n    \"b26\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper revolve around improving the robustness of deep learning computer vision models to distributional shifts, particularly when encountering image corruptions. The authors propose analyzing the frequency information of different corruptions and utilizing diverse data augmentation procedures to mitigate trade-offs between different types of corruptions. The key inspiration for their approach comes from the Fourier domain perturbations and the AutoAugment data augmentation policy.\",\n  \"Direct Inspiration\": [\"b5\", \"b0\"],\n  \"Indirect Inspiration\": [\"b10\", \"b9\", \"b16\"],\n  \"Other Inspiration\": [\"b7\", \"b2\", \"b13\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve improving the robustness of deep learning models to distributional shifts and various image corruptions. The paper proposes analyzing these challenges through a Fourier perspective and emphasizes the role of data augmentation strategies, including AutoAugment, Gaussian data augmentation, and adversarial training, in improving model robustness. Novel methods introduced include Fourier heat maps for visualizing model sensitivity to frequency perturbations and examining the trade-offs between different data augmentation techniques.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1, \n    \"b10\": 0.9 \n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8, \n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6, \n    \"b12\": 0.5, \n    \"b16\": 0.4\n  }\n}\n```"], "5c2c7a9217c44a4e7cf318b1": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial attacks on image classification systems, which add small perturbations to images causing incorrect predictions by convolutional networks. The authors propose feature denoising approaches to improve robustness against such attacks. They develop new convolutional network architectures with denoising blocks that are trained end-to-end on adversarial samples. The paper emphasizes the effectiveness of non-local means for feature denoising, leading to models related to self-attention and non-local networks. Their models outperform state-of-the-art in adversarial robustness against various attacks.\",\n  \"Direct Inspiration\": [\"b1\", \"b22\", \"b23\"],\n  \"Indirect Inspiration\": [\"b15\", \"b9\", \"b17\"],\n  \"Other Inspiration\": [\"b8\", \"b20\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the problem of adversarial attacks on image classification systems, which involve small perturbations to images that lead to incorrect predictions. The authors propose feature denoising approaches to improve the robustness of convolutional networks against these attacks. They develop new network architectures with denoising blocks, inspired by self-attention transformers and non-local networks, and demonstrate significant improvements in adversarial robustness through empirical studies.\",\n    \"Direct Inspiration\": [\"b22\", \"b23\"],\n    \"Indirect Inspiration\": [\"b1\", \"b15\"],\n    \"Other Inspiration\": [\"b8\", \"b9\", \"b12\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial attacks on image classification systems. It proposes a method to improve the robustness of convolutional networks by incorporating denoising blocks that are trained end-to-end with adversarial training. Key inspirations include the usage of non-local means for feature denoising and the design principles from self-attention and non-local networks.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b22\": 1,\n    \"b23\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.9,\n    \"b9\": 0.8,\n    \"b7\": 0.7,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b10\": 0.4,\n    \"b14\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial attacks on image classification systems, which add small perturbations to images resulting in incorrect predictions. The authors propose a new convolutional network architecture equipped with feature denoising blocks to improve robustness against these attacks. The denoising operations are inspired by self-attention transformers and non-local networks, and the models are trained end-to-end on adversarial samples. The proposed method shows significant improvements in both white-box and black-box adversarial settings.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b23\": 1.0,\n    \"b1\": 1.0,\n    \"b15\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b20\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the vulnerability of convolutional neural networks to adversarial attacks, which add small perturbations to images causing incorrect predictions. The proposed solution involves developing new convolutional network architectures equipped with feature denoising blocks, trained end-to-end on adversarial samples to reduce feature-map perturbations and improve adversarial robustness. The study explores various denoising operations such as non-local means, bilateral filters, mean filters, and median filters.\",\n  \"Direct Inspiration\": [\"b1\", \"b22\", \"b23\"],\n  \"Indirect Inspiration\": [\"b15\", \"b9\"],\n  \"Other Inspiration\": [\"b7\", \"b12\"]\n}\n```"], "5db80dc83a55acd5c14a2492": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial examples in deep neural networks (DNNs), particularly focusing on linear classifiers with Gaussian mixture data. It proposes a new definition of strong-adversarial examples and derives quantitative formulas for the probabilities of their existence. The paper shows that adversarial-robust classifiers require much higher signal-to-noise ratios (SNR) compared to good-performing classifiers and introduces methods to design better training algorithms for strong-adversarial-robust classifiers.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b4\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of adversarial examples in DNNs, proposing a new definition of strong-adversarial examples and deriving probabilistic formulas for their existence in linear classifiers handling Gaussian mixture data.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1.0,\n        \"b1\": 0.9,\n        \"b13\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b14\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.7,\n        \"b15\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of adversarial examples in deep neural networks (DNNs) and introduces new definitions for strong-adversarial examples. It quantifies the probability of their existence and shows that achieving adversarial robustness requires much higher signal-to-noise ratios (SNR). The study extends the theoretical understanding of adversarial examples, particularly for linear classifiers on Gaussian mixture data.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b1\": 0.95,\n    \"b4\": 0.9,\n    \"b14\": 0.85,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial examples in deep neural networks and proposes a new definition of strong-adversarial examples, specifically for linear classifiers on Gaussian mixture data. The authors derive quantitative formulas for the probabilities of adversarial and strong-adversarial examples and demonstrate that strong-adversarial-robust classifiers require a lower signal-to-noise ratio (SNR) compared to adversarial-robust classifiers.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b1\": 0.9,\n    \"b14\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b10\": 0.75,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.65,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial examples in deep neural networks, which can cause misclassification with minimal perturbations. It proposes a new definition of strong-adversarial examples and derives quantitative formulas for their existence in the context of linear classifiers on Gaussian mixture data. The study shows that while adversarial-robust classifiers require a significantly higher signal-to-noise ratio (SNR), strong-adversarial-robust classifiers can achieve robustness with a more practical SNR, given better training algorithms.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b13\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b14\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.8,\n    \"b15\": 0.8\n  }\n}\n```"], "5da2f8aa3a55ac3402d8c2e1": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial machine learning, particularly focusing on the problem of defending against adversarial attacks in the context of ImageNet. The proposed algorithm, BaRT (Barrage of Random Transforms), leverages a collection of multiple randomized image transformations to create a robust defense against adversarial attacks. By applying different transformations in a stochastic manner, the method aims to prevent adversaries from easily circumventing the defenses.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b25\": 0.75,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.65,\n    \"b12\": 0.6,\n    \"b14\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the development of a robust defense mechanism against adversarial attacks on machine learning models, particularly within the context of ImageNet. The novel algorithm proposed involves using a barrage of randomized transformations to create a robust ensemble defense, which contrasts with prior methods that rely on single transformations or fixed strategies. The paper seeks to overcome the issue of obfuscated gradients, which has rendered many previous defenses ineffective.\",\n  \"Direct Inspiration\": [\"b3\", \"b5\"],\n  \"Indirect Inspiration\": [\"b14\", \"b15\", \"b24\", \"b25\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of defending against adversarial attacks on ImageNet by proposing an ensemble of randomized image transformations that provide robustness against attacks. This approach, termed BaRT (Barrage of Random Transforms), involves selecting and applying different stochastic transformations to images during both training and testing phases. The key challenge is overcoming obfuscated gradients that have thwarted previous defenses. The proposed method leverages multiple weak defenses combined in a randomized manner to create a stronger defense.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b7\": 0.7,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b14\": 0.5,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial attacks on machine learning models, particularly in computer vision. The authors propose a novel defense mechanism called BaRT (Barrage of Random Transforms) that applies a collection of random transformations to images during both training and testing. This approach aims to mitigate the obfuscated gradients issue and improve robustness against adversarial attacks. The key contributions include demonstrating that ensembling weak defenses in a randomized fashion can create a strong defense and presenting state-of-the-art defense results on the ImageNet dataset.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b24\": 0.85,\n    \"b25\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b10\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of defending against adversarial attacks on deep learning models, particularly for computer vision tasks like those involving ImageNet. The proposed solution is a novel defense mechanism called BaRT (Barrage of Random Transforms), which utilizes a collection of randomly parameterized image transformations applied in a random order to create a robust ensemble defense. This method is inspired by previous works on singular transformations but aims to overcome the limitations of obfuscated gradients and provide a state-of-the-art defense against strong adversaries.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b5\": 0.9,\n    \"b25\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b12\": 0.7,\n    \"b24\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b26\": 0.55\n  }\n}\n```"], "5c5ce4fd17c44a400fc38abb": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the vulnerability of Convolutional Neural Networks (CNNs) to adversarial attacks, which involve small, human-imperceptible perturbations to input images that can fool the networks. The proposed algorithm is a model-agnostic defense mechanism based on image super-resolution (SR) and wavelet domain filtering to remove adversarial noise while retaining critical image content. This approach ensures robustness against adversarial attacks without degrading the performance on clean images.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b14\": 1,\n    \"b32\": 1,\n    \"b33\": 1,\n    \"b34\": 1,\n    \"b35\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.9,\n    \"b25\": 0.9,\n    \"b36\": 0.8,\n    \"b24\": 0.8,\n    \"b37\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of defending against adversarial attacks on CNNs by proposing a model-agnostic defense mechanism based on image super-resolution and wavelet domain filtering. The primary contributions include demonstrating the efficacy of image super-resolution as a defense strategy, retaining critical image content, and being agnostic to the target model's architecture.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b14\": 1.0,\n    \"b32\": 1.0,\n    \"b33\": 1.0,\n    \"b34\": 1.0,\n    \"b35\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b36\": 0.8,\n    \"b25\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b38\": 0.6,\n    \"b31\": 0.5,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of Convolutional Neural Networks (CNNs) to adversarial attacks and proposes a novel, model-agnostic defense mechanism using image super-resolution and wavelet filtering. This method enhances image resolution and removes adversarial noise, aiming to recover the original class labels without losing critical image content.\",\n  \"Direct Inspiration\": [\"b13\", \"b14\", \"b32\", \"b33\", \"b34\", \"b35\"],\n  \"Indirect Inspiration\": [\"b25\", \"b17\", \"b24\", \"b36\"],\n  \"Other Inspiration\": [\"b27\", \"b46\", \"b38\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of defending CNNs against adversarial attacks without losing critical image content, by proposing a model-agnostic defense mechanism based on image super-resolution (SR) and wavelet domain filtering. The authors demonstrate that these techniques can effectively suppress adversarial noise and restore original class labels.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b14\": 1,\n    \"b32\": 1,\n    \"b33\": 1,\n    \"b34\": 1,\n    \"b35\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of CNNs to adversarial attacks by proposing a model-agnostic defense mechanism using image super-resolution and wavelet domain filtering. This approach aims to mitigate adversarial perturbations without losing critical image content.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b14\": 1.0,\n    \"b32\": 1.0,\n    \"b33\": 1.0,\n    \"b34\": 1.0,\n    \"b35\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b24\": 0.7,\n    \"b17\": 0.9,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```"], "599c797a601a182cd2641df7": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of defending neural network classifiers against adversarial examples. The proposed solution, MagNet, introduces two novel properties: it does not modify the target classifier and is independent of the process for generating adversarial examples. MagNet uses detectors to identify adversarial examples and autoencoders to reform them, leveraging the manifold hypothesis. The paper evaluates MagNet against popular attacks and demonstrates its effectiveness, particularly in a graybox setting.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b34\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b21\": 0.7,\n    \"b25\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges addressed in this paper are the generation of adversarial examples that fool classifiers without affecting human recognition and the limitations of existing defenses against these examples. The proposed algorithm, MagNet, introduces two novel properties: it does not modify the target classifier and it is independent of the process for generating adversarial examples. MagNet uses detectors to identify adversarial examples and autoencoders to reform them, thereby mitigating mis-classifications.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1,\n        \"b4\": 1,\n        \"b33\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.7,\n        \"b23\": 0.7,\n        \"b24\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.5,\n        \"b21\": 0.5,\n        \"b22\": 0.5,\n        \"b25\": 0.5,\n        \"b34\": 0.5,\n        \"b35\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of defending neural network classifiers against adversarial examples, which are maliciously crafted inputs designed to fool the classifier. The proposed solution, MagNet, introduces two novel properties: it works as a blackbox without modifying the target classifier and is independent of the adversarial example generation process. The defense mechanism is based on detectors and reformers using autoencoders to distinguish between normal and adversarial examples and reform adversarial examples to their closest normal counterparts.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b34\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b21\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of defending neural network classifiers against adversarial examples. The proposed MagNet defense system neither modifies the target classifier nor relies on specific properties of the classifier, making it versatile across different neural networks. MagNet uses detectors to identify out-of-manifold examples and autoencoders to reform adversarial examples. The approach is evaluated under blackbox and graybox attack scenarios, showing significant improvements in classification accuracy on adversarial examples.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b23\": 0.8,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b19\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of defending against adversarial examples in neural networks. It proposes MagNet, a defense mechanism that does not modify the target classifier and is independent of the adversarial example generation process. MagNet uses detectors and autoencoders to detect and reform adversarial examples, respectively. The main contributions include defining adversarial examples, advocating for graybox defense using diversity, and demonstrating the effectiveness of MagNet in various attack scenarios.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b4\": 1.0,\n    \"b23\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b34\": 0.8,\n    \"b35\": 0.8,\n    \"b1\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b25\": 0.7\n  }\n}\n```"], "5ac1829d17c44a1fda917e29": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of poor robustness and vulnerability of artificial neural networks under adversarial attacks. The authors propose a novel approach called L2-nonexpansive neural networks (L2NNNs) that enhances robustness through a combination of three conditions: bounding the Lipschitz constant, maximizing the confidence gap, and maintaining distance in network architecture. The paper introduces new techniques such as weight regularization, a specially designed loss function, and adaptations to various layers to achieve these conditions.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b10\", \"b21\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b17\", \"b5\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b19\", \"b24\", \"b16\", \"b26\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of neural network robustness and vulnerability to adversarial attacks. It proposes a novel approach to enhance robustness by ensuring three conditions: limiting the Lipschitz constant, maximizing confidence gaps, and preserving distance between output logit-vectors. The methods introduced include weight regularization, new loss functions, and adaptations to layers such as norm-pooling and two-sided ReLU.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b21\": 0.8,\n    \"b26\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b19\": 0.65,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.55,\n    \"b8\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of robustness in artificial neural networks under adversarial attacks. The authors propose a novel approach that combines three conditions: bounding the Lipschitz constant, maximizing the confidence gap, and adopting a distance-preserving architecture. These conditions collectively enhance the network's robustness against white-box L2-bounded adversarial attacks, outperforming existing adversarial training methods.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b21\": 0.95,\n    \"b26\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b17\": 0.75,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.65,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"Robustness and vulnerability of artificial neural networks under adversarial attacks.\",\n        \"algorithm\": \"Combination of three conditions: Lipschitz constant no greater than 1, maximizing confidence gap, and network architecture that restricts confidence gaps minimally.\"\n    },\n    \"Direct Inspiration\": {\n        \"b10\": 1.0,\n        \"b21\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b17\": 0.9,\n        \"b5\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.7,\n        \"b26\": 0.7,\n        \"b4\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the robustness of neural networks against adversarial attacks by proposing a new approach called L2-nonexpansive neural networks (L2NNNs). This approach focuses on three conditions: bounding the Lipschitz constant, maximizing the confidence gap, and preserving the distance between input vectors with different labels. The paper introduces new techniques in weight regularization, loss function design, and network architecture adaptations to achieve these conditions.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b21\": 1,\n    \"b26\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b5\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```"], "5c8f2a8b4895d9cbc62ecf7c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating adversarial examples under black-box threat models, where the attacker does not have access to the gradient of the classification loss function. The authors propose a novel approach leveraging bandit optimization to exploit prior information about the gradient, aiming to improve query efficiency compared to existing methods.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b2\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b16\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating black-box adversarial examples, focusing on overcoming the limitations of current methods which require extensive queries. The authors propose a new approach using bandit optimization to exploit prior information about the gradient, enhancing the efficiency and effectiveness of the attack.\",\n  \"Direct Inspiration\": [\"b4\", \"b10\"],\n  \"Indirect Inspiration\": [\"b7\", \"b2\", \"b15\"],\n  \"Other Inspiration\": [\"b22\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating adversarial examples for neural networks in a black-box setting, where gradient information is not directly accessible. The proposed solution leverages bandit optimization to exploit prior information about the gradient, improving the efficiency and effectiveness of black-box adversarial attacks.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0,\n        \"b10\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.7,\n        \"b2\": 0.7,\n        \"b15\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.6,\n        \"b16\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of neural networks to adversarial examples, particularly under black-box threat models where gradient information is not accessible. The authors propose a new approach using bandit optimization to generate black-box adversarial examples more efficiently, leveraging prior information about the gradient.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b7\": 0.6,\n    \"b2\": 0.5,\n    \"b15\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.4,\n    \"b3\": 0.4,\n    \"b12\": 0.3,\n    \"b0\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating black-box adversarial examples for neural networks, which are vulnerable to adversarial attacks. The authors propose a new approach using bandit optimization to exploit prior information about the gradient, aiming to improve the efficiency of current methods.\",\n  \"Direct Inspiration\": [\"b4\", \"b10\"],\n  \"Indirect Inspiration\": [\"b7\", \"b2\", \"b15\"],\n  \"Other Inspiration\": [\"b5\", \"b11\", \"b16\"]\n}\n```"], "5ecbc8eb9fced0a24b52a39b": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the vulnerability of image scaling in machine learning to adversarial attacks, which can manipulate images to change their appearance when scaled to specific dimensions. The paper proposes a comprehensive analysis of image-scaling attacks, identifies their root cause, and develops effective defenses to prevent these attacks by sanitizing specific pixels processed by scaling algorithms.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b9\": 0.7,\n    \"b14\": 0.7,\n    \"b26\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of image scaling algorithms in machine learning, particularly focusing on how adversaries can manipulate images to change appearance when scaled. The authors provide a comprehensive analysis of this issue, identify the root cause, and propose effective defenses. Key contributions include theoretical analysis, practical validation, and the development of a new defense mechanism.\",\n  \"Direct Inspiration\": {\n    \"references\": [\n      \"b34\"\n    ],\n    \"confidence_score\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\n      \"b4\",\n      \"b31\"\n    ],\n    \"confidence_score\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"references\": [\n      \"b5\",\n      \"b9\",\n      \"b14\"\n    ],\n    \"confidence_score\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of image scaling in machine learning, identifying the interplay of downsampling and convolution as the root cause. It proposes a defense mechanism to sanitize pixels and prevent adversarial manipulation during image scaling.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b31\": 0.8,\n    \"b9\": 0.7,\n    \"b14\": 0.7,\n    \"b8\": 0.6,\n    \"b26\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of image-scaling attacks which pose a significant security threat to machine learning systems. The authors analyze the root cause of these attacks and propose effective defenses. They identify the interplay of downsampling and convolution as the underlying vulnerability, validate their findings across popular imaging libraries, and develop a novel defense strategy to sanitize pixels and prevent attacks.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b31\": 0.8,\n    \"b9\": 0.6,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b10\": 0.4,\n    \"b11\": 0.4,\n    \"b12\": 0.4,\n    \"b6\": 0.3,\n    \"b17\": 0.3,\n    \"b30\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of image-scaling attacks in machine learning, identifying vulnerabilities in scaling algorithms and proposing effective defenses. The authors provide a comprehensive theoretical analysis of the root cause of these attacks and validate their findings through empirical evaluation of popular imaging libraries. They develop a new defense mechanism that sanitizes specific pixels processed by scaling algorithms to prevent adversarial manipulation.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7\n  }\n}\n```"], "5eccb534e06a4c1b26a83a1b": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the vulnerability of Deep Neural Networks (DNNs) to bit-flip attacks, particularly the newly proposed Bit-Flip Attack (BFA) which can significantly degrade a DNN's performance with minimal bit-flips. The authors propose weight binarization and piece-wise clustering as effective defensive techniques against BFA. The paper also examines other adversarial defense methods and model regularization techniques.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b4\": 0.7,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b9\": 0.5,\n    \"b5\": 0.5,\n    \"b1\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of Deep Neural Networks (DNNs) to bit-flip attacks and proposes defensive techniques to enhance fault-tolerance. The primary challenges include the susceptibility of DNNs to adversarial bit-flip attacks and the lack of effective defensive measures. The authors propose weight binarization and piece-wise clustering as countermeasures.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b1\": 0.5,\n    \"b9\": 0.5,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of defending against Bit-Flip Attack (BFA) on Deep Neural Networks (DNNs) by proposing weight binarization and piece-wise clustering as countermeasures. The primary contributions include a comprehensive investigation into BFA, proposing binarization-aware training and piece-wise clustering as defensive techniques, and examining additional adversarial defense methods.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b4\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b9\": 0.6,\n    \"b5\": 0.5,\n    \"b1\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of defending Deep Neural Networks (DNNs) against Bit-Flip Attack (BFA), a type of adversarial weight attack. The proposed solution involves weight binarization and piece-wise clustering as effective defensive techniques.\",\n    \"Direct Inspiration\": {\n        \"b16\": 1,\n        \"b17\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b9\": 0.7,\n        \"b14\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b5\": 0.6,\n        \"b7\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the fault-tolerance capability of Deep Neural Networks (DNNs) against bit-flip attacks, including weak random faults and strong malicious attacks like Row-Hammer Attack (RHA) and Bit-Flip Attack (BFA). The proposed solutions include weight binarization and piece-wise clustering as effective defenses.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b14\": 0.7,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b18\": 0.4,\n    \"b6\": 0.4\n  }\n}\n```"], "5e09caba3a55ac662f721afe": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the efficiency and effectiveness of adversarial training in deep learning models. The core contribution is the introduction of a novel method, ATTA (Adversarial Training with Transferable Adversarial examples), which leverages the high transferability of adversarial examples between models from neighboring training epochs. This approach significantly reduces training time while maintaining comparable model robustness compared to traditional methods.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b19\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b26\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the efficiency and effectiveness of adversarial training in deep learning models. It proposes a novel method called ATTA (Adversarial Training with Transferable Adversarial examples) that leverages the high transferability of adversarial examples between neighboring training epochs to accumulate attack strength, thereby reducing computational overhead while maintaining robustness.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b26\": 0.7,\n    \"b36\": 0.7,\n    \"b22\": 0.6,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b31\": 0.5,\n    \"b19\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the efficiency and effectiveness of adversarial training in deep learning models by leveraging high transferability of adversarial examples between neighboring training epochs. The proposed method, ATTA (Adversarial Training with Transferable Adversarial examples), reuses adversarial perturbations across epochs to reduce computation time while maintaining model robustness.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b38\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b23\": 0.7,\n    \"b26\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high computational overhead in adversarial training for deep learning models. It introduces a novel method called ATTA (Adversarial Training with Transferable Adversarial examples) that reuses adversarial perturbations from previous epochs to enhance efficiency and effectiveness. The method leverages the high transferability of adversarial examples between models from neighboring epochs, significantly reducing training time while maintaining model robustness.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.9,\n    \"b19\": 0.8,\n    \"b22\": 0.8,\n    \"b23\": 0.8,\n    \"b26\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b31\": 0.6,\n    \"b6\": 0.5,\n    \"b10\": 0.5,\n    \"b16\": 0.5,\n    \"b24\": 0.5,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"High computational overhead in generating adversarial examples for adversarial training.\",\n        \"inspirations\": \"High transferability of adversarial perturbations between models from neighboring training epochs.\"\n    },\n    \"Direct Inspiration\": {\n        \"b20\": 1,\n        \"b38\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.7,\n        \"b22\": 0.6,\n        \"b23\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b36\": 0.5,\n        \"b26\": 0.4\n    }\n}\n```"], "5eb789d3da5629cf24430b41": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing the intrinsic robustness of neural networks against adversarial attacks in an attack-agnostic manner. The proposed Feature Pyramid Decoder (FPD) framework incorporates denoising and image restoration modules along with a Lipschitz Constant Constraint at the classification layer. It employs a two-phase training strategy to improve both image quality and classification performance.\",\n  \"Direct Inspiration\": [\"b28\"],\n  \"Indirect Inspiration\": [\"b9\", \"b17\", \"b26\", \"b10\", \"b24\"],\n  \"Other Inspiration\": [\"b8\", \"b12\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing the intrinsic robustness of neural networks against adversarial attacks. It proposes an attack-agnostic defence framework called Feature Pyramid Decoder (FPD) that incorporates denoising and image restoration modules, as well as a Lipschitz Constant Constraint at the classification layer. The framework aims to be effective against both white-box and black-box attacks without crafting specific adversarial attacks.\",\n  \"Direct Inspiration\": [\n    \"b28\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b10\",\n    \"b24\",\n    \"b5\",\n    \"b30\"\n  ],\n  \"Other Inspiration\": [\n    \"b9\",\n    \"b17\",\n    \"b26\",\n    \"b15\",\n    \"b4\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the intrinsic robustness of neural networks against adversarial attacks in an attack-agnostic manner. The proposed solution, Feature Pyramid Decoder (FPD), enhances block-based CNNs by incorporating denoising and image restoration modules, and applying a Lipschitz Constant Constraint at the classification layer. The primary contributions include the novel FPD framework and a two-phase training strategy for self-supervised and multi-task learning.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b17\": 0.6,\n    \"b26\": 0.6,\n    \"b0\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the weakness of neural networks in image classification faced with adversarial attacks. It proposes a novel attack-agnostic defense framework, the Feature Pyramid Decoder (FPD), which enhances the intrinsic robustness of CNNs through denoising and image restoration modules, and a Lipschitz Constant Constraint at the classification layer. The framework is trained using a two-phase strategy involving multi-task and self-supervised learning.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b17\": 0.6,\n    \"b10\": 0.5,\n    \"b24\": 0.5,\n    \"b8\": 0.4,\n    \"b12\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing the intrinsic robustness of neural networks against both white-box and black-box adversarial attacks. The proposed Feature Pyramid Decoder (FPD) framework incorporates denoising and image restoration modules, along with a Lipschitz Constant Constraint at the classification layer. The framework aims to achieve attack-agnostic defense without the need for crafting specific adversarial attacks.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b12\": 0.5\n  }\n}\n```"], "5dcbd5da3a55ac789b0dbc7f": ["```json\n{\n  \"Summary\": \"The paper addresses the problem of adversarial attacks on deep neural networks (DNNs) using control theoretic approaches, specifically Lyapunov theory. The main contributions include a novel framework for bounding the response of DNN layers to adversarial perturbations, characterizing the stability and robustness of DNNs, and proposing a new robust training method based on layer-wise spectral regularization. The paper demonstrates the effectiveness of the proposed method through theoretical analysis and extensive empirical tests.\",\n  \"Direct Inspiration\": [\"b10\", \"b27\", \"b36\", \"b7\"],\n  \"Indirect Inspiration\": [\"b12\", \"b33\", \"b9\"],\n  \"Other Inspiration\": [\"b23\", \"b21\", \"b38\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving the robustness of Deep Neural Networks (DNNs) against adversarial attacks by using Lyapunov theory from control theory. The proposed method involves treating each layer of the DNN as a nonlinear system and developing a framework to set tight bounds on the response of individual layers to adversarial perturbations based on the spectral norm of the weights. This approach allows for a more flexible and layer-specific spectral regularization, which leads to more accurate and robust networks compared to existing methods.\",\n    \"Direct Inspiration\": [\"b7\", \"b27\", \"b36\", \"b10\"],\n    \"Indirect Inspiration\": [\"b12\", \"b33\", \"b9\", \"b38\", \"b34\", \"b39\"],\n    \"Other Inspiration\": [\"b21\", \"b37\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the robustness of Deep Neural Networks (DNNs) against adversarial attacks. The authors propose an algorithm based on Lyapunov theory to improve stability and robustness of DNNs by regularizing the spectral norm of the weight matrix at each layer. This novel approach aims to provide a tighter bound on the response of individual layers to adversarial perturbations, thereby enhancing the overall robustness of the network.\",\n  \"Direct Inspiration\": [\"b7\", \"b27\", \"b36\"],\n  \"Indirect Inspiration\": [\"b10\", \"b28\", \"b22\", \"b24\", \"b25\"],\n  \"Other Inspiration\": [\"b3\", \"b31\", \"b4\", \"b1\", \"b30\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of adversarial attacks on DNNs by leveraging Lyapunov theory to establish stability and robustness. It proposes a novel training method that regularizes the spectral norm of weight matrices at each layer independently based on Lyapunov conditions. The approach aims to improve the robustness and accuracy of DNNs against adversarial attacks.\",\n    \"Direct Inspiration\": [\"b7\", \"b27\", \"b36\", \"b10\"],\n    \"Indirect Inspiration\": [\"b12\", \"b9\", \"b35\"],\n    \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of adversarial attacks on deep neural networks (DNNs) using a Lyapunov-theory-based approach to ensure robustness and stability. The proposed method focuses on treating each layer as a nonlinear system and regularizing the spectral norm of weight matrices to improve robustness against adversarial perturbations.\",\n  \"Direct Inspiration\": [\"b7\", \"b27\", \"b36\", \"b10\"],\n  \"Indirect Inspiration\": [\"b9\", \"b35\"],\n  \"Other Inspiration\": [\"b12\", \"b33\"]\n}\n```"], "5f0ee1ca91e011ead96654a4": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of adversarial robustness in deep networks, particularly focusing on how multitask learning affects this robustness. The key contribution is the theoretical and empirical demonstration that multitask learning improves adversarial robustness by increasing output dimensionality, making it harder for adversarial perturbations to fool all tasks simultaneously.\",\n    \"Direct Inspiration\": {\n        \"b33\": 1.0,\n        \"b45\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b46\": 0.8,\n        \"b5\": 0.7,\n        \"b59\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.6,\n        \"b4\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of deep networks being brittle to adversarial examples. It proposes that multitask learning improves adversarial robustness while maintaining performance on natural examples. The authors theoretically and empirically show that increasing output dimensionality in multitask models enhances robustness against both single-task and multitask adversarial attacks.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b33\": 0.8,\n    \"b45\": 0.85,\n    \"b46\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b19\": 0.7,\n    \"b34\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.65,\n    \"b6\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the brittleness of deep networks to adversarial examples and how multitask learning can enhance adversarial robustness. The paper proposes a novel approach by showing that multitask learning not only improves task performance but also enhances robustness against adversarial attacks. The theoretical and empirical results demonstrate that increasing the number of tasks in a model improves its robustness by making it harder for perturbations to affect all tasks simultaneously.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1,\n    \"b33\": 0.9,\n    \"b59\": 0.9,\n    \"b0\": 0.85,\n    \"b4\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b46\": 0.75,\n    \"b5\": 0.7,\n    \"b39\": 0.7,\n    \"b54\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.65,\n    \"b58\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving adversarial robustness in deep networks by investigating the effects of multitask learning. It shows that multitask learning can enhance robustness against adversarial attacks while maintaining performance on natural examples. Theoretical and empirical analyses are provided to support these claims.\",\n    \"Direct Inspiration\": {\n        \"b45\": 1.0,\n        \"b33\": 1.0,\n        \"b59\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.9,\n        \"b46\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.8,\n        \"b34\": 0.8,\n        \"b5\": 0.8\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving adversarial robustness in deep networks. It proposes a novel approach using multitask learning to enhance robustness against adversarial attacks. The primary contributions include theoretical analysis and empirical validation showing that multitask models significantly improve robustness compared to single-task models.\",\n  \"Direct Inspiration\": [\"b33\", \"b45\", \"b46\"],\n  \"Indirect Inspiration\": [\"b2\", \"b59\"],\n  \"Other Inspiration\": [\"b5\", \"b39\", \"b54\"]\n}\n```"], "5dfc9de93a55acedae95f519": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of semantic segmentation of high-resolution remote sensing images (HRRSIs), which are different from natural images due to their wide imaging range and complex backgrounds. The authors propose a new semantic segmentation network called SCAttNet, which integrates spatial and channel attention mechanisms to improve the segmentation accuracy, especially for small objects. The network uses two representative backbones for feature extraction: SegNet and ResNet50. The contributions include the development of the SCAttNet network, visualization of the attention mechanisms, and performance evaluation on the ISPRS Vaihingen and ISPRS Potsdam datasets.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b8\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.5,\n    \"b13\": 0.5,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of semantic segmentation of high-resolution remote sensing images (HRRSIs) by proposing a new network, SCAttNet, which integrates spatial and channel attention mechanisms. This approach aims to improve segmentation accuracy, especially for small objects, by enhancing feature representation both spatially and across channels. The backbone networks used include SegNet and ResNet50, with attention mechanisms added to refine features before final semantic inference.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b8\": 0.85,\n        \"b9\": 0.8,\n        \"b10\": 0.75,\n        \"b11\": 0.8,\n        \"b14\": 0.9,\n        \"b16\": 0.95\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.7,\n        \"b4\": 0.65,\n        \"b12\": 0.7,\n        \"b13\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b17\": 0.6,\n        \"b18\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of semantic segmentation of high-resolution remote sensing images (HRRSIs) and proposes a novel network called SCAttNet. SCAttNet integrates lightweight spatial and channel attention mechanisms to improve the accuracy of semantic segmentation, especially for small objects.\",\n  \"Direct Inspiration\": [\n    \"b16\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b7\",\n    \"b8\",\n    \"b9\",\n    \"b10\",\n    \"b11\",\n    \"b14\"\n  ],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of semantic segmentation of high-resolution remote sensing images (HRRSIs), which involves classifying each pixel into a specific category. Traditional methods face limitations due to reliance on artificial design features, while deep learning methods, though promising, need further adaptations for HRRSIs due to their unique characteristics. The proposed solution involves integrating lightweight spatial and channel attention mechanisms into a new network (SCAttNet) to enhance feature representation and improve segmentation accuracy, especially for small objects.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b8\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b7\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of semantic segmentation of high-resolution remote sensing images (HRRSIs) by proposing a new network, SCAttNet, which integrates both spatial and channel attention mechanisms. This approach aims to improve segmentation accuracy, particularly for small objects, by refining features in channels and spatial positions. The method's effectiveness is validated on the ISPRS Vaihingen and Potsdam datasets, showing notable performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b10\": 0.7,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b4\": 0.5,\n    \"b7\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```"], "573696026e3b12023e515eec": ["```json\n{\n    \"Summary\": \"The paper addresses the degradation problem in deep neural networks by introducing a deep residual learning framework. This framework uses shortcut connections to allow layers to fit a residual mapping, which is easier to optimize than the original mapping. The authors demonstrate the effectiveness of this approach on the ImageNet dataset, showing that their residual networks achieve significantly better results than plain networks.\",\n    \"Direct Inspiration\": [\"b39\", \"b15\", \"b40\", \"b42\"],\n    \"Indirect Inspiration\": [\"b11\", \"b20\", \"b21\", \"b48\"],\n    \"Other Inspiration\": [\"b7\", \"b22\", \"b35\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the degradation problem in deep neural networks, where increasing the depth of the network initially improves accuracy but then leads to higher training error. The authors propose a deep residual learning framework to address this issue, allowing the network to learn residual mappings instead of direct mappings, which simplifies optimization. The paper presents comprehensive experiments demonstrating the effectiveness of this approach on various tasks and datasets, notably achieving state-of-the-art results on the ImageNet classification dataset.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1,\n    \"b15\": 0.9,\n    \"b11\": 0.85,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b42\": 0.65,\n    \"b34\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b32\": 0.5,\n    \"b47\": 0.5,\n    \"b18\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the degradation problem in deep neural networks by introducing a deep residual learning framework. The core contribution is the concept of residual learning, which simplifies the optimization of very deep networks by reformulating the original mapping problem into a residual mapping problem.\",\n  \"Direct Inspiration\": [\"b39\", \"b40\", \"b11\", \"b15\"],\n  \"Indirect Inspiration\": [\"b1\", \"b32\", \"b47\", \"b42\"],\n  \"Other Inspiration\": [\"b18\", \"b20\", \"b34\", \"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the degradation problem in training deep neural networks, where increasing the network depth leads to higher training error, contrary to expectations. The paper proposes a novel deep residual learning framework to address this challenge by introducing shortcut connections that perform identity mapping, allowing the network to learn residual functions instead of directly fitting the desired underlying mapping. This approach makes it easier to optimize very deep networks and leads to significant improvements in training and validation accuracy.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b39\", \"b15\", \"b11\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b20\", \"b7\", \"b21\", \"b22\", \"b35\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b1\", \"b32\", \"b47\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"vanishing/exploding gradients\",\n      \"degradation problem in deep networks\"\n    ],\n    \"algorithm\": \"Deep Residual Learning Framework\"\n  },\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b32\": 1.0,\n    \"b47\": 1.0,\n    \"b15\": 1.0,\n    \"b39\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.9,\n    \"b20\": 0.9,\n    \"b42\": 0.9,\n    \"b11\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b26\": 0.6,\n    \"b10\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```"], "5cf48a3eda56291d582a1174": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of training Graph Convolutional Networks (GCNs) efficiently, particularly focusing on reducing memory usage, computational complexity, and improving convergence speed. The primary contribution is the proposed Cluster-GCN algorithm, which uses graph clustering to create efficient mini-batches, leading to better embedding utilization and reduced computational overhead.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the inefficiency of training Graph Convolutional Networks (GCNs), particularly in terms of memory usage, computational complexity, and convergence speed. The proposed algorithm, Cluster-GCN, leverages graph clustering to optimize embedding utilization, reduce memory requirements, and improve computational efficiency.\",\n    \"Direct Inspiration\": {\n        \"b8\": 0.9,\n        \"b7\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.7,\n        \"b5\": 0.6,\n        \"b1\": 0.5,\n        \"b2\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents the primary challenge of high computational and memory costs in training Graph Convolutional Networks (GCNs), especially for deep models. It proposes Cluster-GCN, which uses graph clustering algorithms to create efficient mini-batches that optimize embedding utilization, leading to significant memory and computational benefits. The new approach also includes a stochastic multi-clustering framework to improve convergence.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.8,\n    \"b2\": 0.7,\n    \"b1\": 0.6,\n    \"b4\": 0.5,\n    \"b6\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficiently training Graph Convolutional Networks (GCNs) on large-scale graphs by proposing a novel algorithm called Cluster-GCN. This algorithm leverages graph clustering to design mini-batches that maximize embedding utilization, thus reducing memory requirements and computational overhead. The proposed method significantly improves the training speed and memory efficiency compared to existing methods like full-batch gradient descent and mini-batch SGD. The paper also introduces a stochastic multi-clustering framework to further enhance the convergence speed.\",\n    \"Direct Inspiration\": {\n        \"b8\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b2\": 0.75,\n        \"b5\": 0.8,\n        \"b9\": 0.85\n    },\n    \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper are the high memory requirement and slow convergence speed of GCN training due to node dependencies and neighborhood expansion in deep GCNs.\",\n    \"inspirations\": \"The paper introduces a novel GCN training algorithm called Cluster-GCN, which exploits graph clustering structures to improve memory and computational efficiency.\"\n  },\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b1\": 0.6,\n    \"b4\": 0.5\n  }\n}\n```"], "58437722ac44360f1082efeb": ["```json\n{\n  \"Summary\": \"The paper addresses the problem of classifying nodes in a graph with limited labeled data by introducing a novel graph-based neural network model that avoids explicit graph-based regularization. It proposes a layer-wise propagation rule for neural networks inspired by spectral graph convolutions, aiming for efficient and scalable semi-supervised classification. The key contributions are a simplified propagation rule motivated by first-order approximation of spectral convolutions and demonstrating the model's performance in terms of accuracy and efficiency on various datasets.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.7,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.5,\n    \"b1\": 0.5,\n    \"b26\": 0.5,\n    \"b20\": 0.4,\n    \"b10\": 0.4,\n    \"b27\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of classifying nodes in a graph with limited labeled data using a novel graph convolutional network (GCN) model. The proposed algorithm avoids explicit graph-based regularization and introduces a layer-wise propagation rule inspired by spectral graph convolutions. The model is designed for fast and scalable semi-supervised classification of nodes in graphs.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b26\": 0.7,\n    \"b30\": 0.7,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of classifying nodes in a graph with limited labeled data by proposing a graph-based semi-supervised learning model. The authors introduce a neural network model that conditions on both node features and adjacency matrix without explicit graph-based regularization. They develop a layer-wise propagation rule for Graph Convolutional Networks (GCNs) motivated by spectral graph convolutions and demonstrate the model's efficiency and accuracy in semi-supervised classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b29\": 0.6,\n    \"b1\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Classifying nodes in a graph with limited labeled data and avoiding explicit graph-based regularization in the loss function.\",\n    \"Inspirations\": \"Introducing a layer-wise propagation rule for neural network models on graphs and using a graph-based neural network model for fast and scalable semi-supervised classification.\"\n  },\n  \"Direct Inspiration\": [\"b11\"],\n  \"Indirect Inspiration\": [\"b5\"],\n  \"Other Inspiration\": [\"b1\", \"b26\", \"b30\", \"b10\", \"b27\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of classifying nodes in a graph with limited labeled data, proposing a graph-based semi-supervised learning approach using a neural network model that avoids explicit graph-based regularization. The authors introduce a layer-wise propagation rule for neural network models directly operating on graphs, motivated by spectral graph convolutions. The model aims to improve classification accuracy and efficiency, particularly for scenarios where the adjacency matrix contains additional information beyond node features.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b29\": 0.6,\n    \"b1\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```"], "573695fd6e3b12023e511373": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generalizing convolutional neural networks (ConvNets) to high-dimensional and complex datasets where the graph structure is not known a priori. It proposes new unsupervised and supervised graph estimation strategies combined with supervised graph convolutions to handle text categorization and bioinformatics problems. The primary inspiration comes from extending the ideas of graph convolutional networks to more complex and large-scale classification tasks.\",\n  \"Direct Inspiration\": [\"b1\"],\n  \"Indirect Inspiration\": [\"b11\", \"b14\"],\n  \"Other Inspiration\": [\"b6\", \"b7\", \"b16\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generalizing convolutional networks to high-dimensional, general datasets without prior knowledge of the graph structure. The proposed solution involves unsupervised and supervised graph estimation strategies combined with supervised graph convolutions, extending previous work to large-scale classification problems like text categorization and bioinformatics.\",\n  \"Direct Inspiration\": [\"b1\"],\n  \"Indirect Inspiration\": [\"b11\", \"b3\", \"b14\"],\n  \"Other Inspiration\": [\"b2\", \"b8\", \"b16\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on generalizing Convolutional Neural Networks (ConvNets) to high-dimensional, non-grid structured data where the graph structure is not known a priori. It proposes unsupervised and supervised graph estimation strategies combined with supervised graph convolutions and extends previous works to large-scale classification problems such as text categorization and bioinformatics.\",\n  \"Direct Inspiration\": [\"b1\"],\n  \"Indirect Inspiration\": [\"b0\", \"b11\", \"b5\", \"b3\", \"b13\", \"b14\", \"b2\"],\n  \"Other Inspiration\": [\"b16\", \"b7\", \"b6\", \"b19\", \"b20\", \"b15\", \"b17\", \"b4\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generalizing Convolutional Neural Networks (ConvNets) to high-dimensional, non-Euclidean data where the graph structure is not known a priori. The authors propose novel unsupervised and supervised graph estimation strategies combined with supervised graph convolutions. Their method is applied to large-scale classification problems such as text categorization and bioinformatics, showing that it can match or outperform fully-connected networks with fewer parameters.\",\n  \"Direct Inspiration\": [\"b1\"],\n  \"Indirect Inspiration\": [\"b0\", \"b11\"],\n  \"Other Inspiration\": [\"b2\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generalizing Convolutional Networks (ConvNets) to high-dimensional, general datasets where the graph structure is not known a priori. The authors propose unsupervised and supervised graph estimation strategies combined with supervised graph convolutions to handle such data and demonstrate their method on text categorization and bioinformatics.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b6\": 0.7,\n    \"b16\": 0.6\n  }\n}\n```"], "5db92a1a47c8f76646200974": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning with hypergraphs in real-world network datasets by proposing HyperGCN, a novel training scheme for Graph Convolutional Networks (GCNs) on hypergraphs. HyperGCN approximates each hyperedge by a set of pairwise edges, reducing the complexity compared to previous methods. The paper discusses the use of HyperGCN for semi-supervised learning (SSL) and combinatorial optimization, highlighting its effectiveness and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b7\": 0.9,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b21\": 0.7,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b51\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning from hypergraphs, which represent complex relationships beyond pairwise associations. The proposed algorithm, HyperGCN, offers a novel training scheme for Graph Convolutional Networks (GCNs) on hypergraphs, which is effective for semi-supervised learning (SSL) and combinatorial optimization. HyperGCN approximates each hyperedge with a linear number of pairwise edges, enhancing efficiency over previous methods that use quadratic approximations.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b7\": 1,\n    \"b31\": 1,\n    \"b21\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.9,\n    \"b48\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b51\": 0.7,\n    \"b1\": 0.7,\n    \"b35\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces HyperGCN, a novel method for training Graph Convolutional Networks (GCNs) on hypergraphs. The main challenges addressed include the computational inefficiency of existing methods and the need for effective learning on hypergraphs in both semi-supervised learning (SSL) and combinatorial optimization. By approximating each hyperedge with a linear number of pairwise edges, HyperGCN achieves faster training times and effective performance. The approach is inspired by the spectral theory of hypergraphs and builds on existing hypergraph neural network models.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b16\": 0.9,\n    \"b21\": 0.9,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b24\": 0.8,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of learning with hypergraphs in various real-world network datasets. The authors propose HyperGCN, a novel training scheme for Graph Convolutional Networks (GCNs) specifically designed for hypergraphs. The key challenge is to manage the complex relationships in hypergraphs efficiently, particularly for tasks like semi-supervised learning (SSL) and combinatorial optimization. The proposed method approximates hyperedges by a set of pairwise edges, reducing computational complexity while maintaining effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b7\": 0.9,\n    \"b31\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b51\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b18\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning with hypergraphs, particularly in semi-supervised learning (SSL) and combinatorial optimization. The proposed solution, HyperGCN, approximates each hyperedge with a set of pairwise edges, reducing the complexity from quadratic to linear. This method is shown to be effective compared to state-of-the-art methods such as HGNN.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b7\": 0.9,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.8,\n    \"b6\": 0.7,\n    \"b1\": 0.6\n  }\n}\n```"], "5bdc31b417c44a1f58a0b894": ["```json\n{\n    \"Summary\": \"The paper addresses the limitations of traditional graph convolutional neural networks (GCNNs) in handling complex and multi-modal data correlations. It proposes a hypergraph neural network (HGNN) framework to model high-order data correlations through hyperedges. The HGNN offers a general framework for multi-modal data, efficient hyperedge convolution operations, and better performance in data representation learning tasks like citation network classification and visual object recognition.\",\n    \"Direct Inspiration\": {\n        \"b3\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b23\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.5,\n        \"b10\": 0.5,\n        \"b9\": 0.5,\n        \"b6\": 0.5,\n        \"b11\": 0.5,\n        \"b7\": 0.4,\n        \"b16\": 0.4,\n        \"b8\": 0.4,\n        \"b13\": 0.4,\n        \"b20\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively representing complex and high-order data correlations in multi-modal datasets using graph-based convolutional neural networks. To tackle this, the authors propose a hypergraph neural networks (HGNN) framework which utilizes hypergraph structures to model these correlations more comprehensively than traditional graph convolutional networks (GCNs). The key innovations include the use of hyperedges to encode high-order correlations and the introduction of hyperedge convolution operations to improve computational efficiency and effectiveness in representation learning.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b11\": 0.6,\n    \"b7\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of modeling complex, multi-modal data correlations that go beyond pairwise connections in traditional graph convolutional neural networks. To tackle this, the authors propose a Hypergraph Neural Networks (HGNN) framework that uses hypergraph structures to encode high-order data correlations through hyperedge convolution operations. This approach is shown to be effective in handling multi-modal data and improves performance on tasks such as citation network classification and visual object recognition.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b11\": 0.7,\n    \"b23\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b6\": 0.65,\n    \"b7\": 0.6,\n    \"b8\": 0.7,\n    \"b13\": 0.6,\n    \"b16\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of modeling complex and high-order data correlations in multi-modal data using traditional graph convolutional neural networks (GCNs). To tackle these challenges, the authors propose a Hypergraph Neural Networks (HGNN) framework that leverages hypergraph structures to encode high-order data correlations through hyperedge convolution operations. The novel approach is demonstrated to be effective in several tasks, including citation network classification and visual object recognition.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b13\": 0.6,\n    \"b16\": 0.6,\n    \"b20\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of modeling complex, high-order correlations in multi-modal data using graph-based convolutional neural networks (GCNs). Traditional GCNs are limited to pairwise connections, which is insufficient for complex data structures. The authors propose a hypergraph neural networks (HGNN) framework, which employs hypergraphs to encode high-order correlations beyond pairwise connections. This framework introduces a hyperedge convolution operation to efficiently utilize these complex correlations for data representation learning. The HGNN framework is shown to be effective on citation network classification and visual object recognition tasks, outperforming traditional GCN methods.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b9\": 0.6,\n    \"b6\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```"], "5e5e18de93d709897ce37796": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of over-fitting and over-smoothing in deep Graph Convolutional Networks (GCNs) for node classification. The proposed solution, DropEdge, randomly drops edges during training to reduce over-fitting by augmenting data and to mitigate over-smoothing by reducing the convergence speed of the node features to a stationary point.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b18\": 0.9,\n    \"b24\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b17\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of over-fitting and over-smoothing in deep Graph Convolutional Networks (GCNs) and proposes a novel method called DropEdge to mitigate these issues. DropEdge randomly drops out a certain rate of edges in the input graph during training, which serves as both a data augmentation technique and a message passing reducer, thus helping to prevent over-fitting and over-smoothing. The method is shown to improve the performance of several GCN architectures on various benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.8,\n    \"b17\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of over-fitting and over-smoothing in deep Graph Convolutional Networks (GCNs) used for node classification. It proposes a novel method called DropEdge, which involves randomly dropping edges during training to prevent these issues.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b18\": 1,\n    \"b32\": 1,\n    \"b24\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.9,\n    \"b8\": 0.9,\n    \"b31\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are over-fitting and over-smoothing in deep Graph Convolutional Networks (GCNs). The proposed algorithm, DropEdge, addresses these challenges by randomly dropping certain edges in the graph during training to prevent over-fitting and over-smoothing.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b15\": 1.0,\n    \"b32\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b17\": 0.7,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": {\n    \"primary_challenges\": [\"over-fitting\", \"over-smoothing\"],\n    \"novel_methods\": [\"DropEdge\"]\n  },\n  \"Direct Inspiration\": [\"b18\", \"b24\"],\n  \"Indirect Inspiration\": [\"b32\", \"b15\"],\n  \"Other Inspiration\": [\"b29\", \"b14\", \"b17\", \"b8\"]\n}\n```"], "5e5e18a493d709897ce22b32": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Scalability and efficiency of training deep Graph Convolutional Networks (GCNs) on large datasets and mitigating 'neighbor explosion'.\",\n    \"Inspirations\": \"Using graph sampling to create efficient minibatches for GCN training while addressing bias and variance issues.\"\n  },\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b4\": 1.0,\n    \"b14\": 1.0,\n    \"b3\": 0.9,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b29\": 0.8,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is the 'neighbor explosion' problem in Graph Convolutional Networks (GCNs) when scaling to larger datasets and deeper layers. The proposed solution, GraphSAINT, introduces a graph sampling based inductive learning method that samples the training graph first and then constructs a full GCN on the subgraph. This method addresses both scalability and accuracy issues by resolving the neighbor explosion problem and introducing normalization techniques to handle biases in the minibatch estimator. Experimental results show significant performance gains in both training accuracy and time.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b26\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge discussed in the paper is the 'neighbor explosion' problem in Graph Convolutional Networks (GCNs), which occurs when deeper layers require exponentially more support nodes, leading to increased training time and computational complexity. To address this, the paper proposes GraphSAINT, a novel graph sampling-based method for efficient training of deep GCNs. The method involves sampling the training graph first and then building a full GCN on the subgraph, thus avoiding the 'neighbor explosion' issue. The paper also develops normalization techniques and variance reduction analysis to ensure unbiased and efficient training.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b4\": 0.9,\n    \"b14\": 0.85,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b26\": 0.7,\n    \"b23\": 0.65,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.55,\n    \"b31\": 0.5,\n    \"b18\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently training deep Graph Convolutional Networks (GCNs) on large datasets by presenting GraphSAINT, a novel graph sampling-based method. This method mitigates the 'neighbor explosion' problem seen in GCNs by sampling the training graph first and then constructing a complete GCN on the subgraph. The paper introduces normalization techniques to address bias and variance reduction analysis to improve training quality. Key inspirations include methods that tackle the scalability of GCNs through layer sampling and graph sampling techniques.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b4\": 0.9,\n    \"b14\": 0.8,\n    \"b3\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.6,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the scalability challenges of training deep Graph Convolutional Networks (GCNs) on large graphs by proposing GraphSAINT, a graph sampling-based method. GraphSAINT resolves the 'neighbor explosion' issue inherent in deep GCNs by sampling subgraphs first and then building full GCNs on these subgraphs. The method also introduces normalization techniques and variance reduction analysis to address biases and improve training quality.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b4\": 0.8,\n    \"b3\": 0.7,\n    \"b14\": 0.7,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.6,\n    \"b8\": 0.5,\n    \"b29\": 0.6,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b30\": 0.4,\n    \"b31\": 0.4,\n    \"b18\": 0.4\n  }\n}\n```"], "5f7fdd328de39f0828397afd": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the scalability of Graph Neural Networks (GNNs) to handle billion-scale graphs and improving the efficiency and performance of GNN training and inference. The novel algorithm proposed, Graph neural network via Bidirectional Propagation (GBP), achieves sub-linear time complexity and superior performance through localized bidirectional propagation.\",\n  \"Direct Inspiration\": [\"b29\", \"b15\", \"b3\"],\n  \"Indirect Inspiration\": [\"b10\", \"b7\", \"b36\", \"b5\"],\n  \"Other Inspiration\": [\"b14\", \"b16\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalability and performance in Graph Neural Networks (GNNs) on large graphs. It proposes a new algorithm, Graph neural network via Bidirectional Propagation (GBP), which achieves sub-linear time complexity and superior performance by performing propagation from both the feature vector and the training/testing nodes.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b29\": 0.9,\n    \"b15\": 0.9,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b7\": 0.7,\n    \"b36\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b5\": 0.6,\n    \"b4\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling Graph Neural Networks (GNNs) to graphs with billions of edges. It introduces GBP (Graph neural network via Bidirectional Propagation), a scalable GNN with sub-linear time complexity. The algorithm uses bidirectional propagation to achieve efficient and scalable training and inference, and demonstrates superior performance across various datasets, including a graph with over 1.8 billion edges.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b15\": 1,\n    \"b3\": 1,\n    \"b10\": 0.9,\n    \"b7\": 0.9,\n    \"b36\": 0.9,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b39\": 0.8,\n    \"b1\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b4\": 0.7,\n    \"b13\": 0.6,\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling Graph Neural Networks (GNNs) to large graphs with billions of edges. The novel contribution is the Graph neural network via Bidirectional Propagation (GBP), which achieves sub-linear time complexity and superior performance in practice.\",\n  \"Direct Inspiration\": {\n    \"b29\": 0.9,\n    \"b15\": 0.8,\n    \"b10\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.6,\n    \"b16\": 0.6,\n    \"b3\": 0.6,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b4\": 0.5,\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces GBP (Graph neural network via Bidirectional Propagation), addressing the scalability issue of GNNs for graphs with billions of edges. It proposes a novel bidirectional propagation algorithm to reduce time complexity and improve performance in training and inference phases.\",\n  \"Direct Inspiration\": [\"b29\", \"b15\"],\n  \"Indirect Inspiration\": [\"b10\", \"b7\", \"b36\"],\n  \"Other Inspiration\": [\"b14\", \"b3\", \"b16\"]\n}\n```"], "5f03f3b611dc83056223205d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of performance degradation in deep graph neural networks (GNNs) due to the over-smoothing issue. It proposes a novel algorithm, Deep Adaptive Graph Neural Network (DAGNN), which decouples representation transformation from propagation and uses an adaptive adjustment mechanism to balance local and global information for each node. This allows the network to learn from larger receptive fields without performance deterioration.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b14\": 1.0,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b11\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the performance degradation in deep graph neural networks due to the over-smoothing issue and the entanglement of transformation and propagation operations. The authors propose a novel method called Deep Adaptive Graph Neural Network (DAGNN), which decouples these two operations and introduces an adaptive adjustment mechanism to balance local and global neighborhood information.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b32\": 1.0,\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b8\": 0.7,\n    \"b29\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on addressing the over-smoothing issue in Graph Convolutional Networks (GCNs) and proposes a novel Deep Adaptive Graph Neural Network (DAGNN) to enhance node representation learning by decoupling transformation and propagation processes. The primary challenges identified include performance degradation in deep GCNs due to the entanglement of representation transformation and propagation, and the over-smoothing issue leading to indistinguishable node representations.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b14\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b10\": 0.9,\n    \"b30\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.75,\n    \"b31\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the performance degradation in deep Graph Neural Networks (GNNs) due to the over-smoothing issue. It proposes decoupling representation transformation and propagation to build deeper GNNs, introducing a new network called Deep Adaptive Graph Neural Network (DAGNN) to adaptively incorporate information from large receptive fields.\",\n    \"Direct Inspiration\": {\n        \"b14\": 0.95,\n        \"b32\": 0.90,\n        \"b10\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.80,\n        \"b30\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.70,\n        \"b11\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performance degradation in graph neural networks (GNNs) due to the over-smoothing issue when stacking multiple layers. It proposes a new method, Deep Adaptive Graph Neural Network (DAGNN), which decouples representation transformation from propagation and incorporates an adaptive adjustment mechanism to balance local and global information, allowing deeper networks without performance deterioration.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b30\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b32\": 0.75,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.65,\n    \"b8\": 0.6,\n    \"b29\": 0.55\n  }\n}\n```"], "5e3d353b3a55ac4de4104f40": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to improve the effectiveness and efficiency of collaborative filtering (CF) by addressing the limitations of the NGCF model, specifically its heavy and burdensome design. The paper introduces LightGCN, which simplifies the model by including only the most essential component of GCN - neighborhood aggregation, and removing feature transformation and nonlinear activation, which were found to negatively impact NGCF's performance. The inspiration for LightGCN comes from empirical findings and the need to remove unnecessary operations in GCN for CF.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b20\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b21\": 0.75,\n    \"b25\": 0.6,\n    \"b29\": 0.6,\n    \"b39\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of information overload in recommender systems and critiques existing methods like NGCF for being overly complex and not optimized for collaborative filtering. The proposed LightGCN model simplifies the design by removing unnecessary operations, such as feature transformation and nonlinear activation, and focuses on neighborhood aggregation for improved performance.\",\n  \"Direct Inspiration\": {\n    \"b38\": 0.9,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.6,\n    \"b39\": 0.6,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving collaborative filtering (CF) in recommender systems by proposing a new model, LightGCN. The authors critique existing methods, particularly NGCF, for their heavy reliance on feature transformation and nonlinear activation, which they find unnecessary and detrimental for CF tasks. LightGCN simplifies the process by focusing on neighborhood aggregation without these operations, resulting in better performance and easier training.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b20\": 0.9,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.6,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b16\": 0.7,\n    \"b25\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies in the NGCF model for collaborative filtering by highlighting that feature transformation and nonlinear activation do not contribute positively to the recommendation task. The authors propose a simplified model, LightGCN, which retains only the essential neighborhood aggregation component from GCN, resulting in better performance and easier training.\",\n  \"Direct Inspiration\": [\"b11\", \"b20\", \"b38\"],\n  \"Indirect Inspiration\": [\"b16\", \"b22\", \"b23\", \"b25\"],\n  \"Other Inspiration\": [\"b7\", \"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving collaborative filtering (CF) in recommender systems by simplifying the existing NGCF model. The authors propose a new model, LightGCN, which eliminates feature transformation and nonlinear activation, focusing solely on neighborhood aggregation. This simplification aims to enhance model performance and ease of training.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b25\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```"], "5a73cbcc17c44a0b3035f1d3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting deception in high-stakes situations by using multi-modal features from video, audio, and text. The proposed method leverages dynamic motion signatures for recognizing facial micro-expressions, integrating these with audio and text features to improve detection accuracy.\",\n  \"Direct Inspiration\": [\"b7\", \"b23\", \"b16\"],\n  \"Indirect Inspiration\": [\"b9\", \"b10\", \"b26\"],\n  \"Other Inspiration\": [\"b2\", \"b18\", \"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately detecting deception, particularly in high-stakes situations such as courtrooms, where traditional methods like polygraphs and fMRI are limited due to cost, overt nature, and reliability issues. The authors propose a multi-modal approach that integrates motion dynamics, audio, and text features to detect micro-expressions and deceptive behavior. The novel method involves using Improved Dense Trajectories (IDT) for motion features, Mel-frequency Cepstral Coefficients (MFCC) for audio features, and Glove for text features, along with a Fisher Vector encoding to aggregate these features into a fixed-length vector for classification.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b23\": 1.0,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting deception in high-stakes situations using a multi-modal approach that includes motion dynamics for facial micro-expressions, audio features, and text features. The proposed solution integrates low-level features such as dense trajectories and MFCCs with high-level features for better deception detection accuracy.\",\n  \"Direct Inspiration\": [\"b7\", \"b23\", \"b16\"],\n  \"Indirect Inspiration\": [\"b9\", \"b10\", \"b18\", \"b2\"],\n  \"Other Inspiration\": [\"b26\", \"b5\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of deception detection in high-stakes situations, proposing a novel approach that uses motion dynamics to recognize facial micro-expressions, supplemented by audio and text features. The approach is inspired by psychological insights on motion dynamics and leverages multi-modal feature extraction to improve detection accuracy.\",\n  \"Direct Inspiration\": [\n    \"b7\",\n    \"b23\",\n    \"b16\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b6\",\n    \"b9\",\n    \"b10\",\n    \"b13\"\n  ],\n  \"Other Inspiration\": [\n    \"b26\",\n    \"b2\",\n    \"b5\",\n    \"b15\",\n    \"b14\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of detecting deception in high-stake situations by leveraging motion dynamics for recognizing facial micro-expressions, and integrating features from multiple modalities (visual, auditory, and textual). The authors propose a novel two-level feature representation method for capturing dynamic motion signatures and demonstrate its effectiveness on courtroom trial videos.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b23\": 0.85,\n        \"b16\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.75,\n        \"b10\": 0.75,\n        \"b26\": 0.7,\n        \"b18\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.65,\n        \"b5\": 0.6,\n        \"b14\": 0.6,\n        \"b15\": 0.6\n    }\n}\n```"], "56d86073dabfae2eee7807ea": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting deceptive behavior in courtrooms, proposing a multimodal system that analyzes both verbal and non-verbal cues using real-life trial data. This novel approach aims to improve accuracy over traditional polygraph tests, which are subject to error and bias, and existing learning-based methods that often rely on artificial settings.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b27\": 0.9,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b40\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b19\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the detection of deceptive behavior in real-life trial data using a multimodal approach that incorporates both verbal and non-verbal cues. Inspired by the limitations of traditional methods like polygraph tests and the need for more reliable data, the authors propose a novel system that leverages a unique dataset of video clips from real court trials to extract linguistic and gesture-based features for deception detection.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b27\": 0.9,\n    \"b32\": 0.85,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.75,\n    \"b19\": 0.7,\n    \"b28\": 0.7,\n    \"b36\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b7\": 0.55,\n    \"b10\": 0.55,\n    \"b14\": 0.5,\n    \"b25\": 0.5,\n    \"b31\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the need for reliable deception detection in court trials, addressing the limitations of polygraph tests, and the lack of real-life data in deception detection research. The proposed algorithm introduces a multimodal system that uses both text and gestures to detect deception in real-life trial data, achieving accuracy significantly above the chance level.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b27\": 0.7,\n    \"b32\": 0.7,\n    \"b19\": 0.6,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of detecting deceptive behavior in court trials, using a novel multimodal system that combines text and gestures. It presents a dataset of real-life court trial videos and demonstrates the system's effectiveness in identifying deception with higher accuracy than humans.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.7,\n    \"b12\": 0.7,\n    \"b19\": 0.7,\n    \"b28\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.6,\n    \"b14\": 0.6,\n    \"b6\": 0.5,\n    \"b36\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting deception in court trials using a multimodal approach, incorporating both verbal and non-verbal cues. It introduces a novel dataset of 121 video clips from real court trials and builds a system to jointly use these modalities to identify deception with an accuracy of 60-75%, outperforming human judgment.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b27\": 0.7,\n    \"b12\": 0.7,\n    \"b40\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b14\": 0.5\n  }\n}\n```"], "5736974d6e3b12023e6388bf": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatic deception detection in real-life settings, proposing a multimodal system that uses verbal and nonverbal data to improve accuracy. The research involves creating a novel dataset from real trials and street interviews, extracting linguistic and gesture features, and developing classifiers to detect deception.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b16\": 0.7,\n    \"b28\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b30\": 0.65,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge outlined in the paper is the need for automated methodologies to detect deception efficiently and reliably in real-life settings, as opposed to controlled lab environments. Another challenge is the lack of real data for training deception detection models due to the artificial nature of lab settings.\",\n    \"Inspirations\": \"The paper is inspired by previous work on deception detection using text and speech modalities and aims to build a multimodal system that uses both verbal and nonverbal features to detect deception in real-life settings.\"\n  },\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b27\": 0.9,\n    \"b28\": 0.9,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b16\": 0.8,\n    \"b22\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b26\": 0.7,\n    \"b41\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting deceptive behavior in real-life settings using a multimodal system that combines verbal and nonverbal features. The proposed system significantly improves over baseline methods and outperforms human capabilities in deception detection. The main contributions include the collection of a novel dataset, extraction of linguistic and gesture features, and the evaluation of classification algorithms for deception detection.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b27\": 0.9,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b16\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of detecting deception in real-life settings using a multimodal system that analyzes both verbal and nonverbal behaviors. The system outperforms traditional polygraph methods and human judgment by leveraging data from real trials and live street interviews, and it uses machine learning classifiers to achieve high accuracy.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b27\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b22\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting deception in real-life settings using a multimodal approach. The authors propose a system that combines verbal and nonverbal behaviors to improve deception detection accuracy. They collected a dataset of 118 video clips from real trials and street interviews, annotated for linguistic and gesture features. Their experiments demonstrate that their system outperforms human ability in detecting deception.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b22\": 0.8,\n    \"b27\": 0.85,\n    \"b28\": 0.87\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.75,\n    \"b30\": 0.78,\n    \"b32\": 0.76\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b41\": 0.72\n  }\n}\n```"], "5db6c73a3a55acec0731cd68": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is handling nested Named Entity Recognition (NER) where entities overlap within a text. The authors propose a novel framework that formulates the NER task as a Machine Reading Comprehension (MRC) task instead of a sequence labeling problem, leveraging the advantages of the MRC framework to tackle entity overlapping and improve performance.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b28\": 1,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.9,\n    \"b35\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.8,\n    \"b13\": 0.8,\n    \"b3\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of nested Named Entity Recognition (NER), where entities overlap within a text. It proposes a novel approach by transforming the NER task into a Machine Reading Comprehension (MRC) task, inspired by the current trend of using question answering frameworks in NLP. The proposed method involves formulating entity recognition as answering specific natural language queries, which allows for handling both flat and nested NER more effectively. The approach is shown to outperform state-of-the-art models on various datasets.\",\n    \"Direct Inspiration\": [\"b21\", \"b22\", \"b28\"],\n    \"Indirect Inspiration\": [\"b35\", \"b36\"],\n    \"Other Inspiration\": [\"b8\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of nested Named Entity Recognition (NER) by proposing a new framework that formalizes NER tasks as Machine Reading Comprehension (MRC) tasks. This approach is inspired by the recent trend of transforming NLP problems into question-answering tasks, such as SQuAD-style MRC tasks. The proposed method handles both flat and nested NER by using natural language queries to extract entities, which helps tackle the issue of overlapping entities and provides semantic prior information for entity categories.\",\n  \"Direct Inspiration\": [\"b21\", \"b28\", \"b22\"],\n  \"Indirect Inspiration\": [\"b36\", \"b35\", \"b8\"],\n  \"Other Inspiration\": [\"b0\", \"b1\", \"b10\", \"b25\", \"b16\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the difficulty in handling nested Named Entity Recognition (NER) using traditional sequence labeling models. The proposed algorithm reformulates NER as a Machine Reading Comprehension (MRC) task, where each entity type is characterized by a natural language query. This approach naturally tackles the entity overlapping issue in nested NER and provides a semantic prior for entity categories, leading to performance improvements over state-of-the-art models.\",\n    \"Direct Inspiration\": {\n        \"b21\": 1,\n        \"b22\": 1,\n        \"b28\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b36\": 0.9,\n        \"b35\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.8,\n        \"b13\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"Challenges\": \"Handling both flat and nested Named Entity Recognition (NER) effectively, especially addressing the issue of entity overlapping in nested NER.\",\n        \"Inspirations\": \"The current trend of formalizing NLP problems as question answering tasks.\"\n    },\n    \"Direct Inspiration\": {\n        \"b21\": 1,\n        \"b28\": 1,\n        \"b22\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.8,\n        \"b36\": 0.75,\n        \"b35\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.6,\n        \"b1\": 0.6,\n        \"b10\": 0.6,\n        \"b25\": 0.6\n    }\n}\n```"], "5d1eb9ecda562961f0b261db": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the vulnerability of graph neural networks (GNNs) to adversarial attacks and the lack of effective mechanisms to ensure the robustness of GNNs against these perturbations. The authors propose a method for provable robustness of GNNs by providing robustness certificates and a robust training mechanism. They focus on perturbations of node attributes and employ convex relaxations to efficiently compute bounds on the worst-case changes in predictions. Additionally, they conduct extensive experiments to show the effectiveness of their proposed method.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b20\": 0.8,\n    \"b9\": 0.7,\n    \"b2\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b16\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the robustness of Graph Neural Networks (GNNs) against adversarial attacks. It introduces a method for provable robustness in GNNs by providing robustness certificates and a robust training procedure. The proposed solution includes convex relaxations and efficient computation of robustness bounds to ensure that GNNs can withstand perturbations in node attributes.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b17\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b21\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of ensuring the robustness of Graph Neural Networks (GNNs) against adversarial attacks. It proposes a method for certifying the robustness of GNNs by providing robustness certificates and a new robust training procedure. The approach focuses on perturbations of node attributes and introduces novel methods for handling discrete/binary data domains and the relational dependencies in graphs.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b17\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b21\": 0.6,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of Graph Neural Networks (GNNs) to adversarial attacks. It proposes the first method for provable robustness of GNNs by introducing robustness certificates and a robust training principle that accounts for perturbations in node attributes. The core contributions include deriving relaxations for efficient computation of worst-case changes and performing extensive experiments to demonstrate improved robustness.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of Graph Neural Networks (GNNs) to adversarial attacks and proposes methods for certifying and improving the robustness of GNNs. The core contributions are robustness certificates and a robust training method that ensures the reliability of GNNs even under perturbations.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b20\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b17\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.8,\n    \"b9\": 0.8\n  }\n}\n```"], "5bdc31b417c44a1f58a0b240": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively using graph-structured data in machine learning, particularly focusing on the limitations of existing graph kernel methods and graph neural networks (GNNs). The authors propose a theoretical exploration of the relationship between GNNs and 1-WL algorithm, showing their equivalence in distinguishing non-isomorphic graphs. They then introduce k-GNNs, which generalize GNNs using the k-dimensional Weisfeiler-Leman algorithm, demonstrating greater power and flexibility, especially in hierarchical structures.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b14\": 1.0,\n    \"b27\": 0.9,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.5,\n    \"b41\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of making Graph Neural Networks (GNNs) as expressive as the Weisfeiler-Lehman (WL) graph kernel while proposing k-GNNs and their hierarchical variants to capture higher-order graph properties that are not visible at the node level. The key contributions include theoretical equivalence between 1-WL and GNNs, the introduction of k-GNNs, and the demonstration that hierarchical k-GNNs outperform traditional GNNs in various tasks.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.95,\n    \"b14\": 0.9,\n    \"b27\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b28\": 0.75,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.65,\n    \"b41\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of developing machine learning models that effectively leverage graph-structured data. It proposes a theoretical exploration of the relationship between Graph Neural Networks (GNNs) and the 1-Weisfeiler-Leman (1-WL) algorithm. It introduces k-GNNs, which are based on the k-dimensional WL algorithm (k-WL), and demonstrates that these higher-order models can capture structural information beyond individual nodes, showing improvements in graph classification and regression tasks.\",\n    \"Direct Inspiration\": {\n        \"b14\": 1.0,\n        \"b27\": 1.0,\n        \"b10\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.9,\n        \"b19\": 0.9,\n        \"b40\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b28\": 0.8,\n        \"b35\": 0.8\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of graph neural networks (GNNs) and Weisfeiler-Lehman (WL) algorithms in distinguishing non-isomorphic graphs and their inability to handle continuous node features. The authors propose k-dimensional GNNs (k-GNNs) as a more powerful alternative by leveraging higher-order message passing directly between subgraph structures and introducing a hierarchical approach to combine graph representations at different granularities.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b15\": 0.7,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b19\": 0.6,\n    \"b1\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of effective graph representation in machine learning, particularly focusing on the limitations of current Graph Neural Networks (GNNs) in capturing higher-order graph structures. It proposes a novel k-GNN model, which generalizes GNNs using the k-dimensional Weisfeiler-Leman (k-WL) algorithm to enhance expressiveness and performance in graph-based tasks.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b27\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.75,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.65,\n    \"b28\": 0.6\n  }\n}\n```"], "5cf48a1eda56291d5827f814": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing Graph Neural Networks (GNNs) that are equivariant to permutation for a wide array of applications such as community detection and recommender systems. The main contribution is proving the universality of such equivariant GNNs using a new version of the Stone-Weierstrass theorem. The paper builds on previous work on invariant GNNs and aims to extend these results to the equivariant case.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b19\", \"b26\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b18\", \"b33\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b31\", \"b15\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges of designing Graph Neural Networks (GNNs) that are invariant or equivariant to permutations, with a primary focus on extending the universal approximation theorem to equivariant GNNs. The authors propose an algorithm based on a new version of the Stone-Weierstrass theorem, aiming to prove the universality of equivariant GNNs for high-order inputs such as graphs.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9,\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.7,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing Graph Neural Networks (GNNs) that are either invariant or equivariant to permutations of their nodes. The main contribution is the extension of the universal approximation theorem to equivariant GNNs, using a new version of the Stone-Weierstrass theorem. The authors provide alternative proofs for both invariant and equivariant GNNs, emphasizing their ability to approximate continuous functions on graphs of varying sizes.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.8,\n    \"b31\": 0.7,\n    \"b2\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing Graph Neural Networks (GNNs) that are invariant or equivariant to permutations. The main contribution is proving universal approximation theorems for invariant and equivariant GNNs, which was previously known only for invariant GNNs.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b2\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing Graph Neural Networks (GNNs) that are equivariant to permutations and proves the universality of such networks. The main contribution is extending the universal approximation theorem to equivariant GNNs using a new version of the Stone-Weierstrass theorem.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b18\": 1,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b22\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b2\": 0.6\n  }\n}\n```"], "5f75eed591e0111c1eb4da5f": ["```json\n{\n    \"Summary\": \"The paper presents UM-GNN, a novel architecture aimed at improving the robustness of Graph Neural Networks (GNNs) against adversarial attacks. Key challenges addressed include discrete adversarial search space, non-i.i.d. nature of nodes, and lack of effective metrics for structural perturbations. The approach uses uncertainty matching to transfer knowledge from a GNN model to a surrogate predictor, achieving significant improvements in robustness against various poisoning attacks.\",\n    \"Direct Inspiration\": {\n        \"b25\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.8,\n        \"b19\": 0.75,\n        \"b10\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b16\": 0.6,\n        \"b9\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the vulnerability of Graph Neural Networks (GNNs) to adversarial attacks, particularly focusing on poisoning attacks. The authors propose a novel approach called Uncertainty Matching-GNN (UM-GNN) to improve the robustness of GNN models. This approach involves jointly training a standard GNN model and a surrogate predictor using an uncertainty matching strategy to transfer knowledge and achieve robustness against structural perturbations.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b10\": 0.7,\n    \"b4\": 0.6,\n    \"b16\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.75,\n    \"b9\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of making graph neural networks (GNNs) robust against adversarial attacks, particularly poisoning attacks. It introduces a novel approach, UM-GNN, which employs an uncertainty matching strategy to transfer knowledge from a GNN to a surrogate predictor, enhancing robustness without relying on the graph structure. The paper also evaluates the proposed method against various poisoning attacks, demonstrating significant improvements over existing methods.\",\n  \n  \"Direct Inspiration\": {\n    \"b25\": 1.0\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b19\": 0.6,\n    \"b20\": 0.7,\n    \"b9\": 0.7\n  },\n  \n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing the robustness of Graph Neural Networks (GNNs) against adversarial attacks, particularly poisoning attacks. It proposes a novel approach, UM-GNN, which combines a standard GNN model with a surrogate predictor using an uncertainty matching strategy to improve robustness against these attacks.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b10\": 0.8,\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b16\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b11\": 0.5,\n    \"b14\": 0.5,\n    \"b9\": 0.4,\n    \"b4\": 0.4,\n    \"b1\": 0.3,\n    \"b20\": 0.3,\n    \"b21\": 0.3,\n    \"b23\": 0.3,\n    \"b3\": 0.3,\n    \"b13\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial attacks on graph neural networks (GNNs) by proposing a novel approach, UM-GNN, to improve the robustness of GNN models. The primary challenges include the discrete adversarial search space, non-i.i.d. nodes, and lack of effective metrics for structural perturbations. The proposed solution involves a joint training of a standard GNN and a surrogate predictor using an uncertainty matching strategy to enhance robustness against structural perturbations.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b19\": 0.7,\n    \"b16\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b20\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```"], "5eccb534e06a4c1b26a83a46": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding the influence of universal adversarial perturbations (UAPs) on deep neural networks (DNNs). The authors propose a novel analysis framework that treats DNN logits as a vector for feature representation to analyze the mutual influence of images and perturbations. They demonstrate that UAPs dominate the features in adversarial examples, while images behave like noise. This insight is leveraged to generate targeted UAPs using random source images as proxy data without access to the original training data.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b31\": 0.7,\n    \"b26\": 0.6,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b29\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper aims to address the challenges of understanding the adversarial vulnerability of deep neural networks (DNNs) by proposing a novel framework that disentangles the image and perturbation components of adversarial examples. The authors leverage the Pearson correlation coefficient (PCC) to analyze the mutual influence of these components on the feature representation of the DNN. Their findings suggest that universal adversarial perturbations (UAPs) dominate the feature representation over the clean images, leading to the insight that UAPs behave as features while images are treated as noise. This insight is used to generate targeted UAPs using proxy images without access to the original training data.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b16\": 0.9,\n    \"b31\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b17\": 0.75,\n    \"b26\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.6,\n    \"b32\": 0.65,\n    \"b42\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of adversarial attacks on deep neural networks (DNNs) by proposing an analysis framework that examines the interaction between images and perturbations. The authors introduce the concept of treating DNN logits as vectors for feature representation and utilize the Pearson correlation coefficient (PCC) to analyze the contribution of features from independent inputs. The novel approach focuses on understanding universal adversarial perturbations (UAPs) and introduces a method to generate targeted UAPs using random source images without original training data.\",\n    \"Direct Inspiration\": {\n        \"b16\": 1.0,\n        \"b31\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b26\": 0.8,\n        \"b17\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.6,\n        \"b27\": 0.6,\n        \"b29\": 0.6,\n        \"b41\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding the interaction between images and perturbations in adversarial examples, particularly focusing on Universal Adversarial Perturbations (UAPs). The authors propose a novel framework using logit vectors and Pearson correlation coefficient (PCC) to analyze the influence of images and perturbations on each other. Their findings suggest that UAPs have dominant features compared to images, which behave like noise. This insight is leveraged to generate targeted UAPs without original training data.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.9,\n    \"b27\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b29\": 0.7,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is understanding the interaction between images and perturbations in adversarial examples, particularly in universal adversarial perturbations (UAPs). The authors propose a novel analysis framework based on treating DNN logit values as feature vectors and using Pearson correlation coefficient (PCC) to analyze the influence of two independent inputs on each other. Their findings suggest that UAPs contain dominant features while images behave like noise, which leads to a new method for generating targeted UAPs without original training data.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.95,\n    \"b31\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b26\": 0.70,\n    \"b17\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.60,\n    \"b29\": 0.55,\n    \"b23\": 0.55\n  }\n}\n```"], "5a260c1d17c44a4ba8a21e76": ["```json\n{\n  \"Summary\": \"The paper addresses the primary challenges of effectively extracting text features and optimizing algorithms for higher accuracy in text classification. The proposed model combines global features extracted via GRU and local features extracted via global average pooling, and fuses them to achieve a comprehensive understanding, outperforming state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in text classification by proposing a novel model that combines global dependency features extracted by GRU and local semantic features extracted by global average pooling. The model aims to reduce feature overlap and improve text understanding and classification accuracy.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b7\": 0.5,\n    \"b8\": 0.5,\n    \"b9\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.6,\n    \"b12\": 0.4,\n    \"b13\": 0.4,\n    \"b14\": 0.4,\n    \"b15\": 0.4,\n    \"b16\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of effectively extracting text features and optimizing algorithms for higher accuracy in text classification. It proposes a novel model that combines global and local feature extraction from multiple modalities using GRU for long-term dependencies and global average pooling for short-term semantic context. The model aims to achieve a comprehensive understanding of language modeling and improve classification accuracy.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b3\": 0.85,\n    \"b11\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.6,\n    \"b7\": 0.6,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b2\": 0.5,\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively extracting text features and optimizing algorithms for higher accuracy in text classification. It proposes a novel model that utilizes different approaches to extract global and local features with low overlap from multiple modalities.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of extracting text features more effectively and optimizing algorithms for higher accuracy in text classification. It proposes a novel representation model that combines global and local feature extraction using GRU for global dependencies and global average pooling for local semantics, aiming to achieve a comprehensive understanding of text. The model is evaluated through experiments, showing improved performance over state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```"], "5c8fd41a4895d9cbc66534e9": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Manual feature extraction from aerial images is labor-intensive and time-consuming.\",\n      \"Existing deep learning methods require large amounts of training data, which is often imperfect or insufficient.\",\n      \"Current models struggle with accurately segmenting complex multiclass objects in urban environments.\"\n    ],\n    \"inspirations\": [\n      \"Deep learning techniques like CNNs for image segmentation.\",\n      \"U-Net and FCNs for improved object segmentation.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b17\", \"b20\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b12\", \"b13\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b10\", \"b11\", \"b24\", \"b26\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges addressed by the paper include the difficulty of manually extracting features from aerial images due to labor and time constraints, the limitations of existing automatic methods due to image noise and shadows, and the scarcity of large-scale, accurately labeled multiclass datasets for training deep learning models.\",\n    \"inspirations\": \"The paper is inspired by deep learning techniques such as convolutional neural networks (CNNs), particularly U-Net and pyramid pooling layers (PPL), to improve object segmentation from aerial images.\"\n  },\n  \"Direct Inspiration\": [\"b17\", \"b20\", \"b26\"],\n  \"Indirect Inspiration\": [\"b12\", \"b13\", \"b24\", \"b29\"],\n  \"Other Inspiration\": [\"b10\", \"b11\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of segmenting various objects from aerial images, particularly in complex urban areas. It proposes an enhanced semantic segmentation model, UNetPPL, which integrates U-Net architecture with an eight-level Pyramid Pooling Layer (PPL) to extract multiscale features and improve segmentation accuracy. The paper highlights the creation of a large, accurately labeled dataset for multiple object classes and demonstrates the superiority of the proposed model over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b17\": \"Inspired by U-Net architecture for more precise segmentation.\",\n    \"b20\": \"Inspired by the pyramid pooling module to enhance global context aggregation.\"\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": \"Inspired by fully convolutional networks (FCNs) for semantic segmentation.\",\n    \"b26\": \"Inspired by prior enhancements to U-Net for object segmentation from satellite images.\"\n  },\n  \"Other Inspiration\": {\n    \"b28\": \"Motivated by the limitations of OpenStreetMap data in training CNNs.\",\n    \"b30\": \"Used Adam optimizer for training the deep neural network.\",\n    \"b31\": \"Utilized the TensorFlow platform for implementation.\"\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in automatic object segmentation from aerial images, such as the need for large and accurately labeled datasets and the difficulties in distinguishing between closely spaced objects. The authors propose a novel model, UNetPPL, integrating U-Net architecture with an eight-level Pyramid Pooling Layer (PPL) to enhance segmentation accuracy and performance.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b24\": 0.75,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of segmenting multiple objects from aerial images using deep learning techniques, specifically focusing on urban areas. The proposed solution integrates U-Net architecture with an eight-level pyramid pooling layer (PPL) to enhance the segmentation accuracy and contextual feature extraction. The primary contributions include the creation of a large-scale, accurately labeled dataset and the development of a new semantic segmentation model using PPL.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.95,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b13\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.65,\n    \"b24\": 0.6,\n    \"b26\": 0.55\n  }\n}\n```"], "5b67b45517c44aac1c860823": ["```json\n{\n  \"Summary\": \"The paper addresses the dual challenges of effectiveness and efficiency in ranking models for recommender systems by proposing a technique called ranking distillation (RD). This method aims to achieve the ranking performance of a large model with the online inference efficiency of a small model by using knowledge distillation techniques adapted to ranking problems.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b32\": 0.5,\n    \"b34\": 0.4,\n    \"b36\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of balancing effectiveness and efficiency in ranking models used in information retrieval (IR) systems, particularly recommender systems. Inspired by knowledge distillation (KD) in image recognition, the authors propose a novel technique called ranking distillation (RD) to learn a compact ranking model that maintains high effectiveness while improving inference efficiency. The RD approach leverages a large, well-trained teacher model to generate additional training data for a smaller student model, which results in improved ranking performance and faster online inference.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b14\": 1,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b32\": 0.8,\n    \"b36\": 0.7,\n    \"b38\": 0.7,\n    \"b41\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b18\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.4,\n    \"b24\": 0.5,\n    \"b33\": 0.4,\n    \"b34\": 0.4,\n    \"b39\": 0.4,\n    \"b40\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of balancing effectiveness and efficiency in ranking models, particularly in recommender systems. It introduces a novel technique called ranking distillation (RD), inspired by knowledge distillation (KD), to create a compact ranking model that maintains high effectiveness while improving inference efficiency. The key contributions include the formulation of RD, the representation of the teacher's supervision, and the balance between the trust in training data and teacher-generated data.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1,\n        \"b14\": 1,\n        \"b19\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b22\": 0.8,\n        \"b33\": 0.8,\n        \"b38\": 0.8,\n        \"b41\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.6,\n        \"b32\": 0.6,\n        \"b34\": 0.6,\n        \"b36\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the dual challenges of effectiveness and efficiency in ranking models for recommender systems and IR. It proposes a novel method called ranking distillation (RD), which leverages a large teacher model to generate additional training data for a smaller student model, thereby achieving high ranking performance with reduced online inference latency.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b14\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b32\": 0.9,\n    \"b34\": 0.9,\n    \"b36\": 0.9,\n    \"b38\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b18\": 0.7,\n    \"b21\": 0.7,\n    \"b24\": 0.7,\n    \"b22\": 0.6,\n    \"b33\": 0.6,\n    \"b8\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of balancing effectiveness and efficiency in ranking models for recommender systems. It introduces a novel algorithm called ranking distillation (RD), which combines the concepts of knowledge distillation and learning to rank. The goal is to achieve the ranking performance of a large model while maintaining the online inference efficiency of a small model. This is accomplished by training a small student model using both the training set and additional data generated by a large teacher model.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b0\", \"b14\", \"b19\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b13\", \"b32\", \"b34\", \"b36\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b22\", \"b33\", \"b38\", \"b39\", \"b41\"]\n  }\n}\n```"], "5e3a93a93a55ac06c6119df5": ["```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is the trade-off between accuracy and efficiency in neural recommender models. The authors propose a novel knowledge distillation model for collaborative filtering named collaborative distillation (CD) to address data sparsity and ambiguity issues while maintaining the advantages of both KD and RD. The key contributions include a reformulated CF loss function, a probabilistic rank-aware sampling method, and two training tactics for the student model.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the trade-off between the accuracy and efficiency of neural recommender models, which typically suffer from higher latency during inference due to a large number of model parameters. The authors propose a novel knowledge distillation model for collaborative filtering (CF) named collaborative distillation (CD), which aims to balance effectiveness and efficiency. Key inspirations are drawn from previous work on knowledge distillation (KD) and rank distillation (RD). The novel contributions include an improved CF loss function, probabilistic rank-aware sampling, and two training tactics for the student model.\",\n  \"Direct Inspiration\": [\"b9\", \"b10\"],\n  \"Indirect Inspiration\": [\"b6\", \"b7\", \"b8\"],\n  \"Other Inspiration\": [\"b3\", \"b4\", \"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of balancing effectiveness and efficiency in neural recommender models, which typically suffer from high latency due to a large number of model parameters. It introduces a novel collaborative distillation (CD) model that combines the advantages of knowledge distillation (KD) and rank distillation (RD) to improve the performance of recommendation systems by addressing the sparsity and ambiguity of implicit user feedback.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b6\": 0.6,\n    \"b19\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.4,\n    \"b8\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper aims to develop a recommender model that balances effectiveness and efficiency by employing knowledge distillation (KD). The main challenges include handling sparse implicit user feedback and addressing the ranking problem in top-N recommendations. The proposed model, collaborative distillation (CD), combines the advantages of KD and rank distillation (RD) but improves over RD by preserving the original idea of KD and addressing its limitations.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of balancing effectiveness and efficiency in neural recommender models. The proposed solution is a new knowledge distillation model for collaborative filtering, called collaborative distillation (CD). This model aims to improve the performance of the student model by leveraging both positive and negative correlations among items and by employing a probabilistic rank-aware sampling method.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```"], "5f8d6be69fced0a24bbaaf7b": ["```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the large size of recommender system models which leads to high computational time and memory costs, making them difficult to apply in real-time platforms. The proposed solution, DE-RRD, aims to distill both the latent knowledge stored in the teacher model and the knowledge revealed from the teacher's predictions to improve the student model's performance while maintaining efficiency. The framework consists of two methods: Distillation Experts (DE) and Relaxed Ranking Distillation (RRD).\",\n    \"Direct Inspiration\": [\"b12\", \"b24\"],\n    \"Indirect Inspiration\": [\"b28\", \"b9\", \"b23\"],\n    \"Other Inspiration\": [\"b6\", \"b4\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Large model size leading to high computational time and memory costs\",\n      \"Insufficient utilization of teacher's latent knowledge\",\n      \"Point-wise knowledge distillation failing to maintain ranking orders\"\n    ],\n    \"proposed_algorithm\": \"DE-RRD framework combining Distillation Experts (DE) and Relaxed Ranking Distillation (RRD)\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b12\", \"b24\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b2\", \"b4\", \"b6\", \"b21\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b23\", \"b28\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of balancing effectiveness and efficiency in recommender systems (RS), specifically focusing on reducing model size and inference latency without compromising recommendation performance. The proposed DE-RRD framework combines two methods: Distillation Experts (DE) for transferring latent knowledge from the teacher model and Relaxed Ranking Distillation (RRD) for considering ranking orders in knowledge transfer. The framework aims to improve the student model by leveraging both the teacher's predictions and detailed latent knowledge.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b4\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of large-scale and complex recommender systems (RS) that require high computational resources and memory. The proposed solution, DE-RRD, introduces a novel knowledge distillation framework to reduce model size while maintaining performance. The framework includes two methods: Distillation Experts (DE) to transfer latent knowledge from the teacher model and Relaxed Ranking Distillation (RRD) to consider ranking orders among items.\",\n  \"Direct Inspiration\": [\"b12\", \"b24\"],\n  \"Indirect Inspiration\": [\"b6\", \"b23\", \"b28\"],\n  \"Other Inspiration\": [\"b4\", \"b9\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing the size and inference latency of recommender system models while maintaining or improving their performance. The proposed solution, DE-RRD, includes two novel methods: Distillation Experts (DE) for distilling latent knowledge from the teacher model and Relaxed Ranking Distillation (RRD) for preserving ranking orders in the student model.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b4\": 0.7,\n    \"b6\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```"], "5f0277e911dc830562231dab": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper are addressing various biases in recommender systems, such as popularity bias, previous model bias, and position bias. The proposed algorithm focuses on using a uniform data collection method to alleviate these biases and introduces a knowledge distillation framework (KD-CRec) to effectively use the uniform data.\",\n    \"inspirations\": \"The paper heavily relies on previous works related to counterfactual learning, knowledge distillation, and the impact of uniform data in recommender systems.\"\n  },\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b24\": 0.95,\n    \"b23\": 0.9,\n    \"b14\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b27\": 0.75,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.65,\n    \"b20\": 0.6,\n    \"b26\": 0.55\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses bias problems in recommender systems, particularly previous model bias, position bias, and popularity bias. The authors propose a knowledge distillation framework (KD-CRec) that leverages uniform data to mitigate these biases. The framework includes four approaches: label-based, feature-based, sample-based, and model structure-based distillation.\",\n    \"Direct Inspiration\": [\"b15\", \"b24\"],\n    \"Indirect Inspiration\": [\"b9\", \"b20\", \"b21\", \"b26\"],\n    \"Other Inspiration\": [\"b23\", \"b27\", \"b30\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses bias problems in recommender systems by leveraging uniform data through a knowledge distillation framework (KDCRec). The proposed framework includes four methods: label-based, feature-based, sample-based, and model structure-based distillation to improve recommendation performance.\",\n    \"Direct Inspiration\": {\n        \"b15\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b23\": 0.8,\n        \"b24\": 0.7,\n        \"b27\": 0.6,\n        \"b30\": 0.6,\n        \"b20\": 0.5,\n        \"b26\": 0.5,\n        \"b9\": 0.5,\n        \"b21\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.4,\n        \"b2\": 0.3,\n        \"b6\": 0.3,\n        \"b13\": 0.3,\n        \"b28\": 0.3,\n        \"b32\": 0.3\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the bias problems in recommender systems, specifically focusing on popularity bias, previous model bias, and position bias. It proposes a general knowledge distillation framework (KDCRec) to effectively use uniform data to mitigate these biases. The framework includes label-based distillation, feature-based distillation, sample-based distillation, and model structure-based distillation.\",\n    \"Direct Inspiration\": {\n        \"b15\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b23\": 0.8,\n        \"b24\": 0.7,\n        \"b16\": 0.7,\n        \"b27\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.5,\n        \"b21\": 0.5,\n        \"b31\": 0.4,\n        \"b30\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the bias problems in recommender systems, specifically popularity bias, previous model bias, and position bias. The authors propose a knowledge distillation framework (KD-CRec) to leverage uniform data to address these biases. The framework includes four approaches: label-based distillation, feature-based distillation, sample-based distillation, and model structure-based distillation.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b24\": 0.8,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b26\": 0.6,\n    \"b27\": 0.5\n  }\n}\n```"], "5d04e905da56295d08dd7af3": ["```json\n{\n  \"Summary\": \"The paper addresses two main challenges in recommender systems based on implicit feedback: the difficulty of utilizing implicit feedback due to the lack of negative samples and the time complexity of generating top-K recommendations. The authors propose a binarized collaborative filtering framework with distilling graph convolutional network (DGCN-BinCF) to improve the accuracy and efficiency of recommendations. The framework involves training a GCN-CF model to capture high-order feature interactions and then distilling the ranking information into a binarized model using a novel distillation loss.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.9,\n    \"b10\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper tackles the challenges of utilizing implicit feedback for top-K recommendation and improving computational efficiency. It proposes a novel framework called DGCN-BinCF, which distills ranking information from a GCN-CF model into a binarized model using a knowledge distillation technique. The framework also introduces a method for transforming binary optimization problems into equivalent continuous optimization problems.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.95,\n        \"b7\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.85,\n        \"b10\": 0.8,\n        \"b12\": 0.75,\n        \"b13\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.65,\n        \"b2\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of utilizing implicit feedback for recommender systems, which lacks negative samples and is time-consuming for top-K recommendations. The authors propose a novel framework, DGCN-BinCF, which combines graph convolutional networks (GCN) with binary collaborative filtering (Bin-CF) and knowledge distillation techniques to improve the performance and efficiency of recommendation systems.\",\n    \"Direct Inspiration\": {\n        \"b3\": 1,\n        \"b7\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.9,\n        \"b13\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.8,\n        \"b12\": 0.8\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in top-K recommendation systems based on implicit feedback: the difficulty in utilizing implicit feedback due to the lack of negative samples and the time-consuming nature of generating top-K preferred items for each user. To tackle these challenges, the authors propose a novel binarized collaborative filtering framework with distilling graph convolutional network (DGCN-BinCF). This framework includes training a CF-based GCN model (GCN-CF) to capture high-order feature interactions and then distilling ranking information from the trained GCN-CF model into a binarized model using knowledge distillation techniques.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b10\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b2\": 0.6,\n    \"b1\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in the context of top-K recommendation systems based on implicit feedback: the difficulty of utilizing implicit feedback due to the lack of negative samples and the time-consuming nature of generating top-K preferred items for each user. The authors propose a novel framework, DGCN-BinCF, which combines collaborative filtering (CF) with a graph convolutional network (GCN) and distillation techniques to improve the performance and efficiency of binarized recommendation systems.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```"], "5feb068b91e011f5d3420813": ["```json\n{\n  \"Summary\": \"The paper tackles the problems of 'skip' and 'search' in knowledge distillation for NLP tasks, proposing an attention-based method to combine intermediate teacher layers without skipping any. This new approach aims to enhance the student model's training by leveraging all available knowledge from deeper teacher models.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b20\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b8\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in knowledge distillation (KD) for neural networks, particularly focusing on utilizing intermediate layers' information to improve student models' performance. It identifies skip and search problems in existing methods and proposes an attention-based layer projection (ALP-KD) technique to combine teacher layers without skipping any. The approach aims to enhance the quality of distilled models by leveraging all teacher layers and using attention mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b8\": 0.6,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The main challenges addressed in the paper are the 'skip problem' and 'search problem' in knowledge distillation (KD) for neural networks. The proposed algorithm, ALP-KD (Attention-based Layer Projection for KD), aims to utilize all layers of the teacher model through an attention mechanism, thus avoiding the need to skip layers and simplifying the search for the best layers for distillation.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b20\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.9,\n        \"b21\": 0.9,\n        \"b5\": 0.8,\n        \"b13\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.7,\n        \"b8\": 0.7,\n        \"b18\": 0.6,\n        \"b11\": 0.6,\n        \"b23\": 0.5,\n        \"b22\": 0.5,\n        \"b25\": 0.5,\n        \"b16\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of knowledge distillation (KD) from deep teacher models to student models, focusing on the issues of layer skipping and arbitrary selection in intermediate layer matching. The authors propose an attention-based layer projection (ALP-KD) to combine all teacher layers effectively, thereby avoiding the skip and search problems seen in previous methods.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b20\": 0.9,\n    \"b21\": 0.8,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b13\": 0.7,\n    \"b8\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b18\": 0.65,\n    \"b11\": 0.65,\n    \"b23\": 0.6,\n    \"b22\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in knowledge distillation (KD), particularly focusing on the issues related to distilling information from intermediate layers of deep teacher models to student models. The primary contributions are the proposal of an attention-based layer projection (ALP-KD) to utilize all teacher layers without skipping any, and an approach to avoid the arbitrary selection of teacher layers.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b20\": 1.0,\n    \"b15\": 1.0,\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b13\": 0.8,\n    \"b18\": 0.8,\n    \"b8\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b2\": 0.7,\n    \"b22\": 0.7,\n    \"b25\": 0.7,\n    \"b23\": 0.7,\n    \"b16\": 0.7\n  }\n}\n```"], "5d3ed25a275ded87f97deb36": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating knowledge graphs (KGs) into recommender systems to capture user-specific item-item relatedness. The authors propose Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS), which extends GNNs to heterogeneous KGs. The key contributions include a user-specific relation scoring function and a technique for regularization of edge weights using label smoothness.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b34\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b30\": 0.7,\n    \"b13\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of utilizing Knowledge Graphs (KGs) in recommender systems, specifically focusing on capturing user-specific item-item relatedness. The proposed solution, Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS), aims to extend GNN architecture to KGs to capture semantic relationships and personalized user preferences. The algorithm involves transforming the KG into a user-specific weighted graph and using label smoothness regularization to prevent overfitting.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b34\": 0.9,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b18\": 0.7,\n    \"b30\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) to address challenges in utilizing knowledge graphs (KGs) in recommender systems. The main challenges addressed include the cold-start problem, poor scalability, and the need for end-to-end training. The proposed algorithm captures user-specific item-item relatedness in KGs using a personalized relation scoring function and a graph neural network, while also applying label smoothness regularization to improve generalization.\",\n  \"Direct Inspiration\": {\n    \"primary_references\": [\"b27\", \"b34\"]\n  },\n  \"Indirect Inspiration\": {\n    \"supporting_references\": [\"b31\", \"b13\", \"b18\", \"b30\"]\n  },\n  \"Other Inspiration\": {\n    \"additional_references\": [\"b8\", \"b25\", \"b26\", \"b33\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the incorporation of knowledge graphs (KGs) in recommender systems to capture user-specific item-item relatedness while dealing with the sparsity issue and cold-start problem. The proposed algorithm, Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS), extends GNNs to heterogeneous KGs to capture semantic relationships and personalized user preferences. The paper draws inspiration from various existing methods, particularly in the domain of GNNs, KG-aware recommendation systems, and label smoothness regularization.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b34\": 0.9,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b18\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b33\": 0.6,\n    \"b7\": 0.6,\n    \"b32\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of utilizing knowledge graphs (KGs) in recommender systems to capture user-specific item-item relatedness while overcoming issues such as manual feature engineering and poor scalability. The proposed solution is Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS), which extends GNNs to heterogeneous KGs to capture semantic relationships and personalized user preferences. The method includes a user-specific relation scoring function and a technique for regularizing edge weights to prevent overfitting.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b34\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b17\": 0.8,\n    \"b23\": 0.8,\n    \"b25\": 0.8,\n    \"b26\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b35\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```"], "5c8dd94c4895d9cbc6a7d918": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in embedding bipartite networks, focusing on the limitations of existing methods which do not consider the type information of vertices and fail to preserve the characteristics of bipartite networks. The proposed BiNE method introduces a joint optimization framework to account for both explicit and implicit relations and a biased random walk generator to better capture the network properties.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b13\": 0.85,\n    \"b3\": 0.8,\n    \"b10\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b14\": 0.65,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.55,\n    \"b0\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on the problem of learning vertex representations for bipartite networks. It proposes a novel method called BiNE (Bipartite Network Embedding) to address the limitations of existing network embedding methods, particularly in accounting for both explicit and implicit relations and retaining the properties of the bipartite network.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b7\": 1,\n    \"b13\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b14\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the inefficiency of traditional vector space methods for large-scale dynamic bipartite networks, and the limitations of existing network embedding methods such as DeepWalk, which do not account for the type information of vertices and fail to preserve the characteristics of a bipartite network. The paper proposes BiNE (Bipartite Network Embedding) to address these limitations by introducing a joint optimization framework for both explicit and implicit relations and a biased, self-adaptive random walk generator.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b11\": 1,\n    \"b13\": 1,\n    \"b14\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b10\": 0.9,\n    \"b33\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b28\": 0.7,\n    \"b34\": 0.7,\n    \"b36\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of learning vertex representations for bipartite networks by proposing a method called BiNE. The main challenges identified are the consideration of both explicit and implicit relations in bipartite networks and the preservation of the network's structural properties, such as power-law distribution. The proposed approach involves a joint optimization framework and a biased, self-adaptive random walk generator to address these challenges.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b13\": 0.85,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b10\": 0.7,\n    \"b19\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b33\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of learning vertex representations for bipartite networks. The main challenges identified are the need to account for both explicit and implicit relations in bipartite networks while preserving their unique properties, such as the power-law distribution of vertex degrees. The proposed solution, BiNE (Bipartite Network Embedding), introduces a joint optimization framework and a biased, self-adaptive random walk generator to better model these relations and network properties.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b13\": 1.0,\n    \"b19\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b11\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.5,\n    \"b34\": 0.5\n  }\n}\n```"], "5aed14d617c44a4438159040": ["```json\n{\n  \"Summary\": \"The paper presents YOLOv3, an improved version of the YOLO object detection system. It primarily focuses on enhancing bounding box prediction, class prediction, scale prediction, and feature extraction. YOLOv3 is inspired by previous versions of YOLO and other methods to achieve faster and more accurate object detection.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b6\": 0.8,\n    \"b9\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving YOLO (You Only Look Once) for object detection. The authors propose YOLOv3, which includes enhancements such as better bounding box prediction using dimension clusters, multilabel classification for class prediction, and predictions across three different scales. They also introduced a new feature extractor network called Darknet-53, which is more efficient and faster.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b17\": 0.9,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b8\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on the improvements made to YOLOv3 for object detection, highlighting its speed and accuracy improvements, bounding box prediction, class prediction, multi-scale predictions, and a new feature extractor network called Darknet-53. The paper also discusses the training techniques used and compares YOLOv3's performance with other state-of-the-art models.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include improving the speed and accuracy of object detection models, specifically YOLOv3. The novel contributions involve enhancing bounding box predictions, class predictions, and multi-scale predictions, employing a new feature extractor network, and refining the training process.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b0\": 0.7,\n    \"b7\": 0.6,\n    \"b6\": 0.6,\n    \"b9\": 0.6,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the improvements and updates made to the YOLO (You Only Look Once) object detection system, specifically YOLOv3. The authors introduce a faster and more accurate system by incorporating ideas from previous works, using multi-scale predictions, a new feature extractor network called Darknet-53, and various other techniques.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```"], "5fd3404791e01161cf73952c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning meaningful node representations for bipartite graphs, focusing on capturing global properties such as community structures of homogeneous nodes and long-range dependencies of heterogeneous nodes. The proposed model, BiGI, uses mutual information maximization to achieve this, combining global and local representations with a novel infomax objective.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1.0,\n    \"b41\": 1.0,\n    \"b42\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b37\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning meaningful node representations for bipartite graphs, which have unique structural characteristics compared to homogeneous and heterogeneous graphs. The proposed algorithm, BiGI, aims to capture global properties of bipartite graphs, such as community structures and long-range dependencies, by leveraging mutual information maximization. This is achieved through a novel local-global infomax objective, which integrates information from two node types into local and global representations.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1.0,\n    \"b41\": 1.0,\n    \"b42\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b37\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b31\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a novel Bipartite Graph embedding model called BiGI, which aims to capture the global properties of bipartite graphs, including community structures of homogeneous nodes and long-range dependencies of heterogeneous nodes. The model introduces a local-global infomax objective to maximize mutual information between local and global representations, thus preserving these global properties.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1.0, \n    \"b37\": 1.0, \n    \"b40\": 1.0, \n    \"b42\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9, \n    \"b41\": 0.9, \n    \"b44\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.7, \n    \"b31\": 0.6, \n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of modeling global properties of bipartite graphs, specifically community structures of homogeneous nodes and long-range dependencies of heterogeneous nodes. The proposed solution, BiGI, uses mutual information maximization to create global and local representations, leveraging a novel infomax objective to improve bipartite graph embeddings.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b16\": 0.7,\n    \"b31\": 0.7,\n    \"b41\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b37\": 0.4,\n    \"b40\": 0.4,\n    \"b42\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning meaningful node representations for bipartite graphs, which have distinct structural characteristics compared to homogeneous and heterogeneous graphs. Existing methods either focus on local graph structures or fail to preserve the global properties such as community structures and long-range dependencies. To tackle this, the authors propose a novel Bipartite Graph embedding called BiGI via mutual Information maximization. The main contributions include the introduction of a global representation composed of prototype representations, a subgraph-level attention mechanism for encoding local representations, and a local-global infomax objective to capture both local and global properties.\",\n  \"Direct Inspiration\": [\n    \"b36\",\n    \"b41\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b1\",\n    \"b7\",\n    \"b34\",\n    \"b37\",\n    \"b40\",\n    \"b42\"\n  ],\n  \"Other Inspiration\": [\n    \"b20\"\n  ]\n}\n```"], "5d1eb9dbda562961f0b15d56": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying Graph Neural Networks (GNNs) to bipartite graphs, focusing on the distinct distributions and dimensions of the two components. The authors propose the Bipartite Graph Neural Network (BGNN), which introduces inter-domain message passing and intra-domain alignment to effectively handle bipartite graph structures. The BGNN model is designed to be domain-consistent, unsupervised, and resource-efficient, featuring a cascaded training architecture for better memory efficiency and faster training times.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.9,\n    \"b9\": 0.85,\n    \"b10\": 0.9,\n    \"b7\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying Graph Neural Networks (GNNs) to bipartite graphs, which have two distinct sets of vertices with different feature distributions. The proposed Bipartite Graph Neural Network (BGNN) introduces two novel techniques: Inter-Domain Message Passing (IDMP) and Intra-Domain Alignment (IDA). These techniques are designed to fuse domain information in an unsupervised manner and enable efficient training via a cascaded architecture. The BGNN model shows superior performance compared to existing graph representation methods on several benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b14\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b5\": 0.8,\n    \"b7\": 0.7,\n    \"b9\": 0.65,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of applying Graph Neural Networks (GNNs) to bipartite graphs, where node features in the two components follow different distributions. The proposed Bipartite Graph Neural Network (BGNN) consists of inter-domain message passing (IDMP) and intra-domain alignment (IDA). The BGNN is trained using a cascaded architecture to improve memory efficiency and performance.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b7\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b17\": 0.6,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying Graph Neural Networks (GNNs) to bipartite graphs, which has not been studied before. The primary challenges include different distributions and dimensions of features in the two groups of bipartite graphs. The proposed solution, Bipartite Graph Neural Network (BGNN), includes two main operations: inter-domain message passing (IDMP) and intra-domain alignment (IDA), facilitating unsupervised domain information fusion and efficient cascaded training.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b7\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies the primary challenge of applying Graph Neural Networks (GNNs) to bipartite graphs due to different distributions and dimensions of features in two groups. The proposed Bipartite Graph Neural Network (BGNN) introduces two novel techniques: inter-domain message passing (IDMP) and intra-domain alignment (IDA), leveraging adversarial models for unsupervised domain information fusion and introducing a cascaded training architecture for efficient large-scale graph learning.\",\n  \"Direct Inspiration\": [\"b3\", \"b14\"],\n  \"Indirect Inspiration\": [\"b5\", \"b9\", \"b10\"],\n  \"Other Inspiration\": [\"b7\", \"b4\"]\n}\n```"], "5ece0f029e795ebde7de1cce": ["```json\n{\n    \"Summary\": \"The primary challenges addressed in this paper are the heterogeneity of node features in bipartite graphs and the scalability issues when dealing with extremely large-scale graphs. The proposed Cascade-BGNN algorithm introduces three key designs: inter-domain message passing (IDMP), intra-domain alignment (IDA) using adversarial learning, and a cascaded training method. These innovations aim to efficiently and effectively learn node representations in a self-supervised manner.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9,\n        \"b15\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b7\": 0.8,\n        \"b1\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b13\": 0.6,\n        \"b0\": 0.6,\n        \"b18\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of node representation learning on large-scale bipartite graphs in a self-supervised manner. It proposes Cascade-BGNN, a highly efficient graph neural network framework designed to handle the distinct feature distributions and scalability issues in bipartite graphs. Key innovations include inter-domain message passing (IDMP), intra-domain alignment (IDA) using adversarial models, and a cascaded training approach to improve memory efficiency and scalability.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b6\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of node representation learning in large-scale bipartite graphs with limited labels. It proposes a novel framework called Cascade-BGNN which incorporates inter-domain message passing (IDMP) and intra-domain alignment (IDA) to effectively capture features from distinct node domains. The model utilizes self-supervised learning and cascaded training to improve scalability and efficiency.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9,\n        \"b5\": 0.9,\n        \"b7\": 0.9,\n        \"b15\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b4\": 0.7,\n        \"b6\": 0.7,\n        \"b13\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.5,\n        \"b10\": 0.5,\n        \"b18\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of node representation learning on large-scale bipartite graphs in a self-supervised manner. The main challenges are the different feature distributions in bipartite graph partitions and the limitations due to the lack of node-wise labels and computational inefficiency. The proposed Cascade-BGNN framework introduces three key designs: inter-domain message passing (IDMP), intra-domain alignment (IDA), and cascaded training to improve scalability, efficiency, and effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b7\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.8,\n    \"b4\": 0.8,\n    \"b6\": 0.8,\n    \"b13\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in node representation learning on large-scale bipartite graphs: the different feature distributions of nodes in each partition and the limited label issue with computational inefficiency. To tackle these, the paper proposes Cascade-BGNN, a self-supervised learning framework that includes inter-domain message passing (IDMP) and intra-domain alignment (IDA) to aggregate and align features from different domains effectively. The model also introduces a cascaded training method to improve scalability and efficiency.\",\n  \n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b3\": 0.9,\n    \"b15\": 0.9\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b7\": 0.8\n  },\n  \n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b6\": 0.7,\n    \"b13\": 0.7\n  }\n}\n```"], "5edcbb9a91e0110f1d6db1c2": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of precisely predicting user preferences in large-scale e-commerce scenarios. The proposed Hierarchical bipartite Graph Neural Network (HiGNN) aims to exploit hierarchical high-order connections and non-linear interactions to improve tasks like user preference prediction and personalized browsing navigation. The approach stacks multiple GNN modules in a hierarchical fashion and uses general clustering algorithms to build a coarsened graph, making it scalable to real-world applications.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b18\": 0.9,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b7\": 0.7,\n    \"b22\": 0.7,\n    \"b28\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of precisely predicting user preferences in large-scale e-commerce environments. It proposes a Hierarchical bipartite Graph Neural Network (HiGNN) to learn hierarchical representations on bi-partite graphs, capturing non-linear interactions and hierarchical high-order connections to improve prediction tasks like CTR and CVR.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b19\": 0.8,\n    \"b21\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.75,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.65,\n    \"b26\": 0.6,\n    \"b30\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting user preferences and improving personalized recommendations in large-scale e-commerce scenarios by proposing a novel Hierarchical bipartite Graph Neural Network (HiGNN). This method learns hierarchical representations on bi-partite graphs to exploit high-order connections and non-linear interactions, making it scalable and efficient for large-scale applications.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b19\": 1.0,\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving user preference prediction and personalized browsing navigation in large-scale e-commerce scenarios by proposing a Hierarchical bipartite Graph Neural Network (HiGNN). This novel approach combines graph neural networks with hierarchical representation learning to exploit high-order connections and non-linear interactions in user-item graphs, making it scalable to large-scale applications.\",\n  \"Direct Inspiration\": [\"b17\", \"b18\", \"b19\", \"b21\"],\n  \"Indirect Inspiration\": [\"b7\", \"b16\"],\n  \"Other Inspiration\": [\"b0\", \"b4\", \"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting user preferences in large-scale e-commerce by proposing a Hierarchical bipartite Graph Neural Network (HiGNN). HiGNN leverages hierarchical representations on bi-partite graphs to capture both high-order connections and non-linear interactions for improved prediction tasks such as Click Through Rate (CTR) and Conversion Rate (CVR). The paper is inspired by previous works on graph neural networks, collaborative filtering, and hierarchical graph representations.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b18\": 0.9,\n    \"b19\": 0.9,\n    \"b21\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b7\": 0.6,\n    \"b28\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.4,\n    \"b29\": 0.4,\n    \"b30\": 0.3,\n    \"b31\": 0.3\n  }\n}\n```"], "5daedf5f47c8f76646131c6d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of personalized micro-video recommendation by considering the semantic gap and varying user preferences across different modalities (visual, acoustic, and textual). The proposed algorithm, Multi-modal Graph Convolution Network (MMGCN), uses a modality-aware bipartite user-item graph and information propagation mechanism inspired by Graph Convolution Networks (GCNs) to capture modal-specific user preferences.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b33\": 0.6,\n    \"b38\": 0.8,\n    \"b41\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b16\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of personalized micro-video recommendation by incorporating multi-modal information into historical interactions between users and micro-videos. It proposes a Multi-modal Graph Convolution Network (MMGCN) to encode high-order connectivity between users and micro-videos in each modality, capturing user preferences on modal-specific contents.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b19\": 0.6,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b16\": 0.5,\n    \"b29\": 0.5,\n    \"b38\": 0.5,\n    \"b41\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the semantic gap between different modalities, varying contributions of modalities to user preferences, and the need to distinguish and consider modal-specific user preferences. The proposed algorithm, MMGCN, uses graph convolution networks to capture high-order connectivity between users and micro-videos across different modalities to improve recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b38\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of personalized micro-video recommendation by proposing a Multi-modal Graph Convolution Network (MMGCN) that incorporates and distinguishes user preferences across different modalities (visual, acoustic, textual). The model uses a graph convolutional network (GCN) inspired mechanism to encode high-order connectivity between users and micro-videos in each modality, aiming to capture user preferences more accurately.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b7\": 0.7,\n    \"b16\": 0.7,\n    \"b29\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b33\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of personalized micro-video recommendation by leveraging multi-modal information (visual, acoustic, and textual). It highlights the limitations of existing multimedia recommendation models that treat multi-modal information as a whole and proposes a Multi-modal Graph Convolution Network (MMGCN) to capture modal-specific user preferences. The proposed model constructs a user-item bipartite graph for each modality and employs an aggregation and combination mechanism inspired by graph convolution networks (GCNs) to encode higher-order connectivity and user preferences.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b21\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b16\": 0.7,\n    \"b41\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b29\": 0.55\n  }\n}\n```"], "5f00587b9fced0a24b1fbbf1": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of MHC binding prediction, crucial for neoantigen identification in personalized immunotherapy. The proposed algorithm, USMPep, is a single-layer recurrent neural network trained end-to-end on a regression task, capable of handling variable-length input without heuristics or task-specific prior knowledge. The method offers competitive performance using standard training procedures and benchmark datasets, emphasizing simplicity and generalizability.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b10\": 0.9,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting MHC (Major Histocompatibility Complex) binding, which is crucial for personalized immunotherapy. The proposed algorithm, USMPep, utilizes a single-layer recurrent neural network trained end-to-end on a regression task without task-specific prior knowledge. This approach aims to simplify the prediction process compared to existing methods, which often involve complex training procedures and heuristic rules.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.95,\n    \"b11\": 0.9,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b14\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b15\": 0.65,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting MHC binding, a crucial task for personalized immunotherapy, using a simplified algorithm based on a recurrent neural network. It aims to overcome the complexity and limitations of existing methods by proposing a straightforward, end-to-end training approach without prior knowledge or artificial data.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b12\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.6,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b15\": 0.5,\n    \"b18\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include the difficulty of predicting MHC binding due to the structural diversity of peptides and the limitations of existing algorithms, such as complicated training procedures, reliance on fixed-length peptides, and lack of standardized evaluation. The authors propose a novel approach using a single-layer recurrent neural network trained end-to-end on a regression task without task-specific prior knowledge, capable of handling variable-length input. This approach contrasts with existing methods that use complex training protocols, artificial negative peptides, and heuristic rules.\",\n    \"Direct Inspiration\": [\"b9\", \"b11\"],\n    \"Indirect Inspiration\": [\"b10\", \"b12\", \"b14\"],\n    \"Other Inspiration\": [\"b7\", \"b8\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting MHC binding for personalized immunotherapy, proposing a novel single-layer recurrent neural network model named USMPep. This model is trained end-to-end on a regression task without task-specific prior knowledge and can incorporate variable length input. The authors emphasize the simplicity and competitive performance of their approach compared to existing complex methodologies.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b10\": 0.95,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b13\": 0.75,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b8\": 0.85,\n    \"b15\": 0.65\n  }\n}\n```"], "5f8cf5159e795ea21aee7f07": ["```json\n{\n  \"Summary\": \"The paper addresses two main challenges in existing graph neural networks (GNNs): their inability to generalize to heterophilic graphs and the over-smoothing problem that occurs with deep networks. The proposed GPR-GNN model combines Generalized PageRank (GPR) techniques with GNNs to adaptively learn propagation rules and mitigate over-smoothing, demonstrating superior performance on both homophilic and heterophilic graphs.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b22\": 1,\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in graph neural networks (GNNs): (1) the limitation of existing GNN architectures to effectively handle heterophilic (disassortative) graphs, and (2) the performance degradation due to feature over-smoothing in deep GNNs. The authors propose a novel GNN architecture called GPR-GNN, which combines Generalized PageRank (GPR) techniques with GNNs. This model adaptively learns the contributions of different propagation steps and adjusts them according to the homophily/heterophily structure of the graph. The empirical and theoretical results show that GPR-GNN outperforms existing methods on both homophilic and heterophilic graphs while mitigating feature over-smoothing.\",\n  \"Direct Inspiration\": [\"b19\", \"b22\", \"b28\"],\n  \"Indirect Inspiration\": [\"b10\", \"b36\"],\n  \"Other Inspiration\": [\"b7\", \"b9\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the limitations of existing Graph Neural Networks (GNNs) in handling heterophilic graphs and the issue of feature over-smoothing in deep GNNs. The proposed GPR-GNN model combines Generalized PageRank (GPR) techniques with GNNs to overcome these challenges. The model adaptively learns the weights for feature propagation steps, allowing it to handle both homophilic and heterophilic graphs effectively and mitigate the feature over-smoothing problem.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b22\": 0.9,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b18\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Existing GNN architectures are primarily designed for homophilic graphs and struggle with heterophilic graphs.\",\n      \"Most GNNs face performance degradation when increasing the number of layers due to feature-oversmoothing.\"\n    ],\n    \"novel_method\": \"GPR-GNN, a new model combining Generalized PageRank (GPR) techniques with GNNs, capable of handling both homophilic and heterophilic graphs and mitigating feature-oversmoothing.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\n      \"b22\",\n      \"b19\",\n      \"b18\",\n      \"b28\"\n    ]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\n      \"b36\",\n      \"b10\"\n    ]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\n      \"b9\",\n      \"b37\",\n      \"b14\"\n    ]\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper are: (1) Most existing GNN architectures are tailored for homophilic graphs and fail to perform well on heterophilic graphs. (2) The issue of feature oversmoothing in deep GNNs, which limits their effectiveness.\",\n    \"algorithm\": \"The algorithm proposed is GPR-GNN, which combines GNNs with Generalized PageRank (GPR) techniques. This approach allows adaptive learning of propagation rules and mitigates feature oversmoothing, making it effective for both homophilic and heterophilic graphs.\"\n  },\n  \"Direct Inspiration\": [\n    \"b19\",\n    \"b22\",\n    \"b36\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b28\",\n    \"b10\",\n    \"b18\"\n  ],\n  \"Other Inspiration\": [\n    \"b3\",\n    \"b12\"\n  ]\n}\n```"], "5db929b747c8f766461fa94f": ["```json\n{\n  \"Summary\": \"The paper presents a novel technique called Graph Diffusion Convolution (GDC), which combines the strengths of both spatial and spectral methods to enhance message passing in Graph Neural Networks (GNNs). GDC aggregates information from a larger neighborhood using a sparsified generalized form of graph diffusion, addressing the limitations of traditional message-passing neural networks (MPNNs) and spectral-based models.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b22\": 1.0,\n    \"b72\": 0.9,\n    \"b79\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b55\": 0.8,\n    \"b35\": 0.8,\n    \"b13\": 0.8,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b75\": 0.6,\n    \"b33\": 0.6,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Graph Neural Networks (GNNs) by proposing a novel technique called Graph Diffusion Convolution (GDC). This technique aims to reconcile the strengths of message-passing neural networks (MPNNs) and spectral-based models, while overcoming their respective limitations. The key contributions include the introduction of GDC, the analysis of its spectral properties, and the demonstration of its wide applicability to various graph-related tasks.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b22\": 0.85,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b34\": 0.65,\n    \"b62\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b75\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of current Graph Neural Networks (GNNs), particularly focusing on their reliance on immediate neighbor information and the arbitrary nature of message passing. The authors propose Graph Diffusion Convolution (GDC), inspired by spectral methods, which aggregates information from a larger neighborhood using a sparsified generalized form of graph diffusion. The core contributions include proposing GDC, analyzing its spectral properties, and demonstrating its applicability to various models and tasks.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b72\": 0.9,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b35\": 0.7,\n    \"b32\": 0.6,\n    \"b77\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b79\": 0.4,\n    \"b80\": 0.4,\n    \"b6\": 0.3,\n    \"b34\": 0.3,\n    \"b62\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of current Graph Neural Networks (GNNs) and proposes a novel method called Graph Diffusion Convolution (GDC) to improve message passing by leveraging spectral methods. It aims to enhance the aggregation of information from larger neighborhoods and improve performance across various tasks while maintaining scalability and generalizability.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b35\": 1,\n    \"b55\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b22\": 0.9,\n    \"b72\": 0.9,\n    \"b79\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.8,\n    \"b6\": 0.8,\n    \"b34\": 0.8,\n    \"b62\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in this paper are the limitations of current Graph Neural Networks (GNNs), specifically Graph Convolutional Networks (GCNs) and Message Passing Neural Networks (MPNNs), which only pass messages between neighboring nodes in each layer. The paper proposes a novel technique called Graph Diffusion Convolution (GDC) to overcome these limitations by aggregating information from a larger neighborhood using a generalized form of graph diffusion. This approach aims to combine the strengths of spatial and spectral-based methods, making it scalable, generalizable to unseen graphs, and applicable to any graph-based model.\",\n  \"Direct Inspiration\": [\"b15\", \"b31\"],\n  \"Indirect Inspiration\": [\"b22\", \"b32\", \"b72\", \"b79\"],\n  \"Other Inspiration\": [\"b35\", \"b55\"]\n}\n```"], "5e5e18ad93d709897ce2654c": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of inductive matrix completion without using content. It proposes a novel method, Inductive Graph-based Matrix Completion (IGMC), which transforms matrix completion into a link prediction problem using local graph patterns. The method involves extracting enclosing subgraphs around each user-item pair and feeding them to a graph neural network (GNN) to predict ratings. The paper highlights the superior performance of IGMC on several benchmark datasets, its robustness on sparse matrices, and its ability to generalize to unseen users/items without retraining.\",\n    \"Direct Inspiration\": {\n        \"b13\": 0.9,\n        \"b24\": 0.85,\n        \"b1\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.75,\n        \"b38\": 0.75,\n        \"b41\": 0.7,\n        \"b22\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.65,\n        \"b35\": 0.65,\n        \"b37\": 0.6,\n        \"b39\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of making matrix completion inductive without relying on content information. The proposed algorithm, Inductive Graph-based Matrix Completion (IGMC), uses local graph patterns and graph neural networks (GNN) to predict missing entries in the rating matrix by transforming the problem into a link prediction task. This approach avoids the scalability issues and the need for retraining that existing methods face.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b24\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b14\": 0.5,\n    \"b25\": 0.5,\n    \"b38\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges of matrix completion, particularly focusing on the inductive matrix completion problem where the goal is to predict ratings for unseen user-item pairs without relying on side information. The proposed solution, Inductive Graph-based Matrix Completion (IGMC), uses graph neural networks (GNNs) to learn local graph patterns from subgraphs around user-item pairs to make predictions. This method avoids the limitations of existing methods that either require retraining for new users/items or rely heavily on content information.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b1\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.75,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b39\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the limitation of existing matrix completion algorithms to generalize to unseen users/items without retraining, which is a significant issue in dynamic environments. The proposed algorithm, Inductive Graph-based Matrix Completion (IGMC), leverages local graph patterns in bipartite graphs to make inductive predictions without requiring side content or the entire rating matrix as input.\",\n  \"Direct Inspiration\": [\"b13\"],\n  \"Indirect Inspiration\": [\"b1\", \"b5\", \"b14\", \"b24\", \"b41\"],\n  \"Other Inspiration\": [\"b25\", \"b29\", \"b38\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of matrix completion in recommender systems, focusing on overcoming the limitations of transductive matrix factorization methods and inductive matrix completion (IMC) methods that rely heavily on content quality. The proposed method, Inductive Graph-based Matrix Completion (IGMC), leverages local graph patterns learned through a graph neural network (GNN) to predict ratings without using content or requiring the entire rating matrix as input. This approach is scalable and effective, achieving state-of-the-art performance on benchmark datasets and demonstrating strong transfer learning capabilities.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b5\": 0.7,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.5,\n    \"b39\": 0.5,\n    \"b41\": 0.4,\n    \"b46\": 0.4\n  }\n}\n```"], "5f6c6a6391e0119671e8583c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of high-performance and scalable graph pattern matching. It proposes GraphPi, a system with novel components including a 2-cycle based automorphism elimination algorithm, a 2-phase computation-avoid schedule generator, an accurate performance prediction model, and an optimization technique using the Inclusion-Exclusion Principle. The system aims to eliminate redundant computations and improve performance significantly.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b17\": 0.9,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of graph pattern matching, which include redundant computation due to automorphisms and the performance variability of different schedules. The proposed solution, GraphPi, includes four novel components: a 2-cycle based automorphism elimination algorithm, a 2-phase computation-avoid schedule generator, a performance prediction model, and an optimization technique using the Inclusion-Exclusion Principle.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b11\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b14\", \"b17\", \"b22\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b13\", \"b25\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high-performance, scalable pattern matching in large graph datasets. It proposes GraphPi, which introduces a novel 2-cycle based automorphism elimination algorithm, a 2-phase computation-avoid schedule generator, an accurate performance prediction model, and an optimization technique using the Inclusion-Exclusion Principle. These innovations aim to reduce redundant computations, select optimal schedules and restrictions, and improve overall performance in distributed environments.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b20\": 0.6,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of automorphisms and inefficient schedules in graph pattern matching. It proposes GraphPi, a high-performance distributed system with novel components such as a 2-cycle based automorphism elimination algorithm, a 2-phase computation-avoid schedule generator, a performance prediction model, and the Inclusion-Exclusion Principle for counting embeddings.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b17\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b14\": 0.7,\n    \"b22\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of high-performance distributed pattern matching in graph mining, specifically focusing on eliminating redundant computations caused by automorphisms and optimizing the selection of schedules and restriction sets. GraphPi introduces four novel components: a 2-cycle based automorphism elimination algorithm, a 2-phase computation-avoid schedule generator, a performance prediction model for selecting optimal combinations of schedules and restrictions, and an optimization technique using the Inclusion-Exclusion Principle for counting embeddings.\",\n    \"Direct Inspiration\": [\"b11\", \"b17\"],\n    \"Indirect Inspiration\": [\"b13\", \"b14\", \"b22\"],\n    \"Other Inspiration\": [\"b26\", \"b27\", \"b28\"]\n}\n```"], "555048d345ce0a409eb71be1": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively capturing semantic information in text for classification purposes. It proposes the Recurrent Convolutional Neural Network (RCNN) model, which combines a bi-directional recurrent structure with a max-pooling layer to capture contextual information and key components in texts. This model aims to overcome the limitations of traditional feature representation methods and other neural network models like RecursiveNN and RecurrentNN.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b23\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b7\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of text classification, particularly the limitations of traditional feature representation methods that ignore contextual information. The authors propose a Recurrent Convolutional Neural Network (RCNN) that combines a bi-directional recurrent structure and a max-pooling layer to better capture the semantics of texts while maintaining a time complexity of O(n).\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b16\": 0.85,\n        \"b23\": 0.8,\n        \"b24\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.7,\n        \"b12\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.6,\n        \"b1\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of feature representation in text classification, particularly focusing on the limitations of traditional methods like the bag-of-words model and the need for contextual information. The proposed solution is a Recurrent Convolutional Neural Network (RCNN) that combines the strengths of both recurrent and convolutional neural networks to better capture semantics and improve classification accuracy.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b16\": 0.9,\n    \"b23\": 0.85,\n    \"b24\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b10\": 0.75,\n    \"b12\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.65,\n    \"b17\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in text classification, particularly the feature representation problem. It introduces a novel Recurrent Convolutional Neural Network (RCNN) that combines bi-directional recurrent structures and max-pooling layers to better capture contextual information and semantics, overcoming limitations in traditional and existing neural network models.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b23\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b7\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of capturing semantic and contextual information for text classification. It proposes a Recurrent Convolutional Neural Network (RCNN) that combines bi-directional recurrent structures with max-pooling layers to leverage the advantages of both recurrent and convolutional neural models. This approach aims to solve the limitations of existing models, such as RecursiveNN and RecurrentNN, by improving context capture and reducing bias.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b7\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b10\": 0.7\n  }\n}\n```"], "5dc5488edf1a9c0c41511e1e": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is minimizing the amount of data transferred during matrix-matrix multiplication (MMM), both across the memory hierarchy and between processors. The proposed algorithm, COSMA, aims to achieve I/O optimality for all parameter combinations by explicitly modeling data reuse and parallelizing a near-optimal sequential schedule.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b34\": 0.9,\n    \"b53\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b56\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b50\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents COSMA, a new algorithm for matrix-matrix multiplication (MMM) that aims to minimize data transfer both across the memory hierarchy and between processors. The authors highlight the limitations of existing algorithms like ScaLAPACK, CARMA, and CTF, which either require manual tuning, are inefficient for certain matrix dimensions, or have other constraints. COSMA is proposed as an I/O optimal algorithm for any combination of input parameters and is based on a new constructive proof of a sequential I/O lower bound. The algorithm is designed to handle various matrix dimensions and numbers of processors efficiently.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b22\": 1.0,\n    \"b53\": 1.0,\n    \"b56\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b34\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b50\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are minimizing the amount of data transferred in matrix-matrix multiplication (MMM), both across the memory hierarchy and between processors, and addressing the inefficiencies of existing state-of-the-art algorithms. The proposed solution, COSMA (Communication Optimal Spartition-based Matrix multiplication Algorithm), aims to achieve I/O optimality for all combinations of parameters by explicitly modeling data reuse and deriving optimal domain decomposition. It also seeks to improve practical performance by minimizing communication costs and integrating effectively with existing data formats.\",\n  \n  \"Direct Inspiration\": [\"b22\", \"b34\", \"b56\"],\n\n  \"Indirect Inspiration\": [\"b0\", \"b53\"],\n\n  \"Other Inspiration\": [\"b10\", \"b50\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing matrix-matrix multiplication (MMM) by minimizing data transfer across memory hierarchies and between processors. The proposed algorithm, COSMA, focuses on deriving I/O optimal schedules using the red-blue pebble game to parallelize sequential schedules and achieve optimal domain decomposition. COSMA is designed to overcome limitations of existing algorithms like ScaLAPACK, CARMA, and CTF.\",\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [\"b22\", \"b34\", \"b53\"],\n  \"Other Inspiration\": [\"b10\", \"b56\", \"b50\"]\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is minimizing the amount of transferred data in matrix-matrix multiplication (MMM) algorithms, focusing on both vertical and horizontal I/O. The COSMA algorithm is introduced as a novel approach to achieve I/O optimality for all combinations of input parameters by explicitly modeling data reuse in the red-blue pebble game and parallelizing the sequential schedule.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.9,\n    \"b53\": 0.8,\n    \"b56\": 0.8,\n    \"b50\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```"], "5c04967517c44a2c74708b7e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of scaling neural networks for deep learning, particularly focusing on hardware constraints and the difficulty of designing efficient model parallelism algorithms. The authors propose GPipe, a library that enables efficient training of large neural networks by partitioning models across multiple accelerators and supporting re-materialization.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.9,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b38\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces GPipe, a flexible library for efficient training of large neural networks by enabling model parallelism. The primary challenges addressed include hardware constraints, memory limitations, and communication bandwidths. The novel contributions are in developing a pipeline parallelism algorithm with batch splitting, allowing large models to be trained across multiple accelerators with consistent gradient updates.\",\n  \"Direct Inspiration\": [\"b12\", \"b13\", \"b15\"],\n  \"Indirect Inspiration\": [\"b11\", \"b14\"],\n  \"Other Inspiration\": [\"b16\", \"b17\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the practical challenges of scaling neural networks, particularly focusing on hardware constraints like memory limitations and communication bandwidths on accelerators. It introduces GPipe, a flexible library that enables efficient training of large neural networks by employing a novel pipeline parallelism algorithm with batch splitting and re-materialization to overcome these challenges.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b44\": 0.85,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.75,\n    \"b14\": 0.7,\n    \"b34\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b38\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is scaling large neural networks efficiently despite hardware constraints. The authors propose GPipe, a pipeline parallelism algorithm that partitions models across multiple accelerators and applies synchronous mini-batch gradient descent. This approach allows scaling models beyond single accelerator memory limits while maintaining training efficiency.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b13\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b11\": 0.8,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.5,\n    \"b24\": 0.5,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of efficiently scaling large neural networks, particularly for image classification and natural language processing, by introducing GPipe. GPipe is a flexible library that allows partitioning models across multiple accelerators to bypass hardware constraints. It employs a novel pipeline parallelism algorithm with batch splitting to train large models effectively. The paper demonstrates GPipe's performance on AmoebaNet and Transformer models, showing significant improvements in scalability and efficiency.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b12\", \"b13\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b14\", \"b33\", \"b34\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b38\", \"b39\", \"b40\"]\n  }\n}\n```"], "5fa909a591e011e83f7406b0": ["```json\n{\n  \"Summary\": \"The paper introduces Sandslash, an in-memory GPM system designed to balance productivity, expressiveness, and efficiency in graph pattern mining. Sandslash offers a novel two-level API that automates high-level optimizations while allowing low-level customizations. It outperforms existing systems significantly in terms of speed and ease of programming.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1,\n    \"b28\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b55\": 0.8,\n    \"b41\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses challenges in Graph Pattern Mining (GPM), particularly in developing efficient parallel solutions. It introduces Sandslash, an in-memory GPM system offering high productivity and efficiency through a novel programming interface that separates problem specifications from algorithmic optimizations. Sandslash combines the strengths of existing high-level and low-level GPM systems, automating various optimizations and enabling user-defined customizations.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b28\": 1.0,\n    \"b38\": 1.0,\n    \"b55\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b41\": 0.8,\n    \"b57\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges outlined in the paper are efficient and parallel solutions for Graph Pattern Mining (GPM) problems. Sandslash, the proposed system, aims to provide high productivity and efficiency without compromising generality by using a novel programming interface that separates problem specifications from algorithmic optimizations.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1.0,\n    \"b28\": 1.0,\n    \"b11\": 1.0,\n    \"b55\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b57\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents Sandslash, an in-memory GPM system designed to improve productivity and efficiency in graph pattern mining without compromising generality. Sandslash introduces a novel programming interface that separates problem specifications from algorithmic optimizations, offering both high-level and low-level APIs to leverage application-specific knowledge. The system automates many optimizations previously only available in handwritten solutions, resulting in substantial performance improvements over existing GPM systems.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b38\": 0.9,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b55\": 0.7,\n    \"b53\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the complexity of programming efficient parallel solutions for graph pattern mining (GPM) problems. The novel algorithm proposed by the authors is Sandslash, an in-memory GPM system that separates problem specifications from algorithmic optimizations. Sandslash provides a high-level interface for specifying patterns and enables efficient search strategies, data representations, and optimizations automatically. Additionally, it offers a low-level interface for users to leverage application-specific knowledge for further performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b28\": 1,\n    \"b38\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.9,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b2\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```"], "5eff040a91e011ea6db8de11": ["```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper include the efficient training of large DNN models on hardware accelerators with limited memory and diverse interconnect capabilities. The proposed algorithm, DAPPLE, combines pipeline parallelism and data parallelism to ensure both training convergence and memory efficiency. Key design challenges include determining an optimal parallelization strategy and scheduling pipeline stage computations to balance parallelism, memory consumption, and execution efficiency.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b11\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b7\": 0.7,\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.5,\n    \"b14\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of training large deep neural network (DNN) models on hardware accelerators with diversified interconnect capabilities. It proposes a novel distributed training scheme called DAPPLE that combines pipeline parallelism and data parallelism to ensure both training convergence and memory efficiency. The key contributions include a synchronous pipeline planner for optimal parallelization strategies and a novel pipeline stage scheduling algorithm to balance parallelism, memory consumption, and execution efficiency.\",\n  \"Direct Inspiration\": [\"b9\", \"b11\", \"b13\"],\n  \"Indirect Inspiration\": [\"b4\", \"b5\", \"b7\", \"b12\", \"b15\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training large DNN models on hardware accelerators with limited memory. It proposes a distributed training scheme, DAPPLE, which combines data and pipeline parallelism to ensure both training convergence and memory efficiency. The key design challenges include determining an optimal parallelization strategy and scheduling pipeline stage computations to balance parallelism, memory consumption, and execution efficiency.\",\n  \"Direct Inspiration\": [\"b9\", \"b11\", \"b13\"],\n  \"Indirect Inspiration\": [\"b4\", \"b5\", \"b7\", \"b12\", \"b15\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in training large Deep Neural Network (DNN) models on hardware accelerators by combining data parallelism (DP) and pipeline parallelism to ensure both training convergence and memory efficiency. The proposed method, DAPPLE, introduces a synchronous pipeline planner and a novel pipeline stage scheduling algorithm to optimize parallelization strategies and minimize memory consumption.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b13\": 0.9,\n    \"b12\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b14\": 0.6,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b7\": 0.5,\n    \"b16\": 0.4,\n    \"b17\": 0.4,\n    \"b18\": 0.4,\n    \"b19\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of training large DNN models on hardware accelerators and proposes a distributed training scheme called DAPPLE, which combines pipeline parallelism and data parallelism to ensure both training convergence and memory efficiency. The main innovations include a synchronous pipeline planner for optimal parallelization strategies and a novel pipeline stage scheduling algorithm to balance parallelism, memory consumption, and execution efficiency.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b7\": 0.6,\n    \"b11\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b12\": 0.7,\n    \"b14\": 0.5,\n    \"b16\": 0.4,\n    \"b17\": 0.4,\n    \"b18\": 0.4,\n    \"b19\": 0.4,\n    \"b20\": 0.5\n  }\n}\n```"], "5c2c7a9217c44a4e7cf317b4": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in this paper include handling irregular applications on GPUs, specifically for graph analytics, which do not conform to the traditional SIMD model. The paper introduces SIMD-X, a new parallel framework for programming and processing single instruction multiple, complex, data on GPUs. SIMD-X aims to provide ease of programming through the Active-Compute-Combine (ACC) model while enabling system-level optimizations at runtime to handle task-parallel complexity. The framework introduces just-in-time (JIT) task management and push-pull based kernel fusion to balance parallel workloads and reduce kernel invocation overhead.\",\n  \"Direct Inspiration\": [\n    \"b40\",\n    \"b49\",\n    \"b74\",\n    \"b75\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b48\",\n    \"b32\",\n    \"b77\"\n  ],\n  \"Other Inspiration\": [\n    \"b3\",\n    \"b45\",\n    \"b65\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": \"This paper introduces SIMD-X, a new parallel framework for graph computation on GPUs that addresses the challenges of irregular applications by decoupling programming and processing. The framework consists of an Active-Compute-Combine (ACC) programming model, just-in-time (JIT) task management, and push-pull based kernel fusion, each designed to overcome issues such as workload imbalance and kernel invocation overhead.\",\n    \"Direct Inspiration\": {\n        \"b40\": 0.9,\n        \"b75\": 0.9,\n        \"b74\": 0.9,\n        \"b77\": 0.8,\n        \"b32\": 0.8,\n        \"b49\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b48\": 0.6,\n        \"b65\": 0.6\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of graph computing on GPUs, focusing on irregular applications that do not conform to the traditional SIMD model. The proposed framework, SIMD-X, decouples programming and processing, utilizing a data-parallel model for ease of programming while enabling system-level optimizations at runtime. The main contributions include the Active-Compute-Combine (ACC) programming model, just-in-time (JIT) task management, and a push-pull based kernel fusion technique.\",\n    \"Direct Inspiration\": {\n        \"b40\": 1.0,\n        \"b75\": 1.0,\n        \"b77\": 1.0,\n        \"b74\": 1.0,\n        \"b48\": 0.9,\n        \"b32\": 0.9,\n        \"b49\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b68\": 0.7,\n        \"b65\": 0.7,\n        \"b71\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b17\": 0.6,\n        \"b45\": 0.6,\n        \"b3\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges addressed in the paper involve enabling efficient graph computations on GPUs, particularly dealing with irregular applications that do not conform to the traditional SIMD model. The proposed solution, SIMD-X, aims to decouple programming and processing, provide a new Active-Compute-Combine (ACC) programming model, implement JIT task management, and introduce push-pull based kernel fusion to optimize performance and resource utilization on GPUs.\",\n  \"Direct Inspiration\": {\n    \"b48\": 1.0,\n    \"b75\": 1.0,\n    \"b40\": 1.0,\n    \"b74\": 1.0,\n    \"b77\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b49\": 0.8,\n    \"b70\": 0.8,\n    \"b71\": 0.8,\n    \"b68\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.6,\n    \"b65\": 0.6,\n    \"b85\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of irregular graph computations on GPUs, which do not fit well with the traditional SIMD model. It proposes a new framework, SIMD-X, which decouples the programming and processing to handle these irregularities. The framework introduces the Active-Compute-Combine (ACC) model, just-in-time (JIT) task management, and push-pull based kernel fusion to improve performance and ease of programming.\",\n    \"Direct Inspiration\": {\n        \"b49\": 0.9,\n        \"b40\": 0.8,\n        \"b75\": 0.85,\n        \"b74\": 0.88\n    },\n    \"Indirect Inspiration\": {\n        \"b32\": 0.75,\n        \"b48\": 0.7,\n        \"b77\": 0.65,\n        \"b71\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.5,\n        \"b65\": 0.4\n    }\n}\n```"], "5ea16b2b91e011fa08b8f8d9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of training massive deep neural networks efficiently by leveraging pipeline parallelism in PyTorch. It introduces torchgpipe, a library that implements the GPipe methodology with a focus on optimizing pipeline-parallel computations and integrating gradient checkpointing to reduce memory consumption.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b8\": 0.9,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b9\": 0.7,\n    \"b20\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently training large-scale deep neural networks using pipeline parallelism. It introduces torchgpipe, a library for implementing GPipe in PyTorch, which combines pipeline parallelism and gradient checkpointing to optimize memory and computational efficiency.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b8\": 0.9,\n    \"b11\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b15\": 0.6,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficiently training very large deep neural networks by developing a PyTorch library, torchgpipe, for micro-batch pipeline parallelism with gradient checkpointing. The novel contributions include optimized pipeline-parallel computations for PyTorch's define-by-run and eager execution environment, and the design of specific features to handle various complexities in PyTorch.\",\n    \"Direct Inspiration\": {\n        \"b10\": 1.0,\n        \"b8\": 0.9,\n        \"b11\": 0.9,\n        \"b6\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.8,\n        \"b13\": 0.8,\n        \"b25\": 0.8,\n        \"b1\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.7,\n        \"b23\": 0.7,\n        \"b20\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently training large-scale deep neural networks by proposing a PyTorch library named 'torchgpipe' for implementing GPipe, a pipeline parallelism technique with gradient checkpointing. The primary objective is to enhance the efficiency of pipeline-parallel computations in PyTorch's define-by-run environment. Key contributions include optimized task scheduling, management of forward and backward dependencies, and minimizing data transfer delays between devices.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b10\", \"b8\", \"b11\", \"b6\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b15\", \"b13\", \"b25\", \"b14\", \"b18\", \"b17\", \"b28\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b22\", \"b20\", \"b12\", \"b3\", \"b5\", \"b24\", \"b16\", \"b4\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Training massive deep neural networks efficiently while managing resource constraints and computational complexity.\",\n    \"inspirations\": \"Existing methods of model parallelism, data parallelism, and gradient checkpointing, with a focus on scalable and efficient training via pipeline parallelism.\"\n  },\n  \"Direct Inspiration\": [\n    \"b10\",\n    \"b8\",\n    \"b6\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b4\",\n    \"b3\",\n    \"b25\",\n    \"b11\"\n  ],\n  \"Other Inspiration\": [\n    \"b1\"\n  ]\n}\n```"], "5fb24ee191e01186d3f5decc": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of incorporating user preferences, particularly soft lexical constraints, into neural machine translation (NMT) without increasing computational costs. The proposed model, EDITOR, improves upon the Levenshtein Transformer by introducing a reposition operation, which enhances the handling of lexical constraints and speeds up decoding. The EDITOR model outperforms existing methods in translation quality and decoding speed, particularly when using soft constraints.\",\n    \"Direct Inspiration\": {\n        \"b22\": 1,\n        \"b36\": 0.9,\n        \"b47\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b20\": 0.7,\n        \"b26\": 0.7,\n        \"b49\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.5,\n        \"b43\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating soft lexical constraints in neural machine translation (NMT) without significantly increasing computational costs or reducing translation fluency. The proposed model, EDITOR, builds on non-autoregressive sequence generation and introduces a novel reposition operation to better handle lexical constraints and reordering.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b43\": 0.6,\n    \"b20\": 0.5,\n    \"b26\": 0.5,\n    \"b47\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces EDITOR, a novel translation model that incorporates users' lexical constraints seamlessly into neural machine translation (NMT). The primary challenges addressed include improving the flexibility and efficiency of incorporating lexical constraints in NMT without increasing computational cost and maintaining fluency in the output. The EDITOR model builds on the Levenshtein Transformer and introduces a reposition operation to separate lexical choice from reordering decisions, enhancing the efficiency of the translation process.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b36\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.7,\n    \"b16\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces EDITOR, a new model for neural machine translation (MT) that incorporates user-specified lexical constraints without increasing decoding time and computational cost. The EDITOR model builds on the Levenshtein Transformer, replacing the deletion operation with a reposition operation to enhance efficiency and flexibility. Key challenges addressed include improving translation quality and speed, effectively exploiting lexical constraints, and separating lexical choice from reordering decisions.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b36\": 0.85,\n    \"b47\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.7,\n    \"b16\": 0.65,\n    \"b43\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.5,\n    \"b20\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents a new translation model called EDITOR, which aims to seamlessly integrate users' lexical preferences into neural machine translation (NMT) without the computational overhead of traditional constrained decoding methods. The model is built on advancements in non-autoregressive sequence generation and leverages a novel reposition operation to enhance flexibility and efficiency in incorporating soft lexical constraints. The primary challenge addressed is the effective and efficient inclusion of user preferences in NMT outputs while maintaining fluency and reducing decoding time.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b26\": 0.7,\n    \"b43\": 0.6,\n    \"b16\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b49\": 0.5\n  }\n}\n```"], "5aed14d617c44a44381595bd": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in fairness within machine learning, particularly how decisions reshape populations over time. It introduces a one-step feedback model to quantify the long-term impact of classification on different groups and evaluates various fairness criteria, such as demographic parity and equality of opportunity, in lending scenarios. The paper also examines the role of measurement errors and alternatives to hard fairness constraints.\",\n  \n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b13\": 1\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.8,\n    \"b15\": 0.8\n  },\n  \n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b6\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper revolve around assessing the long-term well-being of disadvantaged groups under various fairness criteria in automated decision-making. The authors introduce a one-step feedback model to quantify the impact of decision-making policies on different population groups over time and use a hypothetical lending scenario to illustrate their findings.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b13\": 0.9,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b1\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b10\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of ensuring fairness in automated decision-making, particularly in lending practices, to protect disadvantaged groups. It introduces a one-step feedback model that quantifies the long-term impact of classification policies on different population groups. The proposed model evaluates fairness criteria such as demographic parity and equality of opportunity, analyzing their outcomes and potential pitfalls. The study includes theoretical characterizations, experiments with real-world data, and discussions on measurement error and alternative fairness approaches.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b13\": 0.9,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b6\": 0.7,\n    \"b1\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of fairness in automated decision-making, particularly in the context of lending practices. It introduces a one-step feedback model to evaluate how different fairness criteria impact the long-term well-being of disadvantaged groups. The paper explores demographic parity and equality of opportunity as fairness criteria and examines their effects on the population over time.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b18\": 0.7,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of evaluating the long-term impact of fairness criteria in automated decision-making, particularly in the context of lending practices. It proposes a one-step feedback model to expose how decisions reshape the underlying population over time and examines different fairness criteria like demographic parity and equality of opportunity. The paper aims to determine when these fairness criteria benefit disadvantaged groups and introduces the concept of outcome curves to describe various regimes where one criterion may be preferable over another.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b6\": 0.6,\n    \"b10\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```"], "5a4aef9e17c44a2190f7a4bd": ["```json\n{\n    \"Summary\": \"The primary challenge in the paper is the accurate localization of keypoints in multi-person pose estimation, especially in cases with occluded, invisible, or crowded backgrounds. The proposed solution is the Cascaded Pyramid Network (CPN), which includes two stages: GlobalNet for feature representation and RefineNet for addressing hard keypoints through an online hard keypoints mining loss. The paper demonstrates the effectiveness of CPN in achieving state-of-the-art results on the COCO multi-person keypoint benchmark.\",\n    \"Direct Inspiration\": {\n        \"b4\": 0.9,\n        \"b14\": 0.9,\n        \"b23\": 0.8,\n        \"b26\": 0.8,\n        \"b27\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.7,\n        \"b22\": 0.7,\n        \"b42\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.6,\n        \"b2\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in multi-person pose estimation, particularly occluded and invisible keypoints, and proposes a novel network structure called Cascaded Pyramid Network (CPN) with two stages: GlobalNet and RefineNet. The GlobalNet learns feature representation using a feature pyramid network while RefineNet addresses hard keypoints using an online hard keypoints mining loss.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b27\": 0.85,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.75,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in multi-person pose estimation, particularly focusing on occluded, invisible, and crowded keypoints. The authors propose a novel Cascaded Pyramid Network (CPN) consisting of GlobalNet and RefineNet to improve the localization of these 'hard' keypoints.\",\n    \"Direct Inspiration\": {\n        \"b26\": 0.9,\n        \"b27\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b23\": 0.8,\n        \"b14\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.7,\n        \"b29\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses multi-person pose estimation challenges, particularly occluded and invisible keypoints, by introducing a Cascaded Pyramid Network (CPN) with two stages: GlobalNet and RefineNet. GlobalNet provides a pyramid feature representation to infer occluded and invisible joints, while RefineNet explicitly addresses 'hard' joints using an online hard keypoints mining loss. The method is based on a top-down pipeline, incorporating insights from previous research to achieve state-of-the-art results.\",\n  \"Direct Inspiration\": [\"b26\", \"b27\"],\n  \"Indirect Inspiration\": [\"b14\", \"b23\"],\n  \"Other Inspiration\": [\"b4\", \"b29\", \"b18\", \"b25\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multi-person pose estimation, especially the localization of occluded and invisible keypoints. It proposes a novel network structure called Cascaded Pyramid Network (CPN) which includes GlobalNet and RefineNet. The GlobalNet is based on a feature pyramid network and RefineNet focuses on hard keypoints using an online hard keypoints mining loss. The top-down pipeline is used, involving a human detector followed by keypoint localization.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b26\": 0.9,\n    \"b27\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b29\": 0.6,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b22\": 0.4,\n    \"b39\": 0.4\n  }\n}\n```"], "5e5e191993d709897ce5087d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of building scalable dialogue systems for virtual assistants that can support a large and growing number of services across multiple domains. The authors introduce the Schema-Guided Dialogue (SGD) dataset, the largest public task-oriented dialogue corpus, and propose a novel schema-guided approach for task-oriented dialogue. They present a zero-shot dialogue state tracking model that can generalize to unseen services by leveraging large pretrained models like BERT.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b5\": 0.9,\n    \"b3\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b15\": 0.75,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.65,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scaling virtual assistants to support a large number of services across multiple domains. They present the Schema-Guided Dialogue (SGD) dataset, which is the largest public task-oriented dialogue corpus designed to capture these challenges. The proposed schema-guided paradigm and novel architecture for multi-domain dialogue state tracking using large pretrained models like BERT aim to handle unseen services and dynamic API changes effectively.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b1\": 0.9,\n    \"b15\": 0.9,\n    \"b4\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the need for virtual assistants to support a large and constantly increasing number of services across multiple domains, which existing datasets fail to adequately capture. The paper introduces the Schema-Guided Dialogue (SGD) dataset and proposes a schema-guided paradigm for task-oriented dialogue. This approach involves using a service's schema as input to a unified dialogue model, enabling the model to generalize to unseen services and be robust to API changes. The paper presents a novel architecture for multi-domain dialogue state tracking based on pretrained models like BERT.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b3\": 1,\n    \"b5\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b13\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b12\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in developing scalable dialogue systems for virtual assistants that can handle tasks across multiple application domains. The proposed solution is a schema-guided dialogue dataset (SGD) and a novel architecture for multi-domain dialogue state tracking using large pretrained models like BERT. The approach allows for effective generalization to unseen services and APIs, enabling robust and scalable virtual assistants.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b1\": 0.9,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b13\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b20\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scaling virtual assistants to support a large number of services across multiple domains. It introduces the Schema-Guided Dialogue (SGD) dataset, which is the largest public task-oriented dialogue corpus, and proposes a novel architecture for multi-domain dialogue state tracking using a schema-guided paradigm and large pretrained models like BERT.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b3\": 0.9,\n    \"b4\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b18\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b13\": 0.6,\n    \"b16\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```"], "5c5ce50d17c44a400fc38e42": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of recommending courses to students based on their individual academic history and goals. The proposed solution utilizes Recurrent Neural Networks (RNNs) to predict course grades and recommend preparatory courses, focusing on overcoming limitations of existing prerequisite course information. The approach is validated through grade prediction, prerequisite prediction, and course selection prediction tasks. The use of LSTM variants and a custom masked loss function are notable contributions.\",\n    \"Direct Inspiration\": {\n        \"b24\": 1,\n        \"b5\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b22\": 0.8,\n        \"b3\": 0.7,\n        \"b10\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b8\": 0.6,\n        \"b23\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of recommending appropriate preparatory courses for university students based on their existing knowledge and curricular goals. It proposes a model using Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks to predict course grades, infer prerequisite relationships, and recommend courses that prepare students for their desired target courses.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b24\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b1\", \"b8\", \"b22\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b5\", \"b17\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"Challenges\": \"Identifying appropriate preparatory courses for students given their existing knowledge and course history, and overcoming limitations of current prerequisite listings.\",\n        \"Inspirations\": \"Enhanced course recommendation systems using machine learning, specifically RNNs, to tailor suggestions based on individual student's course history.\"\n    },\n    \"Direct Inspiration\": {\n        \"b14\": 0.9,\n        \"b15\": 0.9,\n        \"b22\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.7,\n        \"b5\": 0.7,\n        \"b24\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b20\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a novel approach to goal-based course recommendation using Recurrent Neural Networks (RNNs) to address the shortcomings of current prerequisite course information. The primary challenges addressed include outdated prerequisites, lack of comprehensiveness, individualized student knowledge, and oversubscribed courses. The approach involves predicting grades and recommending preparatory courses based on a student's course history, utilizing LSTM variants to enhance prediction accuracy.\",\n  \"Direct Inspiration\": [\"b14\", \"b15\", \"b22\", \"b24\"],\n  \"Indirect Inspiration\": [\"b5\", \"b10\", \"b20\"],\n  \"Other Inspiration\": [\"b3\", \"b8\", \"b23\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of course recommendation in higher education, specifically focusing on goal-based recommendations for preparation courses. It proposes a novel application of Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) networks, to predict student grades and infer prerequisite relationships. The approach aims to provide personalized course recommendations based on students' academic history and target goals, addressing shortcomings in existing prerequisite systems.\",\n    \"Direct Inspiration\": {\n        \"b24\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b22\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.7,\n        \"b9\": 0.65,\n        \"b10\": 0.6,\n        \"b23\": 0.6\n    }\n}\n```"], "5f8ebbb99fced0a24b4e1966": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to improve search result diversification by considering global interactions among candidate documents rather than relying on greedy sequential selection. The authors propose a novel Diversity Encoder with Self-Attention (DESA) framework that uses a self-attention based encoder-decoder structure to model interactions between candidate documents and subtopics, aiming to achieve global optimal rankings.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of search result diversification, specifically focusing on overcoming the limitations of greedy sequential selection methods. The proposed algorithm, Diversity Encoder with Self-Attention (DESA), utilizes a self-attention based encoder-decoder structure to model interactions between candidate documents and subtopics simultaneously. This approach aims to achieve a global optimal ranking by considering both novelty and subtopic coverage for each candidate document.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b13\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b19\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed by the paper are the limitations of existing search result diversification models, specifically the inability to achieve global optimal rankings due to the traditional use of greedy document sequential selection. The novel approach proposed by the authors, DESA (Diversity Encoder with Self-Attention), aims to overcome these challenges by modeling the interactions between candidate documents and subtopics using a self-attention based encoder-decoder structure. This allows for a global consideration of document novelty and subtopic coverage, leading to potentially better diversified search results.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the limitation of existing search result diversification methods that rely on greedy document selection processes, which may not lead to globally optimal rankings. The proposed algorithm, DESA, utilizes a self-attention based encoder-decoder structure to model the interactions between candidate documents and subtopics, aiming for a global optimal ranking that considers both novelty and subtopic coverage.\",\n  \"Direct Inspiration\": [\"b13\", \"b14\"],\n  \"Indirect Inspiration\": [\"b4\", \"b6\", \"b7\", \"b8\", \"b19\"],\n  \"Other Inspiration\": [\"b21\", \"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of search result diversification by proposing a new framework called Diversity Encoder with Self-Attention (DESA). The main challenges include handling ambiguous or vague queries and achieving global optimal document rankings by modeling the interactions between candidate documents and subtopics using a self-attention based encoder-decoder structure.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7\n  }\n}\n```"], "5b67b46f17c44aac1c86329e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of suboptimal search result diversification due to the limitations of greedy sequential document selection. The authors propose an enhanced model, M2Div, that integrates Monte Carlo tree search (MCTS) with Markov decision processes (MDP) to improve document ranking. The core contribution is the use of reinforcement learning and MCTS to explore potential document rankings, thereby increasing the likelihood of achieving a globally optimal ranking.\",\n  \"Direct Inspiration\": [\"b26\", \"b27\"],\n  \"Indirect Inspiration\": [\"b32\"],\n  \"Other Inspiration\": [\"b2\", \"b24\", \"b38\", \"b30\", \"b33\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper focuses on addressing the suboptimal performance of greedy sequential decision-making in search result diversification. It proposes a novel algorithm called M2Div, which enhances the Markov Decision Process (MDP) with Monte Carlo Tree Search (MCTS) to achieve more globally optimal document rankings. The model leverages reinforcement learning to train the policy and value functions, aiming to improve the ranking quality based on \u03b1-NDCG@M.\",\n    \"Direct Inspiration\": {\n        \"b26\": 1,\n        \"b27\": 1,\n        \"b32\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.9,\n        \"b24\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.8,\n        \"b30\": 0.8,\n        \"b31\": 0.8,\n        \"b33\": 0.8,\n        \"b35\": 0.8,\n        \"b38\": 0.8\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges addressed in the paper include overcoming the suboptimality of greedy sequential document selection in search result diversification and enhancing the ranking model to achieve a more globally optimal diverse ranking. The authors propose an enhanced MDP model for diverse ranking, named M2Div, which utilizes Monte Carlo tree search (MCTS) inspired by the success of AlphaGo and AlphaGo Zero.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b27\": 1,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b24\": 0.8,\n    \"b38\": 0.7,\n    \"b30\": 0.7,\n    \"b31\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of suboptimal diverse ranking in information retrieval. It proposes a novel algorithm called M2Div, which integrates Markov Decision Process (MDP) and Monte Carlo Tree Search (MCTS) to enhance the ranking process. The model uses a recurrent neural network (RNN) to produce policy and value functions, enabling more globally optimal document selection by exploring future ranking possibilities.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b27\": 0.9,\n    \"b32\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b24\": 0.75,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.65,\n    \"b31\": 0.65,\n    \"b33\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of suboptimal rankings in search result diversification due to greedy sequential document selection. It proposes a new model, M2Div, which enhances the MDP model for diverse ranking with Monte Carlo tree search (MCTS) to improve the selection of documents by exploring future possible rankings. The model uses a recurrent neural network (RNN) to guide document selection and reinforcement learning to train the model parameters.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b27\": 0.9,\n    \"b32\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b24\": 0.75,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.65,\n    \"b30\": 0.65,\n    \"b31\": 0.65,\n    \"b33\": 0.65\n  }\n}\n```"], "5f0277e911dc830562231dac": ["```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is how to effectively sample high-quality training data for supervised diversification models in search result diversification. The proposed method, DVGAN, tackles this problem by introducing a Generative Adversarial Network (GAN) framework that combines explicit and implicit approaches to improve diversification quality. The generator generates negative training samples, and the discriminator provides rewards for better sampling, leveraging both explicit document-subtopic relevance and implicit document-document similarity features.\",\n  \"Direct Inspiration\": [\"b18\"],\n  \"Indirect Inspiration\": [\"b9\", \"b22\", \"b25\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of diversifying search results for ambiguous queries by introducing a novel framework called DVGAN, which employs a Generative Adversarial Network (GAN) to improve search result diversification. The main contributions include the combination of explicit and implicit diversification approaches, the introduction of GAN to generate high-quality negative samples, and the proposal of new sampling algorithms to enhance training data quality and quantity.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of search result diversification to handle ambiguous or broad queries in search engines. The primary challenge is how to sample high-quality training data to improve the performance of supervised diversification models. The proposed solution is DVGAN, a framework that combines Generative Adversarial Networks (GAN) with explicit and implicit diversification approaches to improve the quality and quantity of training samples.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b25\": 0.7,\n    \"b23\": 0.6,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.5,\n    \"b5\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include handling query ambiguity and effectively generating high-quality training data for supervised diversification models. The proposed algorithm, DVGAN, uses a Generative Adversarial Network (GAN) to improve search result diversification by combining explicit and implicit features. The generator generates negative training samples, while the discriminator provides rewards for better sampling, leveraging explicit document-subtopic relevance features and implicit document-document similarity features.\",\n  \"Direct Inspiration\": [\"b18\", \"b9\"],\n  \"Indirect Inspiration\": [\"b22\", \"b25\"],\n  \"Other Inspiration\": [\"b3\", \"b11\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of sampling high-quality training data for supervised search result diversification models, which often face issues with either the quality or quantity of training data. The novel approach proposed is the use of a Generative Adversarial Network (GAN) to improve the performance of search result diversification by combining explicit and implicit diversification features. This approach introduces a generator to produce negative training samples and a discriminator to provide rewards for better sampling, thereby enhancing the overall sampling performance.\",\n  \"Direct Inspiration\": [\"b18\", \"b9\"],\n  \"Indirect Inspiration\": [\"b22\", \"b25\", \"b11\"],\n  \"Other Inspiration\": [\"b23\"]\n}\n```"], "5f0277e911dc830562231dea": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the limitations of existing policy gradient algorithms in reinforcement learning to rank (RLTR) for information retrieval (IR), specifically their failure to account for the relative ordering nature of IR ranking and high variance in gradient estimation. The paper introduces a novel Pairwise Policy Gradient (PPG) algorithm inspired by pairwise learning to rank, which performs intra-query pairwise comparisons to estimate gradients with low variance and improve ranking performance.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1.0,\n    \"b39\": 1.0,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.7,\n    \"b38\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of applying policy gradient algorithms to document ranking in Information Retrieval (IR). Specifically, it highlights the limitations in capturing the relative ordering nature of IR ranking and high variance in gradient estimation. To overcome these issues, the paper proposes a novel Pairwise Policy Gradient (PPG) algorithm that uses intra-query pairwise comparisons to estimate gradients with low variance.\",\n  \"Direct Inspiration\": [\"b30\", \"b34\", \"b39\"],\n  \"Indirect Inspiration\": [\"b6\", \"b14\"],\n  \"Other Inspiration\": [\"b0\", \"b2\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the limitations of existing policy gradient algorithms for document ranking in information retrieval (IR). Specifically, these algorithms fail to consider the relative ordering nature of IR ranking and suffer from high variance gradient estimations. The paper proposes a new policy gradient algorithm, Pairwise Policy Gradient (PPG), inspired by pairwise learning to rank, which uses intra-query pairwise comparisons to address these issues.\",\n  \"Direct Inspiration\": [\"b0\", \"b2\", \"b15\"],\n  \"Indirect Inspiration\": [\"b34\", \"b39\"],\n  \"Other Inspiration\": [\"b30\", \"b10\", \"b6\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of using policy gradient algorithms in reinforcement learning to rank (RL2R) for information retrieval (IR) tasks. The main problems identified are the failure to consider the intra-query relative ordering nature of IR ranking and the high variance in gradient estimation. The proposed solution, Pairwise Policy Gradient (PPG), incorporates pairwise comparisons within the same query to address these issues, aiming for lower variance and improved convergence.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1.0,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.8,\n    \"b10\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing reinforcement learning to rank (RL2R) methods in information retrieval (IR), specifically focusing on the policy gradient algorithm's inability to consider the relative ordering nature of IR ranking and its high variance gradient estimation. The authors propose a novel policy gradient algorithm called Pairwise Policy Gradient (PPG) that leverages intra-query pairwise comparisons to address these challenges. PPG aims to reduce the estimation variance and incorporate the relative ordering nature of IR ranking, showing theoretical and empirical advantages over existing methods.\",\n  \"Direct Inspiration\": [\"b30\", \"b34\", \"b39\"],\n  \"Indirect Inspiration\": [\"b6\", \"b14\", \"b26\", \"b38\"],\n  \"Other Inspiration\": [\"b10\", \"b31\"]\n}\n```"], "5c8c8ddc4895d9cbc60e60c3": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"Current recommendation methods in session-based recommendations fail to account for both the user's sequential behavior and their main purpose within a session, leading to inaccurate recommendations.\",\n    \"novel_methods\": \"The paper proposes the Neural Attentive Recommendation Machine (NARM), which uses a hybrid encoder with an attention mechanism to model the user's sequential behavior and capture the user's main purpose in a session. It combines these two aspects into a unified session representation and uses a bi-linear matching scheme to compute recommendation scores.\"\n  },\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b39\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b23\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b30\": 0.6,\n    \"b40\": 0.55,\n    \"b44\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"Current recommendation methods struggle with session-based recommendations due to their reliance on the user's sequential behavior without considering the user's main purpose. This can lead to inaccurate recommendations when users click on items by accident or out of curiosity.\",\n        \"inspirations\": \"The authors propose a Neural Attentive Recommendation Machine (NARM) that combines the user's sequential behavior and main purpose in the current session using a novel hybrid encoder with an attention mechanism.\"\n    },\n    \"Direct Inspiration\": {\n        \"b11\": 1.0,\n        \"b39\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b32\": 0.8,\n        \"b44\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b30\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"Current recommendation systems struggle when recommendations are based solely on user sessions, as they fail to emphasize the user's main purpose in the current session.\",\n        \"algorithm\": \"Neural Attentive Recommendation Machine (NARM) which uses a hybrid encoder with an attention mechanism to model both the user's sequential behavior and main purpose in the current session.\"\n    },\n    \"Direct Inspiration\": [\n        \"b11\",\n        \"b39\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b32\",\n        \"b44\"\n    ],\n    \"Other Inspiration\": [\n        \"b25\",\n        \"b34\"\n    ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in session-based recommendation systems by proposing a novel model, Neural Attentive Recommendation Machine (NARM), which considers both the user's sequential behavior and main purpose within a session. The model incorporates a hybrid encoder with an attention mechanism to improve recommendation accuracy.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b39\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of session-based recommendation systems, which struggle to accurately predict a user's next click based on their entire session due to potential irrelevant clicks. The authors propose a novel Neural Attentive Recommendation Machine (NARM) that employs a hybrid encoder with an attention mechanism to model both the user's sequential behavior and main purpose within the session, thereby improving recommendation accuracy.\",\n    \"Direct Inspiration\": {\n        \"b11\": 1.0,\n        \"b39\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b32\": 0.7,\n        \"b44\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.6,\n        \"b35\": 0.6,\n        \"b45\": 0.6,\n        \"b25\": 0.6,\n        \"b34\": 0.6\n    }\n}\n```"], "5b67b45517c44aac1c860885": ["```json\n{\n    \"Summary\": \"The main challenge addressed in this paper is improving the predictive accuracy of session-based recommender systems (SRS) by effectively capturing both long-term and short-term user interests. The proposed solution is a Short-Term Attention/Memory Priority (STAMP) model that incorporates a novel attention mechanism to enhance the prediction of users' next actions in a session.\",\n    \"Direct Inspiration\": {\n        \"b4\": 0.9,\n        \"b5\": 0.8,\n        \"b16\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.6,\n        \"b10\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.4,\n        \"b20\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately predicting the next action in session-based recommender systems (SRS) by introducing a Short-Term Attention/Memory Priority (STAMP) model. The key innovations include accounting for both users' long-term and short-term interests through a novel attention mechanism, which enhances prediction accuracy by prioritizing recent actions.\",\n  \"Direct Inspiration\": [\n    \"b4\",\n    \"b9\",\n    \"b19\",\n    \"b20\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b5\",\n    \"b16\",\n    \"b6\"\n  ],\n  \"Other Inspiration\": [\n    \"b1\",\n    \"b2\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately predicting users' next actions in session-based recommender systems (SRS) by considering both long-term and short-term user interests. The authors propose the Short-Term Attention/Memory Priority (STAMP) model, which introduces a novel neural attention mechanism that accounts for users' general and current interests simultaneously. The STAMP model aims to improve predictive accuracy by dynamically weighting historical clicks and the last-click in a session to capture users' temporal interests and interest drift.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b5\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b9\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting user actions in session-based recommender systems (SRS) considering both long-term and short-term user interests. It introduces the STAMP model, which incorporates a short-term attention/memory priority mechanism to improve predictive accuracy by capturing users' temporal interests more effectively.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.85,\n    \"b6\": 0.8,\n    \"b16\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b19\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b20\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving session-based recommender systems (SRS) by capturing both short-term and long-term user interests simultaneously. It proposes a Short-Term Attention/Memory Priority (STAMP) model that leverages an attention mechanism to enhance the prediction of user actions. The model takes into account the user's last-click (representing short-term interests) and the average of all previous clicks (representing long-term interests). The proposed solution aims to overcome the limitations of traditional RNN models which fail to distinguish between these two types of interests effectively.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b5\": 0.9,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b17\": 0.7,\n    \"b10\": 0.6\n  }\n}\n```"], "5dbebb7447c8f766462c2328": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of representing a user's preference in session-based recommender systems (SBRS) by capturing the inherent order of item transition patterns within a session. It proposes a novel Full Graph Neural Network (FGNN) model, which uses a session graph to represent the sequence of user-item interactions and a weighted graph attention layer (WGAT) network to compute information flow between items. The paper aims to improve next-item recommendation by leveraging graph neural networks to better capture complex item transition patterns.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b15\": 1.0,\n    \"b18\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of capturing the inherent order of item transition patterns in session-based recommender systems (SBRS), which traditional methods like content-based RS and collaborative filtering RS fail to do. The authors propose a model named Full Graph Neural Network (FGNN) that utilizes graph neural networks (GNN) to learn the inherent order of item transitions and compute session-level representations for recommendations.\",\n  \"Direct Inspiration\": [\"b15\", \"b18\", \"b37\"],\n  \"Indirect Inspiration\": [\"b8\", \"b17\", \"b34\"],\n  \"Other Inspiration\": [\"b9\", \"b6\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of capturing the inherent order of item transitions in session-based recommender systems (SBRS), which traditional methods fail to adequately represent. The proposed approach, Full Graph Neural Network (FGNN), uses a session graph and a multiple weighted graph attention layer network to learn item representations and generate recommendations.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b18\": 1,\n    \"b37\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b9\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b26\": 0.6,\n    \"b34\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Ignoring user's recent preference in content-based and collaborative filtering RS\",\n      \"Inability to capture user's preference shift within a session\",\n      \"Limitations of modeling sequence data with RNNs and self-attention mechanisms\",\n      \"Simplistic division of user preference into long-term and short-term\"\n    ],\n    \"Proposed Algorithm\": \"Full Graph Neural Network (FGNN) with multiple weighted graph attention layers (WGAT) to capture inherent order of item transitions in session-based recommendation.\"\n  },\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b15\": 0.9,\n    \"b18\": 0.85,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b34\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b26\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing recommender systems that fail to account for shifts in user preferences within sessions. The authors propose a Full Graph Neural Network (FGNN) model to learn the inherent order of item transition patterns using a session graph and multiple weighted graph attention layers.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b18\": 0.8,\n    \"b37\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6\n  }\n}\n```"], "5d04e90ada56295d08ddb6b8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of selection bias and lack of hierarchical knowledge in existing medical code representation and medication recommendation models. It proposes G-BERT, which combines pre-training techniques and graph neural networks (GNN) for improved medical code representation and medication recommendation.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b1\": 0.9,\n    \"Devlin et al., 2018\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"Radford et al., 2018\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"Velickovic et al., 2017\": 0.7,\n    \"Shang et al., 2019\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in computational medication recommendation: selection bias and lack of hierarchical knowledge in medical code representations. It proposes G-BERT, which combines pre-training techniques and graph neural networks for better medical code representation and medication recommendation.\",\n  \"Direct Inspiration\": [\"b2\", \"b4\"],\n  \"Indirect Inspiration\": [\"b1\", \"b3\", \"b6\", \"b7\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in computational medication recommendation using EHR data: selection bias and lack of hierarchical knowledge embedding. The proposed G-BERT model combines pre-training techniques and graph neural networks to improve medical code representation and medication recommendation. Key inspirations include BERT for pre-training and GNNs for hierarchical knowledge embedding.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.4,\n    \"b6\": 0.3,\n    \"b7\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in computational medication recommendation: selection bias and lack of hierarchical knowledge in medical code representation. The proposed G-BERT model combines pre-training techniques and graph neural networks to provide better medical code representation and medication recommendation.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"Devlin et al., 2018\": 0.85,\n    \"Radford et al., 2018\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b3\": 0.75,\n    \"b7\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"Shang et al., 2019\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in computational medication recommendation by proposing G-BERT, which combines pre-training techniques and graph neural networks to improve medical code representation. The primary challenges include selection bias and lack of hierarchical knowledge in existing works.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b4\": 1,\n    \"b1\": 0.9,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"Radford et al., 2018\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"Velickovic et al., 2017\": 0.7,\n    \"Devlin et al., 2018\": 0.7\n  }\n}\n```"], "5f1ff7ea91e011d50a621ab3": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the limitations of existing methods in remote sensing semantic segmentation, particularly their dependency on hand-crafted features and the computational complexity of advanced methods. The proposed algorithm, MACU-Net, introduces multi-scale skip connections, asymmetric convolution blocks, and channel attention blocks to enhance feature extraction and realign channel-wise feature responses adaptively.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b14\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in semantic segmentation of high-resolution remote sensing images. The authors propose MACU-Net, a novel algorithm that incorporates multi-scale skip connections and asymmetric convolution blocks to improve feature extraction and segmentation accuracy.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b11\": 0.9,\n    \"b14\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.8,\n    \"b13\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in semantic segmentation of high-resolution remote sensing images. It proposes a new network architecture called MACU-Net, which incorporates multi-scale skip connections and asymmetric convolution blocks (ACB) to enhance feature extraction and representation. The MACU-Net is compared against several existing frameworks, demonstrating superior performance.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b14\": 0.9,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b23\": 0.6,\n    \"b24\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"Challenges\": \"High dependency on hand-crafted visual features or mid-level semantic features that restrict flexibility and adaptability in semantic segmentation using remote sensing images.\",\n        \"Inspirations\": \"Design of multi-scale skip connections to realign channel-wise feature responses adaptively and substitution of all convolutional layers with asymmetric convolution blocks.\"\n    },\n    \"Direct Inspiration\": [\"b14\", \"b15\", \"b16\", \"b22\", \"b18\"],\n    \"Indirect Inspiration\": [\"b10\", \"b11\", \"b12\", \"b13\", \"b19\", \"b20\", \"b21\"],\n    \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of semantic segmentation using high-resolution remote sensing images. The primary novel contributions are the introduction of a multi-scale skip connection to realign channel-wise feature responses adaptively and the substitution of all convolutional layers of U-Net with asymmetric convolution blocks to enhance representational ability.\",\n  \"Direct Inspiration\": [\n    \"b16\",\n    \"b22\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b10\",\n    \"b11\",\n    \"b12\",\n    \"b13\",\n    \"b14\",\n    \"b15\"\n  ],\n  \"Other Inspiration\": []\n}\n```"], "5ce3a70cced107d4c652a6ae": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the varied sizes and band reflectance of buildings, obscured views caused by trees and shadows, and high intra-class and low inter-class variation of building objects in high-resolution remote sensing images. The paper proposes a new gated residual refinement network (GRRNet) for building extraction using both high-resolution aerial images and LiDAR data. It introduces a new gated feature labeling (GFL) unit to reduce unnecessary feature transmission and refine coarse classification maps, and compares the performance of state-of-the-art deep models using a large dataset from different city scenes.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b41\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b26\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b3\": 0.65,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of accurately and automatically extracting buildings from high-resolution aerial images and LiDAR data, particularly difficulties posed by varied building sizes and occlusions, and high intra-class/low inter-class variation. The authors propose a new gated semantic segmentation neural network (GRRNet) to refine coarse classification maps through gated feature labeling units, aiming to improve the existing FCN models' performance on building extraction tasks.\",\n    \"Direct Inspiration\": [\"b17\", \"b41\"],\n    \"Indirect Inspiration\": [\"b25\", \"b13\", \"b3\"],\n    \"Other Inspiration\": [\"b26\", \"b24\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of accurate and automatic building extraction from high-resolution aerial images and LiDAR data. The main contributions include proposing a new gated semantic segmentation neural network (GRRNet), analyzing the effect of gated feature labeling (GFL) units, and comparing the performance of state-of-the-art deep models.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b41\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.9,\n    \"b13\": 0.9,\n    \"b3\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.7,\n    \"b31\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of accurately and automatically extracting building information from high-resolution aerial images and LiDAR data. It proposes a new gated residual refinement network (GRRNet) that combines a modified residual learning network with a gated feature labeling (GFL) unit to improve the selection and transmission of features, aiming to refine coarse classification maps and improve the extraction results. The study also involves a comparative analysis of the performance of state-of-the-art deep models on a large dataset from different city scenes.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b41\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b25\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.7,\n    \"b24\": 0.7,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Varied sizes and band reflectance of buildings, often obscured by trees and shadows.\",\n      \"High intra-class and low inter-class variation of building objects in high-resolution remote sensing images.\"\n    ],\n    \"Inspirations\": [\n      \"Using multi-source data to provide complementary information on building objects.\",\n      \"Deep Convolutional Neural Networks (CNNs) for processing remote sensing images.\",\n      \"Fully Convolutional Networks (FCNs) for semantic segmentation and pixel-wise labeling.\",\n      \"Gate mechanisms in GSN and G-FRNet for feature selection.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b41\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.9,\n    \"b13\": 0.85,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.75,\n    \"b24\": 0.7,\n    \"b50\": 0.65\n  }\n}\n```"], "5ede0553e06a4c1b26a8419c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning graph representations in an unsupervised manner, specifically through a self-supervised approach that maximizes mutual information (MI) between different structural views of graphs. It aims to overcome the limitations of requiring task-dependent labels and specialized encoders by leveraging recent advances in multi-view visual representation learning.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b5\": 1.0,\n    \"b51\": 1.0,\n    \"b55\": 1.0,\n    \"b49\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b24\": 0.7,\n    \"b67\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b28\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": [\n    \"The paper addresses the challenge of unsupervised learning on graphs, where labeling is costly and difficult. The authors propose a self-supervised approach that maximizes mutual information between representations from different structural views of graphs. This method outperforms previous models in node and graph classification tasks without requiring specialized architectures.\"\n  ],\n  \"Direct Inspiration\": [\n    \"b51\",\n    \"b2\",\n    \"b5\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b49\",\n    \"b55\"\n  ],\n  \"Other Inspiration\": [\n    \"b26\",\n    \"b18\",\n    \"b24\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning graph representations without relying on task-dependent labels, proposing a self-supervised approach that maximizes mutual information between node and graph representations from different structural views of graphs. The method aims to improve performance on node and graph classification tasks without requiring specialized architectures, and it systematically evaluates the major components of the framework to achieve state-of-the-art results.\",\n  \"Direct Inspiration\": [\n    \"b51\",\n    \"b2\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b55\",\n    \"b49\",\n    \"b18\"\n  ],\n  \"Other Inspiration\": [\n    \"b26\",\n    \"b64\",\n    \"b54\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning effective graph representations in the absence of labeled data. It introduces a self-supervised approach using contrastive learning to maximize mutual information (MI) between different structural views of graphs. The method is inspired by recent advances in multi-view visual representation learning and aims to outperform existing self-supervised models and supervised baselines on node and graph classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b5\": 1.0,\n    \"b51\": 1.0,\n    \"b55\": 1.0,\n    \"b49\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9,\n    \"b26\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.8,\n    \"b54\": 0.8,\n    \"b64\": 0.8,\n    \"b67\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of unsupervised representation learning on graphs, particularly focusing on maximizing mutual information (MI) between node and graph representations from different structural views. The proposed algorithm uses graph diffusion to create these views and demonstrates significant improvements over state-of-the-art models in node and graph classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b51\": 0.95,\n    \"b2\": 0.90,\n    \"b5\": 0.90,\n    \"b55\": 0.85,\n    \"b49\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.75,\n    \"b26\": 0.70,\n    \"b24\": 0.70,\n    \"b32\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b67\": 0.60,\n    \"b64\": 0.60,\n    \"b54\": 0.60\n  }\n}\n```"], "5eda19c991e01187f5d6d814": ["```json\n{\n  \"Summary\": \"The paper focuses on analyzing the convergence and stability properties of Graph Convolutional Networks (GCNs) on large random graphs. It introduces a 'continuous' counterpart to discrete GCNs, studies invariance and equivariance to isomorphism of random graph models, and provides non-asymptotic convergence results. The stability analysis of GCNs to small deformations of the underlying random graph model is also covered.\",\n  \"Direct Inspiration\": [\"b31\", \"b3\", \"b37\"],\n  \"Indirect Inspiration\": [\"b7\", \"b13\", \"b24\", \"b18\"],\n  \"Other Inspiration\": [\"b15\", \"b17\", \"b46\", \"b28\", \"b23\", \"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the convergence and stability properties of Graph Convolutional Networks (GCNs) on large random graphs. It proposes a continuous counterpart to discrete GCNs, provides non-asymptotic convergence results, and studies the stability of GCNs to small deformations of the underlying random graph model. The key contributions include defining intuitive notions of model deformations, characterizing stability using a Wasserstein-type metric, and relaxing the usual smoothness assumptions on the similarity kernel.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b3\": 0.9,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b13\": 0.7,\n    \"b24\": 0.7,\n    \"b22\": 0.6,\n    \"b28\": 0.5,\n    \"b39\": 0.5,\n    \"b38\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.4,\n    \"b44\": 0.4,\n    \"b10\": 0.4,\n    \"b45\": 0.4,\n    \"b15\": 0.4,\n    \"b17\": 0.4,\n    \"b46\": 0.4,\n    \"b9\": 0.4,\n    \"b18\": 0.4,\n    \"b23\": 0.4,\n    \"b27\": 0.4,\n    \"b20\": 0.4,\n    \"b29\": 0.4,\n    \"b35\": 0.4,\n    \"b8\": 0.4,\n    \"b4\": 0.4,\n    \"b19\": 0.4,\n    \"b25\": 0.4,\n    \"b34\": 0.4,\n    \"b0\": 0.4,\n    \"b42\": 0.4,\n    \"b40\": 0.4,\n    \"b14\": 0.4,\n    \"b30\": 0.4,\n    \"b6\": 0.4,\n    \"b1\": 0.4,\n    \"b12\": 0.4,\n    \"b41\": 0.4,\n    \"b43\": 0.4,\n    \"b36\": 0.4,\n    \"b16\": 0.4,\n    \"b32\": 0.4,\n    \"b33\": 0.4,\n    \"b11\": 0.4,\n    \"b21\": 0.4,\n    \"b26\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper include understanding the convergence and stability properties of Graph Convolutional Networks (GCNs) on large random graphs, defining a continuous counterpart to discrete GCNs, and analyzing the stability of GCNs to small deformations of the underlying graph models. The paper takes inspiration from classical CNN architectures and their stability properties, adapting these concepts to the domain of GCNs.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b3\": 0.9,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.7,\n    \"b28\": 0.6,\n    \"b17\": 0.5,\n    \"b38\": 0.4,\n    \"b15\": 0.4\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.3,\n    \"b8\": 0.3,\n    \"b7\": 0.2,\n    \"b13\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the convergence and stability properties of Graph Convolutional Networks (GCNs) on large random graphs, proposing a continuous counterpart to discrete GCNs, and studying notions of invariance, equivariance, and stability under graph model deformations. It aims to provide non-asymptotic bounds and intuitive geometric perturbations for GCNs.\",\n  \"Direct Inspiration\": [\"b31\", \"b3\", \"b37\"],\n  \"Indirect Inspiration\": [\"b7\", \"b13\", \"b24\", \"b18\", \"b9\"],\n  \"Other Inspiration\": [\"b28\", \"b29\", \"b30\", \"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the convergence and stability properties of Graph Convolutional Networks (GCNs) on large random graphs, introducing a continuous counterpart to discrete GCNs. This approach allows for defining intuitive notions of model deformations and characterizing their stability, particularly in the context of relatively sparse random graphs.\",\n  \"Direct Inspiration\": [\"b31\", \"b3\"],\n  \"Indirect Inspiration\": [\"b7\", \"b13\", \"b24\", \"b18\", \"b22\", \"b45\"],\n  \"Other Inspiration\": [\"b5\", \"b44\", \"b10\"]\n}\n```"], "5a73cbcc17c44a0b3035f264": ["```json\n{\n  \"Summary\": \"The paper addresses the limitations of Message Passing Neural Networks (MPNNs), specifically their inability to handle permutation covariance, and proposes a new class of neural network architectures called compositional networks (comp-nets) to overcome this issue. The main challenges discussed include handling permutation invariance and capturing multiscale structure in graph learning.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b14\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b2\", \"b16\", \"b7\", \"b6\", \"b19\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b32\", \"b4\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the limitation of Message Passing Neural Networks (MPNNs) in handling permutation invariance issues, impacting their representation power. The authors propose a new class of neural network architectures called compositional networks (comp-nets), which aim to address these limitations by emphasizing the connection to convolutional networks and introducing the concept of steerability. The paper outlines the theoretical foundations and practical implementation of this new architecture and demonstrates its superior performance over MPNNs on several standard datasets.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b14\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b2\", \"b16\", \"b7\", \"b6\", \"b19\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b4\", \"b35\", \"b42\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of learning on graphs, particularly focusing on the limitations of Message Passing Neural Networks (MPNNs) due to their permutation invariance. The authors propose a novel neural network architecture called compositional networks (comp-nets) that emphasize covariance and steerability, inspired by classical convolutional neural networks (CNNs). The comp-nets aim to improve representation power by introducing covariant tensor activations.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.9,\n    \"b2\": 0.8,\n    \"b16\": 0.8,\n    \"b7\": 0.8,\n    \"b6\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b43\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in graph learning, particularly focusing on the limitations of Message Passing Neural Networks (MPNNs) in terms of representation power due to their permutation invariance. The authors propose a new class of neural network architectures called compositional networks (comp-nets) that generalize MPNNs and incorporate the concept of steerability to improve representation power.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b17\": 0.9,\n    \"b28\": 0.9,\n    \"b2\": 0.8,\n    \"b16\": 0.8,\n    \"b6\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a novel class of neural network architectures called compositional networks (comp-nets) to address the limitations of message passing neural networks (MPNNs) in graph learning. The primary challenges addressed include capturing multiscale structure, ensuring permutation invariance, and improving representation power by introducing steerability through covariant tensor propagation.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b25\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.6,\n    \"b2\": 0.5,\n    \"b16\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```"], "5a4aef9e17c44a2190f7a8e4": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of image restoration and generation using deep convolutional neural networks (ConvNets). The authors propose a novel approach where a randomly-initialized ConvNet is used as an image prior, showing that this method performs well without the need for training on large datasets. The core contribution is demonstrating that the structure of ConvNets alone can capture significant image statistics necessary for various image restoration tasks, such as denoising, super-resolution, and inpainting.\",\n    \"Direct Inspiration\": {\n        \"b20\": 0.9,\n        \"b7\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.7,\n        \"b18\": 0.75,\n        \"b28\": 0.75,\n        \"b14\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.65,\n        \"b24\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of image restoration and generation by investigating the implicit priors captured by convolutional neural networks (ConvNets) without any learning from data. It proposes using randomly-initialized ConvNets as image priors for tasks like denoising, super-resolution, and inpainting, demonstrating competitive results compared to learned methods. The key inspiration comes from the notion that the structure of deep networks itself can capture significant image statistics.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b18\": 0.6,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b2\": 0.5,\n    \"b24\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of image restoration by using a randomly-initialized deep convolutional neural network (ConvNet) as an implicit image prior. The key contribution is showing that the structure of the ConvNet itself, without any learned parameters from data, can effectively restore images in tasks such as denoising, super-resolution, and inpainting.\",\n    \"Direct Inspiration\": {\n        \"b20\": 1,\n        \"b7\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b18\": 0.7,\n        \"b24\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.6,\n        \"b19\": 0.5,\n        \"b28\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of image restoration and generation using convolutional neural networks (ConvNets) without relying on learning from large datasets. The novel approach involves using randomly initialized ConvNets to upsample images, serving as an implicit handcrafted prior. The main contributions include demonstrating the effectiveness of untrained ConvNets in various inverse problems such as denoising, super-resolution, and inpainting, and showing that the structure of ConvNets captures essential image statistics independently of learned parameters.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b7\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b18\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.6,\n    \"b2\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is to demonstrate that a deep convolutional neural network (ConvNet) can be effectively used for image restoration and generation tasks without the need for learning from large datasets. The paper proposes a method that uses a randomly-initialized ConvNet to upsample or restore images, showing that the structure of the network itself captures significant image statistics necessary for these tasks.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b18\": 0.8,\n    \"b28\": 0.8,\n    \"b7\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b2\": 0.5,\n    \"b19\": 0.5\n  }\n}\n```"], "5db9295f47c8f766461f5135": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in adversarial training for image classification by introducing a novel feature-scattering approach. The primary challenges include label leaking and neglecting inter-sample relationships. The proposed method leverages inter-sample relationships to generate adversarial examples in an unsupervised manner and integrates this with a bilevel optimization framework for model training.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b35\": 1.0,\n    \"b31\": 0.9,\n    \"b55\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b50\": 0.8,\n    \"b21\": 0.8,\n    \"b58\": 0.8,\n    \"b60\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b57\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in adversarial training for image classification, focusing on issues like label leaking and neglecting inter-sample relationships. It proposes a novel feature-scattering approach for generating adversarial images, leveraging inter-sample structures in an unsupervised fashion, and a new adversarial training formulation that falls into bilevel optimization.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b35\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.9,\n    \"b55\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b50\": 0.7,\n    \"b21\": 0.7,\n    \"b57\": 0.6,\n    \"b1\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving model robustness against adversarial examples in image classification. The authors propose a novel feature-scattering approach for generating adversarial images in an unsupervised manner, which leverages the inter-sample relationships. This method deviates from the conventional minimax formulation and introduces a bilevel optimization framework for adversarial training.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b35\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b57\": 0.8,\n    \"b55\": 0.8,\n    \"b27\": 0.7,\n    \"b58\": 0.7,\n    \"b21\": 0.7,\n    \"b50\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b60\": 0.6,\n    \"b10\": 0.5,\n    \"b8\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in improving model robustness against adversarial examples in image classification. The proposed solution leverages inter-sample relationships for generating adversarial examples using a novel feature-scattering approach in an unsupervised manner. This method deviates from conventional adversarial training methods, aiming to capture the inter-sample structure to improve performance.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b23\": 1.0,\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b50\": 0.8,\n    \"b21\": 0.8,\n    \"b55\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.7,\n    \"b57\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges addressed in this paper are the label leaking and gradient masking issues in adversarial training. The proposed solution is a novel feature-scattering approach for generating adversarial images that leverages inter-sample relationships in an unsupervised fashion to improve model robustness.\",\n    \"Direct Inspiration\": [\"b23\", \"b31\", \"b35\"],\n    \"Indirect Inspiration\": [\"b50\", \"b21\", \"b55\"],\n    \"Other Inspiration\": [\"b1\", \"b57\"]\n}\n```"], "5736986b6e3b12023e72fc2d": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of text classification by proposing a novel approach using character-level Convolutional Networks (ConvNets) instead of traditional word-based methods. The authors explore treating text as a raw signal and apply temporal ConvNets to it, aiming to simplify the engineering process by avoiding the need for word-level information and syntactic or semantic structures. They introduce a modular design with key components like temporal max-pooling and stochastic gradient descent for optimization, and compare their methods against traditional and other deep learning models.\",\n    \"Direct Inspiration\": {\n        \"b16\": 1,\n        \"b17\": 1,\n        \"b27\": 0.95,\n        \"b28\": 0.95\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.9,\n        \"b12\": 0.9,\n        \"b14\": 0.85,\n        \"b15\": 0.85\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.8,\n        \"b2\": 0.75,\n        \"b6\": 0.7,\n        \"b10\": 0.8,\n        \"b22\": 0.8,\n        \"b23\": 0.75,\n        \"b24\": 0.7,\n        \"b25\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of text classification by applying temporal (one-dimensional) convolutional networks (ConvNets) at the character level, rather than the traditional word-based methods. The main inspiration comes from the effectiveness of ConvNets in handling raw signals in other domains such as computer vision and speech recognition. The authors highlight the simplicity and effectiveness of their approach, especially in handling large-scale datasets and various languages without requiring knowledge of syntactic or semantic structures.\",\n  \"Direct Inspiration\": [\"b16\", \"b17\", \"b27\", \"b28\"],\n  \"Indirect Inspiration\": [\"b5\", \"b12\", \"b14\"],\n  \"Other Inspiration\": [\"b1\", \"b2\", \"b23\", \"b25\", \"b30\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of text classification by treating text as a raw signal at the character level and applying temporal ConvNets. The main contributions include the design of character-level ConvNets, character quantization methods, model design for text classification, and data augmentation using a thesaurus.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 0.9,\n    \"b16\": 0.95,\n    \"b17\": 0.9,\n    \"b27\": 0.85,\n    \"b28\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b12\": 0.75,\n    \"b26\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of text classification by treating text as a raw signal at the character level and applying temporal convolutional networks (ConvNets). It emphasizes the novel approach of using ConvNets solely on characters, avoiding the need for word or syntactic knowledge, which simplifies the engineering process and allows for language-agnostic models.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b5\", \"b15\", \"b12\", \"b27\", \"b28\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b16\", \"b17\", \"b26\", \"b25\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b1\", \"b2\", \"b23\", \"b22\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of text classification by applying character-level Convolutional Networks (ConvNets) to raw text signals. The authors propose a novel approach that treats text as a raw signal at the character level, eliminating the need for syntactic or semantic knowledge. They design two ConvNet models and employ techniques like temporal max-pooling, stochastic gradient descent, and data augmentation using a thesaurus. The paper compares the proposed method with traditional and other deep learning models for text classification.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b17\": 0.85,\n    \"b27\": 0.8,\n    \"b28\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b12\": 0.65,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b2\": 0.45,\n    \"b6\": 0.4,\n    \"b10\": 0.35\n  }\n}\n```"], "5843777eac44360f108417ec": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving text classification by incorporating knowledge of document structure into the model architecture, proposing the Hierarchical Attention Network (HAN) which leverages hierarchical document representations and attention mechanisms at both word and sentence levels to capture context-dependent importance of different parts of the document.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b29\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b5\": 0.8,\n    \"b8\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is improving text classification by incorporating knowledge of document structure into the model architecture. The proposed solution is a novel neural architecture called Hierarchical Attention Network (HAN), which captures hierarchical structure and context-dependent importance of words and sentences using attention mechanisms.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1.0,\n        \"b29\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8,\n        \"b5\": 0.8,\n        \"b31\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.6,\n        \"b26\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is improving text classification by better incorporating document structure into model architecture. The authors propose a new neural architecture called Hierarchical Attention Network (HAN) that captures hierarchical document structure and context-dependent relevance of words and sentences using attention mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b29\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b5\": 0.9,\n    \"b31\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b3\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in this paper is improving text classification by incorporating document structure into the model architecture. The proposed solution is the Hierarchical Attention Network (HAN), which captures the hierarchical nature of documents and employs attention mechanisms at both the word and sentence levels to focus on the most informative parts of a document.\",\n    \"Direct Inspiration\": {\n        \"b0\": 0.95,\n        \"b29\": 0.90\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.85,\n        \"b5\": 0.80,\n        \"b31\": 0.75,\n        \"b26\": 0.70\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.65,\n        \"b7\": 0.60,\n        \"b21\": 0.55,\n        \"b3\": 0.50\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving text classification by incorporating knowledge of document structure into the model architecture. The authors propose a novel neural architecture, the Hierarchical Attention Network (HAN), which captures the hierarchical structure of documents and uses two levels of attention mechanisms to differentially weight words and sentences based on their context-dependent importance.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b31\": 0.7,\n    \"b7\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```"], "5f8fffb591e01125c27ddec9": ["```json\n{\n  \"Summary\": \"The main challenges addressed in the paper are overfitting in GNNs and the inefficacy of traditional data augmentation methods on graph data. The proposed algorithm, FLAG (Free Large-scale Adversarial Augmentation on Graphs), introduces adversarial perturbations in the node feature space to improve GNN performance.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b27\", \"b13\", \"b23\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b15\", \"b36\", \"b43\", \"b9\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b32\", \"b21\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of overfitting and handling out-of-distribution test nodes in Graph Neural Networks (GNNs). It proposes a novel algorithm, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which applies adversarial perturbations in the input node feature space to improve GNN performance. FLAG is designed to be simple, scalable, efficient, and versatile, working with various GNN architectures and datasets.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b32\": 1,\n    \"b43\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b13\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of overfitting in Graph Neural Networks (GNNs) when trained on large-scale datasets and proposes a novel method called FLAG (Free Large-scale Adversarial Augmentation on Graphs). FLAG introduces adversarial perturbations in the input node feature space to improve GNN performance while maintaining efficiency and scalability.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b23\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b36\": 0.6,\n    \"b43\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the overfitting problem in Graph Neural Networks (GNNs) and proposes FLAG (Free Large-scale Adversarial Augmentation on Graphs), which adds gradient-based adversarial perturbations to input node features without changing graph structures. FLAG is designed to be scalable, efficient, and applicable to any GNN model for both node and graph classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b32\": 0.9,\n    \"b43\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b23\": 0.6,\n    \"b36\": 0.6,\n    \"b9\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.5,\n    \"b20\": 0.4,\n    \"b14\": 0.4,\n    \"b31\": 0.4,\n    \"b37\": 0.4,\n    \"b21\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of overfitting in Graph Neural Networks (GNNs) on large-scale datasets and the lack of effective data augmentation techniques tailored for GNNs. The authors propose FLAG (Free Large-scale Adversarial Augmentation on Graphs), which operates in the node feature space by adding gradient-based adversarial perturbations. FLAG is designed to be scalable, efficient, and versatile, showing significant improvements in various GNN models and datasets.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b32\": 0.85,\n    \"b36\": 0.8,\n    \"b43\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.6,\n    \"b23\": 0.65,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.55,\n    \"b21\": 0.5,\n    \"b14\": 0.5\n  }\n}\n```"], "5aed14e217c44a4438159759": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in machine learning tasks involving graph-structured datasets by proposing a novel Gated Attention Network (GaAN) for graph learning. The primary contributions include a multi-head attention-based aggregator with additional gates for controlling importance, a unified framework for transforming graph aggregators to graph recurrent neural networks, and achieving state-of-the-art performance on multiple datasets.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b27\": 1.0,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in leveraging graph structured datasets for machine learning tasks, particularly focusing on graph convolution and graph aggregators. The primary contribution is the development of Gated Attention Networks (GaAN) for learning on graphs, which introduces a gating mechanism to improve the multi-head attention model by controlling the importance of each attention head.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b26\", \"b10\", \"b27\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b15\", \"b18\", \"b21\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b6\", \"b24\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges addressed in this paper include capturing the structural information of graphs and reducing computational overhead for large graphs. The proposed solution is a Gated Attention Network (GaAN) that uses a gated multi-head attention mechanism to control the importance of each attention head. The paper demonstrates the effectiveness of GaAN through experiments on node classification and traffic speed forecasting datasets.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9,\n    \"b15\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b21\": 0.7,\n    \"b24\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing effective graph aggregators for machine learning tasks on graph-structured datasets. Inspired by the limitations of existing graph convolutional networks, the authors propose the Gated Attention Networks (GaAN), which introduce gates to control the importance of each attention head in multi-head attention mechanisms. The paper demonstrates the effectiveness of GaAN in inductive node classification and spatiotemporal forecasting tasks.\",\n  \"Direct Inspiration\": [\"b26\", \"b10\", \"b27\"],\n  \"Indirect Inspiration\": [\"b18\", \"b15\", \"b0\"],\n  \"Other Inspiration\": [\"b1\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of learning from graph-structured data, specifically focusing on improving graph convolutional neural networks (GCNs) by proposing Gated Attention Networks (GaAN). GaAN incorporates a gating mechanism into the multi-head attention mechanism to control the importance of each attention head, enhancing the model's ability to capture structural information in graphs. The paper demonstrates the effectiveness of GaAN on node classification and spatiotemporal forecasting tasks.\",\n    \"Direct Inspiration\": {\n        \"b26\": 1.0,\n        \"b27\": 0.9,\n        \"b10\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.7,\n        \"b15\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b21\": 0.6\n    }\n}\n```"], "53e9a5ccb7602d9702ef4f04": ["```json\n{\n  \"Summary\": \"The paper evaluates the predictive power of Joseph E. Granville's market forecasting methods, particularly his ability to predict the direction of the Dow Jones Industrial Average (DJIA). The study employs statistical tests to analyze the accuracy of Granville's buy and sell recommendations over a specific period. The paper also explores the economic implications of following Granville's advice and compares his performance with other market timing strategies.\",\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b3\", \"b4\", \"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the predictive ability of Joseph Granville's stock market forecasting method, particularly focused on his ability to predict the direction of the Dow Jones Industrial Average (DJIA). It examines the accuracy of Granville's predictions over a specific period and assesses whether his method performs better than chance using statistical tests. The primary challenge is to determine the statistical validity of Granville's market predictions and to understand the economic implications of following his advice.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b4\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of forecasting the direction of the Dow Jones Industrial Average (DJIA) with statistical significance, inspired by Joseph E. Granville's market forecasting methods. Granville uses a range of technical indicators and judgmental approaches to make predictions, which the paper aims to validate through statistical testing and comparison with actual market performance.\",\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b4\", \"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper evaluates the market forecasting methods of Joseph E. Granville, focusing on his ability to predict the direction of the Dow Jones Industrial Average (DJIA) using a variety of technical indicators. The study tests the statistical significance of Granville's predictions and examines the economic outcomes of following his advice.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper revolve around the accuracy and reliability of stock market forecasts, specifically the DJIA. The paper analyzes Joseph E. Granville's method of using a combination of technical indicators to predict market movements and tests the statistical significance of his forecasts. The inspiration comes from the need to validate a market forecasting technique that appears to outperform chance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b6\": 0.5\n  }\n}\n```"], "5d9ed4a047c8f76646fb6da2": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating gazetteer information into Chinese Named Entity Recognition (NER) systems, which can introduce errors due to the ambiguous nature of the Chinese language. The authors propose a novel multi-digraph structure combined with an adapted Gated Graph Sequence Neural Network (GGNN) and a BiLSTM-CRF model to resolve conflicting matches and effectively utilize contextual information. The contributions include a new neural approach to NER using a graph structure for gazetteer information, significant performance improvements over previous methods, and the release of a new dataset in the e-commerce domain.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b16\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b9\": 0.7,\n    \"b4\": 0.65,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Named Entity Recognition (NER) systems by using gazetteers while mitigating the issue of erroneous and irrelevant information introduced by these gazetteers, especially in the context of the Chinese language. The authors propose a novel multi-digraph structure combined with an adapted Gated Graph Sequence Neural Networks (GGNN) and a standard bidirectional LSTM-CRF to effectively resolve conflicting matches and leverage contextual information.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.95,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b15\": 0.75,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Chinese Named Entity Recognition (NER) by effectively incorporating named entity gazetteers while resolving the issue of conflicting matches. The authors propose a novel multi-digraph structure combined with an adapted Gated Graph Sequence Neural Networks (GGNN) and a standard bidirectional LSTM-CRF (BiLSTM-CRF). This approach aims to leverage gazetteer knowledge more effectively and improve the precision and recall of NER systems.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the effective incorporation of named entity (NE) gazetteers into Chinese NER systems, given the ambiguity and errors that can arise from wrongly matched entities. The authors propose a novel multi-digraph structure combined with an adapted Gated Graph Sequence Neural Networks (GGNN) and a BiLSTM-CRF layer to resolve these issues. The paper emphasizes the use of a data-driven approach to learn how to combine gazetteer information and resolve matching conflicts based on contextual information.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b11\": 0.7,\n    \"b13\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the incorporation of named entity (NE) gazetteers in Chinese named entity recognition (NER) systems, which often results in erroneous and conflicting matches due to the inherent ambiguity in the Chinese language. The authors propose a novel multi-digraph structure combined with an adapted Gated Graph Sequence Neural Network (GGNN) and a bidirectional LSTM-CRF to effectively utilize contextual information and resolve these conflicts.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b12\": 0.6,\n    \"b15\": 0.6,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b4\": 0.5\n  }\n}\n```"], "5a73cb7417c44a0b3035a202": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of recovering high-resolution (HR) images from low-resolution (LR) images, focusing on the use of very deep convolutional neural networks (CNNs) to tackle issues like the vanishing-gradient problem. The proposed method, SRDenseNet, employs dense connections to improve information flow and integrate both low-level and high-level features for better super-resolution (SR) performance. Deconvolution layers are also utilized to recover image details and speed up the reconstruction process.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b10\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b5\": 0.8,\n    \"b6\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b16\": 0.7,\n    \"b20\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of recovering high-resolution (HR) images from low-resolution (LR) images, a problem known for being highly ill-posed. The main contribution is the introduction of SRDenseNet, which utilizes dense connected convolutional networks to improve information flow and alleviate the vanishing-gradient problem. The paper integrates dense skip connections and deconvolution layers to enhance image super-resolution (SISR) performance.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b10\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b7\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of recovering high-resolution images from low-resolution counterparts, particularly focusing on the difficulty of capturing high-frequency details. The proposed solution, SRDenseNet, incorporates dense skip connections and deconvolution layers to improve information flow, alleviate the vanishing-gradient problem, and enhance feature reuse, resulting in better reconstruction performance.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b10\": 0.9,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the recovery of high-frequency details in high-resolution (HR) images from low-resolution (LR) images in image super-resolution (SR). The proposed solution, SRDenseNet, utilizes dense connected convolutional networks to improve the flow of information through the network and alleviate the vanishing-gradient problem. The novel method combines low-level and high-level features using dense skip connections and integrates deconvolution layers to speed up the reconstruction process.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b10\": 0.95,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.85,\n    \"b2\": 0.8,\n    \"b16\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b7\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of recovering high-resolution images from low-resolution counterparts, focusing on the use of dense connections within deep convolutional neural networks to improve information flow, mitigate the vanishing gradient problem, and reduce feature redundancy. They propose the SRDenseNet method, which combines low-level and high-level features using dense skip connections and integrates deconvolution layers for effective upscaling and reconstruction of image details.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.9,\n        \"b10\": 0.8,\n        \"b11\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b2\": 0.6,\n        \"b16\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.4,\n        \"b7\": 0.4\n    }\n}\n```"], "58437725ac44360f1082f7f7": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of accelerating single image super-resolution (SR) while maintaining or improving performance. It introduces the Fast Super-Resolution Convolutional Neural Network (FSRCNN), which utilizes a compact hourglass-shaped structure, deconvolution layers, and layer sharing for different upscaling factors.\",\n  \"Direct Inspiration\": [\"b0\", \"b1\"],\n  \"Indirect Inspiration\": [\"b12\", \"b13\", \"b14\", \"b22\"],\n  \"Other Inspiration\": [\"b7\", \"b9\", \"b17\", \"b18\", \"b19\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accelerating Single Image Super-Resolution (SR) while maintaining high-quality outputs. The proposed Fast Super-Resolution Convolutional Neural Networks (FSRCNN) structure replaces bicubic interpolation with a deconvolution layer and introduces a compact hourglass-like architecture to improve computational efficiency and speed without compromising performance.\",\n  \"Direct Inspiration\": [\"b0\", \"b1\"],\n  \"Indirect Inspiration\": [\"b7\", \"b12\", \"b13\", \"b14\"],\n  \"Other Inspiration\": [\"b19\", \"b20\", \"b22\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of accelerating single image super-resolution (SR) while maintaining high quality. The proposed Fast Super-Resolution Convolutional Neural Network (FSRCNN) achieves significant speedup and superior performance compared to the Super-Resolution Convolutional Neural Network (SRCNN). The key innovations include the use of a deconvolution layer, an hourglass-shaped network structure, and shared convolution layers for different upscaling factors.\",\n    \"Direct Inspiration\": [\n        \"b0\",\n        \"b1\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b7\",\n        \"b12\",\n        \"b13\",\n        \"b22\"\n    ],\n    \"Other Inspiration\": [\n        \"b14\",\n        \"b19\",\n        \"b20\"\n    ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the speed and efficiency of single image super-resolution using convolutional neural networks (CNNs). The proposed Fast Super-Resolution Convolutional Neural Network (FSRCNN) aims to overcome limitations of the Super-Resolution Convolutional Neural Network (SRCNN) by introducing a more concise and efficient network structure, incorporating deconvolution layers, and optimizing the mapping process. The novel FSRCNN achieves significant speed-ups while maintaining or improving image restoration quality.\",\n  \"Direct Inspiration\": [\"b0\", \"b1\"],\n  \"Indirect Inspiration\": [\"b2\", \"b4\", \"b6\", \"b7\"],\n  \"Other Inspiration\": [\"b12\", \"b13\", \"b14\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of significantly accelerating the Super-Resolution Convolutional Neural Network (SRCNN) while maintaining or improving its performance. The authors propose a new network structure termed Fast Super-Resolution Convolutional Neural Networks (FSRCNN) which introduces a deconvolution layer to replace bicubic interpolation, and adds shrinking and expanding layers to reduce computation complexity. The FSRCNN achieves a speed-up of more than 40\u00d7 compared to SRCNN-Ex, with superior performance.\",\n    \"Direct Inspiration\": [\"b0\", \"b1\"],\n    \"Indirect Inspiration\": [\"b7\", \"b19\", \"b20\"],\n    \"Other Inspiration\": [\"b24\", \"b22\", \"b9\"]\n}\n```"], "5d0b003a8607575390fb4f6a": ["```json\n{\n    \"Summary\": {\n        \"Challenges\": \"The main challenges addressed are the inefficiency and segmentation errors in existing Chinese NER models, especially lattice-structured models.\",\n        \"Inspirations\": \"Inspired by works integrating word information into character-based models, specifically lattice LSTM models, and multi-task learning for NLP.\"\n    },\n    \"Direct Inspiration\": {\n        \"b40\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b25\": 0.8,\n        \"b0\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5,\n        \"b18\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Chinese Named Entity Recognition (NER) by proposing a novel word-character LSTM (WC-LSTM) model. The main challenges include segmentation errors in character-based models and inefficiencies in lattice-structured models. The proposed solution integrates word information into character-based models without degenerating into a partial word-based model and allows for batch training. The effectiveness of the model is demonstrated on four datasets, achieving state-of-the-art results.\",\n  \"Direct Inspiration\": [\n    \"b40\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b34\"\n  ],\n  \"Other Inspiration\": [\n    \"b2\",\n    \"b36\",\n    \"b18\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of integrating word information into character-based models for Chinese Named Entity Recognition (NER). The authors propose a novel word-character LSTM (WC-LSTM) that prevents model degeneration into a partial word-based model and introduces four strategies to extract fixed-sized useful information from different words, enabling batch training. The proposed model outperforms state-of-the-art models on four Chinese NER datasets.\",\n  \"Direct Inspiration\": [\"b40\"],\n  \"Indirect Inspiration\": [\"b34\", \"b0\", \"b7\", \"b24\"],\n  \"Other Inspiration\": [\"b2\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the inefficiency and segmentation errors in Chinese Named Entity Recognition (NER) models, particularly those using lattice-structured LSTM models. The authors propose a novel word-character LSTM (WC-LSTM) to integrate word information into character-based models without shortcut paths, ensuring batch training efficiency. They introduce four strategies to encode word information into fixed-sized vectors and demonstrate the model's superior performance on four datasets.\",\n  \"Direct Inspiration\": [\"b40\"],\n  \"Indirect Inspiration\": [\"b34\", \"b18\"],\n  \"Other Inspiration\": [\"b25\", \"b0\", \"b24\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the inefficiency and errors in Chinese NER models due to segmentation errors and the variable length of word paths. The proposed solution is a novel word-character LSTM (WC-LSTM) model that integrates word information into a character-based model without shortcut paths, enabling batch training and improved efficiency.\",\n  \"Direct Inspiration\": [\"b40\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b2\", \"b18\"]\n}\n```"], "5c8bd2e54895d9cbc6af9826": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of inferring linguistic typology from parallel corpora using massively multilingual neural machine translation (NMT) systems. It proposes novel methods for aggregating the values of the latent state of the encoder neural network to a single vector representing the entire language. The study demonstrates the effectiveness of these methods for typology prediction, particularly for syntactic features.\",\n    \"Direct Inspiration\": {\n        \"b4\": 0.9,\n        \"b12\": 0.9,\n        \"b34\": 0.9,\n        \"b19\": 0.9,\n        \"b2\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b22\": 0.8,\n        \"b31\": 0.8,\n        \"b17\": 0.8,\n        \"b14\": 0.8,\n        \"b18\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.7,\n        \"b5\": 0.7,\n        \"b37\": 0.7,\n        \"b30\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the sparse population of typological databases and the necessity to infer missing typological features for languages. The authors propose an algorithm that leverages a massively multilingual neural machine translation (NMT) system to infer these typological features. They introduce a novel method for aggregating latent state values of the NMT encoder neural network to create a single vector representing an entire language.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b12\": 0.9,\n    \"b38\": 0.9,\n    \"b3\": 0.9,\n    \"b34\": 0.9,\n    \"b19\": 0.9,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b31\": 0.8,\n    \"b17\": 0.8,\n    \"b14\": 0.8,\n    \"b21\": 0.8,\n    \"b30\": 0.8,\n    \"b5\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b36\": 0.7,\n    \"b37\": 0.7,\n    \"b33\": 0.7,\n    \"b27\": 0.7,\n    \"b32\": 0.7,\n    \"b23\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the inference of linguistic typology from parallel corpora using a massively multi-lingual neural machine translation (NMT) system. The proposed algorithm involves training an NMT model on 1017 languages and using the learned representations to infer typological features for each language. The motivation comes from the mismatch between the needs of NLP tasks and the sparsity of typological databases, and the paper proposes a novel method for aggregating the values of the latent state of the encoder neural network to a single vector representing the entire language.\",\n    \"Direct Inspiration\": {\n        \"b22\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b5\": 0.8,\n        \"b17\": 0.8,\n        \"b31\": 0.7,\n        \"b34\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.6,\n        \"b38\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inferring linguistic typology from parallel corpora by training a massively multi-lingual neural machine translation (NMT) system. The motivations include the mismatch between the needs of NLP tasks and the sparse population of typological databases, as well as prior work in linguistics and NLP showing the extraction of syntactic knowledge from neural nets. The novel contributions of the paper include several methods for discovering feature vectors for typology prediction, particularly a new method for aggregating the values of the latent state of the encoder neural network to a single vector representing the entire language.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b12\": 0.9,\n    \"b38\": 0.9,\n    \"b3\": 0.9,\n    \"b34\": 0.9,\n    \"b19\": 0.9,\n    \"b2\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b17\": 0.8,\n    \"b14\": 0.8,\n    \"b21\": 0.8,\n    \"b30\": 0.8,\n    \"b5\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b7\": 0.7,\n    \"b28\": 0.7,\n    \"b11\": 0.7,\n    \"b29\": 0.7,\n    \"b0\": 0.7,\n    \"b26\": 0.7,\n    \"b35\": 0.7,\n    \"b37\": 0.7,\n    \"b8\": 0.7,\n    \"b10\": 0.7,\n    \"b9\": 0.7,\n    \"b6\": 0.7,\n    \"b25\": 0.7,\n    \"b20\": 0.7,\n    \"b15\": 0.7,\n    \"b36\": 0.7,\n    \"b16\": 0.7,\n    \"b27\": 0.7,\n    \"b33\": 0.7,\n    \"b32\": 0.7,\n    \"b23\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inferring linguistic typology from parallel corpora through training a massively multilingual neural machine translation (NMT) system. It proposes a novel method for aggregating the values of the latent state of the encoder neural network to a single vector representing the entire language. Key inspirations include prior work in translation studies, contrastive linguistic analysis, and NLP approaches to extract syntactic knowledge from neural nets.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b12\": 0.9,\n    \"b38\": 0.9,\n    \"b34\": 0.9,\n    \"b19\": 0.9,\n    \"b2\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b17\": 0.8,\n    \"b31\": 0.8,\n    \"b37\": 0.8,\n    \"b21\": 0.8,\n    \"b30\": 0.8,\n    \"b5\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b36\": 0.7,\n    \"b20\": 0.7,\n    \"b23\": 0.7\n  }\n}\n```"], "5da2f8aa3a55ac3402d8c092": ["```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing CNN-based single image super-resolution (SISR) methods, proposing a deep second-order attention network (SAN) that leverages second-order channel attention (SOCA) and non-locally enhanced residual groups (NLRG) for improved feature expression and correlation learning.\",\n  \"Direct Inspiration\": [\"b15\"],\n  \"Indirect Inspiration\": [\"b2\", \"b12\", \"b20\", \"b38\", \"b39\"],\n  \"Other Inspiration\": [\"b9\", \"b33\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing CNN-based single image super-resolution (SISR) methods, such as not fully utilizing information from the original low-resolution (LR) images and not exploiting inherent feature correlations. The proposed solution is a deep second-order attention network (SAN) that incorporates a second-order channel attention (SOCA) mechanism and a non-locally enhanced residual group (NLRG) structure to improve feature expression and correlation learning.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b39\": 0.8,\n    \"b38\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.6,\n    \"b9\": 0.6,\n    \"b33\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.4,\n    \"b36\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing Single Image Super-Resolution (SISR) methods, particularly focusing on the underutilization of low-resolution (LR) image information and insufficient exploitation of feature correlations in Convolutional Neural Networks (CNNs). The authors propose a deep second-order attention network (SAN) which includes a Second-Order Channel Attention (SOCA) mechanism and Non-Locally Enhanced Residual Group (NLRG) structure to overcome these challenges.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b33\": 0.85,\n    \"b38\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b20\": 0.75,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges addressed in the paper are the limitations of existing SISR methods, including the underutilization of original LR image information and the lack of consideration for feature correlations in intermediate layers. The proposed solution is a deep second-order attention network (SAN) with a second-order channel attention (SOCA) mechanism and a non-locally enhanced residual group (NLRG) structure to improve feature expression and correlation learning.\",\n  \"Direct Inspiration\": [\n    \"b15\",\n    \"b20\",\n    \"b38\",\n    \"b39\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b2\",\n    \"b12\",\n    \"b13\",\n    \"b9\",\n    \"b33\"\n  ],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of single image super-resolution (SISR) by proposing a deep second-order attention network (SAN). Key challenges include the ill-posed nature of SISR, the limitations of existing CNN-based SR models in utilizing original LR information, and the need for better feature correlation learning. The SAN improves feature expression and correlation learning through a second-order channel attention (SOCA) mechanism and a non-locally enhanced residual group (NLRG) structure.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b20\": 0.9,\n    \"b39\": 0.9,\n    \"b38\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b33\": 0.8,\n    \"b19\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b6\": 0.65\n  }\n}\n```"], "599c7982601a182cd2645d4e": ["```json\n{\n  \"Summary\": \"The main challenges addressed in the paper are: (1) high computational cost and visible reconstruction artifacts due to pre-defined upsampling operators, (2) blurry predictions resulting from the \u21132 loss function, and (3) difficulties in training for large scaling factors and generating intermediate SR predictions at multiple resolutions. The proposed Laplacian Pyramid Super-Resolution Network (LapSRN) addresses these issues by extracting features directly from LR images, employing a robust Charbonnier loss function, and progressively predicting sub-band residuals using a cascade of CNNs.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b10\": 0.9,\n    \"b19\": 0.8,\n    \"b35\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.7,\n    \"b20\": 0.6,\n    \"b28\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b22\": 0.4,\n    \"b18\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three main challenges in single-image super-resolution (SR): the inefficiency and artifacts from pre-defined upsampling operators, the blurriness caused by \u21132 loss, and the difficulty in training for large scaling factors. The proposed Laplacian Pyramid Super-Resolution Network (LapSRN) aims to overcome these issues by using a cascade of convolutional neural networks to progressively predict sub-band residuals, optimizing with a robust Charbonnier loss function, and enabling flexible intermediate SR predictions.\",\n  \"Direct Inspiration\": [\"b9\", \"b10\", \"b19\", \"b35\"],\n  \"Indirect Inspiration\": [\"b8\", \"b20\", \"b22\", \"b30\"],\n  \"Other Inspiration\": [\"b6\", \"b16\", \"b23\", \"b36\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on addressing the challenges in single-image super-resolution (SR), specifically the issues with pre-defined upsampling operators, the limitations of \u21132 loss, and the difficulties of training for large scaling factors. The authors propose the Laplacian Pyramid Super-Resolution Network (LapSRN) that uses a cascade of convolutional neural networks (CNNs) to progressively predict sub-band residuals in a coarse-to-fine fashion. The network utilizes a robust Charbonnier loss function for better handling outliers, and it generates multiple intermediate SR predictions in a single feed-forward pass.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b10\": 0.9,\n    \"b19\": 0.9,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.7,\n    \"b20\": 0.6,\n    \"b28\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.4,\n    \"b22\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three main issues in existing single-image super-resolution (SR) methods: high computational cost and artifacts from predefined upsampling operators, blurry predictions due to \u21132 loss, and the difficulty in training for large scaling factors and generating intermediate SR predictions. The proposed solution, Laplacian Pyramid Super-Resolution Network (LapSRN), progressively predicts sub-band residuals using convolutional and transposed convolutional layers and employs a robust Charbonnier loss function for better handling outliers.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.95,\n    \"b10\": 0.85,\n    \"b19\": 0.90,\n    \"b35\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.70,\n    \"b28\": 0.65,\n    \"b32\": 0.60\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.55,\n    \"b22\": 0.50\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three primary challenges in single-image super-resolution (SR): 1) the use of pre-defined upsampling operators leading to computational cost and artifacts, 2) the use of \u21132 loss function resulting in blurry predictions, and 3) difficulties in training for large scaling factors and lack of intermediate SR predictions. The proposed solution, Laplacian Pyramid Super-Resolution Network (LapSRN), utilizes a cascade of convolutional neural networks (CNNs) to progressively predict sub-band residuals in a coarse-to-fine fashion, optimized with a robust Charbonnier loss function.\",\n  \n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b10\": 0.9,\n    \"b19\": 0.9,\n    \"b35\": 0.8\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b28\": 0.7,\n    \"b30\": 0.7,\n    \"b20\": 0.6\n  },\n  \n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b23\": 0.5,\n    \"b36\": 0.5\n  }\n}\n```"], "5efb0d5691e011063336d39c": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of incomplete and noisy annotation in distantly supervised Named Entity Recognition (NER) for open-domain scenarios. It proposes a novel two-stage framework called BOND, which leverages pre-trained language models (e.g., BERT, RoBERTa) to improve the quality of predictions and employ a teacher-student framework to iteratively refine pseudo-labels.\",\n    \"Direct Inspiration\": {\n        \"b5\": 0.9,\n        \"b22\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b27\": 0.75,\n        \"b29\": 0.7,\n        \"b45\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b17\": 0.6,\n        \"b30\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in Named Entity Recognition (NER) with distant supervision: incomplete annotation due to limited coverage of knowledge bases and noisy annotation caused by labeling ambiguity. The proposed solution, BOND, leverages pre-trained language models (e.g., BERT, RoBERTa) in a two-stage training framework to improve the quality of distant labels and enhance model fitting through a teacher-student framework.\",\n  \"Direct Inspiration\": [\"b5\", \"b22\", \"b45\", \"b30\"],\n  \"Indirect Inspiration\": [\"b29\", \"b17\"],\n  \"Other Inspiration\": [\"b2\", \"b7\", \"b8\", \"b14\", \"b24\", \"b35\", \"b37\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of label scarcity, noisy annotation, and incomplete annotation in Named Entity Recognition (NER) tasks. It proposes a model called BOND, which leverages pre-trained language models and a two-stage training framework to improve the quality of distant supervision labels and enhance the NER model's performance.\",\n    \"Direct Inspiration\": {\n        \"b5\": 0.9,\n        \"b22\": 0.9,\n        \"b45\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b29\": 0.7,\n        \"b30\": 0.7,\n        \"b17\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.6,\n        \"b7\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in Named Entity Recognition (NER) related to noisy and incomplete annotations generated through distant supervision. It proposes a two-stage training framework leveraging pre-trained language models like RoBERTa to improve label quality and model performance.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.7,\n    \"b45\": 0.7,\n    \"b30\": 0.6,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.5,\n    \"b2\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of label scarcity, incomplete annotation, and noisy annotation in Named Entity Recognition (NER) tasks. It proposes a novel two-stage framework called BOND (BERT-Assisted Open-Domain Named entity recognition with Distant Supervision) that leverages pre-trained language models to improve label quality and model fitting through fine-tuning and a teacher-student training approach.\",\n  \"Direct Inspiration\": [\"b5\", \"b22\"],\n  \"Indirect Inspiration\": [\"b27\", \"b29\", \"b45\"],\n  \"Other Inspiration\": [\"b24\", \"b10\", \"b15\"]\n}\n```"], "5c5c55bfe1cd8e03e71689a9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving recommendation systems by leveraging sequences of user-item interactions. It critiques existing RNN and CNN-based models, particularly Caser, for their inefficiencies in handling long-range dependencies in sequences. The paper proposes a novel CNN-based generative model using 1D dilated convolutions and residual learning to better capture item inter-dependencies and optimize performance in sequential recommendation tasks.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b30\": 0.9,\n    \"b21\": 0.8,\n    \"b17\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.6,\n    \"b19\": 0.5,\n    \"b24\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.4,\n    \"b3\": 0.4,\n    \"b25\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing RNN and CNN-based models in sequential recommendation systems, focusing on the drawbacks of the Caser model. It proposes a novel CNN-based sequence embedding model that leverages dilated convolutions and residual networks to better capture long-range dependencies and improve training efficiency.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b30\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b17\": 0.8,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving real-world recommender systems by leveraging sequences of user-item interactions. The proposed model tackles the limitations of RNN-based and CNN-based models like Caser by introducing a novel CNN-based sequential recommendation model that employs 1D dilated convolution layers and residual networks to handle long-range dependencies and improve efficiency. The model explicitly encodes item inter-dependencies and removes the pooling layer to enhance performance.\",\n  \"Direct Inspiration\": [\"b28\", \"b30\"],\n  \"Indirect Inspiration\": [\"b14\", \"b21\", \"b25\"],\n  \"Other Inspiration\": [\"b3\", \"b17\", \"b9\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses limitations in existing CNN-based sequential recommendation models (e.g., Caser) by proposing a new CNN-based generative model that employs 1D dilated convolutions and residual networks. The challenges include modeling long-range dependencies and preserving item inter-dependencies in sequences. The proposed method aims to improve training speed and recommendation accuracy for large-scale datasets.\",\n  \"Direct Inspiration\": [\"b28\", \"b30\"],\n  \"Indirect Inspiration\": [\"b3\", \"b21\", \"b25\"],\n  \"Other Inspiration\": [\"b14\", \"b19\", \"b24\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing RNN-based and CNN-based models for sequential recommendations. It proposes a novel CNN-based sequential recommendation model that uses 1D dilated convolutional layers and residual learning to effectively model long-range item sequences and complex inter-dependencies, achieving state-of-the-art results.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b14\": 0.85,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.75,\n    \"b25\": 0.75,\n    \"b17\": 0.7\n  }\n}\n```"], "5d343abd275ded87f9774ef3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing non-differentiable ranking metrics like NDCG in Learning-to-Rank (LTR) tasks using deep neural networks. It proposes directly optimizing NDCG by approximating it with a smooth, differentiable function, potentially achieving comparable results to state-of-the-art algorithms like LambdaMART.\",\n  \"Direct Inspiration\": [\"b13\"],\n  \"Indirect Inspiration\": [\"b14\", \"b16\"],\n  \"Other Inspiration\": [\"b4\", \"b6\", \"b7\", \"b9\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"This paper focuses on optimizing the ranking metric NDCG directly using deep neural networks, addressing the non-differentiability issue of ranking metrics. The novel ApproxNDCG method is proposed, which leverages recent advances in neural network training and powerful optimizers.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b16\": 0.7,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b5\": 0.5,\n    \"b8\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the non-differentiability of ranking metrics like NDCG, which complicates their optimization using gradient descent methods. The paper proposes a novel approach by directly optimizing NDCG using deep neural networks and revisits the ApproxNDCG method in light of recent advances in hardware and software for training neural networks. The key inspiration for this work appears to be the framework proposed by Qin et al. [b13] which approximates the rank variable using a sum of sigmoids.\",\n  \"Direct Inspiration\": [\"b13\"],\n  \"Indirect Inspiration\": [\"b14\", \"b16\"],\n  \"Other Inspiration\": [\"b18\", \"b2\", \"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the optimization of non-differentiable ranking metrics like NDCG using gradient descent methods. The proposed algorithm, ApproxNDCG, approximates the rank variable with a sum of sigmoids to enable direct gradient computation and leverages recent advances in deep neural networks to optimize NDCG directly. The paper compares this approach with existing state-of-the-art LTR algorithms such as LambdaMART.\",\n  \"Direct Inspiration\": {\n    \"1\": \"b13\",\n    \"2\": \"b16\"\n  },\n  \"Indirect Inspiration\": {\n    \"1\": \"b14\",\n    \"2\": \"b4\"\n  },\n  \"Other Inspiration\": {\n    \"1\": \"b7\",\n    \"2\": \"b15\",\n    \"3\": \"b9\"\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing non-differentiable ranking metrics like NDCG using gradient descent methods. The proposed solution is to use a smooth approximation of NDCG called ApproxNDCG and leverage deep neural networks to directly optimize this metric, yielding results comparable to state-of-the-art LTR algorithms like LambdaMART.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b16\": 0.7,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b5\": 0.5,\n    \"b8\": 0.4,\n    \"b18\": 0.4\n  }\n}\n```"], "573696046e3b12023e517e10": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges in single image super-resolution (SISR) by proposing a novel deep learning method that improves the limitations of existing methods like SRCNN. The primary contributions include utilizing a very deep network with a large receptive field, implementing residual learning for faster convergence, and developing a single-model approach for multi-scale super-resolution.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b21\": 0.9,\n        \"b4\": 0.8,\n        \"b19\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b23\": 0.6,\n        \"b24\": 0.6,\n        \"b7\": 0.6,\n        \"b25\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of single image super-resolution (SISR), focusing on generating high-resolution images from low-resolution inputs. It highlights the limitations of existing methods, particularly SRCNN, and proposes a new method that leverages a very deep convolutional network, residual learning, high learning rates, and adjustable gradient clipping. The paper also introduces a single-model approach for multiple scale factors.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b21\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.7,\n    \"b24\": 0.7,\n    \"b28\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of generating high-resolution images from low-resolution ones, known as single image super-resolution (SISR). The primary challenges identified are the reliance on small image regions for context, slow training convergence, and the need for multiple scale factors. The proposed solution involves a very deep convolutional network using large receptive fields, residual-learning CNNs, high learning rates with gradient clipping, and a single-model approach for multi-scale super-resolution.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b19\": 0.8,\n    \"b23\": 0.7,\n    \"b24\": 0.7,\n    \"b27\": 0.6,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b5\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of generating high-resolution images from low-resolution inputs, particularly focusing on the limitations of existing methods like SRCNN in terms of context, convergence speed, and scale factor adaptability. The proposed method introduces a very deep convolutional network utilizing residual-learning and adjustable gradient clipping to speed up training and handle multiple scales efficiently.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b21\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b19\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in single image super-resolution (SISR) by proposing a very deep convolutional network. The key challenges include limited context in small image regions, slow training convergence, and scale-specific models. The proposed solutions involve utilizing large receptive fields for contextual information, residual-learning CNN with high learning rates for faster convergence, and a single-model approach for multi-scale factor super-resolution.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b21\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b23\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.7,\n    \"b28\": 0.7,\n    \"b19\": 0.6\n  }\n}\n```"], "5e71f49891e0115656f5d0a5": ["```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in the field of image super-resolution (SR): the ill-posed nature of mapping from low-resolution (LR) to high-resolution (HR) images, and the difficulty of obtaining a good SR model when paired data is unavailable. The authors propose a novel dual regression scheme that forms a closed-loop to enhance SR performance by introducing an additional constraint to reduce the possible space of mapping functions and by enabling the use of unpaired data to adapt SR models to real-world applications.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b18\": 0.85,\n    \"b53\": 0.9,\n    \"b45\": 0.8,\n    \"b56\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b57\": 0.75,\n    \"b58\": 0.7,\n    \"b44\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b42\": 0.6,\n    \"b43\": 0.6,\n    \"b55\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses two main challenges in image super-resolution (SR): the large space of possible mapping functions between low-resolution (LR) and high-resolution (HR) images, and the difficulty in training SR models with unpaired data. The authors propose a novel dual regression scheme that introduces an additional constraint to form a closed-loop, thereby reducing the possible function space and enabling the use of unpaired data for training. This approach aims to improve SR performance by making it easier to learn good mappings from LR to HR images and by adapting SR models to real-world data.\",\n    \"Direct Inspiration\": {\n        \"b45\": 0.9,\n        \"b56\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.8,\n        \"b28\": 0.8,\n        \"b53\": 0.8,\n        \"b58\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.7,\n        \"b44\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses two primary challenges in image super-resolution (SR): the large space of possible mapping functions from low-resolution (LR) to high-resolution (HR) images, and the difficulty of obtaining paired training data in real-world applications. The proposed solution is a novel dual regression scheme that adds an additional constraint to form a closed-loop, reducing the function space and allowing for training with both paired and unpaired data.\",\n    \"Direct Inspiration\": {\n        \"b18\": 0.9,\n        \"b45\": 0.9,\n        \"b56\": 0.9,\n        \"b53\": 0.8,\n        \"b58\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b28\": 0.7,\n        \"b4\": 0.7,\n        \"b57\": 0.6\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in image super-resolution (SR): the large space of possible functions mapping low-resolution (LR) to high-resolution (HR) images, and the difficulty of obtaining paired training data for real-world applications. The authors propose a novel dual regression scheme that forms a closed-loop to improve SR performance by introducing an additional constraint to reduce the possible function space and allowing adaptation to unpaired real-world data.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1,\n    \"b56\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b28\": 0.8,\n    \"b53\": 0.8,\n    \"b58\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b57\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses two primary challenges in image super-resolution (SR): the large possible space of mapping functions from low-resolution (LR) to high-resolution (HR) images and the difficulty in obtaining a promising SR model when paired training data are unavailable. The proposed solution is a novel dual regression scheme that forms a closed-loop to enhance SR performance by introducing an additional constraint to reduce the possible function space and by adapting SR models to real-world data using unpaired data.\",\n    \"Direct Inspiration\": {\n        \"b45\": 1.0,\n        \"b56\": 1.0,\n        \"b58\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.8,\n        \"b28\": 0.8,\n        \"b53\": 0.8,\n        \"b4\": 0.8,\n        \"b57\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.6,\n        \"b42\": 0.6,\n        \"b43\": 0.6,\n        \"b55\": 0.6\n    }\n}\n```"], "5e79da4491e0115bb1157b77": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of enhancing literature recommender systems (LRS) by classifying the semantic relations between document pairs. It combines relation extraction, document classification, and document similarity to enable analogical queries and improve LRS. The authors propose a multi-class document classification method, implement multiple models using various embeddings and architectures, and introduce a novel dataset using Wikipedia and Wikidata.\",\n    \"Direct Inspiration\": {\n        \"b9\": 0.9,\n        \"b14\": 0.9,\n        \"b22\": 0.85,\n        \"b30\": 0.85,\n        \"b32\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.75,\n        \"b20\": 0.7,\n        \"b23\": 0.7,\n        \"b40\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b41\": 0.6,\n        \"b35\": 0.55,\n        \"b27\": 0.5,\n        \"b8\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of similarity measures in literature recommender systems (LRS), which often neglect the various facets of extensive documents. The authors propose a method to classify the semantic relations of document pairs to enable more intuitive analogical queries. The methodology combines relation extraction, document classification, and similarity measures using models based on GloVe, Paragraph Vectors (Doc2vec), BERT, and XLNet. The evaluation is conducted using a novel dataset composed of Wikipedia article pairs and Wikidata properties.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b5\": 0.85,\n    \"b23\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.8,\n    \"b22\": 0.8,\n    \"b14\": 0.85,\n    \"b40\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.75,\n    \"b35\": 0.75,\n    \"b37\": 0.7,\n    \"b16\": 0.75,\n    \"b8\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving literature recommender systems (LRS) by classifying semantic relations between document pairs, allowing for more nuanced recommendations beyond simple similarity measures. The authors propose a novel dataset and multiple models leveraging various embeddings and architectures to achieve this goal.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b14\": 0.8,\n    \"b16\": 0.85,\n    \"b22\": 0.8,\n    \"b30\": 0.75,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b12\": 0.7,\n    \"b31\": 0.65,\n    \"b33\": 0.7,\n    \"b37\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.55,\n    \"b5\": 0.6,\n    \"b6\": 0.55,\n    \"b26\": 0.6,\n    \"b35\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving literature recommender systems by classifying the semantic relations of document pairs, enabling more nuanced recommendations and analogical queries. It combines ideas from relation extraction, document classification, and document similarity to achieve this goal. The authors propose a method for classifying semantic relations between document pairs and evaluate six different models using word-based and deep contextual language models. Additionally, they introduce a novel dataset composed of Wikipedia article pairs and Wikidata properties defining their semantic relations.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b14\": 1.0,\n    \"b22\": 0.9,\n    \"b30\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b8\": 0.5,\n    \"b20\": 0.5,\n    \"b26\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of recommending documents in digital libraries by focusing on the semantic relations between document pairs. The authors propose a method to classify these semantic relations using models based on word embeddings and deep contextual language models. They evaluate the techniques using a novel dataset built from Wikipedia and Wikidata, which includes 32,168 article pairs and their corresponding semantic relations.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b14\": 0.92,\n    \"b22\": 0.89,\n    \"b30\": 0.90,\n    \"b32\": 0.87,\n    \"b40\": 0.91\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b16\": 0.78,\n    \"b23\": 0.80,\n    \"b9\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b12\": 0.68,\n    \"b35\": 0.70,\n    \"b26\": 0.72\n  }\n}\n```"], "5d3ed25a275ded87f97deaab": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper include the difficulty of typing on mobile devices, the complexity of capturing user intent dynamically, and the reliance on handcrafted features in existing methods which do not fully exploit interaction information.\",\n    \"inspirations\": \"The paper proposes a novel Metapath-guided Embedding method for Intent Recommendation (MEIRec) using a Heterogeneous Information Network (HIN) and heterogeneous Graph Neural Network (GNN) to exploit rich interaction information and reduce parameter space.\"\n  },\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b7\": 0.85,\n    \"b18\": 0.85,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b13\": 0.6,\n    \"b21\": 0.6,\n    \"b23\": 0.6,\n    \"b9\": 0.6,\n    \"b16\": 0.6,\n    \"b11\": 0.6,\n    \"b20\": 0.6,\n    \"b1\": 0.6,\n    \"b4\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of intent recommendation in mobile e-commerce, which involves recommending personalized search intents to users based on their historical behaviors without any query input. The proposed solution, MEIRec, utilizes a heterogeneous information network (HIN) and a novel metapath-guided embedding method leveraging graph neural networks (GNN) to capture rich interaction information. Key contributions include the novel problem definition, the MEIRec model, and experimental validation showing significant performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b19\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.65,\n    \"b3\": 0.6,\n    \"b23\": 0.55,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of intent recommendation in mobile e-commerce platforms, focusing on providing personalized query suggestions based on user historical behaviors without query input. The proposed solution, MEIRec, is based on a Heterogeneous Information Network (HIN) and uses a Metapath-guided Embedding method with a Graph Neural Network (GNN) to exploit rich interaction information among users, items, and queries. The model also features a uniform term embedding mechanism to reduce parameter space while handling large-scale data.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b18\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b6\": 0.65,\n    \"b19\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the development of an intent recommendation system for mobile e-commerce platforms that leverages user interaction data to recommend personalized queries without user input. The proposed solution involves modeling the intent recommendation system as a Heterogeneous Information Network (HIN) and using a novel Metapath-guided Embedding method with a Heterogeneous Graph Neural Network (GNN) to fully utilize rich interaction information. The uniform term embedding mechanism is introduced to reduce parameter space and handle large-scale data.\",\n  \"Direct Inspiration\": [\"b6\", \"b3\", \"b17\"],\n  \"Indirect Inspiration\": [\"b7\", \"b18\", \"b22\"],\n  \"Other Inspiration\": [\"b11\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of providing personalized intent recommendations on mobile e-commerce platforms by leveraging rich interaction data. The novel MEIRec model utilizes a Heterogeneous Information Network (HIN) and a heterogeneous Graph Neural Network (GNN) to capture complex interactions among users, items, and queries. The paper highlights the importance of metapath-guided neighbors for aggregating rich neighbor information and introduces a uniform term embedding mechanism to reduce the parameter space.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b3\": 0.7,\n    \"b17\": 0.8,\n    \"b7\": 0.8,\n    \"b18\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b4\": 0.6,\n    \"b19\": 0.7\n  }\n}\n```"], "5550446645ce0a409eb4d54a": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of target-dependent sentiment classification on Twitter, proposing an Adaptive Recursive Neural Network (AdaRNN) to integrate target information into the sentiment analysis process. The novel approach employs multiple composition functions and adaptively selects them based on context and syntactic structure to propagate sentiments toward the target node more effectively. This method outperforms baseline models in accuracy and F1-score.\"\n\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b8\": 0.85\n    },\n    \n    \"Indirect Inspiration\": {\n        \"b6\": 0.75,\n        \"b12\": 0.7,\n        \"b14\": 0.65,\n        \"b15\": 0.65\n    },\n    \n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b4\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of target-dependent sentiment classification on Twitter, where users may express different sentiments towards multiple targets within a single tweet. The authors propose the Adaptive Recursive Neural Network (AdaRNN) to effectively propagate sentiments towards specific targets by employing multiple composition functions and adaptively selecting them based on context and linguistic tags.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b1\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.85,\n    \"b12\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.75,\n    \"b14\": 0.75,\n    \"b15\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of target-dependent sentiment classification on Twitter by integrating target information with Recursive Neural Networks (RNN). The primary contribution is the development of an Adaptive Recursive Neural Network (AdaRNN) that employs multiple composition functions and adaptively selects them based on context and linguistic tags to propagate sentiments towards the target. This method improves upon traditional learning-based methods and other baselines, providing better performance without relying on hand-crafted rules.\",\n  \"Direct Inspiration\": [\"b1\", \"b2\", \"b8\"],\n  \"Indirect Inspiration\": [\"b6\", \"b12\", \"b13\"],\n  \"Other Inspiration\": [\"b14\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge in the paper is to accurately classify sentiments in tweets for specific targets, addressing the limitations of traditional methods that do not consider target-specific sentiments. The proposed algorithm, Adaptive Recursive Neural Network (AdaRNN), integrates target information with Recursive Neural Network (RNN) to leverage deep learning models' capabilities. AdaRNN employs a novel adaptive multi-compositionality layer to propagate sentiments of words towards the target based on context and syntactic structure.\",\n  \"Direct Inspiration\": [\"b1\", \"b8\"],\n  \"Indirect Inspiration\": [\"b2\", \"b6\", \"b12\"],\n  \"Other Inspiration\": [\"b0\", \"b14\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of target-dependent sentiment classification on Twitter, where multiple entities can be mentioned within a single tweet, presenting a unique challenge for accurate sentiment analysis. The authors propose an Adaptive Recursive Neural Network (AdaRNN) to integrate target information leveraging deep learning. They introduce a novel adaptive multi-compositionality layer in RNN and employ dependency parsing for sentiment propagation towards the target. The paper demonstrates improved performance over baseline methods using a newly annotated dataset.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b2\": 0.7\n  }\n}\n```"], "5e5e18f793d709897ce40800": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of unstable entity copying and multi-token entity prediction in the CopyRE model for relation extraction. The authors propose CopyMTL, a multi-task learning model with a new architecture for entity copying and a sequence labeling layer to improve performance.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b7\": 0.9,\n    \"b0\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b2\": 0.6,\n    \"b20\": 0.6,\n    \"b16\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of unstable entity copying and handling multi-token entities in the CopyRE model for relation extraction. The authors propose a new model, CopyMTL, which introduces a multi-task learning framework and a new architecture for entity copying to overcome these issues.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b16\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of relation extraction in Knowledge Graph (KG) construction, focusing on the instability of entity copying and the inability to handle multi-token entities in the existing CopyRE model. The proposed solution, CopyMTL, introduces a new model architecture and a multi-task learning framework to improve entity copying and handle multi-token entities effectively.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b19\": 0.95,\n    \"b8\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b0\": 0.75,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b4\": 0.65,\n    \"b10\": 0.5,\n    \"b13\": 0.55,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in relation extraction: the instability of entity copying in CopyRE and the inability to handle multi-token entities. The proposed solution, CopyMTL, introduces a new architecture for entity copying and a multi-task framework to enhance the recognition of multi-token entities. The core contributions include a detailed analysis of CopyRE's shortcomings, a new model architecture to improve entity copying, and a multi-task learning approach to handle multi-token entities, resulting in state-of-the-art performance.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b16\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b0\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in the CopyRE model for relation extraction: instability in entity copying and the inability to handle multi-token entities. The authors propose a novel multi-task learning model, CopyMTL, which incorporates a new architecture for entity copying and a sequence labeling layer to predict multi-token entities.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b0\": 0.6,\n    \"b20\": 0.6,\n    \"b13\": 0.5\n  }\n}\n```"], "5bdc31b417c44a1f58a0bbc4": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of machine reading comprehension (MRC) with unanswerable questions by proposing a unified model called U-Net. The U-Net model decomposes the problem into three sub-tasks\u2014answer pointer, no-answer pointer, and answer verifier\u2014incorporated into a multi-task learning framework. The model introduces a universal node to encode common information from both the question and the passage, enabling end-to-end learning.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of machine reading comprehension (MRC) with unanswerable questions by proposing a unified model called U-Net. This model decomposes the problem into three sub-tasks: answer pointer, no-answer pointer, and answer verifier, and integrates them into a multi-task learning setup. The key innovation is the introduction of a universal node that connects the question and passage, facilitating the representation and interaction of information in an end-to-end fashion.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.85,\n    \"b9\": 0.95,\n    \"b11\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b8\": 0.75,\n    \"b12\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b0\": 0.55,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Machine Reading Comprehension (MRC) with unanswerable questions, proposing the U-Net model which integrates answer pointer, no-answer pointer, and answer verifier into a unified architecture. The model uses a universal node to encode shared information from both question and passage, allowing for end-to-end training and improved performance on the SQuAD 2.0 dataset.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b11\": 0.8,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b8\": 0.6,\n    \"b10\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of machine reading comprehension (MRC) with unanswerable questions. The authors propose a unified model called U-Net that integrates three sub-tasks: answer pointer, no-answer pointer, and answer verifier, using a universal node to improve the model's performance in predicting unanswerability and identifying answer spans.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b3\": 0.9,\n    \"b11\": 0.85,\n    \"b1\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b4\": 0.7,\n    \"b10\": 0.65,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.55,\n    \"b5\": 0.5,\n    \"b12\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on the challenge of machine reading comprehension (MRC) with unanswerable questions. It introduces the U-Net model, which decomposes the problem into three interrelated sub-tasks: answer pointer, no-answer pointer, and answer verifier, all integrated into a unified, end-to-end trainable model. The key innovation is the use of a universal node to encode shared information between the question and passage, improving the model's conciseness and performance.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b4\": 0.7,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```"], "5fd0a7f691e01147f1d1e367": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in neural summarization, specifically focusing on abstractive summarization and introducing a controllable summarization framework (CTRLsum) that uses control tokens (keywords and prompts) to tailor summaries according to user preferences. The novel contribution of the paper is the development of CTRLsum, which offers flexibility in text manipulation without requiring extra annotations or pre-defined control aspects.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b27\": 0.85,\n    \"b40\": 0.8,\n    \"b24\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b14\": 0.7,\n    \"b43\": 0.7,\n    \"b22\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b39\": 0.55,\n    \"b42\": 0.55,\n    \"b34\": 0.55,\n    \"b36\": 0.5,\n    \"b1\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the need for a controllable summarization system that allows users to manipulate the summaries based on their preferences. The proposed algorithm, CTRLsum, introduces control tokens in the form of keywords and descriptive prompts to guide the summarization model. This method enables flexible and user-specific summarization without requiring extra human annotations or pre-defined control aspects.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b27\": 1.0,\n    \"b40\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b14\": 0.8,\n    \"b3\": 0.8,\n    \"b43\": 0.8,\n    \"b1\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed in this paper is the development of a controllable summarization framework that can generate summaries based on user preferences without requiring extensive manual annotations or pre-defined control aspects.\",\n    \"inspirations\": \"The paper is inspired by the need to create summaries that are tailored to user-specific interests and preferences, moving beyond generic summarization models.\"\n  },\n  \"Direct Inspiration\": [\n    \"b6\",\n    \"b27\",\n    \"b40\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b24\",\n    \"b14\",\n    \"b3\",\n    \"b43\"\n  ],\n  \"Other Inspiration\": [\n    \"b1\",\n    \"b36\",\n    \"b22\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in neural summarization systems, specifically focusing on controllable abstractive summarization to cater to user preferences. The proposed solution, CTRLsum, uses control tokens in the form of keywords or descriptive prompts to guide the summarization process without requiring extra human annotations. The system aims to achieve various types of control, such as entity-centric, length-controllable, and purpose-driven summarization, while outperforming existing models in both controlled and uncontrolled settings.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b14\": 0.7,\n    \"b24\": 0.8,\n    \"b40\": 0.75,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b22\": 0.65,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the need for controllable summarization, where the summary content can be manipulated based on user preferences. The proposed algorithm, CTRLsum, uses control tokens (keywords and prompts) to guide the summarization process. This approach allows for flexible text manipulation without the need for extra human annotations or pre-defined control aspects during training.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b27\": 1,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b14\": 0.8,\n    \"b43\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.7,\n    \"b36\": 0.6,\n    \"b1\": 0.6\n  }\n}\n```"], "5ea013bb9fced0a24b9e73f4": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is improving the accuracy of remote sensing image segmentation by combining features expressed in different feature spaces with Riemannian manifold space. The proposed algorithm aims to effectively utilize the characteristics of the Riemannian manifold space to enhance the feature expression and complement traditional spectral space and label field. The novel contribution includes the development of four geodesic-kernel-based manifold projection algorithms.\",\n  \"Direct Inspiration\": [\"b19\", \"b20\"],\n  \"Indirect Inspiration\": [\"b4\", \"b21\", \"b22\"],\n  \"Other Inspiration\": [\"b16\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in remote sensing image segmentation by proposing four geodesic-kernel-based manifold projection algorithms. The main contributions include combining features from different spaces with the Riemannian manifold, demonstrating the effectiveness of Riemannian manifold space for feature expression, and exploring the complementarity among different feature spaces.\",\n    \"Direct Inspiration\": {\n        \"b19\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b16\": 0.7,\n        \"b21\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.5,\n        \"b18\": 0.7,\n        \"b22\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in remote sensing image segmentation, especially in dealing with noise and outliers, and proposes a novel manifold projection algorithm that combines features from Riemannian manifold space, spectral space, and label field to improve segmentation accuracy.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b16\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b10\": 0.6,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately segmenting remote sensing images by leveraging the Riemannian manifold space. The authors propose four geodesic-kernel-based manifold projection algorithms that combine features from spectral space, label field, and Riemannian manifold space to improve segmentation performance, especially in the presence of noise and outliers.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b21\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Difficulty in accessing large training sets for remote sensing images.\",\n      \"Noise and outliers affecting segmentation accuracy.\",\n      \"Inability of pixel-wise segmentation algorithms to handle textures and low-frequency variations.\"\n    ],\n    \"Inspirations\": [\n      \"Combining features expressed in different feature spaces with Riemannian manifold space for high accuracy segmentation.\",\n      \"Exploring the complementarity of features expressed in Riemannian manifold space, spectral space, and label fields.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b18\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b4\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```"], "573695fe6e3b12023e5121fc": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper focus on the instability of deep neural networks to adversarial perturbations and the need for a reliable method to compute these perturbations. The authors propose the DeepFool algorithm as a novel and efficient approach to accurately estimate and compare the robustness of different classifiers against adversarial attacks.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b3\": 1,\n    \"b4\": 0.9,\n    \"b1\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b13\": 0.6,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b20\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial perturbations in deep neural networks, which can cause classifiers to misclassify minimally perturbed data. The authors propose a novel method called DeepFool to accurately compute and compare the robustness of different classifiers against such perturbations. The method is shown to be more reliable and efficient than existing approaches, providing better understanding and robustness enhancement techniques.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.7,\n    \"b4\": 0.7,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b20\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial perturbations in deep neural networks, proposing the DeepFool algorithm to accurately compute and compare the robustness of classifiers. The method is validated through extensive experiments and shown to outperform existing techniques in terms of accuracy and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b1\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b13\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial perturbations in deep neural networks, proposing a novel method called 'DeepFool' to compute and compare the robustness of classifiers to such perturbations. The method is claimed to be more accurate and efficient than existing methods, providing better understanding and robustness insights.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b3\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b20\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adversarial perturbations in deep neural networks. It introduces the DeepFool algorithm, a method to compute adversarial perturbations more accurately and efficiently than existing methods, and demonstrates its effectiveness through extensive experiments. The paper also explores the impact of fine-tuning with adversarial examples on the robustness of networks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b4\": 0.6,\n    \"b1\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.4,\n    \"b20\": 0.4\n  }\n}\n```"], "58437722ac44360f1082f135": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of individuals and communities managing their social connections to protect privacy against network analysis tools. It introduces novel methods for disguising centrality and community structure, leveraging simple heuristics like ROAM (Remove One, Add Many) and DICE (Disconnect Internally, Connect Externally) to achieve these goals.\",\n    \"Direct Inspiration\": {\n        \"b24\": 1.0,\n        \"b17\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b14\": 0.8,\n        \"b22\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.7,\n        \"b13\": 0.65,\n        \"b15\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of protecting individual and community privacy in social networks against network analysis tools. It proposes heuristic algorithms, ROAM for individual disguise and DICE for community concealment, to minimize centrality measures and reduce detectability without compromising influence.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b13\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b14\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of disguising individuals' and communities' centrality in social networks to protect privacy from network analysis tools. It proposes a heuristic method (ROAM) for individuals and another (DICE) for communities to rewire social connections while minimizing centrality measures without compromising influence.\",\n  \"Direct Inspiration\": [\n    \"b24\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b9\",\n    \"b10\",\n    \"b14\"\n  ],\n  \"Other Inspiration\": [\n    \"b13\",\n    \"b15\",\n    \"b16\",\n    \"b17\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are related to protecting individual and community privacy in social networks against network analysis tools, focusing on centrality measures and community detection algorithms. The proposed algorithms aim to disguise the importance of nodes and communities by rewiring social connections, using scalable heuristics like ROAM (Remove One, Add Many) and DICE (Disconnect Internally, Connect Externally). These methods are designed to be simple enough for average users to apply without extensive technical knowledge.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b22\": 0.7,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses privacy concerns in social networks by proposing methods to disguise individuals' centrality and communities' existence to avoid detection by network analysis tools. The main challenges are reducing centrality without compromising influence and concealing communities within a network. The proposed solutions include the ROAM heuristic for individuals and the DICE heuristic for communities.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```"], "5e15adcb3a55ac47ab5b0b8c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of working with heterogeneous graphs in Graph Neural Networks (GNNs), proposing a novel framework called Graph Transformer Networks (GTNs) to learn new graph structures and node representations simultaneously. The primary challenges include handling multiple types of nodes and edges and overcoming the limitations of fixed graph structures in existing GNN methods.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.95,\n    \"b42\": 0.90,\n    \"b15\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.80,\n    \"b32\": 0.75,\n    \"b13\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.65,\n    \"b27\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is transforming heterogeneous graphs into useful meta-path graphs for node representation learning in an end-to-end fashion. The proposed Graph Transformer Network (GTN) aims to learn new graph structures by identifying useful meta-paths and multi-hop connections, addressing the limitations of fixed and homogeneous graph structures in traditional GNNs. The GTN is inspired by Spatial Transformer Networks and aims to generate interpretable graph structures that provide insights into effective meta-paths for prediction.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.9,\n    \"b42\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b32\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the limitations of Graph Neural Networks (GNNs) in handling heterogeneous graphs and proposes a novel framework called Graph Transformer Networks (GTNs) that learns to transform heterogeneous graphs into useful meta-path graphs for each task in an end-to-end fashion. The main challenges are the fixed and homogeneous graph structures assumed by most GNNs, noisy graphs, and the manual design of meta-paths. The proposed GTNs learn new graph structures, identify useful meta-paths, and perform node representation learning effectively.\",\n    \"Direct Inspiration\": {\n        \"b36\": 1.0,\n        \"b42\": 0.9,\n        \"b15\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.7,\n        \"b32\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.5,\n        \"b9\": 0.4,\n        \"b27\": 0.3\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of fixed and homogeneous graph structures in GNNs by proposing Graph Transformer Networks (GTNs) that dynamically learn and transform heterogeneous graphs into useful meta-path graphs for node representation learning. The core contributions are the novel GTN framework, which allows for end-to-end learning of graph structures and node representations, and the interpretable graph generation process.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b18\": 1.0,\n    \"b36\": 1.0,\n    \"b42\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of transforming heterogeneous graphs into useful meta-path graphs for node representation learning in an end-to-end fashion. The proposed Graph Transformer Networks (GTNs) learn to generate new graph structures, identify useful meta-paths, and perform graph convolution on these learned graphs.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b36\": 0.8,\n    \"b42\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b32\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```"], "5fdc8e9d91e01104c91811a8": ["```json\n{\n  \"Summary\": \"The primary challenges are integrating NLP transformers' attention mechanisms with graph neural networks (GNNs) to handle the unique graph structures and large node sizes in graph datasets. The proposed algorithm, Graph Transformer, incorporates sparsity and positional encodings, leveraging Laplacian eigenvectors for node positions.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b27\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b19\": 0.75,\n    \"b26\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b20\": 0.7,\n    \"b36\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the challenge of developing a Graph Transformer that adapts the attention mechanisms of NLP Transformers to graph structures, emphasizing sparsity and positional encodings. Key contributions include the introduction of Laplacian eigenvectors for positional encoding and a Graph Transformer architecture that incorporates edge features.\",\n  \"Direct Inspiration\": [\n    \"b27\",\n    \"b8\",\n    \"b2\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b15\",\n    \"b30\",\n    \"b36\",\n    \"b19\"\n  ],\n  \"Other Inspiration\": [\n    \"b20\",\n    \"b9\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the integration of NLP transformers and GNNs, proposing an improved Graph Transformer model that leverages sparsity and positional encodings. The primary challenges include capturing global information while maintaining local contexts and effectively using positional encodings in graphs.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b15\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b37\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the integration of NLP transformer models and Graph Neural Networks (GNNs) to propose an improved version of Graph Transformer. The primary challenges are efficiently handling graph sparsity and incorporating positional encodings. The algorithm leverages ideas from both NLP transformers and GNNs, particularly focusing on the use of Laplacian eigenvectors for positional encoding and edge feature utilization in graphs.\",\n  \"Direct Inspiration\": [\"b27\", \"b36\", \"b2\"],\n  \"Indirect Inspiration\": [\"b1\", \"b8\", \"b15\", \"b20\"],\n  \"Other Inspiration\": [\"b34\", \"b30\", \"b37\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting transformer architectures, successful in NLP, to graph datasets. The authors propose a Graph Transformer model that leverages sparsity and positional encodings, using Laplacian eigenvectors for node positional features, to improve performance on graph datasets.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b8\": 0.9,\n    \"b2\": 0.9,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b20\": 0.7,\n    \"b30\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b34\": 0.6,\n    \"b37\": 0.6,\n    \"b16\": 0.5\n  }\n}\n```"], "599c7987601a182cd2648373": ["```json\n{\n  \"Summary\": \"The paper introduces the Transformer model, which addresses the challenge of parallelizing computation in sequence transduction tasks by relying entirely on an attention mechanism, thus avoiding the sequential nature of recurrent neural networks. This model achieves significant improvements in computational efficiency and translation quality.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b6\": 0.9,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b14\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b27\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper proposes the Transformer model, which replaces recurrent neural networks with self-attention mechanisms to improve parallelization and computational efficiency in sequence transduction tasks. The primary challenge addressed is the limitation of sequential computation in recurrent models, which the Transformer overcomes by using self-attention to draw global dependencies.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b7\": 0.8,\n        \"b14\": 0.8,\n        \"b30\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.6,\n        \"b6\": 0.6,\n        \"b28\": 0.6,\n        \"b4\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.5,\n        \"b21\": 0.5,\n        \"b3\": 0.5,\n        \"b22\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes the Transformer model architecture, which relies entirely on an attention mechanism to draw global dependencies between input and output, allowing for significantly more parallelization and achieving state-of-the-art translation quality. The primary challenge addressed is the limitation of sequential computation in recurrent models, which the Transformer overcomes by using self-attention mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.85,\n    \"b19\": 0.8,\n    \"b21\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b28\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper outlines challenges in recurrent neural networks, particularly their sequential nature which impedes parallelization. The proposed Transformer model eschews recurrence and relies entirely on attention mechanisms to address these challenges, allowing for more parallelization and improved performance in tasks like translation.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b7\": 0.9,\n    \"b14\": 0.9,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b13\": 0.6,\n    \"b19\": 0.6,\n    \"b22\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b4\": 0.5,\n    \"b8\": 0.4,\n    \"b9\": 0.4,\n    \"b12\": 0.4,\n    \"b15\": 0.4,\n    \"b18\": 0.4,\n    \"b21\": 0.4,\n    \"b23\": 0.4,\n    \"b27\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing sequential computation in neural networks, especially for sequence modeling and transduction tasks. The authors propose the Transformer model, which relies entirely on attention mechanisms instead of recurrence, enabling significant parallelization and improved performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b7\": 1.0,\n    \"b14\": 1.0,\n    \"b19\": 1.0,\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b13\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b27\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```"], "5f7fdd328de39f0828397c88": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of determining the capacity of message-passing neural networks (MPNN) to distinguish between different graphs. It introduces the concepts of communication capacity and communication complexity to study the information exchange capabilities of MPNNs. The paper provides theoretical and empirical evidence on the relationship between the network's depth, width, message size, and the cut-structure of the input graph, and their impact on graph isomorphism classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b7\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b11\": 0.6,\n    \"b16\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of determining the conditions under which graph neural networks, particularly message-passing neural networks (MPNNs), can distinguish between different graphs. It proposes the concept of communication capacity as a measure to evaluate how much information can be exchanged between nodes during the forward pass of an MPNN. The paper derives theoretical bounds on the communication capacity necessary for distinguishing various types of graphs and provides empirical evidence supporting these theoretical predictions.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b32\": 0.7,\n    \"b28\": 0.6,\n    \"b3\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.4,\n    \"b6\": 0.3,\n    \"b9\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of determining the conditions under which current graph neural networks, specifically message-passing neural networks (MPNN), can distinguish between different graphs. It introduces the concept of communication capacity, which measures the amount of information that nodes can exchange during the forward pass. The paper derives hardness results for determining the isomorphism class of connected graphs and trees, showing that the communication capacity of MPNN needs to grow at least linearly with the number of nodes for trees, and quadratically for connected graphs.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b28\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b2\": 0.6,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of determining the power of message-passing neural networks (MPNN) in distinguishing between different graphs. The authors propose a novel metric called communication capacity, which measures the amount of information nodes can exchange during the forward pass. They derive theoretical bounds and perform empirical studies to validate their findings.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b32\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.85,\n    \"b3\": 0.85,\n    \"b7\": 0.8,\n    \"b8\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.75,\n    \"b29\": 0.75,\n    \"b30\": 0.75,\n    \"b31\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of understanding the capacity of message-passing neural networks (MPNN) to distinguish between different graphs. It introduces the concepts of communication capacity and communication complexity to analyze the information exchange during the forward pass of MPNNs, deriving theoretical results on the minimal capacity required for graph isomorphism tasks.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b2\": 0.9,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b6\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b5\": 0.5\n  }\n}\n```"], "58d82fcbd649053542fd66af": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing and expressing convolutional neural networks (CNNs) with identity parameterizations. The authors propose a theoretical framework for deep linear and non-linear residual networks, proving that identity parameterizations simplify optimization and enhance expressivity. They experimentally validate their findings with a simplified all-convolutional residual architecture achieving competitive results on CIFAR10, CIFAR100, and ImageNet datasets.\",\n  \"Direct Inspiration\": [\"b5\", \"b6\", \"b14\"],\n  \"Indirect Inspiration\": [\"b8\", \"b11\", \"b13\"],\n  \"Other Inspiration\": [\"b7\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in optimizing deep convolutional neural networks, particularly focusing on identity parameterizations in residual networks. The authors propose novel theoretical insights and simplified architectures for better optimization and representation, implementing these ideas into experiments with promising results.\",\n  \"Direct Inspiration\": [\"b5\", \"b6\"],\n  \"Indirect Inspiration\": [\"b8\", \"b11\"],\n  \"Other Inspiration\": [\"b7\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in optimizing and representing deep neural networks, specifically through identity parameterizations inspired by residual networks. The authors propose theoretical insights and a simplified architecture for convolutional networks that achieve competitive performance on image classification benchmarks.\",\n  \"Direct Inspiration\": [\"b5\"],\n  \"Indirect Inspiration\": [\"b6\", \"b7\", \"b11\"],\n  \"Other Inspiration\": [\"b8\", \"b14\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of optimizing deep neural networks, particularly focusing on identity parameterizations within residual networks. The paper theoretically explores linear residual networks and extends insights to non-linear cases, providing significant contributions to understanding optimization landscapes and expressivity of these networks.\",\n    \"Direct Inspiration\": [\"b5\", \"b6\"],\n    \"Indirect Inspiration\": [\"b8\", \"b11\", \"b14\"],\n    \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of optimizing deep neural networks, particularly focusing on identity parameterizations and residual networks. The authors propose theoretical insights and experimental validations to show how identity parameterizations can simplify optimization and enhance representation in deep linear and non-linear networks. They also introduce a simpler yet competitive all-convolutional residual network architecture for image classification benchmarks like CIFAR10, CIFAR100, and ImageNet.\",\n  \"Direct Inspiration\": [\"b5\", \"b6\"],\n  \"Indirect Inspiration\": [\"b8\", \"b11\"],\n  \"Other Inspiration\": [\"b7\", \"b14\"]\n}\n```"], "5e09cab43a55ac662f721ac6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of performing category-level pose estimation for articulated objects using a learning-based approach. The primary contributions include the introduction of a novel representation called Articulation-aware Normalized Coordinate Space Hierarchy (ANCSH), a neural network based on PointNet++ to predict this representation, and a combined optimization scheme leveraging joint constraints for part pose and scale estimation.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.5,\n    \"b17\": 0.5,\n    \"b8\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of category-level pose estimation for articulated objects, emphasizing the need for a shared representation that generalizes across instances within a category, handling the high degree of freedom in poses, and effectively leveraging joint constraints. The proposed solution includes the Articulation-aware Normalized Coordinate Space Hierarchy (ANCSH), a PointNet++ based neural network for predicting ANCSH, a combined optimization scheme for part pose and scale estimation, and a two-step approach for high-accuracy joint parameter estimation.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b8\": 0.7,\n    \"b17\": 0.7,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of category-level pose estimation for articulated objects from a single depth image, focusing on per-part 6D poses, 3D scales, joint parameters, and joint states. It introduces a novel representation called Articulation-aware Normalized Coordinate Space Hierarchy (ANCSH) and proposes a PointNet++ based neural network to predict this representation. The approach leverages joint constraints for accurate pose estimation and employs a two-step process for high-accuracy joint parameter estimation.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b11\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b17\": 0.6,\n    \"b8\": 0.6,\n    \"b0\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of category-level pose estimation for articulated objects without relying on exact 3D CAD models. It introduces a novel representation called Articulation-aware Normalized Coordinate Space Hierarchy (ANCSH) to estimate per-part 6D poses, 3D scales, joint parameters, and joint states from a single depth image. Key contributions include a new category-level representation, a PointNet++ based neural network, a combined optimization scheme leveraging joint constraints, and a two-step approach for high-accuracy joint parameter estimation.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b12\": 0.5,\n    \"b13\": 0.5,\n    \"b11\": 0.5,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of category-level pose estimation for articulated objects from a single depth image. It introduces the Articulation-aware Normalized Coordinate Space Hierarchy (ANCSH) to represent articulated objects at both object and part levels. The paper leverages a neural network to predict ANCSH representations and uses a combined optimization scheme to ensure joint constraints are met for part pose and scale estimation.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b11\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```"], "5bdc315017c44a1f58a05c5e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of building reliable named entity recognition (NER) models without handcrafting features and the need for large amounts of manually annotated data for training. It proposes two neural architectures, Fuzzy-LSTM-CRF and AutoNER, to handle multi-label tokens and noisy distant supervision. The primary contributions include the AutoNER model with the Tie or Break scheme, the Fuzzy-LSTM-CRF model, and techniques to refine distant supervision for better NER performance.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b13\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b4\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of building reliable named entity recognition (NER) models without requiring large amounts of manually annotated sentences. The authors propose two novel neural architectures: the Fuzzy-LSTM-CRF model and AutoNER. The Fuzzy-LSTM-CRF model uses a modified IOBES scheme to handle multi-typed and unknown-typed tokens, while AutoNER employs a new Tie or Break tagging scheme to better exploit the noisy distant supervision. The main contributions include the introduction of AutoNER, the revision of the traditional NER model to the Fuzzy-LSTM-CRF model, and the refinement of distant supervision to improve NER performance.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b13\": 0.9,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b22\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of building reliable named entity recognition (NER) models without extensive manual annotation. It proposes AutoNER, a novel neural model with a new 'Tie or Break' tagging scheme, and revises the traditional NER model to a Fuzzy-LSTM-CRF model. The key contributions include refining distant supervision, incorporating high-quality phrases, and demonstrating improved performance on benchmark datasets without additional human effort.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b19\": 0.9,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b7\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of building reliable named entity recognition (NER) models without extensive manual annotation, particularly in domain-specific settings where expert annotation is expensive. The authors propose two neural architectures, Fuzzy-LSTM-CRF and AutoNER, to tackle issues such as multi-label tokens and noisy distant supervision. The novel contributions include the introduction of a Fuzzy CRF layer for handling multi-label tokens and a new Tie or Break tagging scheme for more robust entity detection.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b13\": 1.0,\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b7\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b5\": 0.7,\n    \"b15\": 0.7,\n    \"b22\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of building reliable named entity recognition (NER) models without extensive manual annotation, particularly in domain-specific settings where expert annotation is costly. It proposes AutoNER, a novel neural model with the 'Tie or Break' scheme, and revises traditional models to the Fuzzy-LSTM-CRF model. The paper focuses on refining distant supervision to reduce false negatives and enhance NER performance.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b13\": 0.85,\n    \"b14\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b7\": 0.75,\n    \"b19\": 0.75,\n    \"b22\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b5\": 0.7,\n    \"b15\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```"], "5bbacb9e17c44aecc4eaff64": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying named entity recognition (NER) to languages with limited labeled data by proposing a novel unsupervised cross-lingual transfer method. This approach combines dictionary-based and embedding-based lexical mapping methods and incorporates an order-invariant self-attention mechanism to handle word order differences.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b32\": 0.9,\n    \"b44\": 0.85,\n    \"b25\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.75,\n    \"b50\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of unsupervised cross-lingual Named Entity Recognition (NER) with limited labeled data in the target language. The main challenges are effective lexical mapping between languages and handling word order differences. The authors propose a hybrid approach combining dictionary-based and embedding-based methods for lexical mapping and introduce an order-invariant self-attention mechanism in the neural architecture to address word order differences.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b32\": 0.9,\n    \"b44\": 0.9,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b23\": 0.7,\n    \"b38\": 0.6,\n    \"b50\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b19\": 0.5,\n    \"b7\": 0.5,\n    \"b22\": 0.5,\n    \"b46\": 0.5,\n    \"b27\": 0.5,\n    \"b35\": 0.5,\n    \"b26\": 0.5,\n    \"b36\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performing unsupervised cross-lingual NER with limited annotated data, focusing on lexical mapping and word order differences. A novel approach combining dictionary-based and embedding-based methods for lexical mapping and an order-invariant self-attention mechanism for word order differences is proposed.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.95,\n    \"b32\": 0.9,\n    \"b44\": 0.85,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b38\": 0.7,\n    \"b23\": 0.65,\n    \"b50\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.55,\n    \"b46\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of cross-lingual named entity recognition (NER), specifically focusing on unsupervised transfer where no labeled data is available in the target language. The main challenges include effective lexical mapping between languages and addressing word order differences. The authors propose a novel approach that combines dictionary-based and embedding-based methods for lexical mapping and incorporate an order-invariant self-attention mechanism to handle word order differences.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b32\": 1,\n    \"b44\": 1,\n    \"b25\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b38\": 0.9,\n    \"b23\": 0.8,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b50\": 0.6,\n    \"b3\": 0.5,\n    \"b15\": 0.5,\n    \"b17\": 0.4,\n    \"b14\": 0.4,\n    \"b1\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performing Named Entity Recognition (NER) in low-resource languages through unsupervised cross-lingual transfer. The primary challenges are lexical mapping between languages and word order differences. The proposed method combines dictionary-based and embedding-based approaches for lexical mapping and incorporates an order-invariant self-attention mechanism to handle word order differences.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.9,\n    \"b2\": 0.8,\n    \"b50\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.7,\n    \"b46\": 0.7,\n    \"b22\": 0.7\n  }\n}\n```"], "5c8ddce94895d9cbc6a97820": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of Named Entity Recognition (NER) in Chinese by proposing a lattice LSTM-CRF model that incorporates latent word information into character-based LSTM-CRF models. This approach aims to overcome the limitations of character-based methods, which do not exploit explicit word and word sequence information, and word-based methods, which suffer from segmentation errors.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b27\": 1,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b24\": 0.8,\n    \"b20\": 0.8,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Named Entity Recognition (NER) in Chinese by overcoming issues related to word segmentation errors and effectively integrating character and word information. The proposed approach involves a novel lattice LSTM-CRF model that leverages latent word information in a character-based framework to improve NER performance across various datasets.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b27\": 0.9,\n    \"b5\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b24\": 0.7,\n    \"b20\": 0.7,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b10\": 0.5,\n    \"b9\": 0.5,\n    \"b47\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Named Entity Recognition (NER) for the Chinese language. Traditional sequence labeling methods face issues like error propagation in the segmentation\u2192NER pipeline and underutilization of explicit word information in character-based models. The proposed solution integrates latent word information into a character-based LSTM-CRF model using a lattice structure. This approach constructs a word-character lattice from a large lexicon, leveraging lattice LSTM to control information flow dynamically. The results show significant improvements over both character and word sequence labeling models using LSTM-CRF.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b24\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b21\": 0.7,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the effective recognition of named entities in Chinese text without relying on word segmentation, which can introduce errors. The proposed solution is a lattice LSTM-CRF model that integrates latent word information into character-based NER using a lattice structure to represent lexicon words from the sentence. This approach aims to leverage both character-level and word-level information to improve NER performance across different domains.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b27\": 1,\n    \"b21\": 0.9,\n    \"b13\": 0.8,\n    \"b24\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b16\": 0.7,\n    \"b23\": 0.7,\n    \"b35\": 0.7,\n    \"b4\": 0.7,\n    \"b14\": 0.7,\n    \"b6\": 0.6,\n    \"b9\": 0.6,\n    \"b53\": 0.6,\n    \"b31\": 0.6,\n    \"b29\": 0.6,\n    \"b15\": 0.6,\n    \"b26\": 0.6,\n    \"b37\": 0.6,\n    \"b34\": 0.6,\n    \"b51\": 0.6,\n    \"b42\": 0.6,\n    \"b41\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.5,\n    \"b46\": 0.5,\n    \"b40\": 0.5,\n    \"b38\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of named entity recognition (NER) in Chinese text, particularly focusing on the integration of latent word information into character-based LSTM-CRF models using a lattice structure. This approach aims to overcome the limitations of both word-based and character-based methods while avoiding segmentation errors that can propagate in a pipeline approach.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b27\": 0.9,\n    \"b5\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b24\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b29\": 0.6,\n    \"b15\": 0.6,\n    \"b26\": 0.6,\n    \"b37\": 0.6,\n    \"b34\": 0.6,\n    \"b51\": 0.6\n  }\n}\n```"], "573696106e3b12023e5227c8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of Named Entity Recognition (NER) with limited supervised training data and no language-specific resources or features. The proposed solutions include neural architectures that incorporate a bidirectional LSTM with a sequential conditional random field layer (LSTM-CRF) and a new model inspired by transition-based parsing with stack LSTMs (S-LSTM). Key inspirations include methods for capturing orthographic and distributional evidence through character-based word representation and distributional representations.\",\n  \"Direct Inspiration\": [\"b7\", \"b11\", \"b19\"],\n  \"Indirect Inspiration\": [\"b25\", \"b28\", \"b29\", \"b33\"],\n  \"Other Inspiration\": [\"b10\", \"b16\", \"b22\", \"b23\", \"b36\", \"b38\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of Named Entity Recognition (NER), particularly the limited availability of supervised training data and the difficulty of generalizing from small samples. The authors propose neural architectures that avoid language-specific resources and features by leveraging unsupervised learning from unannotated corpora. The key models proposed include a bidirectional LSTM with a sequential conditional random field layer (LSTM-CRF) and a new model inspired by transition-based parsing with states represented by stack LSTMs (S-LSTM). The paper demonstrates state-of-the-art performance in multiple languages without hand-engineered features or gazetteers.\",\n    \"Direct Inspiration\": {\n        \"b11\": 1,\n        \"b25\": 1,\n        \"b28\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b19\": 0.7,\n        \"b22\": 0.8,\n        \"b29\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.6,\n        \"b16\": 0.65,\n        \"b33\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in Named Entity Recognition (NER) by proposing neural architectures that require no language-specific resources or features beyond a small amount of supervised training data and unlabeled corpora. The main contributions include the use of bidirectional LSTM with a sequential conditional random layer (LSTM-CRF) and a new model inspired by transition-based parsing with states represented by stack LSTMs (S-LSTM). The paper aims to capture orthographic and distributional evidence for identifying names, combining character-based word representation models with distributional representations.\",\n    \"Direct Inspiration\": [\"b11\", \"b25\"],\n    \"Indirect Inspiration\": [\"b7\", \"b16\", \"b22\", \"b28\"],\n    \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of Named Entity Recognition (NER) with limited supervised training data and language-specific resources. It proposes two neural architectures (LSTM-CRF and S-LSTM) that leverage both orthographic and distributional evidence from a small amount of supervised training data and unlabeled corpora to achieve state-of-the-art performance. The main inspirations include the use of bidirectional LSTMs, CRFs, and transition-based parsing algorithms.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b19\": 1.0,\n    \"b11\": 1.0,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b25\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.7,\n    \"b38\": 0.7,\n    \"b33\": 0.6,\n    \"b10\": 0.6,\n    \"b22\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Named Entity Recognition (NER) with limited supervised training data and no language-specific resources. The authors propose neural architectures that utilize bidirectional LSTM-CRF and a novel transition-based chunking model using stack LSTMs. They focus on capturing orthographic and distributional evidence for identifying names and achieve state-of-the-art performance in multiple languages without hand-engineered features.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b25\": 0.9,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b22\": 0.5,\n    \"b29\": 0.4\n  }\n}\n```"], "5d9edc1647c8f76646032985": ["```json\n{\n  \"Summary\": \"The paper addresses limitations in existing neural network models for Named Entity Recognition (NER), particularly those using simple feed-forward networks and solely relying on word embeddings. The authors propose a hybrid model combining bi-directional Long Short-Term Memory (BLSTM) networks and Convolutional Neural Networks (CNNs) to leverage both character- and word-level features. Additionally, they introduce a new lexicon encoding scheme for better partial matching.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b13\": 0.9,\n    \"b32\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b14\": 0.7,\n    \"b2\": 0.7,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.5,\n    \"b30\": 0.5,\n    \"b25\": 0.5,\n    \"b1\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of limited context and lack of character-level feature exploitation in existing neural network models for Named Entity Recognition (NER). The authors propose a hybrid model combining bi-directional Long Short-Term Memory (BLSTM) networks and Convolutional Neural Networks (CNNs) to learn both character- and word-level features. They also introduce a new lexicon encoding scheme and matching algorithm to improve NER performance.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b32\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b3\": 0.7,\n    \"b22\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses limitations of a previously proposed neural network model for named entity recognition (NER) by introducing a hybrid model that combines bi-directional long-short term memory (BLSTM) networks and convolutional neural networks (CNNs). The primary challenges are the restricted context usage and the inability to exploit character-level features in the previous model. The proposed solution enhances context utilization and incorporates character-level features, establishing a new state of the art in NER performance.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b18\": 0.8,\n    \"b11\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b3\": 0.6,\n    \"b22\": 0.6,\n    \"b14\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of limited context and the lack of character-level feature exploitation in neural network models for named entity recognition (NER). It proposes a hybrid model combining bi-directional long-short term memory (BLSTM) and convolutional neural networks (CNNs) to learn both character and word-level features. The model is evaluated on English NER datasets and establishes a new state of the art. Additionally, the paper introduces a new lexicon encoding scheme to improve NER performance.\",\n  \"Direct Inspiration\": [\n    \"b8\",\n    \"b13\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b32\",\n    \"b18\"\n  ],\n  \"Other Inspiration\": [\n    \"b12\",\n    \"b11\",\n    \"b14\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges addressed in the paper include the limitations of previous neural network models for named entity recognition (NER), such as the use of simple feed-forward neural networks that restrict the use of context to a fixed-sized window and the inability to exploit explicit character-level features. The proposed solution is a hybrid model combining bi-directional LSTMs and CNNs to learn both character- and word-level features. The paper introduces a new lexicon encoding scheme and matching algorithm, establishing a new state-of-the-art performance on English NER datasets.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b13\": 0.9,\n    \"b32\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b30\": 0.7,\n    \"b25\": 0.7,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.5,\n    \"b3\": 0.5,\n    \"b22\": 0.5,\n    \"b14\": 0.4,\n    \"b20\": 0.4,\n    \"b2\": 0.4,\n    \"b26\": 0.4\n  }\n}\n```"], "5d4d46fb3a55acff992fdbc4": ["```json\n{\n  \"Summary\": \"The primary challenges in the paper involve the difficulty in Chinese NER due to uncertain boundaries and conflicts among potential words. The algorithm proposed by the authors leverages a CNN with a rethinking mechanism to process characters and potential words in parallel, addressing these challenges through high-level semantic refinement. The novel contributions include a CNN structure for parallel processing, a rethinking mechanism to resolve word conflicts, and improved performance and efficiency over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.95,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of Chinese Named Entity Recognition (NER), particularly focusing on uncertain boundaries and complex compositions. The proposed solution is a novel convolutional neural network (CNN) with a rethinking mechanism to process characters and potential words in parallel, and to resolve conflicts among potential words.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b7\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the inefficiencies of RNN-based methods for Chinese NER due to their sequential nature and the conflicts arising from potential words in the lexicon. The authors propose a novel convolutional neural network (CNN) with a rethinking mechanism that processes characters and potential words in parallel and uses high-level features to resolve conflicts between potential words.\",\n  \"Direct Inspiration\": {\n    \"1\": \"b7\",\n    \"2\": \"b5\"\n  },\n  \"Indirect Inspiration\": {\n    \"1\": \"b4\",\n    \"2\": \"b2\"\n  },\n  \"Other Inspiration\": {\n    \"1\": \"b3\",\n    \"2\": \"b6\"\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in Chinese Named Entity Recognition (NER): inefficiency and conflicts in potential word recognition. The proposed solution is a novel convolutional neural network (CNN) with a rethinking mechanism that processes sentences and potential words in parallel, and uses feedback to resolve conflicts. The model aims to improve computational efficiency and accuracy by leveraging high-level semantic features.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges of Chinese Named Entity Recognition (NER) by proposing a novel Convolutional Neural Network (CNN) structure combined with a rethinking mechanism. The main challenges include inefficiencies in RNN-based methods due to lack of GPU parallelism and difficulties in handling conflicts between potential words in the lexicon. The proposed method leverages CNNs for parallel processing and introduces a feedback layer to refine high-level semantic features, tackling word conflicts effectively.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b5\": 1.0,\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"Strubell et al., 2017\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"Passos et al., 2014\": 0.6,\n    \"Chen et al., 2015\": 0.6\n  }\n}\n```"], "58d82fcbd649053542fd5d36": ["```json\n{\n  \"Summary\": \"The paper introduces MOLIERE, a hypothesis generation system leveraging the entire MEDLINE dataset to construct a large knowledge network. The key challenges addressed are the difficulty of researchers in processing vast amounts of biomedical data and generating useful hypotheses. The novel contributions include the use of state-of-the-art tools like ToPMine, FastText, and PLDA+, and the construction of an extensive, generalized knowledge network for discovering non-trivial connections between biomedical concepts.\",\n  \"Direct Inspiration\": {\n    \"b41\": 1,\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b6\": 0.8,\n    \"b24\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.6,\n    \"b28\": 0.6,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the difficulty researchers face in querying and understanding the growing volume of biomedical data. The authors propose MOLIERE, a hypothesis generation system that uses advanced data mining techniques and natural language processing to link biomedical concepts. The system utilizes the entire MEDLINE dataset, employing tools like ToPMine and FastText to generate a large network of public knowledge, which can be queried to produce human-readable text highlighting relationships between concepts.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b12\": 0.9,\n    \"b6\": 0.8,\n    \"b41\": 0.9,\n    \"b43\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b24\": 0.75,\n    \"b26\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b45\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently generating hypotheses from vast biomedical datasets like MEDLINE. It introduces MOLIERE, a novel hypothesis generation system that leverages natural language processing and data mining techniques to construct a large network of biomedical knowledge. The system uses tools such as ToPMine and FastText to identify meaningful connections and generate human-readable text highlighting relationships between biomedical concepts.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b12\": 0.9,\n    \"b24\": 0.8,\n    \"b43\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b45\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b41\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces MOLIERE, a hypothesis generation system that mines the entire MEDLINE dataset using advanced NLP and data mining techniques. The system aims to generate usable research hypotheses by constructing a large network of biomedical knowledge and performing shortest-path queries. The primary challenge addressed is the overwhelming volume of biomedical literature, which makes it difficult for researchers to identify relevant information and generate new hypotheses.\",\n  \"Direct Inspiration\": {\n    \"b41\": 1.0,\n    \"b34\": 0.9,\n    \"b42\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b6\": 0.7,\n    \"b24\": 0.7,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently generating hypotheses from vast biomedical databases like MEDLINE. The authors propose a novel system, MOLIERE, which constructs a large knowledge network utilizing state-of-the-art tools such as ToPMine and FastText. This system is aimed at improving the productivity of researchers by enabling them to query relationships within the entire MEDLINE dataset.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b12\": 0.85,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.75,\n    \"b43\": 0.7,\n    \"b45\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b28\": 0.55\n  }\n}\n```"], "5e9ef9b69fced0a24b1b65a2": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of constructing high-quality topic taxonomies for document collections by integrating text data and network structures. The proposed framework, NetTaxo, introduces a novel instance-level motif selection mechanism to refine the information extracted from heterogeneous text-rich networks, enhancing the quality of taxonomy construction.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b19\": 0.9,\n    \"b32\": 0.85,\n    \"b45\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b12\": 0.75,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.65,\n    \"b14\": 0.6,\n    \"b37\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of constructing high-quality topic taxonomies for document collections by integrating text data with network structures. The proposed algorithm, NetTaxo, incorporates motif patterns and an instance-level motif selection mechanism to effectively use both textual and network data. The key contributions include a novel taxonomy construction framework, an adaptive instance-level motif selection method, and extensive experimental validation demonstrating the superiority of the approach.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b19\": 1.0,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.9,\n    \"b39\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge of the paper is constructing high-quality topic taxonomies for document collections by effectively integrating text and network data. The proposed algorithm, NetTaxo, leverages motif patterns to extract useful features from heterogeneous text-rich networks. Key contributions include a novel instance-level motif selection mechanism, hierarchical embedding, and clustering framework for automatic topic taxonomy construction.\",\n  \"Direct Inspiration\": [\"b45\", \"b6\", \"b19\", \"b32\"],\n  \"Indirect Inspiration\": [\"b3\", \"b12\", \"b17\", \"b39\"],\n  \"Other Inspiration\": [\"b14\", \"b18\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include the effective integration of network and text data for automatic topic taxonomy construction, the selection of useful motif patterns and instances, and the refinement of term embeddings to improve clustering quality. The proposed algorithm, NetTaxo, addresses these challenges through a hierarchical embedding and clustering framework, incorporating text data and network structures, and using a novel instance-level motif selection method.\",\n    \"Direct Inspiration\": {\n        \"b45\": 0.9,\n        \"b32\": 0.8,\n        \"b6\": 0.8,\n        \"b19\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.7,\n        \"b12\": 0.7,\n        \"b17\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b34\": 0.6,\n        \"b31\": 0.6,\n        \"b25\": 0.6,\n        \"b42\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of constructing high-quality topic taxonomies by integrating text data and network structures, proposing the NetTaxo framework which uses motif patterns for effective term embedding and clustering.\",\n    \"Direct Inspiration\": {\n        \"b45\": 1.0,\n        \"b32\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b19\": 0.8,\n        \"b3\": 0.7,\n        \"b17\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.6,\n        \"b31\": 0.6\n    }\n}\n```"], "58437725ac44360f108302aa": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of single image super-resolution (SISR) by proposing an efficient sub-pixel convolution layer that upscales low resolution (LR) images to high resolution (HR) at the last layer of a convolutional neural network (CNN). This approach aims to reduce computational complexity and improve reconstruction accuracy by avoiding upscaling before feature extraction.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b2\": 1,\n    \"b43\": 0.9,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b20\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b23\": 0.6,\n    \"b48\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the recovery of high-resolution (HR) images from low-resolution (LR) counterparts, which is a highly ill-posed problem due to the loss of high-frequency information during downsampling. The paper proposes an efficient sub-pixel convolution layer to perform upscaling at the end of the network, thus improving computational efficiency and reconstruction accuracy.\",\n  \"Direct Inspiration\": [\"b6\", \"b2\", \"b30\"],\n  \"Indirect Inspiration\": [\"b5\", \"b26\", \"b43\"],\n  \"Other Inspiration\": [\"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of super-resolution (SR) in digital image processing, particularly single image super-resolution (SISR). The proposed method introduces a novel network architecture that performs most of the SR operations in low-resolution (LR) space, culminating in a sub-pixel convolution layer that efficiently upscales the image at the final stage. This approach significantly reduces computational and memory complexity while achieving high-quality high-resolution (HR) images and videos in real-time.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b2\": 1.0,\n    \"b30\": 1.0,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b23\": 0.8,\n    \"b26\": 0.8,\n    \"b43\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of super-resolution (SR), specifically single-image super-resolution (SISR), and proposes a novel network architecture to enhance the resolution of low-resolution (LR) images into high-resolution (HR) images. The proposed method employs a more efficient sub-pixel convolution layer to learn the upscaling operation, reducing computational complexity and improving reconstruction accuracy.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b6\", \"b2\", \"b30\", \"b5\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b20\", \"b48\", \"b26\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b43\", \"b15\", \"b29\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of recovering high-resolution (HR) images or videos from low-resolution (LR) counterparts, focusing on Super-Resolution (SR). The proposed method introduces an efficient sub-pixel convolution layer to handle upscaling at the last layer of the network, which reduces computational complexity and improves reconstruction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b2\": 1,\n    \"b30\": 1,\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b48\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b26\": 0.7,\n    \"b43\": 0.7\n  }\n}\n```"], "573695fd6e3b12023e510ff5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of image super-resolution by proposing a deeply-recursive convolutional network (DRCN) that utilizes a large receptive field without increasing the number of parameters. The primary inspirations are the limitations of previous methods, such as SRCNN, and the issues of vanishing/exploding gradients. The authors introduce two key innovations: recursive supervision and skip-connections to address these challenges.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b16\": 0.9,\n    \"b0\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b18\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in image super-resolution (SR) by proposing a deeply-recursive convolutional network (DRCN) to effectively expand the receptive field without increasing model parameters. The primary challenges include managing large receptive fields and addressing exploding/vanishing gradients. The paper introduces recursive-supervision and skip-connection methods to improve training stability and performance.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.5,\n    \"b23\": 0.5,\n    \"b0\": 0.6,\n    \"b20\": 0.6,\n    \"b2\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of image super-resolution (SR) by proposing a deeply recursive convolutional network (DRCN) that increases the receptive field without increasing the number of parameters. The main challenges include overfitting, exploding/vanishing gradients, and difficulty in learning long-range dependencies. To tackle these issues, the authors introduce recursive-supervision and skip-connection methods.\",\n  \"Direct Inspiration\": [\"b4\", \"b16\", \"b15\"],\n  \"Indirect Inspiration\": [\"b2\", \"b17\", \"b27\", \"b28\", \"b18\"],\n  \"Other Inspiration\": [\"b0\", \"b20\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of increasing the receptive field of convolutional networks for image super-resolution without significantly increasing the number of parameters or causing overfitting. The proposed solution is a deeply-recursive convolutional network (DRCN) with recursive-supervision and skip-connections to overcome the issues of vanishing/exploding gradients and ensure efficient training.\",\n  \n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b16\": 0.9,\n    \"b15\": 0.8,\n    \"b2\": 0.8\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b23\": 0.7,\n    \"b28\": 0.6,\n    \"b14\": 0.6\n  },\n  \n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b10\": 0.5,\n    \"b22\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of image super-resolution (SR) by proposing a deeply-recursive convolutional network (DRCN) that increases the receptive field without introducing more parameters. The main challenges include handling the ill-posed nature of SR, overfitting, and training difficulties due to vanishing/exploding gradients. The authors propose two key solutions: recursive-supervision and skip-connection to improve training efficiency and performance.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b15\": 0.9,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b20\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```"], "5ec49a639fced0a24b4de82e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of sentence simplification by proposing an iterative, edit-based unsupervised approach. The main aim is to overcome the limitations of existing Seq2Seq models and previous unsupervised methods. The approach utilizes a scoring function to evaluate candidate sentences based on fluency, simplicity, and meaning preservation, and employs a series of lexical and syntactic operations for simplification.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.8,\n    \"b25\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in sentence simplification: the lack of insight and control in simplification operations in Seq2Seq models, and the requirement for large volumes of aligned data. The proposed method is an iterative, edit-based unsupervised approach that uses a scoring function to generate and select simplified candidate sentences based on fluency, simplicity, and meaning preservation.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b33\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.7,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5,\n    \"b13\": 0.4,\n    \"b9\": 0.35\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of sentence simplification by proposing an iterative, edit-based unsupervised approach. It focuses on generating simplified sentences through lexical simplification, phrase extraction, deletion, and reordering, guided by a scoring function that evaluates fluency, simplicity, and meaning preservation.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.8,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of sentence simplification by proposing an iterative, edit-based unsupervised approach. It aims to overcome the limitations of supervised Seq2Seq models, such as lack of interpretability and the need for large aligned datasets. The proposed method uses a scoring function to evaluate candidate simplified sentences based on fluency, simplicity, and meaning preservation, and iteratively edits sentences using operations like removal, extraction, reordering, and substitution.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b33\": 0.9,\n    \"b25\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.7,\n    \"b39\": 0.7,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b12\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of supervised Seq2Seq models in sentence simplification, such as lack of interpretability, controllability, and the need for large parallel datasets. An unsupervised, iterative, edit-based approach is proposed to simplify sentences by iteratively applying operations like removal, extraction, reordering, and substitution. The method is evaluated on the Newsela and WikiLarge corpora, showing competitive performance with state-of-the-art supervised models while offering better interpretability and controllability.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b33\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.7,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b18\": 0.4,\n    \"b25\": 0.4,\n    \"b20\": 0.3\n  }\n}\n```"], "5db9298647c8f766461f8ed6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of extending graph neural networks (GNNs) to non-Euclidean geometries, specifically hyperbolic spaces, to improve their ability to capture structural properties of graphs. The authors propose a general framework for GNNs on Riemannian manifolds and demonstrate that hyperbolic GNNs (HGNNs) outperform Euclidean counterparts in several tasks, including full-graph classification and predicting properties of synthetic and real-world data.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of supervised learning on entire graphs, proposing the use of graph neural networks (GNNs) operating on Riemannian manifolds, particularly focusing on hyperbolic geometry. The primary challenges include capturing structural properties of graphs and improving representational efficiency and generalization performance. The proposed algorithm extends GNNs to be manifold-agnostic, enabling the comparison of traditional Euclidean GNNs with hyperbolic GNNs operating on the Poincar\u00e9 ball and the Lorentz model.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of supervised learning on entire graphs using Graph Neural Networks (GNNs) with a focus on hyperbolic geometry. The core contribution lies in extending GNNs to operate on Riemannian manifolds, specifically hyperbolic spaces, to improve representational efficiency and generalization performance for full-graph classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b27\": 0.6,\n    \"b17\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of supervised learning on entire graphs using graph neural networks (GNNs). It focuses on incorporating hyperbolic geometry into GNNs to capture structural properties more efficiently. The proposed approach is manifold-agnostic and aims to improve the representation and generalization performance of GNNs for various tasks, including full-graph classification, molecular property prediction, and blockchain transaction graph analysis.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.9,\n    \"b16\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b25\": 0.7,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of supervised learning on entire graphs by extending graph neural networks (GNNs) to operate on Riemannian manifolds, including non-Euclidean geometries such as hyperbolic space. The work is primarily motivated by the need to capture structural properties of graphs more effectively, inspired by prior success in hyperbolic geometry for complex networks. Key contributions include the development of manifold-agnostic GNNs, demonstrating the efficiency of hyperbolic GNNs in various applications, and proposing extensions of standard classification and regression methods to Riemannian manifolds.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b31\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b16\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```"], "5db929e147c8f766461fc024": ["```json\n{\n  \"Summary\": \"The paper addresses the primary challenges of temporal relation (TempRel) extraction in natural language processing, focusing on the limitations of existing datasets and the moderate performance of neural approaches. The authors propose using the MATRES dataset and a long short-term memory (LSTM) system to improve TempRel extraction. The paper also explores various word embedding techniques and introduces a common sense encoder (CSE) to enhance the performance of the LSTM system.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b32\": 0.9,\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b29\": 0.8,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the difficulty and time-consuming nature of TempRel annotation, the small and low-quality datasets available, and the moderate improvements shown by neural approaches. The proposed algorithm by the author involves using a standard LSTM system combined with various word embedding techniques and an updated version of TEMPROB, a temporal common sense knowledge base, to significantly improve TempRel extraction performance over previous state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b32\": 1.0,\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b45\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b12\": 0.7,\n    \"b36\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the difficulty and time consumption of annotating temporal relations (TempRels) in natural language, and the low inter-annotator agreement in existing datasets. The proposed algorithm leverages a standard LSTM system combined with various word embedding techniques and a common sense encoder (CSE) to improve the performance of TempRel extraction. This method significantly outperforms the previous state-of-the-art system, CogCompTime, particularly on the MATRES dataset.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1.0,\n    \"b31\": 1.0,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b17\": 0.8,\n    \"b45\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b36\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Temporal Relation (TempRel) extraction, specifically the difficulties in annotation, small dataset sizes, and low inter-annotator agreements. The authors propose a Long Short-Term Memory (LSTM) system combined with various word embeddings and a common sense encoder (CSE) to significantly improve performance over the state-of-the-art CogCompTime system.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b32\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b36\": 0.6,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the difficulty of temporal relation (TempRel) extraction in natural language processing, particularly due to small and low-quality datasets. The authors propose using a long short-term memory (LSTM) system and various word embedding techniques (word2vec, GloVe, FastText, ELMo, and BERT) to improve performance on the MATRES dataset, significantly outperforming previous state-of-the-art systems like CogCompTime. They also introduce a common sense encoder (CSE) to further enhance the LSTM system by leveraging temporal common sense knowledge from an updated version of TEMPROB.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1.0,\n    \"b32\": 1.0,\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b35\": 0.6,\n    \"b36\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```"], "5e85c28491e0114016e821a6": ["```json\n{\n    \"Summary\": \"The main challenge addressed by the paper is to improve code prediction accuracy in Integrated Development Environments (IDEs) using machine learning, specifically focusing on Transformer models. The paper proposes novel models, TravTrans and TravTrans+, which leverage the syntactic structure of code more effectively than previous sequence-based models and RNNs. The authors aim to enhance the Transformer architecture by incorporating additional tree structure information, which significantly improves code prediction results.\",\n    \"Direct Inspiration\": {\n        \"b35\": 1.0,\n        \"b8\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.9,\n        \"b42\": 0.9,\n        \"b6\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.5,\n        \"b23\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving code prediction accuracy in IDEs using machine learning models, specifically transformers. It proposes several models that incorporate the syntactic structure of code into the transformer architecture to enhance prediction performance.\",\n  \"Direct Inspiration\": [\"b35\", \"b8\"],\n  \"Indirect Inspiration\": [\"b7\", \"b6\", \"b13\", \"b23\"],\n  \"Other Inspiration\": [\"b42\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving code prediction accuracy in IDE autocompletion by leveraging syntactic structure information in addition to token sequences. It introduces novel models based on the Transformer architecture, specifically TravTrans and TravTrans+, which incorporate AST information to enhance the model's performance. The research demonstrates that these models significantly outperform previous state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b35\": 0.9,\n    \"b7\": 0.8,\n    \"b8\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.6,\n    \"b6\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.4,\n    \"b23\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in improving code prediction in IDEs, particularly the accurate ranking of suggestions for next tokens in code completion. The authors propose Transformer-based models that integrate syntactic structure from Abstract Syntax Trees (ASTs) to outperform state-of-the-art RNN and other sequence-based models. The key contributions include the development of TravTrans+ model, which incorporates tree structure information into the Transformer architecture, significantly enhancing prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b8\": 0.9,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b27\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the accuracy of code prediction in integrated development environments (IDEs). The authors propose novel Transformer-based models, TravTrans and TravTrans+, which incorporate the syntactic structure of code through abstract syntax trees (ASTs). Their models significantly outperform previous state-of-the-art models in terms of prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.8,\n    \"b22\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```\n"], "5a260bfb17c44a4ba8a1c61e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of building a very deep network for Single Image Super-Resolution (SISR) with fewer parameters. The proposed Deep Recursive Residual Network (DRRN) achieves better performance while maintaining a compact model by introducing both global and local residual learning and recursive learning of residual units.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.95,\n    \"b12\": 0.95,\n    \"b13\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.70,\n    \"b23\": 0.60\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.50,\n    \"b30\": 0.50\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses Single Image Super-Resolution (SISR) by proposing a novel Deep Recursive Residual Network (DRRN). The main challenges include improving the performance of very deep networks while keeping the model compact. The proposed solution introduces both global and local residual learning and recursive learning of residual units to achieve better performance with fewer parameters.\",\n  \"Direct Inspiration\": [\n    \"b7\",\n    \"b12\",\n    \"b13\",\n    \"b16\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b1\",\n    \"b23\",\n    \"b30\"\n  ],\n  \"Other Inspiration\": [\n    \"b8\",\n    \"b15\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving Single Image Super-Resolution (SISR) by proposing a novel Deep Recursive Residual Network (DRRN). The primary challenges include achieving high-resolution image reconstruction with fewer parameters and overcoming the difficulty of training very deep networks. The proposed DRRN introduces two major algorithmic novelties: Global Residual Learning (GRL) and Local Residual Learning (LRL) to ease training and improve performance, and recursive learning of residual units to keep the model compact.\",\n    \"Direct Inspiration\": {\n        \"b12\": 0.9,\n        \"b13\": 0.9,\n        \"b7\": 0.8,\n        \"b16\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b23\": 0.6,\n        \"b30\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.5,\n        \"b8\": 0.4,\n        \"b15\": 0.4\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge of the paper is improving the performance of Single Image Super-Resolution (SISR) while maintaining a compact model size. The authors propose a novel Deep Recursive Residual Network (DRRN) that introduces global and local residual learning and recursive learning of residual units to achieve superior performance with fewer parameters.\",\n    \"Direct Inspiration\": {\n        \"b12\": 1,\n        \"b13\": 1,\n        \"b16\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.9,\n        \"b7\": 0.9,\n        \"b23\": 0.8,\n        \"b30\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.7,\n        \"b25\": 0.6,\n        \"b26\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the high computational cost and parameter demand of very deep networks for Single Image Super-Resolution (SISR). The novel approach proposed by the authors is the Deep Recursive Residual Network (DRRN), which introduces both global and local residual learning, and recursive learning of residual units to achieve better performance with fewer parameters.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b13\": 1,\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b7\": 0.9,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b30\": 0.7\n  }\n}\n```"], "5b1642388fbcbf6e5a9b54be": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of noisy labeling in distant supervision for relation classification. The authors propose a novel model with two modules: an instance selector and a relation classifier, utilizing reinforcement learning to jointly train these modules and filter out noisy data.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b9\": 0.85,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.75,\n    \"b15\": 0.7,\n    \"b23\": 0.65,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of noisy labeling in distant supervision for relation classification. It proposes a novel model consisting of an instance selector and a relation classifier, leveraging reinforcement learning to improve sentence-level relation prediction.\",\n  \"Direct Inspiration\": [\"b16\", \"b9\", \"b22\"],\n  \"Indirect Inspiration\": [\"b13\", \"b5\", \"b15\", \"b23\", \"b6\", \"b7\"],\n  \"Other Inspiration\": [\"b1\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of noisy labeling in relation classification using distant supervision and proposes a novel model that consists of an instance selector and a relation classifier. The instance selector filters out noisy sentences, and the relation classifier predicts relations at the sentence level, leveraging reinforcement learning techniques to improve the selection process.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b15\": 0.6,\n    \"b23\": 0.6,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of noisy labeling in distant supervision for relation classification. The authors propose a novel model consisting of an instance selector and a relation classifier to handle the limitations of previous methods, particularly focusing on sentence-level prediction and filtering out entirely noisy bags. The instance selector is trained using reinforcement learning, and the relation classifier utilizes a CNN architecture.\",\n  \"Direct Inspiration\": [\"b16\"],\n  \"Indirect Inspiration\": [\"b13\", \"b9\", \"b5\"],\n  \"Other Inspiration\": [\"b22\", \"b6\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of noisy labeling in distant supervision for relation classification. It proposes a novel model consisting of an instance selector and a relation classifier, designed to filter out noisy sentences and achieve sentence-level prediction using reinforcement learning techniques.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```"], "5b3d98b017c44a510f7ffede": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting suicide by hanging in prisons using a real-time intelligent video-based system. It proposes a scaling algorithm, feature selection approach, and oversampling technique to improve detection accuracy and generalization capacity. The primary inspirations include previous works on video surveillance systems, human action recognition, and depth camera usage for detecting behaviors.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.6,\n    \"b6\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b5\": 0.5,\n    \"b7\": 0.5,\n    \"b10\": 0.4,\n    \"b11\": 0.4,\n    \"b12\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces a real-time intelligent system for detecting suicide by hanging in prisons, addressing the challenge of creating a robust and efficient video surveillance system capable of identifying suicidal behaviors under varied and realistic conditions. The proposed method involves the use of depth cameras for 3D spatial information, a scaling algorithm to handle morphological differences, feature selection for real-time application, and oversampling to balance the dataset.\",\n    \"Direct Inspiration\": {\n        \"b9\": 0.9,\n        \"b1\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.7,\n        \"b12\": 0.6,\n        \"b11\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.5,\n        \"b4\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a real-time intelligent system for detecting suicide by hanging in prisons. The primary challenges outlined include creating a realistic dataset, dealing with unbalanced data, and ensuring the system operates effectively under various conditions. The algorithm proposed involves using a depth camera to capture 3D spatial information, extracting pose and motion features, and employing classifiers to detect suicidal behavior. Key innovations include a scaling algorithm for morphological differences, feature selection for real-time application, and oversampling the minority class.\",\n  \"Direct Inspiration\": [\n    \"b9\",\n    \"b1\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b6\",\n    \"b3\",\n    \"b5\",\n    \"b8\",\n    \"b12\",\n    \"b11\"\n  ],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting suicide by hanging in prisons using a real-time intelligent system. It builds on previous work by creating a challenging dataset, using an efficient algorithm to rebalance the dataset, and evaluating several classifiers to achieve high detection accuracy.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b8\": 0.6,\n    \"b0\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a real-time intelligent system for detecting suicide by hanging in prisons. It addresses the challenges of detecting suicidal behavior in real-world conditions by using a depth camera to capture 3D spatial information and proposes a novel scaling algorithm to handle morphological differences. The paper also applies feature selection to speed up the algorithm and improve generalization, evaluates various classifiers, and oversamples the minority class to balance the dataset.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b9\": 0.85,\n    \"b4\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b8\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.60,\n    \"b3\": 0.55,\n    \"b5\": 0.50\n  }\n}\n```"], "5db9299b47c8f766461f9984": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of optimizing multiscale segmentation for high resolution (HR) remote sensing images by proposing a novel unsupervised cross-scale optimization method. The key contributions are the development of a local spectral heterogeneity measure for evaluating intra-object and inter-object heterogeneity and directly searching for optimal objects at different scales to fuse the final segmentation results.\",\n    \"Direct Inspiration\": [\"b20\", \"b48\", \"b5\"],\n    \"Indirect Inspiration\": [\"b10\", \"b21\", \"b31\", \"b49\", \"b12\", \"b17\"],\n    \"Other Inspiration\": [\"b29\", \"b45\", \"b41\", \"b33\", \"b2\", \"b36\", \"b43\", \"b38\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing multiscale segmentation for high-resolution (HR) remote sensing images using an unsupervised method. It introduces a novel local spectral heterogeneity measure to search for optimal segmentation objects at different scales and combines them to achieve better segmentation results. The key contributions are the development of a local spectral heterogeneity measure and a method that directly searches for optimal objects at different scales for fusion.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.95,\n    \"b36\": 0.9,\n    \"b28\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b43\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b46\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Difficulty in selecting appropriate scale parameters for segmentation in HR images.\",\n      \"Need for a method that adapts scale parameters to different land covers.\"\n    ],\n    \"Inspirations\": [\n      \"Development of an unsupervised cross-scale optimization method.\",\n      \"Use of local spectral heterogeneity measure for segmentation optimization.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b36\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b12\": 0.6,\n    \"b17\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high-resolution (HR) image segmentation in Geographic Object-Based Image Analysis (GEOBIA), focusing on selecting optimal scale parameters for segmentation to account for spatial variations. It proposes a novel unsupervised cross-scale optimization method using local spectral heterogeneity to find optimal objects at different scales and combine them for better segmentation results.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b41\": 0.9,\n    \"b28\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.8,\n    \"b0\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b46\": 0.65,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing scale parameters for high-resolution remote sensing image segmentation within the framework of Geographic Object-Based Image Analysis (GEOBIA). It proposes a novel unsupervised cross-scale optimization method that measures local spectral heterogeneity to find optimal segmentation objects at different scales and combines them into a final segmentation result. The key contributions include the development of a local spectral heterogeneity measure and a method to select optimal objects directly at different scales, providing promising segmentation results over various land cover types.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b48\": 0.8,\n    \"b5\": 0.8,\n    \"b43\": 0.7,\n    \"b44\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.6,\n    \"b0\": 0.6,\n    \"b22\": 0.5,\n    \"b46\": 0.5\n  }\n}\n```"], "5e8da0c991e011f2de583820": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the distinguishing power of different classes of MPNNs (Message Passing Neural Networks) and their relationship with the Weisfeiler-Lehman (WL) algorithm. The paper proposes novel insights into how anonymous and degree-aware MPNNs compare with the WL algorithm in terms of distinguishing power.\",\n  \"Direct Inspiration\": [\"b11\", \"b15\"],\n  \"Indirect Inspiration\": [\"b8\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the distinguishing power of anonymous and degree-aware Message Passing Neural Networks (MPNNs) in relation to the Weisfeiler-Lehman (WL) algorithm, proposing various theoretical results and simulations to compare these models.\",\n  \"Direct Inspiration\": [\"b15\", \"b11\"],\n  \"Indirect Inspiration\": [\"b8\"],\n  \"Other Inspiration\": [\"b3\", \"b4\", \"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the distinguishing power of GNNs, the impact of degree information on MPNNs, and the simulation of the WL algorithm by GNNs. The authors propose to explore anonymous and degree-aware MPNNs to address these challenges. The main inspirations are derived from the WL algorithm and various GNN architectures.\",\n  \"Direct Inspiration\": [\"b11\", \"b15\"],\n  \"Indirect Inspiration\": [\"b8\", \"b3\"],\n  \"Other Inspiration\": [\"b4\", \"b6\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the distinguishing power of Graph Neural Networks (GNNs) and their variants, particularly focusing on anonymous and degree-aware Message Passing Neural Networks (MPNNs). The main challenge is to compare the power of these models with the classical Weisfeiler-Lehman (WL) algorithm. The authors propose a framework to analyze the distinguishing power and show that degree-aware MPNNs can achieve a distinguishing power that may be one step ahead of the WL algorithm.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Distinguishing power of GNNs and MPNNs\",\n      \"Comparison to the Weisfeiler-Lehman (WL) algorithm\",\n      \"Degree-aware vs. anonymous MPNNs\"\n    ],\n    \"Inspirations\": [\n      \"Recent studies linking GNNs to WL algorithm\",\n      \"Empirical results suggesting parameter adjustments for GCNs\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b8\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b15\", \"b11\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": []\n  }\n}\n```"], "58d82fced649053542fd729f": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is improving the perceptual quality of single image super-resolution (SISR) results, particularly addressing the loss of high-frequency information and the regression-to-the-mean problem. The proposed solution involves a novel modification of recent texture synthesis networks combined with adversarial training and perceptual losses to produce realistic textures at large magnification ratios.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b48\": 0.85,\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b24\": 0.7,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b37\": 0.6,\n    \"b9\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing and recovering high-resolution (HR) images from low-resolution (LR) counterparts, known as single image super-resolution (SISR). The authors propose a novel approach using a fully convolutional neural network architecture combined with adversarial training and perceptual losses to produce realistic textures at large magnification ratios. The method aims to address the issue of loss of high-frequency information in current state-of-the-art methods that often result in blurry and unnatural images.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b17\": 0.9,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b48\": 0.8,\n    \"b7\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the ill-posed problem of single image super-resolution (SISR), particularly the loss of high-frequency information leading to blurry and unnatural images. The authors propose a novel approach combining recent texture synthesis networks, adversarial training, and perceptual losses to achieve realistic textures at large magnification ratios.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b17\": 0.9,\n    \"b48\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b24\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing and recovering high-resolution images from low-resolution counterparts, known as single image super-resolution (SISR). The primary issue is the loss of high-frequency information, leading to blurry and unnatural images. The authors propose a novel approach using a fully convolutional neural network architecture, combined with texture synthesis networks, adversarial training, and perceptual losses to produce realistic textures at large magnification ratios.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b48\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b7\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the perceptual quality of single image super-resolution (SISR) results, particularly focusing on overcoming the loss of high-frequency information and the unnatural appearance of textures in super-resolved images. The proposed solution involves using a fully convolutional neural network architecture with a novel modification of recent texture synthesis networks, combined with adversarial training and perceptual losses.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b48\": 0.9,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b24\": 0.7,\n    \"b31\": 0.6\n  }\n}\n```"], "5a260c8117c44a4ba8a30ec9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of image restoration, proposing a very deep persistent memory network (MemNet) that introduces a memory block to explicitly mine persistent memory through an adaptive learning process. The key contributions include the development of a memory block with a gating mechanism, the creation of a very deep end-to-end network for image restoration, and demonstrating state-of-the-art performance in image denoising, super-resolution, and JPEG deblocking.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b24\": 1,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b19\": 0.7,\n    \"b26\": 0.7,\n    \"b39\": 0.6,\n    \"b31\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.4,\n    \"b13\": 0.4,\n    \"b22\": 0.3,\n    \"b33\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in image restoration, specifically the lack of long-term memory in deep convolutional neural networks (CNNs). The proposed solution is a very deep persistent memory network (MemNet) which introduces memory blocks to mine persistent memory through an adaptive learning process, improving image denoising, super-resolution, and JPEG deblocking.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b24\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b19\": 0.75,\n    \"b39\": 0.7,\n    \"b26\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b31\": 0.55\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of image restoration, specifically targeting image denoising, single-image super-resolution (SISR), and JPEG deblocking. It proposes a very deep persistent memory network (MemNet) that introduces memory blocks to explicitly mine persistent memory through an adaptive learning process. The network aims to overcome the lack of long-term memory in existing models by using a memory block that contains a recursive unit and a gate unit. This approach is inspired by neuroscience and aims to achieve state-of-the-art performance in various image restoration tasks.\",\n    \"Direct Inspiration\": {\n        \"b5\": 0.95,\n        \"b24\": 0.95\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.85,\n        \"b20\": 0.85,\n        \"b39\": 0.85,\n        \"b26\": 0.80\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.75,\n        \"b3\": 0.70\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of image restoration by proposing a very deep persistent memory network (MemNet) that introduces a memory block to explicitly mine persistent memory through an adaptive learning process. Key inspirations include neuroscience for recursive connections and advancements in convolutional neural networks (CNNs) for image restoration.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b24\": 0.9,\n    \"b19\": 0.8,\n    \"b20\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.7,\n    \"b11\": 0.7,\n    \"b31\": 0.6,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b3\": 0.5,\n    \"b7\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the lack of long-term memory in very deep convolutional neural networks (CNNs) for image restoration tasks such as image denoising, single-image super-resolution (SISR), and JPEG deblocking. The proposed solution is a very deep persistent memory network (MemNet) that introduces memory blocks to explicitly mine persistent memory through an adaptive learning process.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b24\": 1.0,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b19\": 0.6,\n    \"b26\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```"], "5c8d57d74895d9cbc653a9e9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of single image super-resolution (SR), which is an ill-posed inverse problem aiming to recover a high-resolution (HR) image from a low-resolution (LR) image. The proposed method, Deep Back-Projection Networks (DBPN), introduces an end-to-end trainable architecture based on iterative up-and downsampling. The key contributions include an iterative error-correcting feedback mechanism, mutually connected up-and down-sampling stages, and improvements with dense connections inspired by DenseNets.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b19\": 0.8,\n    \"b22\": 0.8,\n    \"b35\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of single image super-resolution (SR), which is an ill-posed inverse problem aiming to recover a high-resolution (HR) image from a low-resolution (LR) image. The authors propose a novel approach called Deep Back-Projection Networks (DBPN), which utilizes iterative up-and downsampling to enhance the SR process. Key contributions include an iterative error-correcting feedback mechanism, mutually connected up-and down-sampling stages, and improved accuracy through dense connections.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b19\": 0.7,\n    \"b22\": 0.7,\n    \"b40\": 0.7,\n    \"b6\": 0.6,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of single image super-resolution (SR) by proposing a novel deep learning architecture called Deep Back-Projection Networks (DBPN). The key contributions include an iterative error-correcting feedback mechanism and mutually connected up-and down-sampling stages that allow for better reconstruction of high-resolution (HR) images from low-resolution (LR) images. The architecture is inspired by iterative back-projection methods and dense connectivity, aiming to improve performance especially for large scaling factors.\",\n  \"Direct Inspiration\": [\n    \"b16\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b14\"\n  ],\n  \"Other Inspiration\": [\n    \"b5\",\n    \"b6\",\n    \"b19\",\n    \"b22\",\n    \"b35\",\n    \"b40\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of single image super-resolution (SR) by proposing an end-to-end trainable architecture based on iterative up-and downsampling: Deep Back-Projection Networks (DBPN). The novel contributions include iterative error-correcting feedback mechanism, mutually connected up-and down-sampling stages, and improvement with dense connection inspired by DenseNets.\",\n  \"Direct Inspiration\": [\"b16\"],\n  \"Indirect Inspiration\": [\"b14\", \"b5\", \"b6\", \"b22\", \"b35\", \"b40\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of super-resolution (SR) by proposing Deep Back-Projection Networks (DBPN). The primary challenges include the ill-posed nature of SR and the limitations of current feed-forward SR networks, especially for large scaling factors. The paper is inspired by the iterative back-projection method and introduces an end-to-end trainable architecture with iterative up-and downsampling to improve SR performance.\",\n    \"Direct Inspiration\": {\n        \"b16\": 1,\n        \"b14\": 0.95\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.85,\n        \"b19\": 0.8,\n        \"b40\": 0.75,\n        \"b22\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.65,\n        \"b35\": 0.6\n    }\n}\n```"], "5aed14e217c44a4438159a0f": ["```json\n{\n    \"Summary\": \"The primary challenge addressed in this paper is the development of an efficient Single Image Super-Resolution (SISR) method that balances high reconstruction performance with low computational cost and memory consumption. The proposed algorithm, Information Distillation Network (IDN), introduces a novel architecture with lightweight parameters and computational complexity. Key components include feature extraction, enhancement, and compression units designed to progressively distill and enhance residual information, achieving competitive results while being faster than many existing methods.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1,\n        \"b11\": 1,\n        \"b12\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.8,\n        \"b21\": 0.8,\n        \"b22\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b14\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of single image super-resolution (SISR), which is inherently ill-posed due to the infinite number of high-resolution (HR) images that can correspond to a given low-resolution (LR) image. The proposed algorithm, Information Distillation Network (IDN), aims to achieve high reconstruction accuracy with lower computational cost and memory consumption. The IDN consists of a feature extraction block, multiple information distillation blocks, and a reconstruction block. Key innovations include the enhancement unit, which gathers and aggregates short-path and long-path features, and the compression unit, which reduces redundant information.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b11\": 1,\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b21\": 0.9,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b20\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of computational cost and memory consumption in Single Image Super-Resolution (SISR) using deep convolutional neural networks (CNNs). The authors propose a novel information distillation network (IDN) that is lightweight and efficient. The IDN uses an enhancement unit and a compression unit to improve representation power and reduce redundancy, respectively. The paper aims to achieve real-time speed while maintaining high reconstruction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b11\": 1.0,\n    \"b12\": 1.0,\n    \"b21\": 1.0,\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b16\": 0.8,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b9\": 0.5,\n    \"b6\": 0.5,\n    \"b19\": 0.5,\n    \"b24\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high computational cost and memory consumption in deep convolutional neural networks (CNNs) used for single image super-resolution (SISR). It proposes a novel Information Distillation Network (IDN) featuring lightweight parameters and computational complexity, which aims to improve the representational power of the network while maintaining high reconstruction accuracy in real-time.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b21\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.6,\n    \"b16\": 0.6,\n    \"b22\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the ill-posed nature of the single image super-resolution (SISR) problem, where multiple high-resolution (HR) images can correspond to the same low-resolution (LR) image. The paper proposes a novel information distillation network (IDN) that aims to address the drawbacks of existing deep convolutional neural network (CNN) based super-resolution methods, such as high computational cost and memory consumption. The IDN is designed with lightweight parameters and computational complexity, featuring a feature extraction block, multiple information distillation blocks, and a reconstruction block. The key component, the information distillation block, contains an enhancement unit and a compression unit to improve the network's representation power and distill useful information, respectively.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.9,\n    \"b12\": 0.8,\n    \"b16\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b3\": 0.7,\n    \"b14\": 0.7,\n    \"b22\": 0.7\n  }\n}\n```"], "5bdc315017c44a1f58a05e7e": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in single-image super-resolution (SISR) including reproducibility issues, inadequate feature utilization, and poor scalability. It proposes a novel multi-scale residual network (MSRN) featuring multi-scale residual blocks (MSRB) for superior performance without relying on complex training tricks.\",\n  \"Direct Inspiration\": [\"b0\", \"b8\", \"b7\"],\n  \"Indirect Inspiration\": [\"b1\", \"b2\", \"b3\", \"b5\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in single-image super-resolution (SISR), specifically the issues of reproducibility, feature utilization, and scalability in existing models. The authors propose a novel multi-scale residual network (MSRN) and multi-scale residual block (MSRB) to improve feature detection and fusion at different scales, and introduce a simple, flexible reconstruction module for efficient upscaling.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b7\": 0.8,\n    \"b8\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b4\": 0.6,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b5\": 0.5,\n    \"b9\": 0.5,\n    \"b11\": 0.5,\n    \"b12\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of reproducing existing SR models, inadequate feature utilization, and poor scalability. It proposes a novel multi-scale residual network (MSRN) and a corresponding multi-scale residual block (MSRB) to enhance image feature extraction and fusion at different scales.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b7\": 0.85,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b2\": 0.7,\n    \"b3\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b11\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses key challenges in single-image super-resolution (SISR), such as reproducibility, feature utilization, and scalability. The authors propose a novel multi-scale residual network (MSRN) and a multi-scale residual block (MSRB) to tackle these issues. The MSRB is designed to detect image features at different scales and achieve feature fusion, while the entire network is structured to easily migrate to different upscaling factors.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.8,\n    \"b7\": 0.9,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b5\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b4\": 0.6,\n    \"b6\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in single-image super-resolution (SISR), particularly the difficulties in reproducing results, inadequate feature utilization, and poor scalability of existing models. The authors propose a novel multi-scale residual network (MSRN) and multi-scale residual block (MSRB) to effectively address these issues by maximizing feature utilization and simplifying model scalability.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b7\": 0.9,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```"], "5a73cbc317c44a0b3035eccf": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of super-resolution (SR) on 'non-ideal' images where the acquisition process is unknown and non-ideal. It introduces a 'Zero-Shot' SR (ZSSR) method that leverages internal image statistics and trains a small image-specific CNN at test time using examples extracted solely from the low-resolution input image itself. This approach allows the method to adapt to different settings per image and outperform state-of-the-art (SotA) supervised methods on real images with unknown and varying acquisition settings.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b14\": 1,\n    \"b23\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b8\": 0.75,\n    \"b9\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the poor performance of state-of-the-art (SotA) super-resolution (SR) methods on non-ideal images. The authors propose a novel 'Zero-Shot' SR (ZSSR) approach that leverages the internal recurrence of information within a single image to train a small image-specific CNN at test time, without relying on any prior image examples or prior training. This method adapts to different settings per image and outperforms externally-trained SotA SR methods on non-ideal images.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b23\": 0.9,\n    \"b12\": 0.8,\n    \"b8\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b3\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve the limitations of state-of-the-art (SotA) supervised super-resolution (SR) methods when dealing with non-ideal, real-world low-resolution (LR) images. These methods typically fail when the images contain noise, compression artifacts, or are generated with non-bicubic downscaling kernels. The authors propose a novel 'Zero-Shot' SR (ZSSR) approach that leverages the internal recurrence of information within a single image, training a small, image-specific CNN at test time without relying on any prior examples or training data. This method outperforms existing SR techniques on non-ideal images.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b23\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b12\": 0.7,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b6\": 0.5,\n    \"b15\": 0.4,\n    \"b1\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of current state-of-the-art (SotA) Super-Resolution (SR) methods that rely on extensive external databases for training. These methods perform poorly on 'non-ideal' low-resolution (LR) images, such as those with unknown downscaling kernels, sensor noise, and compression artifacts. The proposed solution, Zero-Shot SR (ZSSR), leverages the internal recurrence of information within a single image by training a small, image-specific CNN at test time. This method adapts to various settings per image and outperforms externally trained SotA SR methods on 'non-ideal' images.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b14\": 0.8,\n    \"b23\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b3\": 0.7,\n    \"b6\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of state-of-the-art (SotA) supervised super-resolution (SR) methods, which perform poorly on 'non-ideal' images due to their reliance on specific training conditions. The authors propose a novel 'Zero-Shot' SR (ZSSR) method that leverages the internal recurrence of information within a single image, training a small image-specific CNN at test time using only the input image itself. This approach allows ZSSR to adapt to different settings per image and outperform supervised SR methods on real images with unknown acquisition processes.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b23\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.6,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b3\": 0.4,\n    \"b9\": 0.4\n  }\n}\n```"], "5aed14d617c44a4438158e20": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-resolution (HR) images from low-resolution (LR) counterparts by introducing a novel approach called Spatial Feature Transform (SFT). The key challenge is to recover realistic textures faithful to inherent classes (e.g., sky, building, plant) using categorical priors. The proposed method uses semantic segmentation maps as categorical priors to condition the super-resolution (SR) process, enhancing the visual quality of the generated images.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b37\": 0.9,\n    \"b20\": 0.8,\n    \"b2\": 0.8,\n    \"b46\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b21\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b30\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating realistic textures in single image super-resolution (SR) by introducing categorical priors using semantic segmentation maps. The authors propose a novel approach called Spatial Feature Transform (SFT) to incorporate these priors effectively into the SR network. The SFT layer allows for adaptive feature modulation based on spatial conditions, enhancing the realism of the generated textures.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b37\": 1,\n    \"b46\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b20\": 0.8,\n    \"b31\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b21\": 0.6,\n    \"b25\": 0.6,\n    \"b22\": 0.5,\n    \"b42\": 0.5,\n    \"b43\": 0.5,\n    \"b27\": 0.5,\n    \"b47\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of recovering high-resolution images from low-resolution inputs, specifically focusing on the issue of generating realistic textures. The authors propose a novel approach called Spatial Feature Transform (SFT), which uses semantic segmentation probability maps to condition a super-resolution (SR) network. This method aims to generate rich and realistic textures by transforming intermediate features in the network based on categorical priors.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b37\": 1,\n    \"b20\": 0.9,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.8,\n    \"b31\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b21\": 0.6,\n    \"b25\": 0.6,\n    \"b22\": 0.6,\n    \"b42\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating realistic textures in single image super-resolution (SR) by leveraging class-conditional information. It proposes a novel approach called Spatial Feature Transform (SFT) that utilizes semantic segmentation maps as categorical priors to condition the SR network, allowing it to generate more accurate and visually pleasing textures. This method aims to overcome the limitations of existing techniques that struggle with texture recovery due to a lack of strong prior information.\",\n    \"Direct Inspiration\": {\n        \"b26\": 1.0,\n        \"b37\": 0.9,\n        \"b46\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b20\": 0.75,\n        \"b30\": 0.7,\n        \"b31\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b42\": 0.55,\n        \"b43\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is generating realistic textures in single image super-resolution (SISR) while addressing the inherent ill-posed nature of the problem. The proposed algorithm, SFT-GAN, introduces Spatial Feature Transform (SFT) layers to incorporate categorical priors via semantic segmentation probability maps, enhancing the texture recovery of different regions in an image. The approach is parameter-efficient, extensible, and can be integrated into existing SR networks.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b37\": 1,\n    \"b46\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b2\": 0.8,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b42\": 0.5,\n    \"b43\": 0.5\n  }\n}\n```"], "5f7d893591e011346ad27d16": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating content plans into large pre-trained language models to generate more relevant and coherent text. The proposed system, PAIR (Planning And Iterative Refinement), combines a content planning model based on BERT for keyphrase assignment with a seq2seq generation framework using BART, which refines generated text iteratively to enhance quality without requiring model retraining or architectural changes.\",\n  \"Direct Inspiration\": [\"b27\", \"b19\"],\n  \"Indirect Inspiration\": [\"b5\", \"b41\"],\n  \"Other Inspiration\": [\"b22\", \"b35\", \"b44\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper include improving the utility and quality of large pre-trained language models in text generation tasks, by incorporating content planning and iterative refinement strategies. The paper proposes a content-controlled text generation framework built on BERT for content planning and BART for sequence-to-sequence generation, without requiring model architecture modifications. The framework employs iterative refinement to enhance generation quality, making the process more flexible and efficient.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b27\": 1.0,\n    \"b41\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b10\": 0.7,\n    \"b22\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b44\": 0.6,\n    \"b48\": 0.5,\n    \"b52\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"The primary challenges outlined in the paper are the limitations of large pre-trained language models in generation tasks, specifically their inability to produce specified content in a coherent order without requiring model architecture modification or retraining.\",\n        \"inspirations\": \"The paper is inspired by previous approaches in content planning and iterative refinement for text generation, and aims to integrate these into large models without architectural changes.\"\n    },\n    \"Direct Inspiration\": {\n        \"b5\": 1,\n        \"b27\": 1,\n        \"b19\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b41\": 0.9,\n        \"b22\": 0.8,\n        \"b35\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.7,\n        \"b44\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the content generation quality of large pre-trained language models by incorporating content planning and iterative refinement. The proposed PAIR system, built upon BERT for content planning and BART for text generation, aims to produce coherent and relevant multi-sentence texts by fine-tuning without modifying the model architecture. The iterative refinement process further enhances the text quality by masking and regenerating low-confidence tokens.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b27\": 1.0,\n    \"b19\": 1.0,\n    \"b41\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b35\": 0.8,\n    \"b44\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b51\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating content planning into large pre-trained language models to improve the relevance and coherence of text generation. The proposed method, PAIR (Planning And Iterative Refinement), combines a content planner built on BERT to assign keyphrases to sentences and predict their positions, and a seq2seq generation framework built on BART for fine-tuning. An iterative refinement algorithm is also introduced to enhance the generation quality.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b19\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b41\": 0.75,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b31\": 0.55,\n    \"b44\": 0.5\n  }\n}\n```"], "5b8c9f5317c44af36f8b778e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving single image super-resolution (SISR) by enhancing the efficiency and accuracy of deep convolutional neural networks (CNNs). The main contributions include the introduction of wide activation residual networks (WDSR-A and WDSR-B), which expand feature channels before ReLU activation without increasing computational overhead, and the use of weight normalization instead of batch normalization to improve training stability and accuracy.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b24\": 0.9,\n    \"b35\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b13\": 0.7,\n    \"b16\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b37\": 0.6,\n    \"b0\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving accuracy and efficiency in single image super-resolution (SISR) by proposing two novel network architectures, WDSR-A and WDSR-B. These architectures focus on wide activation before ReLU to enhance information flow without additional computational complexity. The authors also introduce weight normalization to improve training for deeper networks.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b41\": 0.8,\n    \"b35\": 0.7,\n    \"b32\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b26\": 0.6,\n    \"b0\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve improving the accuracy and efficiency of single image super-resolution (SISR) networks. The authors propose the use of wider activation in residual networks and introduce two new architectures: WDSR-A and WDSR-B. They also suggest using weight normalization instead of batch normalization for better training performance.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b24\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b32\": 0.8,\n    \"b35\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving single image super-resolution (SISR) by proposing novel network architectures, namely WDSR-A and WDSR-B, which utilize wider activation before ReLU layers and linear low-rank convolutions to enhance performance without additional computational overhead. The work is motivated by the limitations of existing methods that underuse shallow layer features and the drawbacks of batch normalization.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.95,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.75,\n    \"b41\": 0.75,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b26\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of single image super-resolution (SISR) using deep convolutional neural networks (CNNs). The main challenges are improving accuracy and efficiency of the SR networks by leveraging wider activation and efficient convolutions. The paper introduces two novel network architectures, WDSR-A and WDSR-B, which use wider activation and linear low-rank convolution to improve performance without increasing computational complexity. Additionally, the paper suggests that weight normalization is more suitable than batch normalization for training deep SR networks.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.95,\n    \"b24\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.85,\n    \"b32\": 0.80,\n    \"b41\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.75,\n    \"b11\": 0.70,\n    \"b26\": 0.70\n  }\n}\n```"], "599c7b59601a182cd272bf53": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting time series by using a differentiable version of Dynamic Time Warping (DTW) called soft-DTW. The key innovation is that soft-DTW is differentiable, which allows for better optimization in machine learning tasks involving time series. The paper proposes using soft-DTW as a loss function in predictive models and demonstrates its effectiveness in various tasks such as clustering and prediction.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b17\": 1.0,\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b19\": 0.8,\n    \"b21\": 0.8,\n    \"b8\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b28\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of learning mappings for structured output objects, specifically time series, in supervised learning. It proposes the use of a differentiable soft-DTW loss function, which allows for efficient gradient computation and overcomes the non-differentiability issues of traditional DTW. This enables the development of predictive and generative models for time series using neural networks.\",\n    \"Direct Inspiration\": {\n        \"b17\": 0.95,\n        \"b22\": 0.90\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.85,\n        \"b6\": 0.80\n    },\n    \"Other Inspiration\": {\n        \"b24\": 0.75,\n        \"b21\": 0.70,\n        \"b0\": 0.65,\n        \"b19\": 0.60\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of supervised learning for structured output objects, specifically time series data. The authors propose using the soft-DTW loss, a differentiable variant of the Dynamic Time Warping (DTW) discrepancy, to improve time series prediction, averaging, and clustering tasks. The paper introduces a novel algorithm for differentiating soft-DTW and demonstrates its application in various tasks requiring time series output.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b16\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b21\": 0.75,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.65,\n    \"b7\": 0.6,\n    \"b28\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in supervised learning for time series prediction, particularly focusing on the limitations of Dynamic Time Warping (DTW) due to its non-differentiability and instability in optimization. The proposed solution is the soft-DTW, a differentiable variant of DTW, which allows for effective gradient-based optimization. The algorithm's application spans averaging, clustering, and multi-step prediction of time series.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.9,\n    \"b21\": 0.9,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b8\": 0.7,\n    \"b28\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of supervised learning where the output objects are structured as time series. It introduces a novel algorithm called soft-DTW, which is a differentiable modification of the Dynamic Time Warping (DTW) discrepancy. The soft-DTW algorithm allows for a smoother optimization landscape, making it more suitable for tasks such as averaging, clustering, and prediction of time series.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b17\": 1,\n    \"b7\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.9,\n    \"b22\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.8,\n    \"b25\": 0.75,\n    \"b28\": 0.7\n  }\n}\n```"], "5c8d4bf34895d9cbc64e3332": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of efficiently training Graph Convolutional Networks (GCNs) due to the large receptive field size, which grows exponentially with the number of layers. The proposed solution is a novel control variate-based stochastic approximation algorithm that utilizes historical activations of nodes to reduce variance and achieve low time complexity while maintaining model quality.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b14\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b6\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training graph convolution networks (GCNs) efficiently. The authors propose a novel control variate-based stochastic approximation algorithm to reduce the time complexity and variance of stochastic training. The primary inspiration comes from existing methods like neighbor sampling (NS) and importance sampling (IS) which have limitations in terms of convergence and computational cost.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b6\": 0.6,\n    \"b1\": 0.5,\n    \"b11\": 0.5,\n    \"b10\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently training Graph Convolution Networks (GCNs) by reducing the computational complexity and variance of the stochastic gradient. The authors propose a novel control variate-based stochastic approximation algorithm that leverages historical activations to achieve variance reduction, exact predictions at testing time, and convergence guarantees. This approach significantly reduces the time complexity and retains model quality with minimal neighbor sampling.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b6\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training Graph Convolution Networks (GCNs) efficiently, particularly for large-scale datasets. It proposes novel control variate-based stochastic approximation algorithms that significantly reduce the time complexity of stochastic training while maintaining model quality. The paper shows theoretical results on variance reduction, exact predictions at testing time, and convergence guarantees.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b6\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently training Graph Convolutional Networks (GCNs) for large-scale datasets. The key challenge is the exponentially growing receptive field of nodes, which makes exact stochastic gradient computation computationally expensive. The authors propose a novel control variate-based stochastic approximation algorithm utilizing historical activations of nodes as control variates, thereby reducing variance and time complexity while maintaining model accuracy.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b1\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```"], "5a4aef9e17c44a2190f7a8b8": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of enhancing the performance of lightweight object detection models without incurring high computational costs. It introduces the Receptive Field Block (RFB) module, inspired by the human visual cortex, to improve feature representation in lightweight CNNs. The RFB Net detector, combining RFB with SSD, achieves high accuracy and speed.\",\n    \"Direct Inspiration\": {\n        \"b25\": \"0.9\",\n        \"b39\": \"0.85\"\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": \"0.8\",\n        \"b6\": \"0.75\",\n        \"b34\": \"0.7\"\n    },\n    \"Other Inspiration\": {\n        \"b22\": \"0.6\",\n        \"b23\": \"0.6\"\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing the feature representation of lightweight CNN models for fast and accurate object detection. It proposes a novel module, the Receptive Field Block (RFB), inspired by the structure of receptive fields in the human visual system, to simulate the size and eccentricity of RFs and integrate it into the SSD framework to create the RFB Net detector.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b6\": 0.9,\n    \"b34\": 0.9,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.7,\n    \"b14\": 0.6,\n    \"b5\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.4,\n    \"b23\": 0.4,\n    \"b9\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving the performance of high-speed single-stage object detectors without incurring heavy computational costs. The authors propose a novel module, Receptive Field Block (RFB), inspired by the human visual system's receptive fields, to enhance feature representation in lightweight CNN models. The RFB Net detector integrates this module into the SSD architecture to achieve significant performance gains while maintaining fast inference speeds.\",\n    \"Direct Inspiration\": {\n        \"b25\": 0.9,\n        \"b39\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.7,\n        \"b34\": 0.7,\n        \"b6\": 0.65,\n        \"b36\": 0.65,\n        \"b22\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b23\": 0.5,\n        \"b15\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is to enhance the feature representation of lightweight CNN models for object detection without incurring heavy computational costs. The proposed algorithm is the Receptive Field Block (RFB), inspired by the structure of RFs in the human visual system, which strengthens deep features of lightweight CNNs to improve both speed and accuracy in object detection.\",\n    \"Direct Inspiration\": {\n        \"b39\": 1.0,\n        \"b25\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b34\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b14\": 0.7,\n        \"b22\": 0.7,\n        \"b23\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge is to build a fast yet powerful object detector that balances speed and accuracy without relying on very deep neural networks.\",\n    \"Inspirations\": \"Inspired by the human visual system's receptive fields, the proposed Receptive Field Block (RFB) module aims to enhance feature representation in lightweight CNN models.\"\n  },\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b14\": 0.9,\n    \"b25\": 0.95,\n    \"b39\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b34\": 0.85,\n    \"b36\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.75,\n    \"b20\": 0.75\n  }\n}\n```"], "59ae3c262bbe271c4c71e9e8": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the development of an efficient and accurate deep word-level CNN for text categorization. The authors proposed the Deep Pyramid CNN (DPCNN), which features a deep but low-complexity network architecture that reduces computational time while improving accuracy by exploiting long-range associations in text. The main inspirations come from previous work on CNNs and RNNs, particularly those that have examined the balance between depth and computational efficiency.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b7\": 1,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient and effective text categorization using deep neural networks, specifically focusing on deep word-level CNNs. The proposed solution, DPCNN (Deep Pyramid Convolutional Neural Network), aims to maintain computational efficiency while deepening the network to capture long-range associations in text. The novel methods include downsampling without increasing feature maps, shortcut connections with pre-activation, and enhancing text region embedding with unsupervised embeddings.\",\n  \"Direct Inspiration\": [\"b6\", \"b7\", \"b2\"],\n  \"Indirect Inspiration\": [\"b14\", \"b0\"],\n  \"Other Inspiration\": [\"b1\", \"b11\", \"b3\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are achieving effective and efficient text categorization using deep word-level CNNs in the context of large training datasets. The key innovation proposed is the Deep Pyramid CNN (DPCNN), which alternates convolution blocks and downsampling layers to achieve a deep network with computational efficiency. The DPCNN leverages unsupervised embeddings and pre-activation shortcut connections to improve accuracy and training efficiency.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b7\": 1.0,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient and effective text categorization using deep word-level Convolutional Neural Networks (CNNs). The proposed Deep Pyramid CNN (DPCNN) architecture alternates convolution blocks and downsampling layers to construct a deep network that efficiently captures long-range associations in text while maintaining bounded computational complexity.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b2\": 0.9,\n    \"b6\": 0.95,\n    \"b7\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.6,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient and effective text categorization using deep word-level Convolutional Neural Networks (CNNs). It introduces the Deep Pyramid CNN (DPCNN), which alternates convolution blocks and downsampling layers to create a deep network where computation per layer decreases exponentially. The architecture aims to balance depth and computational efficiency, leveraging shortcut connections with pre-activation and enhanced region embedding using unsupervised embeddings.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b6\": 1,\n    \"b7\": 1,\n    \"b14\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b12\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b9\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```"], "599c7945601a182cd2629f72": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of training deep neural networks online for visual object tracking, combining the efficiency of Correlation Filters (CF) with the discriminative power of CNN features. The novel contribution is the integration of CF as a differentiable CNN layer, enabling end-to-end training and showing improved performance, particularly with ultra-lightweight networks.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b12\": 1,\n    \"b20\": 0.9,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b7\": 0.7,\n    \"b14\": 0.7,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b23\": 0.6,\n    \"b22\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training deep neural networks online for visual object tracking with limited data and computational resources. The proposed solution integrates Correlation Filters (CF) with Convolutional Neural Networks (CNN), allowing end-to-end training to leverage the strengths of both methods. The novel contribution is the closed-form expression for the derivative of the CF, facilitating efficient back-propagation through the CF layer in a CNN architecture.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b12\": 0.9,\n    \"b3\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b6\": 0.75,\n    \"b8\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of online training of deep neural networks for object tracking with limited data. It proposes a novel method that integrates Correlation Filters (CF) into a fully-convolutional Siamese network, enabling end-to-end training and improving tracking performance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b20\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training deep neural networks online to capture previously unseen object classes from one or few examples, particularly in visual object tracking. The authors propose integrating Correlation Filters (CF) into deep convolutional neural networks (CNNs) for improved performance, demonstrating a closed-form solution for differentiating CFs and training them end-to-end within a CNN framework.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b30\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training deep neural networks for visual object tracking with limited data and proposes a novel Correlation Filter (CF) layer integrated into a fully-convolutional Siamese network to enable efficient online learning and end-to-end training.\",\n  \"Direct Inspiration\": [\"b2\", \"b12\"],\n  \"Indirect Inspiration\": [\"b20\", \"b6\", \"b8\", \"b3\"],\n  \"Other Inspiration\": [\"b7\", \"b14\", \"b28\"]\n}\n```"], "5f03f3b611dc830562231f99": ["```json\n{\n  \"Summary\": \"The paper addresses three major challenges in recommender systems: sparsity, uncertainty, and lack of diversity. It proposes a novel Bayesian Graph Convolutional Neural Network-based framework (BGCF) that incorporates a random graph generative model based on node-copying to address these challenges. The proposed method enhances recommendation accuracy and diversity while alleviating data sparsity issues.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b22\": 1.0,\n    \"b24\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b32\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b10\": 0.7,\n    \"b14\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three primary challenges in recommender systems: sparsity, uncertainty, and diversity. It proposes a novel Bayesian Graph Convolutional Neural Network (BGNN) framework to enhance recommendation accuracy and diversity by incorporating graph uncertainty and a Bayesian personalized ranking loss.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b24\": 0.9,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b29\": 0.7,\n    \"b32\": 0.7,\n    \"b36\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b10\": 0.5,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three main challenges in recommender systems: sparsity, uncertainty, and diversity of recommendations. It proposes a novel Bayesian Graph Collaborative Filtering (BGCF) framework that incorporates Bayesian Graph Neural Networks (BGNNs) to handle graph uncertainty and enhance diversity in recommendations. The BGNN approach uses a node-copying generative model to create diverse sample graphs, improving the quality of learned representations while addressing data sparsity.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b32\": 0.7,\n    \"b36\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three major challenges in recommender systems: sparsity, uncertainty, and diversity. It proposes a novel Bayesian Graph Neural Network (BGNN) framework to tackle these issues by incorporating a random graph generative model based on node-copying, and uses a Bayesian Personalized Ranking loss for training.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b24\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.75,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.65,\n    \"b29\": 0.6,\n    \"b32\": 0.55,\n    \"b36\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of data sparsity, uncertainty, and lack of diversity in recommendation systems. It proposes a novel Bayesian Graph Collaborative Filtering (BGCF) framework that incorporates Bayesian Graph Neural Networks (BGNNs) to address these issues by modeling the uncertainty in user-item interaction graphs and promoting diversity in recommendations.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.8,\n    \"b21\": 0.8,\n    \"b29\": 0.8,\n    \"b32\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```"], "53e9bb36b7602d970477616b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of applying Flash Translation Layer (FTL) algorithms to embedded applications, focusing on improving storage performance and minimizing RAM memory requirements. It surveys state-of-the-art FTL algorithms and provides performance results based on their implementation.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b13\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b5\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in this paper are the performance degradation caused by the erase-before-write architecture of flash memory and the significant RAM memory requirements for maintaining the logical-to-physical address mapping. The paper focuses on various FTL (Flash Translation Layer) algorithms that address these challenges by optimizing write performance and minimizing erase operations while balancing RAM usage. The study provides a taxonomy of FTL algorithms, discusses their functionalities, and presents performance results from their implementations.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1.0,\n        \"b1\": 0.9,\n        \"b5\": 0.8,\n        \"b8\": 0.8,\n        \"b12\": 0.8,\n        \"b13\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.7,\n        \"b9\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.5,\n        \"b15\": 0.5,\n        \"b7\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are optimizing storage performance and minimizing RAM memory requirements in embedded applications using Flash Translation Layer (FTL) algorithms for flash memory systems. The paper surveys state-of-the-art FTL algorithms, classifies them, and presents performance results based on their implementation.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b1\": 1.0,\n    \"b5\": 1.0,\n    \"b8\": 1.0,\n    \"b12\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b15\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of optimizing Flash Translation Layer (FTL) algorithms for flash memory systems, focusing on improving storage performance and reducing RAM memory requirements. It proposes a taxonomy for FTL algorithms and discusses various mapping techniques including sector mapping, block mapping, and hybrid mapping. The paper aims to minimize erase operations and manage mapping information efficiently.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1,\n        \"b13\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.9,\n        \"b1\": 0.9,\n        \"b5\": 0.8,\n        \"b12\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.7,\n        \"b9\": 0.7,\n        \"b6\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include optimizing the performance of Flash Translation Layer (FTL) algorithms in terms of write performance and RAM memory requirements. The paper discusses various FTL algorithms, classifies them, and provides performance results based on the authors' implementation. The core contributions revolve around addressing the erase-before-write architecture of flash memory and minimizing erase operations to enhance storage performance while reducing RAM memory usage.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b8\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b5\": 0.7,\n    \"b9\": 0.6,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5\n  }\n}\n```"], "5e5e18b693d709897ce29a22": ["```json\n{\n  \"Summary\": \"The paper addresses the problem of neural text degeneration in open-ended natural language applications, characterized by repetitive and dull outputs, often due to flaws in the maximum likelihood training objective. The authors propose a novel unlikelihood training objective to mitigate these issues by decreasing the model's probability of generating certain negative candidates, thereby improving the diversity and quality of generated text.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b26\": 0.6,\n    \"b11\": 0.6,\n    \"b8\": 0.8\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of neural text degeneration in open-ended text generation tasks, focusing on issues like repetition and token distribution mismatch. The authors propose an unlikelihood training objective to improve the model's performance by penalizing certain tokens' probabilities, thereby reducing text degeneration.\",\n    \"Direct Inspiration\": {\n        \"b10\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.7,\n        \"b7\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5,\n        \"b13\": 0.5,\n        \"b4\": 0.5,\n        \"b8\": 0.5,\n        \"b3\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of neural text degeneration\u2014manifested as repetition and token distribution mismatch\u2014in standard neural language models trained with maximum likelihood estimation (MLE). The authors propose a novel unlikelihood training objective to mitigate these issues, demonstrating significant improvements over traditional methods.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b19\": 0.9,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b11\": 0.7,\n    \"b23\": 0.6,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b4\": 0.5,\n    \"b13\": 0.5,\n    \"b3\": 0.4,\n    \"b21\": 0.4,\n    \"b20\": 0.4,\n    \"b28\": 0.4,\n    \"b24\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of neural text generation degeneration, characterized by repetitive and dull text in open-ended applications. The authors propose an unlikelihood training objective to mitigate these issues, focusing on reducing the probability of negative candidates (repeating and frequent tokens) during model training.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b7\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of neural text degeneration in language models, particularly the repetition and token distribution mismatch observed in generated text. The authors propose a novel unlikelihood training objective to mitigate these issues, demonstrating that their approach significantly improves the quality of generated text by reducing repetition and adjusting token distributions.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b19\": 0.75,\n    \"b24\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b25\": 0.55\n  }\n}\n```"], "5e5e18ca93d709897ce315f0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of local normalization in neural auto-regressive models, including exposure bias and lack of long-range coherency, by proposing a residual Energy-Based Model (EBM) for text generation. The EBM leverages locally normalized language models and uses a novel training strategy to improve perplexity and generation quality.\",\n  \"Direct Inspiration\": [\"b1\", \"b9\"],\n  \"Indirect Inspiration\": [\"b27\", \"b34\", \"b23\"],\n  \"Other Inspiration\": [\"b10\", \"b19\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in parametric text generation, particularly related to local normalization, exposure bias, and lack of long-range coherency in auto-regressive models. The authors propose using a residual energy-based model (EBM) that integrates a locally normalized language model with an energy function to achieve globally normalized text generation, aiming to improve coherency and reduce perplexity. They build upon previous works on discriminators and importance sampling for training and evaluation.\",\n  \"Direct Inspiration\": [\"b1\", \"b9\"],\n  \"Indirect Inspiration\": [\"b27\", \"b34\", \"b23\"],\n  \"Other Inspiration\": [\"b15\", \"b11\", \"b17\", \"b25\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in parametric text generation using large neural auto-regressive models, such as exposure bias and lack of long-range coherency. It proposes a novel approach by integrating energy-based models (EBMs) with auto-regressive models, leveraging advancements in locally normalized language modeling and importance sampling. The core contributions include a formalization of residual interpretation and development of lower and upper bounds for log-probability under the joint model.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b9\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b34\": 0.85,\n    \"b23\": 0.75,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.65,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in parametric text generation using large neural auto-regressive models, specifically focusing on issues like exposure bias and lack of long-range coherence. It proposes using Residual Energy-Based Models (EBMs) to overcome these challenges by incorporating a locally normalized language model with an energy function, enabling globally normalized text generation. The paper builds upon prior works related to discriminative re-ranking, noise contrastive estimation, and importance sampling for efficient training and generation.\",\n  \"Direct Inspiration\": [\"b1\", \"b9\", \"b34\"],\n  \"Indirect Inspiration\": [\"b11\", \"b17\", \"b25\"],\n  \"Other Inspiration\": [\"b15\", \"b10\", \"b19\", \"b23\", \"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of exposure bias and lack of long-range coherence in parametric text generation using large neural auto-regressive models. It proposes a novel approach using Energy-Based Models (EBMs) combined with locally normalized language models to improve text generation quality. The main contributions include leveraging recent advancements in language modeling and developing efficient training and evaluation methods utilizing importance sampling and Noise Contrastive Estimation (NCE).\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b10\": 0.8,\n    \"b34\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b28\": 0.6,\n    \"b26\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```"], "5fe30a2291e01125d4b5b5e3": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"Controlling a generic pretrained language model (LM) to satisfy certain desiderata like avoiding toxic content, preventing demographic biases, or steering towards a certain topic or style.\",\n    \"inspirations\": \"Inspired by reinforcement learning (RL) methods and the need for both pointwise and distributional control in text generation.\"\n  },\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b61\": 1,\n    \"b30\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.85,\n    \"b2\": 0.85,\n    \"b13\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.8,\n    \"b52\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include controlling a generic pretrained language model (LM) to avoid issues like toxic content, demographic biases, and ensuring coherence and fluency in generated text. The proposed algorithm, Generation with Distributional Control (GDC), aims to address these challenges by formalizing text generation as a constraint satisfaction problem over the probability distribution, incorporating both pointwise and distributional constraints. The GDC algorithm combines a KL divergence minimization objective and a novel KL-adaptive Distributional Policy Gradient (DPG) method to achieve better performance in controlled text generation.\",\n  \"Direct Inspiration\": {\n    \"b41\": 1.0,\n    \"b61\": 1.0,\n    \"b43\": 0.9,\n    \"b2\": 0.9,\n    \"b28\": 0.9,\n    \"b52\": 0.9,\n    \"b30\": 0.8,\n    \"b19\": 0.8,\n    \"b13\": 0.8,\n    \"b49\": 0.7,\n    \"b5\": 0.7,\n    \"b36\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b51\": 0.6,\n    \"b40\": 0.6,\n    \"b47\": 0.6,\n    \"b7\": 0.6,\n    \"b16\": 0.7,\n    \"b26\": 0.7,\n    \"b3\": 0.7,\n    \"b8\": 0.7,\n    \"b27\": 0.7,\n    \"b60\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b34\": 0.5,\n    \"b12\": 0.5,\n    \"b33\": 0.5,\n    \"b0\": 0.5,\n    \"b56\": 0.5,\n    \"b45\": 0.5,\n    \"b48\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of controlled text generation, introducing a novel approach called Generation with Distributional Control (GDC). This method formalizes the problem as a constraint satisfaction problem over probability distributions and proposes the KL-Adaptive Distributional Policy Gradient (DPG) algorithm for approximating the optimal distribution. The approach aims to balance pointwise and distributional constraints, providing a unified framework to tackle issues such as bias in pretrained language models.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b36\": 0.95,\n    \"b61\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b25\": 0.85,\n    \"b48\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.75,\n    \"b26\": 0.75,\n    \"b3\": 0.75,\n    \"b12\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of controlling pretrained neural language models to satisfy specific constraints, such as avoiding biases or steering text generation towards certain topics. The authors propose a novel Generation with Distributional Control (GDC) approach, which formulates controlled text generation as a constraint satisfaction problem over probability distributions. They introduce the KL-adaptive DPG algorithm for approximating the desired distribution and demonstrate its effectiveness in experiments.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b61\": 0.9,\n    \"b36\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.7,\n    \"b2\": 0.7,\n    \"b28\": 0.7,\n    \"b52\": 0.7,\n    \"b51\": 0.6,\n    \"b40\": 0.6,\n    \"b47\": 0.6,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b49\": 0.5,\n    \"b54\": 0.5,\n    \"b5\": 0.5,\n    \"b16\": 0.5,\n    \"b26\": 0.5,\n    \"b3\": 0.5,\n    \"b35\": 0.5,\n    \"b48\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of controlled text generation, focusing on maintaining desirable properties such as avoiding toxic content and demographic biases while ensuring the generated text remains coherent and fluent. The authors propose the Generation with Distributional Control (GDC) approach, which formalizes the problem as a constraint satisfaction problem over the probability distribution representing the desired target language model. They introduce a KL-adaptive Distributional Policy Gradient (DPG) algorithm to approximate the optimal distribution, achieving better performance in terms of fluency, diversity, and bias mitigation compared to existing baselines.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.9,\n    \"b61\": 0.85,\n    \"b13\": 0.8,\n    \"b35\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.7,\n    \"b34\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b8\": 0.55\n  }\n}\n```"], "5c8fa6494895d9cbc658642f": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the tendency of neural conversation generation models to produce generic and 'safe' responses. The paper introduces a new decoding objective that incorporates distributional constraints to encourage the generation of more content-rich responses. These constraints include a topic distribution constraint and a semantic similarity constraint.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b0\": 0.9,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.7,\n    \"b13\": 0.6,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating more content-rich and less generic responses in neural conversation models. It proposes a new decoding objective that incorporates distributional constraints to encourage the use of more content words. The constraints include topic similarity, estimated using an HMM-LDA model, and semantic similarity, measured using fixed-dimensional sentence embeddings. The proposed approach is empirically shown to generate more content-rich responses compared to existing baselines.\",\n  \"Direct Inspiration\": [\"b12\", \"b0\"],\n  \"Indirect Inspiration\": [\"b17\", \"b42\"],\n  \"Other Inspiration\": [\"b1\", \"b34\", \"b13\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the tendency of neural conversation generation models to produce generic responses. The paper proposes a new decoding objective that incorporates side-information in the form of distributional constraints to encourage more content-rich responses. The approach leverages unsupervised topic models and semantic similarity measures to enhance the diversity and relevance of generated responses.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b0\": 0.85,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.75,\n    \"b14\": 0.7,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.65,\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating more content-rich and less generic responses in neural conversation models. It proposes a new decoding objective that incorporates distributional constraints in the form of topic and semantic similarity. The approach aims to balance grammaticality and content relevance by using fixed-dimensional sentence embeddings and a topic model.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b12\": 0.9,\n    \"b17\": 1,\n    \"b42\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b2\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating non-generic, content-rich responses in neural conversation generation. It proposes a new decoding objective that incorporates distributional constraints to encourage the use of more content words. The constraints include a topic similarity constraint and a semantic similarity constraint. The approach is empirically shown to generate more content-rich responses compared to existing methods.\",\n    \"Direct Inspiration\": {\n        \"b12\": 0.9,\n        \"b0\": 0.9,\n        \"b17\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.7,\n        \"b40\": 0.7,\n        \"b42\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b25\": 0.6\n    }\n}\n```"], "5f7fdd328de39f0828397e7f": ["```json\n{\n  \"Summary\": \"The paper introduces a two-stage framework named Glance and Focus Network (GFNet) to reduce computational costs in high-resolution image classification by dynamically identifying class-discriminative regions and performing adaptive inference. The primary challenges addressed are efficiently identifying class-discriminative regions and adaptively allocating computation across different images.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1.0,\n    \"b63\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b49\": 0.8,\n    \"b15\": 0.7,\n    \"b20\": 0.7,\n    \"b42\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b7\": 0.6,\n    \"b2\": 0.5,\n    \"b16\": 0.5,\n    \"b53\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Reduce computational cost introduced by high-resolution inputs in image classification tasks.\",\n      \"Efficiently identify class-discriminative regions.\",\n      \"Adaptively allocate computation to each individual image.\"\n    ],\n    \"algorithm\": \"GFNet - a two-stage framework (glance and focus) to dynamically identify and process class-discriminative image regions for efficient inference.\"\n  },\n  \"Direct Inspiration\": {\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.9,\n    \"b42\": 0.9,\n    \"b15\": 0.9,\n    \"b20\": 0.9,\n    \"b49\": 0.9,\n    \"b63\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing the computational cost and memory footprint introduced by high-resolution inputs in image classification tasks. The proposed solution, Glance and Focus Network (GFNet), dynamically identifies and focuses on class-discriminative regions of images, thereby improving computational efficiency without sacrificing accuracy.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9,\n    \"b49\": 0.85,\n    \"b16\": 0.8,\n    \"b42\": 0.8,\n    \"b15\": 0.8,\n    \"b20\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b7\": 0.7,\n    \"b63\": 0.65,\n    \"b19\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b12\": 0.6,\n    \"b39\": 0.55,\n    \"b35\": 0.55,\n    \"b65\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing computational costs and memory footprint associated with high-resolution image inputs in CNN-based image classification tasks. The proposed solution, GFNet, dynamically identifies and processes class-discriminative regions in images, following a two-stage 'glance and focus' approach.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9,\n    \"b63\": 0.85,\n    \"b49\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.75,\n    \"b42\": 0.75,\n    \"b15\": 0.75,\n    \"b20\": 0.75,\n    \"b65\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.65,\n    \"b19\": 0.6,\n    \"b12\": 0.6,\n    \"b39\": 0.55,\n    \"b11\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes the Glance and Focus Network (GFNet) framework to reduce computational cost and memory footprint in high-resolution image classification tasks. The primary challenges addressed are efficiently identifying class-discriminative regions and adaptively allocating computation across images. The GFNet employs a two-stage sequential decision process, starting with a down-sampled full image (glance step) and subsequently focusing on smaller, class-discriminative patches (focus stages). This approach aims to improve computational efficiency and adaptability without sacrificing accuracy.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1.0,\n    \"b63\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b42\": 0.8,\n    \"b20\": 0.8,\n    \"b49\": 0.8,\n    \"b19\": 0.8,\n    \"b11\": 0.7,\n    \"b7\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b60\": 0.6,\n    \"b13\": 0.6,\n    \"b17\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```"], "5eb3df3191e011cea6a7c3c8": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is improving the accuracy of Named Entity Recognition (NER) by combining various techniques and embeddings without preprocessing data or using lexicons. The paper proposes a novel approach by integrating Bidirectional LSTM with CNN and CRF layers, leveraging public word embeddings and character features.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b4\": 0.8,\n    \"b5\": 0.9,\n    \"b6\": 0.9,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b10\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in Named Entity Recognition (NER) by combining techniques from previous models and exploring different word embeddings. Key methods include the combination of CNN, Bidirectional LSTM, and CRF models, along with word and character features.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.9,\n        \"b8\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b7\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.6,\n        \"b10\": 0.6,\n        \"b3\": 0.5,\n        \"b18\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Named Entity Recognition (NER) by combining techniques from Chiu et al. (2015) and Huang et al. (2015), using public word embeddings and character features without preprocessing or lexicon. It explores the impact of different word embeddings (Glove and Fasttext) and the addition of word and character features on the performance of NER models.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b4\": 1,\n    \"b5\": 1,\n    \"b6\": 0.9,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.65,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in Named Entity Recognition (NER) by integrating public word embeddings with word and character features in a Bidirectional LSTM model, both with and without a CRF layer. The primary inspiration comes from combining techniques proposed in previous research, focusing on public embeddings like Glove and Fasttext, and experimenting with various model configurations to enhance NER accuracy.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b9\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Named Entity Recognition (NER) by combining various word and character features using Bidirectional LSTM and CRF models. The authors explore different word embeddings and assess their impact on NER performance without preprocessing the data or using lexicons.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.9,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b11\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```"], "53e9b593b7602d97040d7111": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in Japanese Named Entity Recognition (NER) by proposing an improved Transformation Based Learning (TBL) post-processing approach. The key challenges include the inconsistency between training and evaluation corpora and the need for a universal NE annotator that adapts to different environments. The proposed TBL approach aims to revise NE recognition results based on differences between the original annotator's results and the answer corpus, leveraging a small annotated corpus to generate transformation rules.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b5\": 0.7,\n    \"b12\": 0.8,\n    \"b17\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Japanese Named Entity Recognition (NER) by proposing a Transformation Based Learning (TBL) post-processing approach. This method aims to correct errors arising from differences in feature distributions across corpora, an issue common in statistical models. Key inspirations include the need to adapt existing models to new domains effectively and the utilization of error-driven learning to automatically generate transformation rules that enhance NER accuracy.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b11\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving the accuracy of Japanese Named Entity Recognition (NER) by proposing a Transformation Based Learning (TBL) post-processing approach. The algorithm aims to adapt to different corpora by revising the NE recognition results based on the differences between the original annotator's results and the answer corpus. Key challenges include handling inconsistencies between training and evaluation corpora, and optimizing the transformation rules to improve system efficiency and accuracy.\",\n    \"Direct Inspiration\": {\n        \"b11\": 0.95,\n        \"b15\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b12\": 0.85\n    },\n    \"Other Inspiration\": {\n        \"b17\": 0.75\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inconsistency between the training and evaluation corpora in Japanese Named Entity Recognition (NER), leading to degraded performance. The proposed solution is an improved Transformation Based Learning (TBL) post-processing approach to adapt NER to different environments, leveraging transformation rules to correct errors and optimize performance.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b11\": 0.95,\n    \"b15\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of adapting Named Entity Recognition (NER) systems to different domains without the need for extensive manual annotation. It proposes an improved Transformation Based Learning (TBL) approach for post-processing Japanese NER results. The method focuses on generating transformation rules that can adapt the NER outputs to new corpora by correcting errors based on differences between the initial annotator's results and the manually annotated corpus.\",\n    \"Direct Inspiration\": {\n        \"b11\": 1.0,\n        \"b10\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.8,\n        \"b15\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b17\": 0.6\n    }\n}\n```"], "5db9297247c8f766461f6d13": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in generating long and diverse texts from structured data, focusing on dynamic input modeling, inter-sentence coherence, and expression diversity. The proposed solution, the Planning-based Hierarchical Variational Model (PHVM), incorporates a novel planning mechanism and hierarchical latent structure inspired by human writing processes.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b31\": 0.85,\n    \"b34\": 0.8,\n    \"b33\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.6,\n    \"b37\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b19\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include: 1) modeling input data dynamically during generation to avoid accumulated errors, 2) capturing inter-sentence coherence in long text generation, and 3) generating diversified expressions. The proposed algorithm, Planning-based Hierarchical Variational Model (PHVM), addresses these challenges by integrating a novel planning mechanism into a hierarchical generation process. This model uses a global planning latent variable for high-level planning and local latent variables for sentence realization, enabling better modeling of input data, capturing inter-sentence coherence, and injecting variations at both high and low levels.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b37\": 0.8,\n    \"b30\": 0.7,\n    \"b36\": 0.6\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.7,\n    \"b34\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b22\": 0.5,\n    \"b25\": 0.5,\n    \"b27\": 0.5,\n    \"b42\": 0.5,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in generating long and diverse texts from structured data, focusing on issues such as dynamic modeling of input data, inter-sentence coherence, and expression diversity. The authors propose a Planning-based Hierarchical Variational Model (PHVM) that integrates a novel planning mechanism and hierarchical generation process to better model input data and capture inter-sentence coherence and expression diversity.\",\n    \"Direct Inspiration\": {\n        \"b37\": 0.9,\n        \"b14\": 0.85,\n        \"b4\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b31\": 0.75,\n        \"b42\": 0.7,\n        \"b9\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b33\": 0.6,\n        \"b30\": 0.55\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include dynamic modeling of input data during generation, capturing inter-sentence coherence in long text generation, and generating diversified expressions. The proposed solution, a Planning-based Hierarchical Variational Model (PHVM), introduces a novel planning mechanism and a hierarchical generation process to address these challenges. The model segments input data into groups for planning and uses hierarchical latent variables to inject variations at both high-level planning and low-level realization stages.\",\n    \"Direct Inspiration\": {\n        \"b14\": 1,\n        \"b31\": 0.9,\n        \"b33\": 0.9,\n        \"b34\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.7,\n        \"b25\": 0.6,\n        \"b9\": 0.6,\n        \"b42\": 0.6,\n        \"b37\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b30\": 0.5,\n        \"b36\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three main challenges in data-to-text generation: (1) dynamic modeling of input data during generation, (2) capturing inter-sentence coherence, and (3) generating diversified expressions. The proposed solution is a Planning-based Hierarchical Variational Model (PHVM) that integrates a novel planning mechanism with a hierarchical generation process. This model uses a global planning latent variable for high-level planning and local latent variables for sentence realization, improving coverage, coherence, and diversity in long text generation.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.8,\n    \"b14\": 0.7,\n    \"b31\": 0.6,\n    \"b34\": 0.6,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```\n"], "53e9b0deb7602d9703b5a742": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving Japanese Named Entity Recognition (NER) by proposing a Transformation-Based Learning (TBL) post-processing approach. The primary challenges include handling inconsistencies in training and evaluation corpora, and the limitations of existing statistical models. The proposed method leverages transformation rules to adapt the NE annotator to different domains effectively.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b2\": 0.7,\n        \"b3\": 0.6,\n        \"b4\": 0.6,\n        \"b5\": 0.8,\n        \"b6\": 0.5,\n        \"b7\": 0.5,\n        \"b12\": 0.9,\n        \"b14\": 0.75,\n        \"b15\": 0.75\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Japanese Named Entity Recognition (NER) by proposing an improved Transformation Based Learning (TBL) post-processing approach. The paper aims to overcome the limitations of traditional statistical models and adapt to different corpora more effectively. The key contributions include the use of TBL to acquire transformation rules automatically, optimizing the rule set to improve accuracy and efficiency, and leveraging linguistic knowledge.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b5\": 0.7,\n    \"b3\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Named Entity Recognition (NER) in Japanese texts, specifically focusing on the limitations of existing statistical models when applied to different domains. It proposes an improved Transformation Based Learning (TBL) post-processing approach to revise the NE recognition results, leveraging transformation rules to adapt to different corpora and overcome the inconsistencies between training and evaluation datasets.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Japanese Named Entity Recognition (NER), particularly the degradation of performance due to differences between training and evaluation corpora. It proposes an improved Transformation Based Learning (TBL) post-processing approach to revise and optimize the NER results, making the system adaptable to different domains.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b13\": 0.95,\n    \"b15\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in Japanese Named Entity Recognition (NER), particularly the degradation of performance due to differences in feature distributions across different corpora and the limitations of existing statistical models. The authors propose an improved post-processing approach based on Transformation Based Learning (TBL) to revise the original NE recognition results. This approach leverages transformation rules and optimizes these rules to adapt to different domains effectively.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1,\n        \"b15\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.8,\n        \"b14\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b5\": 0.6,\n        \"b9\": 0.6\n    }\n}\n```"], "5c35e4263a55ac5137e002ea": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of converting unstructured electronic medical records into structured data to improve clinical research, diagnosis, and treatment efficiency. It proposes a novel method for model creation or adaptation, using NeuroNER for NER training and modeling, and a unique relation extraction method for structured report generation.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b1\": 0.75,\n    \"b2\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b6\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges of creating structured electronic medical records from unstructured medical reports, particularly focusing on chest x-rays. The proposed method involves two stages: a training stage for model creation and NER training using NeuroNER, and a testing stage for extracting entities and generating structured reports. The paper introduces novel features such as automatic model adaptation and a new relation extraction algorithm to improve the accuracy and efficiency of medical record generation.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of transforming unstructured electronic medical records into structured formats to improve the efficiency and accuracy of medical diagnosis and treatment. It proposes a method for modeling mechanism creation/adaptation and a novel relation extraction algorithm to generate structured electronic medical records from unstructured chest x-ray reports.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of transforming unstructured electronic medical records into structured formats to optimize doctors' work and reduce costs in diagnosis and treatment. The proposed method involves a novel modeling mechanism for new model creation or adaptation, and a different relation extraction algorithm to trace dependency parsing trees of unstructured data.\",\n    \"Direct Inspiration\": {\n        \"b9\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b5\": 0.8,\n        \"b6\": 0.8,\n        \"b7\": 0.8,\n        \"b8\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.5,\n        \"b1\": 0.5,\n        \"b2\": 0.5,\n        \"b3\": 0.5,\n        \"b10\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of converting unstructured electronic medical records into structured formats to optimize doctors' work and reduce diagnostic costs. The proposed method involves a novel NER model training mechanism and a unique relation extraction algorithm to handle new tags and classify entities into specific categories such as finding, location, and implant. The paper aims to improve the efficiency and accuracy of generating structured medical reports from unstructured data, specifically for chest x-ray reports.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5\n  }\n}\n```"], "5e09a701df1a9c0c4167614c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating universal adversarial triggers for natural language processing (NLP) models. These triggers are input-agnostic sequences of tokens that cause a model to produce a specific prediction when concatenated to any input. The paper proposes a gradient-guided search algorithm to find these triggers, which can have significant security implications and provide insights into global model behavior.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b7\": 0.5,\n    \"b13\": 0.5,\n    \"b14\": 0.5,\n    \"b24\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating universal adversarial triggers for NLP models. These triggers are input-agnostic sequences that can provoke specific predictions regardless of the input, posing significant security risks and offering insights into model behaviors. The authors propose a gradient-guided search algorithm inspired by HotFlip to identify these triggers.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b14\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating universal adversarial triggers, which are input-agnostic sequences of tokens that can cause a model to produce a specific prediction when concatenated to any input. The algorithm proposed to find these triggers involves a gradient-guided search over tokens, aiming to minimize the loss for the target class across batches of examples. This method builds upon existing techniques such as HotFlip and emphasizes the security implications and model interpretability provided by identifying these triggers.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b16\": 0.9,\n    \"b4\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b7\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b2\": 0.55\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges posed by adversarial attacks on NLP models, focusing on the creation of universal adversarial triggers that can cause targeted predictions when concatenated with any input. The proposed algorithm uses a gradient-guided search to iteratively update token sequences, leading to significant security implications and insights into model behaviors.\",\n    \"Direct Inspiration\": {\n        \"b18\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b16\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b7\": 0.5,\n        \"b13\": 0.5,\n        \"b24\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses adversarial attacks on NLP models by introducing universal adversarial triggers, which are input-agnostic sequences of tokens that cause a model to produce a specific prediction when concatenated with any input from a dataset. The main challenge is to design a gradient-guided search over tokens to find these triggers, which have significant security implications and provide insights into global model behavior. The proposed method adapts universal adversarial perturbation to discrete textual inputs and leverages a HotFlip-inspired token replacement strategy.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b4\": 0.9,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b13\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b2\": 0.6,\n    \"b7\": 0.6,\n    \"b14\": 0.6,\n    \"b24\": 0.5\n  }\n}\n```"], "5de632503a55ac4f55c25481": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of Single Image Super-Resolution (SISR) by proposing a method to modify low-resolution (LR) images with arbitrary downsampling kernels so that they can be processed by existing state-of-the-art deep neural networks (DNNs) trained for bicubic kernels. The novel contribution is the use of a correction filter inspired by generalized sampling literature to transform LR images to match those obtained by bicubic kernels, thus improving DNN performance on untrained kernels.\",\n  \"Direct Inspiration\": [\"b7\", \"b30\"],\n  \"Indirect Inspiration\": [\"b25\", \"b36\", \"b12\", \"b37\"],\n  \"Other Inspiration\": [\"b13\", \"b32\", \"b40\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the performance drop of state-of-the-art DNN-based SISR methods when tested on images that do not fit the acquisition process assumption used in their training phase, typically the bicubic downsampling kernel. The authors propose a novel method inspired by the generalized sampling literature to transform the LR image to match one obtained by a bicubic kernel, enabling the use of pre-trained DNN super-resolvers. This involves a correction filter with a closed-form expression for known kernels and an iterative optimization algorithm for unknown kernels, improving performance significantly.\",\n  \"Direct Inspiration\": [\"b7\", \"b30\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b13\", \"b40\", \"b32\", \"b25\", \"b37\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the performance drop of DNN-based Single Image Super-Resolution (SISR) methods when the test data does not match the training data, specifically when the downsampling kernel differs. The proposed algorithm transforms the low-resolution (LR) image to match the one obtained by the bicubic kernel, which allows the use of existing state-of-the-art DNN super-resolvers. The authors take inspiration from the generalized sampling literature.\",\n  \"Direct Inspiration\": [\"b7\", \"b30\"],\n  \"Indirect Inspiration\": [\"b13\", \"b40\", \"b32\", \"b25\", \"b37\", \"b12\", \"b3\", \"b29\", \"b36\"],\n  \"Other Inspiration\": [\"b4\", \"b8\", \"b10\", \"b22\", \"b38\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Single Image Super-Resolution (SISR) and its performance drop when tested on images with different acquisition processes than the ones used for training, typically the bicubic kernel. The authors propose a method inspired by generalized sampling literature to transform LR images so that they can be effectively processed by existing DNN super-resolvers trained on bicubic kernels.\",\n  \"Direct Inspiration\": [\n    \"b7\",\n    \"b30\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b25\",\n    \"b36\",\n    \"b12\"\n  ],\n  \"Other Inspiration\": [\n    \"b13\",\n    \"b40\",\n    \"b32\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of single image super-resolution (SISR) where the test data mismatch the training data, particularly when the downsampling kernel is unknown or different from the bicubic kernel used during training. The authors propose a method inspired by the generalized sampling literature to use a correction filter that transforms the low-resolution (LR) image to match the bicubic kernel assumption, allowing existing deep neural networks (DNNs) trained on bicubic kernel data to be effectively used.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b36\": 0.8,\n    \"b37\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```"], "5eccb534e06a4c1b26a836d5": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The main challenge in the paper is to improve Single Image Super Resolution (SISR) by addressing the ill-posed nature of mapping between low-resolution (LR) and high-resolution (HR) images. The authors aim to enhance the performance of deep learning-based SISR methods by incorporating non-local cross-scale self-similarity.\",\n    \"Inspirations\": \"The paper is inspired by previous works on non-local attention modules and cross-scale self-similarity in image super-resolution. The authors propose a novel Cross-Scale Non-Local (CS-NL) attention module and a Self-Exemplars Mining (SEM) module to tackle the challenges.\"\n  },\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b1\": 1.0,\n    \"b19\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b13\": 0.9,\n    \"b18\": 0.9,\n    \"b36\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.8,\n    \"b10\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Single Image Super-Resolution (SISR) by proposing a novel Cross-Scale Non-Local (CS-NL) attention module. It integrates local, in-scale non-local, and cross-scale non-local priors into a Self-Exemplars Mining (SEM) module, embedded within a recurrent framework to achieve state-of-the-art performance in SISR.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b8\": 0.7,\n    \"b19\": 0.6,\n    \"b36\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b18\": 0.5,\n    \"b37\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Single Image Super-Resolution (SISR) by proposing a novel Cross-Scale Non-Local (CS-NL) attention module. It aims to mine long-range dependencies between low-resolution and high-resolution patches within the same image. The proposed solution integrates local prior, in-scale non-local prior, and cross-scale non-local prior into a Self-Exemplars Mining (SEM) module, which is then embedded into a recurrent framework for improved SISR performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b8\": 0.7,\n    \"b13\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b36\": 0.5,\n    \"b37\": 0.5,\n    \"b38\": 0.5,\n    \"b41\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Single Image Super-Resolution (SISR) by proposing a novel Cross-Scale Non-Local (CS-NL) attention module. The CS-NL module leverages cross-scale non-local self-similarities to improve the high-resolution image reconstruction. Additionally, the paper introduces a Self-Exemplars Mining (SEM) module that integrates local, in-scale non-local, and cross-scale non-local priors into a recurrent framework for enhanced SISR performance.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b10\": 0.8,\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b36\": 0.7,\n    \"b37\": 0.8,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b13\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the ill-posed nature of Single Image Super Resolution (SISR) and the limitations of traditional and deep learning-based methods in effectively utilizing non-local and cross-scale self-similarity. The paper proposes a novel Cross-Scale Non-Local (CS-NL) attention module and a Self-Exemplars Mining (SEM) module, integrated into a recurrent framework, to address these challenges.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b8\": 0.9,\n    \"b10\": 0.8,\n    \"b18\": 0.9,\n    \"b19\": 0.9,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.7,\n    \"b38\": 0.7\n  }\n}\n```"], "5eccb534e06a4c1b26a8358b": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in single image super-resolution (SISR), primarily focusing on improving the utilization of hierarchical features in deep convolutional neural networks (CNNs). The proposed solutions include a Residual Feature Aggregation (RFA) framework and an Enhanced Spatial Attention (ESA) block. The RFA framework aims to make better use of local residual features by aggregating them non-locally, while the ESA block enhances the spatial distribution of these features with a lightweight design.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b12\": 0.95,\n    \"b13\": 0.9,\n    \"b2\": 0.85,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.75,\n    \"b39\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b22\": 0.6,\n    \"b24\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of single image super-resolution (SISR) by proposing a residual feature aggregation (RFA) framework and enhanced spatial attention (ESA) block to improve feature representation and computational efficiency in convolutional neural networks (CNNs). The main contributions include the RFA framework for better feature utilization, the lightweight ESA block for spatial attention, and the RFANet combining both innovations for superior performance in SISR.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b12\": 1.0,\n    \"b2\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b39\": 0.8,\n    \"b9\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b30\": 0.6,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in single image super-resolution (SISR) by proposing a residual feature aggregation (RFA) framework and an enhanced spatial attention (ESA) block. These methods aim to improve the feature representation and efficiency of convolutional neural networks (CNNs) for SISR tasks.\",\n  \"Direct Inspiration\": [\"b3\", \"b12\", \"b2\", \"b37\", \"b18\"],\n  \"Indirect Inspiration\": [\"b17\", \"b39\", \"b9\"],\n  \"Other Inspiration\": [\"b13\", \"b23\", \"b24\", \"b22\", \"b15\", \"b30\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of single image super-resolution (SISR) using a novel residual feature aggregation (RFA) framework and enhanced spatial attention (ESA) block to achieve more accurate image SR. The authors propose RFANet by combining the RFA framework with the ESA block, demonstrating superior results compared to existing methods like RCAN and SAN while using fewer parameters.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b12\": 0.85,\n    \"b2\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b9\": 0.65,\n    \"b39\": 0.6,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b24\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in single image super-resolution (SISR) by proposing a Residual Feature Aggregation (RFA) framework and an Enhanced Spatial Attention (ESA) block. These methods aim to make better use of local residual features and improve the feature representation for more accurate image super-resolution.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b12\": 0.85,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b39\": 0.7,\n    \"b2\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b18\": 0.65\n  }\n}\n```"], "5d9edc7547c8f766460401fb": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The paper aims to improve natural language processing (NLP) by using very deep convolutional neural networks (ConvNets) to learn hierarchical sentence representations, addressing limitations in current models like LSTMs.\",\n    \"Inspirations\": \"The design of the architecture is heavily inspired by recent advancements in computer vision, particularly VGG and ResNet architectures, and previous applications of ConvNets in NLP.\"\n  },\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b17\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b3\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in NLP of developing deep architectures capable of learning hierarchical representations of whole sentences, inspired by recent advancements in computer vision, particularly in using deep convolutional neural networks (ConvNets). The authors propose a novel architecture using very deep convolutional layers (up to 29 layers) to improve performance on sentence classification tasks. The design principles are inspired by VGG and ResNet architectures from computer vision.\",\n    \"Direct Inspiration\": [\"b6\", \"b17\"],\n    \"Indirect Inspiration\": [\"b24\", \"b10\", \"b21\"],\n    \"Other Inspiration\": [\"b13\", \"b11\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Developing deep architectures to learn hierarchical representations of sentences in NLP, addressing long-range dependencies and task-specific structures.\",\n    \"inspirations\": \"Inspired by deep convolutional networks in computer vision, particularly VGG and ResNet architectures.\"\n  },\n  \"Direct Inspiration\": [\"b17\", \"b6\"],\n  \"Indirect Inspiration\": [\"b24\", \"b10\"],\n  \"Other Inspiration\": [\"b13\", \"b12\", \"b22\", \"b21\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of developing deep architectures for NLP that can learn hierarchical representations of whole sentences. The proposed solution is inspired by successful deep convolutional neural network architectures in computer vision, specifically VGG and ResNet, and applies these principles to NLP tasks such as sentence classification.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.95,\n        \"b17\": 0.95,\n        \"b24\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.75,\n        \"b11\": 0.75,\n        \"b13\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.6,\n        \"b21\": 0.65,\n        \"b22\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed by the paper is to develop deep architectures capable of learning hierarchical representations of whole sentences for NLP tasks. The proposed method uses deep convolutional neural networks (ConvNets) inspired by recent progress in computer vision, specifically leveraging the designs of VGG and ResNet architectures.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b17\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b18\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```"], "5e5e18d493d709897ce3320c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of fine-tuning large-scale pretrained language models, like BERT LARGE, on small datasets. It proposes a novel regularization technique called 'mixout,' which aims to stabilize fine-tuning by minimizing deviation from pretrained model parameters (w pre) and avoiding optimization divergence. This is achieved through an adaptive L2-penalty towards w pre, contrasting traditional dropout methods that penalize deviation towards the origin.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b16\": 1,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b22\": 0.8,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of finetuning large-scale pretrained language models on small datasets, proposing a novel regularization technique called mixout. Mixout replaces dropout and aims to prevent the model parameters from deviating too much from the pretrained parameters, thereby improving finetuning stability and performance.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b22\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b27\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the instability of finetuning large pretrained language models, like BERT, on small datasets. The authors propose an algorithm, mixout(u), as a generalization of dropout to address this challenge. Mixout(u) regularizes the finetuning process by penalizing deviations from pretrained model parameters, thereby stabilizing the finetuning and improving performance.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b24\": 0.7,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b23\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the instability of finetuning large-scale language models like BERT on small datasets. The paper proposes a novel regularization technique called mixout, which aims to mitigate this issue by preventing the optimization from deviating excessively from pretrained model parameters.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b16\": 1,\n    \"b26\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b22\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b27\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the instability of finetuning large-scale pretrained language models (e.g., BERT LARGE) on small datasets, proposing mixout as a novel regularization technique. Mixout modifies the dropout mechanism to prevent the model from deviating significantly from pretrained parameters, thereby improving finetuning stability and performance.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b16\": 1,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b21\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.7\n  }\n}\n```"], "53e99de8b7602d97026a7ad9": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency and poor scalability of gradient-domain compositing techniques for large-scale digital imagery. The authors propose a novel approach that reduces the scale of the problem by solving a reduced linear system, leveraging the observation that the difference between a simple color composite and its gradient-domain composite is largely smooth. This is achieved by using a quadtree to adaptively subdivide the domain, significantly reducing memory and time requirements while maintaining visual fidelity.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b25\": 0.8,\n    \"b21\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.6,\n    \"b27\": 0.6,\n    \"b5\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b28\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently performing gradient-domain compositing for large-scale digital imagery. The authors propose a novel approach that reduces the problem scale by leveraging the smoothness pattern between seams and employing a quadtree structure to solve a smaller linear system.\",\n  \"Direct Inspiration\": [\"b16\", \"b25\"],\n  \"Indirect Inspiration\": [\"b27\", \"b20\"],\n  \"Other Inspiration\": [\"b21\", \"b28\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the scalability issues of gradient-domain compositing techniques when working with multi-megapixel digital imagery. The authors propose a novel approach using a quadtree-based adaptive subdivision that significantly reduces the problem size, making it feasible to compute large composites efficiently both in terms of time and memory.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b21\": 1,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b5\": 0.7,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the scalability challenges of gradient-domain compositing, which is a widely used technique in computational photography and video. Traditional methods become impractical for high-resolution images due to the large linear systems involved. The authors propose a novel approach that reduces the problem size by exploiting the smoothness of the composite difference near seams through adaptive quadtree subdivision, significantly reducing memory and time requirements while maintaining visual quality.\",\n    \"Direct Inspiration\": {\n        \"b19\": 1.0,\n        \"b21\": 0.9,\n        \"b27\": 0.85,\n        \"b16\": 0.8,\n        \"b25\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b20\": 0.75,\n        \"b5\": 0.6,\n        \"b28\": 0.55\n    },\n    \"Other Inspiration\": {\n        \"b14\": 0.5,\n        \"b18\": 0.45\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of poor scalability in gradient-domain compositing for multi-megapixel digital imagery. A novel approach is proposed that reduces the problem scale by using a quadtree structure to adaptively subdivide the domain, solving a linear system with O(p) variables instead of O(n), where p is the total length of the seams between image regions. This results in significant reductions in time and memory requirements while achieving visually identical results.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b6\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```"], "573697f96e3b12023e6d2f31": ["```json\n{\n  \"Summary\": \"The paper addresses three primary challenges in ASR: the need for various resources in acoustic modeling, reliance on GMM models for DNN training, and the expertise required to configure hyper-parameters. The proposed solution is the Eesen framework, which simplifies acoustic modeling by using RNNs with LSTM units and the CTC objective function, and employs a generalized decoding method based on WFSTs. Eesen aims to reduce complexity, improve performance, and provide an open-source platform for benchmarking.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b8\": 1.0,\n    \"b10\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b27\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the complexities of training and deploying state-of-the-art ASR systems, focusing on the limitations of hybrid HMM/GMM-DNN models and the challenges of end-to-end ASR systems. The authors propose the Eesen framework, which utilizes deep bidirectional RNNs trained with the CTC objective function, and introduces a generalized decoding method based on weighted finite-state transducers (WFSTs).\",\n  \"Direct Inspiration\": [\"b6\", \"b8\", \"b10\", \"b13\"],\n  \"Indirect Inspiration\": [\"b18\", \"b14\"],\n  \"Other Inspiration\": [\"b16\", \"b17\", \"b19\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in automatic speech recognition (ASR), such as dependency on resources and expertise in hybrid HMM/DNN models, and proposes the Eesen framework using deep bidirectional RNNs with the CTC objective function for end-to-end ASR. Eesen simplifies acoustic modeling and decoding using weighted finite-state transducers (WFSTs).\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b8\": 0.9,\n    \"b10\": 0.9,\n    \"b13\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.75,\n    \"b16\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges in building state-of-the-art Automatic Speech Recognition (ASR) systems, particularly the complexity, resource requirements, and reliance on expert configurations. It introduces the Eesen framework, which simplifies acoustic modeling by using deep recurrent neural networks (RNNs) with Long Short-Term Memory (LSTM) units and the connectionist temporal classification (CTC) objective function. Eesen employs a novel decoding method based on weighted finite-state transducers (WFSTs) to integrate CTC labels, lexicons, and language models.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b8\": 1,\n    \"b10\": 1,\n    \"b13\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.9,\n    \"b17\": 0.9,\n    \"b18\": 0.9,\n    \"b19\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b14\": 0.7,\n    \"b28\": 0.7,\n    \"b29\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in automatic speech recognition (ASR), particularly the complexity of building state-of-the-art ASR systems, the reliance on GMM models for initial training, and the lack of a shared experimental platform for benchmarking. The authors propose the Eesen framework, which simplifies acoustic modeling by using deep recurrent neural networks (RNNs) with Long Short-Term Memory (LSTM) units and the Connectionist Temporal Classification (CTC) objective function. The framework also introduces a generalized decoding method based on weighted finite-state transducers (WFSTs).\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b6\": 0.9,\n    \"b8\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b22\": 0.6,\n    \"b24\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b28\": 0.5,\n    \"b29\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```"], "5b67b4b417c44aac1c86789a": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of multilingual speech recognition, particularly for low-resource languages. It proposes the use of sequence-to-sequence attention-based models, specifically the ASR Transformer with sub-words generated by byte pair encoding (BPE), to eliminate the need for a pronunciation lexicon. It also introduces methods to incorporate language information into the model to improve performance.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.9,\n        \"b9\": 0.8,\n        \"b10\": 0.8,\n        \"b11\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.7,\n        \"b16\": 0.7,\n        \"b17\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is multilingual ASR on low-resource languages, specifically removing the dependency on language-specific pronunciation lexicons. The proposed solution is the use of sub-words generated by byte pair encoding (BPE) as the multilingual modeling unit within an ASR Transformer model. This approach is built upon the sequence-to-sequence attention-based models and incorporates language information into the model to improve performance and reduce word error rate (WER).\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b12\": 0.9,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multilingual speech recognition for low-resource languages. The novel contributions include employing sub-words generated by byte pair encoding (BPE) as the multilingual modeling unit, using the ASR Transformer as the sequence-to-sequence attention-based model, and incorporating language information into the model to alleviate language confusion. The proposed method shows significant improvements in word error rate (WER) compared to existing models like SHL-MLSTM.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of multilingual ASR for low-resource languages, proposing the use of sub-words generated by byte pair encoding (BPE) and employing the ASR Transformer architecture. It seeks to remove dependency on pronunciation lexicons and incorporates language information into the model to reduce language confusion during testing.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b12\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.5,\n    \"b11\": 0.4\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed in the paper is the limitation of multilingual ASR models that require a hand-designed language-specific pronunciation lexicon, which is not feasible for low-resource languages.\",\n    \"inspirations\": \"The paper is inspired by recent research on sequence-to-sequence attention-based models for ASR, aiming to remove the dependency on pronunciation lexicons and improve performance on low-resource languages.\"\n  },\n  \"Direct Inspiration\": [\n    \"b8\",\n    \"b10\",\n    \"b12\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b4\",\n    \"b9\",\n    \"b11\",\n    \"b16\"\n  ],\n  \"Other Inspiration\": [\n    \"b5\",\n    \"b6\",\n    \"b13\",\n    \"b14\"\n  ]\n}\n```"], "5c3ff3ecdf5b8c0b3cd013d4": ["```json\n{\n    \"Summary\": \"The paper addresses challenges in automatic speech recognition (ASR) involving low-resource languages and the complexities of training LSTM models. The authors propose advanced RNN structures, including local window BLSTM, gated recurrent units (GRUs), and residual architecture-based models, to overcome these limitations. The paper builds on previous work and aims to improve both performance and training efficiency in ASR tasks.\",\n    \"Direct Inspiration\": [\"b17\", \"b18\", \"b19\", \"b20\"],\n    \"Indirect Inspiration\": [\"b21\", \"b22\", \"b26\"],\n    \"Other Inspiration\": [\"b24\", \"b25\", \"b33\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in training LSTM models for low-resource languages by proposing advanced RNN structures such as local window BLSTM, gated recurrent units (GRUs), and residual architecture-based models. The aim is to reduce the vanishing gradient problem, speed up training, and improve performance in low-resource speech recognition tasks.\",\n  \"Direct Inspiration\": [\n    \"b17\",\n    \"b18\",\n    \"b19\",\n    \"b20\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b21\",\n    \"b22\",\n    \"b24\",\n    \"b25\"\n  ],\n  \"Other Inspiration\": [\n    \"b8\",\n    \"b10\",\n    \"b11\",\n    \"b15\",\n    \"b26\",\n    \"b27\",\n    \"b28\",\n    \"b29\",\n    \"b30\",\n    \"b31\",\n    \"b32\",\n    \"b33\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in automatic speech recognition (ASR) for low-resource languages using advanced recurrent neural network (RNN) structures, specifically focusing on LSTM and GRU units with local window and residual architectures to overcome issues with training complexity and vanishing gradients.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b21\": 0.85,\n    \"b22\": 0.85,\n    \"b17\": 0.8,\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b15\": 0.65,\n    \"b26\": 0.6,\n    \"b30\": 0.55,\n    \"b31\": 0.55,\n    \"b32\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b24\": 0.45,\n    \"b25\": 0.45,\n    \"b33\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in training LSTM models for low-resource languages in automatic speech recognition (ASR). It introduces advanced RNN structures, such as local window BLSTM, gated recurrent units (GRUs), and residual architecture-based models to improve performance and training efficiency. The paper focuses on reducing the vanishing gradient problem and improving training speed and accuracy.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b30\": 0.8,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of utilizing LSTM models for low-resource languages in speech recognition tasks. It aims to solve issues such as complex training mechanisms, vanishing gradients, and the need for careful tuning by introducing advanced RNN structures including local window BLSTM, gated recurrent units, and residual architecture-based models. The paper's solutions build on previous work and achieve significant improvements in error rates and training times.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b17\": 0.9,\n    \"b18\": 0.9,\n    \"b19\": 0.9,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5,\n    \"b29\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5,\n    \"b33\": 0.5\n  }\n}\n```"], "5a73cbcc17c44a0b3035f3c9": ["```json\n{\n  \"Summary\": \"The paper discusses the challenges and limitations of current deep learning systems, focusing on their data hunger, shallow nature, limited capacity for transfer, struggles with hierarchical structure, open-ended inference, transparency, integration with prior knowledge, distinction between causation and correlation, and stability in dynamic environments. The authors propose the use of data augmentation and explore the limitations of deep learning in various domains like language, speech, and image recognition.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b48\": 1.0,\n    \"b33\": 0.9,\n    \"b42\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b70\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b47\": 0.6,\n    \"b11\": 0.5,\n    \"b36\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the principles, capabilities, and limitations of deep learning systems, particularly focusing on challenges such as data dependency, lack of hierarchical structure, limited transfer capacity, and opacity. It highlights the need for further integration of prior knowledge and better handling of causation and correlation.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b27\": 0.9,\n    \"b48\": 0.85,\n    \"b43\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.75,\n    \"b22\": 0.7,\n    \"b47\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b4\": 0.55,\n    \"b70\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the limitations of deep learning, particularly in terms of data hunger, shallow learning, limited transferability, inability to handle hierarchical structures, struggles with open-ended inference, lack of transparency, poor integration with prior knowledge, inability to distinguish causation from correlation, and assumptions about a stable world.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b48\": 1,\n    \"b22\": 0.9,\n    \"b42\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.8,\n    \"b43\": 0.8,\n    \"b46\": 0.7,\n    \"b47\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b11\": 0.6,\n    \"b4\": 0.5,\n    \"b70\": 0.5,\n    \"b59\": 0.5,\n    \"b58\": 0.5,\n    \"b36\": 0.5,\n    \"b38\": 0.5,\n    \"b15\": 0.5,\n    \"b55\": 0.5,\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the strengths and limitations of deep learning, particularly in neural networks, and identifies several challenges such as data hunger, limited capacity for transfer, difficulty with hierarchical structures, struggles with open-ended inference, lack of transparency, poor integration with prior knowledge, inability to distinguish causation from correlation, and presumption of a stable world.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b48\": 0.9,\n    \"b27\": 0.9,\n    \"b46\": 0.8,\n    \"b42\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b43\": 0.7,\n    \"b11\": 0.6,\n    \"b13\": 0.6,\n    \"b47\": 0.6,\n    \"b25\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b59\": 0.5,\n    \"b58\": 0.5,\n    \"b36\": 0.4,\n    \"b64\": 0.4,\n    \"b38\": 0.4,\n    \"b15\": 0.3,\n    \"b31\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of deep learning, particularly its data hunger, limited capacity for transfer, difficulties with hierarchical structures, struggles with open-ended inference, lack of transparency, poor integration with prior knowledge, inability to distinguish causation from correlation, and reliance on stable environments. The authors propose improvements and highlight the limitations of current deep learning techniques.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b27\": 1.0,\n    \"b48\": 1.0,\n    \"b42\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.7,\n    \"b25\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.5,\n    \"b13\": 0.5,\n    \"b70\": 0.5,\n    \"b59\": 0.5,\n    \"b58\": 0.5,\n    \"b36\": 0.5,\n    \"b38\": 0.5,\n    \"b34\": 0.5,\n    \"b55\": 0.5,\n    \"b31\": 0.5\n  }\n}\n```"], "5ac1829d17c44a1fda9180a3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of training speech recognition systems in low-resource languages. The authors propose a multi-lingual CTC-based model that simplifies the training process by eliminating the need for explicit alignment of transcriptions with training data. The model shares hidden layers across languages and uses sequence training to create a language-independent feature extractor that can be adapted to new languages.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b20\": 0.8,\n    \"b22\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b15\": 0.75,\n    \"b12\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the development of a multi-lingual, cross-lingual speech recognition system that can perform well on low-resource languages without requiring extensive manual annotation. The proposed solution involves using a Connectionist Temporal Classification (CTC) based multi-lingual model with Bidirectional LSTM layers to create language-independent feature representations. This approach facilitates end-to-end training and avoids the need for explicit transcription alignments, making the system more adaptable and efficient for low-resource languages.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b19\": 1.0,\n    \"b20\": 1.0,\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of training speech recognition systems on low-resource languages. It proposes a multi-lingual CTC model that shares Bi-LSTM layers across languages and demonstrates that this approach can create language-independent feature extractors adaptable to new languages with minimal data.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.9,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b12\": 0.7,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training speech recognition systems for low-resource languages. Traditional methods like frame-level segmentation and Hidden Markov Models (HMMs) are labor-intensive and less effective across diverse languages. The proposed solution is a multi-lingual Connectionist Temporal Classification (CTC) model using a bidirectional LSTM network, which simplifies training and improves language-independent feature extraction.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.95,\n    \"b20\": 0.92,\n    \"b22\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.85,\n    \"b19\": 0.80,\n    \"b21\": 0.78\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.75,\n    \"b10\": 0.70\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of training speech recognition systems in low-resource languages by proposing a multi-lingual CTC model that can learn language-independent representations and be adapted to new languages with minimal data. The model aims to avoid the cumbersome multi-step process of traditional HMM/DNN systems and leverages sequence training for end-to-end language universal speech recognition.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b20\": 1.0,\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b12\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```"], "5c04966a17c44a2c74708508": ["```json\n{\n  \"Summary\": \"The primary challenges addressed by the paper are the limitations of grapheme-based Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) systems in multilingual contexts, particularly issues related to large vocabulary sizes and label sparsity in languages with extensive grapheme sets. The paper proposes using Unicode bytes instead of graphemes to create more efficient and scalable models. The proposed models include the Audio-to-Byte (A2B) model for ASR, based on Listen, Attend and Spell (LAS), and the Byte-To-Audio (B2A) model for TTS, based on Tacotron 2.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b19\": 0.95,\n    \"b21\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.85,\n    \"b23\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.80,\n    \"b29\": 0.80\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of expanding coverage of languages in ASR and TTS systems, particularly focusing on the issues of large grapheme vocabularies and label sparsity. The authors propose using Unicode byte sequences instead of graphemes to represent text, leveraging models like Listen, Attend and Spell (LAS) for ASR and Tacotron 2 for TTS. This approach allows for compact models suitable for multilingual applications and on-device deployments.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b8\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b20\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in expanding ASR and TTS coverage across multiple languages using graphemes, which often result in large and inconsistent vocabularies, especially for languages with extensive character sets. The authors propose using Unicode byte sequences for representing text to create more compact and language-independent models. They present the Audio-to-Byte (A2B) model for ASR and the Byte-to-Audio (B2A) model for TTS, showing improved performance across several languages.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b19\": 0.85,\n    \"b21\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b25\": 0.6,\n    \"b24\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in multilingual Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) systems, focusing on the limitations of phoneme-based models and proposing byte-based models for better performance and scalability. The authors introduce the Audio-to-Byte (A2B) model for ASR, based on the Listen, Attend and Spell (LAS) architecture, and the Byte-to-Audio (B2A) model for TTS, based on the Tacotron 2 architecture, both of which represent text using Unicode bytes to handle multiple languages efficiently.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b8\": 0.95,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b24\": 0.65,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of expanding the coverage of Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) systems to multiple languages with varying grapheme vocabularies. The proposed algorithm adopts Unicode bytes for text representation, which is language-independent and allows for more compact, efficient models. The key contributions are the Audio-to-Byte (A2B) model for ASR and the Byte-to-Audio (B2A) model for TTS, both leveraging the language independence of Unicode bytes to improve performance and scalability.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b8\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b23\": 0.8,\n    \"b24\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b27\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```"], "5db92aec47c8f76646216865": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Automatic Speech Recognition (ASR) for the under-resourced Somali language using limited training data. The authors propose using a recently introduced neural network architecture, factorised time-delay neural networks (TDNN-F), and semi-supervised training with unannotated Somali speech data.\",\n  \"Direct Inspiration\": [\"b8\", \"b12\"],\n  \"Indirect Inspiration\": [\"b7\", \"b13\"],\n  \"Other Inspiration\": [\"b14\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an effective automatic speech recognition (ASR) system for the under-resourced Somali language. The authors propose the use of factorized time-delay neural networks (TDNN-F) and semi-supervised training to improve ASR performance. They incorporate multilingual training data and additional unannotated Somali speech data, applying confidence thresholds to enhance the quality of transcriptions.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b7\": 0.7,\n    \"b14\": 0.6,\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an effective automatic speech recognition (ASR) system for the under-resourced Somali language. To tackle this, the authors propose using a factorized time-delay neural network (TDNN-F) model and semi-supervised training with multilingual data from better-resourced but unrelated languages.\",\n  \n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b7\": 0.95\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b8\": 0.75\n  },\n  \n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b15\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an automatic speech recognition (ASR) system for Somali, a highly under-resourced language. It proposes using a hybrid neural network acoustic model and semi-supervised training with both transcribed and untranscribed Somali speech data, as well as multilingual resources. The paper aims to improve ASR performance despite limited computational resources and data.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b13\": 1.0,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge outlined in the paper is the development of an automatic speech recognition (ASR) system for the extremely under-resourced Somali language, with very limited computational resources.\",\n    \"Inspirations\": \"The paper is inspired by the success of radio browsing systems in Uganda and the need to develop a corresponding system for Somalia. The authors took inspiration from hybrid neural network acoustic models, factorised time-delay neural networks (TDNN-F), and semi-supervised training approaches applied in low-resource settings.\"\n  },\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b8\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b14\": 0.9,\n    \"b15\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b11\": 0.8\n  }\n}\n```"], "5c04966a17c44a2c74708401": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing automatic speech recognition (ASR) systems for low-resource languages. The proposed algorithm, LM fusion transfer, integrates an external language model into the decoder network of a sequence-to-sequence (S2S) model during the adaptation stage to leverage linguistic context for better performance in new languages. This approach is built upon methods like shallow fusion, deep fusion, and cold fusion, and aims to improve the efficiency and effectiveness of cross-lingual adaptation in ASR.\",\n  \"Direct Inspiration\": [\"b19\", \"b20\"],\n  \"Indirect Inspiration\": [\"b4\", \"b17\", \"b18\"],\n  \"Other Inspiration\": [\"b5\", \"b6\", \"b9\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Fast system development for low-resourced new languages in automatic speech recognition (ASR), particularly addressing data sparseness in sequence-to-sequence (S2S) models.\",\n    \"inspirations\": \"The use of external language models (LMs) during adaptation, and investigating whether linguistic context from LMs can improve adaptation to new languages.\"\n  },\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b17\": 0.8,\n    \"b18\": 0.8,\n    \"b19\": 0.85,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b9\": 0.75,\n    \"b21\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b1\": 0.6,\n    \"b3\": 0.6,\n    \"b7\": 0.65,\n    \"b8\": 0.65,\n    \"b10\": 0.65,\n    \"b11\": 0.65,\n    \"b12\": 0.65,\n    \"b13\": 0.65,\n    \"b15\": 0.65,\n    \"b16\": 0.65,\n    \"b26\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing automatic speech recognition (ASR) systems for low-resourced new languages. It proposes a novel LM fusion transfer method that integrates an external language model (LM) into the decoder network of sequence-to-sequence (S2S) models during adaptation. This method aims to leverage linguistic context for better performance in low-resource scenarios, improving over existing methods like shallow fusion and cold fusion.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b18\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b19\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing automatic speech recognition (ASR) systems for low-resource languages. It proposes a novel method called LM fusion transfer, which integrates an external language model (LM) into the decoder network of a sequence-to-sequence (S2S) model during the adaptation stage. This method aims to better incorporate linguistic context from the external LM, thereby improving performance in low-resource language scenarios.\",\n  \"Direct Inspiration\": [\"b20\", \"b18\"],\n  \"Indirect Inspiration\": [\"b4\", \"b17\", \"b19\"],\n  \"Other Inspiration\": [\"b5\", \"b9\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the development of automatic speech recognition (ASR) systems for low-resource languages, specifically addressing the data sparseness problem in sequence-to-sequence (S2S) models. The proposed algorithm involves leveraging linguistic context during adaptation through LM fusion transfer, integrating an external language model (LM) into the S2S model during the adaptation stage to improve performance.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b9\": 0.7,\n    \"b17\": 0.7\n  }\n}\n```"], "5c7a561ff56def9798e6a297": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of building automatic speech recognition (ASR) systems for novel languages with limited labeled data. It proposes a language-adversarial transfer learning method to enhance the performance of the target ASR model by transferring knowledge from source models trained on multiple languages. The key innovation is the use of adversarial learning to ensure that shared layers of the source model learn language-invariant features, thereby improving the efficiency of cross-lingual knowledge transfer.\",\n  \"Direct Inspiration\": [\"b31\", \"b32\"],\n  \"Indirect Inspiration\": [\"b5\", \"b29\", \"b10\", \"b21\"],\n  \"Other Inspiration\": [\"b9\", \"b28\", \"b2\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of building an automatic speech recognition (ASR) system for novel languages with limited labeled training data. It proposes a language-adversarial transfer learning method based on the Shared Hidden Layer (SHL) model to improve performance by ensuring the shared layers learn language-invariant features. This method is inspired by adversarial learning techniques used in domain adaptation.\",\n    \"Direct Inspiration\": {\n        \"b31\": 1.0,\n        \"b32\": 0.9,\n        \"b33\": 0.9,\n        \"b34\": 0.8,\n        \"b35\": 0.8,\n        \"b36\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.7,\n        \"b5\": 0.7,\n        \"b9\": 0.7,\n        \"b10\": 0.7,\n        \"b25\": 0.7,\n        \"b28\": 0.7,\n        \"b30\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.6,\n        \"b1\": 0.6,\n        \"b4\": 0.6,\n        \"b6\": 0.6,\n        \"b7\": 0.6,\n        \"b8\": 0.6,\n        \"b11\": 0.6,\n        \"b12\": 0.6,\n        \"b13\": 0.6,\n        \"b14\": 0.6,\n        \"b15\": 0.6,\n        \"b16\": 0.6,\n        \"b17\": 0.6,\n        \"b18\": 0.6,\n        \"b19\": 0.6,\n        \"b20\": 0.6,\n        \"b21\": 0.6,\n        \"b22\": 0.6,\n        \"b23\": 0.6,\n        \"b24\": 0.6,\n        \"b26\": 0.6,\n        \"b27\": 0.6,\n        \"b29\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in rapidly building an automatic speech recognition (ASR) system for a novel language with limited labeled data. It proposes a language-adversarial transfer learning method to improve the performance by leveraging multilingual training and adversarial learning techniques to ensure the shared layers of the source model learn more language-invariant features.\",\n  \"Direct Inspiration\": [\"b31\", \"b32\"],\n  \"Indirect Inspiration\": [\"b28\", \"b5\", \"b2\"],\n  \"Other Inspiration\": [\"b9\", \"b25\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve the rapid construction of automatic speech recognition (ASR) systems for novel languages with limited labeled training data. The proposed solution is a language-adversarial transfer learning method that aims to create language-invariant features by using adversarial learning to improve the performance of low-resource speech recognition systems. The method involves a Shared Hidden Layer Model (SHL-Model) with an additional adversarial language discriminator.\",\n  \"Direct Inspiration\": [\n    \"b31\",\n    \"b32\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b2\",\n    \"b5\",\n    \"b9\",\n    \"b25\",\n    \"b28\",\n    \"b30\",\n    \"b34\",\n    \"b35\"\n  ],\n  \"Other Inspiration\": [\n    \"b21\",\n    \"b27\",\n    \"b29\",\n    \"b33\",\n    \"b36\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is building an ASR system for a novel language with significantly less labeled training data. The authors propose a language-adversarial transfer learning method to improve performance by ensuring shared layers learn language-invariant features.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b32\": 0.9,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b2\": 0.6,\n    \"b25\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.4,\n    \"b10\": 0.4,\n    \"b21\": 0.4,\n    \"b29\": 0.3\n  }\n}\n```"], "5f91548b91e011126509bd5a": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in GCN-based recommendation models, such as sparse supervision signals, skewed data distribution, and interaction noises. It proposes a novel Self-supervised Graph Learning (SGL) paradigm combining supervised learning with self-supervised learning (SSL) to enhance recommendation accuracy and robustness. The main contributions include data augmentation techniques (embedding masking, embedding dropout, node dropout, and edge dropout) and contrastive learning to improve representation learning and mitigate degree biases.\",\n  \"Direct Inspiration\": [\"b4\", \"b7\", \"b8\", \"b16\"],\n  \"Indirect Inspiration\": [\"b2\", \"b6\", \"b15\", \"b21\", \"b29\", \"b33\", \"b40\", \"b43\"],\n  \"Other Inspiration\": [\"b5\", \"b9\", \"b10\", \"b11\", \"b14\", \"b18\", \"b24\", \"b26\", \"b31\", \"b35\", \"b37\", \"b44\", \"b47\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of GCN-based recommendation models such as sparse supervision signals, skewed data distribution, and noises in interactions. It proposes a novel approach, Self-supervised Graph Learning (SGL), which incorporates self-supervised learning (SSL) to enhance recommendation accuracy, particularly for long-tail items and robustness against interaction noises. The method involves data augmentation and contrastive learning to generate multiple views of nodes and maximize agreement between different views of the same node.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b4\": 0.9,\n    \"b7\": 0.9,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.8,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of GCN-based recommendation models, specifically sparse supervision signals, skewed data distribution, and noise in user interactions. The proposed solution is a Self-supervised Graph Learning (SGL) paradigm that leverages data augmentation and contrastive learning to improve node representation learning. The SGL is implemented on a state-of-the-art GCN model, LightGCN, and demonstrates significant improvements in recommendation accuracy, especially for long-tail items and robustness against interaction noises.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b8\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b23\": 0.7,\n    \"b24\": 0.7,\n    \"b37\": 0.7,\n    \"b43\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b6\": 0.5,\n    \"b9\": 0.4,\n    \"b11\": 0.4,\n    \"b15\": 0.5,\n    \"b18\": 0.5,\n    \"b21\": 0.5,\n    \"b26\": 0.4,\n    \"b29\": 0.5,\n    \"b31\": 0.5,\n    \"b33\": 0.4,\n    \"b35\": 0.4,\n    \"b40\": 0.4,\n    \"b44\": 0.4,\n    \"b47\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of GCN-based recommendation models, such as sparse supervision signals, skewed data distribution, and interaction noises, by introducing a self-supervised learning (SSL) approach. The proposed Self-supervised Graph Learning (SGL) method involves data augmentation and contrastive learning to generate multiple views for each node and maximize agreement between different views of the same node. This approach leverages SSL to exploit unlabeled data and mitigate the biases and noise issues inherent in traditional GCN-based models.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b16\": 1.0,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b23\": 0.8,\n    \"b35\": 0.7,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b40\": 0.6,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of GCN-based recommendation models, such as sparse supervision signals, skewed data distribution, and interaction noise. It proposes the Self-supervised Graph Learning (SGL) paradigm, which integrates self-supervised learning with classical GCN to enhance the robustness and accuracy of recommendation systems. SGL involves data augmentation and contrastive learning to generate multiple representation views for nodes, improving model performance, especially for long-tail items.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b35\": 0.9,\n    \"b43\": 0.9,\n    \"b47\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.6,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b15\": 0.5,\n    \"b29\": 0.5,\n    \"b21\": 0.4\n  }\n}\n```"], "5ea2b8c391e01167f5a89e2d": ["```json\n{\n  \"Summary\": \"The paper proposes a new supervised contrastive loss that completely eliminates the need for a reference distribution, contrasting embeddings from the same class to be closer than those from different classes. The main challenge addressed is improving the robustness and generalization of models, particularly in comparison to traditional cross-entropy loss. The work is inspired by recent successes in self-supervised learning, specifically contrastive learning techniques.\",\n  \"Direct Inspiration\": [\"b5\", \"b20\", \"b24\", \"b45\"],\n  \"Indirect Inspiration\": [\"b42\", \"b49\", \"b18\"],\n  \"Other Inspiration\": [\"b19\", \"b21\", \"b39\", \"b47\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the limitations of the cross-entropy loss function in supervised learning, particularly its lack of robustness to noisy labels and poor margins, which affect generalization performance. The authors propose a novel supervised contrastive loss that leverages multiple positives and negatives per anchor point, inspired by self-supervised contrastive learning and metric learning literature. This new loss function aims to improve the clustering of representation space, increase robustness, and enhance generalization performance.\",\n    \"Direct Inspiration\": [\"b5\", \"b45\"],\n    \"Indirect Inspiration\": [\"b20\", \"b24\", \"b42\", \"b47\"],\n    \"Other Inspiration\": [\"b8\", \"b9\", \"b54\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The paper addresses the limitations of the cross-entropy loss function in supervised learning, particularly its lack of robustness to noisy labels and poor margins. It proposes a novel supervised contrastive loss that leverages multiple positives per anchor, improving generalization and robustness.\",\n    \"algorithm\": \"The proposed algorithm uses a contrastive loss function with many positives and negatives per anchor, selected using provided labels. This loss function is designed to be stable to train and less sensitive to hyperparameters compared to cross-entropy loss.\"\n  },\n  \"Direct Inspiration\": [\"b5\", \"b20\", \"b24\", \"b45\"],\n  \"Indirect Inspiration\": [\"b19\", \"b42\", \"b47\", \"b49\"],\n  \"Other Inspiration\": [\"b8\", \"b9\", \"b23\", \"b54\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper focuses on developing a new supervised contrastive loss function to address the shortcomings of cross-entropy loss, particularly its sensitivity to noisy labels and poor margins. The proposed loss improves robustness, generalization, and top-1 accuracy on large-scale datasets like ImageNet. Key innovations include many positives per anchor and leveraging labeled data to select positives and negatives.\",\n    \"Direct Inspiration\": {\n        \"b5\": 0.9,\n        \"b24\": 0.8,\n        \"b20\": 0.8,\n        \"b45\": 0.9,\n        \"b49\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.7,\n        \"b9\": 0.7,\n        \"b19\": 0.7,\n        \"b21\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.6,\n        \"b42\": 0.6,\n        \"b47\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of cross-entropy loss in supervised learning, proposing a new supervised contrastive loss. The key challenges include robustness to noisy labels and poor margins of cross-entropy loss. The proposed algorithm utilizes many positives and negatives per anchor, unlike conventional self-supervised contrastive learning, and shows improved performance on large-scale datasets like ImageNet.\",\n  \"Direct Inspiration\": [\"b5\", \"b45\", \"b20\", \"b24\"],\n  \"Indirect Inspiration\": [\"b47\", \"b39\", \"b42\"],\n  \"Other Inspiration\": [\"b49\", \"b16\", \"b18\"]\n}\n```"], "5f9a9af391e0114d7e7813ed": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised graph representation learning and proposes a novel contrastive learning framework called Graph Contrastive learning with Adaptive augmentation (GCA). The primary contributions include a joint data augmentation scheme for both topology and node attributes and the implementation of adaptive graph augmentation to preserve important structural and semantic patterns.\",\n  \"Direct Inspiration\": {\n    \"b43\": 1,\n    \"b29\": 1,\n    \"b14\": 1\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b18\": 0.85,\n    \"b21\": 0.85,\n    \"b42\": 0.85,\n    \"b23\": 0.9,\n    \"b47\": 0.8\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in unsupervised graph representation learning, particularly focusing on adaptive data augmentation for contrastive learning. The proposed method, Graph Contrastive learning with Adaptive augmentation (GCA), introduces a novel framework that performs stochastic graph augmentation at both topology and node attribute levels to preserve important graph patterns and improve the quality of learned representations.\",\n    \"Direct Inspiration\": {\n        \"b43\": 1,\n        \"b29\": 0.9,\n        \"b14\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b23\": 0.8,\n        \"b47\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.65,\n        \"b21\": 0.6,\n        \"b42\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised graph representation learning using Graph Neural Networks (GNN) with a focus on adaptive data augmentation strategies for contrastive learning. The proposed method, Graph Contrastive Learning with Adaptive Augmentation (GCA), enhances both topology and attribute-level data augmentation to improve node embeddings by preserving important graph structures and attributes.\",\n  \"Direct Inspiration\": [\"b14\", \"b29\", \"b43\"],\n  \"Indirect Inspiration\": [\"b47\"],\n  \"Other Inspiration\": [\"b0\", \"b15\", \"b21\", \"b38\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies two primary challenges in graph representation learning: the insufficiency of simple data augmentation in generating diverse contexts for nodes, and the neglect of the discrepancy in the impact of nodes and edges during data augmentation. To address these, the paper proposes a novel contrastive framework for unsupervised graph representation learning called Graph Contrastive Learning with Adaptive augmentation (GCA). The GCA framework performs adaptive data augmentation on both topology and attribute levels, encouraging the model to learn important features from both aspects.\",\n  \"Direct Inspiration\": {\n    \"b43\": 0.95,\n    \"b29\": 0.90,\n    \"b14\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b47\": 0.8,\n    \"b23\": 0.75,\n    \"b45\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b21\": 0.6,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in unsupervised graph representation learning, specifically focusing on the limitations of existing data augmentation schemes in contrastive learning (CL) methods for graphs. It proposes a novel framework, Graph Contrastive learning with Adaptive augmentation (GCA), which performs adaptive data augmentation on both topology and attribute levels to learn important features effectively. The core contributions include a general contrastive framework for unsupervised graph representation learning with strong, adaptive data augmentation, and comprehensive empirical studies demonstrating its effectiveness.\",\n    \"Direct Inspiration\": [\n        \"b43\",\n        \"b29\",\n        \"b14\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b18\",\n        \"b21\",\n        \"b23\",\n        \"b47\"\n    ],\n    \"Other Inspiration\": [\n        \"b45\"\n    ]\n}\n```"], "5f7af09591e011983cc81efc": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving contrastive self-supervised learning by focusing on the importance of hard negatives. The authors propose a novel method called MoCHi, which synthesizes hard negatives through feature-level mixing, in order to enhance the learning process and achieve better and faster performance in visual representation learning.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b12\": 1.0,\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.8,\n    \"b47\": 0.7,\n    \"b66\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b70\": 0.6,\n    \"b86\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving contrastive self-supervised learning by focusing on the importance of hard negatives. The proposed algorithm, MoCHi, introduces a novel approach called hard negative mixing, which synthesizes hard negatives in the embedding space to facilitate better and faster learning.\",\n  \"Direct Inspiration\": [\"b31\", \"b12\"],\n  \"Indirect Inspiration\": [\"b10\", \"b47\", \"b78\", \"b66\"],\n  \"Other Inspiration\": [\"b70\", \"b86\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving contrastive self-supervised learning by focusing on hard negatives. It introduces a novel approach called MoCHi, which involves synthesizing hard negatives through feature-level mixing to enhance learning efficiency and representation quality.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b47\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b34\": 0.7,\n    \"b70\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving self-supervised contrastive learning by focusing on the selection and synthesis of hard negatives. Inspired by the success of data mixing techniques, the authors propose a novel method called MoCHi, which involves mixing hard negative samples to enhance the quality of learned visual representations. The primary contributions include a detailed analysis of the need for harder negatives, the introduction of hard negative mixing in the embedding space, and extensive evaluation showing improved generalization and faster learning.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b10\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b47\": 0.7,\n    \"b66\": 0.7,\n    \"b78\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.6,\n    \"b14\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving contrastive self-supervised learning by focusing on the role of hard negatives. It proposes MoCHi, a method for synthesizing hard negatives through feature-level mixing, to enhance learning and representation quality.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b31\": 1,\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b47\": 0.8,\n    \"b78\": 0.7,\n    \"b66\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b70\": 0.5,\n    \"b84\": 0.5\n  }\n}\n```"], "5f88146591e0118ce8f040a7": ["```json\n{\n  \"Summary\": \"The paper investigates the role of different negatives in Contrastive Instance Discrimination (CID) within the context of self-supervised learning, specifically using the MoCo v2 method. It evaluates the necessity and sufficiency of negatives based on their difficulty, as measured by the dot product between embeddings, and finds that the hardest 5% of negatives are necessary and sufficient for strong downstream performance, while the easiest 95% are largely unnecessary. The study also explores the semantic properties of negatives and suggests potential improvements for CID methods.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b11\": 0.95,\n    \"b19\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b16\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b12\": 0.65,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper investigates the role of negative samples in contrastive instance discrimination (CID) specifically focusing on their difficulty. It reveals that a small fraction of the hardest negatives are necessary and sufficient for effective representation learning, while the easiest negatives are largely unnecessary. The findings suggest potential improvements in CID methods by focusing on harder negatives and possibly reducing the computational cost.\",\n    \"Direct Inspiration\": {\n        \"b11\": 1.0,\n        \"b9\": 0.9,\n        \"b12\": 0.8,\n        \"b19\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.75,\n        \"b18\": 0.8,\n        \"b4\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.65,\n        \"b26\": 0.6,\n        \"b30\": 0.6,\n        \"b0\": 0.55,\n        \"b13\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding the role of negative examples in contrastive instance discrimination (CID) within self-supervised learning (SSL). The authors propose an empirical investigation into the difficulty of negatives and how it affects training, focusing on MoCo v2 and the downstream task of linear classification on ImageNet. Key findings include that the hardest 5% of negatives are necessary and sufficient for effective training, while the very hardest 0.1% of negatives are often detrimental.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b9\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b4\": 0.7,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively utilizing negative samples in contrastive instance discrimination (CID) for self-supervised learning (SSL). It empirically investigates the role of negative sample difficulty in training, concluding that the top 5% hardest negatives are most effective, while the very hardest 0.1% can be detrimental. The study focuses on MoCo v2 and suggests that understanding and leveraging negative difficulty can improve SSL methods.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper focuses on investigating the impact of the difficulty of negatives in contrastive instance discrimination (CID) within self-supervised learning (SSL). The primary challenges addressed are understanding the relative importance and properties of negatives used in CID, especially within the MoCo v2 framework. The authors propose empirical investigations to determine which negatives are necessary or sufficient for effective learning and downstream task performance.\",\n    \"Direct Inspiration\": {\n        \"b11\": 1,\n        \"b9\": 0.9,\n        \"b12\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.8,\n        \"b4\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.6,\n        \"b16\": 0.6,\n        \"b18\": 0.5\n    }\n}\n```"], "5f842b5891e01129be18ffbd": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the effective selection of negative samples in unsupervised contrastive learning. The proposed algorithm aims to enhance the learning process by using 'hard negative sampling,' which focuses on selecting negative samples that are both true negatives and hard in the sense that they are close to the anchor in the embedding space. This method adjusts the hardness level to balance learning signal from hard negatives and the potential harm from false negatives.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b9\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b14\": 0.8,\n    \"b47\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of selecting informative negative samples in contrastive learning for better representation learning. It proposes a novel hard negative sampling method that balances the need for true negative samples and the hardness of samples, improving the learning signal during training.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b12\": 0.8,\n    \"b14\": 0.85,\n    \"b29\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effective negative sampling in contrastive learning for representation learning. It proposes a novel hard negative sampling method that aims to provide more informative negative samples by focusing on 'true negatives' and controlling the degree of hardness of these samples. The method is designed to improve the learning signal during training and is validated through empirical results on various datasets.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b14\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b15\": 0.6,\n    \"b29\": 0.5,\n    \"b34\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficiently selecting negative samples in contrastive learning to improve representation learning. It proposes a hard negative sampling method based on two principles: ensuring true negatives and selecting hard negatives that are similar to the anchor. The method is designed to be computationally efficient and is tested on various datasets, showing improvements over baseline methods.\",\n    \"Direct Inspiration\": {\n        \"b9\": 1.0,\n        \"b12\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b47\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b14\": 0.6,\n        \"b29\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of selecting informative negative samples in contrastive learning, proposing a novel hard negative sampling method. This method aims to improve the learning signal by selecting negatives that the current embedding believes are similar to the anchor, adhering to two guiding principles: sampling true negatives and selecting hard negatives. The proposed approach balances the trade-off between learning signal and false negative approximation, integrating these principles into an efficient sampling strategy with minimal computational overhead.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b12\": 0.9,\n    \"b47\": 0.8,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b1\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```"], "5efdaf7b91e01191d3d28242": ["```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper include the sampling bias in contrastive learning where negative samples may inadvertently be similar to positive samples, leading to significant performance drops. The authors propose a 'debiased contrastive loss' to correct for this bias, allowing for better performance in unsupervised learning tasks without requiring labeled data. The paper further provides theoretical analysis and empirical validation across various benchmarks in vision, language, and reinforcement learning.\",\n  \"Direct Inspiration\": [\"b0\", \"b1\", \"b15\", \"b34\"],\n  \"Indirect Inspiration\": [\"b23\", \"b30\"],\n  \"Other Inspiration\": [\"b3\", \"b4\", \"b6\", \"b9\", \"b10\", \"b17\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of sampling bias in contrastive learning, where negative examples may be semantically similar to the anchor point leading to performance drops. The authors propose a debiased contrastive loss that approximates the distribution of negative examples more accurately without requiring labeled data, thereby improving the performance of unsupervised representation learning in various domains including vision, language, and reinforcement learning.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.6,\n    \"b23\": 0.7,\n    \"b34\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of sampling bias in contrastive learning, which arises when negative samples are similar to positive samples, leading to performance drops. The authors propose a debiased contrastive loss that approximates the distribution of negative examples to correct for this bias, improving performance across vision, language, and reinforcement learning tasks. The approach is theoretically analyzed and empirically validated to outperform state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b15\": 0.9,\n    \"b0\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.75,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.6,\n    \"b2\": 0.55,\n    \"b12\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of sampling bias in contrastive learning, which arises when negative examples are drawn uniformly from unlabeled data, potentially including semantically similar examples. The proposed solution is a debiased contrastive loss that approximates the distribution of negative examples to correct for this bias. This new objective is shown to improve performance across vision, language, and reinforcement learning benchmarks, and is theoretically analyzed to demonstrate its relation to supervised learning.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.95,\n    \"b15\": 0.85,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b21\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.6,\n    \"b32\": 0.55,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of sampling bias in contrastive learning, which occurs when semantically similar negative samples are drawn from the training data, leading to performance drops. The authors propose a debiased contrastive loss that corrects for this bias using positive examples and unlabeled data. This approach is validated through theoretical analysis and empirical experiments in vision, language, and reinforcement learning tasks, showing improved performance over standard contrastive learning methods.\",\n  \"Direct Inspiration\": [\"b1\", \"b15\"],\n  \"Indirect Inspiration\": [\"b0\", \"b23\", \"b3\"],\n  \"Other Inspiration\": [\"b9\", \"b21\", \"b32\"]\n}\n```"], "5d9c5e4d3a55ac916a95fbd8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the out-of-distribution (OOD) detection capabilities of Variational Autoencoders (VAEs). The main contributions include identifying the limitations of current VAE models in distinguishing inliers from outliers, proposing a negative sampling method to enhance the discriminative power of VAEs, and demonstrating the effectiveness of this method through empirical evidence.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b0\": 1,\n    \"b3\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the discriminative power of Variational Autoencoders (VAEs) for out-of-distribution (OOD) detection. The authors propose a method incorporating negative sampling to mitigate the weaknesses of VAEs in distinguishing between inliers and outliers. The primary contributions include the empirical demonstration that the choice of observation model affects OOD likelihood estimates, the introduction of negative sampling, and evidence that this approach significantly enhances OOD detection.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.95,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.85,\n    \"b0\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the discriminative power of Variational Autoencoders (VAEs) in distinguishing inliers from out-of-distribution (OOD) samples. The authors propose the use of negative sampling, introducing an additional prior distribution for negative samples, and demonstrate that this approach significantly enhances the OOD detection capabilities of VAEs. The paper also explores the impact of different noise models on the VAE's performance.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b0\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the out-of-distribution (OOD) detection capabilities of Variational Autoencoders (VAEs). The authors identify that VAEs often fail to distinguish common objects from OOD samples, particularly influenced by the observation model used. The paper proposes using negative sampling techniques, including auxiliary datasets and adversarial training, to enhance the discriminative power of VAEs. Experimental results show that these methods significantly improve OOD detection performance.\",\n  \"Direct Inspiration\": [\"b3\", \"b6\"],\n  \"Indirect Inspiration\": [\"b8\", \"b0\"],\n  \"Other Inspiration\": [\"b4\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of out-of-distribution (OOD) detection in Variational Autoencoder (VAE) models, demonstrating that the extent of bad likelihood estimates in VAEs depends on the observation model. The authors propose negative sampling in VAEs to improve discriminative power against OOD samples by introducing an additional prior distribution in the latent space. They provide empirical evidence showing that negative samples from either an auxiliary dataset or adversarial training significantly enhance the model's performance.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b8\": 0.9,\n    \"b3\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```"], "5bdc315017c44a1f58a05bba": ["```json\n{\n  \"Summary\": \"The paper addresses the theoretical understanding of negative sampling methods used for parameter estimation in conditional models, particularly focusing on Noise Contrastive Estimation (NCE). The authors identify that while NCE is well-understood in joint models, there are subtle issues when generalizing to conditional models. They provide theoretical analysis and consistency results for both binary classification-based and ranking-based NCE variants, and discuss their statistical efficiency. They also apply these results to existing methods, offering a unified theoretical framework.\",\n  \"Direct Inspiration\": [\n    \"b9\",\n    \"b8\",\n    \"b4\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b6\"\n  ],\n  \"Other Inspiration\": [\n    \"b3\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of estimating parameters in conditional models commonly used in NLP, particularly under computational constraints. It proposes two Noise Contrastive Estimation (NCE) variants - binary classification-based and ranking-based - for consistent parameter estimation in conditional models, analyzing their statistical efficiency and consistency.\",\n  \"Direct Inspiration\": [\"b9\", \"b4\"],\n  \"Indirect Inspiration\": [\"b8\", \"b6\"],\n  \"Other Inspiration\": [\"b1\", \"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses parameter estimation in conditional models, particularly focusing on the challenges related to computational expenses in calculating the partition function Z(x; \u2713) during Maximum Likelihood Estimation (MLE). The authors propose variants of Noise Contrastive Estimation (NCE) for parameter estimation in these models and analyze their theoretical consistency and efficiency.\",\n  \"Direct Inspiration\": [\"b9\", \"b8\"],\n  \"Indirect Inspiration\": [\"b4\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of parameter estimation in conditional models within NLP, specifically focusing on the theoretical understanding and consistency of Negative Contrastive Estimation (NCE) methods. It proposes and analyzes binary classification and ranking-based variants of NCE, demonstrating their respective consistency under different assumptions.\",\n  \"Direct Inspiration\": [\"b9\", \"b8\", \"b4\"],\n  \"Indirect Inspiration\": [\"b6\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses parameter estimation in conditional models used extensively in NLP. It focuses on the theoretical analysis of Noise Contrastive Estimation (NCE) methods, particularly in conditional models, and compares binary classification and ranking-based variants of NCE. The paper identifies the challenges of computational expense in maximum likelihood estimation and the lack of theoretical understanding in negative sampling methods. It proposes formal results for the consistency and efficiency of NCE variants and evaluates these methods through experiments.\",\n  \"Direct Inspiration\": [\"b4\", \"b9\"],\n  \"Indirect Inspiration\": [\"b8\", \"b6\"],\n  \"Other Inspiration\": [\"b0\"]\n}\n```"], "599c7959601a182cd2633b3e": ["```json\n{\n  \"Summary\": \"The paper addresses the cold-start problem in collaborative filtering (CF) by combining CF with deep neural networks (DNN) to effectively extract complex features from content data. The major challenge is the increased computational cost when combining these models. The authors propose a general framework that integrates CF and DNN, and introduce novel sampling strategies to improve training efficiency.\",\n  \"Direct Inspiration\": [\n    \"b0\",\n    \"b3\",\n    \"b29\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b6\",\n    \"b24\",\n    \"b27\",\n    \"b30\",\n    \"b31\"\n  ],\n  \"Other Inspiration\": [\n    \"b2\",\n    \"b15\",\n    \"b16\",\n    \"b17\",\n    \"b18\",\n    \"b21\",\n    \"b22\",\n    \"b25\",\n    \"b26\",\n    \"b32\",\n    \"b36\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the cold-start problem in collaborative filtering (CF) by proposing a hybrid framework that combines CF with deep neural networks. The main challenges include the computational cost of combining CF and deep neural networks, and the inefficient sampling strategies in existing methods. The authors introduce three novel sampling strategies to improve training efficiency and recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b3\": 0.9,\n    \"b31\": 0.8,\n    \"b36\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.6,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b15\": 0.5,\n    \"b25\": 0.5,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the cold-start problem in collaborative filtering (CF) by proposing a general framework that combines CF with deep neural networks. This hybrid framework leverages the power of CF to capture user preferences and the strength of deep neural networks to extract high-level features from content data. The authors identify and tackle the computational challenges associated with training such a hybrid model, specifically focusing on the inefficiency of existing stochastic sampling techniques. They propose three novel mini-batch sampling strategies to improve training efficiency: Stratified Sampling, Negative Sharing, and a combined approach of both.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b3\": 1.0,\n    \"b29\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b25\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of traditional Collaborative Filtering (CF) methods in dealing with the 'cold-start' problem and incorporating complex data features such as images, audio, and text. It proposes a unified framework combining CF and deep neural networks to leverage the strengths of both. The main contributions include novel sampling strategies to improve training efficiency and performance.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b3\": 0.9,\n    \"b29\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b14\": 0.7,\n    \"b24\": 0.75,\n    \"b27\": 0.75,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.65,\n    \"b18\": 0.7,\n    \"b31\": 0.8,\n    \"b36\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the 'cold-start' problem in collaborative filtering by proposing a general framework that combines collaborative filtering with deep neural networks. This approach aims to capture user preferences and extract high-level features from content data efficiently. Key contributions include novel mini-batch sampling strategies to reduce computational costs and improve training efficiency.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b3\": 1,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b10\": 0.7,\n    \"b14\": 0.8,\n    \"b18\": 0.8,\n    \"b24\": 0.7,\n    \"b27\": 0.7,\n    \"b30\": 0.7,\n    \"b31\": 0.8,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```"], "599c795d601a182cd2635171": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficiently training deep metric learning models for feature embedding by proposing a novel approach that combines global and triplet loss, utilizing a smart sampling method with low computational complexity. This method focuses on finding hard negative and positive samples, enhancing training robustness and accuracy.\",\n    \"Direct Inspiration\": [\"b6\", \"b9\", \"b15\", \"b25\"],\n    \"Indirect Inspiration\": [\"b4\", \"b14\", \"b23\"],\n    \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training deep metric learning models, specifically triplet networks, for effective feature embedding in scenarios with a large number of classes and few samples per class. The authors propose a novel approach that combines triplet loss and global loss, using a smart sampling method to reduce the computational complexity of selecting effective training samples. Additionally, they introduce an adaptive controller to optimize the training process automatically.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b9\": 1,\n    \"b15\": 1,\n    \"b25\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b23\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b20\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently training deep metric learning models for large-scale datasets with numerous classes and few samples per class. It proposes a novel approach combining global and triplet loss with a smart sampling method to improve training efficiency and accuracy.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b6\": 1,\n    \"b25\": 1,\n    \"b4\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.9,\n    \"b14\": 0.8,\n    \"b23\": 0.8,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b3\": 0.5,\n    \"b1\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training deep metric learning models for effective feature embedding in scenarios with a large number of classes and a low number of samples per class. The proposed method combines global and triplet loss functions with a novel smart sampling method to improve computational efficiency and training robustness. Additionally, an adaptive controller is introduced to optimize training performance automatically.\",\n  \"Direct Inspiration\": [\"b6\", \"b9\", \"b4\", \"b25\"],\n  \"Indirect Inspiration\": [\"b15\", \"b17\", \"b23\", \"b14\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges outlined include the infeasibility of forming all possible triplets due to high complexity and the need for effective hard positive and negative sampling for robust training of triplet networks. The novel contributions include a combination of global and triplet loss computed using a smart sampling method with low computational complexity, and an adaptive controller for hyper-parameter tuning.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b25\": 1.0,\n    \"b9\": 0.9,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b14\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b20\": 0.5\n  }\n}\n```"], "5a260c8117c44a4ba8a30b08": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of class imbalance in training one-stage object detectors, proposing a novel loss function called Focal Loss which down-weights the loss assigned to well-classified examples. This approach allows training to focus on hard examples and improves the performance of one-stage detectors to match or surpass two-stage detectors. The proposed method is evaluated using the RetinaNet detector, which combines Focal Loss with an efficient in-network feature pyramid.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b18\": 0.9,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b12\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extreme foreground-background class imbalance in dense object detectors by introducing a novel loss function called Focal Loss. This loss function reshapes the standard cross-entropy loss to down-weight well-classified examples, focusing training on hard examples. The proposed Focal Loss is demonstrated to significantly improve the accuracy of a simple one-stage detector, named RetinaNet, surpassing the performance of state-of-the-art two-stage detectors.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b26\": 1.0,\n    \"b20\": 0.9,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b12\": 0.7,\n    \"b22\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the extreme foreground-background class imbalance encountered during the training of dense object detectors. The algorithm proposed to address this challenge is the Focal Loss, which reshapes the standard cross-entropy loss to down-weight the loss assigned to well-classified examples, thereby focusing training on a sparse set of hard examples. The paper introduces RetinaNet, a one-stage detector that incorporates this novel loss function, achieving state-of-the-art accuracy while maintaining efficient processing speed.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b18\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b24\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the extreme foreground-background class imbalance encountered during the training of dense object detectors. The proposed solution is a novel loss function called Focal Loss, which dynamically scales the cross entropy loss to focus training on hard examples and prevent easy negatives from overwhelming the detector. The paper demonstrates the effectiveness of Focal Loss by designing a dense detector named RetinaNet, which surpasses the accuracy of existing state-of-the-art two-stage detectors.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b8\": 0.8,\n    \"b12\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extreme foreground-background class imbalance in training dense object detectors. To overcome this, it introduces a novel loss function called Focal Loss, which down-weights the loss assigned to well-classified examples, focusing training on hard examples. The proposed RetinaNet detector, using the Focal Loss, surpasses the accuracy of state-of-the-art two-stage detectors while maintaining high speed.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b20\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b24\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```"], "5f03f3b611dc83056223206d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of interpreting Graph Neural Networks (GNNs) at the model-level by proposing a novel interpretation technique called XGNN. This technique investigates what graph patterns can maximize a certain prediction by training a graph generator using reinforcement learning. The generated graph patterns are then used to explain the deep graph models, aiming to provide higher-level insights and a more general understanding of how GNNs work.\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of interpreting Graph Neural Networks (GNNs) at the model-level, which is less explored compared to example-level interpretations. The novel algorithm proposed, XGNN, aims to generate graph patterns that maximize certain predictions, formulated as a reinforcement learning problem. The approach incorporates policy gradients and graph rules to ensure the generation of valid and human-intelligible graphs.\",\n    \"Direct Inspiration\": {\n        \"b34\": 0.95,\n        \"b39\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b3\": 0.75,\n        \"b23\": 0.7,\n        \"b24\": 0.7,\n        \"b25\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b27\": 0.6,\n        \"b41\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges in the paper revolve around the lack of interpretable mechanisms in Graph Neural Networks (GNNs), which limits their trustworthiness in critical applications. The authors propose a novel interpretation technique, XGNN, to provide model-level explanations for GNNs by generating graph patterns that maximize certain predictions. This approach is different from existing example-level interpretation techniques and addresses specific challenges in optimizing graph structures for interpretability.\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.95,\n    \"b39\": 0.9,\n    \"b3\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b23\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b36\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the lack of human-intelligible explanations for the predictions made by Graph Neural Networks (GNNs). The proposed solution, XGNN, aims to provide model-level interpretations for GNNs by investigating what graph patterns can maximize a certain prediction. This is achieved by training a graph generator using reinforcement learning, which generates graph patterns that can be used to explain deep graph models.\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.9,\n    \"b39\": 0.85,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b23\": 0.7,\n    \"b24\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.65,\n    \"b41\": 0.65,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the lack of human-intelligible explanations for Graph Neural Networks (GNNs), which prevents their use in critical applications. The paper proposes a novel interpretation technique known as XGNN for explaining deep graph models at the model-level by generating graph patterns that can be used to explain GNNs. The technique is formulated as a reinforcement learning problem and incorporates several graph rules to ensure valid graph generation.\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b18\": 0.6,\n    \"b36\": 0.6,\n    \"b38\": 0.6,\n    \"b40\": 0.7\n  }\n}\n```"], "5550414c45ce0a409eb39fa8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding the transition from general to specific features in neural networks and quantifying how well features transfer from one task to another. The authors propose methods to study transfer learning, focusing on the impact of feature generality and specificity on performance degradation, optimization difficulties, and the benefits of fine-tuning transferred features.\",\n  \"Direct Inspiration\": [\"b2\", \"b1\", \"b0\", \"b4\", \"b13\", \"b12\"],\n  \"Indirect Inspiration\": [\"b7\", \"b6\", \"b5\"],\n  \"Other Inspiration\": [\"b9\", \"b11\", \"b10\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the phenomenon of general and specific features in deep neural networks, particularly focusing on the transition from general to specific features across network layers. It proposes methods to quantify this transition and examines the performance of transferred features in different scenarios, highlighting the challenges of optimization and feature specificity. The study uses ImageNet datasets and transfer learning techniques to analyze these aspects.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b1\": 0.9,\n        \"b0\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b13\": 0.8,\n        \"b12\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.7,\n        \"b6\": 0.7,\n        \"b5\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the phenomenon of deep neural networks learning first-layer features resembling Gabor filters or color blobs, regardless of the training objective. It investigates the transition from general to specific features within the network layers and quantifies the degree of generality or specificity of each layer. The paper also explores the impact of transferring network layers on performance, highlighting optimization difficulties and the importance of task similarity in transfer learning.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b1\": 0.8,\n    \"b0\": 0.8,\n    \"b4\": 0.7,\n    \"b13\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.6,\n    \"b11\": 0.6,\n    \"b10\": 0.6,\n    \"b7\": 0.5,\n    \"b6\": 0.5,\n    \"b5\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding the transition from general to specific features in deep neural networks, particularly in the context of transfer learning. It proposes methods to quantify feature generality and investigates the performance of transferred features in various layers of neural networks.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b1\": 1.0,\n    \"b0\": 1.0,\n    \"b4\": 0.9,\n    \"b13\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b11\": 0.6,\n    \"b10\": 0.6,\n    \"b7\": 0.5,\n    \"b8\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding the transition from general to specific features in deep neural networks trained on image datasets. The main contributions include quantifying the generality and specificity of features at different network layers, identifying issues causing performance degradation when transferring features without fine-tuning, and demonstrating the benefits of feature transfer even after extensive fine-tuning.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b11\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b1\": 0.8,\n    \"b0\": 0.8,\n    \"b4\": 0.8,\n    \"b13\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b6\": 0.6,\n    \"b5\": 0.6,\n    \"b8\": 0.5\n  }\n}\n```"], "5e68b99493d709897cd373ed": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges posed by Batch Normalization (BN) in deep learning, particularly its dependency on large batch sizes which limits its effectiveness in memory-constrained scenarios. The authors propose Group Normalization (GN) as an alternative that normalizes features within groups of channels rather than across the batch dimension, thus making it independent of batch sizes and more stable across different batch sizes. GN shows competitive performance with BN across various tasks including image classification, object detection, segmentation, and video classification.\",\n    \"Direct Inspiration\": {\n        \"b0\": 0.9,\n        \"b16\": 0.8,\n        \"b17\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.7,\n        \"b6\": 0.6,\n        \"b34\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.5,\n        \"b14\": 0.5,\n        \"b19\": 0.5,\n        \"b20\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the limitations of Batch Normalization (BN) in deep learning, particularly its dependency on large batch sizes, which can hinder training higher-capacity models. It proposes Group Normalization (GN) as an alternative that divides channels into groups and normalizes within each group, independent of batch sizes. GN is shown to perform comparably to BN and better than other normalization methods like Layer Normalization (LN) and Instance Normalization (IN) in various tasks such as image classification, object detection, segmentation, and video classification.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1,\n        \"b1\": 0.9,\n        \"b2\": 0.9,\n        \"b16\": 0.8,\n        \"b17\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.7,\n        \"b19\": 0.7,\n        \"b20\": 0.7,\n        \"b34\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.5,\n        \"b27\": 0.5,\n        \"b35\": 0.5,\n        \"b45\": 0.5,\n        \"b46\": 0.5,\n        \"b47\": 0.5,\n        \"b48\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in this paper is the dependency of Batch Normalization (BN) on a sufficiently large batch size, which can limit its effectiveness in memory-constrained environments and restrict the exploration of higher-capacity models. The paper proposes Group Normalization (GN) as an alternative normalization method that is independent of batch sizes and provides stable performance across a wide range of batch sizes. The proposed GN method is inspired by classical group-wise features like SIFT and HOG and aims to address the drawbacks of BN in training deep neural networks.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b2\": 1.0,\n    \"b13\": 0.9,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b19\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of Batch Normalization (BN) in deep learning, particularly its dependency on large batch sizes which can be memory-consuming and restrict model design. The proposed solution is Group Normalization (GN), which normalizes features within groups of channels, making its computation independent of batch size. This method shows improved performance over BN in various tasks including object detection, segmentation, and video classification under different batch size conditions.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b14\": 1.0,\n    \"b0\": 0.9,\n    \"b16\": 0.9,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b6\": 0.8,\n    \"b27\": 0.7,\n    \"b19\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.6,\n    \"b35\": 0.6,\n    \"b11\": 0.5,\n    \"b12\": 0.5,\n    \"b57\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Batch Normalization (BN) dependency on large batch sizes, which limits model scalability and memory efficiency. The authors propose Group Normalization (GN) as an alternative that normalizes features within groups of channels, making it independent of batch sizes and competitive with BN in various tasks.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b1\": 0.9,\n    \"b2\": 0.9,\n    \"b16\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b6\": 0.7,\n    \"b19\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```"], "599c7945601a182cd262a009": ["```json\n{\n  \"Summary\": \"The paper explores using generative models to improve sample complexity and adaptability to shifting data distributions in text classification tasks. It compares generative and discriminative models based on LSTM architecture and demonstrates the benefits of generative models in continual learning and zero-shot learning scenarios.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b6\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of neural networks in NLP applications, particularly their need for abundant training data and poor generalization to shifting data distributions. The authors propose using generative models to improve sample complexity and adaptability. They compare discriminative and generative models based on LSTM architectures for text classification. The paper finds that while discriminative models achieve lower asymptotic error rates, generative models perform better in low-data regimes and continual learning settings. The novel contributions include empirical validation of generative models\u2019 advantages in certain scenarios and the introduction of 'Shared LSTM' for generative classification.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b2\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b11\": 0.6,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of neural networks requiring extensive training data and generalizing poorly to shifting data distributions. It explores the use of generative models to improve sample complexity and adaptability to new data distributions, contrasting these with discriminative models. The proposed approach focuses on text classification using LSTM-based models, with both generative and discriminative variants. The generative models, including shared and independent LSTMs, are shown to be more effective in scenarios with limited data and continual learning settings.\",\n  \"Direct Inspiration\": [\"b10\", \"b14\"],\n  \"Indirect Inspiration\": [\"b1\", \"b6\", \"b2\"],\n  \"Other Inspiration\": [\"b3\", \"b11\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of training neural network models for natural language processing with limited data and shifting data distributions. It proposes generative models as a solution to these issues, comparing them with discriminative models based on the same LSTM architecture for text classification tasks. The paper highlights the sample complexity and adaptability of generative models, particularly in scenarios like continual learning and zero-shot learning.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b14\": 0.9,\n    \"b6\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b18\": 0.4,\n    \"b2\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of poor generalization of neural networks when data distribution shifts and the necessity of a substantial amount of training data. It proposes using generative models to improve sample complexity and adaptability to shifting data distributions. The paper compares discriminative and generative models, particularly focusing on LSTM-based architectures for text classification, examining their performance in single-task continual learning and zero-shot learning scenarios.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.8,\n    \"b3\": 0.9,\n    \"b6\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b18\": 0.7\n  }\n}\n```"], "5b67b46b17c44aac1c861edd": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is combining the strengths of RNNs and CNNs to effectively handle text categorization, capturing both long-term dependencies and local position-invariant features. The proposed algorithm introduces the Disconnected Recurrent Neural Network (DRNN), which integrates position-invariance into RNN by limiting the transmission step length, effectively combining the strengths of both RNN and CNN. This approach mitigates the burden of modeling entire sequences and maintains position-invariance through max pooling.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.95,\n    \"b21\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.85,\n    \"b5\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.75,\n    \"b28\": 0.70,\n    \"b12\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of text categorization using neural networks, particularly focusing on the limitations of RNNs in capturing key phrase information due to their reliance on the entire sequence. The authors propose a novel model called Disconnected Recurrent Neural Network (DRNN) which incorporates position-invariance and aims to capture both long-term dependencies and local information effectively.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b21\": 1.0,\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b30\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating position-invariance into Recurrent Neural Networks (RNN) for text categorization while capturing both long-term dependencies and local information. The novel model proposed is the Disconnected Recurrent Neural Network (DRNN), which limits information transmission to a fixed window size to achieve position-invariance and alleviate the burden of modeling the entire sequence.\",\n  \"Direct Inspiration\": [\"b20\", \"b21\"],\n  \"Indirect Inspiration\": [\"b3\", \"b31\", \"b4\", \"b36\"],\n  \"Other Inspiration\": [\"b9\", \"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency of Recurrent Neural Networks (RNN) in capturing key information for text categorization due to their reliance on modeling entire sequences, which can be burdensome and neglect important parts. The proposed solution is the Disconnected Recurrent Neural Network (DRNN), which incorporates position-invariance into RNNs by limiting the maximal transmission step length to a fixed value k. This allows the representation at each step to depend on only the previous k-1 words and the current word. The model combines the benefits of RNNs and Convolutional Neural Networks (CNNs), aiming to capture both long-term dependencies and local information effectively.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b31\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating position-invariance into Recurrent Neural Networks (RNN) for text categorization, proposing a novel model called Disconnected Recurrent Neural Network (DRNN). DRNN aims to capture both long-term dependencies and local information by limiting the maximal transmission step length and using max pooling.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b21\": 0.9,\n    \"b36\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```"], "5db929ff47c8f766461fd7e4": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot learning, particularly for text classification, by proposing Induction Networks. These networks use a dynamic routing process inspired by capsule networks to induce class-level representations from few examples. The method aims to generalize well to unseen classes and is demonstrated to outperform existing models on certain datasets.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b18\": 0.85,\n    \"b20\": 0.85,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b12\": 0.7,\n    \"b14\": 0.7,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot learning, particularly in text classification, where there is a scarcity of labeled data. The proposed Induction Networks model aims to induce class-level representations from a small support set using dynamic routing inspired by capsule networks. The model consists of an Encoder Module, Induction Module, and Relation Module, which work together to classify new classes with few examples by learning generalized representations and minimizing sample-level noise.\",\n  \"Direct Inspiration\": [\n    \"b4\",\n    \"b16\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b18\",\n    \"b20\"\n  ],\n  \"Other Inspiration\": [\n    \"b3\",\n    \"b12\",\n    \"b14\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of few-shot learning in text classification, specifically the issue of sample-wise diversity and the need for class-level representation in small support sets. The proposed Induction Networks utilize a dynamic routing process inspired by capsule networks to achieve robust and generalizable class-level representations.\",\n    \"Direct Inspiration\": {\n        \"b16\": 1,\n        \"b4\": 0.9,\n        \"b18\": 0.8,\n        \"b20\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.7,\n        \"b12\": 0.7,\n        \"b14\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.6,\n        \"b17\": 0.6,\n        \"b10\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot learning in text classification by introducing Induction Networks. The primary goal is to create class-level representations from small support sets using a dynamic routing method inspired by capsule networks. This model includes an Encoder Module, Induction Module, and Relation Module, achieving state-of-the-art performance on two datasets.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.95,\n    \"b4\": 0.85,\n    \"b21\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.75,\n    \"b20\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.55,\n    \"b3\": 0.60,\n    \"b12\": 0.50\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the few-shot text classification problem by proposing Induction Networks. The challenges addressed include dealing with sample-wise diversity, generalizing to unseen classes, and mitigating sample-level noise. The paper introduces a dynamic routing induction method inspired by capsule networks.\",\n  \"Direct Inspiration\": [\"b16\"],\n  \"Indirect Inspiration\": [\"b4\", \"b7\", \"b21\"],\n  \"Other Inspiration\": [\"b3\", \"b12\", \"b14\", \"b18\", \"b20\"]\n}\n```"], "5db929e947c8f766461fd005": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the difficulty of text classification models to learn semantic spaces with limited labeled data. The proposed solution, Hierarchical Attention Prototypical Networks (HAPN), employs three levels of attention mechanisms (feature, word, and instance level) to improve the discriminative power and expressiveness of prototypes in few-shot learning scenarios. The model is shown to achieve state-of-the-art performance on FewRel and CSID datasets and is applied to enhance intention detection in chatbots.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b3\": 0.9,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b8\": 0.7,\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot text classification by proposing Hierarchical Attention Prototypical Networks (HAPN). The novel approach incorporates attention mechanisms at the feature, word, and instance levels to improve prototype discrimination and expressiveness, thereby enhancing classification performance. The model is validated on FewRel and CSID datasets, achieving state-of-the-art results.\",\n  \"Direct Inspiration\": [\n    \"b13\",\n    \"b5\",\n    \"b3\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b7\",\n    \"b8\",\n    \"b17\",\n    \"b21\"\n  ],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of few-shot text classification, which is difficult due to limited labeled data. The authors propose Hierarchical Attention Prototypical Networks (HAPN) that uses attention mechanisms at the feature, word, and instance levels to improve classification performance. The model is applied to intention detection in chatbots, demonstrating its extensibility, speed, and enhanced performance.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1,\n        \"b5\": 0.9,\n        \"b3\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b17\": 0.8,\n        \"b21\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.7,\n        \"b1\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the difficulty of learning semantic space with limited labeled data in text classification tasks. The authors propose hierarchical attention prototypical networks (HAPN) to enhance few-shot text classification by using attention mechanisms at feature, word, and instance levels. This approach aims to create more discriminative class prototypes by reducing the adverse effects of noise and enhancing important features.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a hierarchical attention prototypical network (HAPN) to tackle the challenge of few-shot text classification, where limited labeled data is available. The proposed method uses attention mechanisms at three levels: feature, word, and instance, to improve the discrimination and expressiveness of prototypes. The method is applied to intention detection in chatbots, achieving state-of-the-art performance.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b17\": 0.85,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b8\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.65,\n    \"b22\": 0.6\n  }\n}\n```"], "573698456e3b12023e70ee1b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of facial expression recognition in real-world conditions with limited and diverse data. The authors propose a two-stage supervised fine-tuning approach for deep CNN architectures, starting with pre-training on the ImageNet dataset, followed by fine-tuning on the FER-2013 dataset and then on the EmotiW dataset.\",\n  \"Direct Inspiration\": [\"b8\", \"b9\", \"b17\", \"b26\"],\n  \"Indirect Inspiration\": [\"b2\", \"b10\", \"b12\", \"b19\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the real-life imaging conditions and the small dataset size in the SFEW sub-challenge of the EmotiW contest, which makes it prone to overfitting. The algorithm proposed involves a two-stage supervised fine-tuning approach for deep CNN architectures, starting from a pre-training on the ImageNet dataset, followed by fine-tuning on the FER-2013 dataset and then the EmotiW dataset.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b11\": 0.9,\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b26\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is performing facial expression analysis on small, challenging datasets such as the EmotiW dataset. The proposed algorithm involves a two-stage supervised fine-tuning of deep CNN architectures using transfer learning, starting with the ImageNet dataset and then using the FER-2013 dataset before the final fine-tuning on the EmotiW training set.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b10\": 0.90,\n    \"b12\": 0.90,\n    \"b19\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b9\": 0.75,\n    \"b26\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.65,\n    \"b14\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the difficulty of training high-capacity classifiers like CNNs on small datasets such as EmotiW, which is prone to overfitting. The proposed solution is a two-stage supervised fine-tuning approach using deep CNN architectures, starting with pre-training on a large dataset (ImageNet) followed by fine-tuning on the FER-2013 facial expression dataset and finally on the EmotiW dataset.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b9\": 0.9,\n    \"b10\": 1.0,\n    \"b11\": 0.9,\n    \"b14\": 0.8,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b12\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of facial expression analysis under real-life imaging conditions and small dataset sizes. The proposed approach leverages transfer learning and a two-stage supervised fine-tuning of deep CNN architectures to improve emotion recognition performance.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.95,\n    \"b11\": 0.90,\n    \"b14\": 0.88\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.85,\n    \"b8\": 0.80,\n    \"b9\": 0.80,\n    \"b26\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.75,\n    \"b12\": 0.70,\n    \"b19\": 0.70\n  }\n}\n```"], "5c8c52bc4895d9cbc6ddad8d": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"High intra-class variations and high inter-class similarities caused by diversity in head pose, illumination, occlusions, and personal attributes.\",\n    \"novel_methods\": \"Proposing an island loss to simultaneously compress each cluster and push cluster centers apart as isolated 'islands' to enhance discriminative power of learned deep features.\"\n  },\n  \"Direct Inspiration\": {\n    \"b43\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b45\": 0.8,\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b23\": 0.6,\n    \"b49\": 0.6,\n    \"b30\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of facial expression recognition in the wild, specifically focusing on high intra-class variations and high inter-class similarities. The authors propose a novel loss function called 'island loss' to enhance the discriminative power of learned deep features in a CNN, aiming to reduce intra-class variations and increase inter-class distances. They develop an IL-CNN architecture incorporating this island loss and evaluate its effectiveness on several facial expression databases, demonstrating superior performance compared to baseline CNN models and state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b43\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b45\": 0.8,\n    \"b2\": 0.8,\n    \"b15\": 0.7,\n    \"b47\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b25\": 0.6,\n    \"b12\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high intra-class variations and high inter-class similarities in facial expression recognition in the wild. It proposes a novel island loss function to enhance the discriminative power of deep features by compressing intra-class variations and increasing inter-class distances. The proposed IL-CNN architecture incorporates this island loss to achieve superior recognition performance compared to traditional softmax loss and center loss approaches.\",\n  \"Direct Inspiration\": {\n    \"b43\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b47\": 0.6,\n    \"b29\": 0.6,\n    \"b7\": 0.6,\n    \"b45\": 0.6,\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of facial expression recognition in the wild, which suffers from high intra-class variations and high inter-class similarities. To tackle these, the authors propose a novel island loss that simultaneously reduces intra-class variations and increases inter-class distances. They develop an IL-CNN incorporating this island loss and demonstrate its effectiveness on several benchmark datasets.\",\n  \"Direct Inspiration\": {\n    \"b43\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b45\": 0.8,\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b23\": 0.7,\n    \"b49\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of facial expression recognition, particularly the high intra-class variations and inter-class similarities due to various factors such as head pose, illumination, and occlusions. It proposes a novel method called island loss to enhance the discriminative power of deep features by simultaneously reducing intra-class variations and increasing inter-class differences. The proposed IL-CNN model is evaluated on multiple facial expression databases and shows superior performance over traditional methods.\",\n    \"Direct Inspiration\": {\n        \"b43\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.9,\n        \"b23\": 0.9,\n        \"b5\": 0.8,\n        \"b49\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.7,\n        \"b45\": 0.7,\n        \"b2\": 0.7\n    }\n}\n```"], "58437785ac44360f108432a7": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of video-based emotion recognition by proposing a hybrid approach that combines CNN-RNN and C3D networks. The main contribution is the development of this hybrid model, which leverages the strengths of both LSTM networks for sequence processing and C3D networks for capturing spatio-temporal features.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b22\": 0.7,\n    \"b21\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b25\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of video-based emotion recognition by proposing a hybrid CNN-RNN and C3D network. The main contributions include combining the spatial and temporal modeling capabilities of LSTM networks with the spatio-temporal modeling of C3D networks, significantly improving recognition accuracy. The paper is inspired by previous work on CNN-RNN and C3D networks for action recognition and leverages them in the context of emotion recognition.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.95,\n    \"b7\": 0.9,\n    \"b4\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b26\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b6\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of video-based emotion recognition. It proposes a hybrid model combining CNN-RNN and C3D networks to capture both spatial and temporal information from video sequences. The paper highlights the limitations of traditional CNNs in handling temporal data and demonstrates the effectiveness of the proposed hybrid approach, achieving state-of-the-art results in the EmotiW 2016 challenge.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b22\": 0.7,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of video-based emotion recognition by proposing a hybrid CNN-RNN and C3D network. The primary contribution is the integration of these two models to improve performance in the EmotiW challenge. The use of LSTM for sequence processing and C3D for spatio-temporal modeling are key components.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b7\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.9,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper involve improving the performance of emotion recognition in videos by addressing the limitations of traditional CNNs which handle only spatial information and neglect the temporal information. The proposed algorithm is a hybrid model combining CNN-RNN and C3D networks to capture both spatial and temporal information. The main contribution is the development of this hybrid approach to achieve state-of-the-art results in the EmotiW 2016 challenge.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9,\n        \"b4\": 0.9,\n        \"b5\": 0.8,\n        \"b7\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.6,\n        \"b6\": 0.5\n    },\n    \"Other Inspiration\": {}\n}\n```"], "5e09a801df1a9c0c41680233": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the removal of reflections from images taken through transparent glass. The authors propose a novel weakly supervised learning framework that jointly learns the process of generating and separating reflections. This approach is contrasted with traditional methods that treat these processes independently, leading to improved robustness and reflection removal performance. The proposed model incorporates entanglement and disentanglement mapping mechanisms, gradient constraints, and a multi-task learning structure for better edge map estimation.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b23\": 0.9,\n    \"b29\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b25\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b28\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of removing reflections from images taken through transparent glass, which degrade visual quality and hinder computer vision systems. The authors propose a weakly supervised learning framework that jointly learns the generation and separation of reflections, leveraging unpaired data to improve robustness. The method incorporates entanglement and disentanglement mapping mechanisms, gradient constraints, and a multi-task learning structure to achieve better reflection removal performance.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b23\": 0.9,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of removing reflections from images taken through transparent glass. The proposed method utilizes a weakly supervised learning framework and incorporates joint learning of image generation and separation to improve robustness. The key contributions include leveraging deep learning, multi-task learning for edge map estimation, and introducing gradient constraints.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b23\": 1.0,\n    \"b29\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b28\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b0\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the presence of reflections in photos taken through transparent glass, which degrade visual quality and hinder computer vision systems. The proposed algorithm leverages a weakly supervised learning framework to jointly learn the processes of generating and separating reflections, improving the robustness and effectiveness of reflection removal. The model involves entanglement and disentanglement mapping mechanisms, multi-task learning, and gradient constraints to achieve realistic generation results and clearer separation results.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b23\": 0.9,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b16\": 0.7,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b0\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of removing reflections from images taken through transparent glass. The proposed method integrates the processes of image generation and separation in a joint learning framework, leveraging deep learning and adversarial networks to improve robustness and effectiveness in reflection removal.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b4\", \"b23\", \"b29\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b25\", \"b16\", \"b28\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b22\", \"b27\"]\n  }\n}\n```"], "5db9298547c8f766461f8b65": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"Facial expression recognition (FER) in the wild, dealing with issues like large head pose, illumination variance, occlusion, and motion blur.\",\n    \"algorithm\": \"Proposed Frame Attention Networks (FAN) with self-attention and relation-attention modules to adaptively aggregate frame features for video-based FER.\"\n  },\n  \"Direct Inspiration\": [\"b13\", \"b14\", \"b15\"],\n  \"Indirect Inspiration\": [\"b20\", \"b21\"],\n  \"Other Inspiration\": [\"b5\", \"b6\", \"b11\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of video-based facial expression recognition (FER) in the wild, such as large head pose, illumination variance, occlusion, and motion blur. The authors propose the Frame Attention Networks (FAN), inspired by attention mechanisms and neural aggregation networks, to adaptively aggregate frame features for improving FER performance.\",\n  \"Direct Inspiration\": [\"b13\", \"b14\", \"b15\"],\n  \"Indirect Inspiration\": [\"b5\", \"b6\", \"b20\", \"b21\"],\n  \"Other Inspiration\": [\"b3\", \"b11\", \"b23\", \"b30\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of video-based facial expression recognition (FER) in the wild, which is complicated by factors like head pose, illumination variance, occlusion, and motion blur. The proposed solution, Frame Attention Networks (FAN), leverages self-attention and relation-attention mechanisms to adaptively aggregate frame features for improved FER performance.\",\n  \"Direct Inspiration\": [\"b13\", \"b14\", \"b15\"],\n  \"Indirect Inspiration\": [\"b5\", \"b6\", \"b10\", \"b11\"],\n  \"Other Inspiration\": [\"b3\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is improving video-based facial expression recognition (FER) in the wild, addressing issues such as large head pose, illumination variance, occlusion, and motion blur. The proposed solution, Frame Attention Networks (FAN), introduces a novel attention mechanism to adaptively aggregate frame features, enhancing FER performance by utilizing self-attention and relation-attention kernels.\",\n  \"Direct Inspiration\": [\"b13\", \"b14\"],\n  \"Indirect Inspiration\": [\"b15\"],\n  \"Other Inspiration\": [\"b5\", \"b6\", \"b20\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on addressing the challenges in automatic facial expression recognition (FER) in videos, particularly in the wild, where factors like head pose, illumination variance, and occlusion complicate the task. The proposed solution is the Frame Attention Networks (FAN) which adaptively aggregates frame features using self-attention and relation-attention mechanisms.\",\n  \"Direct Inspiration\": [\"b13\", \"b14\", \"b15\"],\n  \"Indirect Inspiration\": [\"b3\", \"b5\", \"b6\"],\n  \"Other Inspiration\": [\"b20\", \"b21\"]\n}\n```"], "5d971ab13a55ac1c613f54e6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of visual sentiment analysis, which is more complex than textual sentiment analysis due to its implicit and subjective nature. The authors propose a Multi-Attentive Pyramidal (MAP) model that extracts visual features from multiple local regions at various scales and captures their associations to improve sentiment representation. The model incorporates a self-attention mechanism inspired by the Transformer model to enhance the associations between local features.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Recognizing emotions expressed in visual content\",\n      \"Analyzing fine-grained emotions in visual sentiment analysis\",\n      \"Extracting and associating sentiment features from local regions rather than entire images\"\n    ],\n    \"Inspirations\": [\n      \"Use of convolutional neural networks (CNNs) for automatic feature extraction\",\n      \"Psychological findings on the importance of local regions in visual sentiment\",\n      \"Recent Transformer model for capturing associations between features\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"References\": [\"b22\", \"b21\"]\n  },\n  \"Indirect Inspiration\": {\n    \"References\": [\"b19\", \"b14\", \"b10\"]\n  },\n  \"Other Inspiration\": {\n    \"References\": [\"b12\", \"b13\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of visual sentiment analysis, particularly the difficulty of analyzing fine-grained emotions from visual content. The proposed solution is a Multi-Attentive Pyramidal (MAP) model that combines pyramidal segmentation and self-attention mechanisms to extract and associate local visual features at multiple scales for improved sentiment representation and classification.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of visual sentiment analysis, particularly the difficulty in analyzing fine-grained emotions and the limitations of traditional hand-crafted features. The proposed Multi-Attentive Pyramidal (MAP) model uses a combination of CNNs and self-attention mechanisms inspired by the Transformer model to extract and associate visual features from multi-scale local regions of an image for better sentiment representation.\",\n  \"Direct Inspiration\": [\"b22\"],\n  \"Indirect Inspiration\": [\"b19\", \"b21\"],\n  \"Other Inspiration\": [\"b10\", \"b14\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of visual sentiment analysis, which involves recognizing emotions expressed in visual content such as images and videos. The authors propose a Multi-Attentive Pyramidal (MAP) model that extracts visual features from multiple local regions at different scales of an image and captures their associations using a self-attention mechanism. This approach aims to improve the accuracy and robustness of sentiment representation by focusing on local regions rather than the entire image.\",\n    \"Direct Inspiration\": {\n        \"b22\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.9,\n        \"b21\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.7,\n        \"b14\": 0.6\n    }\n}\n```"], "57d063e8ac443673542950ad": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of image sentiment analysis by proposing a novel method that utilizes latent correlations among visual, textual, and sentiment views of training images. The main contributions include exploiting multiple views for sentiment classification and demonstrating the effectiveness of this approach through experiments on large-scale image datasets.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b16\": 1,\n    \"b18\": 1,\n    \"b19\": 1,\n    \"b20\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b26\": 0.7,\n    \"b25\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper include the affective gap between low-level visual features and high-level concepts of human sentiments, and the need to effectively utilize visual, textual, and sentiment views for image sentiment analysis.\",\n    \"Inspiration\": \"The authors are inspired by prior studies on image annotation that utilize textual features to improve image content recognition, and they adopt multi-view canonical correlation analysis (CCA) with explicit feature maps to capture latent correlations among visual, textual, and sentiment views.\"\n  },\n  \"Direct Inspiration\": [\n    \"b15\",\n    \"b16\",\n    \"b18\",\n    \"b19\",\n    \"b20\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b17\",\n    \"b26\"\n  ],\n  \"Other Inspiration\": [\n    \"b11\",\n    \"b12\",\n    \"b14\",\n    \"b25\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the effective classification of image sentiment by leveraging the latent correlations among visual, textual, and sentiment views. The authors propose a novel method that uses multi-view canonical correlation analysis (CCA) combined with explicit feature maps to learn a latent embedding space. This space maximizes the correlations among these views to train a sentiment classifier that outperforms conventional methods.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b16\": 0.9,\n    \"b18\": 0.8,\n    \"b19\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b14\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in this paper is the difficulty in associating low-level visual features with high-level human sentiments for image sentiment analysis. The proposed algorithm introduces a novel multi-view approach that uses latent correlations among visual, textual, and sentiment views of training images to create a more effective sentiment classifier. Key inspirations include using textual information alongside visual data, and employing a multi-view canonical correlation analysis (CCA) framework with explicit feature mappings to capture nonlinear relationships.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b16\": 1.0,\n    \"b18\": 1.0,\n    \"b19\": 1.0,\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b14\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the difficulty of associating low-level visual features with high-level human sentiments, and the need to effectively utilize textual information associated with images for sentiment analysis. The proposed method introduces a novel approach that leverages latent correlations among visual, textual, and sentiment views using multi-view canonical correlation analysis (CCA) with explicit feature mappings to train a sentiment classifier.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b16\": 0.95,\n    \"b18\": 0.85,\n    \"b19\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.65,\n    \"b14\": 0.7\n  }\n}\n```"], "5f9be24691e011dcf482d8d6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving datacenter power utilization and efficiency by proposing a system that leverages VM performance criticality and utilization predictions to increase oversubscription while minimizing performance impacts. It introduces a novel pattern-matching algorithm and machine learning model for predicting VM criticality and utilization, a VM placement policy, and a per-VM power capping system.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b8\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving power utilization in datacenters by proposing predictive models and algorithms for VM scheduling, power capping, and oversubscription. It introduces a robust pattern-matching algorithm and a machine learning model to predict workload performance criticality and VM CPU utilization, which are then used to optimize VM placement and power management to reduce costs and improve efficiency.\",\n  \"Direct Inspiration\": {\n    \"Inspired by\": [\"b31\"],\n    \"Motivated by\": [\"b5\"]\n  },\n  \"Indirect Inspiration\": {\n    \"Inspired us\": [\"b8\"],\n    \"Motivated us\": [\"b6\"]\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing power utilization in datacenters to avoid unnecessary capital costs due to under-utilization of power resources. The novel contribution includes an algorithm and ML model for predicting VM performance criticality and utilization, a VM placement policy to minimize capping events, a per-VM power capping system, and a strategy to increase oversubscription based on these predictions.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b31\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of power efficiency and cost in datacenters by proposing a method for increased oversubscription through VM scheduling and power management based on predictions of workload performance criticality and CPU utilization. The main contributions include a new pattern-matching algorithm and ML model for predicting workload criticality, a VM placement policy, a per-VM power capping system, and implementation results demonstrating significant cost savings in Azure's infrastructure.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of power under-utilization in datacenters due to conservative power provisioning, proposing a system that predicts VM performance criticality and CPU utilization to increase oversubscription safely. The key contributions include an algorithm and ML model for predicting performance criticality, a VM placement policy, a per-VM power capping system, and a strategy to increase oversubscription while maintaining performance.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```"], "5fd8acf991e0119b22c1f38d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of long sequence time-series forecasting (LSTF) by proposing an improved Transformer model called Informer. The main challenges include enhancing prediction capacity for long sequences, reducing computational and memory inefficiencies associated with self-attention mechanisms, and improving inference speed. The novel contributions include the ProbSparse Self-attention mechanism, self-attention distilling in the encoder, and a generative-style decoder for efficient long sequence prediction.\",\n  \"Direct Inspiration\": [\"b5\", \"b2\", \"b13\", \"b15\"],\n  \"Indirect Inspiration\": [\"b8\", \"b22\", \"b30\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are enhancing the prediction capacity to meet the increasingly long sequences demand in long sequence time-series forecasting (LSTF), requiring extraordinary long-range alignment ability and efficient operations on long sequence inputs and outputs. The paper proposes the Informer model to tackle these challenges by improving the efficiency of the Transformer models through a ProbSparse self-attention mechanism, self-attention distilling, and a generative-style decoder.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b29\": 0.85,\n    \"b15\": 0.8,\n    \"b13\": 0.75,\n    \"b8\": 0.7,\n    \"b2\": 0.65\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.6,\n    \"b32\": 0.55,\n    \"b4\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.45,\n    \"b7\": 0.4,\n    \"b14\": 0.35,\n    \"b10\": 0.3,\n    \"b1\": 0.25\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of long sequence time-series forecasting (LSTF) by enhancing prediction capacity to handle increasingly long sequences. The proposed algorithm, Informer, seeks to improve Transformer models to be computation, memory, and architecture efficient while maintaining high prediction capacity. The paper introduces a novel ProbSparse self-attention mechanism, self-attention distilling, and a generative style decoder.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b5\": 0.8,\n        \"b13\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.6,\n        \"b15\": 0.6,\n        \"b22\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.3,\n        \"b29\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing prediction capacity for long sequence time-series forecasting (LSTF) by improving the efficiency of the Transformer model. It identifies limitations of existing methods, such as quadratic computation and memory usage, and proposes the Informer model, which includes a ProbSparse self-attention mechanism, self-attention distilling, and a generative-style decoder to tackle these issues.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b5\": 0.8,\n    \"b13\": 0.85,\n    \"b15\": 0.8,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b22\": 0.75,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.65,\n    \"b9\": 0.65,\n    \"b11\": 0.6,\n    \"b21\": 0.6,\n    \"b28\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of long sequence time-series forecasting (LSTF) by proposing a new model, Informer, to enhance prediction capacity and efficiency. The key challenges identified include the quadratic computation complexity of self-attention mechanisms, memory bottlenecks with long sequence inputs, and slow inference speeds for long outputs. The proposed Informer model leverages a ProbSparse self-attention mechanism, self-attention distilling, and a generative-style decoder to overcome these challenges.\",\n    \"Direct Inspiration\": {\n        \"b5\": 0.9,\n        \"b2\": 0.85,\n        \"b13\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b29\": 0.7,\n        \"b15\": 0.65,\n        \"b8\": 0.6,\n        \"b22\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b32\": 0.55,\n        \"b11\": 0.55\n    }\n}\n```"], "5c99fa753cb210d2719cc8de": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of preserving endangered languages, specifically focusing on the Tujia language. It proposes an automatic speech recognition system using cross-language transfer learning and deep learning techniques, particularly convolutional neural networks (CNN) and bi-directional long short-term memory networks (BiLSTM) combined with connectionist temporal classification (CTC). The method leverages Chinese speech datasets to compensate for the lack of Tujia language data.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b35\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper include the preservation and recognition of endangered languages, specifically low-resource languages like the Tujia language. The paper aims to develop an automatic speech recognition system using deep learning techniques to address these challenges.\",\n    \"inspirations\": \"The paper takes inspiration from recent advancements in end-to-end speech recognition technology and transfer learning methods. It also builds on previous research in cross-language speech recognition and the use of convolutional neural networks (CNN) and bi-directional long short-term memory (BiLSTM) networks for feature extraction and sequence modeling.\"\n  },\n  \"Direct Inspiration\": [\n    \"b2\",\n    \"b3\",\n    \"b4\",\n    \"b5\",\n    \"b6\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b7\",\n    \"b8\",\n    \"b9\",\n    \"b10\",\n    \"b12\",\n    \"b13\",\n    \"b14\"\n  ],\n  \"Other Inspiration\": [\n    \"b28\",\n    \"b33\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": {\n        \"Challenges\": \"The primary challenges outlined in the paper are the low-resource nature of endangered languages, particularly the Tujia language, and the requirement for a speech recognition system that can operate effectively despite limited data. The complexity of speech signals and the need for efficient feature extraction and sequence-to-sequence classification are emphasized.\",\n        \"Inspirations\": \"The authors are inspired by recent advances in deep learning, end-to-end speech recognition, and transfer learning techniques, aiming to leverage these methods to improve low-resource speech recognition.\"\n    },\n    \"Direct Inspiration\": {\n        \"b9\": 0.9,\n        \"b10\": 0.9,\n        \"b12\": 0.8,\n        \"b13\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b2\": 0.7,\n        \"b3\": 0.7,\n        \"b4\": 0.7,\n        \"b5\": 0.7,\n        \"b6\": 0.7,\n        \"b7\": 0.7,\n        \"b8\": 0.7,\n        \"b14\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b28\": 0.6,\n        \"b33\": 0.6,\n        \"b35\": 0.6,\n        \"b36\": 0.6,\n        \"b37\": 0.6,\n        \"b38\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of automatic speech recognition for the endangered Tujia language, which is low-resource and has limited recordings. The authors propose a novel method that combines convolutional neural networks for feature extraction and bi-directional long short-term memory networks with cross-language weight sharing and transfer learning to improve recognition accuracy.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.7,\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b33\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of low-resource automatic speech recognition (ASR) for the endangered Tujia language. It proposes a model that integrates convolutional neural networks (CNN) for feature extraction and bi-directional long short-term memory (BiLSTM) networks for cross-language hidden layer weight sharing. The approach leverages transfer learning to adapt a pre-trained model on Chinese speech datasets to the Tujia language, aiming to reduce labor and time costs associated with preserving the Tujia language.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.9,\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.9,\n    \"b9\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b11\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b27\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b36\": 0.6,\n    \"b37\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```"], "5f350c4191e011d4254d01da": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of building accurate RNN-T models for low-resource languages using transfer learning (TL) methods. The authors propose a two-stage TL approach to improve the performance and training convergence of RNN-T models for low-resource languages like Hindi, leveraging models trained on high-resource languages like American English.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b14\": 1.0,\n    \"b15\": 1.0,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b21\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of building accurate automatic speech recognition (ASR) systems for low-resource languages using transfer learning (TL) methods. It specifically focuses on improving the performance of Recurrent Neural Network Transducer (RNN-T) models by leveraging models trained on high-resource languages like American English.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.9,\n    \"b15\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.8,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges of building accurate automatic speech recognition (ASR) systems, particularly for low-resource languages, and proposes transfer learning (TL) methods to improve Recurrent Neural Network Transducer (RNN-T) models. The main contributions include comparing different TL strategies in the RNN-T framework and proposing a two-stage TL approach to enhance ASR accuracy and training stability.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b14\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the difficulty in training accurate RNN-T models for low-resource languages due to limited and diverse training data. The paper proposes a two-stage transfer learning (TL) approach to improve the performance of low-resource RNN-T models by leveraging models trained on high-resource languages like American English. The goal is to achieve significant word error rate (WERR) reduction and faster convergence.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of building accurate automatic speech recognition (ASR) systems for low-resource languages using transfer learning (TL) methods. The authors propose a two-stage TL approach to improve RNN-T models for low-resource languages, demonstrating significant improvements in word error rate and training convergence.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b14\": 0.9,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b1\": 0.8,\n    \"b20\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6\n  }\n}\n```"], "5ecbc7639fced0a24b502b60": ["```json\n{\n  \"Summary\": \"The paper tackles the challenge of improving Automatic Speech Recognition (ASR) in low-resource environments by introducing three main methods: Mono-and-Triphone Learning (MAT) based on multitask learning, Soft One-hot Label (SOL) encoding method, and combining different acoustic features including FMLLR. These approaches aim to prevent overfitting, improve generalization, and enhance the overall performance of ASR systems.\",\n  \"Direct Inspiration\": {\n    \"b29\": 0.95,\n    \"b39\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b20\": 0.75,\n    \"b21\": 0.75,\n    \"b22\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.65,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving Automatic Speech Recognition (ASR) systems, particularly in low-resource environments. It proposes three novel methods: Mono-and-Triphone learning based on multitask learning, Soft One-hot Label (SOL) encoding based on Gaussian distribution, and the combination of different acoustic features for the neural networks.\",\n  \"Direct Inspiration\": [\"b29\", \"b39\"],\n  \"Indirect Inspiration\": [\"b27\", \"b28\", \"b31\", \"b32\"],\n  \"Other Inspiration\": [\"b4\", \"b6\", \"b8\"]\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses challenges in Automatic Speech Recognition (ASR) under low-resource conditions. The authors propose three main methods: Mono-and-Triphone Learning (MAT) based on multitask learning, Soft One-hot Label (SOL) to regularize classifier layers, and combining traditional acoustic features with FMLLR features to improve model generalization.\",\n  \"Direct Inspiration\": {\n    \"b29\": 0.9,\n    \"b39\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of acoustic models in low-resource environments for Automatic Speech Recognition (ASR). The key contributions include the Mono-and-Triphone learning based on multitask learning (MAT) and a new label encoding method called Soft One-hot Label (SOL). These methods aim to improve generalization and prevent overfitting in neural networks used for speech recognition.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b39\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b31\": 0.85,\n    \"b32\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.75,\n    \"b8\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in Automatic Speech Recognition (ASR) in low-resource environments. It proposes a multitask learning approach combining monophone and triphone targets (MAT), a Soft One-hot Label (SOL) encoding method to reduce overconfidence and overfitting, and the combination of different acoustic features to improve model generalization.\",\n  \"Direct Inspiration\": [\"b29\", \"b39\"],\n  \"Indirect Inspiration\": [\"b4\", \"b31\", \"b32\"],\n  \"Other Inspiration\": [\"b6\", \"b7\", \"b8\"]\n}\n```"], "5ecfae0d9e795eb20a615048": ["```json\n{\n  \"Summary\": \"The paper introduces LRSpeech, a TTS and ASR system designed for extremely low-resource settings, aiming to support rare languages with minimal data collection costs and high accuracy. The main challenges addressed include the high cost of data collection and the need for accuracy in industrial deployment. LRSpeech leverages transfer learning, dual transformation, and knowledge distillation to overcome these challenges.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1.0,\n    \"b17\": 1.0,\n    \"b36\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.7,\n    \"b33\": 0.7,\n    \"b45\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of developing TTS and ASR systems for rare languages under extremely low-resource settings. The proposed system, LRSpeech, aims to minimize data collection costs while ensuring high accuracy for industrial deployment. Key techniques include transfer learning from rich-resource languages, iterative accuracy boosting between TTS and ASR through dual transformation, and knowledge distillation.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1,\n    \"b36\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b17\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b29\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a TTS and ASR system under extremely low-resource settings, with the goal of supporting rare languages at low data collection costs while maintaining high accuracy. The proposed LRSpeech system employs techniques such as transfer learning, dual transformation, and knowledge distillation to achieve these objectives.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.9,\n    \"b17\": 0.85,\n    \"b36\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b41\": 0.7,\n    \"b45\": 0.65,\n    \"b33\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b37\": 0.6,\n    \"b42\": 0.6,\n    \"b18\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces LRSpeech, a TTS and ASR system designed for extremely low-resource settings to support rare languages with minimal data collection costs while ensuring high accuracy. The proposed approach leverages techniques such as transfer learning from rich-resource languages, iterative accuracy boosting through dual transformation, and knowledge distillation to refine the models.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.95,\n    \"b36\": 0.9,\n    \"b17\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.75,\n    \"b41\": 0.7,\n    \"b5\": 0.65,\n    \"b40\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.55,\n    \"b13\": 0.5,\n    \"b14\": 0.5,\n    \"b37\": 0.5,\n    \"b42\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the development of a TTS and ASR system under extremely low-resource settings to support rare languages with low data collection costs while maintaining high accuracy suitable for industrial deployment. The proposed system, LRSpeech, leverages techniques such as transfer learning, dual transformation between TTS and ASR, and knowledge distillation to achieve its goals.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.95,\n    \"b36\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.75,\n    \"b41\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b24\": 0.70,\n    \"b39\": 0.68\n  }\n}\n```"], "5ec7a32791e0118397f3ee40": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing E2E ASR systems for low-resource languages by leveraging extra text data and cross-lingual transfer learning. It proposes a hybrid Transformer-LSTM architecture that combines the high modeling capacity and fast training of the Transformer with the text-data leverage capability of the LSTM-based decoder.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing end-to-end (E2E) automatic speech recognition (ASR) systems for low-resource languages. It proposes a hybrid Transformer-LSTM architecture, combining the high encoding capacity and fast training of Transformers with the text-data leveraging capabilities of LSTM-based decoders. The approach also incorporates cross-lingual transfer learning to further improve performance.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an end-to-end (E2E) automatic speech recognition (ASR) system for a low-resource language, leveraging a hybrid Transformer-LSTM architecture to combine the high modeling capacity of the Transformer with the ability of LSTM to utilize extra text data. Cross-lingual transfer learning is also employed to further improve performance.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing end-to-end (E2E) automatic speech recognition (ASR) systems for low-resource languages. The proposed solution is a hybrid Transformer-LSTM architecture that leverages the high modeling capacity and fast training of the Transformer encoder and the text data adaptability of the LSTM-based decoder. The approach also incorporates cross-lingual transfer learning to utilize labeled data from resource-rich languages.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of implementing end-to-end (E2E) automatic speech recognition (ASR) for low-resource languages, focusing on hybrid architectures that combine Transformer and LSTM components. The proposed method aims to leverage extra text data and cross-lingual transfer learning to improve ASR performance with limited labeled data.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b0\", \"b5\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b6\", \"b7\", \"b8\", \"b9\", \"b10\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b15\", \"b16\", \"b17\"]\n  }\n}\n```"], "5bdc317017c44a1f58a08086": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting cloud spot prices in the context of dynamic resource allocation by cloud providers. It proposes a model based on hidden Markov models to predict spot prices and optimal bidding strategies to minimize users' costs while ensuring low job interruption probability.\",\n  \"Direct Inspiration\": [\"b8\", \"b9\"],\n  \"Indirect Inspiration\": [\"b13\", \"b14\"],\n  \"Other Inspiration\": [\"b3\", \"b4\", \"b6\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of meeting fluctuating resource demands in cloud infrastructure, specifically through the use of spot pricing to optimize resource utilization and minimize user costs. The authors propose a hidden Markov model to predict spot prices and develop optimal bidding strategies based on this model.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b16\": 0.7,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b4\": 0.5,\n    \"b13\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting and optimizing spot prices in cloud computing to minimize user costs while ensuring resource utilization. It proposes a hidden Markov model to predict spot prices and optimal bidding strategies based on these predictions.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b22\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of predicting future spot prices in cloud computing environments to optimize resource usage and minimize costs. The authors propose a hidden Markov model to capture the dynamics of spot prices and develop optimal bidding strategies based on this model. The approach takes into account the interplay between on-demand and spot markets and aims to fully utilize cloud capacity rather than maximize profit.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1,\n        \"b9\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.8,\n        \"b22\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.5,\n        \"b4\": 0.5,\n        \"b10\": 0.5,\n        \"b31\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively utilizing cloud infrastructure resources by predicting and optimizing spot prices in the cloud spot market. It proposes a hidden Markov model to predict spot prices and uses this model to develop optimal bidding strategies that minimize costs and interruptions for users.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b9\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```"], "555044e045ce0a409eb518a7": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the need for an efficient and truthful online combinatorial auction mechanism for dynamic resource provisioning in cloud computing. The proposed algorithm optimizes system efficiency across multiple time-slots, handles heterogeneous VM types, and ensures a competitive ratio in social welfare. The framework involves a tailored primal-dual algorithm, a randomized subframework for truthfulness, and a binary search algorithm for improved performance.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b5\": 0.7,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include designing an efficient online combinatorial auction mechanism for dynamic resource provisioning in cloud computing. The proposed framework aims to achieve truthfulness, computational efficiency, and a competitive ratio in social welfare. The key components include a tailored primal-dual algorithm, a randomized subframework for truthfulness, and an approximation algorithm for one-shot optimization.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9,\n        \"b7\": 0.8,\n        \"b10\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b8\": 0.65,\n        \"b11\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b20\": 0.55\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the problem of dynamic resource provisioning in cloud computing using an online combinatorial auction framework. The main challenges include optimizing system efficiency over time, modeling dynamic provisioning of heterogeneous VMs, ensuring truthfulness, and achieving a competitive ratio in social welfare.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9,\n        \"b7\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b5\": 0.7,\n        \"b8\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.6,\n        \"b11\": 0.6,\n        \"b21\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include designing a truthful, computationally efficient online combinatorial auction for cloud computing that accounts for dynamic provisioning of heterogeneous VM types and optimizes system efficiency over time. The proposed algorithm includes a primal-dual decomposition approach, a randomized auction subframework for truthfulness, and an approximation algorithm for one-shot optimization.\",\n    \"Direct Inspiration\": {\n        \"b10\": 0.9,\n        \"b11\": 0.8,\n        \"b3\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.75,\n        \"b7\": 0.7,\n        \"b18\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.6,\n        \"b8\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of dynamic resource provisioning and efficient pricing in cloud computing through an online combinatorial auction framework. It introduces a novel auction mechanism that optimizes system efficiency over time, supports heterogeneous VM types, ensures truthfulness, and guarantees a competitive ratio in social welfare.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b10\": 0.85,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b8\": 0.7,\n    \"b11\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b20\": 0.55\n  }\n}\n```"], "57d063f6ac44367354296741": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently managing bandwidth on wide area networks (WANs) by proposing a system called Pretium. Pretium combines traffic engineering (TE) with dynamic pricing to present users with a menu of service levels and corresponding prices. It aims to improve network utilization while managing user incentives and maintaining service guarantees.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b19\": 1.0,\n    \"b21\": 1.0,\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b7\": 0.9,\n    \"b18\": 0.9,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in WAN bandwidth management, focusing on aligning user incentives with network utilization through dynamic pricing and traffic engineering. The proposed system, Pretium, provides real-time price quotes for network services and adapts to changing demand and usage patterns, ensuring efficient resource allocation and meeting service guarantees.\",\n  \"Direct Inspiration\": [\"b5\", \"b7\", \"b18\"],\n  \"Indirect Inspiration\": [\"b17\", \"b19\", \"b21\", \"b33\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in WAN bandwidth management, particularly dealing with user incentives, dynamic pricing, and efficient resource allocation. It proposes Pretium, a system that combines traffic engineering with dynamic pricing to provide guaranteed service levels and optimize overall system efficiency.\",\n  \"Direct Inspiration\": [\"b17\", \"b19\", \"b21\"],\n  \"Indirect Inspiration\": [\"b5\", \"b7\", \"b18\"],\n  \"Other Inspiration\": [\"b28\", \"b33\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in WAN bandwidth management, specifically issues with static pricing models, lack of service guarantees, and the need for dynamic pricing to improve network utilization. The authors propose Pretium, a system that combines traffic engineering with dynamic pricing to present users with price menus that align individual preferences with platform goals, dynamically updating prices and planning to maintain high system efficiency.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b7\": 0.8,\n    \"b18\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b16\": 0.75,\n    \"b21\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b19\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of combining centralized traffic engineering (TE) with dynamic pricing in WANs to better manage user requests and network utilization. The proposed system, Pretium, dynamically updates prices to align user preferences with network goals, optimizes usage-based costs, and ensures service guarantees despite online arrivals and network changes.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b17\", \"b19\", \"b21\", \"b33\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b5\", \"b7\", \"b18\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": []\n  }\n}\n```"], "573697c96e3b12023e6a9ac3": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are how cloud providers can set spot prices to manage user demand effectively and how users can optimize their bidding strategies in this dynamic environment. The paper proposes a model for understanding how providers set spot prices and develops optimal bidding strategies for users, also adapting these strategies for different job types, such as MapReduce. The work is inspired by and builds on various previous studies, particularly those related to spot pricing, cloud resource allocation, and auction theory.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b8\": 0.9,\n    \"b15\": 0.8,\n    \"b32\": 0.85,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b13\": 0.7,\n    \"b2\": 0.65,\n    \"b29\": 0.65,\n    \"b30\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b12\": 0.5,\n    \"b20\": 0.5,\n    \"b31\": 0.5,\n    \"b19\": 0.55,\n    \"b11\": 0.55,\n    \"b5\": 0.6,\n    \"b38\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing resource allocation in cloud computing through spot pricing. It proposes a model for setting spot prices and develops user bidding strategies to minimize costs while accommodating job interruptibility.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b1\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b15\": 0.8,\n    \"b32\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b12\": 0.6,\n    \"b20\": 0.6,\n    \"b31\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.6,\n    \"b2\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing resource allocation and pricing strategies in cloud computing, specifically focusing on Amazon's EC2 spot pricing. The study develops a model to predict spot prices and proposes optimal user bidding strategies to minimize costs while considering job interruptibility. The research includes empirical validation and adaptation for MapReduce jobs.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.95,\n    \"b13\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b32\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b12\": 0.6,\n    \"b20\": 0.6,\n    \"b31\": 0.6,\n    \"b15\": 0.6,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of optimizing cloud resource allocation and bidding strategies in the context of Amazon EC2's spot pricing. It proposes a model to understand how providers set spot prices and develops optimal bidding strategies for users based on predicted spot price distributions. The primary challenges include predicting spot prices and adapting bidding strategies to different degrees of job interruptibility.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b8\": 0.9,\n    \"b15\": 0.9,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing resource allocation in cloud computing through spot pricing. It proposes a model for setting spot prices and optimal bidding strategies for users, considering job interruptibility and historical price data. The aim is to minimize user costs while understanding provider behavior in setting prices.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b8\": 0.9,\n    \"b15\": 0.9,\n    \"b32\": 0.9,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b13\": 0.75,\n    \"b24\": 0.7,\n    \"b18\": 0.7,\n    \"b34\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b12\": 0.6,\n    \"b20\": 0.6,\n    \"b31\": 0.6,\n    \"b5\": 0.6,\n    \"b38\": 0.6,\n    \"b2\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```"], "57d063d3ac44367354292258": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of accessing opportunistic resources in delay-tolerant and opportunistic resource sharing networks (DT-ORS-Net). The proposed solution is a multi-player Markov stopping game (M-MSG) that generalizes the two-player Markov stopping game (MSG) to handle multiple users and potential conflicts, providing a recursive construction of the Nash equilibrium (NE) strategy.\",\n  \"Direct Inspiration\": [\"b16\"],\n  \"Indirect Inspiration\": [\"b10\", \"b9\", \"b11\", \"b12\"],\n  \"Other Inspiration\": [\"b13\", \"b14\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing resource access strategies in delay-tolerant and opportunistic resource sharing networks (DT-ORS-Net). It introduces a novel multi-player Markov stopping game (M-MSG) framework to handle competition among multiple users for opportunistic resources. The proposed framework aims to optimize users' strategies and ensure Nash equilibrium, preventing selfish behavior and collisions.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in delay-tolerant and opportunistic resource sharing networks (DT-ORS-Net), focusing on the unpredictable quality and timing of opportunistic resources. The authors propose a multi-player Markov stopping game (M-MSG) to develop a Nash equilibrium (NE) strategy for resource access, which considers potential conflicts among users and ensures no incentive to deviate. This framework is applied to cloud-computing and mobile crowdsourcing networks.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing resource access strategies in delay-tolerant and opportunistic resource sharing networks (DT-ORS-Net). The proposed solution is a multi-player Markov stopping game (M-MSG) that allows network users to adapt their decisions to potential conflicts from other users, effectively exploiting opportunistic resources. The contribution includes a recursive construction of the Nash equilibrium (NE) strategy for M-MSG.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b13\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in delay-tolerant and opportunistic resource sharing networks (DT-ORS-Net). It proposes a multi-player Markov stopping game (M-MSG) framework to handle the potential competition from other users and to enforce effective regulations on users' access behaviors. The framework is a generalization of the two-player Markov stopping game (MSG) and includes a recursive construction of the Nash equilibrium (NE) strategy to adapt user decisions to conflicts from other users. Applications of M-MSG to cloud-computing and mobile crowdsourcing networks are demonstrated.\",\n  \"Direct Inspiration\": [\"b16\"],\n  \"Indirect Inspiration\": [\"b9\", \"b10\", \"b11\", \"b12\"],\n  \"Other Inspiration\": [\"b13\", \"b14\", \"b15\", \"b19\"]\n}\n```"], "573696026e3b12023e516718": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper include the slow and complex nature of existing object detection systems, which repurpose classifiers to perform detection using region proposal methods or sliding windows. These systems are hard to optimize and require separate training for each component.\",\n    \"algorithm\": \"The authors propose the YOLO algorithm, which reframes object detection as a single regression problem from image pixels to bounding box coordinates and class probabilities. YOLO uses a single convolutional neural network to predict multiple bounding boxes and class probabilities simultaneously, allowing for end-to-end training and real-time detection speeds.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b12\", \"b27\", \"b21\", \"b7\", \"b25\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b9\", \"b13\", \"b30\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b8\", \"b32\", \"b28\", \"b23\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a fast and accurate object detection system, proposing the YOLO (You Only Look Once) algorithm which reframes object detection as a single regression problem. YOLO predicts bounding boxes and class probabilities directly from full images using a single convolutional neural network, optimizing detection performance end-to-end and achieving real-time detection speeds.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b21\": 0.8,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b12\": 0.7,\n    \"b27\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.6,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of fast and accurate object detection by introducing YOLO (You Only Look Once), a unified model that reframes object detection as a single regression problem. YOLO predicts bounding boxes and class probabilities directly from the full image, which enables real-time processing and improved accuracy over traditional methods like DPM and R-CNN.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b32\": 0.8,\n    \"b27\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.6,\n    \"b13\": 0.6,\n    \"b12\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.4,\n    \"b30\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces YOLO (You Only Look Once) as a fast, unified approach to object detection. YOLO reframes object detection as a single regression problem, predicting bounding boxes and class probabilities from images in a single evaluation. The key challenges addressed include the complexity and speed limitations of traditional object detection pipelines, which require separate training for each component. YOLO's design allows for end-to-end training and real-time performance while maintaining high precision.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b32\": 0.9,\n    \"b21\": 0.9,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b12\": 0.7,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b5\": 0.5,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating a fast and accurate object detection system by proposing YOLO (You Only Look Once). YOLO reframes object detection as a single regression problem, allowing it to predict bounding boxes and class probabilities directly from full images in real-time. The network is inspired by the GoogLeNet model and improves upon existing methods like DPM and R-CNN by unifying the detection pipeline into a single convolutional neural network.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b21\": 0.6,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b32\": 0.5,\n    \"b25\": 0.4\n  }\n}\n```"], "56ab70cd0cf2c98bf5bc717a": ["```json\n{\n    \"Summary\": \"The paper introduces AlphaGo, a program that achieves professional-level play in Go by combining deep neural networks with Monte Carlo Tree Search (MCTS). Major challenges addressed include the large search space and difficulty in board evaluation. Inspirations are drawn from previous successes in other games using MCTS and neural networks.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1.0,\n        \"b14\": 0.9,\n        \"b15\": 0.9,\n        \"b16\": 0.85,\n        \"b17\": 0.8,\n        \"b21\": 0.75\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b2\": 0.7,\n        \"b3\": 0.7,\n        \"b8\": 0.65,\n        \"b9\": 0.65,\n        \"b12\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.6,\n        \"b5\": 0.6,\n        \"b6\": 0.6,\n        \"b7\": 0.6,\n        \"b18\": 0.55,\n        \"b19\": 0.55,\n        \"b20\": 0.55,\n        \"b22\": 0.55,\n        \"b23\": 0.55,\n        \"b24\": 0.55,\n        \"b25\": 0.5,\n        \"b26\": 0.5,\n        \"b28\": 0.5,\n        \"b29\": 0.5,\n        \"b30\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The paper addresses the challenge of playing the game of Go at a professional level using artificial intelligence, which involves navigating an enormous search space and accurately evaluating board positions and moves.\",\n    \"Inspirations\": \"The authors draw inspiration from several key areas: Monte Carlo Tree Search (MCTS), convolutional neural networks used in visual domains, and reinforcement learning strategies. By combining these approaches, they develop a novel algorithm that integrates policy networks and value networks to achieve superhuman performance in Go.\"\n  },\n  \"Direct Inspiration\": [\n    \"b13\",\n    \"b15\",\n    \"b21\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b8\",\n    \"b12\",\n    \"b14\",\n    \"b16\",\n    \"b19\"\n  ],\n  \"Other Inspiration\": [\n    \"b17\",\n    \"b18\",\n    \"b25\",\n    \"b26\",\n    \"b28\",\n    \"b29\",\n    \"b30\",\n    \"b31\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges in the paper include the enormous search space and difficulty of evaluating board positions in the game of Go. The paper is inspired by previous work on Monte Carlo tree search (MCTS) and deep convolutional neural networks, and it introduces a new approach to computer Go using 'value networks' to evaluate board positions and 'policy networks' to select moves. The proposed algorithm, AlphaGo, combines these neural networks with MCTS, achieving a high level of play and defeating a human professional player for the first time.\",\n    \"Direct Inspiration\": {\n        \"b13\": 0.9,\n        \"b15\": 0.8,\n        \"b21\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.75,\n        \"b17\": 0.7,\n        \"b19\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.6,\n        \"b25\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a computer program capable of playing the game of Go at a professional level, a task deemed difficult due to the game's enormous search space and complex evaluation of board positions. The authors propose an innovative approach that combines deep convolutional neural networks with Monte Carlo Tree Search (MCTS). The system, named AlphaGo, uses 'policy networks' to select moves and 'value networks' to evaluate board positions, trained through a combination of supervised learning from human expert games and reinforcement learning from games of self-play.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b15\": 0.9,\n    \"b17\": 0.95,\n    \"b19\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b11\": 0.75,\n    \"b12\": 0.8,\n    \"b14\": 0.78\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b22\": 0.65,\n    \"b23\": 0.68,\n    \"b24\": 0.75,\n    \"b25\": 0.7,\n    \"b26\": 0.72,\n    \"b28\": 0.65,\n    \"b29\": 0.65,\n    \"b30\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The main challenges outlined in the paper include the vast search space and the complexity of evaluating board positions in the game of Go. The novel approach introduced by the authors involves using deep convolutional neural networks for policy and value networks along with Monte Carlo Tree Search (MCTS) to achieve superhuman performance in Go. The key innovations are the integration of deep learning for move selection and position evaluation, leveraging supervised and reinforcement learning techniques to train these networks.\",\n    \"Direct Inspiration\": {\n        \"b13\": 0.9,\n        \"b15\": 0.85,\n        \"b17\": 0.8,\n        \"b18\": 0.8,\n        \"b19\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.75,\n        \"b8\": 0.75,\n        \"b10\": 0.7,\n        \"b12\": 0.7,\n        \"b14\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b21\": 0.55,\n        \"b22\": 0.55,\n        \"b23\": 0.55,\n        \"b24\": 0.55\n    }\n}\n```"], "55a6bae665ce054aad73115b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of deriving efficient representations from high-dimensional sensory inputs for reinforcement learning. It proposes a novel artificial agent, termed a deep Q-network (DQN), which combines reinforcement learning with deep neural networks to learn policies directly from sensory inputs. The DQN was tested on Atari 2600 games and demonstrated superior performance compared to previous algorithms.\",\n  \"Direct Inspiration\": [\"b3\", \"b4\", \"b5\", \"b15\", \"b16\", \"b17\"],\n  \"Indirect Inspiration\": [\"b9\", \"b18\", \"b20\"],\n  \"Other Inspiration\": [\"b6\", \"b12\", \"b19\", \"b21\", \"b22\", \"b23\", \"b24\", \"b25\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of using reinforcement learning to derive efficient representations from high-dimensional sensory inputs and generalize past experiences to new situations. The main contribution is the development of a deep Q-network (DQN) that combines reinforcement learning with deep neural networks to learn policies directly from raw sensory data, achieving human-level performance on Atari 2600 games.\",\n  \"Direct Inspiration\": [\"b3\", \"b4\", \"b5\"],\n  \"Indirect Inspiration\": [\"b6\", \"b9\", \"b15\", \"b16\", \"b17\"],\n  \"Other Inspiration\": [\"b10\", \"b11\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying reinforcement learning to high-dimensional sensory inputs and demonstrates a novel agent, the deep Q-network (DQN), that combines reinforcement learning with deep neural networks to achieve human-level performance in Atari 2600 games.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b15\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b9\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b18\": 0.6,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b24\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of deriving efficient representations from high-dimensional sensory inputs for reinforcement learning. It proposes a novel deep Q-network (DQN) that combines reinforcement learning with deep neural networks to learn policies directly from raw sensory inputs. The approach includes mechanisms like experience replay and iterative updates to stabilize learning.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b4\": 1,\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.7,\n    \"b15\": 0.9,\n    \"b16\": 0.9,\n    \"b17\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b9\": 0.6,\n    \"b18\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the need for reinforcement learning agents to efficiently derive representations from high-dimensional sensory inputs and generalize past experiences to new situations. The paper introduces a novel artificial agent, termed a deep Q-network (DQN), which combines reinforcement learning with deep neural networks to learn successful policies directly from sensory inputs. The algorithm incorporates techniques like experience replay and a target Q-network to address instabilities in reinforcement learning.\",\n  \"Direct Inspiration\": [\"b15\", \"b16\", \"b17\"],\n  \"Indirect Inspiration\": [\"b3\", \"b4\", \"b5\", \"b12\"],\n  \"Other Inspiration\": [\"b10\", \"b11\"]\n}\n```"], "5d8b3b1d3a55acc418bda58c": ["```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include the dependency of NER on textual semantics and context, the limited availability of annotated Portuguese NER datasets, and the complications in benchmarking previous works due to varied dataset combinations and lack of standardized methodologies. The authors propose using a BERT model with a CRF layer for the Portuguese NER task, comparing feature-based and fine-tuning based training strategies, and aim to facilitate reproducibility by making their implementation publicly available.\",\n    \"Direct Inspiration\": {\n        \"b10\": 1,\n        \"b4\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b17\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b5\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of Named Entity Recognition (NER) in Portuguese, focusing on the scarcity of annotated data and the difficulty in benchmarking due to varied datasets and methodologies. The authors propose a BERT model with a Conditional Random Fields (CRF) layer, experimenting with feature-based and fine-tuning strategies. They aim to facilitate reproducibility by making their implementation publicly available.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b17\": 0.7,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b15\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the complexity of named entity recognition (NER) due to contextual dependencies, the lack of available and standardized datasets for Portuguese NER, and the difficulties in benchmarking and reproducing previous results. The proposed algorithm involves using a BERT model combined with a Conditional Random Fields (CRF) layer, and the paper experiments with both feature-based and fine-tuning based training strategies.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b17\": 0.7,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenges of Named Entity Recognition (NER) for the Portuguese language using a BERT model with a Conditional Random Fields (CRF) layer. It compares feature-based and fine-tuning based training strategies and aims to improve reproducibility of results in NER tasks.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b14\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include the complexity of NER tasks due to context dependency and the limited availability of annotated Portuguese NER datasets. The paper proposes using a BERT model with a CRF layer to improve Portuguese NER, comparing feature-based and fine-tuning based training strategies, and aims to facilitate reproducibility by making their implementation publicly available.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1,\n        \"b10\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b17\": 0.8,\n        \"b2\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.7,\n        \"b15\": 0.6,\n        \"b9\": 0.55\n    }\n}\n```"], "5dcbd5da3a55ac789b0dbbdd": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of Transformer-based models for Named Entity Recognition (NER). The authors propose two main improvements: 1) adopting a revised relative positional encoding that is direction-and distance-aware, and 2) using an un-scaled and sharper attention mechanism. Additionally, they explore using Transformer as a character encoder to better handle character-level information compared to CNNs.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b6\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.6,\n    \"b18\": 0.6,\n    \"b22\": 0.6,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b12\": 0.5,\n    \"b33\": 0.5,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of using the Transformer model for named entity recognition (NER), highlighting its shortcomings with positional embeddings and attention distribution. It proposes improvements with a revised relative positional encoding and un-scaled dot-product attention to enhance the Transformer's performance in NER tasks.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b18\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in using Transformer models for Named Entity Recognition (NER). It proposes two key improvements: (1) introducing direction-and distance-aware positional encoding to better capture the contextual information relevant to NER tasks, and (2) using un-scaled dot-product attention to reduce noise and improve the precision of attention mechanisms. Additionally, the authors explore the use of Transformer-based character encoding to handle character-level information more effectively than traditional CNN-based approaches.\",\n  \"Direct Inspiration\": [\"b31\", \"b6\"],\n  \"Indirect Inspiration\": [\"b15\", \"b22\", \"b18\"],\n  \"Other Inspiration\": [\"b34\", \"b7\", \"b16\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the poor performance of the vanilla Transformer encoder in the NER task, particularly due to its lack of direction and distance-awareness and the inefficiency of CNN in representing character-level information. The paper proposes two main improvements: using a revised relative positional encoding for direction and distance-awareness and adopting an un-scaled and sharp attention mechanism. Additionally, the paper explores using the Transformer as a character encoder to better capture character-level features and address the OOV problem.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b15\": 1.0,\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b18\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the underperformance of Transformer models in Named Entity Recognition (NER) tasks. It introduces two main improvements to the Transformer architecture: direction-and distance-aware attention and un-scaled dot-product attention. These changes aim to enhance the model's ability to capture relevant positional information and to focus more sharply on contextually important words. Additionally, the paper explores using a Transformer-based character encoder to better capture n-gram and discontinuous character patterns.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b6\": 1.0,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b18\": 0.7,\n    \"b22\": 0.7,\n    \"b34\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b4\": 0.5,\n    \"b3\": 0.5,\n    \"b36\": 0.5,\n    \"b13\": 0.4,\n    \"b33\": 0.5\n  }\n}\n```"], "5f7fdd328de39f0828397e22": ["```json\n{\n  \"Summary\": \"This paper addresses the limitations of Graph Neural Networks (GNNs) in handling networks with heterophily, where connected nodes are likely from different classes or have dissimilar features. The authors propose a new model, H2GCN, that incorporates three key designs: ego-and neighbor-embedding separation, higher-order neighborhoods, and combination of intermediate representations to improve performance in heterophily settings without sacrificing accuracy in homophily.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b16\": 1.0,\n    \"b35\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b6\": 0.8,\n    \"b0\": 0.8,\n    \"b36\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of Graph Neural Networks (GNNs) in learning from networks with heterophily (low or medium levels of homophily) and proposes a new model, H2GCN, that incorporates three key designs: (D1) ego-and neighbor-embedding separation, (D2) higher-order neighborhoods, and (D3) combination of intermediate representations. These designs are theoretically justified and empirically validated to improve GNN performance in heterophily settings without sacrificing performance in homophily.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b16\": 0.8,\n    \"b35\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.6,\n    \"b0\": 0.6,\n    \"b37\": 0.7,\n    \"b36\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.4,\n    \"b5\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve the limitations of Graph Neural Networks (GNNs) in networks with heterophily. The authors propose three key designs to address these challenges: ego-and neighbor-embedding separation, higher-order neighborhoods, and combining intermediate representations. These designs are integrated into a new model, H2GCN, which adapts effectively to both homophily and heterophily settings.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b16\": 0.9,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b0\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"The primary challenges outlined in the paper include the limitation of Graph Neural Networks (GNNs) in learning over networks with heterophily, which is often ignored in the literature due to evaluation on benchmarks with strong homophily. The paper aims to address this by identifying key designs that can boost learning from the graph structure in heterophily without trading off accuracy in homophily.\",\n        \"algorithm\": \"The primary algorithm proposed is the H2GCN model, which incorporates three key designs: (D1) ego-and neighbor-embedding separation, (D2) higher-order neighborhoods, and (D3) combination of intermediate representations. The model is evaluated on both synthetic and real networks covering the full spectrum of low-to-high homophily.\"\n    },\n    \"Direct Inspiration\": {\n        \"b10\": 0.9,\n        \"b16\": 0.8,\n        \"b35\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.7,\n        \"b6\": 0.7,\n        \"b37\": 0.7,\n        \"b41\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.5,\n        \"b5\": 0.5,\n        \"b36\": 0.5,\n        \"b43\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of Graph Neural Networks (GNNs) in learning over networks with heterophily, where linked nodes are likely from different classes or have dissimilar features. It proposes a new model, H2GCN, which incorporates three key designs: ego-and neighbor-embedding separation, higher-order neighborhoods, and combination of intermediate representations, to improve performance in heterophily settings without compromising accuracy in homophily.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b16\": 1.0,\n    \"b35\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b6\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b5\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```"], "573696ce6e3b12023e5ce95a": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of internal covariate shift in deep neural networks during training. The authors propose a novel method called Batch Normalization that normalizes layer inputs to stabilize their distributions, thereby accelerating training and improving performance. The algorithm adjusts the means and variances of layer inputs, making it possible to use higher learning rates and reducing the need for other regularization techniques such as Dropout.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b21\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.6,\n    \"b16\": 0.5,\n    \"b17\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.4,\n    \"b11\": 0.4,\n    \"b18\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies the primary challenge as the Internal Covariate Shift, which slows down the training of deep neural networks. The proposed solution is Batch Normalization, which aims to reduce this shift, leading to faster and more stable training.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b19\": 0.6,\n    \"b7\": 0.5,\n    \"b9\": 0.5,\n    \"b17\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.4,\n    \"b21\": 0.4,\n    \"b22\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the issue of internal covariate shift, which complicates the training of deep neural networks. The authors propose a novel algorithm, Batch Normalization, to address this issue by normalizing the means and variances of layer inputs. This method is shown to accelerate training, improve gradient flow, and reduce the need for other regularization techniques such as Dropout.\",\n    \"Direct Inspiration\": {\n        \"b10\": 0.9,\n        \"b9\": 0.85,\n        \"b21\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b17\": 0.75,\n        \"b7\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.65,\n        \"b18\": 0.6,\n        \"b6\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in this paper is the internal covariate shift during the training of deep neural networks. The proposed algorithm, Batch Normalization, aims to reduce internal covariate shift by normalizing the means and variances of layer inputs. This approach accelerates the training process and enhances the performance of deep neural networks.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b21\": 0.85,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b10\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b16\": 0.55,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the internal covariate shift in deep networks, which hampers training speed and effectiveness. The proposed solution is Batch Normalization, which normalizes the input of each layer to stabilize the distribution of layer inputs and accelerates the training process.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b21\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b3\": 0.75,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.65,\n    \"b6\": 0.6\n  }\n}\n```"], "59a03016b161e8ad1a7b6ed2": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively predicting outcomes from sparse data, especially in the context of categorical predictor variables. It identifies the limitations of traditional factorization machines (FMs) in handling non-linear and higher-order feature interactions. To overcome these limitations, the authors propose a novel model named Neural Factorization Machines (NFMs), which incorporates a Bi-Interaction pooling operation within a neural network framework to model higher-order and non-linear feature interactions. This approach is shown to significantly improve the performance of predictive models on sparse data.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b30\": 0.9,\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b26\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b20\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is to improve the predictive analytics task by modeling higher-order and non-linear feature interactions effectively, particularly in sparse data settings. The paper introduces a novel model called Neural Factorization Machines (NFMs) which enhances traditional Factorization Machines (FMs) by incorporating a new Bi-Interaction pooling operation within a neural network framework. This approach aims to deepen the FM model to capture higher-order and non-linear feature interactions, thereby improving its expressiveness and performance.\",\n    \"Direct Inspiration\": [\"b8\", \"b30\", \"b43\"],\n    \"Indirect Inspiration\": [\"b15\", \"b26\", \"b27\"],\n    \"Other Inspiration\": [\"b3\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of predicting outcomes with sparse, high-dimensional, and categorical data by introducing a novel model named Neural Factorization Machines (NFMs). NFMs enhance traditional Factorization Machines (FMs) by incorporating non-linear and higher-order feature interactions through a new Bi-Interaction pooling operation within a neural network framework. This method aims to overcome the expressiveness limitations of linear models and improve prediction accuracy for various tasks such as context-aware prediction and personalized tag recommendation.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b15\": 1,\n    \"b30\": 1,\n    \"b43\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.9,\n    \"b27\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b23\": 0.7,\n    \"b17\": 0.6,\n    \"b22\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The paper addresses the limitations of factorization machines (FMs) in handling complex, non-linear feature interactions in sparse data prediction tasks. The proposed solution, Neural Factorization Machines (NFMs), aims to model higher-order and non-linear feature interactions by integrating FMs into a neural network framework using a novel Bi-Interaction pooling operation.\",\n    \"inspirations\": \"The paper draws inspiration from existing methods such as factorization machines and deep neural networks, and seeks to improve their limitations by introducing new modelling techniques.\"\n  },\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b15\": 1,\n    \"b30\": 1,\n    \"b43\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.9,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7,\n    \"b23\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving predictive analytics for sparse data by proposing Neural Factorization Machines (NFMs). The NFMs enhance Factorization Machines (FMs) by modeling higher-order and non-linear feature interactions using a novel Bi-Interaction pooling operation within a neural network framework.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b15\": 0.85,\n    \"b27\": 0.9,\n    \"b30\": 0.85,\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.75,\n    \"b23\": 0.75,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```"], "5b67b47917c44aac1c8637c6": ["```json\n{\n  \"Summary\": \"The paper addresses the limitations of traditional neighborhood aggregation schemes in graph neural networks, particularly focusing on the adaptive, structure-aware representation learning in large and complex graphs. The authors propose Jumping Knowledge Networks (JK-Nets) to selectively exploit information from neighborhoods of differing locality, enhancing the adaptability of representation learning across varying subgraph structures.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b6\": 0.9,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b8\": 0.6,\n    \"b4\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.4,\n    \"b21\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of effectively learning node representations in graphs, particularly focusing on the limitations of current neighborhood aggregation schemes. The authors propose Jumping Knowledge Networks (JK-Nets) which adaptively adjust the influence radii for each node, allowing for structure-aware representations that improve performance on complex graphs with diverse subgraph structures.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b6\": 0.9,\n    \"b32\": 0.9,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b14\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.4,\n    \"b12\": 0.4,\n    \"b18\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving node representations in graphs, particularly focusing on the limitations of existing neighborhood aggregation schemes. It proposes a new architecture, Jumping Knowledge Networks (JK-Nets), which adaptively combines different aggregations to create structure-aware representations. This approach aims to improve performance on large complex graphs with diverse subgraph structures.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.95,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.85,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.75,\n    \"b14\": 0.7,\n    \"b2\": 0.7,\n    \"b13\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges of node representation learning in graphs, particularly focusing on the limitations of existing neighborhood aggregation schemes. The authors propose a new architecture, Jumping Knowledge Networks (JK-Nets), which adaptively learns influence radii for each node, enabling structure-aware representations. The model combines aggregations from different layers to better handle graphs with diverse subgraph structures.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b6\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b22\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of neighborhood aggregation schemes in graph representation learning, specifically focusing on adaptive, structure-aware representations for large complex graphs. The proposed solution is Jumping Knowledge Networks (JK-Nets) which selectively combine different aggregations at the last layer to adaptively adjust the influence radii for each node and task.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b6\": 0.8,\n    \"b8\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.6,\n    \"b22\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.4,\n    \"b3\": 0.3\n  }\n}\n```"], "5f75cce491e0111c1eb4d6e6": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Implicit homophily assumptions in GNN models\",\n      \"Heavy reliance on contextual node features\"\n    ],\n    \"Inspirations\": [\n      \"Incorporate a compatibility matrix to capture both heterophily and homophily\",\n      \"Propose CPGNN to address these challenges\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b22\": 0.9,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b19\": 0.7,\n    \"b21\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b6\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.6,\n    \"b11\": 0.6,\n    \"b20\": 0.6,\n    \"b27\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges addressed in the paper are the implicit homophily assumptions and heavy reliance on contextual node features in existing GNN models. The authors propose a novel CPGNN framework that incorporates a compatibility matrix to capture both heterophily and homophily, enabling better performance in diverse graph settings.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b7\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b8\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in existing Graph Neural Network (GNN) models, specifically the implicit homophily assumptions and the heavy reliance on contextual node features. The authors propose a novel approach called CPGNN that incorporates a compatibility matrix to effectively handle both homophily and heterophily settings. The key contributions include the generalization of GNNs to heterophily settings, the development of the CPGNN framework, and comprehensive evaluation of the proposed method.\",\n  \"Direct Inspiration\": [],\n  \"Indirect Inspiration\": [\n    \"b12\",\n    \"b22\",\n    \"b38\"\n  ],\n  \"Other Inspiration\": [\n    \"b3\",\n    \"b7\",\n    \"b31\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in existing Graph Neural Network (GNN) models: implicit homophily assumptions and heavy reliance on contextual node features. The authors propose CPGNN, a novel approach that incorporates a compatibility matrix to handle both homophily and heterophily, thereby improving performance even with missing or incomplete node features. The CPGNN framework involves prior belief estimation and compatibility-guided propagation, with extensive evaluations showing its effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b22\": 0.9,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b3\": 0.85,\n    \"b31\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in existing Graph Neural Network (GNN) models: implicit homophily assumptions and heavy reliance on contextual node features. To overcome these challenges, the authors propose CPGNN, a novel approach that incorporates a compatibility matrix that captures both heterophily and homophily by modeling the likelihood of connections between nodes in different classes. This method aims to improve performance in heterophily settings and when node features are missing or incomplete.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b22\": 1,\n    \"b38\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b21\": 0.6,\n    \"b32\": 0.6,\n    \"b4\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```"], "555048eb45ce0a409eb72996": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of relation classification in NLP by proposing a convolutional deep neural network (DNN) that extracts lexical and sentence-level features without the need for complicated NLP preprocessing. The main contributions include the use of position features to encode relative distances to target noun pairs and the demonstration that these features are critical for relation classification, outperforming state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b4\": 0.5,\n    \"b18\": 0.4,\n    \"b19\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of relation classification in NLP by proposing a convolutional deep neural network (DNN) to extract lexical and sentence level features without requiring complicated pre-processing like POS tagging and syntactic parsing. The key contributions include the introduction of position features to encode the relative distances to target noun pairs and demonstrating the effectiveness of this method using the SemEval-2010 Task 8 dataset.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b17\": 0.75,\n        \"b14\": 0.7,\n        \"b6\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.6,\n        \"b18\": 0.55,\n        \"b4\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the task of relation classification between pairs of nominals in sentences, aiming to improve upon existing methods that rely heavily on pre-processed NLP tools and are prone to error propagation. The proposed algorithm employs a convolutional deep neural network (DNN) to extract lexical and sentence level features without complicated pre-processing. Key contributions include the introduction of position features to encode relative distances to target noun pairs and the demonstration of the approach's effectiveness using the SemEval-2010 Task 8 dataset.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b14\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b0\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of relation classification by using a convolutional deep neural network (DNN) to extract lexical and sentence level features without complicated preprocessing. The approach aims to improve performance by utilizing word embeddings and position features (PF) to encode relative distances to target noun pairs.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the task of relation classification, which involves predicting semantic relations between pairs of nominals in a sentence. The proposed algorithm employs a convolutional deep neural network (DNN) to extract lexical and sentence-level features for relation classification without complicated NLP preprocessing. The method transforms word tokens into vectors using word embeddings, extracts lexical and sentence-level features, concatenates them, and feeds them into a softmax classifier to predict relationships between marked nouns. Key contributions include leveraging convolutional DNNs for feature extraction and introducing position features to encode relative distances to target noun pairs.\",\n    \"Direct Inspiration\": {\n        \"b3\": 1,\n        \"b14\": 0.95,\n        \"b17\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.85,\n        \"b18\": 0.8,\n        \"b7\": 0.75,\n        \"b4\": 0.7,\n        \"b2\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.6,\n        \"b5\": 0.55,\n        \"b6\": 0.5,\n        \"b9\": 0.45,\n        \"b12\": 0.4,\n        \"b15\": 0.35\n    }\n}\n```"], "5cede0e8da562983788c741f": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of modeling user interests for billion-scale users with diverse interests on the Tmall e-commerce platform. The proposed solution, Multi-Interest Network with Dynamic routing (MIND), aims to enhance recommendation accuracy while managing computational and storage costs by using multiple representation vectors for each user. Key innovations include the multi-interest extractor layer inspired by dynamic routing algorithms, which clusters user behaviors and infers diverse interest representations effectively.\",\n    \"Direct Inspiration\": {\n        \"b19\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b11\": 0.8,\n        \"b4\": 0.7,\n        \"b29\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.6,\n        \"b14\": 0.6,\n        \"b15\": 0.5,\n        \"b2\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the difficulty in modeling and representing the diverse interests of billion-scale users in the Tmall e-commerce platform for personalized recommendation systems. The novel method proposed is the Multi-Interest Network with Dynamic routing (MIND), which uses multiple representation vectors to capture different aspects of user interests efficiently without incurring significant additional costs.\",\n  \"Direct Inspiration\": [\"b19\"],\n  \"Indirect Inspiration\": [\"b10\", \"b11\"],\n  \"Other Inspiration\": [\"b4\", \"b29\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of capturing diverse user interests in personalized recommender systems (RS) for billion-scale users and items on Tmall. It proposes a novel Multi-Interest Network with Dynamic routing (MIND) to model multiple representation vectors for each user, inspired by the dynamic routing algorithm from capsule networks.\",\n    \"Direct Inspiration\": [\"b19\", \"b10\", \"b11\"],\n    \"Indirect Inspiration\": [\"b4\", \"b29\"],\n    \"Other Inspiration\": [\"b20\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of modeling diverse user interests in personalized recommender systems for billion-scale users at Tmall. The proposed solution is the Multi-Interest Network with Dynamic routing (MIND), which uses multiple representation vectors to capture different aspects of user interests, inspired by the dynamic routing algorithm used in capsule networks.\",\n  \"Direct Inspiration\": [\"b19\"],\n  \"Indirect Inspiration\": [\"b10\", \"b11\", \"b4\", \"b29\"],\n  \"Other Inspiration\": [\"b14\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to model and capture the diverse interests of billion-scale users in Tmall's personalized recommender systems without incurring high computational and storage costs. The authors propose the Multi-Interest Network with Dynamic routing (MIND), which uses multiple representation vectors to address this challenge. Key methods include the multi-interest extractor layer inspired by dynamic routing algorithms and the label-aware attention layer.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b8\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```"], "5aed148b17c44a4438154fae": ["```json\n{\n  \"Summary\": \"The paper addresses three main challenges in online personalized news recommendation: 1) dynamic changes in news and user preferences, 2) incorporating user return patterns as feedback, and 3) mitigating recommendation of similar items to prevent user boredom. It proposes a Deep Reinforcement Learning framework using Deep Q-Learning (DQN) to consider both immediate and future rewards, incorporates user activeness as feedback, and employs a Dueling Bandit Gradient Descent (DBGD) method for effective exploration.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b46\": 0.9,\n    \"b14\": 0.85,\n    \"b15\": 0.85,\n    \"b47\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b21\": 0.75,\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b43\": 0.6,\n    \"b50\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses three primary challenges in online personalized news recommendation: the dynamic nature of news and user preferences, the need for better user feedback beyond click/no-click labels, and the tendency of traditional methods to recommend similar items repeatedly. The authors propose a Deep Reinforcement Learning framework using a Deep Q-Learning (DQN) structure to handle these challenges. They incorporate user activeness as feedback and employ the Dueling Bandit Gradient Descent method for exploration.\",\n    \"Direct Inspiration\": {\n        \"b29\": 0.9,\n        \"b46\": 0.8,\n        \"b14\": 0.7,\n        \"b47\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b21\": 0.6,\n        \"b33\": 0.6,\n        \"b34\": 0.6,\n        \"b41\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.5,\n        \"b22\": 0.5,\n        \"b23\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three primary challenges in online personalized news recommendation: handling the dynamic nature of news and user preferences, incorporating user return patterns as feedback, and improving exploration methods to avoid recommending similar items repeatedly. The authors propose a Deep Reinforcement Learning framework using Deep Q-Learning (DQN), considering both current and future rewards, maintaining user activeness scores, and applying Dueling Bandit Gradient Descent for exploration.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b29\", \"b14\", \"b47\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b46\", \"b21\", \"b33\", \"b34\", \"b41\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b43\", \"b50\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three main challenges in online personalized news recommendation: dynamic changes in news content and user preferences, the need to incorporate user return patterns as feedback, and improving exploration strategies to avoid recommending similar items repeatedly. The proposed solution is a Deep Reinforcement Learning framework using Deep Q-Learning (DQN) that considers both immediate and future rewards, maintains user activeness scores for better feedback, and applies Dueling Bandit Gradient Descent for effective exploration.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b14\": 0.9,\n    \"b15\": 0.8,\n    \"b47\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.7,\n    \"b33\": 0.6,\n    \"b34\": 0.6,\n    \"b41\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b9\": 0.5,\n    \"b43\": 0.5,\n    \"b50\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three primary challenges in online personalized news recommendation: handling the dynamic nature of news and user preferences, incorporating user return patterns as feedback, and improving exploration methods to avoid repetitive recommendations. The authors propose a Deep Reinforcement Learning framework using Deep Q-Learning (DQN) to address these challenges. They introduce a Dueling Bandit Gradient Descent (DBGD) method for exploration and maintain an activeness score for user feedback.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b29\": 0.95,\n    \"b33\": 0.85,\n    \"b34\": 0.85,\n    \"b46\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.75,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b47\": 0.6\n  }\n}\n```"], "5d5e6b9a3a55acfce79a16dd": ["```json\n{\n  \"Summary\": \"The paper introduces the High-Resolution Network (HRNet), a novel architecture designed to maintain high-resolution representations throughout the entire network process. This approach allows for more spatially precise and semantically strong representations, which is beneficial for tasks like human pose estimation, semantic segmentation, and object detection. The key innovations include parallel high-to-low resolution streams, repeated multi-resolution fusions, and the introduction of HRNetV1, HRNetV2, and HRNetV2p for different applications.\",\n  \"Direct Inspiration\": {\n    \"b105\": 1.0,\n    \"b106\": 0.9,\n    \"b74\": 0.8,\n    \"b144\": 0.7,\n    \"b83\": 0.6,\n    \"b124\": 0.5\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.6,\n    \"b43\": 0.5,\n    \"b98\": 0.5,\n    \"b150\": 0.5,\n    \"b29\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.4,\n    \"b101\": 0.4,\n    \"b2\": 0.4,\n    \"b85\": 0.4,\n    \"b95\": 0.4,\n    \"b90\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a novel architecture called High-Resolution Net (HRNet) designed to maintain high-resolution representations through the entire process, improving tasks like human pose estimation, semantic segmentation, and object detection. This architecture connects high-to-low resolution convolution streams in parallel and conducts repeated multi-resolution fusions to enhance the high-resolution representations with semantic and spatial precision. The paper presents two versions of HRNet: HRNetV1 for human pose estimation and HRNetV2 for semantic segmentation.\",\n  \"Direct Inspiration\": [\"b83\", \"b124\", \"b95\", \"b2\"],\n  \"Indirect Inspiration\": [\"b15\", \"b144\", \"b39\", \"b101\"],\n  \"Other Inspiration\": [\"b29\", \"b43\", \"b98\", \"b150\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces the High-Resolution Network (HRNet) which maintains high-resolution representations throughout the process. It addresses the challenge of recovering high-resolution representations from low-resolution outputs in existing networks by connecting high-to-low resolution convolution streams in parallel and performing repeated multi-resolution fusions. HRNet shows superior performance in human pose estimation, semantic segmentation, and object detection.\",\n    \"Direct Inspiration\": {\n        \"references\": [\n            \"b105\"\n        ]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\n            \"b74\",\n            \"b22\",\n            \"b144\"\n        ]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\n            \"b124\",\n            \"b19\",\n            \"b83\",\n            \"b2\"\n        ]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a novel architecture called High-Resolution Net (HRNet) designed to maintain high-resolution representations throughout the entire process. HRNet is applied to human pose estimation, semantic segmentation, and object detection, showing superior results over existing methods. The primary challenge addressed is maintaining high-resolution representations without compromising on computational efficiency.\",\n  \"Direct Inspiration\": {\n    \"b83\": 1.0,\n    \"b124\": 1.0,\n    \"b2\": 0.9,\n    \"b85\": 0.9,\n    \"b95\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.8,\n    \"b101\": 0.8,\n    \"b90\": 0.7,\n    \"b15\": 0.7,\n    \"b144\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b74\": 0.6,\n    \"b9\": 0.5,\n    \"b38\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a novel architecture called High-Resolution Net (HRNet) that maintains high-resolution representations throughout the entire process. The primary challenges addressed include recovering high-resolution representations from low-resolution outputs and ensuring that the representations are both semantically strong and spatially precise. The proposed HRNet connects high-to-low resolution convolution streams in parallel and conducts repeated multi-resolution fusions, resulting in superior performance in tasks like human pose estimation and semantic segmentation.\",\n  \"Direct Inspiration\": {\n    \"b105\": 1.0,\n    \"b106\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b83\": 0.8,\n    \"b124\": 0.8,\n    \"b15\": 0.7,\n    \"b144\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b38\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```"], "5bdc31b417c44a1f58a0ba6c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing powerful Graph Neural Networks (GNNs) with a theoretical understanding of their representational capacity. The authors propose the Graph Isomorphism Network (GIN) which is shown to have maximum discriminative power among GNNs, validated through both theoretical proofs and empirical experiments.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9,\n    \"b36\": 0.85,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.75,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.65,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a theoretically grounded and maximally powerful Graph Neural Network (GNN) for graph structured data. It introduces the Graph Isomorphism Network (GIN), which is shown to have representational power equivalent to the Weisfeiler-Lehman (WL) graph isomorphism test. The GIN is designed using principles from multisets and injective functions to ensure its discriminative power.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b23\": 0.8,\n    \"b21\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b1\": 0.7,\n    \"b4\": 0.7,\n    \"b7\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the limited theoretical understanding of the properties and representational capacities of Graph Neural Networks (GNNs). The authors propose a new neural architecture, the Graph Isomorphism Network (GIN), which they claim to have maximum discriminative power, equivalent to the Weisfeiler-Lehman (WL) test. This is validated through experiments on graph classification datasets.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1.0,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.8,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b15\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a theoretically grounded Graph Neural Network (GNN) that maximizes discriminative power for tasks such as node and graph classification. The proposed Graph Isomorphism Network (GIN) is inspired by the Weisfeiler-Lehman (WL) test and aims to create a GNN that is as powerful as the WL test in distinguishing graph structures.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.9,\n    \"b12\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.7,\n    \"b38\": 0.6,\n    \"b16\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing the representational power of Graph Neural Networks (GNNs) by introducing the Graph Isomorphism Network (GIN), which aims to match the discriminative power of the Weisfeiler-Lehman (WL) test. The primary inspiration for this work is derived from the limitations of current GNNs in theoretical understanding and their empirical design based on heuristics. The authors propose a novel architecture that ensures injective aggregation functions, making the GIN as powerful as the WL test in distinguishing graph structures.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.8,\n    \"b21\": 0.7,\n    \"b39\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```"], "5cede10fda562983788ef75c": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the need for efficient multi-scale feature extraction in vision tasks such as image classification, object detection, and semantic segmentation. The proposed algorithm, Res2Net, addresses this challenge by using a novel neural network module that replaces conventional 3x3 filters with smaller groups of filters connected in a hierarchical residual-like style, enhancing the multi-scale representation capability at a more granular level.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b55\": 1,\n    \"b59\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b8\": 0.8,\n    \"b10\": 0.8,\n    \"b46\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.6,\n    \"b5\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving multi-scale feature representation in Convolutional Neural Networks (CNNs) for various vision tasks. The proposed solution, named Res2Net, enhances the multi-scale representation ability at a more granular level by using smaller filter groups connected in a hierarchical residual-like style. This approach allows for a larger number of equivalent feature scales, leading to improved performance in state-of-the-art CNN architectures.\",\n    \"Direct Inspiration\": {\n        \"b22\": 1,\n        \"b55\": 1,\n        \"b59\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b27\": 0.8,\n        \"b46\": 0.8,\n        \"b50\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.6,\n        \"b8\": 0.6,\n        \"b10\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing efficient multi-scale feature extractors for convolutional neural networks (CNNs) to improve performance in various vision tasks. The proposed solution is the Res2Net module, which enhances multi-scale representation at a more granular level by using smaller filter groups connected hierarchically.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b55\": 0.85,\n    \"b59\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b8\": 0.7,\n    \"b10\": 0.7,\n    \"b47\": 0.7,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b46\": 0.65,\n    \"b50\": 0.6,\n    \"b25\": 0.65,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing efficient multi-scale representations for visual cognition tasks. The authors propose the Res2Net module, which enhances multi-scale feature extraction at a more granular level by using smaller filter groups connected in a hierarchical residual-like style. This approach allows for richer receptive field sizes with minimal computational overhead, improving performance in tasks such as image classification, object detection, and semantic segmentation.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b55\": 1.0,\n    \"b59\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.9,\n    \"b46\": 0.9,\n    \"b50\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.8,\n    \"b10\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving multi-scale feature representation in convolutional neural networks (CNNs) for various vision tasks. It proposes a novel approach, the Res2Net module, which enhances multi-scale representation at a more granular level by replacing conventional 3\u00d73 filters with smaller filter groups connected in a hierarchical residual-like style.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b55\": 0.85,\n    \"b59\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b8\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.6,\n    \"b49\": 0.6,\n    \"b50\": 0.6,\n    \"b51\": 0.6,\n    \"b9\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```"], "5f058d15dfae54570ec57ea1": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges include improving image classification accuracy while optimizing for training efficiency and memory usage on general/commercial processing hardware. Additionally, the paper addresses the need for a versatile backbone with universally improved feature representations to boost performance across multiple tasks.\",\n    \"inspirations\": \"The authors are inspired by previous work on cross-channel information, group or depth-wise convolution, and existing attention mechanisms in neural network design.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b22\", \"b28\", \"b37\", \"b54\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b25\", \"b60\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b29\", \"b64\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving image classification and transfer learning performance by introducing a novel architecture called ResNeSt, which incorporates Split-Attention blocks to enhance cross-channel information. This architecture aims to outperform existing ResNet variants and state-of-the-art CNN models produced via neural architecture search.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b28\": 1.0,\n    \"b37\": 1.0,\n    \"b54\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b60\": 0.6,\n    \"b44\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating a versatile backbone neural network that improves performance across multiple computer vision tasks. It proposes a novel architectural modification of ResNet, called ResNeSt, which incorporates Split-Attention blocks. These blocks divide feature-maps into groups and splits, combining them using a weighted mechanism based on global contextual information. The ResNeSt architecture aims to achieve high computational efficiency and state-of-the-art performance on tasks like image classification, object detection, instance segmentation, and semantic segmentation.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b28\": 0.9,\n    \"b37\": 0.9,\n    \"b54\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b60\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b55\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve improving the training efficiency and memory usage of neural architecture search (NAS)-derived models, as well as creating a versatile backbone network that can be effectively used across multiple computer vision tasks. The paper introduces a novel architecture called ResNeSt, which incorporates Split-Attention blocks to improve cross-channel information representation and achieve state-of-the-art performance in image classification, object detection, instance segmentation, and semantic segmentation.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b28\": 1,\n    \"b37\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b54\": 0.9,\n    \"b60\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents a novel architecture called ResNeSt, which enhances the ResNet architecture by introducing Split-Attention blocks. These blocks incorporate feature-map split attention within individual network blocks, aiming to improve cross-channel interactions and thus achieve better performance across various tasks like image classification, object detection, and semantic segmentation. The main challenge addressed is optimizing neural networks not only for accuracy but also for training efficiency and memory usage on general/commercial hardware.\",\n  \"Direct Inspiration\": [\"b22\", \"b28\", \"b37\"],\n  \"Indirect Inspiration\": [\"b25\", \"b54\", \"b60\"],\n  \"Other Inspiration\": [\"b29\"]\n}\n```"], "573696cd6e3b12023e5ce382": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of hyperparameter optimization in machine learning, particularly focusing on the computational inefficiency and memory constraints associated with reverse-mode differentiation (RMD) for hyperparameters. The main contribution is an algorithm that efficiently computes gradients with respect to hyperparameters by exactly reversing stochastic gradient descent with momentum, significantly reducing memory requirements and enabling the optimization of thousands of hyperparameters.\",\n  \"Direct Inspiration\": [\"b3\", \"b2\"],\n  \"Indirect Inspiration\": [\"b12\", \"b11\", \"b36\"],\n  \"Other Inspiration\": [\"b24\", \"b31\", \"b38\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve the difficulty of optimizing a large number of hyperparameters in machine learning models and the memory constraints associated with reverse-mode differentiation for hyperparameters. The authors propose an algorithm that reverses stochastic gradient descent with momentum to compute hypergradients efficiently, reducing memory requirements significantly. They explore the applications of this method in optimizing thousands of hyperparameters, providing insight into learning procedures.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b2\": 0.8,\n    \"b12\": 0.8,\n    \"b31\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.6,\n    \"b6\": 0.6,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.5,\n    \"b37\": 0.5,\n    \"b19\": 0.5,\n    \"b11\": 0.4,\n    \"b36\": 0.4,\n    \"b14\": 0.4,\n    \"b38\": 0.4,\n    \"b21\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of effectively optimizing a large number of hyperparameters in machine learning models, which traditional methods struggle with due to memory constraints during reverse-mode differentiation. The proposed algorithm allows the computation of gradients with respect to hyperparameters efficiently by exactly reversing stochastic gradient descent with momentum and optimizing memory usage.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9,\n        \"b2\": 0.85,\n        \"b12\": 0.8,\n        \"b20\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b31\": 0.7,\n        \"b37\": 0.65,\n        \"b38\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.75,\n        \"b24\": 0.7,\n        \"b36\": 0.65,\n        \"b11\": 0.6,\n        \"b14\": 0.55,\n        \"b21\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing hyperparameters in machine learning models, particularly in scenarios involving a large number of hyperparameters. The proposed algorithm uses gradient-based optimization for hyperparameters by reversing stochastic gradient descent with momentum, significantly reducing memory requirements and allowing for efficient optimization of thousands of hyperparameters.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b31\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b20\": 0.6,\n    \"b24\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing hyperparameters in machine learning models. It proposes a novel algorithm that uses reverse-mode differentiation to compute gradients with respect to hyperparameters, making it feasible to optimize a large number of hyperparameters. The method reduces memory requirements significantly and opens up new possibilities for hyperparameter-rich models.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b2\": 0.9,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b6\": 0.6,\n    \"b11\": 0.5\n  }\n}\n```"], "5da2f8a647c8f76646083cd9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of semantic segmentation of remote sensing images by proposing a novel technique that exploits the multi-context paradigm without increasing the number of parameters. The technique involves using dilated convolutions to process input patches of varying sizes, determining the best patch size adaptively during the training phase, and selecting the optimal patch size for the inference stage.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b27\": 0.85,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.75,\n    \"b21\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of semantic segmentation of remote sensing images by proposing a novel technique that exploits the multi-context paradigm using dilated convolutions. This method aims to adaptively determine the best patch size during training, reducing computational complexity and improving accuracy without increasing the number of parameters.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b20\": 0.8,\n    \"b21\": 0.8,\n    \"b23\": 0.8,\n    \"b24\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b26\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of semantic segmentation in remote sensing images, particularly focusing on the definition of optimal patch sizes and multi-context information aggregation. The proposed method uses a novel technique based on dilated convolutions to determine the best patch size during the training phase and aggregates multi-context information without increasing the number of trainable parameters.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b19\": 0.9,\n    \"b20\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b27\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5,\n    \"b34\": 0.5,\n    \"b35\": 0.5,\n    \"b36\": 0.5,\n    \"b37\": 0.5,\n    \"b38\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of semantic segmentation of remote sensing images by proposing a novel multi-context approach using dilated convolutions. This methodology adapts the best input patch size during training, mitigating the limitations of fixed patch sizes from previous methods.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b19\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.7,\n    \"b25\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of semantic segmentation in remote sensing images, focusing on the importance of multi-context information and the optimal patch size. The proposed approach leverages dilated convolutions to handle varying patch sizes without increasing the number of parameters, aiming to improve the segmentation accuracy and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b21\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b10\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b33\": 0.6,\n    \"b35\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```"], "5fe4094e9e795e14f30e634a": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Gap between pre-training and fine-tuning objectives\",\n      \"Preserving node and graph-level information with unlabeled data\"\n    ],\n    \"proposed_algorithm\": \"L2P-GNN\"\n  },\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b13\": 1,\n    \"b8\": 1,\n    \"b16\": 1,\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b18\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b14\": 0.5,\n    \"b39\": 0.5,\n    \"b45\": 0.5,\n    \"b15\": 0.5,\n    \"b30\": 0.5,\n    \"b29\": 0.5,\n    \"b27\": 0.5,\n    \"b44\": 0.5,\n    \"b23\": 0.5,\n    \"b46\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Gap between pre-training and fine-tuning optimization objectives.\",\n      \"Preserving both node- and graph-level information with completely unlabeled graph data.\"\n    ],\n    \"Proposed Algorithm\": \"L2P-GNN, which learns to pre-train at both node and graph levels in a fully self-supervised manner.\"\n  },\n  \"Direct Inspiration\": {\n    \"References\": [\"b7\", \"b13\", \"b8\", \"b16\", \"b12\"]\n  },\n  \"Indirect Inspiration\": {\n    \"References\": [\"b28\", \"b18\", \"b17\"]\n  },\n  \"Other Inspiration\": {\n    \"References\": [\"b19\", \"b14\", \"b39\", \"b45\", \"b15\", \"b30\", \"b29\", \"b27\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of bridging the gap between pre-training and fine-tuning in Graph Neural Networks (GNNs), proposing a new pre-training strategy called L2P-GNN. The method aims to mimic fine-tuning during pre-training to enhance adaptability and uses a self-supervised dual adaptation mechanism to capture both node and graph-level information.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b7\": 0.9,\n    \"b13\": 0.9,\n    \"b8\": 0.9,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b18\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.7,\n    \"b47\": 0.7,\n    \"b10\": 0.7,\n    \"b19\": 0.7,\n    \"b14\": 0.7,\n    \"b39\": 0.7,\n    \"b45\": 0.7,\n    \"b15\": 0.7,\n    \"b30\": 0.7,\n    \"b29\": 0.7,\n    \"b27\": 0.7,\n    \"b23\": 0.7,\n    \"b46\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of bridging the gap between pre-training and fine-tuning in Graph Neural Networks (GNNs) by proposing L2P-GNN, a strategy that incorporates both node- and graph-level self-supervised learning. The key contributions include mimicking fine-tuning during pre-training and introducing a dual adaptation mechanism to preserve local and global graph properties.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b13\": 1,\n    \"b8\": 1,\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.8,\n    \"b18\": 0.8,\n    \"b28\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of narrowing the gap between pre-training and fine-tuning phases in GNNs, proposing L2P-GNN, a pretraining strategy that learns to pre-train at both node and graph levels in a fully self-supervised manner. The method leverages meta-learning to simulate fine-tuning during pre-training, aiming to enhance generalization and quick adaptation to new tasks.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b17\": 0.9,\n    \"b18\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b8\": 0.7,\n    \"b16\": 0.7\n  }\n}\n```"], "5aed14d617c44a4438159123": ["```json\n{\n    \"Summary\": {\n        \"challenges\": \"The primary challenge outlined in the paper is to systematically compare the performance of convolutional and recurrent architectures on a broad range of sequence modeling tasks, traditionally dominated by recurrent networks.\",\n        \"inspirations\": \"The authors are inspired by the recent success of convolutional architectures in audio synthesis, word-level language modeling, and machine translation, and seek to determine if these successes can be generalized to a wider range of sequence modeling tasks.\"\n    },\n    \"Direct Inspiration\": {\n        \"b20\": 1.0,\n        \"b37\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.8,\n        \"b57\": 0.8,\n        \"b35\": 0.8,\n        \"b74\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.6,\n        \"b54\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of sequence modeling by comparing convolutional and recurrent architectures, particularly focusing on Temporal Convolutional Networks (TCNs). It aims to determine whether convolutional networks can outperform recurrent networks across various sequence modeling tasks.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1.0,\n    \"b20\": 1.0,\n    \"b11\": 0.9,\n    \"b57\": 0.9,\n    \"b35\": 0.9,\n    \"b74\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b54\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b67\": 0.7,\n    \"b56\": 0.7,\n    \"b31\": 0.7,\n    \"b10\": 0.7,\n    \"b29\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper discusses the primary challenge of evaluating convolutional and recurrent architectures on a broad range of sequence modeling tasks. The algorithm proposed is a Temporal Convolutional Network (TCN), which is designed to handle sequence prediction tasks with a simple yet powerful architecture. The TCN aims to outperform traditional recurrent networks such as LSTMs and GRUs by leveraging causal and dilated convolutions along with residual connections.\",\n    \"Direct Inspiration\": {\n        \"b37\": 1,\n        \"b20\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.9,\n        \"b57\": 0.9,\n        \"b35\": 0.9,\n        \"b74\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b67\": 0.8,\n        \"b53\": 0.8,\n        \"b59\": 0.8,\n        \"b63\": 0.8,\n        \"b28\": 0.8,\n        \"b39\": 0.8\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include evaluating the effectiveness of convolutional versus recurrent architectures on a broad range of sequence modeling tasks. The proposed algorithm is a Temporal Convolutional Network (TCN) that is compared against canonical recurrent architectures such as LSTMs and GRUs. The TCN combines simplicity, autoregressive prediction, and very long memory to outperform baseline recurrent architectures.\",\n  \"Direct Inspiration\": [\"b37\", \"b20\"],\n  \"Indirect Inspiration\": [\"b11\", \"b57\", \"b35\", \"b74\"],\n  \"Other Inspiration\": [\"b45\", \"b61\", \"b30\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The paper addresses the challenge of determining whether recent successes of convolutional sequence modeling are confined to specific domains or if they can be generalized to a broader range of sequence modeling tasks. It conducts a systematic empirical evaluation of convolutional and recurrent architectures on a comprehensive set of sequence modeling tasks.\",\n    \"Inspirations\": \"The proposed Temporal Convolutional Network (TCN) architecture is inspired by recent convolutional architectures for sequential data, especially in machine translation, audio synthesis, and language modeling.\"\n  },\n  \"Direct Inspiration\": {\n    \"b37\": 1,\n    \"b20b\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b57\": 0.8,\n    \"b35\": 0.8,\n    \"b74\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b54\": 0.6,\n    \"b45\": 0.6,\n    \"b61\": 0.6,\n    \"b30\": 0.6,\n    \"b67\": 0.6,\n    \"b5\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b17\": 0.6,\n    \"b36\": 0.6,\n    \"b38\": 0.6,\n    \"b75\": 0.6,\n    \"b15\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```"], "5e5e189993d709897ce1e202": ["```json\n{\n  \"Summary\": \"The paper addresses the convergence issues of adaptive learning rate optimization methods, particularly focusing on the variance of the adaptive learning rate in the early stages of training. It introduces a new variant of Adam, called Rectified Adam (RAdam), which rectifies this variance to improve performance. The main contributions are the identification of the variance issue and the proposal of RAdam as a solution.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b9\", \"b28\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b10\", \"b16\", \"b21\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b11\", \"b18\", \"b25\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of convergence issues in adaptive learning rate optimization methods, particularly in the early stages of training due to high variance. Inspired by these findings, the authors propose Rectified Adam (RAdam), which rectifies the variance of the adaptive learning rate. The paper also offers theoretical and empirical justifications for the warmup heuristic.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0,\n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the convergence issue in adaptive learning rate algorithms like Adam, due to the undesirably large variance of the adaptive learning rate in the early training stages. The paper introduces a novel variant of Adam, called Rectified Adam (RAdam), which rectifies the variance of the adaptive learning rate based on theoretical analysis.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b28\": 1.0,\n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b18\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the convergence issues faced by adaptive learning rate optimization methods, particularly Adam, which can result in bad/suspicious local optima in the early stages of training due to high variance. The authors propose a novel variant called Rectified Adam (RAdam) to rectify this variance and provide a theoretical justification for the warmup heuristic.\",\n    \"Direct Inspiration\": {\n        \"b16\": 0.9,\n        \"b10\": 0.9,\n        \"b18\": 0.95,\n        \"b11\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b28\": 0.7,\n        \"b25\": 0.7,\n        \"b9\": 0.7,\n        \"b31\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.65,\n        \"b23\": 0.6,\n        \"b21\": 0.6,\n        \"b29\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of convergence issues in adaptive learning rate optimization algorithms, specifically identifying the problem of large variance in the early stages of training. The authors propose a new variant of Adam, called Rectified Adam (RAdam), which explicitly rectifies the variance of the adaptive learning rate, providing both empirical and theoretical justification for its effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b25\": 0.9,\n    \"b9\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b10\": 0.8,\n    \"b21\": 0.75,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.65,\n    \"b4\": 0.65,\n    \"b11\": 0.6,\n    \"b18\": 0.6,\n    \"b31\": 0.6,\n    \"b5\": 0.5,\n    \"b29\": 0.5,\n    \"b3\": 0.5,\n    \"b13\": 0.5,\n    \"b14\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```"], "5d1eb9ddda562961f0b17476": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the significant distribution shift between training and testing data distributions due to different pre-processing methods, which detrimentally affects the performance of Convolutional Neural Networks (CNNs). The proposed algorithm addresses this by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same Region of Classification (RoC) sampling. This approach retains the advantages of existing pre-processing protocols and compensates for the distribution shift by fine-tuning two layers to adjust for changes in crop size.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b21\": 0.7,\n    \"b31\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b25\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of distribution shift between training and testing data in CNNs due to different data pre-processing procedures at these stages. The authors propose an approach to jointly optimize resolutions and scales at training and testing times, compensating for the shift by fine-tuning two layers. This improves performance, reduces memory consumption, and enhances computational efficiency. The method allows for better accuracy with higher resolution at test time without extensive retraining.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b21\": 0.7,\n    \"b31\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of distribution shift between training and testing data in Convolutional Neural Networks (CNNs) due to different data pre-processing procedures. The primary contribution is a method to jointly optimize the choice of resolutions and scales at both training and test time, while keeping the same Region of Classification (RoC) sampling. This approach requires fine-tuning two layers to compensate for the shift in statistics caused by changing the crop size, thereby improving test-time performance and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b10\": 0.9,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b19\": 0.6,\n    \"b21\": 0.6,\n    \"b31\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.4,\n    \"b13\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the problem of distribution shift between training and testing data in Convolutional Neural Networks (CNNs) due to different data pre-processing procedures. It proposes a method to jointly optimize the choice of resolutions and scales at training and test time, specifically by fine-tuning two layers to compensate for the shift in statistics caused by changing crop sizes.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b21\": 0.7,\n    \"b31\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b19\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of distribution shift between training and testing data in convolutional neural networks (CNNs) due to different data pre-processing procedures. It proposes a novel method of jointly optimizing resolutions and scales at training and test time to mitigate this shift, focusing on fine-tuning specific layers to address the statistical changes caused by resizing the images.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b10\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b16\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```"], "5d84a3433a55acc20782ce9e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of building automatic speech recognition (ASR) systems with limited transcribed training data. It proposes a semi-supervised self-training approach for sequence-to-sequence models with attention, incorporating a strong baseline acoustic model, a beam search decoder leveraging an externally-trained neural language model, and a novel ensemble approach for self-training. The paper also evaluates methods for pseudo-label filtering and gives a comprehensive empirical evaluation of the self-training algorithm's components.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b8\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of building effective automatic speech recognition (ASR) systems with limited transcribed training data. It proposes a self-training algorithm for sequence-to-sequence models that leverages unpaired audio and text data. Key components include a strong baseline acoustic model, a robust beam search decoder, and a novel ensemble approach for self-training. The paper also evaluates pseudo-label filtering methods to improve model quality.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b2\": 0.9,\n        \"b7\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.6,\n        \"b5\": 0.6,\n        \"b6\": 0.6,\n        \"b9\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.4,\n        \"b8\": 0.4,\n        \"b10\": 0.4,\n        \"b11\": 0.3,\n        \"b12\": 0.3,\n        \"b13\": 0.3,\n        \"b14\": 0.3,\n        \"b15\": 0.3,\n        \"b16\": 0.3,\n        \"b17\": 0.3,\n        \"b18\": 0.3\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of building automatic speech recognition (ASR) systems with limited transcribed training data by proposing a semi-supervised self-training algorithm for sequence-to-sequence models with attention. The algorithm involves a strong baseline acoustic model, a robust beam search decoder, and a novel ensemble approach for self-training. The self-training algorithm improves word error rate (WER) significantly, demonstrating effectiveness through comprehensive empirical evaluation.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b7\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of building automatic speech recognition (ASR) systems with limited transcribed training data. It revisits self-training for sequence-to-sequence models with attention, using a robust beam search decoder and a novel ensemble approach to improve label diversity. The paper also evaluates the effect of various components, such as language models and pseudo-label filtering, on model performance.\",\n    \"Direct Inspiration\": [\"b1\", \"b2\"],\n    \"Indirect Inspiration\": [\"b3\", \"b4\", \"b5\", \"b6\", \"b7\", \"b9\"],\n    \"Other Inspiration\": [\"b8\", \"b10\", \"b11\", \"b12\", \"b13\", \"b14\", \"b15\", \"b16\", \"b17\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing effective automatic speech recognition (ASR) systems with limited transcribed training data. The proposed algorithm revisits self-training for sequence-to-sequence models with attention, leveraging unpaired audio and text data through a robust self-training approach. Key components include a strong baseline acoustic model, a robust beam search decoder with an external neural language model, and a novel ensemble approach for self-training.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b1\", \"b2\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b4\", \"b5\", \"b6\", \"b7\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b3\", \"b8\"]\n  }\n}\n```"], "5aed14d117c44a4438158af2": ["```json\n{\n  \"Summary\": \"The paper presents a new open source toolkit named ESPnet, designed for end-to-end speech processing. It aims to provide a neural end-to-end platform for automatic speech recognition (ASR) and other speech processing tasks. The main challenges addressed include creating a unified neural network architecture for ASR, integrating various end-to-end ASR techniques, and ensuring robustness and fast convergence in training. ESPnet leverages existing toolkits such as Chainer and PyTorch for deep learning, and it adopts a hybrid CTC/attention approach for improved performance.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b16\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b11\": 0.85,\n    \"b31\": 0.7,\n    \"b34\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.65,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces ESPnet, an end-to-end speech processing toolkit that aims to provide a neural end-to-end platform for ASR and other speech processing. The main challenges addressed include providing a single neural network architecture for speech recognition, integrating CTC and attention-based methods, and ensuring robustness and fast convergence during training. ESPnet leverages existing frameworks like Chainer and PyTorch, and integrates features from the Kaldi toolkit for data processing and feature extraction.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1.0,\n        \"b16\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b11\": 0.8,\n        \"b14\": 0.8,\n        \"b31\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b32\": 0.6,\n        \"b33\": 0.6,\n        \"b34\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is the development of an end-to-end speech processing toolkit, ESPnet, that integrates dynamic neural network toolkits to provide a single neural network architecture for Automatic Speech Recognition (ASR). ESPnet aims to improve the robustness and achieve fast convergence by combining both connectionist temporal classification (CTC) and attention-based encoder-decoder mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.7,\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7,\n    \"b36\": 0.7,\n    \"b37\": 0.7,\n    \"b38\": 0.7,\n    \"b39\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents ESPnet, a novel open-source toolkit for end-to-end speech processing. The main challenges addressed include the integration of dynamic neural network toolkits, adoption of end-to-end ASR techniques, and achieving robust performance on ASR benchmarks. The primary inspirations are derived from existing open-source toolkits, end-to-end ASR frameworks, and specific attention and CTC-based architectures.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b11\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b15\": 0.7,\n    \"b31\": 0.7,\n    \"b32\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces ESPnet, an open source toolkit for end-to-end automatic speech recognition (ASR) and other speech processing tasks, which distinguishes itself by using a fully neural network-based architecture rather than the traditional hybrid DNN/HMM approach. The primary challenges addressed include achieving robust and fast training, effective decoding, and providing a comprehensive setup for various speech recognition experiments.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b16\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b10\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b34\": 0.8,\n    \"b35\": 0.8,\n    \"b36\": 0.8,\n    \"b37\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.6,\n    \"b33\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```"], "5e5e190d93d709897ce4ae6f": ["```json\n{\n  \"Summary\": \"The paper addresses the primary challenges of subnet waste, role mismatch, and non-pre-trained attention modules in speech-to-text translation (ST) systems. It proposes a Tandem Connectionist Encoding Network (TCEN) that reuses all subnets during pre-training, maintains subnet role consistency, and pre-trains the attention module.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b9\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b3\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b30\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenges in end-to-end speech-to-text (ST) translation, particularly focusing on the limitations of previous models in terms of subnet waste, role mismatch, and the non-pre-trained attention module. The proposed Tandem Connectionist Encoding Network (TCEN) aims to reuse all subnets in pre-training, maintain role consistency, and pre-train the attention module. Key innovations include a model architecture that concatenates an ASR encoder and an MT encoder and the use of multi-task learning to address semantic and length inconsistencies.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b9\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b8\": 0.8,\n    \"b30\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in Speech-to-Text (ST) translation, focusing on issues like subnet waste, role mismatch, and non-pre-trained attention modules in existing models. The proposed solution, a Tandem Connectionist Encoding Network (TCEN), reuses all subnets in pre-training, maintains role consistency, and pre-trains the attention module. The novel contributions include a new architecture with speech and text encoders, solutions for semantic and length consistency, and improvements in multi-task learning for ST models.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1,\n        \"b9\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.7,\n        \"b8\": 0.6,\n        \"b30\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.4,\n        \"b32\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses key challenges in end-to-end speech-to-text translation, including subnet waste, role mismatch, and non-pre-trained attention modules. It proposes the Tandem Connectionist Encoding Network (TCEN) which reuses all subnets in pre-training, keeps subnet roles consistent, and pre-trains the attention module. The novel methods include using a shared projection matrix for semantic consistency and transforming MT source sentences to mimic CTC output sequences for length consistency.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b21\": 0.7,\n    \"b25\": 0.7,\n    \"b30\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in speech-to-text (ST) translation, particularly focusing on issues with existing pre-training and multitask learning methods such as subnet waste, role mismatch, and non-pre-trained attention modules. The authors propose a Tandem Connectionist Encoding Network (TCEN) to mitigate these problems by reusing all pre-trained subnets, maintaining role consistency, and pre-training the attention module. The TCEN includes a speech encoder, a text encoder, and a target text decoder, with techniques to ensure semantic and length consistency.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b9\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b8\": 0.75,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b24\": 0.55,\n    \"b25\": 0.5\n  }\n}\n```"], "5cede0eeda562983788cd285": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of end-to-end speech translation (ST) models, which suffer from data scarcity and performance gaps compared to text translation models. The proposed solution involves leveraging knowledge distillation, where a text translation model (teacher) helps train the ST model (student) by providing smoother output probabilities and other forms of guidance. This approach aims to enhance the ST model's translation quality significantly.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b11\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b8\": 0.7,\n    \"b9\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.55,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving end-to-end speech translation (ST) models, which typically suffer from performance issues due to scarce paired speech-text data. The proposed solution leverages knowledge distillation from a text translation model (teacher) to an end-to-end ST model (student) to improve the ST model's performance.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b11\": 0.9,\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the inferior performance of end-to-end speech translation (ST) models compared to traditional pipeline systems, primarily due to the scarcity of paired speech and text data. The proposed algorithm leverages knowledge distillation to improve the ST model by training it with both ground-truth translations and the outputs of a text translation model (teacher model).\",\n    \"Direct Inspiration\": {\n        \"reference_number\": [\"b10\", \"b11\", \"b12\", \"b13\", \"b14\"]\n    },\n    \"Indirect Inspiration\": {\n        \"reference_number\": [\"b6\", \"b7\", \"b8\", \"b9\"]\n    },\n    \"Other Inspiration\": {\n        \"reference_number\": [\"b3\", \"b20\", \"b21\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training an end-to-end speech translation (ST) model directly from speech signals without text guidance while achieving performance comparable to traditional methods. The authors propose a method based on knowledge distillation, where a text translation model (MT) serves as the teacher to guide the ST model (student). This approach leverages the strengths of text translation models to improve the efficiency and accuracy of end-to-end ST models.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b7\": 1.0,\n    \"b9\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b12\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b4\": 0.5,\n    \"b11\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving end-to-end speech translation (ST) models, which suffer from performance issues due to data scarcity. The authors propose using knowledge distillation, where a text translation model (teacher) guides the learning of the end-to-end ST model (student), leveraging the superior performance of text translation models to enhance the ST model's output.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b11\": 1.0,\n    \"b12\": 0.9,\n    \"b13\": 0.9,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7\n  }\n}\n```"], "5c2c7a9217c44a4e7cf3161b": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper center around generating expressive speech synthesis using end-to-end text-to-speech (TTS) models. The authors propose incorporating Variational Autoencoder (VAE) into the Tacotron2 model to learn and control the latent representation of speaker state, thus manipulating speaking styles in synthesized speech. The proposed methods include direct manipulation of latent variables and style transfer from reference audio, aiming to enhance the flexibility and expressiveness of TTS models.\",\n  \"Direct Inspiration\": [\n    \"b6\",\n    \"b11\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b3\",\n    \"b8\"\n  ],\n  \"Other Inspiration\": [\n    \"b0\",\n    \"b1\",\n    \"b2\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge is to control the speaking style in text-to-speech (TTS) synthesis while ensuring high-quality, expressive speech generation. The authors seek to introduce Variational Autoencoder (VAE) into the Tacotron2 model to learn the latent representation of speaker state, enabling style control and transfer.\",\n    \"inspirations\": \"The paper is inspired by recent progress in end-to-end TTS models and various works on expressive speech synthesis, particularly leveraging deep generative models like VAE and GAN.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\n      \"b11\"\n    ]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\n      \"b3\",\n      \"b6\",\n      \"b8\"\n    ]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\n      \"b0\",\n      \"b1\",\n      \"b2\"\n    ]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of controlling speaking styles in end-to-end text-to-speech (TTS) models by introducing Variational Autoencoders (VAEs) into Tacotron2. The proposed model aims to learn the latent representation of the speaker's state in a continuous space for style control and transfer, as well as for data augmentation.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b6\": 0.8,\n    \"b3\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.6,\n    \"b9\": 0.5\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of controlling speaking style in end-to-end text-to-speech (TTS) models by introducing a Variational Autoencoder (VAE) into Tacotron2. The proposed model aims to manipulate latent variables to control speaking style and achieve style transfer from reference audio. The paper highlights the novelty of the method in comparison to existing approaches, particularly in controlling speaking style and using end-to-end TTS models.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b3\": 0.9,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in expressive speech synthesis by incorporating Variational Autoencoders (VAE) into an end-to-end text-to-speech (TTS) model, specifically Tacotron2. This integration aims to control the speaking style in speech synthesis through direct manipulation of latent variables or variational inference from reference audio. The novel contributions include solving the KL-divergence collapse problem and demonstrating style transfer and control using the proposed model.\",\n  \"Direct Inspiration\": [\n    \"b6\",\n    \"b11\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b0\",\n    \"b3\",\n    \"b8\"\n  ],\n  \"Other Inspiration\": []\n}\n```"], "5c04967517c44a2c747089d5": ["```json\n{\n  \"Summary\": \"The main challenges identified in the paper are the need for large-scale parallel speech-to-translation training data for end-to-end ST models, and the limitations of multi-task learning in compromising performance and sub-optimal solutions. The paper proposes using a large amount of synthesized data from weakly supervised datasets and pre-trained components to improve the performance of end-to-end ST models, even without fully supervised training data.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b8\": 1,\n    \"b9\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training end-to-end speech-to-text (ST) translation models with limited parallel speech-to-translation data. The proposed solution involves using large datasets, multi-task learning, pretraining, and synthesizing training data from weakly supervised sources. The key contributions include demonstrating significant performance improvements with synthesized data and showing that high-quality end-to-end ST models can be trained without fully supervised data.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b8\": 1.0,\n    \"b9\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b14\": 0.8,\n    \"b19\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in training end-to-end speech-to-text (ST) translation systems, primarily due to the scarcity of large parallel speech-to-translation datasets. It leverages multi-task learning, pre-trained components, and synthesized data to overcome these limitations. The proposed solution includes training on extensive datasets and using weakly supervised data for significant performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b9\": 0.90,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.80,\n    \"b14\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.70,\n    \"b20\": 0.65,\n    \"b25\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge is the requirement for large-scale parallel speech-to-translation training data, which is expensive to obtain. This limits the performance of end-to-end speech-to-text (ST) translation systems.\",\n    \"inspirations\": \"The paper leverages multi-task learning and pre-training techniques from previous works to improve the performance of end-to-end ST systems. It also introduces the use of synthetic data generated from weakly supervised datasets to further enhance performance.\"\n  },\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b9\": 1.0,\n    \"b10\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b15\": 0.8,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training end-to-end speech-to-text (ST) translation models without requiring a large set of parallel speech-to-translation training data. It proposes using synthesized data from weakly supervised datasets and pre-trained components to improve the performance of end-to-end ST models. The paper demonstrates that these methods can significantly enhance the quality of the ST model, even outperforming traditional multi-task learning approaches.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b9\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```"], "5a73cb6317c44a0b30358203": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is building an integrated end-to-end TTS system that can be trained on <text, audio> pairs with minimal human annotation, overcoming the complex and error-prone nature of traditional TTS pipelines. The paper proposes Tacotron, an end-to-end generative TTS model based on the sequence-to-sequence (seq2seq) model with attention, which predicts raw spectrogram from characters, incorporating several techniques to improve the vanilla seq2seq model.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b12\": 0.9,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b22\": 0.7,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b6\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the complexity of modern text-to-speech (TTS) systems, which require extensive domain expertise and are laborious to design. The proposed solution is Tacotron, an end-to-end generative TTS model based on sequence-to-sequence (seq2seq) with attention. Tacotron aims to simplify the TTS pipeline by training on <text, audio> pairs without requiring phoneme-level alignment, thus addressing issues like compounded errors from multiple stages and the need for laborious feature engineering.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.95,\n    \"b3\": 0.9,\n    \"b14\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b20\": 0.75,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b13\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the complexity and limitations of modern text-to-speech (TTS) pipelines by proposing Tacotron, an end-to-end generative TTS model based on sequence-to-sequence (seq2seq) with attention. The challenges include the need for laborious feature engineering, error compounding in multi-stage models, and the difficulty in handling large variations at the signal level. Tacotron aims to alleviate these challenges by using characters as input and predicting raw spectrograms, employing several techniques to improve the vanilla seq2seq model.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b17\": 0.9,\n    \"b22\": 0.85,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.65,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of modern text-to-speech (TTS) systems, which are complex and laborious to design due to their multi-component nature. The proposed solution, Tacotron, is an end-to-end generative TTS model that simplifies this process by using sequence-to-sequence (seq2seq) with attention to directly predict spectrograms from text. This results in a more robust and scalable system that can be trained from scratch without requiring extensive feature engineering or pre-aligned phoneme data.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b17\": 0.9,\n    \"b22\": 0.85,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b2\": 0.65,\n    \"b12\": 0.6,\n    \"b13\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b16\": 0.45,\n    \"b9\": 0.4,\n    \"b21\": 0.35,\n    \"b23\": 0.3,\n    \"b8\": 0.25,\n    \"b11\": 0.2,\n    \"b4\": 0.15,\n    \"b19\": 0.1\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of modern text-to-speech (TTS) systems, specifically their complexity, the need for extensive domain expertise, and the compounded errors when using multiple independently trained components. The proposed solution, Tacotron, is an end-to-end generative TTS model based on the sequence-to-sequence (seq2seq) with attention paradigm. This model can be trained from scratch on <text, audio> pairs, eliminating the need for phoneme-level alignment and extensive feature engineering. It simplifies the pipeline, enhances robustness, and allows easier adaptation to new data, producing higher naturalness in synthesized speech compared to traditional methods.\",\n    \"Direct Inspiration\": {\n        \"b3\": 1,\n        \"b17\": 1,\n        \"b20\": 0.9,\n        \"b21\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.8,\n        \"b22\": 0.8,\n        \"b14\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.7,\n        \"b24\": 0.7,\n        \"b1\": 0.7,\n        \"b2\": 0.7,\n        \"b13\": 0.7,\n        \"b9\": 0.7,\n        \"b6\": 0.7,\n        \"b16\": 0.7\n    }\n}\n```"], "5ce3af9aced107d4c65f6b25": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in automatic speech recognition (ASR) using sequence-to-sequence (S2S) models, focusing on the inefficiency and complexity of traditional hybrid systems. The authors propose a novel ASR model based purely on self-attention mechanisms without any recurrence or convolution. Key contributions include the use of deep Transformer architectures and stochastic residual layers to regularize and improve performance.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b5\": 0.9,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses challenges in acoustic modeling within end-to-end ASR models, focusing on leveraging self-attention networks, specifically the Transformer model, to improve performance. The paper proposes novel methods such as using a deep Transformer architecture and stochastic residual connections inspired by Stochastic Residual Networks for image classification.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b6\": 0.85,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.75,\n    \"b8\": 0.7,\n    \"b19\": 0.65,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving end-to-end automatic speech recognition (ASR) by leveraging Transformer models with self-attention mechanisms. The authors propose a novel architecture that employs both an acoustic encoder and a character-generating decoder without any recurrence or convolution. They introduce the concept of stochastic depth inspired by Stochastic Residual Networks to enhance model regularization, which is crucial for achieving state-of-the-art performance on the Switchboard (SWB) benchmark.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1.0,\n        \"b9\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b7\": 0.7,\n        \"b15\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.5,\n        \"b10\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper revolve around improving the performance of end-to-end automatic speech recognition (ASR) systems using the Transformer model. The key contributions include demonstrating the importance of model depth, introducing stochastic residual layers inspired by Stochastic Residual Networks, and achieving state-of-the-art results on the Switchboard benchmark.\",\n  \"Direct Inspiration\": [\"b6\", \"b9\"],\n  \"Indirect Inspiration\": [\"b2\", \"b3\", \"b5\"],\n  \"Other Inspiration\": [\"b0\", \"b12\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of adapting the Transformer model for automatic speech recognition (ASR) by leveraging self-attention mechanisms without recurrence or convolution. Key contributions include demonstrating the importance of model depth, introducing stochastic depth inspired by Stochastic Residual Networks, and achieving state-of-the-art results on the Switchboard benchmark.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b10\": 0.5\n  }\n}\n```"], "5cede0eada562983788c8fb4": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of building a direct end-to-end speech-to-speech sequence transducer. The model aims to perform many-to-one voice conversion by projecting away non-linguistic information while retaining linguistic content. The novel contributions include using an end-to-end attention-based encoder-decoder model, multitask training with an ASR decoder, and applying these methods to voice normalization and improving speech intelligibility for hearing-impaired speakers.\",\n    \"Direct Inspiration\": {\n        \"b16\": 1.0,\n        \"b17\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.8,\n        \"b18\": 0.7,\n        \"b20\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.8,\n        \"b22\": 0.7,\n        \"b28\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the development of an end-to-end speech-to-speech sequence transducer that can normalize arbitrary speech from multiple accents and imperfections, generating output in the voice of a predefined target speaker. The authors propose a model that combines attention-based speech recognition and synthesis models and demonstrate its ability to improve intelligibility and naturalness of speech, particularly for speakers with vocal disabilities.\",\n  \"Direct Inspiration\": [\"b15\", \"b16\", \"b17\"],\n  \"Indirect Inspiration\": [\"b3\", \"b1\", \"b21\", \"b22\", \"b18\", \"b19\", \"b20\"],\n  \"Other Inspiration\": [\"b23\", \"b25\", \"b26\", \"b28\", \"b30\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents an end-to-end speech-to-speech sequence transducer, called Parrotron, which aims to normalize speech from multiple accents and background noise into a single predefined target voice. The model is evaluated on tasks such as voice normalization, speech conversion for deaf speakers, and speech separation. Key challenges include dealing with non-linguistic information, ensuring intelligibility and naturalness, and handling atypical speech from hearing-impaired individuals. The architecture integrates attention-based encoder-decoder models and multitask training with ASR decoders.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b16\": 0.9,\n    \"b17\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b22\": 0.75,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.65,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the challenge of creating an end-to-end speech-to-speech sequence transducer capable of normalizing speech from various accents and speakers into a single target voice. The proposed model, Parrotron, directly generates target speech spectrograms from input spectrograms without intermediate text representations. It leverages attention-based encoder-decoder models and multitask training with an ASR decoder to retain linguistic content while discarding speaker-specific characteristics and background noise.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b3\": 0.9,\n    \"b16\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b15\": 0.7,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of direct end-to-end speech-to-speech sequence transduction without intermediate discrete representation. The goal is to normalize speech from multiple accents and conditions into the voice of a single predefined target speaker, focusing on many-to-one voice conversion. The proposed model, Parrotron, is evaluated for voice normalization, intelligibility improvement for deaf speakers, and speech separation tasks.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1,\n        \"b3\": 1,\n        \"b16\": 0.9,\n        \"b17\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.7,\n        \"b10\": 0.6,\n        \"b18\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.5,\n        \"b15\": 0.5,\n        \"b22\": 0.5,\n        \"b23\": 0.5,\n        \"b28\": 0.5\n    }\n}\n```"], "5550443b45ce0a409eb4c3b9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of end-to-end speech recognition by proposing a system that replaces much of the traditional speech pipeline with a single recurrent neural network (RNN) architecture. It focuses on using deep bidirectional LSTM networks combined with Connectionist Temporal Classification (CTC) to directly transcribe raw speech waveforms with minimal preprocessing. The novel contributions include the integration of CTC for sequence transcription without prior alignment and an objective function to optimize word error rate.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b9\": 0.9,\n    \"b10\": 0.8,\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b13\": 0.6,\n    \"b14\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.4,\n    \"b16\": 0.4,\n    \"b18\": 0.4,\n    \"b20\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of developing an end-to-end speech recognition system by replacing traditional components with a single recurrent neural network (RNN) architecture. The proposed method uses deep bidirectional Long Short-Term Memory (LSTM) networks with a Connectionist Temporal Classification (CTC) output layer, trained directly on text transcripts without phonetic representation. The primary goal is to minimize preprocessing and human intervention, and to directly optimize the word error rate.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b9\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b15\": 0.7,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b4\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of creating an end-to-end speech recognition system using a recurrent neural network (RNN) architecture, specifically a deep bidirectional LSTM network with a Connectionist Temporal Classification (CTC) output layer. The goal is to replace traditional components like feature extraction, phonetic representation, and HMM-based modeling with an RNN trained directly on text transcripts. The key contributions include the use of spectrograms for minimal preprocessing, training with CTC for sequence transcription without prior alignment, and introducing a novel objective function to optimize word error rate.\",\n    \"Direct Inspiration\": {\n        \"b21\": 1.0,\n        \"b9\": 0.95,\n        \"b10\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.75,\n        \"b7\": 0.7,\n        \"b8\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5,\n        \"b13\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of replacing complex speech recognition pipelines with a single recurrent neural network (RNN) architecture, specifically using deep bidirectional LSTM networks with Connectionist Temporal Classification (CTC) output layers. The novel contributions include the use of spectrograms for minimal preprocessing, the elimination of phonetic representations and pronunciation dictionaries, and the introduction of a new objective function to optimize word error rate directly.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b9\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b8\": 0.75,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.65,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving end-to-end speech recognition by replacing the complex traditional pipelines with a single recurrent neural network (RNN) architecture. The authors propose using deep bidirectional LSTM networks with a Connectionist Temporal Classification (CTC) output layer to train the network directly on text transcripts, eliminating the need for phonetic representations and pronunciation dictionaries. A novel objective function to optimize word error rate is introduced, demonstrating state-of-the-art accuracy on the Wall Street Journal corpus.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b9\": 0.95,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.85,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.75,\n    \"b7\": 0.7\n  }\n}\n```"], "58437722ac44360f1082f160": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in Automatic Speech Recognition (ASR) by extending sequence-to-sequence (seq2seq) models with attention mechanisms. It introduces very deep hybrid convolutional and recurrent models, leveraging recent advancements in the vision community such as Network-in-Network (NiN), Batch Normalization (BN), Residual Networks (ResNets), and Convolutional LSTM (ConvLSTM). The aim is to improve the performance and robustness of ASR models by increasing their depth and expressive power while maintaining manageable parameters.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b2\": 0.9,\n    \"b3\": 0.85,\n    \"b20\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b13\": 0.65,\n    \"b17\": 0.65,\n    \"b18\": 0.7,\n    \"b23\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b24\": 0.55,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Automatic Speech Recognition (ASR) by proposing a novel seq2seq model incorporating very deep hybrid convolutional and recurrent neural networks. The model leverages advances from the vision community, specifically Network-in-Network (NiN), Batch Normalization (BN), Residual Networks (ResNets), and Convolutional LSTMs (ConvLSTM) to enhance depth, expressive power, and generalization while managing the number of parameters.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b2\": 0.9,\n    \"b20\": 0.9,\n    \"b22\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b7\": 0.7,\n    \"b13\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b21\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper revolve around improving the performance of sequence-to-sequence (seq2seq) models for automatic speech recognition (ASR) by leveraging advanced deep learning techniques from the computer vision community. The paper proposes a novel approach by integrating very deep convolutional neural networks (CNNs) and recurrent neural networks (RNNs) with techniques like Network-in-Network (NiN), Batch Normalization (BN), Residual Networks (ResNets), and Convolutional LSTMs (ConvLSTMs). The authors aim to improve the generalization, robustness, and training efficiency of ASR models.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b2\": 1.0,\n    \"b13\": 1.0,\n    \"b17\": 1.0,\n    \"b20\": 1.0,\n    \"b22\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b7\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b12\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper revolve around improving end-to-end speech recognition models by leveraging very deep hybrid convolutional and recurrent models, inspired by recent developments in the vision community. The key innovations introduced include Network-in-Network (NiN), Batch Normalization (BN), Residual Networks (ResNets), and Convolutional LSTMs (ConvLSTMs) to enhance model depth, expressiveness, and generalization while managing the number of parameters.\",\n    \"Direct Inspiration\": {\n        \"b0\": 0.9,\n        \"b2\": 0.85,\n        \"b17\": 0.8,\n        \"b20\": 0.8,\n        \"b22\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.8,\n        \"b21\": 0.75,\n        \"b23\": 0.85\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.7,\n        \"b5\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is improving the performance of sequence-to-sequence (seq2seq) models for Automatic Speech Recognition (ASR) by leveraging very deep neural network architectures, specifically hybrid convolutional and recurrent models. The paper is inspired by advancements in computer vision, particularly the success of very deep convolutional networks. Key innovations include the use of Network-in-Network (NiN), Batch Normalization (BN), Residual Networks (ResNets), and Convolutional LSTMs. These techniques are applied to enhance the encoder part of the seq2seq model, leading to significant improvements in Word Error Rate (WER).\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b2\": 0.9,\n    \"b13\": 0.95,\n    \"b20\": 0.9,\n    \"b22\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b17\": 0.75,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```"], "5e7345fd91e011a051ebf85f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges faced by End-to-End (E2E) Automatic Speech Recognition (ASR) models by proposing a deliberation model that combines acoustics and first-pass text hypotheses for second-pass decoding. It aims to bridge the quality gap between streaming RNN-T models and large conventional models by leveraging both acoustics and text in a two-pass framework, achieving significant WER reduction.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b7\": 0.8,\n    \"b19\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the quality of E2E ASR models compared to conventional models. The proposed solution is a deliberation-based two-pass E2E ASR system, which combines acoustic and first-pass text hypotheses for second-pass decoding, inspired by previous work on deliberation models and neural correction models.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b21\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include improving the performance of E2E ASR models to be competitive with state-of-the-art conventional models, particularly in terms of WER and handling proper nouns. The proposed algorithm combines acoustics and first-pass text hypotheses using a deliberation network to enhance second-pass decoding and achieve better performance.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b7\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges faced by end-to-end (E2E) automatic speech recognition (ASR) models compared to conventional models. It proposes a deliberation-based two-pass E2E ASR model that combines acoustics and first-pass text hypotheses for improved second-pass decoding. The novel contributions include bidirectional encoding of first-pass hypotheses and joint training of all model components.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b15\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b10\": 0.4,\n    \"b11\": 0.4,\n    \"b12\": 0.4,\n    \"b16\": 0.3,\n    \"b17\": 0.3,\n    \"b18\": 0.3,\n    \"b19\": 0.3,\n    \"b20\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving end-to-end (E2E) automatic speech recognition (ASR) models, specifically by proposing a deliberation-based two-pass E2E ASR model. The main challenge is to bridge the performance gap between streaming recurrent neural network transducers (RNN-T) and large conventional models. The proposed model combines acoustics and first-pass text hypotheses for second-pass decoding, leveraging bidirectional encoding for context information. The model is inspired by and builds upon previous work in two-pass frameworks, neural correction models, and MWER training.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```"], "5a73cb3517c44a0b303556bf": ["```json\n{\n  \"Summary\": \"The paper focuses on addressing the limitations of end-to-end automatic speech recognition (ASR) systems, particularly in language modeling capacity and real-time streaming capabilities. It explores the sequence-to-sequence RNN-T architecture and introduces methods such as pre-training with CTC, hierarchical-CTC, and the use of wordpieces for improved performance.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b13\": 0.85,\n    \"b14\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.6,\n    \"b7\": 0.55,\n    \"b10\": 0.5,\n    \"b11\": 0.5,\n    \"b16\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.45,\n    \"b20\": 0.4,\n    \"b21\": 0.4,\n    \"b22\": 0.4,\n    \"b23\": 0.35,\n    \"b24\": 0.35,\n    \"b25\": 0.35,\n    \"b26\": 0.35,\n    \"b27\": 0.35\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving end-to-end automatic speech recognition (ASR) systems, particularly in terms of their language modeling capabilities. The authors propose enhancements to the Recurrent Neural Network Transducer (RNN-T) architecture by incorporating text and pronunciation data, and investigate the use of wordpieces as sub-word units for better performance in ASR tasks.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b12\", \"b14\", \"b15\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b6\", \"b13\", \"b20\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b10\", \"b11\", \"b18\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in improving end-to-end automatic speech recognition (ASR) systems, particularly focusing on the RNN-Transducer (RNN-T) architecture. Inspired by the limitations of conventional ASR systems and previously proposed sequence-to-sequence models, the paper introduces methods to incorporate pronunciation and text data to enhance the language modeling capabilities of RNN-T. Additionally, the use of wordpieces as sub-word units is explored to improve recognition performance.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b15\": 1,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b13\": 0.8,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving end-to-end automatic speech recognition (ASR) systems, particularly focusing on the RNN-Transducer (RNN-T) model. The authors propose novel methods to enhance language modeling capacity by incorporating text and pronunciation data, and investigate the use of wordpieces as sub-word units for better performance.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b13\": 0.9,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b10\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are improving end-to-end ASR performance, particularly in streaming recognition, and incorporating text and pronunciation data to enhance language modeling. The proposed solution focuses on using the Recurrent Neural Network Transducer (RNN-T) architecture, incorporating pre-trained models for better initialization, and exploring the use of wordpieces as sub-word units.\",\n  \"Direct Inspiration\": [\"b12\", \"b14\", \"b15\"],\n  \"Indirect Inspiration\": [\"b6\", \"b13\", \"b20\"],\n  \"Other Inspiration\": [\"b10\", \"b11\"]\n}\n```"], "5ecbc6199fced0a24b4eefa9": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the scarcity of intent-labeled speech data required for training end-to-end (E2E) speech-to-intent (S2I) models. The paper proposes two novel methods to tackle this problem: (1) jointly training the S2I model with a text-to-intent (T2I) model using BERT-based text embeddings and (2) augmenting the S2I training data by converting text-to-intent data into synthetic speech-to-intent data using a text-to-speech (TTS) system.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.9,\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b10\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b14\": 0.7,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b9\": 0.5,\n    \"b12\": 0.6,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of training end-to-end (E2E) spoken language understanding (SLU) systems due to the scarcity of intent-labeled speech data. The authors propose two methods to leverage text-to-intent data: 1) Joint training of speech-to-intent and text-to-intent models with shared intent classification layer, and 2) Data augmentation by converting text-to-intent data into synthetic speech-to-intent data using a multi-speaker text-to-speech (TTS) system.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b4\": 0.8,\n    \"b6\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b7\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b15\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\"Scarcity of intent-labeled speech data\", \"Leveraging text-to-intent data for speech-to-intent training\", \"Integration of text and speech modalities in an end-to-end SLU system\"],\n    \"inspirations\": [\"Using pre-trained models like BERT for text embeddings\", \"Data augmentation techniques\"]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b0\", \"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b6\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b10\", \"b11\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b12\", \"b13\", \"b14\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of limited training data for end-to-end speech-to-intent (S2I) systems, proposing methods that leverage text-to-intent data to improve S2I performance. Key contributions include joint training of a speech-to-intent model and a text-to-intent model, and data augmentation through synthetic speech generation.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b6\": 1.0,\n    \"b10\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b7\": 0.8,\n    \"b5\": 0.75,\n    \"b12\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"This paper addresses the challenge of training end-to-end speech-to-intent (S2I) models with limited intent-labeled speech data. The authors propose two main methods: joint training of speech-to-intent and text-to-intent models using BERT-based text embeddings, and data augmentation by converting text-to-intent data into synthetic speech-to-intent data using a multi-speaker TTS system. The primary inspiration for these methods comes from the need to leverage text-based resources and embeddings to improve the performance of S2I models.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0,\n        \"b6\": 0.9,\n        \"b13\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b11\": 0.8,\n        \"b15\": 0.7,\n        \"b16\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.6,\n        \"b1\": 0.6,\n        \"b2\": 0.6,\n        \"b3\": 0.6,\n        \"b5\": 0.6,\n        \"b9\": 0.6,\n        \"b12\": 0.6\n    }\n}\n```"], "5b1643998fbcbf6e5a9bc25e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of phase inconsistency in single-channel speaker-independent multispeaker speech separation. It proposes a novel end-to-end speech separation algorithm that incorporates iterative phase reconstruction via time-frequency masking for signal-level approximation, achieving significant improvements over previous methods.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b6\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b1\": 0.7,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the cocktail party problem by proposing a novel end-to-end speech separation algorithm that incorporates iterative phase reconstruction via time-frequency masking for signal-level approximation. The primary challenge is the phase inconsistency problem in the time-frequency domain, which the proposed method aims to solve through deep learning-based phase reconstruction.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.85,\n    \"b1\": 0.80,\n    \"b5\": 0.75,\n    \"b6\": 0.70,\n    \"b20\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.60,\n    \"b14\": 0.55,\n    \"b27\": 0.50\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper focuses on solving the cocktail party problem by introducing an end-to-end speech separation algorithm that incorporates iterative phase reconstruction via time-frequency (T-F) masking for signal-level approximation. The proposed method significantly improves the scale-invariant signal-to-distortion ratio (SI-SDR) compared to previous methods.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1.0,\n        \"b0\": 0.9,\n        \"b1\": 0.9,\n        \"b5\": 0.8,\n        \"b6\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b20\": 0.7,\n        \"b14\": 0.7,\n        \"b13\": 0.6,\n        \"b10\": 0.6,\n        \"b11\": 0.6,\n        \"b12\": 0.6,\n        \"b18\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.5,\n        \"b22\": 0.5,\n        \"b23\": 0.5,\n        \"b24\": 0.5,\n        \"b25\": 0.5,\n        \"b26\": 0.5,\n        \"b27\": 0.5,\n        \"b28\": 0.5,\n        \"b29\": 0.5,\n        \"b30\": 0.5,\n        \"b31\": 0.5,\n        \"b32\": 0.5,\n        \"b33\": 0.5,\n        \"b34\": 0.5,\n        \"b35\": 0.5,\n        \"b36\": 0.5,\n        \"b37\": 0.5,\n        \"b38\": 0.5,\n        \"b39\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of phase inconsistency in single-channel speaker-independent multispeaker speech separation by proposing a novel end-to-end speech separation algorithm. This algorithm trains through iterative phase reconstruction via T-F masking for signal-level approximation, achieving significant improvements in scale-invariant SDR on the wsj0-2mix corpus.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b6\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b1\": 0.6,\n    \"b20\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b5\": 0.4,\n    \"b13\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the phase inconsistency problem in time-domain re-synthesis after performing T-F masking for speech separation. The novel approach proposed by the authors involves an end-to-end speech separation algorithm that trains through iterative phase reconstruction via T-F masking for signal-level approximation, which has shown significant improvements in performance compared to previous methods.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b6\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b1\": 0.7,\n    \"b5\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```"], "5d04e8dbda56295d08db13cf": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of online recognition, timestamping, and efficient computation in attention-based ASR models. It proposes a novel Continuous Integrate-and-Fire (CIF) mechanism inspired by the integrate-and-fire model from spiking neural networks. The CIF mechanism integrates weights and state information until a threshold is reached, indicating an acoustic boundary, which is then used for further recognition. Supporting strategies such as scaling, quantity loss, and tail handling are also introduced to improve performance.\",\n  \"Direct Inspiration\": [\"b4\", \"b5\"],\n  \"Indirect Inspiration\": [\"b1\", \"b3\"],\n  \"Other Inspiration\": [\"b7\", \"b8\", \"b13\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of attention-based models in ASR, particularly their inefficiency in online recognition and timestamping. The proposed Continuous Integrate-and-Fire (CIF) model draws inspiration from the integrate-and-fire model in spiking neural networks, aiming to provide efficient, soft, and monotonic alignment in the encoder-decoder framework.\",\n  \"Direct Inspiration\": [\"b4\", \"b5\"],\n  \"Indirect Inspiration\": [\"b0\", \"b1\", \"b3\"],\n  \"Other Inspiration\": [\"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies of attention-based ASR models, particularly their inability to support online recognition and their computational burdens. Inspired by the integrate-and-fire model from spiking neural networks, the authors propose a Continuous Integrate-and-Fire (CIF) mechanism for more efficient and monotonic alignment in ASR systems. This model integrates acoustic information until a threshold is reached, then fires this information to the decoder for label prediction. Supporting strategies like scaling, quantity loss, and tail handling are also introduced to improve performance.\",\n  \"Direct Inspiration\": [\"b4\", \"b5\"],\n  \"Indirect Inspiration\": [\"b1\", \"b3\"],\n  \"Other Inspiration\": [\"b14\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in automatic speech recognition (ASR) systems, particularly the inefficiency and inability to support online recognition of attention-based models due to their requirement to refer to the entire encoded sequence. The proposed solution, Continuous Integrate-and-Fire (CIF), is inspired by the integrate-and-fire model from spiking neural networks and aims to create a soft and monotonic alignment mechanism that efficiently locates acoustic boundaries and supports back-propagation.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b4\", \"b5\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b0\", \"b1\", \"b2\", \"b3\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b6\", \"b10\", \"b11\", \"b13\", \"b14\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of attention-based ASR models in real applications, specifically their lack of support for online recognition and inefficiency due to non-frame-synchronous processing. It proposes a novel Continuous Integrate-and-Fire (CIF) model inspired by the integrate-and-fire mechanism in spiking neural networks, aiming to achieve soft and monotonic alignment in the encoder-decoder framework for ASR. The CIF model accumulates weights and integrates information until an acoustic boundary is reached, and then fires the integrated information to the decoder. The paper also introduces several strategies to improve training and performance of the CIF-based model.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b1\": 0.7,\n    \"b3\": 0.7\n  }\n}\n```"], "5cede102da562983788e2310": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of speech translation by proposing a direct model, a two-stage model, and an attention-passing variant. The primary issues examined include the performance comparison between direct and cascaded models under varying data conditions, and improving data efficiency without sacrificing accuracy. Key methods include multi-task training and the introduction of a two-stage model with an attention-passing mechanism.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.9,\n    \"b6\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.8,\n    \"b7\": 0.7,\n    \"b26\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving speech translation models, specifically comparing direct and cascaded approaches. It identifies two main issues: the data efficiency of direct models and the error propagation in cascaded models. The paper proposes a two-stage model and an attention-passing variant to overcome these challenges.\",\n  \"Direct Inspiration\": [\"b23\", \"b10\"],\n  \"Indirect Inspiration\": [\"b24\", \"b6\"],\n  \"Other Inspiration\": [\"b7\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in speech translation, specifically comparing direct and cascaded models, and improving data efficiency through multi-task and two-stage models. The key contributions include empirical evidence favoring direct models with sufficient data, multi-task training for direct models using auxiliary ASR and MT data, and a two-stage model with attention-passing to improve data efficiency while mitigating error propagation.\",\n  \"Direct Inspiration\": [\"b23\", \"b10\", \"b24\", \"b6\"],\n  \"Indirect Inspiration\": [\"b2\", \"b26\"],\n  \"Other Inspiration\": [\"b7\", \"b3\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in speech translation by comparing direct models and cascaded models. It proposes a two-stage model and an attention-passing variant to improve data efficiency and reduce error propagation.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b6\": 0.8,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.7,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving speech translation models, particularly focusing on the comparison between direct and cascaded models. It proposes a direct model, a two-stage model, and an attention-passing variant of the two-stage model to tackle issues such as data efficiency and error propagation.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.9,\n    \"b6\": 0.9,\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.8,\n    \"b26\": 0.7,\n    \"b7\": 0.7\n  }\n}\n```"], "5dee1b4b3a55ac3d409adcbb": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of asynchronous encoding and decoding in attention-based sequence-to-sequence models, which limits their application in online speech recognition. To tackle this, the authors propose the Sync-Transformer model that performs encoding and decoding simultaneously. This model combines elements from the Transformer and Self-Attention Transducer (SA-T) models, and introduces a forward-backward algorithm to optimize alignment paths.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b5\": 1.0,\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of asynchronous encoding and decoding in conventional attention-based sequence-to-sequence models, which limits their application in online speech recognition. The proposed solution, the synchronous transformer model (Sync-Transformer), allows for simultaneous encoding and decoding. This model combines the transformer with the self-attention transducer (SA-T) and incorporates a forward-backward algorithm to optimize alignment paths. Key modifications include focusing on left contexts during self-attention and processing input sequences chunk by chunk.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b5\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b10\": 0.75,\n    \"b11\": 0.75,\n    \"b12\": 0.75,\n    \"b13\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b7\": 0.5,\n    \"b8\": 0.5,\n    \"b14\": 0.5,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a synchronous transformer (Sync-Transformer) model for online speech recognition, addressing the asynchronous nature of conventional attention-based sequence-to-sequence models. The Sync-Transformer synchronizes encoding and decoding processes, utilizing a modified self-attention mechanism to focus only on past contexts and a forward-backward algorithm for optimizing alignment paths.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b2\": 0.5,\n    \"b3\": 0.5,\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b14\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of synchronous encoding and decoding in speech recognition models, proposing the Sync-Transformer model. The authors aim to overcome the asynchronous nature of conventional attention-based sequence-to-sequence models and the complexity of existing methods by introducing a synchronous approach that combines the transformer and self-attention transducer (SA-T).\",\n  \"Direct Inspiration\": [\n    \"b1\",\n    \"b5\",\n    \"b9\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b6\",\n    \"b7\",\n    \"b8\",\n    \"b10\",\n    \"b11\",\n    \"b12\",\n    \"b13\"\n  ],\n  \"Other Inspiration\": [\n    \"b14\",\n    \"b15\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying attention-based sequence-to-sequence models for online speech recognition, which typically involves asynchronous encoding and decoding processes. This prevents their application in online settings. The proposed solution is the Sync-Transformer, which performs encoding and decoding simultaneously by modifying the self-attention mechanism to only focus on left contexts and adopting a chunkwise processing approach. This model also leverages a forward-backward algorithm to optimize alignment paths, similar to methods used in Neural Transducer and RNN-Transducer models.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b10\": 0.75,\n    \"b11\": 0.75,\n    \"b12\": 0.75,\n    \"b13\": 0.7,\n    \"b15\": 0.65\n  },\n  \"Other Inspiration\": {}\n}\n```"], "5b3d98cc17c44a510f801c3a": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating realistic talking head videos from a single still image and an audio track. The proposed solution is an end-to-end temporal GAN model that captures natural facial expressions and synchronized mouth movements, handles variable length sequences, and maintains the identity of the speaker throughout the sequence. The key contributions include a novel RNN-based generator and sequence discriminator, subject-independent generation, and a comprehensive performance assessment.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b19\": 0.85,\n    \"b14\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b11\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating realistic speech-driven facial animations from a single still image and an audio track. The proposed solution introduces a temporal generative adversarial network (GAN) that utilizes an RNN-based generator and sequence discriminator to produce synchronized mouth movements and natural facial expressions. This method is subject independent, avoids handcrafted features, and requires no post-processing. The paper also conducts a comprehensive assessment of the model's performance, including image quality metrics, lip-reading accuracy, face verification, and realism through an online Turing test.\",\n    \"Direct Inspiration\": {\n        \"b4\": 0.9,\n        \"b14\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.7,\n        \"b19\": 0.7,\n        \"b24\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b21\": 0.6,\n        \"b23\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating realistic talking head videos from a single still image and an audio track. The authors propose a temporal generative adversarial network (GAN) that captures the dynamics of the entire face, including synchronized mouth movements and natural facial expressions. The method is subject-independent and does not rely on handcrafted audio or visual features, making it versatile and robust for various applications.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b14\": 0.9,\n    \"b19\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b21\": 0.7,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.5,\n    \"b11\": 0.6,\n    \"b5\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a temporal generative adversarial network (GAN) that generates realistic talking head videos from raw audio and a single still image. The key challenges addressed include achieving synchronized lip movements, natural facial expressions, and subject independence without handcrafted features or post-processing.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b19\": 0.6,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b21\": 0.5,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of generating realistic talking heads from a single still image and an audio track, focusing on synchronized lip movements and natural facial expressions. The proposed method, a temporal generative adversarial network (GAN), is subject-independent and generates high-quality, natural video sequences without post-processing. The approach is novel in its use of an RNN-based generator and sequence discriminator, handling variable length sequences and capturing facial dynamics comprehensively.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b4\": 0.85,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```"], "5bbacb9e17c44aecc4eaff2b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of data efficiency in end-to-end text-to-speech (TTS) training by leveraging large-scale, publicly available, and unpaired text and speech data. The proposed approach involves a semi-supervised framework using Tacotron, incorporating pre-trained word vectors for the encoder and pre-training the decoder with unpaired speech data to improve performance using minimal paired data.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b7\": 0.9,\n    \"b6\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b12\": 0.5,\n    \"b13\": 0.5,\n    \"b14\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving data efficiency for end-to-end TTS training by leveraging large-scale, publicly available, and unpaired text and speech data. The proposed semi-supervised framework involves transferring textual and acoustic representations learned from unpaired data to Tacotron and fine-tuning with a small amount of paired data. Key novel methods include encoder conditioning on pre-trained word vectors and decoder pre-training.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.9,\n        \"b7\": 0.8,\n        \"b8\": 0.8,\n        \"b9\": 0.8,\n        \"b14\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.6,\n        \"b12\": 0.6,\n        \"b13\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.5,\n        \"b15\": 0.5,\n        \"b16\": 0.4,\n        \"b17\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving data efficiency in end-to-end text-to-speech (TTS) models by leveraging large-scale, publicly available, and unpaired text and speech data. The proposed approach includes a semi-supervised framework for training Tacotron, which involves transferring textual and acoustic representations from unpaired data to the model and then fine-tuning with a small amount of paired data.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.9,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of data efficiency in end-to-end text-to-speech (TTS) systems, particularly focusing on reducing the need for large, high-quality paired text-audio datasets. The authors propose a semi-supervised framework for training the Tacotron model by leveraging unpaired text and speech data. Key methods include conditioning the encoder on pre-trained word vectors and pre-training the decoder with an independent speech dataset.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.95,\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b8\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.75,\n    \"b14\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b16\": 0.65,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"The primary challenge addressed in the paper is improving the data efficiency for end-to-end TTS training by leveraging large-scale, publicly available, and unpaired text and speech data.\",\n        \"inspirations\": \"The inspirations include previous work on unsupervised and weakly supervised learning for TTS, as well as techniques for conditioning on pre-trained word vectors and pre-training decoders.\"\n    },\n    \"Direct Inspiration\": {\n        \"b0\": 0.95,\n        \"b6\": 0.90\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.85,\n        \"b12\": 0.85,\n        \"b13\": 0.85,\n        \"b14\": 0.80,\n        \"b16\": 0.80\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.75,\n        \"b8\": 0.75,\n        \"b9\": 0.70,\n        \"b10\": 0.70,\n        \"b17\": 0.70\n    }\n}\n```"], "5dd6604a3a55ac78684acf68": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of overfitting in end-to-end speech-to-text translation models due to limited paired translated speech-to-text data. It proposes the use of spectrogram augmentation (SpecAugment) as a data augmentation technique to improve model generalization and robustness.\",\n    \"Direct Inspiration\": {\n        \"b15\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.8,\n        \"b13\": 0.7,\n        \"b22\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.5,\n        \"b7\": 0.5,\n        \"b18\": 0.5,\n        \"b27\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the overfitting of end-to-end speech-to-text translation (ST) models due to a lack of sufficient paired translated speech-to-text data. To tackle this, the authors propose the use of spectrogram augmentation (SpecAugment) to improve generalization and robustness of these models. The main contributions include an extensive empirical investigation of SpecAugment on top-performing ST systems and the exploration of its effectiveness with varying amounts of training data.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b27\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training end-to-end speech-to-text translation (ST) models with limited paired translated speech-to-text data, which can lead to overfitting. The authors propose using SpecAugment, a spectrogram augmentation technique, to improve generalization and robustness of the direct ST models. The study investigates the effectiveness of SpecAugment in different data scenarios and demonstrates significant performance gains and reduced overfitting.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b18\": 0.4,\n    \"b17\": 0.3,\n    \"b22\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in training end-to-end speech-to-text (ST) translation models, particularly the issue of overfitting due to limited paired speech-to-translation data. The core contribution is the study and application of spectrogram augmentation (SpecAugment) to enhance model robustness and generalization.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b7\": 0.7,\n    \"b18\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the overfitting of end-to-end speech-to-text (ST) models due to the limited availability of paired translated speech-to-text data. The proposed solution is the use of spectrogram augmentation (SpecAugment) to enhance the generalization and robustness of these models. The paper explores the effectiveness of SpecAugment in improving model performance and reducing overfitting, specifically in low-resource scenarios.\",\n  \"Direct Inspiration\": [\"b15\", \"b16\"],\n  \"Indirect Inspiration\": [\"b14\", \"b13\"],\n  \"Other Inspiration\": [\"b11\", \"b12\", \"b7\", \"b18\"]\n}\n```"], "5c04966a17c44a2c74708959": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in end-to-end speech recognition systems, particularly the difficulty of learning pronunciation patterns in languages like English. It proposes a novel Pronunciation-Assisted Sub-Word Modeling (PASM) method that incorporates pronunciation dictionaries and an aligner to improve sub-word segmentation, enhancing ASR performance.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving end-to-end speech recognition systems by incorporating pronunciation information into sub-word modeling. The authors propose a Pronunciation-Assisted Sub-word Modeling (PASM) method that uses a pronunciation dictionary and an alignment tool called fast align to better segment sub-words in a way that aligns phonetic units with letter sequences. This approach aims to enhance the system's ability to learn pronunciation patterns, thereby improving performance in automatic speech recognition (ASR).\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b6\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b7\": 0.8,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating pronunciation information into end-to-end ASR systems, which typically rely on sub-word modeling based on spelling alone. The novel method proposed is Pronunciation-Assisted Sub-word Modeling (PASM), which utilizes a pronunciation dictionary and alignment techniques to improve the segmentation process for better ASR performance.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b3\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b4\": 0.7,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving end-to-end automatic speech recognition (ASR) systems by incorporating pronunciation information into sub-word modeling. The novel method proposed is called pronunciation-assisted sub-word modeling (PASM), which uses fast align to align a pronunciation lexicon and guide sub-word segmentation. The main goal is to improve the mapping between phonemes and spellings, thereby enhancing ASR performance.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.85,\n    \"b6\": 0.80,\n    \"b7\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.70,\n    \"b10\": 0.65,\n    \"b12\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is improving end-to-end ASR systems by incorporating pronunciation information into sub-word modeling. The proposed method, Pronunciation-Assisted Sub-word Modeling (PASM), leverages a pronunciation dictionary and the Fast Align tool to generate consistent letter-phoneme pairs, which guide the sub-word segmentation process. This method aims to enhance the learning of pronunciation patterns, especially for languages with weak spelling-pronunciation correspondence, such as English.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b4\": 0.6,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b5\": 0.5\n  }\n}\n```"], "58d82fd2d649053542fd75bc": ["```json\n{\n    \"Summary\": \"The primary challenge in the paper is to improve the training efficiency and effectiveness of end-to-end neural network models for automatic speech recognition by combining CNNs with CTC, without relying on recurrent layers like RNNs/LSTMs. The proposed approach aims to leverage the strengths of CNNs in modeling local structures and temporal dependencies within the spectrogram, using a combination of small filter sizes, multiple stacked convolutional layers, and pooling along the frequency band.\",\n    \"Direct Inspiration\": [\"b7\", \"b8\", \"b9\", \"b10\", \"b21\"],\n    \"Indirect Inspiration\": [\"b1\", \"b2\", \"b3\", \"b12\", \"b13\"],\n    \"Other Inspiration\": [\"b16\", \"b17\", \"b18\", \"b19\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of training speed and gradient issues in RNN/LSTM-based end-to-end speech recognition systems. It proposes a novel framework combining CNNs with CTC for end-to-end speech recognition, avoiding the use of intermediate recurrent layers.\",\n  \"Direct Inspiration\": [\"b7\", \"b8\", \"b9\", \"b21\"],\n  \"Indirect Inspiration\": [\"b10\", \"b11\", \"b12\", \"b13\", \"b16\"],\n  \"Other Inspiration\": [\"b3\", \"b14\", \"b15\", \"b17\", \"b18\", \"b19\", \"b20\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of combining Convolutional Neural Networks (CNNs) with Connectionist Temporal Classification (CTC) for end-to-end speech recognition without relying on Recurrent Neural Networks (RNNs). It proposes a novel framework that leverages the strengths of both CNNs and CTC to overcome the limitations of traditional hybrid systems and RNN-based models, such as slow training speeds and gradient vanishing/exploding issues.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b21\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b10\", \"b22\", \"b23\", \"b27\", \"b29\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b7\", \"b12\", \"b13\", \"b3\"]\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in hybrid automatic speech recognition systems that combine CNNs with HMMs/GMMs, which often require laborious and time-consuming hyperparameter tuning. It proposes an end-to-end speech recognition framework combining CNNs with CTC to capture long-term dependencies without intermediate recurrent layers. This approach is evaluated on the TIMIT dataset, with results comparable to those obtained using multiple layers of LSTMs.\",\n    \"Direct Inspiration\": [\n        \"b3\",\n        \"b7\",\n        \"b21\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b8\",\n        \"b10\",\n        \"b12\",\n        \"b13\",\n        \"b16\"\n    ],\n    \"Other Inspiration\": [\n        \"b0\",\n        \"b1\",\n        \"b2\"\n    ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of hybrid speech recognition systems that combine CNNs with HMMs/GMMs, such as separate training of modules and laborious hyperparameter tuning. It proposes an end-to-end speech framework combining CNNs with CTC, avoiding recurrent layers to improve training speed and avoid gradient issues.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b21\": 0.9,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b10\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b2\": 0.5,\n    \"b11\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```"], "53e9bd9eb7602d9704a48620": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of resource allocation inefficiency in modern out-of-order processors by proposing a new algorithm called Cherry (Checkpointed Early Resource Recycling). Cherry decouples the recycling of resources from the retirement of instructions, enabling more efficient resource utilization and potential performance improvements. The paper also explores the integration of Cherry with Speculative Multithreading (SM) to further enhance instruction overlap within and across threads.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b8\": 0.8,\n    \"b13\": 0.8,\n    \"b18\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b12\": 0.7,\n    \"b17\": 0.7,\n    \"b20\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The main challenge addressed in this paper is the inefficient resource utilization in modern out-of-order processors due to instructions holding onto resources until they retire. The proposed solution, CHeckpointed Early Resource RecYcling (Cherry), aims to decouple resource recycling from instruction retirement, thereby improving performance by utilizing resources more efficiently. Cherry relies on state checkpointing to maintain a correct architectural state during exceptions and uses the cache hierarchy to support long checkpoint intervals.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9,\n        \"b8\": 0.9,\n        \"b13\": 0.9,\n        \"b18\": 0.9,\n        \"b19\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.7,\n        \"b5\": 0.7,\n        \"b20\": 0.7,\n        \"b22\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.6,\n        \"b17\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge addressed in the paper is the inefficient resource utilization in modern out-of-order processors due to instructions holding resources until retirement. This causes performance limitations as new instructions may not find available resources to allocate.\",\n    \"Algorithm\": \"The paper proposes CHeckpointed Early Resource RecYcling (Cherry), which decouples the recycling of resources from the retirement of instructions. This allows resources to be released early and gradually, enhancing their utilization efficiency. The approach relies on state checkpointing to ensure precise exception handling and uses the cache hierarchy to buffer memory system updates.\"\n  },\n  \"Direct Inspiration\": [\"b0\", \"b5\", \"b20\", \"b22\"],\n  \"Indirect Inspiration\": [\"b3\", \"b8\", \"b13\", \"b18\", \"b19\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the primary challenge of resource inefficiency in modern out-of-order processors due to instructions holding onto resources until retirement. The authors propose the Cherry mechanism, which involves checkpointed early resource recycling to improve resource utilization and processor performance. Cherry utilizes state checkpointing to ensure precise exceptions and buffer memory system updates through the cache hierarchy, allowing longer checkpoint intervals and efficient resource recycling.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.95,\n    \"b8\": 0.95,\n    \"b13\": 0.95,\n    \"b18\": 0.95,\n    \"b19\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b5\": 0.8,\n    \"b20\": 0.8,\n    \"b22\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of resource utilization inefficiency in modern out-of-order processors caused by the retention of resources until instruction retirement. It proposes the CHeckpointed Early Resource RecYcling (Cherry) algorithm, which decouples resource recycling from instruction retirement to enhance performance and resource efficiency. Cherry leverages state checkpointing to handle exceptions and uses the cache hierarchy to manage memory system updates.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b8\": 0.9,\n    \"b13\": 0.9,\n    \"b18\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b5\": 0.7,\n    \"b20\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```"], "53e99f5db7602d970283022a": ["```json\n{\n  \"Summary\": \"The paper addresses the critical issue of limiting power consumption in computing, particularly in portable and mobile platforms, as well as in large-scale server farms. It proposes techniques and algorithms to reduce power consumption at the logic, architecture, and operating systems levels, focusing on CMOS logic circuits and emphasizing the importance of reducing voltage, activity, and efficient parallel processing.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the critical issue of limiting power consumption in computing, particularly for portable and mobile platforms as well as server farms. It proposes various techniques at the logic, architecture, and operating systems levels to reduce power consumption, focusing on CMOS logic circuits. The main challenges are the rapid growth in power consumption and power density, and the limitations of reducing supply voltage due to increased leakage current. Key techniques include clock gating, half-frequency clocks, asynchronous logic, memory banking, and voltage scaling.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.7,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the critical issue of limiting power consumption in various computing platforms, including portable and mobile devices, as well as server farms. It discusses the rapid growth in power consumption and power density, and presents a model of power-performance trade-offs for CMOS logic circuits. The paper proposes several techniques at the logic, architecture, and operating systems levels to reduce power consumption, such as clock gating, half-frequency clocks, asynchronous logic, memory banking, and voltage scaling.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b2\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.85,\n    \"b4\": 0.80,\n    \"b5\": 0.75,\n    \"b6\": 0.70,\n    \"b7\": 0.65\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the critical issue of power consumption in computing, particularly for portable devices and large server farms. It proposes architectural improvements, parallel processing, and voltage scaling techniques as solutions. The focus is on CMOS logic circuits and the challenges of reducing power without significantly impacting performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.9,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the critical issue of limiting power consumption in computing, particularly in portable devices and large-scale server farms. It explores various techniques at the logic, architecture, and operating system levels to reduce power consumption, focusing on CMOS logic circuits. The paper introduces power equations to model power-performance trade-offs and examines methods like clock gating, voltage scaling, and parallel processing to achieve power efficiency.\",\n    \"Direct Inspiration\": [\"b1\", \"b2\", \"b3\"],\n    \"Indirect Inspiration\": [\"b4\", \"b5\"],\n    \"Other Inspiration\": [\"b6\", \"b7\"]\n}\n```"], "53e99804b7602d970201668d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving fetch performance in processor designs without increasing complexity and cost. The authors propose a new fetch engine architecture based on instruction streams, which provides high performance and low complexity by leveraging high-level programming constructs. The key innovation is the next stream predictor, which fetches multiple basic blocks in a single cycle without complex circuitry.\",\n  \"Direct Inspiration\": [\"b29\", \"b31\"],\n  \"Indirect Inspiration\": [\"b8\", \"b25\", \"b30\"],\n  \"Other Inspiration\": [\"b38\", \"b2\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper involve designing a fetch engine that balances high performance with low complexity and implementation cost. The proposed solution is the stream fetch architecture, which focuses on fetching instruction streams efficiently.\",\n    \"inspirations\": \"The paper is inspired by previous works on fetch architectures, particularly those that deal with dynamic branch prediction and trace cache mechanisms.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b29\", \"b31\", \"b25\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b38\", \"b30\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b8\", \"b22\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents a novel stream fetch architecture designed to improve fetch performance in processor designs while maintaining low complexity and cost. The challenges addressed include the need for high fetch performance, the complexity of existing fetch architectures, and the inefficiency of fetching instruction traces. The proposed solution utilizes instruction streams, which are sequences of instructions between taken branches, to optimize fetch performance and reduce complexity.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b29\", \"b31\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b38\", \"b8\", \"b22\", \"b30\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b25\", \"b13\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing fetch performance in processor architectures by proposing a novel fetch engine based on instruction streams. This engine improves performance while reducing complexity and cost by leveraging high-level programming constructs and a next stream predictor for stream-level sequencing.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.85,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.75,\n    \"b8\": 0.7,\n    \"b20\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving fetch performance in processor design while minimizing complexity and resource usage. The authors propose a novel fetch architecture based on instruction streams, which provides semantic information about code behavior and improves performance by fetching long sequences of instructions with minimal complexity. The stream fetch architecture introduces a next stream predictor, enabling high performance comparable to trace caches but with lower implementation cost.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b31\": 1.0,\n    \"b25\": 1.0,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b30\": 0.8,\n    \"b26\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b10\": 0.6,\n    \"b33\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```"], "5e16fa233a55acac60fd369d": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of data leakage in multi-node distributed learning systems by developing an improved method to extract ground-truth labels from shared gradients with 100% accuracy, thereby enhancing data extraction fidelity. The proposed method, iDLG, builds upon the shortcomings of the existing DLG technique by Zhu et al. (b0).\",\n    \"Direct Inspiration\": {\n        \"b0\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b7\": 0.7,\n        \"b8\": 0.7,\n        \"b9\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.5,\n        \"b2\": 0.5,\n        \"b4\": 0.5,\n        \"b5\": 0.5,\n        \"b6\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the leakage of private training data from shared gradients in multi-node distributed learning systems, inspired by the prior work 'Deep Leakage from Gradient' (DLG) by Zhu et al. [b0]. The proposed algorithm, Improved DLG (iDLG), addresses the shortcomings of DLG by introducing an analytical approach to reliably extract ground-truth labels from shared gradients, thereby improving the fidelity of the extracted data.\",\n  \"Direct Inspiration\": [\"b0\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b6\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of data privacy in distributed learning systems by proposing an improved method called iDLG to extract ground-truth labels from shared gradients with 100% accuracy. This method builds upon the existing DLG approach, overcoming its limitations in generating wrong labels and low-quality dummy data.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b3\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of data leakage in distributed learning systems by improving upon the Deep Leakage from Gradient (DLG) method, proposing an analytical approach to extract ground-truth labels with 100% accuracy from shared gradients.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b2\": 0.7,\n        \"b3\": 0.7,\n        \"b4\": 0.7,\n        \"b5\": 0.7,\n        \"b6\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.6,\n        \"b8\": 0.6,\n        \"b9\": 0.6,\n        \"b10\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of data leakage in multi-node distributed learning systems, particularly through shared gradients. It introduces an improved method, iDLG, which reliably extracts ground-truth labels from shared gradients by leveraging the signs of gradient values. This new method promises higher accuracy and fidelity in data extraction compared to the previous DLG approach.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b2\": 0.5,\n    \"b3\": 0.5,\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.5,\n    \"b10\": 0.6\n  }\n}\n```"], "5c9deddb3cb210d271be80f6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving LLT miss frequency and latency in heterogeneous CPU-GPU systems. The proposed algorithm, DUCATI, combines two mechanisms: Unified Cache and TLB (UCAT) and DRAM-TLB to extend TLB coverage and reduce LLT miss penalties.\",\n  \"Direct Inspiration\": {\n    \"b48\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b45\": 0.7,\n    \"b58\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b53\": 0.6,\n    \"b54\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Last-Level TLB (LLT) coverage and reducing LLT miss latency in heterogeneous CPU-GPU systems. It proposes two novel hardware mechanisms: Unified Cache and TLB (UCAT), which reduces LLT miss frequency by allowing the LLC to hold TLB entries, and a DRAM-TLB, which reduces LLT miss penalty by storing TLB entries in GPU DRAM. The combination of these mechanisms, termed DUCATI, significantly enhances TLB coverage and reduces LLT miss latency, resulting in substantial performance improvements for high-performance computing workloads.\",\n  \"Direct Inspiration\": {\n    \"b48\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b45\": 0.7,\n    \"b58\": 0.7,\n    \"b29\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Last-Level TLB (LLT) coverage and reducing LLT miss latency in heterogeneous CPU-GPU systems. The proposed solutions include a Unified Cache and TLB (UCAT) and a DRAM-TLB to enhance TLB coverage and reduce translation latency, culminating in the DUCATI architecture which combines both approaches.\",\n  \"Direct Inspiration\": {\n    \"b48\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b5\": 0.7,\n    \"b23\": 0.7,\n    \"b29\": 0.6,\n    \"b45\": 0.6,\n    \"b58\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b6\": 0.5,\n    \"b11\": 0.5,\n    \"b18\": 0.5,\n    \"b42\": 0.5,\n    \"b53\": 0.5,\n    \"b54\": 0.5,\n    \"b60\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Last-Level TLB (LLT) performance in heterogeneous CPU-GPU systems by proposing novel hardware mechanisms, specifically Unified Cache and TLB (UCAT) and DRAM-TLB, to extend TLB reach and reduce LLT miss penalties. The proposed DUCATI framework combines these mechanisms to improve TLB coverage and LLT miss latency significantly.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b48\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b2\", \"b5\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b23\", \"b29\", \"b33\", \"b45\", \"b58\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper are improving the Last-Level TLB (LLT) coverage and reducing LLT miss penalty in heterogeneous CPU-GPU systems.\",\n    \"inspirations\": \"The algorithm proposed is DUCATI, which combines UCAT (Unified Cache and TLB) to increase on-die TLB coverage and DRAM-TLB to reduce LLT miss penalty.\"\n  },\n  \"Direct Inspiration\": {\n    \"b48\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b2\": 0.75,\n    \"b5\": 0.7,\n    \"b45\": 0.65,\n    \"b58\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.55,\n    \"b10\": 0.5\n  }\n}\n```"], "5e3940c73a55ace46ed436d2": ["```json\n{\n  \"Summary\": \"The paper focuses on incorporating human knowledge into AI through knowledge representation and reasoning, specifically using knowledge graphs (KGs). Key challenges include effective knowledge representation learning (KRL) and knowledge graph embedding (KGE) to map entities and relations into low-dimensional vectors while capturing their semantic meanings. The paper also addresses knowledge acquisition tasks such as knowledge graph completion (KGC), triple classification, entity recognition, and relation extraction.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b6\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.65,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include effective knowledge representation and reasoning inspired by human problem-solving, and the development of knowledge graphs and knowledge representation learning (KRL) techniques. The paper aims to survey recent advances in KRL, focusing on embedding techniques and their applications in various AI tasks.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b12\": 1.0,\n    \"b13\": 1.0,\n    \"b14\": 1.0,\n    \"b15\": 0.9,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b8\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of knowledge representation and reasoning in artificial intelligence by leveraging knowledge graphs. It reviews recent advances in knowledge representation learning (KRL) or knowledge graph embedding (KGE), focusing on embedding entities and relations into low-dimensional vectors while capturing their semantic meanings. The paper categorizes KRL into four aspects: representation space, scoring function, encoding models, and auxiliary information.\",\n    \"Direct Inspiration\": {\n        \"b11\": 1.0,\n        \"b12\": 1.0,\n        \"b13\": 1.0,\n        \"b14\": 1.0,\n        \"b15\": 1.0,\n        \"b16\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b7\": 0.8,\n        \"b10\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b4\": 0.6,\n        \"b8\": 0.6,\n        \"b9\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges of incorporating human knowledge into AI, particularly through knowledge representation and reasoning using knowledge graphs. The focus is on Knowledge Representation Learning (KRL) or Knowledge Graph Embedding (KGE) to map entities and relations into low-dimensional vectors while capturing their semantic meanings.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b12\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b7\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of incorporating human knowledge into AI via knowledge representation and reasoning. It focuses on knowledge representation learning (KRL) or knowledge graph embedding (KGE) to map entities and relations into low-dimensional vectors while capturing their semantic meanings. The paper reviews various KRL models, their components, and real-world applications of knowledge graphs.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```"], "5c04967517c44a2c74708d15": ["```json\n{\n    \"Summary\": \"The paper discusses the challenges and findings from the Fact Extraction and VERification (FEVER) shared task, which focuses on verifying the veracity of human-generated textual claims against textual evidence retrieved from Wikipedia. A key challenge highlighted is the need to identify relevant evidence from a large textual corpus and reason correctly with respect to the claim. The paper also outlines the limitations of using human annotators to identify correct evidence and the balance between annotation velocity and evidence recall.\",\n    \"Direct Inspiration\": {\n        \"b15\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b3\": 0.8,\n        \"b6\": 0.8\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents the Fact Extraction and VERification (FEVER) shared task, which aims to verify the veracity of human-generated textual claims against textual evidence retrieved from Wikipedia. The primary challenges include identifying relevant evidence from a large textual corpus and reasoning correctly with respect to the claims. The paper describes the dataset creation, task description, scoring metrics, and the results of submissions to the task.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on the challenges of automatic information verification, specifically predicting the veracity of human-generated textual claims against textual evidence retrieved from Wikipedia. The key challenges include identifying relevant evidence from a large corpus and reasoning correctly with respect to the claim. The FEVER shared task was designed to address these challenges by requiring systems to label claims and provide evidence for the labels. The primary scoring metric is the FEVER score, which conditions label accuracy on providing a complete set of evidence.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b2\": 0.7,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents the FEVER shared task, focused on the automatic verification of textual claims against Wikipedia evidence. The main challenges include identifying evidence from a large corpus and correctly reasoning with respect to the claim. The paper discusses the dataset construction, task description, scoring metrics, and participant results. The inspiration stems from needs in the fields of information extraction and verification.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the verification of human-generated textual claims against factual evidence from Wikipedia. The paper presents the FEVER shared task, aiming to develop systems that can predict the veracity of claims by retrieving and reasoning with relevant evidence from a large textual corpus. The novel contribution of the paper is the creation of a dedicated dataset and evaluation metrics to facilitate research in automatic information verification.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6\n  }\n}\n```"], "5e7232fe93d709897cfa3461": ["```json\n{\n    \"Summary\": \"The primary challenge addressed in this paper is the high false alarm rate and missing rate in anomaly-based intrusion detection systems (IDS) due to reliance on traditional statistical methods with limited sample sizes. The proposed solution introduces a fuzzy support vector machine (FSVM) method based on support vector data description (SVDD) to effectively distinguish between noises and support vectors (SV) distributed on the boundary, with the goal of improving classification accuracy and efficiency in IDS.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b3\": 0.9,\n        \"b4\": 0.9,\n        \"b5\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b60\": 0.7,\n        \"b61\": 0.7,\n        \"b62\": 0.7,\n        \"b63\": 0.7,\n        \"b64\": 0.7,\n        \"b65\": 0.7,\n        \"b66\": 0.7,\n        \"b67\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.5,\n        \"b6\": 0.5,\n        \"b8\": 0.5,\n        \"b10\": 0.5,\n        \"b12\": 0.5,\n        \"b13\": 0.5,\n        \"b14\": 0.5,\n        \"b15\": 0.5,\n        \"b16\": 0.5,\n        \"b17\": 0.5,\n        \"b18\": 0.5,\n        \"b20\": 0.5,\n        \"b21\": 0.5,\n        \"b22\": 0.5,\n        \"b23\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the need to improve anomaly-based intrusion detection systems (IDS) using system call sequences to overcome the limitations of traditional statistics-based methods, which require large data samples. The paper proposes a novel fuzzy support vector machine (FSVM) method based on support vector data description (SVDD) to effectively distinguish between noise and support vectors (SVs) in system call sequences. The novel approach focuses on constructing a fuzzy membership function that assigns different weights to samples based on their distance from the cluster center, thereby improving detection accuracy and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b67\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b64\": 0.8,\n    \"b65\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b2\": 0.6,\n    \"b5\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of intrusion detection systems (IDS) with limited anomaly data, proposing a novel fuzzy support vector machine (FSVM) based on support vector data description (SVDD) to improve classification accuracy and efficiency. The method is designed to distinguish between noises and support vectors more effectively and is particularly suited for real-time anomaly detection in IDS.\",\n  \"Direct Inspiration\": [\"b1\", \"b2\", \"b3\", \"b4\", \"b5\"],\n  \"Indirect Inspiration\": [\"b60\", \"b61\", \"b62\", \"b63\", \"b64\", \"b65\", \"b67\", \"b68\"],\n  \"Other Inspiration\": [\"b69\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in anomaly-based intrusion detection systems (IDS) that require effective classification of system calls in environments with limited data. The proposed method introduces a fuzzy support vector machine (FSVM) based on support vector data description (SVDD), aiming to improve classification accuracy by effectively distinguishing between noises, outliers, and support vectors (SVs). Key contributions include a new fuzzy membership function and an efficient method to speed up detection and training processes, particularly useful for real-time IDS applications.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b3\": 1,\n    \"b4\": 1,\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b60\": 0.9,\n    \"b61\": 0.9,\n    \"b62\": 0.9,\n    \"b63\": 0.9,\n    \"b64\": 0.9,\n    \"b65\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b67\": 0.8,\n    \"b68\": 0.8,\n    \"b69\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high false alarm and missing rates in anomaly-based intrusion detection systems (IDS) due to the reliance on traditional statistical methods. The authors propose a novel fuzzy support vector machine (FSVM) method based on support vector data description (SVDD) to classify system call sequences. This method aims to effectively distinguish noises and support vectors (SVs) distributed on the boundary by assigning different weights to different samples, thereby improving detection performance and computational efficiency.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.95,\n    \"b3\": 0.95,\n    \"b4\": 0.95,\n    \"b5\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b26\": 0.7,\n    \"b27\": 0.7,\n    \"b28\": 0.7,\n    \"b29\": 0.7,\n    \"b30\": 0.7,\n    \"b31\": 0.7,\n    \"b32\": 0.7,\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7,\n    \"b36\": 0.7,\n    \"b37\": 0.7,\n    \"b38\": 0.7,\n    \"b39\": 0.7,\n    \"b40\": 0.7,\n    \"b41\": 0.7,\n    \"b42\": 0.7,\n    \"b43\": 0.7,\n    \"b44\": 0.7,\n    \"b45\": 0.7,\n    \"b46\": 0.7,\n    \"b47\": 0.7,\n    \"b48\": 0.7,\n    \"b49\": 0.7,\n    \"b50\": 0.7,\n    \"b51\": 0.7,\n    \"b52\": 0.7,\n    \"b53\": 0.7,\n    \"b54\": 0.7,\n    \"b55\": 0.7,\n    \"b56\": 0.7,\n    \"b57\": 0.7,\n    \"b58\": 0.7,\n    \"b59\": 0.7,\n    \"b60\": 0.7,\n    \"b61\": 0.7,\n    \"b62\": 0.7,\n    \"b63\": 0.7,\n    \"b64\": 0.7,\n    \"b65\": 0.7,\n    \"b66\": 0.7,\n    \"b67\": 0.7,\n    \"b68\": 0.7,\n    \"b69\": 0.7\n  },\n  \"Other Inspiration\": {\n  }\n}\n```"], "5ce3a7b6ced107d4c654a979": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of dynamic trusted authentication of software by proposing a method for extracting and describing software trajectory based on cross-references of binary files. The main goal is to ensure that the real-time behavior of software matches the expected behavior. The method involves constructing function chains starting at input variables and optimizing these chains for practicality and validity.\",\n  \"Direct Inspiration\": [\"b19\", \"b20\"],\n  \"Indirect Inspiration\": [\"b21\"],\n  \"Other Inspiration\": [\"b1\", \"b3\", \"b7\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the dynamic trusted authentication of software, which aims to ensure real-time behavior of software matches the expected behavior. The authors propose a novel method for extracting and describing software trajectory based on cross-references of binary files. This method involves constructing a suitable size function chain starting at input variables and optimizing this chain for practicality and validity.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.9,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of dynamic trusted authentication of software, focusing on the extraction and description of software behavior, real-time monitoring, and contrastive analysis of intended and running-time behavior. The authors propose a method based on cross-references of binary files to build and optimize function chains, which are then used to describe software behavior from an attacker's perspective.\",\n    \"Direct Inspiration\": {\n        \"b19\": 0.9,\n        \"b20\": 0.85,\n        \"b21\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.75,\n        \"b3\": 0.7,\n        \"b6\": 0.7,\n        \"b7\": 0.7,\n        \"b8\": 0.7,\n        \"b10\": 0.7,\n        \"b11\": 0.7,\n        \"b13\": 0.6,\n        \"b14\": 0.6,\n        \"b15\": 0.6,\n        \"b16\": 0.6,\n        \"b17\": 0.65,\n        \"b18\": 0.6,\n        \"b22\": 0.65,\n        \"b23\": 0.6,\n        \"b24\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.5,\n        \"b2\": 0.5,\n        \"b4\": 0.5,\n        \"b5\": 0.5,\n        \"b9\": 0.5,\n        \"b12\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of dynamic trusted authentication of software by proposing a method to extract and describe software behavior using cross-references of binary files. The proposed method constructs a function chain starting at input variables, optimizes and simplifies it for practical application, and is validated through specific experiments.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b13\": 0.5,\n    \"b14\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the dynamic trusted authentication of software, which involves accurately abstracting and describing software behavior, real-time monitoring, and contrasting intended versus running-time behavior. The proposed algorithm involves extracting and describing software trajectory based on cross-references in binary files, constructing function chains starting from input variables, and optimizing these chains for practical use.\",\n    \"Direct Inspiration\": {\n        \"b19\": 1.0,\n        \"b20\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.8,\n        \"b21\": 0.85\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.7,\n        \"b3\": 0.6,\n        \"b6\": 0.6,\n        \"b22\": 0.75\n    }\n}\n```"], "5cb06564ced107d4c6006f1c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting silent data corruption (SDC) in processor instructions caused by soft errors. The novel contribution is the development of SDCPredictor, an intelligent prediction model using random regression forest algorithm to predict the SDC proneness of program instructions based on static and dynamic features, without the need for fault injections.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.95,\n    \"b11\": 0.9,\n    \"b12\": 0.85,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b18\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting silent data corruption (SDC) caused by soft errors in processors. It introduces SDCPredictor, a novel prediction model using random regression forests to identify SDC-prone instructions based on analyzing static and dynamic features of instructions, without relying on fault injections. The method aims to improve prediction accuracy and reduce time and manpower compared to previous approaches.\",\n  \"Direct Inspiration\": [\"b10\", \"b11\", \"b12\", \"b15\"],\n  \"Indirect Inspiration\": [\"b9\", \"b13\"],\n  \"Other Inspiration\": [\"b14\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying SDC-prone instructions in processors to mitigate soft errors. It proposes SDCPredictor, a novel approach using random regression forest algorithm to predict SDC-prone instructions based on static and dynamic features without fault injections.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.85,\n    \"b11\": 0.8,\n    \"b12\": 0.75,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b14\": 0.65\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the increasing threat of soft errors in modern processors, specifically focusing on silent data corruption (SDC) errors. It proposes a novel method, SDCPredictor, which uses a random regression forest algorithm to predict SDC-prone instructions without the need for time-consuming fault injections. The method analyzes static and dynamic features of instructions to improve prediction accuracy and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.85,\n    \"b11\": 0.8,\n    \"b12\": 0.75,\n    \"b13\": 0.7,\n    \"b15\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.55,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the increasing threat of soft errors in processor design due to trends towards smaller transistor size, lower core voltage, and higher frequency. The proposed method, SDCPredictor, uses a random regression forest algorithm to predict SDC-prone instructions by analyzing static and dynamic features, thus avoiding the time-consuming fault injection process. The primary goals are to improve prediction accuracy and reduce performance overhead in soft error protection.\",\n  \"Direct Inspiration\": {\n    \"b15\": \"0.95\",\n    \"b10\": \"0.90\",\n    \"b11\": \"0.85\"\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": \"0.75\",\n    \"b12\": \"0.70\",\n    \"b13\": \"0.65\"\n  },\n  \"Other Inspiration\": {\n    \"b14\": \"0.60\",\n    \"b18\": \"0.55\"\n  }\n}\n```"], "5ac1827b17c44a1fda915855": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting and correcting control flow errors in distributed computing systems, particularly in the context of both software and hardware redundancy. The authors propose a novel control flow checking technology based on updating a signature at directed edges (CFCVE), which aims to improve fault coverage, reduce memory overhead, and enhance performance compared to existing solutions.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b13\", \"b14\", \"b15\", \"b16\", \"b17\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b12\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": []\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of ensuring secure communication and operation in distributed computing environments by proposing a novel control flow checking technology called CFCVE. This technique aims to enhance error detection in control flows by updating signatures at directed edges of the control flow graph, achieving higher fault coverage with less memory overhead compared to existing solutions like CFCSS and RSCFC.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1,\n        \"b14\": 1,\n        \"b15\": 1,\n        \"b16\": 1,\n        \"b17\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b11\": 0.8,\n        \"b12\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b18\": 0.5,\n        \"b19\": 0.5,\n        \"b20\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of secure communication and operation in distributed computing, specifically focusing on control flow errors and software-based error detection methods. The proposed algorithm, CFCVE, updates signatures at directed edges to improve error detection and fault coverage, drawing inspiration from existing control flow checking techniques.\",\n  \"Direct Inspiration\": [\"b13\", \"b14\", \"b15\", \"b16\", \"b17\"],\n  \"Indirect Inspiration\": [\"b12\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": {\n        \"Challenges\": \"The primary challenges discussed in the paper are ensuring the integrity and secure operation of distributed computing environments, detecting and correcting control flow errors, and mitigating vulnerabilities such as SEU-induced soft errors and buffer overflow attacks.\",\n        \"Algorithm\": \"The paper proposes a novel control flow checking technology named CFCVE, which updates signatures at directed edges of the control flow graph to detect single inter-block control flow errors. This method aims to provide higher fault coverage with less memory overhead compared to existing methods like CFCSS and RSCFC.\"\n    },\n    \"Direct Inspiration\": {\n        \"b13\": 1,\n        \"b14\": 1,\n        \"b15\": 1,\n        \"b16\": 1,\n        \"b17\": 1\n    },\n    \"Indirect Inspiration\": {},\n    \"Other Inspiration\": {\n        \"b12\": 0.9\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting and correcting control flow errors in software systems, which are crucial for ensuring secure and reliable operation, especially in distributed computing environments. The authors propose a new control flow checking technology, CFCVE, which updates signatures at directed edges in a control flow graph, improving fault coverage and reducing memory overhead compared to existing methods.\",\n  \"Direct Inspiration\": [\"b13\", \"b14\", \"b15\", \"b16\", \"b17\"],\n  \"Indirect Inspiration\": [\"b12\"],\n  \"Other Inspiration\": [\"b8\"]\n}\n```"], "5c2c7a6717c44a4e7cf30ab1": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of mitigating silent data corruption (SDC) in space-borne computers due to SEU-induced soft errors. The authors propose SDCPredictor, a novel configurable protection method that predicts the SDC vulnerability of program instructions using a random forest algorithm. This approach aims to identify the most SDC-vulnerable instructions without requiring fault injections, thereby saving significant time and improving prediction accuracy compared to previous methods.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the identification and protection of SDC-causing instructions in space-borne computers to ensure their functionality and reliability. The authors propose SDCPredictor, a new configurable protection approach based on the random forest algorithm, to predict the SDC vulnerability of program instructions, which aims to improve prediction accuracy and reduce time costs compared to previous methods.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b11\": 0.9,\n    \"b12\": 0.9,\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b16\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.5,\n    \"b18\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of mitigating SEU-induced soft errors in space-borne computers, specifically focusing on silent data corruption (SDC). The authors propose SDCPredictor, a new configurable protection approach using a random forest algorithm to predict the SDC vulnerability of program instructions without requiring fault injections, achieving significant time-saving and better prediction accuracy compared to previous methods.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b18\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Silent Data Corruptions (SDCs) in space-borne computers and proposes a novel software-based approach, SDCPredictor, which uses a random forest algorithm to predict SDC vulnerabilities in program instructions. The goal is to identify SDC-vulnerable instructions with high accuracy and efficiency, avoiding the need for costly fault injections.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the occurrence of SEU-induced soft errors in space-borne computers, which can lead to silent data corruption (SDC). The paper proposes SDCPredictor, an intelligent prediction model based on the random forests algorithm, to predict the SDC vulnerability of program instructions without requiring fault injections, thus saving significant time. The paper aims to find the most SDC-vulnerable instructions for a given performance cost budget.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.85,\n    \"b10\": 0.8,\n    \"b11\": 0.75,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.65,\n    \"b18\": 0.6\n  }\n}\n```"], "599c7ec9601a182cd28c8607": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of ensuring secure communication and operation in distributed computing environments, with a particular focus on control flow checking in software to detect and correct errors caused by software attacks and SEU-induced soft errors. The proposed algorithm, CFCBS, introduces a novel branch sequence signature and employs a coarse granularity checking mechanism to enhance error detection and reduce performance overhead.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b12\": 1,\n    \"b13\": 1,\n    \"b14\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is ensuring the integrity of control flow in software applications to prevent vulnerabilities such as those introduced by soft errors or malicious attacks. The proposed algorithm, CFCBS (Control Flow Checking using Branch Sequence), aims to detect control flow faults in software through run-time checks using branch sequence signatures, which contain information about path length and branch selection. The approach is inspired by previous control flow checking techniques but introduces novel methods to enhance error coverage and reduce overhead.\",\n  \"Direct Inspiration\": [\"b11\", \"b12\", \"b13\", \"b14\"],\n  \"Indirect Inspiration\": [\"b9\", \"b10\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of ensuring secure control flow in distributed computing environments, specifically focusing on detecting and correcting control flow errors through a novel software-based method. The proposed algorithm, CFCBS (Control Flow Checking using Branch Sequence), enhances error detection by embedding measurement instructions into applications using code rewriting techniques and checking control flow at runtime. This method draws inspiration from several existing techniques but introduces a unique branch sequence signature to improve error coverage and reduce performance overhead.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b11\", \"b12\", \"b13\", \"b14\"],\n        \"confidence_scores\": {\n            \"b11\": 1,\n            \"b12\": 1,\n            \"b13\": 1,\n            \"b14\": 1\n        }\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b9\", \"b10\"],\n        \"confidence_scores\": {\n            \"b9\": 0.8,\n            \"b10\": 0.8\n        }\n    },\n    \"Other Inspiration\": {\n        \"references\": [],\n        \"confidence_scores\": {}\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of control flow errors in software systems, specifically focusing on a software-only solution for detecting and correcting control flow errors at runtime without requiring additional hardware. The proposed algorithm, Control Flow Checking using Branch Sequence (CFCBS), introduces novel methods for signature assignments, computations, and comparisons to enhance error coverage and reduce performance overhead.\",\n  \"Direct Inspiration\": [\"b9\", \"b10\", \"b11\", \"b12\", \"b13\", \"b14\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting control flow errors in software applications without requiring additional hardware. The proposed algorithm, Control Flow Checking using Branch Sequence (CFCBS), embeds measurement instructions into applications using code rewriting techniques to check control flow at runtime. The method incorporates a unique branch sequence signature for each basic block, performs coarse granularity checking, and reduces performance overhead by limiting the number of inserted instructions.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b12\": 1.0,\n    \"b13\": 1.0,\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b2\": 0.5,\n    \"b3\": 0.5,\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.5,\n    \"b7\": 0.5,\n    \"b8\": 0.5\n  }\n}\n```"], "558aed5ce4b0b32fcb39b5f6": ["```json\n{\n  \"Summary\": \"The paper addresses key challenges in VLSI routing at the nanometer scale, including optimizing wirelength and congestion in multi-million gate designs, handling vias and multiple metal layers, and adhering to signal integrity and density constraints. It proposes a high-performance routing technique based on Discrete Lagrange Multipliers (DLM) to handle net weights and timing optimization effectively. The technique is implemented in the FGR router, showing improved performance over existing solutions.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b17\": 0.9,\n    \"b31\": 0.9,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b24\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of VLSI routing at the nanometer scale, including large wiring databases, sophisticated design rules, unreliable vias, signal integrity constraints, and CMP considerations. The authors propose a high-performance routing technique based on Discrete Lagrange Multipliers (DLM) to balance wirelength and congestion, handle net weights and timing optimization, and reduce violations and wirelength in multi-million gate designs. The proposed algorithms are implemented in FGR, which achieves superior performance on the ISPD '07 Global Routing Contest benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b17\": 0.9,\n    \"b31\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b8\": 0.6,\n    \"b14\": 0.6,\n    \"b22\": 0.5,\n    \"b24\": 0.5,\n    \"b36\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.4,\n    \"b15\": 0.4,\n    \"b1\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the persistent challenges in VLSI routing at the nanometer scale, including large wiring databases, sophisticated design rules, unreliable vias, signal integrity constraints, and CMP considerations. To tackle these challenges, the authors propose a high-performance routing technique based on Discrete Lagrange Multipliers (DLM), which provides a natural way to handle net weights and timing optimization. The novel contributions of the paper include the application of DLM in global routing, extensions of A*-search to avoid congestion, and improved wirelength on ISPD '07 benchmarks.\",\n  \"Direct Inspiration\": {\n    \"References\": [\"b27\", \"b17\"]\n  },\n  \"Indirect Inspiration\": {\n    \"References\": [\"b31\", \"b32\"]\n  },\n  \"Other Inspiration\": {\n    \"References\": [\"b22\", \"b24\", \"b36\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in VLSI routing at the nanometer scale, including large wiring databases, sophisticated design rules, unreliable vias, signal integrity constraints, and CMP considerations. The proposed high-performance routing technique is based on Discrete Lagrange Multipliers (DLM) to balance interconnect length and congestion, with key contributions such as handling net weights and timing optimization, extending A*-search to restructure net topologies, and improving wirelength on the ISPD '07 Global Routing Contest benchmarks.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b27\", \"b17\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b31\", \"b32\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b22\", \"b24\", \"b36\", \"b8\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in VLSI routing at the nanometer scale, focusing on balanced routing techniques to handle large wiring databases, sophisticated design rules, and signal integrity constraints. It proposes a high-performance routing technique based on Discrete Lagrange Multipliers (DLM) to handle net weights and timing optimization, with key contributions including a new congestion penalty function and via pricing optimization.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.9,\n    \"b36\": 0.8,\n    \"b22\": 0.8,\n    \"b24\": 0.8,\n    \"b31\": 0.85,\n    \"b32\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b14\": 0.7\n  }\n}\n```"], "53e9984bb7602d970207d617": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing register pressure in dynamically scheduled processors by proposing a novel register renaming approach using virtual-physical registers. This new scheme aims to increase processor performance while reducing hardware costs associated with larger physical register files.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of reducing register pressure in dynamically scheduled processors to improve performance and reduce hardware costs. It proposes a novel register renaming approach using virtual-physical registers, which allocates physical registers for shorter periods compared to traditional methods. This approach aims to maintain high instruction-level parallelism (ILP) while minimizing the number of physical registers required.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b10\": 0.85,\n        \"b14\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b2\": 0.75,\n        \"b3\": 0.8,\n        \"b11\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.5,\n        \"b8\": 0.55\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of reducing register pressure in dynamically scheduled processors to increase performance and decrease hardware costs. It proposes a novel register renaming scheme using virtual-physical registers to alleviate this pressure.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b10\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.75,\n        \"b3\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b13\": 0.5,\n        \"b14\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of reducing register pressure in dynamically scheduled processors by introducing a novel register renaming approach based on virtual-physical registers. This approach aims to increase processor performance by enlarging the instruction window size or reducing hardware cost without loss of performance.\",\n    \"Direct Inspiration\": [\"b7\", \"b10\"],\n    \"Indirect Inspiration\": [\"b3\", \"b8\"],\n    \"Other Inspiration\": [\"b1\", \"b2\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of register pressure in dynamically scheduled processors due to the conventional register renaming scheme. It proposes a novel register renaming approach, called virtual-physical registers, which significantly reduces register pressure by allocating physical registers for a shorter period. This new method results in either increased performance through a larger active instruction window or reduced hardware costs without performance loss.\",\n  \"Direct Inspiration\": [\"b7\", \"b10\"],\n  \"Indirect Inspiration\": [\"b1\", \"b11\", \"b2\", \"b3\", \"b14\"],\n  \"Other Inspiration\": [\"b13\", \"b8\"]\n}\n```"], "556b086b240114513674708f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying true performance bottlenecks in modern out-of-order CPUs. It proposes a Top-Down Analysis method that categorizes CPU execution time and guides users to focus on critical bottlenecks through a hierarchical drill-down approach. The method is validated using Intel's 3rd generation Core (Ivy Bridge) and has been adopted by tools like VTune and Linux perf utility.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying performance bottlenecks in modern out-of-order CPUs, which is complicated by the numerous performance events that may or may not be relevant. The proposed solution is a Top-Down Analysis method that uses a hierarchical approach to guide users through the performance data, focusing on significant issues and disregarding insignificant ones. This method is implemented using a small set of Top-Down oriented counters and has been adopted by tools like VTune and Linux perf utility.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying performance bottlenecks in modern out-of-order CPUs due to the complexity and volume of performance events. It proposes a Top-Down Analysis method, which categorizes and drills down into performance issues in a hierarchical manner to focus on significant bottlenecks. The method is validated on Intel's Ivy Bridge architecture and adopted by tools like VTune and Linux perf utility. This results in a novel performance counters architecture to identify bottlenecks efficiently.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying performance bottlenecks in out-of-order CPUs through a structured hierarchical Top-Down Analysis. This method simplifies the process by categorizing CPU execution time into four main domains and recursively drilling down to specific performance issues. The paper also introduces novel low-cost performance counters to gather necessary metrics.\",\n  \"Direct Inspiration\": [\n    \"b1\",\n    \"b2\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b3\"\n  ],\n  \"Other Inspiration\": [\n    \"b6\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying performance bottlenecks in modern out-of-order CPUs, which is complicated due to the numerous performance events and the sophisticated nature of these CPUs. The proposed solution is a Top-Down Analysis method that hierarchically categorizes execution time to guide users in identifying and addressing critical bottlenecks quickly and effectively. This method is particularly featured with the Intel 3rd generation Core and is adopted by multiple tools.\",\n  \"Direct Inspiration\": [\"b1\", \"b2\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b3\", \"b5\", \"b6\"]\n}\n```"], "53e9a90db7602d970325e354": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in improving the quality of synthetic images generated by computers, particularly focusing on shading techniques for realistic image rendering. The primary challenges include achieving real-time dynamic color displays, representing objects with smooth curved surfaces, and mitigating the effects of digital sampling techniques. The paper discusses existing methods of object modeling and highlights the limitations of polygonal approximation in producing realistic shading. The authors analyze various shading algorithms, including those by Warnock, Newell, Newell, and Sancha, and Gouraud, and propose enhancements to reduce shading discontinuities and improve image realism.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b6\": 0.85,\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b9\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the challenges of improving the quality of synthetic images, specifically focusing on shading techniques for computer-generated images. The authors aim to achieve a degree of realism by understanding the human visual system and developing algorithms that enhance image quality. Key challenges include real-time display, representation of smooth curved surfaces, and minimizing digital sampling effects. The paper proposes a novel shading approach that modifies existing shading rules to reduce discontinuities in shading between adjacent polygons.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b7\": 0.7,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of improving the quality of synthetic images, focusing on real-time display, smooth curved surfaces representation, and mitigating digital sampling effects. It proposes methods for shading using planar polygonal approximation and discusses the influence of hidden surface algorithms on shading.\",\n    \"Direct Inspiration\": {\n        \"b10\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b6\": 0.7,\n        \"b7\": 0.6,\n        \"b8\": 0.6,\n        \"b9\": 0.6\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving the quality of synthetic images, with a focus on real-time display, smooth curved surface representation, and digital sampling effects reduction. The primary concern is enhancing shading techniques for better realism in computer-generated images. The paper critiques existing methods and proposes improvements rooted in understanding the human visual system.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b6\": 0.85,\n    \"b10\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.75,\n    \"b9\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in enhancing the quality of synthetic images, focusing on shading algorithms for three-dimensional objects. The key contributions revolve around developing methods that achieve real-time display, represent smooth curved surfaces, and mitigate the effects of digital sampling techniques. The paper explores different object modeling techniques and their impact on image quality, emphasizing the use of planar polygons for curved surface representation to simplify hidden surface removal and shading. It evaluates existing shading techniques and proposes improvements to achieve a realistic appearance in computer-generated images.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b6\": 0.8,\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b9\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```"], "557d11a66feeaa8086da604a": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of data dependencies, branch instructions, and imprecise interrupts in pipelined CPU architectures. It proposes a dependency-resolution mechanism (out-of-order instruction issue) and a precise interrupt mechanism that work together to overcome these challenges.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b25\": 0.85,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b5\": 0.65,\n    \"b7\": 0.6,\n    \"b8\": 0.55,\n    \"b20\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.45,\n    \"b6\": 0.4,\n    \"b14\": 0.35,\n    \"b15\": 0.3,\n    \"b17\": 0.25,\n    \"b18\": 0.2,\n    \"b19\": 0.15,\n    \"b21\": 0.1\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are data dependencies, branch instructions, and imprecise interrupts in pipelined CPUs with multiple functional units. The paper proposes a novel dependency-resolution mechanism that integrates out-of-order instruction issue and precise interrupt handling, with a focus on Tomasulo's algorithm and its extensions.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.9,\n    \"b23\": 0.85,\n    \"b8\": 0.8,\n    \"b26\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b21\": 0.65,\n    \"b7\": 0.6,\n    \"b5\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b17\": 0.45,\n    \"b9\": 0.4,\n    \"b10\": 0.35\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of data dependencies, branch instructions, and imprecise interrupts in pipelined CPUs, proposing a novel dependency-resolution mechanism that integrates out-of-order instruction issue with precise interrupt handling.\",\n  \"Direct Inspiration\": [\"b23\", \"b25\", \"8\"],\n  \"Indirect Inspiration\": [\"b21\", \"b26\", \"27\"],\n  \"Other Inspiration\": [\"b2\", \"b5\", \"b7\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of data dependencies, branch instructions, and imprecise interrupts in pipelined CPUs. It proposes a dependency-resolution mechanism, synonymous with an out-of-order instruction issue mechanism, and introduces the Register Update Unit (RUU) to resolve dependencies and implement precise interrupts.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b8\": 0.9,\n    \"b23\": 0.9,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b26\": 0.7,\n    \"b17\": 0.6,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b5\": 0.5,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges faced by pipelined CPUs, particularly data dependencies and branch instructions, and proposes a hardware dependency-resolution mechanism that also handles precise interrupts. The core contribution is the Register Update Unit (RUU) that integrates out-of-order instruction issue and precise interrupts to alleviate performance degradation.\",\n    \"Direct Inspiration\": {\n        \"b23\": 0.9,\n        \"b25\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.7,\n        \"b20\": 0.6,\n        \"b26\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.5,\n        \"b6\": 0.5,\n        \"b7\": 0.4\n    }\n}\n```"], "5e16fa233a55acac60fd363d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of irregular and regular execution patterns in Graph Convolutional Networks (GCNs) and proposes a hybrid architecture accelerator, HyGCN, to efficiently perform GCNs. The design leverages edge-centric and MVM-centric programming models and introduces specialized engines for Aggregation and Combination phases to optimize parallelism and data reuse.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1,\n    \"b45\": 1,\n    \"b46\": 1,\n    \"b47\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b44\": 0.7,\n    \"b25\": 0.6,\n    \"b39\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.4,\n    \"b14\": 0.3,\n    \"b16\": 0.3,\n    \"b28\": 0.3,\n    \"b43\": 0.3,\n    \"b12\": 0.2,\n    \"b13\": 0.2,\n    \"b15\": 0.2,\n    \"b26\": 0.2,\n    \"b40\": 0.2,\n    \"b10\": 0.1,\n    \"b8\": 0.1,\n    \"b22\": 0.1,\n    \"b30\": 0.1,\n    \"b41\": 0.1,\n    \"b6\": 0.1,\n    \"b20\": 0.1,\n    \"b11\": 0.1,\n    \"b23\": 0.1,\n    \"b3\": 0.1,\n    \"b31\": 0.1,\n    \"b29\": 0.1,\n    \"b17\": 0.1,\n    \"b19\": 0.1\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The paper addresses the dynamic and irregular execution patterns in the Aggregation phase of GCNs and the static and regular patterns in the Combination phase, emphasizing the need to alleviate irregularity, exploit intra-vertex parallelism, and fuse the execution of the two phases for high-performance and energy-efficient GCN acceleration.\",\n    \"Inspirations\": \"The architecture design is inspired by the limitations of existing CPU and GPU architectures in handling GCN-specific characteristics and the need for a specialized accelerator.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b38\", \"b45\", \"b46\", \"b47\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b2\", \"b14\", \"b18\", \"b20\", \"b28\", \"b43\", \"b44\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b13\", \"b25\", \"b16\", \"b39\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of hybrid execution patterns in GCNs, specifically the dynamic and irregular Aggregation phase and the static and regular Combination phase. It proposes the HyGCN accelerator to efficiently perform GCNs by exploiting edge-level and intra-vertex parallelism, reusing inter-vertex data, and fusing the execution of Aggregation and Combination phases.\",\n  \"Direct Inspiration\": {\n    \"b38\": 0.9,\n    \"b45\": 0.9,\n    \"b46\": 0.9,\n    \"b47\": 0.9,\n    \"b18\": 0.8,\n    \"b44\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b14\": 0.7,\n    \"b20\": 0.7,\n    \"b28\": 0.7,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b12\": 0.6,\n    \"b16\": 0.6,\n    \"b13\": 0.6,\n    \"b15\": 0.6,\n    \"b40\": 0.6,\n    \"b26\": 0.6,\n    \"b10\": 0.6,\n    \"b22\": 0.6,\n    \"b8\": 0.6,\n    \"b30\": 0.6,\n    \"b41\": 0.6,\n    \"b11\": 0.6,\n    \"b39\": 0.6,\n    \"b6\": 0.6,\n    \"b19\": 0.6,\n    \"b23\": 0.6,\n    \"b29\": 0.6,\n    \"b3\": 0.6,\n    \"b31\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper outlines the primary challenge of achieving high-performance and energy-efficient acceleration of Graph Convolutional Networks (GCNs) by addressing the irregularity in the Aggregation phase and the regularity in the Combination phase. Inspired by existing neural network and graph analytics architectures, the authors propose HyGCN, a hybrid architecture that includes innovative methods such as interval-shard graph partitioning, window sliding-shrinking, and multi-granular systolic arrays to improve data reuse, reduce unnecessary accesses, and perform MVMs in parallel.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1,\n    \"b45\": 1,\n    \"b46\": 1,\n    \"b47\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b14\": 0.8,\n    \"b18\": 0.8,\n    \"b20\": 0.8,\n    \"b28\": 0.8,\n    \"b43\": 0.8,\n    \"b44\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b25\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in executing Graph Convolutional Networks (GCNs) efficiently due to the hybrid execution patterns of Aggregation and Combination phases. Inspired by the powerful learning capability of neural networks and the convolutional layers of GCNs, the authors propose a novel GCN accelerator, HyGCN, using a hybrid architecture to enhance performance and energy efficiency. Key contributions include a new programming model, efficient processing engines for the Aggregation and Combination phases, and methods to fuse these phases' execution.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1,\n    \"b45\": 1,\n    \"b46\": 1,\n    \"b47\": 1,\n    \"b18\": 0.9,\n    \"b44\": 0.9,\n    \"b39\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b15\": 0.8,\n    \"b16\": 0.75,\n    \"b20\": 0.75,\n    \"b26\": 0.75,\n    \"b10\": 0.7,\n    \"b30\": 0.7,\n    \"b41\": 0.7,\n    \"b11\": 0.65,\n    \"b8\": 0.65,\n    \"b22\": 0.65,\n    \"b3\": 0.6,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b14\": 0.5,\n    \"b12\": 0.5,\n    \"b13\": 0.5,\n    \"b43\": 0.5,\n    \"b28\": 0.5,\n    \"b23\": 0.5,\n    \"b29\": 0.4,\n    \"b6\": 0.4,\n    \"b19\": 0.4,\n    \"b17\": 0.4\n  }\n}\n```"], "53e997ddb7602d9701fd5474": ["```json\n{\n  \"Summary\": \"The paper presents the predictor cbp1.5, which is a specific instance of the GPPM predictors. The primary challenge is to create an efficient branch predictor by balancing accuracy and resource constraints. The authors propose cbp1.5 as a practical version of an ideal GPPM predictor, introducing various 'degradations' to account for real-world limitations.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.8,\n    \"b3\": 0.7,\n    \"b4\": 0.6\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.5,\n    \"b6\": 0.4\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the cbp1.5 predictor, a global-history based predictor derived from PPM, which achieved fifth place in the first Championship Branch Prediction competition. It details the structure, operation, and performance of cbp1.5, comparing it to an ideal version, GPPM-ideal, and studying the degradations needed to transition from GPPM-ideal to cbp1.5. The primary challenges involve optimizing branch prediction accuracy under real-world constraints.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.7,\n    \"b4\": 0.6,\n    \"b5\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the design and performance of the cbp1.5 predictor, a global-history PPM-like predictor used in the first Championship Branch Prediction competition. It addresses the challenges of optimizing branch prediction by comparing a practical implementation (cbp1.5) with an ideal version (GPPM-ideal) and introduces various degradations to account for real-life constraints.\",\n  \"Direct Inspiration\": [\"b1\", \"b2\", \"b3\"],\n  \"Indirect Inspiration\": [\"b4\", \"b5\"],\n  \"Other Inspiration\": [\"b6\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper outlines the design and implementation of the cbp1.5 predictor, which is a global-history PPM-like predictor used in branch prediction. The primary challenges addressed include optimizing prediction accuracy and managing storage constraints. The paper describes the cbp1.5 predictor's structure, prediction and update mechanisms, and compares it to an ideal version (GPPM-ideal) to understand performance trade-offs.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.8,\n        \"b2\": 0.7,\n        \"b3\": 0.6,\n        \"b4\": 0.5,\n        \"b5\": 0.4,\n        \"b6\": 0.3\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper describes a global-history based branch predictor, cbp1.5, which is a degraded version of an ideal predictor called GPPM-ideal. The primary challenges addressed include optimizing the prediction accuracy while managing real-life constraints that cause mispredictions. The proposed algorithm uses a combination of bimodal and global tables with varying history lengths to make predictions and updates based on the longest matching history.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b6\": 0.5\n  }\n}\n```"], "59ae3be32bbe271c4c71b8e0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of defining weak memory models for the RISC-V ISA, proposing two models (WMM and WMM-S) that balance definitional simplicity and implementation flexibility. It introduces operational definitions using the I2E framework, aiming to simplify the complex operational models of ARM and POWER ISAs.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b0\", \"b1\", \"b24\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b6\", \"b7\", \"b9\", \"b14\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b19\", \"b20\", \"b21\", \"b22\", \"b23\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces two weak memory models, WMM and WMM-S, for the RISC-V ISA, addressing the complexities in defining weak memory models for commercial ISAs like ARM and POWER. The primary challenge is to balance definitional simplicity and implementation flexibility. The authors propose operational definitions for these models using the Instantaneous Instruction Execution (I2E) framework, which simplifies reasoning about program behaviors.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.9,\n    \"b10\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of defining and implementing weak memory models for the RISC-V ISA, specifically proposing two models: WMM and WMM-S. These models aim to balance definitional simplicity and implementation flexibility, providing operational definitions that avoid the complexity of microarchitectural details seen in other ISAs such as ARM and POWER. The paper introduces novel approaches like the Instantaneous Instruction Execution (I2E) framework and invalidation buffers to manage memory consistency.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.95,\n        \"b7\": 0.95,\n        \"b14\": 0.90,\n        \"b24\": 0.90\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.80,\n        \"b20\": 0.80,\n        \"b21\": 0.80,\n        \"b22\": 0.80,\n        \"b23\": 0.80\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.75,\n        \"b9\": 0.75,\n        \"b25\": 0.70\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of defining weak memory models for the RISC-V ISA, proposing two models, WMM and WMM-S, which offer different balances between simplicity and implementation flexibility. The proposed models are inspired by existing memory models but provide operational definitions that simplify the complexity observed in ARM and POWER models.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b7\": 1,\n    \"b10\": 1,\n    \"b14\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b9\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.8,\n    \"b18\": 0.8,\n    \"b19\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of defining weak memory models for the RISC-V ISA, proposing two models: WMM and WMM-S, which balance definitional simplicity and implementation flexibility differently. The paper is inspired by previous work on strong and weak memory models, operational definitions, and the complexities of out-of-order execution and speculative techniques.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b14\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b9\": 0.7,\n    \"b19\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b16\": 0.6,\n    \"b18\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```"], "53e99a04b7602d970224f3f6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of wire delays and complexity in superscalar microarchitectures by proposing dynamic code partitioning mechanisms for clustered microarchitectures. The proposed approach aims to minimize communication overheads and improve workload balance between clusters, outperforming existing static and dynamic partitioning schemes.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.95,\n    \"b17\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b12\": 0.70,\n    \"b14\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scaling-up superscalar microarchitectures, specifically focusing on the issues of wire delays and complexity in issue and rename logic. It proposes dynamic code partitioning mechanisms for clustered microarchitectures, demonstrating significant performance improvements over static approaches. The main contributions include the development of dynamic cluster assignment mechanisms and the evaluation of their effectiveness in improving workload balance and reducing inter-cluster communication delays.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b12\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scaling up superscalar microarchitectures, particularly focusing on wire delays and the complexity of issue and rename logic. It proposes solutions through clustering and dynamic code partitioning mechanisms, which are shown to be more effective than static methods, achieving significant performance improvements.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b15\", \"b17\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b1\", \"b12\", \"b14\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b2\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling up superscalar microarchitectures, focusing on wire delays and the complexity of issue and rename logic. The authors propose dynamic code partitioning mechanisms for clustered microarchitectures, aiming to balance workloads between integer and floating-point (FP) clusters and reduce inter-cluster communication delays. The proposed dynamic mechanisms are evaluated and shown to outperform static ones and previously proposed dynamic schemes.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b12\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of code partitioning in clustered microarchitectures, proposing dynamic mechanisms to improve performance. It compares these dynamic schemes with static approaches and reports significant speed-ups for the SpecInt95 benchmark suite.\",\n    \"Direct Inspiration\": {\n        \"b15\": 1.0,\n        \"b17\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b12\": 0.7,\n        \"b14\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6\n    }\n}\n```"], "5dce78623a55ac93ec834bf7": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of scalability with increasing network bandwidth in FPGA-based designs, introducing Limago, an open-source 100 Gbit/s TCP/IP network stack on an FPGA. Limago inherits features from an existing 10 Gbit/s design and adapts them to achieve higher bandwidth while introducing novel features like TCP Window Scale option.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b4\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling FPGA-based TCP/IP packet processing from 10 Gbit/s to 100 Gbit/s. The proposed solution, Limago, introduces significant modifications to existing architectures to achieve this, including widening data paths and increasing operating frequencies. Key inspirations include previous designs and methodologies for FPGA implementations, scalable network processing, and specific techniques like checksum computations and cuckoo hashing.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is achieving scalability with increasing network bandwidth, specifically upgrading an existing open-source TCP/IP stack from 10 Gbit/s to 100 Gbit/s. The paper introduces Limago, an open-source 100 Gbit/s TCP/IP network stack on an FPGA, which adapts and extends existing features to increase the supported bandwidth while maintaining a high-productivity design methodology based on Vivado-HLS. Key novel features include an 8x widened data path, doubled operating frequency, low-level architectural changes, balanced pipeline stages, the TCP Window Scale option, and redesigned accessory modules.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.75,\n    \"b12\": 0.7,\n    \"b13\": 0.65,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b2\": 0.5,\n    \"b3\": 0.5,\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.5,\n    \"b7\": 0.5,\n    \"b8\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling FPGA-based TCP/IP network stack from 10 Gbit/s to 100 Gbit/s, introducing Limago. The key contributions include widening the data path, increasing operating frequency, and incorporating novel features such as TCP Window Scale option to address Long Fat Pipe issue.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b11\": 0.9,\n    \"b12\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scaling FPGA-based packet processing to 100 Gbit/s, introducing Limago, an open-source 100 Gbit/s TCP/IP network stack on an FPGA. Limago builds on an existing 10 Gbit/s stack, enhancing it to support higher bandwidth while maintaining scalability and adding novel features such as the TCP Window Scale option.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b4\": 0.8,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b11\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```"], "53e9ad2db7602d97037121c7": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in multimodal fusion for multimedia analysis, focusing on different fusion levels (feature, decision, and hybrid), methods (rule-based, classification-based, estimation-based), and the issues of synchronization, optimal modality selection, and the use of correlation, context, and confidence. The paper aims to provide a comprehensive survey on multimodal fusion methodologies and their application in various multimedia analysis tasks.\",\n  \"Direct Inspiration\": {\n    \"b44\": 0.9,\n    \"b121\": 0.9,\n    \"b144\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b102\": 0.7,\n    \"b99\": 0.7,\n    \"b17\": 0.7,\n    \"b28\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b143\": 0.6,\n    \"b149\": 0.6,\n    \"b15\": 0.6,\n    \"b87\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges and methodologies in multimodal fusion for multimedia analysis, focusing on strategies for fusion at different levels (feature, decision, and hybrid) and the synchronization and selection of modalities.\",\n  \"Direct Inspiration\": {\n    \"b44\": 0.9,\n    \"b121\": 0.9,\n    \"b144\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b102\": 0.7,\n    \"b99\": 0.7,\n    \"b17\": 0.7,\n    \"b28\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b143\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b120\": 0.6,\n    \"b105\": 0.6,\n    \"b19\": 0.6,\n    \"b146\": 0.6,\n    \"b153\": 0.6,\n    \"b59\": 0.6,\n    \"b96\": 0.6,\n    \"b4\": 0.6,\n    \"b78\": 0.6,\n    \"b47\": 0.6,\n    \"b23\": 0.6,\n    \"b133\": 0.6,\n    \"b149\": 0.6,\n    \"b15\": 0.6,\n    \"b87\": 0.6,\n    \"b85\": 0.6,\n    \"b56\": 0.6,\n    \"b68\": 0.6,\n    \"b136\": 0.6,\n    \"b86\": 0.6,\n    \"b57\": 0.6,\n    \"b58\": 0.6,\n    \"b77\": 0.6,\n    \"b54\": 0.6,\n    \"b82\": 0.6,\n    \"b151\": 0.6,\n    \"b113\": 0.6,\n    \"b107\": 0.6,\n    \"b100\": 0.6,\n    \"b48\": 0.6,\n    \"b31\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of multimodal fusion in multimedia analysis, focusing on different levels (feature, decision, and hybrid) and methods (rule-based, classification-based, and estimation-based) of fusion. Key challenges include synchronization, correlation, and confidence levels between modalities.\",\n    \"Direct Inspiration\": {\n        \"b44\": 0.9,\n        \"b121\": 0.9,\n        \"b144\": 0.8,\n        \"b28\": 0.8,\n        \"b7\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b17\": 0.7,\n        \"b99\": 0.7,\n        \"b102\": 0.7,\n        \"b8\": 0.6,\n        \"b143\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b85\": 0.5,\n        \"b56\": 0.5,\n        \"b86\": 0.5,\n        \"b54\": 0.5,\n        \"b82\": 0.5,\n        \"b58\": 0.5,\n        \"b77\": 0.5,\n        \"b39\": 0.5,\n        \"b66\": 0.5,\n        \"b152\": 0.5,\n        \"b68\": 0.5,\n        \"b107\": 0.5,\n        \"b113\": 0.5,\n        \"b100\": 0.5,\n        \"b48\": 0.5,\n        \"b31\": 0.5,\n        \"b11\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in multimodal fusion for multimedia analysis tasks, including varying data capture rates, processing times, correlations, confidence levels, costs, and optimal selection of media streams. It proposes methodologies for early fusion, late fusion, and hybrid fusion strategies to effectively combine and analyze multiple modalities, focusing on rule-based, classification-based, and estimation-based methods.\",\n    \"Direct Inspiration\": {\n        \"b44\": 0.9,\n        \"b121\": 0.9,\n        \"b144\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b102\": 0.8,\n        \"b99\": 0.75,\n        \"b17\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b149\": 0.85,\n        \"b7\": 0.75,\n        \"b8\": 0.75\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of multimodal fusion in multimedia analysis, focusing on different fusion levels (feature, decision, and hybrid), synchronization issues, and optimal modality selection. It proposes a survey of various fusion methods and highlights their advantages, limitations, and usage in multimedia tasks.\",\n  \"Direct Inspiration\": {\n    \"b44\": 0.95,\n    \"b121\": 0.90,\n    \"b144\": 0.85,\n    \"b56\": 0.80,\n    \"b102\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b99\": 0.70,\n    \"b17\": 0.65,\n    \"b28\": 0.60,\n    \"b7\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.50,\n    \"b143\": 0.45,\n    \"b150\": 0.40,\n    \"b138\": 0.35\n  }\n}\n```"], "5d8dded23a55acd1b54967df": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of semi-supervised object classification using graph neural networks (GNNs) and proposes a novel architecture-agnostic framework called GraphMix. This framework incorporates data augmentation techniques, specifically interpolation-based methods and self-supervised learning, to regularize GNNs without significant additional computational cost.\",\n    \"Direct Inspiration\": {\n        \"b46\": 1,\n        \"b41\": 1,\n        \"b42\": 1,\n        \"b2\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b25\": 0.9,\n        \"b36\": 0.9,\n        \"b21\": 0.8,\n        \"b39\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.7,\n        \"b40\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is the lack of efficient data augmentation techniques for graph-structured data, which are crucial for semi-supervised object classification using Graph Neural Networks (GNNs). The proposed GraphMix framework integrates interpolation-based data augmentation and self-supervised learning approaches to enhance the performance of GNNs without significant computational overhead.\",\n  \"Direct Inspiration\": {\n    \"b46\": 1.0,\n    \"b41\": 1.0,\n    \"b42\": 1.0,\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b36\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of regularizing Graph Neural Networks (GNNs) for semi-supervised object classification using graph-structured data. The proposed method, GraphMix, integrates interpolation-based data augmentation and self-supervised learning techniques to improve performance without significant additional computational costs.\",\n  \"Direct Inspiration\": {\n    \"b46\": 0.9,\n    \"b41\": 0.9,\n    \"b42\": 0.9,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b39\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of semi-supervised object classification using graph structured data. It proposes an architecture-agnostic framework called GraphMix, which utilizes interpolation-based data augmentation and self-target-prediction based data-augmentation techniques to improve the performance of Graph Neural Networks (GNNs) without incurring significant additional computation cost.\",\n  \"Direct Inspiration\": {\n    \"b41\": 0.9,\n    \"b46\": 0.9,\n    \"b2\": 0.85,\n    \"b42\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of semi-supervised object classification using graph structured data. The primary innovation is the GraphMix framework, which integrates regularization techniques and self-supervised learning into Graph Neural Networks (GNNs). The method emphasizes efficient data augmentation and accurate target prediction for unlabeled data to enhance the performance of GNNs.\",\n  \"Direct Inspiration\": [\"b41\", \"b42\", \"b2\", \"b25\", \"b36\", \"b46\"],\n  \"Indirect Inspiration\": [\"b21\", \"b39\"],\n  \"Other Inspiration\": [\"b15\", \"b40\"]\n}\n```"], "5eede1bc91e0116a822a4942": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the inefficiencies in implementing bitserial operations for higher-accuracy training techniques, the bottlenecks caused by glue layers in BNNs, and the lack of optimized libraries for low-bitwidth implementations. The proposed Riptide system addresses these issues by suggesting efficient bitserial implementations, producing fully bitserial versions of glue layers, and optimizing memory access for packed representations.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b12\": 1,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b4\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper include the lack of efficient bitserial implementations for higher-accuracy training techniques, the bottleneck created by glue layers, and the absence of optimized libraries for low-bitwidth implementations. The paper introduces Riptide, a system that addresses these issues by analyzing barriers to bitserial implementations, producing fully bitserial versions of glue layers, and optimizing scheduling techniques for binarized linear algebra routines. The system achieves significant speedups for binarized versions of standard networks while maintaining state-of-the-art accuracy.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b4\": 0.8,\n    \"b6\": 0.95,\n    \"b12\": 0.9,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b3\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.75,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper presents Riptide, an end-to-end system for producing binarized versions of convolutional neural networks (BNNs) that yield significant speedups. The primary challenges addressed include the lack of efficient bitserial implementations for higher-accuracy training techniques, the bottlenecks posed by glue layers, and the absence of optimized libraries for low-bitwidth implementations. The proposed solutions involve creating fully bitserial versions of glue layers, analyzing and implementing efficient bitserial operations, and scheduling optimizations specific to packed representations.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b6\": 0.9,\n        \"b12\": 0.85,\n        \"b22\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b19\": 0.8,\n        \"b20\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.7,\n        \"b9\": 0.7,\n        \"b18\": 0.75,\n        \"b2\": 0.75\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in achieving measurable performance gains with binarized neural networks (BNNs) on real-world processors. The three primary challenges identified are: 1) implementing efficient bitserial versions of high-accuracy training techniques, 2) overcoming bottlenecks caused by 'glue' layers, and 3) generating efficient machine code for low-bitwidth implementations. The Riptide system is introduced to tackle these challenges by suggesting efficient bitserial implementations, optimizing glue layers, and combining standard scheduling techniques with memory access optimization for packed representations. The paper claims to achieve significant end-to-end speedups for binarized versions of standard networks while maintaining state-of-the-art accuracy.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b19\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.75,\n    \"b22\": 0.75,\n    \"b4\": 0.7,\n    \"b9\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b20\": 0.55,\n    \"b8\": 0.55,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents Riptide, an end-to-end system for producing binarized versions of convolutional neural networks (CNNs) that yield significant speedups while maintaining state-of-the-art accuracy. The main challenges addressed include the lack of efficient bitserial implementations of recent higher-accuracy techniques, bottlenecks in glue layers, and the absence of optimized libraries for low-bitwidth implementations. The proposed method includes efficient bitserial implementation of accuracy-enhancing techniques, fully bitserial versions of glue layers, and optimized scheduling of binarized linear algebra routines.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b6\": 0.85,\n    \"b12\": 0.8,\n    \"b22\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b4\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b3\": 0.55,\n    \"b8\": 0.5,\n    \"b9\": 0.5,\n    \"b14\": 0.5\n  }\n}\n```"], "53e9a584b7602d9702eab396": ["```json\n{\n    \"Summary\": \"The paper addresses the capacity problem in dynamic branch prediction for very large workloads by introducing a two-level hierarchical branch predictor implemented in the IBM zEnterprise EC12. The predictors aim to minimize penalties from instruction cache misses, incorrect branch paths, and target redirects by employing a semi-exclusive structure with a second-level predictor used to backfill the first-level predictor. The design focuses on achieving a balance between capacity, latency, power consumption, and prediction accuracy.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1,\n        \"b2\": 0.9,\n        \"b14\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.8,\n        \"b17\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.7,\n        \"b6\": 0.7,\n        \"b8\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the capacity limitation of branch predictors in large workloads, which affects performance. The paper proposes a two-level hierarchical branch predictor design implemented in the IBM zEnterprise EC12. This design aims to achieve the benefits of a large capacity predictor with minimal impact on latency and power by using a semi-exclusive two-level structure, where the second level backs up the first level predictor to minimize instruction cache misses, incorrect branch paths, and target redirects.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b2\": 0.90,\n    \"b13\": 0.85,\n    \"b14\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b8\": 0.70,\n    \"b16\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.60,\n    \"b17\": 0.55,\n    \"b19\": 0.50\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the capacity limitation of branch predictors in handling very large workloads, which affects performance despite the accuracy of dynamic prediction algorithms. The paper proposes a two-level hierarchical branch predictor implemented in the IBM zEnterprise EC12 to address this challenge. The design includes history-based predictors and aims to minimize penalties from instruction cache misses, incorrect branch paths, and target redirects by using a small first level predictor backed by a larger second level predictor.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b2\": 0.9,\n    \"b10\": 0.85,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b16\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b19\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of capacity limitations in dynamic branch predictors which affects performance in large workloads. The paper proposes a two-level hierarchical branch predictor implemented in the IBM zEnterprise EC12, which aims to achieve high capacity with minimal impact on latency and power. The proposed design includes a first level predictor close to the instruction fetch pipeline and a second level predictor that is accessed only when needed, thereby balancing accuracy, latency, and power consumption.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.95,\n    \"b10\": 0.9,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b8\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of limited capacity in branch prediction structures for large workloads by proposing a two-level hierarchical branch predictor for the IBM zEnterprise EC12. This predictor aims to minimize instruction cache misses, incorrect branch paths, and target redirects, while balancing access latency, silicon area, and power consumption.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b1\", \"b2\", \"b13\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b14\", \"b19\", \"b25\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b6\", \"b8\", \"b12\", \"b16\"]\n  }\n}\n```"], "5d04eeba8607575390f83f3a": ["```json\n{\n  \"Summary\": \"The paper addresses performance and energy efficiency challenges in ReRAM-based DNN accelerators, focusing on exploiting sparsity in weights and activations. It introduces a Sparse ReRAM Engine (SRE) that leverages fine-grained OU-based computations, combining row-wise weight compression with dynamic wordline activation for significant performance speedup and energy savings.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b39\": 0.9,\n    \"b44\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.7,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.6,\n    \"b5\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses performance and energy efficiency challenges in memristor-based neural network accelerators, specifically ReRAM-based DNN accelerators. It proposes a Sparse ReRAM Engine (SRE) that takes advantage of fine-grained Operation Unit (OU)-based computations to jointly exploit weight and activation sparsity, achieving significant performance speedups and energy savings.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b39\": 1.0,\n    \"b44\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b43\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses performance and energy efficiency challenges in memristor-based DNN accelerators, specifically ReRAM-based architectures. It proposes a Sparse ReRAM Engine (SRE) that leverages fine-grained OU-based computations to exploit weight and activation sparsity, offering significant performance speedups and energy savings.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b39\": 1,\n    \"b44\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b23\": 0.8,\n    \"b43\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses performance and energy efficiency challenges in ReRAM-based DNN accelerators by proposing a Sparse ReRAM Engine (SRE) that exploits weight and activation sparsity at a fine granularity. The novel contributions include the introduction of Dynamic OU Formation and OU-based weight compression techniques to enhance performance and energy efficiency.\",\n  \"Direct Inspiration\": [\n    \"b8\",\n    \"b39\",\n    \"b43\",\n    \"b44\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b23\",\n    \"b30\",\n    \"b41\",\n    \"b46\"\n  ],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The paper addresses performance and energy efficiency challenges in memristor-based DNN accelerators, focusing on efficiently exploiting weight and activation sparsity in ReRAM-based architectures.\",\n    \"Inspirations\": \"The paper is inspired by the need to improve sparsity exploitation in ReRAM-based DNN accelerators and seeks to overcome limitations of existing over-idealized designs.\"\n  },\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b39\": 0.9,\n    \"b43\": 0.8,\n    \"b44\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.7,\n    \"b30\": 0.7,\n    \"b5\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5\n  }\n}\n```"], "5f33bcf791e011861cfa0fd7": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing model inference acceleration for CNNs by leveraging a framework called Woodpecker-DL (WPK), which integrates local and global graph optimization, automated searches using genetic algorithms and reinforcement learning, and system-level exploration to exploit third-party superiorities. The goal is to allow non-expert users to achieve high-speed model inference without needing deep hardware or parallel programming knowledge.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b24\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b17\": 0.8,\n    \"b27\": 0.8,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing CNN model inference to reduce computational cost and improve performance on GPUs. It introduces Woodpecker-DL (WPK), a hardware-aware optimization framework that combines graph optimization, automated searches with genetic algorithms and reinforcement learning, and system-level exploration to exploit third-party implementations. The framework aims to simplify the process for non-expert users to achieve high-speed model inference without deep hardware knowledge.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b24\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b17\": 0.8,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.5,\n    \"b7\": 0.5,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.4,\n    \"b16\": 0.4,\n    \"b18\": 0.4,\n    \"b19\": 0.4,\n    \"b20\": 0.4,\n    \"b21\": 0.4,\n    \"b23\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of high computational costs and inefficient model inference in Convolutional Neural Networks (CNNs). It proposes an optimization framework, Woodpecker-DL (WPK), which leverages graph-level optimizations, automated searches using genetic algorithms and reinforcement learning, and system-level exploration to identify best-performing operator functions from third-party implementations.\",\n    \"Direct Inspiration\": {\n        \"b23\": 1,\n        \"b24\": 0.95,\n        \"b25\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.7,\n        \"b5\": 0.7,\n        \"b6\": 0.7,\n        \"b7\": 0.7,\n        \"b9\": 0.75,\n        \"b10\": 0.75,\n        \"b11\": 0.75,\n        \"b12\": 0.75,\n        \"b13\": 0.75,\n        \"b14\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b2\": 0.6,\n        \"b3\": 0.6,\n        \"b15\": 0.65,\n        \"b16\": 0.65,\n        \"b17\": 0.8,\n        \"b18\": 0.8,\n        \"b19\": 0.8,\n        \"b20\": 0.8,\n        \"b21\": 0.8,\n        \"b22\": 0.85\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing Convolutional Neural Network (CNN) model inference by leveraging various optimization techniques, including graph optimization, automated searches with genetic algorithms and reinforcement learning, and system-level exploration to exploit third-party implementations. The proposed framework, Woodpecker-DL (WPK), aims to enable non-expert users to achieve high-speed model inference without deep understanding of underlying hardware architectures.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b23\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.9,\n    \"b24\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.8,\n    \"b9\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the high computational cost of CNN models, the inefficiency of existing deep learning frameworks and libraries, and the difficulty in implementing high-performance novel primitives. The proposed algorithm, Woodpecker-DL (WPK), addresses these challenges with a hardware-aware optimization framework that leverages global and local graph optimization, automated searches with genetic algorithms and reinforcement learning, and automatic code generation using a customized DSL compiler.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.95,\n    \"b24\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b9\": 0.85,\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b14\": 0.85,\n    \"b16\": 0.8,\n    \"b17\": 0.85,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.75,\n    \"b23\": 0.75,\n    \"b27\": 0.75,\n    \"b29\": 0.7\n  }\n}\n```"], "53e9980eb7602d9702022371": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of balancing single-thread performance with system throughput in multicore processors by introducing Continual Flow Pipelines (CFP). CFP allows processors to sustain a large number of in-flight instructions without scaling cycle-critical structures, thereby achieving high single-thread performance and high throughput. The proposed architecture includes a non-blocking register file and a unified mechanism for non-blocking register files and schedulers, significantly enhancing performance and memory latency tolerance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b13\": 0.6,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving high single-thread performance while maintaining high system throughput on multi-core processors. The proposed solution, Continual Flow Pipelines (CFP), allows processors to sustain a large number of in-flight instructions without scaling cycle-critical structures, thus achieving high performance and memory latency tolerance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b11\": 0.8,\n    \"b12\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b16\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of balancing single-thread performance and system throughput in multi-core microprocessors. It introduces Continual Flow Pipelines (CFP), an architecture that can sustain a large number of in-flight instructions without scaling up cycle-critical structures, thereby achieving high single-thread performance while allowing multiple cores on a single chip for high throughput. Key contributions include the introduction of a non-blocking register file and a unified mechanism for non-blocking register file and scheduler.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b13\": 0.5,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving high single-thread performance in multi-core processors while managing memory latency and maintaining high throughput. It proposes Continual Flow Pipelines (CFP), a processor architecture that keeps cycle-critical structures small to tolerate memory latencies, allowing high single-thread performance and multiple cores on a single chip.\",\n  \"Direct Inspiration\": [\"b1\", \"b12\"],\n  \"Indirect Inspiration\": [\"b6\", \"b11\"],\n  \"Other Inspiration\": [\"b13\", \"b16\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is achieving high single-thread performance in multi-core processors while managing memory latencies and resource constraints. The proposed Continual Flow Pipelines (CFP) architecture aims to sustain a large number of in-flight instructions without requiring large cycle-critical structures, thus balancing single-thread performance with system throughput. The paper is inspired by the limitations of conventional designs and tries to improve upon them by making register files and schedulers non-blocking.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b13\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```"], "53e9b355b7602d9703e2fc83": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving throughput and reducing latency in simultaneous multithreading (SMT) architectures. It proposes a novel SMT architecture that leverages existing superscalar technology, introduces minimal changes, and uses intelligent instruction selection heuristics to boost performance.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b20\": 0.8,\n    \"b25\": 0.7,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents an extension to simultaneous multithreading (SMT) by demonstrating throughput gains, improving single-thread performance, analyzing bottlenecks, and proposing heuristics for instruction selection. The architecture leverages existing superscalar technology, modifies fetch unit policies, and addresses register file size and instruction queue handling.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b20\": 0.8,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b16\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of increasing throughput in simultaneous multithreading (SMT) processors without extensive changes to conventional superscalar architecture. It proposes an architecture that leverages existing superscalar technology, improves thread selection heuristics, and relieves bottlenecks in instruction fetch and issue processes.\",\n    \"Direct Inspiration\": {\n        \"b26\": 1.0,\n        \"b25\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.8,\n        \"b20\": 0.8,\n        \"b21\": 0.7,\n        \"b7\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b17\": 0.6,\n        \"b29\": 0.6,\n        \"b16\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in this paper include achieving high throughput for simultaneous multithreading (SMT) without extensive modifications to superscalar processors, ensuring single-thread performance is not compromised, and addressing bottlenecks in instruction fetch and issue processes. The paper proposes an SMT architecture based on existing superscalar technology, with several heuristics developed for improving instruction fetch and issue processes.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b17\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include increasing throughput without extensive changes to superscalar processors, maintaining single-thread performance, relieving bottlenecks in a detailed architectural model, and efficiently selecting and issuing instructions from multiple threads. The paper proposes a simultaneous multithreading (SMT) architecture that leverages existing superscalar technology and introduces heuristics for instruction selection to boost throughput.\",\n    \"Direct Inspiration\": {\n        \"b26\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.8,\n        \"b20\": 0.8,\n        \"b25\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b17\": 0.6,\n        \"b29\": 0.6\n    }\n}\n```"], "53e99b9bb7602d9702446a39": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of recognizing patterns in data that can be modeled as collections of linear subspaces, specifically focusing on handling and representing these subspaces efficiently. The authors propose using the Grassmann manifold and Grassmann kernels to analytically compare various subspace distances, leveraging the Projection and Binet-Cauchy metrics for practical applications in classification problems.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b15\": 0.8,\n    \"b5\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b12\": 0.5,\n    \"b26\": 0.5,\n    \"b20\": 0.5,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of recognizing individuals from multiple images taken under different conditions, by modeling data as a collection of linear subspaces. The authors propose a novel approach by formulating the problem on the Grassmann manifold, using Grassmann kernels to enable kernel-based algorithms for subspace comparison. They emphasize the Projection metric and the Binet-Cauchy metric, providing an analytic exposition and integrating feature extraction and distance measurement around the Grassmann kernel.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9,\n    \"b12\": 0.8,\n    \"b26\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b0\": 0.7,\n    \"b3\": 0.7,\n    \"b22\": 0.7,\n    \"b21\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of recognizing individuals from multiple images by modeling data as a collection of linear subspaces and formulating problems on the Grassmann manifold. The proposed algorithm leverages Grassmann kernels to enable kernel-based algorithms in this unconventional space, providing a more consistent and robust approach to subspace-based learning.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.8,\n    \"b25\": 0.8,\n    \"b15\": 0.8\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of recognizing individuals from multiple images by representing data as a collection of linear subspaces and comparing these subspaces using various metrics on the Grassmann manifold. The proposed approach integrates feature extraction and distance measurement around the Grassmann kernel, providing a consistent and robust framework for subspace-based learning.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b23\": 0.9,\n        \"b25\": 0.8,\n        \"b15\": 0.8,\n        \"b5\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b20\": 0.7,\n        \"b24\": 0.7,\n        \"b0\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b2\": 0.6,\n        \"b21\": 0.6,\n        \"b22\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of recognizing individuals from multiple images taken under varying conditions by modeling data as collections of linear subspaces. It proposes formulating problems on the Grassmann manifold to leverage Grassmann kernels and perform analytic comparisons of subspace distances. The paper specifically explores the Projection and Binet-Cauchy metrics and applies kernel-based algorithms for classification problems.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b23\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b24\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```"], "59ec02da0cf22f5df7319dc3": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of training a Go-playing AI, AlphaGo Zero, using reinforcement learning from scratch without human intervention. The novel approach combines a deep neural network and Monte Carlo Tree Search (MCTS) to improve policy and value estimations iteratively through self-play. The paper highlights the effectiveness of this method compared to previous versions of AlphaGo that relied on supervised learning and human data.\",\n    \"Direct Inspiration\": {\n        \"b19\": 0.9,\n        \"b20\": 0.9,\n        \"b21\": 0.9,\n        \"b22\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.7,\n        \"b11\": 0.8,\n        \"b23\": 0.8,\n        \"b37\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b15\": 0.6,\n        \"b16\": 0.6,\n        \"b17\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the challenge of training a superhuman-level Go playing program, AlphaGo Zero, using a pure reinforcement learning approach without human data. The novel method combines a deep neural network with Monte-Carlo Tree Search (MCTS) to iteratively improve its performance through self-play.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b11\": 0.95,\n    \"b19\": 0.85,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b22\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.75,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of creating a superhuman-level Go player without human intervention by using a deep neural network and a novel reinforcement learning algorithm. Key components include a combined policy and value network, Monte-Carlo Tree Search (MCTS), and self-play reinforcement learning. The neural network is trained to predict move probabilities and game outcomes, iteratively improving through self-play games.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1,\n        \"b11\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.9,\n        \"b20\": 0.9,\n        \"b21\": 0.85,\n        \"b22\": 0.85\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.8,\n        \"b17\": 0.8,\n        \"b23\": 0.75\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents a novel reinforcement learning algorithm for training a deep neural network (AlphaGo Zero) that combines policy and value networks into a single architecture. The main challenge addressed is achieving superhuman performance in the game of Go without human data, relying purely on self-play. The algorithm uses Monte Carlo Tree Search (MCTS) for policy improvement and evaluation, iteratively improving the neural network's parameters through self-play.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b19\": 0.9,\n    \"b20\": 0.9,\n    \"b21\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.7,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b27\": 0.6,\n    \"b28\": 0.6,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6,\n    \"b36\": 0.6,\n    \"b37\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training a superhuman-level Go playing AI, AlphaGo Zero, from scratch without human intervention using reinforcement learning. It proposes a novel algorithm that combines Monte-Carlo Tree Search (MCTS) with a deep neural network that estimates both move probabilities and values. The neural network is trained entirely through self-play, starting with random parameters.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b19\": 0.9,\n    \"b20\": 0.9,\n    \"b21\": 0.9,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```"], "5ef0816891e0112aee042a73": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of registering a detailed 3D mesh template to a human face in an image for applications like virtual try-on of lipstick and puppeteering of virtual avatars. The proposed model, termed 'attention mesh,' uses region-specific heads and spatial transformers to achieve high accuracy without the performance drawbacks of cascaded models.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.6,\n    \"b7\": 0.5,\n    \"b2\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of registering a detailed 3D mesh template to a human face in an image for applications like AR makeup and puppeteering. The proposed solution, termed 'attention mesh', uses region-specific heads with spatial transformers to achieve high-quality landmark prediction while maintaining performance efficiency. The architecture improves upon the cascaded model approach by integrating specialized submodels for different facial regions, thereby enhancing the overall accuracy without the need for multiple independent models.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1,\n        \"b3\": 1,\n        \"b6\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8,\n        \"b5\": 0.7,\n        \"b2\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of registering a detailed 3D mesh template to a human face on an image, with applications in virtual try-on of lipstick and puppeteering of virtual avatars. The primary challenge is achieving high accuracy in perceptually significant regions (e.g., lips, eyes) without incurring performance penalties typical of cascaded approaches. The novel contribution is a unified model termed 'attention mesh' that employs region-specific heads with spatial transformers to achieve high accuracy and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b1\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b2\": 0.5,\n    \"b7\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of registering a detailed 3D mesh to a human face in an image, aiming for accurate lip and eye contours critical for applications like virtual try-on and puppeteering. The proposed solution is a unified model called 'attention mesh,' which uses region-specific heads with spatial transformers to achieve high quality and performance, avoiding the drawbacks of cascaded approaches.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b6\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b1\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b5\": 0.4,\n    \"b7\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurate 3D face mesh registration for applications like virtual try-on of makeup and puppeteering. The authors propose an 'attention mesh' model that uses region-specific heads and spatial transformers to achieve high-quality landmark predictions while maintaining efficiency suitable for mobile devices.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.6,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.4,\n    \"b5\": 0.5,\n    \"b7\": 0.3\n  }\n}\n```"], "5e15adca3a55ac47ab5b0729": ["```json\n{\n  \"Summary\": \"The paper addresses the limitations of Graph Convolutional Neural Networks (GCNs) in embedding real-world scale-free and hierarchical graphs, proposing Hyperbolic Graph Convolution Networks (HGCN) to leverage hyperbolic geometry for better representation. The primary challenges include transforming Euclidean input features into hyperbolic embeddings, performing set aggregation in hyperbolic space, and choosing the right curvature for each layer. The HGCN model resolves these challenges by deriving GCN operations in hyperbolic space, introducing hyperbolic attention-based aggregation, and using trainable curvatures at different layers.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b1\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.9,\n    \"b20\": 0.9,\n    \"b28\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b25\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of embedding real-world scale-free and hierarchical graphs, which are often poorly represented in Euclidean space, into hyperbolic space to reduce distortion. The proposed solution, Hyperbolic Graph Convolutional Networks (HGCN), extends GCNs to hyperbolic geometry to leverage both graph structure and rich node features, introducing novel methods such as hyperbolic attention-based aggregation and trainable curvatures at different layers.\",\n    \"Direct Inspiration\": [\"b28\", \"b25\"],\n    \"Indirect Inspiration\": [\"b14\", \"b20\", \"b9\"],\n    \"Other Inspiration\": [\"b5\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on addressing the challenges of embedding scale-free and hierarchical graphs using Hyperbolic Graph Convolutional Networks (HGCN). The main challenges include transforming Euclidean input features to hyperbolic space, performing set aggregation in hyperbolic space, and selecting appropriate curvatures at each layer of the network. The proposed HGCN combines the expressiveness of GCNs and hyperbolic geometry to achieve low-distortion embeddings and accurate models for real-world hierarchical and scale-free graphs.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b28\": 1,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b20\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of embedding scale-free and hierarchical graphs using Graph Convolutional Neural Networks (GCNs) in hyperbolic space. It proposes Hyperbolic Graph Convolutional Networks (HGCN), which combines the expressiveness of GCNs with the low-distortion properties of hyperbolic geometry. The authors introduce novel methods such as hyperbolic attention-based aggregation and layer-wise trainable curvature to achieve improved representations of graphs.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b25\": 0.9,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b20\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.7,\n    \"b32\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of embedding real-world graphs with scale-free or hierarchical structures in a way that minimizes distortion. It proposes a novel Hyperbolic Graph Convolutional Network (HGCN) that combines the strengths of Graph Convolutional Networks (GCNs) and hyperbolic geometry. This involves deriving core operations of GCNs in hyperbolic space, introducing hyperbolic attention-based aggregation, and applying feature transformations in hyperbolic spaces with trainable curvatures.\",\n  \"Direct Inspiration\": [\"b28\", \"b9\", \"b25\"],\n  \"Indirect Inspiration\": [\"b5\", \"b31\", \"b14\", \"b40\"],\n  \"Other Inspiration\": [\"b33\", \"b34\", \"b32\"]\n}\n```"], "5e8ef2ae91e011679da0f1a3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving topic coherence in neural topic models by incorporating contextualized document embeddings derived from pre-trained language models, specifically BERT. The authors propose an extension to the ProdLDA model by integrating BERT representations, which significantly enhances topic coherence.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b9\": 0.6,\n    \"b14\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating contextual information into neural topic models to enhance topic coherence. The authors propose extending ProdLDA, a state-of-the-art topic model, by integrating BERT representations to achieve this goal. This approach demonstrates significant improvements in topic coherence and competitive results in topic diversity.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b24\": 0.9,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving topic coherence in topic modeling by incorporating contextualized document embeddings derived from BERT into neural topic models. It extends the ProdLDA model with SBERT representations to achieve this goal, leading to significant improvements in topic coherence and competitive results in topic diversity.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b24\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b6\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving topic coherence in neural topic models by incorporating contextual information from pre-trained language models, specifically BERT. The authors propose an extension of the ProdLDA model, integrating BERT representations to enhance the model's performance in terms of coherence and topic diversity.\",\n  \"Direct Inspiration\": [\"b5\", \"b24\"],\n  \"Indirect Inspiration\": [\"b20\", \"b13\"],\n  \"Other Inspiration\": [\"b7\", \"b9\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving topic coherence in neural topic models by incorporating contextual information from pre-trained language models, specifically BERT. The proposed method extends the state-of-the-art ProdLDA model by integrating BERT representations, leading to significant improvements in topic coherence and competitive results in topic diversity.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.95,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b9\": 0.75,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b14\": 0.55\n  }\n}\n```"], "56d91a37dabfae2eee74e343": ["```json\n{\n  \"Summary\": \"The paper addresses the increasing power dissipation in microprocessors due to higher clock frequencies and added functionalities. It proposes the selective cache ways approach, which aims to reduce dynamic power consumption by partially enabling cache ways based on application requirements. This method balances performance degradation and energy savings by selectively disabling parts of the cache during less demanding periods.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b6\": 0.9,\n    \"b14\": 0.8,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b10\": 0.7,\n    \"b16\": 0.6,\n    \"b24\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b12\": 0.55,\n    \"b17\": 0.6,\n    \"b20\": 0.45\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the increasing power dissipation in microprocessors due to higher clock rates and additional hardware support. It proposes the 'selective cache ways' approach to reduce energy dissipation by enabling only a subset of cache ways based on workload demands. This method seeks to balance performance and energy efficiency by selectively enabling cache ways to match application requirements, thereby reducing the number of signal transitions within the microprocessor.\",\n    \"Direct Inspiration\": {\n        \"b5\": 0.9,\n        \"b17\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.7,\n        \"b22\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b14\": 0.65,\n        \"b24\": 0.6,\n        \"b25\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the significant increase in power dissipation in microprocessors due to higher clock rates and additional hardware support. The paper proposes an algorithm called 'selective cache ways' to reduce power dissipation by enabling only a subset of cache ways based on application requirements, thereby reducing cache switching activity. The paper examines subarray partitioning for optimal L1 cache performance and evaluates the energy savings and performance impact of this approach.\",\n    \"Direct Inspiration\": {\n        \"b22\": 0.9,\n        \"b17\": 0.8,\n        \"b5\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.7,\n        \"b10\": 0.6,\n        \"b7\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.5,\n        \"b6\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges of increasing power dissipation in microprocessors due to higher clock frequencies and increased functionality. It introduces the 'selective cache ways' approach to reduce power dissipation by enabling only a subset of the cache ways when full performance is not required. This approach aims to balance performance and energy savings, adapting to varying cache requirements of different applications.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b17\": 1,\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b25\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of increasing power dissipation in microprocessors due to higher clock frequencies and increased functionality. The proposed solution is the selective cache ways approach, which reduces energy dissipation by enabling only a subset of cache ways based on application requirements, thereby reducing switching activity without significantly degrading performance.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1,\n        \"b5\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.8,\n        \"b22\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.7,\n        \"b14\": 0.7,\n        \"b24\": 0.6\n    }\n}\n```"], "5e5e199693d709897ce77d76": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in human identification using ECG biometrics, specifically focusing on improving generalization and robustness of identification systems through a novel Cascaded CNN method. The proposed method involves pre-processing ECG signals, using a feature extraction CNN (F-CNN), a matching CNN (M-CNN), and combining these into a Cascaded CNN for effective and efficient identification.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b21\": 0.9,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b46\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.65,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in traditional biometric recognition techniques like fingerprint, palm print, face, iris, or speech recognition, which lack liveness checks and are susceptible to data replication and forgery. The proposed solution is a CNN-based method called Cascaded CNN for human identification via ECG biometrics. This approach aims to improve effectiveness and efficiency by extracting features using F-CNN and matching them using M-CNN. The cascaded architecture ensures better generalization and portability for variable groups.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b46\": 0.9,\n    \"b18\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b50\": 0.7,\n    \"b49\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of human identification via ECG biometrics, focusing on improving generalization and effectiveness with a proposed Cascaded CNN method. This method combines two CNNs: F-CNN for feature extraction and M-CNN for matching, aiming to overcome limitations of traditional and previous neural network-based approaches.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b46\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b49\": 0.8,\n    \"b50\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of human identification via electrocardiogram (ECG) biometrics, particularly focusing on one-lead ECG signals and the limitations of previous neural network-based methods. It proposes a novel Cascaded CNN method consisting of F-CNN for feature extraction and M-CNN for feature fusion and matching, aimed at improving effectiveness, efficiency, and generalization ability.\",\n    \"Direct Inspiration\": [\"b5\", \"b22\", \"b46\"],\n    \"Indirect Inspiration\": [\"b18\", \"b32\", \"b50\"],\n    \"Other Inspiration\": [\"b49\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in human identification using ECG signals, focusing on one-lead ECG biometrics to improve effectiveness and robustness. The proposed Cascaded CNN method combines F-CNN for feature extraction and M-CNN for feature fusion and comparison, significantly enhancing performance over traditional and existing methods.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b46\": 1.0,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b32\": 0.7,\n    \"b49\": 0.7,\n    \"b50\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```"], "53e9b8fcb7602d97044e29ac": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is L1 instruction fetch misses, which remain a critical performance bottleneck in various workloads. The proposed algorithm, RAS-Directed Instruction Prefetching (RDIP), seeks to simplify and reduce the storage and energy requirements of accurate hardware instruction prefetching by exploiting program context captured in the return address stack (RAS).\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b20\": 0.75,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the critical performance bottleneck caused by L1 instruction fetch misses in server, cloud, and smartphone workloads. The proposed RDIP algorithm leverages the return address stack (RAS) to create signatures for program contexts to predict and prefetch instructions accurately with reduced storage and energy requirements.\",\n  \"Direct Inspiration\": [\"b8\", \"b2\", \"b20\"],\n  \"Indirect Inspiration\": [\"b9\", \"b5\", \"b24\", \"b27\", \"b28\", \"b33\", \"b36\"],\n  \"Other Inspiration\": [\"b4\", \"b32\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of L1 instruction fetch misses, proposing a novel hardware instruction prefetching algorithm called RAS-Directed Instruction Prefetching (RDIP). The key idea is to exploit the program context, as captured in the call stack, to predict and prefetch instructions. RDIP uses return address stack (RAS) signatures to correlate with L1 instruction misses and generate prefetch requests. The approach aims to reduce storage and energy overheads compared to previous methods while achieving significant performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b8\": 0.95,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.85,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.75,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the critical problem of L1 instruction fetch misses in various computing workloads, which leads to significant performance bottlenecks. The proposed RDIP algorithm leverages the return address stack (RAS) to create compact signatures that correlate with instruction cache misses, allowing for efficient and accurate prefetching. The primary challenge is to reduce storage and energy requirements while maintaining high prefetch accuracy and coverage.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1.0,\n        \"b9\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b20\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.7,\n        \"b11\": 0.6,\n        \"b19\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the performance bottleneck caused by L1 instruction fetch misses in various computing workloads. The proposed solution is RDIP (RAS-Directed Instruction Prefetching), which uses the state of the return address stack (RAS) to generate signatures that predict and prefetch instruction cache misses. This technique aims to reduce storage and energy requirements while improving prefetch accuracy and performance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b8\": 1.0,\n    \"b9\": 0.9,\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```"], "5ea013159fced0a24b9cf180": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper relate to the inefficiencies and performance penalties associated with TCP protocol conformance in short-lived connections and L7 proxying. The proposed solution, AccelTCP, is a dual-stack TCP design that offloads selective stateful TCP operations to modern NICs. This approach aims to save compute cycles and memory bandwidth, thereby improving performance for short-lived connections and L7 proxying.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b28\": 0.9,\n    \"b39\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b16\": 0.7,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the performance penalty of TCP in handling short-lived connections and L7 proxying. The proposed algorithm, AccelTCP, aims to address these challenges by offloading selective stateful TCP operations to modern smart NICs, thereby saving CPU cycles and memory bandwidth. This dual-stack TCP design splits functionality between a host and a NIC stack.\",\n  \"Direct Inspiration\": [\"b3\", \"b28\", \"b39\", \"b53\", \"b59\"],\n  \"Indirect Inspiration\": [\"b35\", \"b41\", \"b42\", \"b5\", \"b6\", \"b68\"],\n  \"Other Inspiration\": [\"b16\", \"b4\", \"b31\", \"b29\", \"b63\", \"b52\", \"b62\", \"b64\", \"b34\", \"b1\", \"b36\", \"b37\", \"b58\", \"b8\", \"b33\", \"b45\", \"b48\", \"b55\", \"b32\", \"b65\", \"b10\", \"b13\", \"b26\", \"b0\", \"b12\", \"b24\", \"b60\", \"b47\", \"b51\", \"b14\", \"b7\", \"b19\", \"b49\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of poor performance in handling short-lived TCP connections and L7 proxying due to the overheads of TCP operations. The proposed solution, AccelTCP, leverages modern NICs to offload stateful TCP operations, resulting in significant performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b28\": 0.9,\n    \"b39\": 0.9,\n    \"b41\": 0.8,\n    \"b42\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.7,\n    \"b35\": 0.7,\n    \"b59\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b16\": 0.6,\n    \"b68\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of TCP protocol conformance in short-lived connections and L7 proxying. It proposes AccelTCP, a dual-stack TCP design leveraging NICs for offloading specific TCP operations to improve performance and efficiency. The key contributions include the offloading of connection management and packet relaying to NICs, thereby saving CPU cycles and memory bandwidth.\",\n  \n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b28\": 1.0,\n    \"b39\": 1.0\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b35\": 0.7,\n    \"b41\": 0.7,\n    \"b42\": 0.7\n  },\n  \n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b4\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges addressed by the paper include the inefficiencies in handling short-lived TCP connections and layer-7 (L7) proxying, as well as the overhead associated with maintaining TCP protocol conformance.\",\n    \"Inspirations\": \"The paper takes inspiration from recent kernel-bypass TCP stacks and the potential of smart NICs for offloading stateful TCP operations to improve performance.\"\n  },\n  \"Direct Inspiration\": [\n    \"b3\",\n    \"b28\",\n    \"b39\",\n    \"b53\",\n    \"b59\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b35\",\n    \"b41\",\n    \"b42\"\n  ],\n  \"Other Inspiration\": [\n    \"b4\",\n    \"b16\",\n    \"b31\"\n  ]\n}\n```"], "5f76f20a91e011f31b98056c": ["```json\n{\n    \"Summary\": \"The paper introduces Crowdsourced Stereotype Pairs (CrowS-Pairs), a challenge set for measuring social biases in language models, covering nine types of bias. The primary challenge is quantifying and mitigating biases in pretrained language models that can cause harm in downstream applications. The paper aims to provide a reliable quantitative benchmark to measure these biases directly in the models.\",\n    \"Direct Inspiration\": {\n        \"b21\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b16\": 0.8,\n        \"b14\": 0.8,\n        \"b33\": 0.7,\n        \"b31\": 0.7\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the issue of social biases present in large pretrained language models and proposes a new benchmark, CrowS-Pairs, to measure these biases. CrowS-Pairs is a crowdsourced dataset that evaluates the extent to which language models prefer stereotypical sentences over less stereotypical ones across nine categories of social bias. The dataset includes minimally different pairs of sentences reflecting stereotypes and anti-stereotypes about historically disadvantaged groups in the United States.\",\n    \"Direct Inspiration\": {\n        \"b21\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b16\": 0.7,\n        \"b14\": 0.7,\n        \"b2\": 0.6,\n        \"b3\": 0.6,\n        \"b7\": 0.6,\n        \"b19\": 0.6,\n        \"b38\": 0.6,\n        \"b27\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b33\": 0.5,\n        \"b31\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of quantifying social biases in large pretrained language models. The authors propose a new benchmark, Crowdsourced Stereotype Pairs (CrowS-Pairs), to measure these biases more reliably compared to existing datasets. The focus is on explicit stereotypes about historically disadvantaged groups in the United States, covering nine bias types. The paper demonstrates that these language models exhibit varying degrees of bias across different categories.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b7\": 0.6,\n    \"b19\": 0.6,\n    \"b27\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of measuring social biases in large pretrained language models and introduces CrowS-Pairs, a crowdsourced dataset designed to evaluate the degree of nine types of social bias in these models. The paper aims to provide a reliable quantitative benchmark for bias measurement to improve the development of debiased models.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b7\": 0.6,\n    \"b19\": 0.7,\n    \"b27\": 0.6,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b16\": 0.5,\n    \"b14\": 0.5,\n    \"b33\": 0.5,\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of quantifying social biases in large pretrained language models, which often contain biases against historically disadvantaged groups. The authors propose the Crowdsourced Stereotype Pairs (CrowS-Pairs) dataset to measure nine types of social bias in language models, using explicit stereotype expressions. This dataset is crowdsourced, providing greater diversity and acknowledgement of bias in the United States. The paper aims to create a reliable benchmark to measure and address biases at the source, comparing CrowS-Pairs with existing datasets like StereoSet.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b7\": 0.6,\n    \"b19\": 0.6,\n    \"b27\": 0.6,\n    \"b38\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.8,\n    \"b16\": 0.8,\n    \"b14\": 0.8,\n    \"b33\": 0.5,\n    \"b31\": 0.5\n  }\n}\n```"], "5b1642d68fbcbf6e5a9b7e77": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of gender bias in coreference resolution systems. It introduces a new dataset called WinoBias to test this bias and proposes methods to mitigate it, including data augmentation through gender swapping and debiasing word embeddings. The study reveals significant gender bias in existing systems and demonstrates that the proposed methods can effectively reduce this bias without compromising overall performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b6\": 0.9,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b9\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of gender bias in coreference resolution systems. The authors propose a new dataset, WinoBias, to test for gender bias and introduce methods such as gender swapping and debiased word embeddings to mitigate this bias. The key contributions include identifying the sources of bias in training data and auxiliary resources and providing strategies to eliminate such biases.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b9\": 0.7,\n    \"b20\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses gender bias in coreference resolution systems and introduces the WinoBias dataset to evaluate this bias. It proposes methods to mitigate the identified biases, including data augmentation through gender swapping and the use of debiased word embeddings.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b9\": 0.5,\n    \"b20\": 0.4,\n    \"b18\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of gender bias in coreference resolution systems. The authors propose a new dataset called WinoBias to test coreference systems for gender bias and suggest a data augmentation approach to mitigate this bias. They also evaluate existing systems and demonstrate that their methods can reduce bias without significantly affecting overall performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b6\": 0.5,\n    \"b9\": 0.4,\n    \"b18\": 0.4,\n    \"b20\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses gender bias in coreference resolution systems. It introduces the WinoBias dataset to evaluate gender bias, showing that systems often link gendered pronouns to stereotypical occupations more accurately than to non-stereotypical ones. The paper suggests methods to reduce this bias, such as data augmentation through gender swapping and debiasing word embeddings.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.75,\n    \"b6\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b9\": 0.5,\n    \"b18\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```"], "5aed14c317c44a4438157aca": ["```json\n{\n  \"Summary\": \"The paper discusses the extension of classical signal processing concepts to graph signal processing (GSP), addressing challenges such as defining smoothness on graphs, sampling, filtering, and frequency response. It emphasizes the importance of underlying graph structures and their role in modeling complex data interactions. The paper also explores applications in areas such as urban mobility, social networks, and sensor networks.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b11\": 0.7,\n    \"b50\": 0.6,\n    \"b65\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extending classical signal processing concepts to data residing on graphs, focusing on graph signal processing (GSP). It aims to model complex data interactions using graphs, introducing methods for tasks such as graph Fourier transform, sampling, filtering, and smoothness analysis. The key inspiration comes from classical signal processing and spectral graph theory, with significant emphasis on the use of graph Laplacians and adjacency matrices to define graph frequencies and shifts.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b50\": 0.8,\n    \"b65\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.6,\n    \"b46\": 0.6,\n    \"b47\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include extending classical signal processing concepts to data residing on graphs, tackling complex tasks such as sampling, denoising, and learning underlying graph structures. The paper is inspired by the field of graph signal processing (GSP) and builds upon various foundational concepts from classical signal processing and spectral graph theory.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1,\n        \"b2\": 1,\n        \"b65\": 0.9,\n        \"b11\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b12\": 0.8,\n        \"b50\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b44\": 0.6,\n        \"b45\": 0.5,\n        \"b46\": 0.5,\n        \"b47\": 0.5,\n        \"b48\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extending classical signal processing concepts to data residing on graphs, focusing on tasks such as graph Fourier transform, filtering, frequency response, and sampling. The novel approach lies in the development of graph signal processing (GSP) which allows for the principled handling of graph-structured data.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1,\n    \"b65\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b50\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.6,\n    \"b46\": 0.6,\n    \"b47\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of extending classical signal processing concepts to data residing on graphs, a field known as Graph Signal Processing (GSP). Key issues discussed include the definition of graph signals, graph Fourier transform, filtering, sampling, and smoothness of graph signals. The paper also explores applications in urban settings, social networks, and various types of graphs.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1,\n        \"b2\": 1,\n        \"b50\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.8,\n        \"b65\": 0.7,\n        \"b44\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b45\": 0.6,\n        \"b51\": 0.6\n    }\n}\n```"], "53e99f7fb7602d9702853a66": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper revolve around optimizing Monte Carlo Tree Search (MCTS) for a variety of decision-making problems, particularly in AI applications such as games and planning problems. The paper aims to survey MCTS research, detailing its algorithm, variations, enhancements, and applications across different domains.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b171\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b179\": 0.8\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The paper outlines the primary challenge of optimizing decision-making in complex domains using Monte Carlo Tree Search (MCTS). The algorithm must balance exploration and exploitation to effectively build a search tree in domains with high branching factors, deep trees, and where domain-specific knowledge is minimal.\",\n    \"inspirations\": \"The paper is inspired by the success of MCTS in computer Go and aims to extend and refine the algorithm for broader applications. The authors survey existing research to provide a comprehensive understanding of MCTS and its variations.\"\n  },\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b171\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b179\": 0.8\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on Monte Carlo Tree Search (MCTS), a method for making optimal decisions in domains represented as trees of sequential decisions, particularly in games and planning problems. The main challenges addressed include understanding the conditions for MCTS success and failure, and extending and refining the basic algorithm to improve its performance across various domains. The authors aim to provide a comprehensive survey of MCTS research, its mathematical foundations, variations, enhancements, and applications.\",\n  \"Direct Inspiration\": [\n    \"b16\",\n    \"b171\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b179\"\n  ],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper provides a comprehensive survey of Monte Carlo Tree Search (MCTS) research, focusing on understanding the algorithm, its variations and enhancements, and its applications in various domains. The primary challenges include balancing exploration and exploitation in the search tree and adapting the basic MCTS algorithm to specific domains to realize its full potential. The authors aim to improve the reader's understanding of how MCTS can be applied to new research questions and problem domains.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b171\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b179\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper surveys the range of published work on Monte Carlo Tree Search (MCTS), aiming to provide tools to solve new problems using MCTS and investigate this powerful approach to searching trees and directed graphs. It outlines the primary challenges in applying MCTS to various domains, and reviews the algorithm, its variations, enhancements, and performance across different applications.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b171\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b179\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```"], "5cede104da562983788e3653": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the need for a model that can automatically infer missing facts in knowledge graphs while maintaining mathematical transparency and principled understanding, unlike deep, non-linear models. The paper introduces TuckER, a linear model for link prediction on knowledge graphs, based on Tucker decomposition. TuckER is proposed as a strong baseline model that is simple, expressive, and achieves state-of-the-art results across standard datasets. The key contributions include proving that TuckER is fully expressive, deriving a bound on the embedding dimensionality for full expressiveness, and showing how TuckER subsumes several previously proposed tensor factorization approaches.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.9,\n    \"b28\": 0.9,\n    \"b25\": 0.9,\n    \"b0\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b1\": 0.7,\n    \"b30\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces TuckER, a new linear model for link prediction on knowledge graphs, based on Tucker decomposition. It addresses the challenges of predicting missing facts in knowledge graphs by proposing a fully expressive model that subsumes several previous tensor factorization approaches. The key contributions include proving full expressiveness, deriving a dimensionality bound, and demonstrating that TuckER encompasses previous models like RESCAL, DistMult, ComplEx, and SimplE.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b4\": 0.75,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.9,\n    \"b28\": 0.85,\n    \"b25\": 0.8,\n    \"b9\": 0.75,\n    \"Kazemi and Poole (2018)\": 0.7\n  }\n}\n```", "```json\n{\n   \"Summary\":\"The paper introduces TuckER, a new linear model for link prediction on knowledge graphs, based on Tucker decomposition. The key contributions include proving TuckER's full expressiveness, deriving a bound on embedding dimensionality, and showing that several previous state-of-the-art linear models are special cases of TuckER.\",\n   \"Direct Inspiration\":{\n      \"b27\":0.9,\n      \"b18\":0.8\n   },\n   \"Indirect Inspiration\":{\n      \"b16\":0.7,\n      \"b28\":0.7,\n      \"b25\":0.7,\n      \"b9\":0.7\n   },\n   \"Other Inspiration\":{\n      \"b4\":0.6,\n      \"b0\":0.6,\n      \"b26\":0.5,\n      \"b30\":0.5,\n      \"b12\":0.5,\n      \"b24\":0.4,\n      \"b7\":0.4\n   }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the need for algorithms that can automatically infer missing facts in knowledge graphs. The proposed algorithm, TuckER, is a linear model based on Tucker decomposition, which is fully expressive and subsumes several previous tensor factorization approaches. The key contributions include proposing TuckER for link prediction, proving its full expressiveness, and showing that it subsumes previous models.\",\n  \"Direct Inspiration\": [\n    \"b16\",\n    \"b28\",\n    \"b25\",\n    \"b9\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b4\",\n    \"b0\"\n  ],\n  \"Other Inspiration\": [\n    \"b27\",\n    \"b18\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces TuckER, a new linear model for link prediction on knowledge graphs based on Tucker decomposition. The model is simple, expressive, and achieves state-of-the-art results. The key contributions are proving TuckER's full expressiveness, deriving a bound on the embedding dimensionality for full expressiveness, and showing how TuckER subsumes several previously proposed tensor factorization approaches.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1,\n    \"b16\": 0.9,\n    \"b28\": 0.9,\n    \"b25\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b1\": 0.6,\n    \"b30\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```"], "5736960c6e3b12023e51ee06": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Performing machine learning tasks on graph-structured data, especially for problems requiring sequential outputs.\",\n      \"Learning features on graphs that encode the partial output sequence and the remaining sequence.\"\n    ],\n    \"inspirations\": [\n      \"Previous work on Graph Neural Networks (GNNs) and their applications.\",\n      \"Program verification tasks that require outputting logical formulas.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b10\": 0.6,\n    \"b8\": 0.5,\n    \"b4\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of extending Graph Neural Networks (GNNs) to output sequences instead of single outputs, which is crucial for tasks requiring sequential outputs such as program verification. The authors propose Gated Graph Sequence Neural Networks (GGS-NNs) to handle this challenge. They adapt the existing GNN framework by incorporating modern practices around Recurrent Neural Networks, specifically Gated Recurrent Units (GRUs), to allow for the sequential output of logical formulas and other tasks requiring stepwise outputs.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b3\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b10\", \"b24\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b4\", \"b5\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performing machine learning tasks on graph-structured data by extending Graph Neural Networks (GNNs) to output sequences. The primary challenge is to produce outputs that are sequences, such as paths or logical formulas, rather than single outputs like graph-level classifications. They introduce Gated Graph Sequence Neural Networks (GGS-NNs) as a solution. The paper also highlights the broader applicability of GNNs and their extensions to various problems.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performing machine learning tasks on graph-structured data, specifically requiring sequence outputs from graph inputs. The authors propose an extension of Graph Neural Networks (GNNs) called Gated Graph Sequence Neural Networks (GGS-NNs), which are designed to handle sequential outputs. The novel contributions include adapting the GNN framework to produce sequences and applying it to program verification tasks.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b20\": 0.5,\n    \"b25\": 0.5,\n    \"b8\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning features on graph-structured data to output sequences rather than single values. The proposed solution, Gated Graph Sequence Neural Networks (GGS-NNs), extends Graph Neural Networks (GNNs) to handle sequential outputs, which is beneficial for tasks such as program verification. The work builds on previous methods like Graph Neural Networks and integrates modern practices around Recurrent Neural Networks.\",\n  \"Direct Inspiration\": [\"b3\", \"b24\"],\n  \"Indirect Inspiration\": [\"b10\", \"b5\"],\n  \"Other Inspiration\": [\"b8\", \"b14\"]\n}\n```"], "5f993ec291e011a3fbe2fb5c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating simpler, scalable models for transductive node classification using graph neural networks (GNNs). The proposed method, termed Correct and Smooth (C&S), combines simple node feature-based predictors with classical label propagation techniques to correct and smooth predictions. This approach outperforms complex GNN models in terms of accuracy, training time, and parameter efficiency.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.95,\n    \"b22\": 0.90,\n    \"b49\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.85,\n    \"b41\": 0.80,\n    \"b36\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.75,\n    \"b42\": 0.75,\n    \"b37\": 0.70\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of performance and scalability in graph neural networks (GNNs) by proposing a simpler pipeline called Correct and Smooth (C&S) for transductive node classification. The core contributions include using label propagation for error correction and smoothing predictions, which improves accuracy and reduces training time compared to traditional GNNs.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b49\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.9,\n    \"b22\": 0.8,\n    \"b41\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b42\": 0.7,\n    \"b37\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces a simpler, more scalable pipeline for transductive node classification on graphs, called Correct and Smooth (C&S). The main challenges addressed are the complexity and scalability issues of current Graph Neural Networks (GNNs). The proposed algorithm combines a simple base prediction using node features with post-processing steps involving label propagation to correct and smooth predictions, leading to state-of-the-art performance with fewer parameters and faster training times.\",\n    \n    \"Direct Inspiration\": {\n        \"b19\": 1.0,\n        \"b49\": 1.0,\n        \"b22\": 0.9\n    },\n    \n    \"Indirect Inspiration\": {\n        \"b41\": 0.7,\n        \"b17\": 0.7\n    },\n    \n    \"Other Inspiration\": {\n        \"b21\": 0.5,\n        \"b42\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of complex and computationally expensive Graph Neural Networks (GNNs) by proposing a simpler, scalable pipeline for transductive node classification. The method, termed Correct and Smooth (C&S), combines a base predictor that ignores graph structure with two label propagation steps for error correction and prediction smoothing. This approach leverages classical graph-based semi-supervised learning techniques and directly incorporates labels into predictions, achieving state-of-the-art performance with fewer parameters and faster training times.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b19\": 0.9,\n    \"b22\": 0.95,\n    \"b41\": 0.9,\n    \"b49\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b42\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.75,\n    \"b25\": 0.75,\n    \"b36\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of understanding performance gains and scaling in complex GNN architectures. It proposes a simpler pipeline for transductive node classification that combines base predictions with classical label propagation methods for error correction and smoothing, achieving state-of-the-art performance with fewer parameters and faster training times.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b22\": 0.8,\n    \"b49\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b41\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.65,\n    \"b42\": 0.6,\n    \"b36\": 0.55\n  }\n}\n```"], "5c234870da562935fc1d4db3": ["```json\n{\n    \"Summary\": \"The paper addresses the inefficiency of high-end device optimizations in mobile apps. It proposes a novel method to track and prioritize Critical Instruction Chains (CritIC) in mobile apps using a criticality metric and software-based optimizations. The main contributions are identifying self-contained instruction chains, leveraging ARM's 16-bit instruction format to double fetch bandwidth, and implementing a profiler and compiler pass to optimize CritIC sequences.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1.0,\n        \"b3\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b8\": 0.8,\n        \"b11\": 0.8,\n        \"b25\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.6,\n        \"b22\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper are the inefficacy of existing instruction prioritization and memory prefetching mechanisms for mobile apps, which are highly user-interactive and have different workload characteristics compared to server/desktop workloads. The bottleneck shifts to the front-end (Fetch stage) of the pipeline in mobile apps.\",\n    \"inspirations\": \"The paper is inspired by the concept of 'criticality' of instructions used in high-end device optimizations and proposes tracking criticality at the granularity of self-contained instruction chains (CritIC) to better utilize existing resources in resource-constrained environments.\"\n  },\n  \"Direct Inspiration\": {\n    \"reference_numbers\": [\"b17\", \"b3\", \"b32\"]\n  },\n  \"Indirect Inspiration\": {\n    \"reference_numbers\": [\"b3\", \"b7\", \"b8\", \"b11\"]\n  },\n  \"Other Inspiration\": {\n    \"reference_numbers\": [\"b18\", \"b19\", \"b20\", \"b21\", \"b22\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges in the paper include the inefficiency of traditional criticality-based optimizations for mobile apps and the need for a novel approach to handle critical instruction chains in a resource-constrained environment. The proposed algorithm introduces Critical Instruction Chains (CritIC) to optimize the fetch stage of the CPU pipeline by leveraging ARM's 16-bit instruction format, aiming to achieve significant performance improvements with minimal hardware modifications.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9,\n        \"b17\": 0.85,\n        \"b25\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.75,\n        \"b11\": 0.75,\n        \"b31\": 0.7,\n        \"b32\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.6,\n        \"b22\": 0.6,\n        \"b18\": 0.65,\n        \"b19\": 0.65,\n        \"b20\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the unique challenges of optimizing mobile applications compared to desktop/server workloads by tracking and prioritizing Critical Instruction Chains (CritICs) in software. The proposed solution involves minimal hardware support and leverages ARM's 16-bit instruction format to double fetch bandwidth and reduce pipeline stalls.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.7,\n    \"b11\": 0.7,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.5,\n    \"b18\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficacy of traditional critical instruction optimizations for mobile apps due to their unique characteristics, and proposes a novel method to prioritize and optimize Critical Instruction Chains (CritIC) in software, leveraging ARM's 16-bit instruction format to double fetch bandwidth and enhance performance.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b3\": 0.85,\n    \"b32\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b8\": 0.7,\n    \"b11\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b22\": 0.55,\n    \"b31\": 0.5\n  }\n}\n```"], "55465e4c0cf2939c2feea7c8": ["```json\n{\n    \"Summary\": \"The primary challenge addressed in this paper is the performance degradation caused by the indirect branch instruction in switch-based interpreters, particularly on deeply pipelined architectures. The paper evaluates the performance of indirect branch prediction on modern Intel processors and introduces jump threading as an optimization to bypass the switch mechanism.\",\n    \"Direct Inspiration\": {\n        \"b5\": 1,\n        \"b10\": 1,\n        \"b30\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b11\": 0.75,\n        \"b23\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b24\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges addressed in the paper are the performance degradation of switch-based interpreters due to the indirect branch instruction and the need for better branch prediction. The paper proposes revisiting the performance of switch-based interpreters on recent Intel processor generations and evaluating the state-of-the-art indirect branch predictor, ITTAGE.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b10\": 0.75,\n    \"b11\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance challenges of switch-based interpreters, focusing on the impact of the indirect branch instruction on modern Intel processors. It proposes the use of state-of-the-art indirect branch predictor ITTAGE to improve prediction accuracy, and revisits previous findings on the predictability and techniques to optimize the dispatch loop in interpreters.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b11\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b7\": 0.5,\n    \"b18\": 0.4,\n    \"b28\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of switch-based interpreters by revisiting the impact of indirect branch prediction on modern Intel processors. It evaluates and compares the performance of existing branch predictors, particularly the ITTAGE predictor, on interpreters for languages like Python, Javascript, and CLI. The paper demonstrates that indirect branch prediction is less critical on newer architectures and proposes optimizations like jump threading to enhance interpreter efficiency.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b30\", \"b11\", \"b10\", \"b23\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b3\", \"b7\", \"b28\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b5\", \"b22\", \"b6\", \"b8\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance issues of switch-based interpreters, particularly focusing on the impact of the indirect branch instruction on modern Intel processors. It revisits the performance of switch-based interpreters, evaluates the state-of-the-art indirect branch predictor ITTAGE, and proposes optimizations such as jump threading to improve interpreter performance.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b10\": 1,\n    \"b22\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b11\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```"], "5da1a6d447c8f766460688bc": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving single-thread performance in processors by enhancing the issue queue (IQ) organization. The proposed algorithm, SWQUE, dynamically configures the IQ to either a priority-correcting circular queue (CIRC-PC) or an AGE-based configuration based on the capacity demand. This approach aims to achieve both correct prioritization and high capacity efficiency.\",\n    \"Direct Inspiration\": {\n        \"b7\": 1,\n        \"b18\": 1,\n        \"b20\": 1,\n        \"b22\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b8\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.6,\n        \"b16\": 0.6,\n        \"b17\": 0.6,\n        \"b25\": 0.5,\n        \"b26\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of achieving high single-thread performance in processors by focusing on the issue queue (IQ). The proposed solution, the Switching Issue Queue (SWQUE), dynamically configures the IQ to achieve correct prioritization and high capacity efficiency based on the execution's demand. The novel approach aims to improve performance during both priority-sensitive and capacity-demanding phases using a modified circular queue (CIRC-PC) and an age matrix-based system (AGE).\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b18\": 1.0,\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b16\": 0.7,\n    \"b26\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses improving single-thread performance in processors by focusing on the issue queue (IQ) organization. The primary challenges include ensuring correct priority assignment and capacity efficiency in IQs. The proposed algorithm, switching issue queue (SWQUE), dynamically configures the IQ based on the degree of capacity demand, switching between a modified circular queue (CIRC-PC) for priority-sensitive phases and an AGE organization for capacity-demanding phases.\",\n    \"Direct Inspiration\": {\n        \"b5\": 1,\n        \"b7\": 0.9,\n        \"b18\": 0.95,\n        \"b22\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.7,\n        \"b11\": 0.7,\n        \"b16\": 0.6,\n        \"b17\": 0.85\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.5,\n        \"b20\": 0.6,\n        \"b25\": 0.4,\n        \"b26\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing single-thread performance by improving the issue queue (IQ) in processors. The proposed solution, the switching issue queue (SWQUE), dynamically configures the IQ as either a modified circular queue (CIRC-PC) or an age-based queue (AGE) based on the phase of execution. This mechanism ensures correct prioritization and high capacity efficiency, depending on the need.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b7\": 0.85,\n    \"b18\": 0.9,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b17\": 0.75,\n    \"b20\": 0.75,\n    \"b26\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b8\": 0.65,\n    \"b11\": 0.6,\n    \"b14\": 0.6,\n    \"b16\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving single-thread performance through a novel issue queue (IQ) organization called the switching issue queue (SWQUE). The primary challenge is achieving correct prioritization and high capacity efficiency in the IQ. SWQUE dynamically configures the IQ as either a modified circular queue (CIRC-PC) or an age-based queue (AGE) based on the degree of capacity demand.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b18\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b9\": 0.6,\n    \"b16\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```"], "53e99822b7602d9702043cce": ["```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the difficulty in providing high instruction bandwidth for aggressive wide-issue superscalar processors, particularly for codes with frequent control flow transfers, few loops, high instruction-cache miss rates, and large code sizes. The proposed algorithm, called the Software Trace Cache (STC), addresses these issues by reordering code to maximize the sequentiality of instructions and minimize instruction cache misses. The paper highlights the limitations of past works and presents a novel, fully-automated, compile-time code reordering technique aimed at wide-issue superscalars.\",\n    \"Direct Inspiration\": {\n        \"b12\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b8\": 0.7,\n        \"b10\": 0.7,\n        \"b13\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b4\": 0.6,\n        \"b7\": 0.6,\n        \"b2\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of increasing the instruction bandwidth in aggressive wide-issue superscalars by proposing a novel code reordering technique called the Software Trace Cache (STC). This technique aims to maximize the sequentiality of instructions while minimizing instruction cache misses. The STC is particularly effective for large codes with few loops and deterministic execution sequences, such as database management systems and certain SPEC applications.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b8\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b7\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b5\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of delivering a high bandwidth of useful instructions to aggressive wide-issue superscalar processors, especially for codes with frequent control flow transfers, few loops, high instruction-cache miss rates, and large code sizes. It proposes a novel compile-time code reordering technique called Software Trace Cache (STC) to maximize instruction sequentiality while minimizing cache misses. The STC improves performance by inlining popular functions and optimizing the memory mapping of basic blocks based on execution frequency.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b3\": 0.8,\n    \"b8\": 0.85,\n    \"b10\": 0.8,\n    \"b13\": 0.75,\n    \"b5\": 0.75,\n    \"b4\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of providing high instruction bandwidth for aggressive wide-issue superscalar processors, particularly in applications with frequent control flow transfers, few loops, high instruction-cache miss rates, and large code sizes. It introduces a fully-automated, compile-time code reordering technique called the Software Trace Cache (STC) that focuses on maximizing the sequentiality of instructions while minimizing instruction cache misses.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b3\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b10\": 0.6,\n    \"b5\": 0.6,\n    \"b2\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.4,\n    \"b7\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the difficulty of supplying a high bandwidth of useful instructions to aggressive wide-issue superscalar processors, especially for codes with frequent control flow transfers, few loops, high instruction-cache miss rates, and large code sizes. The proposed solution is a fully-automated compile-time code reordering technique called the Software Trace Cache (STC), which focuses on maximizing the sequentiality of instructions while minimizing instruction cache misses.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b3\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b10\": 0.8,\n    \"b13\": 0.8,\n    \"b5\": 0.8,\n    \"b4\": 0.8,\n    \"b7\": 0.8,\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```"], "558ab239e4b037c08758a249": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving branch prediction accuracy and overall fetch performance by examining the effect of code reordering optimizations on both static and dynamic branch predictors. The proposed method uses the Software Trace Cache (STC) algorithm to optimize code layout, increasing the fraction of 'not taken' branches, which enhances static predictors' accuracy and reduces prediction table interference in dynamic predictors.\",\n    \"Direct Inspiration\": {\n        \"b18\": 1.0,\n        \"b19\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b12\": 0.85,\n        \"b21\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.7,\n        \"b14\": 0.65,\n        \"b25\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving fetch performance in processors by focusing on the number of instruction cache misses, the width of instructions fetched each cycle, and branch prediction accuracy. It proposes the use of code reordering techniques to improve instruction cache performance and fetch bandwidth, and examines their interaction with both static and dynamic branch predictors using the Software Trace Cache layout optimization.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b6\": 0.8,\n    \"b20\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b10\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges related to instruction fetch performance, focusing on instruction cache misses, instruction fetch width, and branch prediction accuracy. The authors propose using code reordering techniques to optimize instruction cache performance and examine the interaction of these optimizations with static and dynamic branch predictors. They highlight the benefits of increased not-taken branch predictions and reduced prediction table interference, leading to overall processor performance improvements.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b20\": 0.7,\n    \"b23\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b12\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving fetch performance, which depends on instruction cache misses, fetch width, and branch prediction accuracy. The authors propose using code reordering techniques to enhance branch prediction accuracy and overall processor performance. They specifically investigate the interaction between code reordering optimizations and both static and dynamic branch predictors using the Software Trace Cache (STC) layout optimization.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b18\", \"b19\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b25\", \"b10\", \"b12\", \"b21\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b20\", \"b23\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of fetch performance in processors, which depend on instruction cache misses, instruction fetch width, and branch prediction accuracy. It proposes using code reordering techniques to optimize instruction cache performance and fetch bandwidth while examining the impact on both static and dynamic branch predictors, particularly using the Software Trace Cache (STC) layout optimization.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b25\": 0.7,\n    \"b19\": 0.6,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```"], "5c8b99db4895d9cbc69c7956": ["```json\n{\n  \"Summary\": \"The paper addresses performance and energy efficiency challenges in PHP web applications by proposing specialized hardware accelerators for hash table access, heap management, string manipulation, and regular expression processing. The authors identify inefficiencies in current PHP application runtimes and suggest domain-specific hardware optimizations inspired by prior research.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b20\": 0.8,\n    \"b29\": 0.85,\n    \"b30\": 0.85,\n    \"b38\": 0.8,\n    \"b66\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b53\": 0.7,\n    \"b56\": 0.65,\n    \"b64\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.55,\n    \"b16\": 0.55,\n    \"b18\": 0.55,\n    \"b23\": 0.5,\n    \"b32\": 0.5,\n    \"b33\": 0.5,\n    \"b37\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance inefficiencies in PHP web applications, particularly focusing on microarchitectural bottlenecks and proposing specialized hardware accelerators to optimize performance. It highlights challenges such as high abstraction overheads, inefficient memory allocation, and the need for faster string manipulation and regular expression processing.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b20\": 0.8,\n    \"b29\": 0.8,\n    \"b30\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b32\": 0.7,\n    \"b37\": 0.7,\n    \"b44\": 0.7,\n    \"b55\": 0.7,\n    \"b56\": 0.7,\n    \"b64\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b46\": 0.6,\n    \"b53\": 0.6,\n    \"b58\": 0.6,\n    \"b61\": 0.6,\n    \"b62\": 0.6,\n    \"b66\": 0.6,\n    \"b72\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance and energy-efficiency bottlenecks in server-side PHP web applications, which are widespread and critical for many web services. The authors propose several hardware accelerators to mitigate the overheads of common tasks in PHP applications: hash table access, heap management, string manipulation, and regular expression processing. These optimizations aim to improve the execution efficiency of PHP applications by targeting their unique runtime characteristics.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b38\": 0.85,\n    \"b29\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.75,\n    \"b53\": 0.7,\n    \"b56\": 0.65,\n    \"b66\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b33\": 0.55,\n    \"b44\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance and energy-efficiency bottlenecks in real-world PHP applications, identifying key activities such as hash map access, heap management, string manipulation, and regular expression processing as areas for specialized hardware acceleration.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b38\": 0.85,\n    \"b20\": 0.8,\n    \"b29\": 0.9,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b53\": 0.7,\n    \"b46\": 0.65,\n    \"b66\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b49\": 0.6,\n    \"b37\": 0.55,\n    \"b56\": 0.55,\n    \"b64\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses performance and energy-efficiency challenges in server-side PHP web applications. It proposes specialized hardware accelerators for four major activities: hash table access, heap management, string manipulation, and regular expression processing. These proposals are based on an in-depth microarchitectural analysis, which finds that traditional optimization methods are not effective due to the flat execution profiles of PHP applications.\",\n  \"Direct Inspiration\": [\"b17\", \"b20\", \"b29\", \"b30\", \"b38\"],\n  \"Indirect Inspiration\": [\"b3\", \"b6\", \"b21\", \"b32\", \"b35\", \"b37\", \"b44\", \"b46\", \"b49\", \"b53\", \"b55\", \"b56\", \"b58\", \"b59\", \"b61\", \"b62\", \"b64\", \"b66\", \"b72\"],\n  \"Other Inspiration\": [\"b8\", \"b13\", \"b16\", \"b18\", \"b23\", \"b33\", \"b39\", \"b42\", \"b52\", \"b63\", \"b65\", \"b71\"]\n}\n```"], "58d82fced649053542fd7453": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in unsupervised learning, specifically in learning probability distributions on low-dimensional manifolds. It critiques classical methods like Kullback-Leibler divergence and introduces the Wasserstein GAN (WGAN) as a novel method to minimize the Earth Mover (EM) distance, offering theoretical and empirical benefits over traditional GANs.\",\n  \"Direct Inspiration\": [\"b3\", \"b0\"],\n  \"Indirect Inspiration\": [\"b8\", \"b16\"],\n  \"Other Inspiration\": [\"b22\", \"b5\", \"b15\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in this paper is the issue with unsupervised learning and how to effectively learn a probability distribution, especially when dealing with distributions supported by low-dimensional manifolds. The paper proposes using the Earth Mover (EM) distance and introduces the Wasserstein GAN (WGAN) as a solution to this problem, showing theoretical and empirical benefits over traditional GANs.\",\n  \"Direct Inspiration\": [\"b3\", \"b0\"],\n  \"Indirect Inspiration\": [\"b5\", \"b21\"],\n  \"Other Inspiration\": [\"b8\", \"b16\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised learning, specifically learning a probability distribution when dealing with distributions supported by low-dimensional manifolds. It introduces the Wasserstein Generative Adversarial Network (WGAN) to minimize the Earth Mover (EM) distance, which offers better theoretical properties for convergence and continuity compared to other distances like Jensen-Shannon (JS) and Kullback-Leibler (KL) divergences. The paper provides both theoretical analysis and empirical evidence to support the effectiveness of WGANs in mitigating common training issues in GANs, such as mode collapse and the need for careful balance in training the discriminator and generator.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b0\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b15\": 0.5,\n    \"b22\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised learning, specifically learning a probability distribution over low-dimensional manifolds using generative models. The paper proposes the Wasserstein Generative Adversarial Network (WGAN), which leverages the Earth Mover (EM) distance to offer more stable training and better sample quality compared to traditional GANs.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b8\": 0.9,\n    \"b0\": 0.85,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b10\": 0.7,\n    \"b21\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b22\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in unsupervised learning, particularly focusing on learning probability distributions confined to low-dimensional manifolds. It proposes the Wasserstein Generative Adversarial Network (WGAN), which minimizes the Earth Mover (EM) distance, providing theoretical and empirical evidence that WGANs offer more stable training and superior sample quality compared to traditional GANs.\",\n  \"Direct Inspiration\": [\"b8\", \"b3\", \"b16\"],\n  \"Indirect Inspiration\": [\"b21\", \"b10\"],\n  \"Other Inspiration\": [\"b15\", \"b5\"]\n}\n```"], "5eede0b791e0116a23aafdef": ["```json\n{\n    \"Summary\": \"The main challenge addressed in the paper is developing a self-attention mechanism specifically for 3D point cloud data that adheres to SE(3)-equivariance constraints, improving robustness to nuisance transformations and general performance. The proposed SE(3)-Transformer is designed to be invariant to global rotations and translations of its input, and equivariant to permutations of the input point labels.\",\n    \"Direct Inspiration\": {\n        \"b27\": 1,\n        \"b13\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b28\": 0.8,\n        \"b24\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b26\": 0.7,\n        \"b15\": 0.7,\n        \"b6\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in processing 3D point cloud data, emphasizing the need for neural structures that are invariant to global changes in input pose (translations and rotations). The authors propose the SE(3)-Transformer, a self-attention mechanism that enforces equivariance constraints to improve robustness and performance. The algorithm capitalizes on the symmetry properties of the task and introduces faster implementations of spherical harmonics.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.9,\n    \"b28\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b30\": 0.7,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing a self-attention mechanism for 3D point cloud data that is invariant to global transformations (rotation and translation) and adaptable to varying input sizes. The proposed SE(3)-Transformer aims to improve robustness and performance by adhering to equivariance constraints. Key contributions include a novel self-attention mechanism, resolving issues with angularly constrained filters in concurrent SE(3)-equivariant neural networks, and introducing a faster Pytorch implementation of spherical harmonics.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b13\", \"b24\", \"b27\", \"b28\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b15\", \"b18\", \"b26\", \"b29\", \"b30\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b9\", \"b11\", \"b14\", \"b21\", \"b32\", \"b36\", \"b38\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces the SE(3)-Transformer, a novel self-attention mechanism specifically designed for 3D point cloud data. The primary challenges addressed include adapting neural structures to varying numbers of points while maintaining invariance to global pose changes (3D rotations and translations). The authors propose the use of SE(3)-equivariance to achieve these goals, ensuring that transformations of the input result in equivalent transformations of the output.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b9\": 0.7,\n    \"b26\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of applying self-attention mechanisms to 3D point cloud data, specifically ensuring robustness to nuisance transformations and improving performance by adhering to equivariance constraints. The proposed SE(3)-Transformer is designed to be invariant to global rotations and translations and introduces a novel self-attention mechanism for 3D data, which is also equivariant to permutations of input point labels.\",\n  \"Direct Inspiration\": [\n    \"b24\",\n    \"b27\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b13\",\n    \"b26\",\n    \"b28\",\n    \"b36\",\n    \"b38\"\n  ],\n  \"Other Inspiration\": [\n    \"b15\"\n  ]\n}\n```"], "5d3ed2653a55ac61d998598b": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of hyperparameter tuning and training set size in self-training methods for language models, particularly BERT. It proposes RoBERTa, an improved training recipe that includes longer training, bigger batches, more data, removal of the next sentence prediction objective, and dynamic masking. The paper demonstrates the efficacy of these modifications through extensive experiments and establishes new state-of-the-art results on multiple benchmarks.\",\n    \"Direct Inspiration\": {\n        \"b11\": 1.0,\n        \"b56\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b25\": 0.7,\n        \"b40\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b35\": 0.5,\n        \"b39\": 0.5,\n        \"b52\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents a replication study of BERT pretraining, proposing an improved training recipe called RoBERTa. The primary challenges addressed include the computational expense of training, hyperparameter tuning, and dataset size effects. Key modifications proposed are training the model longer with larger batches, removing the next sentence prediction objective, training on longer sequences, and using dynamic masking patterns. A new dataset, CC-NEWS, is also introduced to control for training set size effects. The paper demonstrates that these improvements lead to better performance on benchmarks such as GLUE, SQuAD, RACE, SuperGLUE, and XNLI.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b56\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b39\": 0.7,\n    \"b25\": 0.6,\n    \"b40\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.4,\n    \"b52\": 0.3,\n    \"b41\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving BERT's pretraining by proposing RoBERTa, which includes several modifications such as training the model longer with larger batches, removing the next sentence prediction objective, training on longer sequences, and dynamically changing the masking pattern. The authors also introduce a new dataset, CC-NEWS, and demonstrate that their improvements lead to better performance on various benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b56\": 0.9,\n    \"b41\": 0.8,\n    \"b52\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b25\": 0.7,\n    \"b21\": 0.7,\n    \"b40\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.5,\n    \"b51\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include understanding the aspects of self-training methods like BERT that contribute to performance, addressing computational expense in training, and clarifying the effects of hyperparameter tuning and training set size. The algorithm proposed, RoBERTa, improves upon BERT by training longer with larger batches, removing the next sentence prediction objective, training on longer sequences, and dynamically changing the masking pattern. The paper is inspired by the need to improve BERT's performance and leverages a novel dataset, CC-NEWS, to achieve this.\",\n    \"Direct Inspiration\": {\n        \"b11\": 1.0,\n        \"b56\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b25\": 0.8,\n        \"b40\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b52\": 0.6,\n        \"b41\": 0.6,\n        \"b7\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in the training of BERT models, particularly the undertraining of BERT, and proposes an improved training recipe named RoBERTa. Key contributions include training with larger batches, removing the next sentence prediction objective, training on longer sequences, and dynamically changing the masking pattern. The paper also introduces a new dataset, CC-NEWS, and demonstrates that increasing data size improves performance on downstream tasks. The paper concludes that BERT's masked language model training objective remains competitive with recent alternatives.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b56\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b52\": 0.7,\n    \"b41\": 0.7,\n    \"b7\": 0.6,\n    \"b51\": 0.6,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5,\n    \"b31\": 0.5\n  }\n}\n```"], "5bdc315017c44a1f58a05e13": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge is determining the compression policy for each layer in deep neural networks to improve hardware efficiency while balancing model size, speed, and accuracy. The complexity of the problem increases with network depth, making manual heuristics suboptimal and time-consuming.\",\n    \"inspiration\": \"The paper proposes AutoML for Model Compression (AMC) to automate the compression process using reinforcement learning to efficiently sample the design space and improve model compression quality.\"\n  },\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b5\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b38\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b23\": 0.7,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"Challenges\": \"The primary challenges include improving hardware efficiency of neural networks while maintaining accuracy, determining compression policy for each layer, and handling the exponential complexity as networks grow deeper.\",\n        \"Inspirations\": \"The proposed AMC leverages reinforcement learning to automate model compression, inspired by the limitations of human heuristics and manual compression strategies.\"\n    },\n    \"Direct Inspiration\": {\n        \"b32\": 1,\n        \"b1\": 1,\n        \"b5\": 0.9,\n        \"b57\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.7,\n        \"b21\": 0.6,\n        \"b22\": 0.6,\n        \"b23\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.4,\n        \"b18\": 0.4,\n        \"b20\": 0.3,\n        \"b31\": 0.3,\n        \"b38\": 0.3\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatically finding the compression policy for deep neural networks to improve hardware efficiency. The proposed solution, AutoML for Model Compression (AMC), leverages reinforcement learning to efficiently explore the design space and improve model compression quality without human intervention.\",\n  \"Direct Inspiration\": [\n    \"b5\",\n    \"b32\",\n    \"b54\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b19\",\n    \"b21\",\n    \"b22\",\n    \"b31\",\n    \"b57\"\n  ],\n  \"Other Inspiration\": [\n    \"b1\",\n    \"b26\",\n    \"b46\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatically compressing deep neural networks to improve hardware efficiency. The proposed method, AutoML for Model Compression (AMC), uses reinforcement learning to automate the model compression process, aiming to outperform human-crafted strategies.\",\n  \"Direct Inspiration\": {\n    \"b57\": 0.9,\n    \"b5\": 0.85,\n    \"b1\": 0.8,\n    \"b32\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b22\": 0.65,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.55,\n    \"b19\": 0.5,\n    \"b18\": 0.45\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of model compression in deep neural networks, aiming to automatically find optimal compression policies using reinforcement learning to outperform manual strategies. The AMC engine leverages reinforcement learning to efficiently sample the design space, improving model compression quality and automating the process without human intervention.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b5\": 0.85,\n        \"b57\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b21\": 0.75,\n        \"b23\": 0.7,\n        \"b32\": 0.7,\n        \"b3\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b15\": 0.6,\n        \"b19\": 0.6,\n        \"b22\": 0.55,\n        \"b38\": 0.55\n    }\n}\n```"], "5aed14d117c44a4438158a8d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting deceptive behavior in real-life scenarios, particularly in the context of social media, cyberbullying, and fake news. It proposes a novel approach using neural networks and multi-modal features (textual, audio, visual, and micro-expressions) to improve the accuracy of deception detection. The proposed method aims to overcome the limitations of previous techniques that relied on constrained environments.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.95,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.75,\n    \"b17\": 0.7,\n    \"b18\": 0.7,\n    \"b21\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of detecting deception, particularly focusing on real-life scenarios such as courtroom trials and social media interactions. It proposes a novel approach using neural networks with multi-modal feature extraction, leveraging textual, audio, visual, and micro-expression features to improve deception detection accuracy.\",\n    \"Direct Inspiration\": {\n        \"b13\": 0.9,\n        \"b14\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.7,\n        \"b17\": 0.6,\n        \"b21\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.5,\n        \"b20\": 0.5,\n        \"b23\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include identifying deceptive behavior accurately in real-life scenarios, particularly within the context of social media, cyberbullying, and fake news. The authors propose a novel approach using neural models that leverage multi-modal features from videos, including visual, textual, audio, and micro-expression features, to detect deception more accurately.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b17\": 0.7,\n    \"b19\": 0.6,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b18\": 0.5,\n    \"b20\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting deception, particularly in real-life scenarios such as courtroom trials, cyberbullying, and the spread of fake news. It proposes the use of neural models that integrate multi-modal features (visual, textual, and audio) to improve the accuracy of deception detection. The paper highlights its novel contribution of using neural networks for deceit classification and emphasizes the importance of multi-modal feature extraction and data fusion techniques.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b14\": 0.9,\n    \"b15\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b20\": 0.8,\n    \"b21\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b22\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting deceptive behavior, which is critical in various contexts such as court trials, social media, and fake news. The paper proposes a novel approach using neural networks that leverage multi-modal features from textual, audio, visual, and micro-expression data for more accurate deception detection.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b17\": 0.7,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b23\": 0.4\n  }\n}\n```"], "5c0495fa17c44a2c747059aa": ["```json\n{\n  \"Summary\": \"The primary challenge the paper addresses is automating the stimuli generation for coverage-driven testing of RTL circuits using coverage-guided mutational fuzz testing. The proposed algorithm applies mutation algorithms from software testing to hardware, introduces MetaReset and Sparse Memories transformations for deterministic test execution, and defines Mux Toggle Coverage as a feedback metric.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b2\": 0.9,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b11\": 0.6,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automating Coverage Directed Test Generation (CDG) by leveraging coverage-guided mutational fuzz testing for FPGA-accelerated RTL simulation. It introduces methods for deterministic test execution, such as MetaReset and Sparse Memories, and proposes the Mux Toggle Coverage metric for effective fuzz testing feedback. The approach is empirically validated and made available as open-source software.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b8\": 0.9,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the automation of stimuli generation for Coverage Directed Test Generation (CDG) to maximize coverage with minimal manual intervention. The authors propose applying coverage-guided mutational fuzz testing, traditionally used in software testing, to the CDG problem for hardware designs, particularly using FPGA-accelerated RTL simulation. They introduce novel methods such as MetaReset transformation, Sparse Memories, and Mux Toggle Coverage to achieve deterministic test execution and effective coverage feedback.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b4\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is automating stimuli generation to maximize coverage with minimal manual intervention in dynamic verification of RTL circuit designs. The novel method proposed is applying coverage-guided mutational fuzz testing to the Coverage Directed Test Generation (CDG) problem, specifically tailored for FPGA-accelerated RTL simulation. Key innovations include defining test stimuli for direct mutation algorithm application, ensuring deterministic test execution with MetaReset transformation, and using Mux Toggle Coverage as feedback.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b16\": 1.0,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b11\": 0.6,\n    \"b4\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.4,\n    \"b12\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in this paper include automating the stimuli generation for coverage-directed test generation (CDG) to achieve high coverage with minimal manual intervention, and adapting coverage-guided mutational fuzz testing techniques for FPGA-accelerated RTL simulation. The paper proposes novel methods such as MetaReset for deterministic test execution and Sparse Memories for efficient memory reset, and defines Mux Toggle Coverage as a feedback metric.\",\n  \"Direct Inspiration\": [\"b2\", \"b16\", \"b8\"],\n  \"Indirect Inspiration\": [\"b11\", \"b4\", \"b7\"],\n  \"Other Inspiration\": [\"b12\"]\n}\n```"], "5550488245ce0a409eb6ef7f": ["```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is the trade-off between prefetch accuracy and the performance/energy overheads caused by inaccurate prefetches. The novel approach proposed by the author is the 'Prefetch Sandbox,' which evaluates different prefetch patterns in a simulated environment to identify the most beneficial ones without issuing real prefetch requests. This technique utilizes a Bloom filter for space efficiency and constant lookup time.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of mitigating the performance impact of long memory latencies in high-performance microprocessors through hardware prefetching techniques. It proposes a novel approach\u2014Sandbox Prefetching\u2014 which evaluates prefetch patterns in a 'sandbox' environment using Bloom filters before issuing them to the memory system. This technique aims to improve prefetch accuracy and overall performance without increasing memory bandwidth contention or cache capacity waste.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.9,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b4\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of prefetching in high performance microprocessors to mitigate the performance impact of long memory latencies. It introduces the Sandbox Prefetching technique, which uses a Bloom filter to evaluate the accuracy of candidate prefetch patterns in a sandbox environment before deploying them in the real memory hierarchy. This approach aims to improve prefetch accuracy while minimizing bandwidth and cache capacity concerns.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b20\": 0.85,\n    \"b11\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving prefetcher accuracy in microprocessors to mitigate long memory latencies. It introduces the 'Prefetch Sandbox' technique, which evaluates prefetch patterns in a sandbox environment using Bloom filters before deploying them in the actual memory hierarchy. This approach aims to balance prefetch accuracy with bandwidth and cache capacity concerns.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the trade-off between prefetch accuracy and coverage in hardware prefetchers. The proposed algorithm, Sandbox Prefetching, aims to address this by using a 'Prefetch Sandbox' to evaluate the accuracy of prefetch candidates without actually issuing prefetch requests to the memory system. This technique ensures that only accurate prefetches are issued, thereby improving performance and reducing unnecessary memory and cache usage.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```"], "573698426e3b12023e70bf13": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of hardware prefetching in modern high-performance processors, focusing on applications with diverse memory patterns. The proposed Best-Offset (BO) prefetcher aims to improve upon the Sandbox prefetcher by incorporating prefetch timeliness, leading to significant speedups on the SPEC CPU2006 benchmarks.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b25\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b10\", \"b21\", \"b4\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": []\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the need for an effective and timely hardware prefetching mechanism for high-performance processors, particularly when dealing with large application working sets that cannot fit in on-chip caches. The proposed Best-Offset (BO) prefetcher is an offset prefetcher that dynamically selects the prefetch offset while taking into account prefetch timeliness, aiming to improve upon the existing Sandbox prefetcher by Pugsley et al.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b21\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b1\": 0.5,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the limitations of existing hardware prefetchers in terms of dealing with diverse memory access patterns and prefetch timeliness. The proposed Best-Offset (BO) prefetcher introduces a new method for selecting the prefetch offset dynamically, taking into account prefetch timeliness, to improve performance over the existing Sandbox prefetcher.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.6,\n    \"b0\": 0.5,\n    \"b6\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing hardware prefetching in modern high-performance processors, introducing the Best-Offset (BO) prefetcher. The BO prefetcher builds on previous offset prefetching techniques but incorporates prefetch timeliness to improve performance, particularly on the SPEC CPU2006 benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b21\": 0.7,\n    \"b14\": 0.7,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are related to improving hardware prefetching in modern high-performance processors, especially for applications with diverse and complex memory access patterns. The proposed Best-Offset (BO) prefetcher aims to dynamically select the optimal prefetch offset while considering prefetch timeliness to achieve significant speedups over existing methods like the Sandbox Prefetcher (SBP).\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b14\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.6,\n    \"b33\": 0.6,\n    \"b30\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```"], "53e99b1bb7602d97023b2390": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the management of megabytes of off-chip correlation information required for prefetching irregular memory access patterns. The proposed algorithm, Irregular Stream Buffer (ISB), introduces a new structural address space that maps correlated physical addresses to consecutive structural addresses, reducing memory traffic and improving coverage and accuracy. Key benefits include improved prediction capability, training on the reference stream, and support for short streams.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b38\": 0.8,\n    \"b41\": 0.85,\n    \"b42\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b24\": 0.75,\n    \"b37\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.65,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of prefetching irregular memory access patterns, which are problematic due to the large storage needed for effective prefetching and the increased memory traffic and prediction latency caused by off-chip meta-data access. The proposed solution, the Irregular Stream Buffer (ISB), introduces a new structural address space to organize correlated memory addresses both temporally and spatially, thereby improving prediction capability, reducing memory traffic, and supporting short streams.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b27\", \"b41\", \"b42\", \"b38\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b19\", \"b24\", \"b37\", \"b11\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b25\", \"b31\", \"b32\", \"b39\", \"b10\", \"b3\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency of existing prefetching techniques for irregular memory access patterns, specifically addressing the issues related to the management of large off-chip correlation information and the high memory traffic overhead. The proposed solution, the Irregular Stream Buffer (ISB), combines PC localization and address correlation to improve prefetching accuracy and coverage while reducing memory traffic overhead.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b41\": 1.0,\n    \"b42\": 1.0,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of prefetching irregular memory access patterns in modern microprocessors, which is problematic due to the large storage required for effective address correlation. It introduces the Irregular Stream Buffer (ISB), a novel correlation-based prefetcher, which combines PC localization and address correlation to improve coverage and accuracy. The ISB employs a new caching scheme that reduces memory traffic overhead and synchronizes the movement of prefetcher meta-data with TLB misses.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b27\": 0.9,\n        \"b41\": 0.9,\n        \"b42\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b37\": 0.7,\n        \"b38\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.6,\n        \"b19\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the problem of prefetching for irregular access patterns, which involves managing large amounts of off-chip correlation information that increases prediction latency and memory traffic. The proposed solution, the Irregular Stream Buffer (ISB), introduces a new correlation-based prefetcher that combines PC localization and address correlation, reduces memory traffic, and improves coverage and accuracy through a novel structural address space.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b41\": 0.9,\n    \"b42\": 0.9,\n    \"b38\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.6,\n    \"b19\": 0.6,\n    \"b22\": 0.7,\n    \"b7\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b26\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```"], "5ee9f15b91e01152af022eaf": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating natural language descriptions from knowledge graphs (KGs) without relying on the assumption that all nodes are connected. It proposes Graformer, a Transformer-based encoder that uses shortest path lengths as relative position information to dynamically learn different structural views of the input graph. This model achieves competitive performance on the AGENDA and WebNLG benchmarks with fewer parameters compared to state-of-the-art models.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.9,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b10\": 0.7,\n    \"b38\": 0.6,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.5,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is generating natural language descriptions from knowledge graphs (KGs) without assuming graph connectivity or using label information to model graph structure. The novel approach introduced, Graformer, is a Transformer-based encoder that interprets shortest path lengths as relative position information in a graph self-attention network. Graformer achieves competitive performance on two KG-to-text generation benchmarks, AGENDA and WebNLG, while being more parameter-efficient and flexible in handling graph structures.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1,\n    \"b30\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.8,\n    \"b7\": 0.8,\n    \"b15\": 0.75,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.65,\n    \"b27\": 0.65,\n    \"b11\": 0.65,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the efficient generation of natural language descriptions from knowledge graphs (KGs), which are often disconnected. The proposed solution is Graformer, a novel Transformer-based encoder that interprets shortest path lengths as relative position information in a graph self-attention network. Graformer dynamically learns different structural views of the input graph using multi-head attention, achieving competitive performance on benchmark datasets with fewer parameters.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b38\": 0.9,\n    \"b7\": 0.9,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"Challenges\": \"The primary challenge addressed in the paper is generating natural language descriptions from knowledge graphs (KGs), particularly those that are not fully connected. The paper introduces Graformer, a novel Transformer-based encoder that interprets shortest path lengths as relative position information to dynamically learn different structural views of the input graph.\",\n        \"Inspirations\": \"The paper draws inspiration from previous works on graph-to-text generation using graph neural networks and Transformers, focusing on relative position embeddings and addressing the limitations of assuming full graph connectivity.\"\n    },\n    \"Direct Inspiration\": {\n        \"b33\": 1,\n        \"b30\": 0.9,\n        \"b15\": 0.8,\n        \"b10\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b38\": 0.7,\n        \"b7\": 0.7,\n        \"b23\": 0.6,\n        \"b14\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.5,\n        \"b16\": 0.5,\n        \"b34\": 0.5,\n        \"b29\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating natural language descriptions from knowledge graphs (KGs), proposing a novel Transformer-based encoder called Graformer. This encoder interprets the lengths of shortest paths as relative position information, allowing it to dynamically learn different structural views of the input graph. Graformer is evaluated on two benchmarks, AGENDA and WebNLG, achieving competitive performance with fewer parameters compared to state-of-the-art models.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.95,\n    \"b33\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.85,\n    \"b10\": 0.85,\n    \"b38\": 0.75,\n    \"b7\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.65,\n    \"b14\": 0.65\n  }\n}\n```"], "573698486e3b12023e711478": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning accurate graph representations that capture both local and global structural information in graphs. The authors propose a novel model, GraRep, which captures different k-step relational information using matrix factorization techniques, and integrates these representations to form a global representation. The model is designed to overcome limitations in existing methods such as DeepWalk and LINE, which fail to explicitly capture k-step information or only capture limited k-step information.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b19\": 0.9,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b26\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning graph representations that accurately capture both local and global structural information. The proposed model, GraRep, captures k-step relational information directly from transition matrices, avoiding complex sampling processes. The model introduces distinct loss functions for different k-step information and optimizes them using matrix factorization techniques to construct global vertex representations.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b19\": 0.8,\n    \"b24\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.6,\n    \"b10\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.4,\n    \"b14\": 0.3,\n    \"b3\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces GraRep, a novel model for learning graph representations that capture different k-step relational information among vertices. It addresses the limitations of previous models like DeepWalk and LINE by defining explicit loss functions for various k-step local relational information and optimizing through matrix factorization. The empirical effectiveness of GraRep is demonstrated through experiments on language network clustering, social network multi-label classification, and citation network visualization tasks.\",\n  \"Direct Inspiration\": [\"b17\", \"b19\", \"b24\"],\n  \"Indirect Inspiration\": [\"b15\"],\n  \"Other Inspiration\": [\"b4\", \"b14\", \"b16\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is learning accurate graph representations that capture global structural information, particularly the k-step relational information among vertices. The proposed GraRep model addresses this by using different loss functions for different k-step relationships and optimizing these through matrix factorization.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b19\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b14\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning accurate graph representations that capture different k-step relational information among vertices. It proposes a novel model called GraRep, which preserves different k-step relational information in distinct subspaces and optimizes the model using matrix factorization techniques.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b17\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b8\": 0.5\n  }\n}\n```"], "5b1642388fbcbf6e5a9b5740": ["```json\n{\n  \"Summary\": \"This paper addresses the challenge of learning structured sentence representations for text classification without relying on explicit structure annotations. The proposed reinforcement learning method identifies task-relevant structures using a policy network, integrating two structured representation models (ID-LSTM and HS-LSTM) with a classification network.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b24\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b20\": 0.55,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning structured representations for text classification without explicit structure annotations. It proposes a reinforcement learning (RL) method to identify task-relevant structures. The novel approach includes two models: Information Distilled LSTM (ID-LSTM) and Hierarchically Structured LSTM (HS-LSTM), which are integrated with a policy network and a classification network.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b3\": 0.9,\n    \"b24\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.6,\n    \"b14\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.4,\n    \"b22\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning structured sentence representations for text classification without explicit structure annotations. It proposes a reinforcement learning (RL) method to identify task-relevant structures by formulating structure discovery as a sequential decision problem, leveraging policy gradient methods. Two structured representation models, ID-LSTM and HS-LSTM, are designed and integrated with a policy network and a classification network.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b22\": 0.7,\n    \"b24\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is learning structured sentence representations for text classification without relying on explicit structure annotations. The authors propose a reinforcement learning (RL) method to identify task-relevant structures using a policy gradient method. They design two structured representation models: Information Distilled LSTM (ID-LSTM) and Hierarchical Structured LSTM (HS-LSTM), which are integrated with a policy network and a classification network.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b3\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b23\": 0.6,\n    \"b5\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.4,\n    \"b22\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning structured sentence representations for text classification without explicit structure annotations. It proposes a reinforcement learning (RL) method to discover task-relevant structures and improve classification performance through two models: Information Distilled LSTM (ID-LSTM) and Hierarchical Structured LSTM (HS-LSTM).\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b3\": 0.9,\n    \"b26\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b22\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b27\": 0.45\n  }\n}\n```"], "5edf5dd891e011bc656deb7d": ["```json\n{\n  \"Summary\": \"The paper discusses the challenges in improving the Transformer-based neural language models, particularly in handling the positional encoding of words. The proposed algorithm, DeBERTa, introduces a disentangled self-attention mechanism where each word is represented using two vectors to separately encode its content and position. This approach aims to address the limitations of existing methods by improving the efficiency and performance of pre-training and downstream tasks. The paper also extends this method to include absolute position encoding in the output layer to better predict masked tokens in certain contexts.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1.0,\n    \"b38\": 1.0,\n    \"b18\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b33\": 0.8,\n    \"b29\": 0.8,\n    \"b49\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b12\": 0.6,\n    \"b7\": 0.6,\n    \"b34\": 0.6,\n    \"b28\": 0.6,\n    \"b46\": 0.6,\n    \"b42\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces DeBERTa, a new Transformer-based neural language model that enhances BERT using a disentangled self-attention mechanism. The primary challenge addressed is the integration of content and relative positional information in attention mechanisms to improve language model performance. DeBERTa represents each word with separate vectors for content and position and introduces absolute positions back in the output layer to complement relative positions.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b18\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.6,\n    \"b29\": 0.6,\n    \"b49\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b44\": 0.5,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces DeBERTa, a Transformer-based neural language model, to address the challenges in natural language processing (NLP) tasks. The primary innovations are the disentangled self-attention mechanism that separates content and position embeddings, and the introduction of absolute positions in the output layer to complement relative positions. These innovations aim to improve the model\u2019s efficiency and performance on both natural language understanding (NLU) and natural language generation (NLG) tasks.\",\n    \"Direct Inspiration\": {\n        \"b33\": 1,\n        \"b3\": 1,\n        \"b10\": 1,\n        \"b29\": 1,\n        \"b49\": 1,\n        \"b12\": 1,\n        \"b34\": 1,\n        \"b18\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b42\": 0.8,\n        \"b28\": 0.8,\n        \"b46\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b27\": 0.7,\n        \"b32\": 0.7,\n        \"b16\": 0.7,\n        \"b19\": 0.7,\n        \"b38\": 0.7,\n        \"b9\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces DeBERTa, a new Transformer-based neural language model that proposes a disentangled self-attention mechanism. This mechanism separately encodes the content and position of each word using distinct vectors and computes attention weights based on these disentangled matrices. The paper highlights challenges in accurately predicting masked tokens based on relative positions alone and proposes incorporating absolute positions in the output layer as a solution. The novel contributions of DeBERTa are evaluated through comprehensive empirical studies on various NLP tasks, showing significant improvements over existing models like RoBERTa and BERT.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.95,\n    \"b18\": 0.9,\n    \"b38\": 0.85,\n    \"b49\": 0.9,\n    \"b44\": 0.9,\n    \"b33\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b29\": 0.8,\n    \"b7\": 0.75,\n    \"b34\": 0.75,\n    \"b28\": 0.75,\n    \"b46\": 0.75,\n    \"b42\": 0.75,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b27\": 0.65,\n    \"b32\": 0.65,\n    \"b19\": 0.65,\n    \"b16\": 0.65,\n    \"b24\": 0.65,\n    \"b13\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the limitations of the standard self-attention mechanism in Transformers, specifically their inability to fully capture the interactions between word contents and their relative positions. The proposed algorithm, DeBERTa, introduces a disentangled self-attention mechanism that represents each word using separate vectors for its content and position, and computes attention weights using disentangled matrices. Additionally, the algorithm enhances the output layer of BERT by incorporating absolute positions to complement relative positions, thereby improving predictions in masked language model tasks.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b38\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b44\": 0.8,\n    \"b33\": 0.7,\n    \"b49\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b29\": 0.55\n  }\n}\n```"], "5aed14d617c44a4438159341": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the efficient incorporation of relative position representations in the self-attention mechanism of the Transformer model, which is crucial for improving translation quality in machine translation tasks. The proposed algorithm extends the self-attention mechanism to consider pairwise relationships between input elements, modeling the input as a labeled, directed, fully-connected graph. This approach replaces absolute position encodings with relative position representations, demonstrating significant improvements in translation quality.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the integration of relative position representations in the self-attention mechanism of the Transformer model to improve translation quality. The main challenges are the efficient incorporation of relative positional information without increasing computational complexity and ensuring the model can generalize to sequence lengths not seen during training.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b5\": 0.7,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b4\": 0.5,\n    \"b7\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating relative position information into the self-attention mechanism of the Transformer model, which is crucial for tasks such as machine translation. The authors propose a novel method by extending self-attention to consider pairwise relationships between input elements, effectively modeling the input as a labeled, directed, fully-connected graph.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b5\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b7\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"Incorporating relative position representations in the self-attention mechanism of the Transformer model to improve translation quality.\",\n        \"inspirations\": \"Developing an efficient method for relative position encoding and extending self-attention to consider pairwise relationships between input elements.\"\n    },\n    \"Direct Inspiration\": {\n        \"b10\": 0.95,\n        \"b11\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.75,\n        \"b4\": 0.75,\n        \"b6\": 0.70\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.65,\n        \"b2\": 0.65,\n        \"b5\": 0.65,\n        \"b12\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating position information in the Transformer model's self-attention mechanism without using recurrence or convolution. The authors propose a novel method for integrating relative position representations, which improves translation quality in machine translation tasks.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b5\": 0.6,\n    \"b8\": 0.5\n  }\n}\n```"], "5fae6dced4150a363cec41f7": ["```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the scalability issues of payload-based intrusion detection systems (IDS) due to high-speed networks and increased traffic, and the high false alarm rates of anomaly-based techniques. The paper proposes using deep learning methods, specifically Autoencoder (AE) and Variational Autoencoder (VAE), combined with One-Class Support Vector Machine (OCSVM), to detect anomalous network traffic from flow-based data. The study highlights the superior performance of VAE-based anomaly detection systems compared to others.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b15\": 0.9,\n    \"b20\": 0.8,\n    \"b21\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b12\": 0.7,\n    \"b23\": 0.6,\n    \"b25\": 0.6,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b2\": 0.5,\n    \"b3\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.5,\n    \"b7\": 0.5,\n    \"b8\": 0.5,\n    \"b9\": 0.5,\n    \"b13\": 0.4,\n    \"b16\": 0.4,\n    \"b17\": 0.4,\n    \"b18\": 0.4,\n    \"b19\": 0.4,\n    \"b24\": 0.4,\n    \"b26\": 0.4,\n    \"b27\": 0.4,\n    \"b28\": 0.4,\n    \"b29\": 0.4,\n    \"b30\": 0.4,\n    \"b31\": 0.4,\n    \"b32\": 0.4,\n    \"b34\": 0.4,\n    \"b35\": 0.4,\n    \"b36\": 0.4,\n    \"b37\": 0.4,\n    \"b38\": 0.3,\n    \"b39\": 0.3,\n    \"b40\": 0.3,\n    \"b41\": 0.3,\n    \"b42\": 0.3,\n    \"b43\": 0.3,\n    \"b44\": 0.3,\n    \"b45\": 0.3,\n    \"b46\": 0.3,\n    \"b47\": 0.3,\n    \"b48\": 0.3,\n    \"b49\": 0.3,\n    \"b50\": 0.3,\n    \"b51\": 0.3,\n    \"b52\": 0.3,\n    \"b53\": 0.3,\n    \"b54\": 0.3,\n    \"b55\": 0.3,\n    \"b56\": 0.3,\n    \"b57\": 0.3,\n    \"b58\": 0.3,\n    \"b59\": 0.3,\n    \"b60\": 0.3,\n    \"b61\": 0.3,\n    \"b62\": 0.3,\n    \"b63\": 0.3,\n    \"b64\": 0.3,\n    \"b65\": 0.3,\n    \"b66\": 0.3,\n    \"b67\": 0.3,\n    \"b68\": 0.3,\n    \"b69\": 0.3,\n    \"b70\": 0.3,\n    \"b71\": 0.3,\n    \"b72\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting anomalous network traffic using flow-based data and deep learning methods, specifically Autoencoder and Variational Autoencoder (VAE). The key contribution is the application of deep learning methods for anomaly detection in flow-based data, which is less information-rich compared to payload-based data.\",\n  \"Direct Inspiration\": [\"b20\", \"b22\"],\n  \"Indirect Inspiration\": [\"b14\", \"b15\", \"b21\", \"b24\"],\n  \"Other Inspiration\": [\"b1\", \"b2\", \"b3\", \"b5\", \"b7\", \"b10\", \"b12\", \"b13\"]\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is the detection of anomalous network traffic using flow-based features with deep learning methods, specifically Autoencoder (AE) and Variational Autoencoder (VAE). The paper emphasizes the issues with traditional IDS, including scalability and false alarms, and proposes a semi-supervised learning approach to train AE and VAE for anomaly detection, showing that VAE performs better than others based on ROC and AUC results.\",\n  \"Direct Inspiration\": [\"b20\", \"b22\"],\n  \"Indirect Inspiration\": [\"b1\", \"b2\", \"b3\", \"b5\", \"b6\", \"b7\"],\n  \"Other Inspiration\": [\"b14\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting anomalous network traffic from flow-based data using deep learning methods, specifically Autoencoder (AE) and Variational Autoencoder (VAE). The primary contribution is the use of these unsupervised learning methods in a semi-supervised manner to detect unknown attacks, showcasing improved performance over previous methods.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b22\": 0.7,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b7\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting anomalous network traffic using flow-based features and deep learning methods, specifically Autoencoders (AE) and Variational Autoencoders (VAE), and employs a semi-supervised learning approach. The study aims to overcome the limitations of payload-based Intrusion Detection Systems (IDS) by focusing on flow-based data, and demonstrates that VAE-based anomaly detection performs better than other methods.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b3\": 0.75,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b12\": 0.65\n  }\n}\n```"], "53e9bb37b7602d97047778cc": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in analyzing computer network flows, with a focus on NetFlow-like systems, and proposes methodologies to enhance network monitoring, security, and data analysis using these systems. It emphasizes the importance of scalability, robustness, and the use of advanced techniques like machine learning for real-time analysis.\",\n  \"Direct Inspiration\": {\n    \"b61\": 0.9,\n    \"b99\": 0.85,\n    \"b121\": 0.8,\n    \"b126\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b64\": 0.65,\n    \"b113\": 0.6,\n    \"b119\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b127\": 0.55,\n    \"b67\": 0.5,\n    \"b85\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges discussed in the paper include the need for efficient network flow analysis for network monitoring, security, and traffic classification, while addressing issues such as high data volume and limited high-level information. The paper reviews various NetFlow-like applications and methodologies, focusing on recent advancements in network security and distributed system designs.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b64\": 0.85,\n    \"b156\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b99\": 0.75,\n    \"b121\": 0.7,\n    \"b27\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b61\": 0.6,\n    \"b31\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of understanding and analyzing network flow data to improve network security, monitoring, and management. The authors propose a survey of NetFlow-like applications, focusing on recent approaches and methodologies.\",\n  \"Direct Inspiration\": {\n    \"b61\": 0.9,\n    \"b31\": 0.8,\n    \"b126\": 0.7,\n    \"b75\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b113\": 0.6,\n    \"b79\": 0.6,\n    \"b67\": 0.6,\n    \"b117\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b85\": 0.5,\n    \"b91\": 0.5,\n    \"b27\": 0.5,\n    \"b152\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents a comprehensive survey of NetFlow-like applications, focusing on network flow analysis methods and their use in security, monitoring, and performance evaluation. It addresses the challenges of handling large data volumes, ensuring data accuracy, and the need for real-time analysis, and proposes the use of advanced techniques like machine learning and distributed systems for improved scalability and robustness.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b64\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b156\": 0.7,\n    \"b99\": 0.75,\n    \"b121\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b61\": 0.6,\n    \"b79\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in computer network flow analysis, focusing on NetFlow-like data for network security, monitoring, application classification, and user identity inference. It emphasizes recent trends in distributed systems and machine learning approaches, highlighting the need for scalable and robust analysis systems.\",\n  \"Direct Inspiration\": {\n    \"b61\": 0.95,\n    \"b75\": 0.9,\n    \"b67\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b64\": 0.8,\n    \"b126\": 0.75,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b79\": 0.65,\n    \"b99\": 0.6\n  }\n}\n```"], "5f50ba4291e01182e69239cb": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurate 3D object detection for autonomous driving systems using a late fusion approach to combine camera images and LiDAR point clouds. The proposed Camera-LiDAR Object Candidates Fusion (CLOCs) method aims to improve 3D detection accuracy by leveraging cross-modality information before Non-Maximum Suppression (NMS). The main contributions include a novel fusion architecture that uses geometric and semantic consistencies to associate detection candidates from different modalities.\",\n  \"Direct Inspiration\": {\n    \"b9\": \"Human annotators use both the camera images together with the LiDAR point clouds to create the ground truth bounding boxes.\",\n    \"b25\": \"MMF adopts continuous convolution to build dense LiDAR BEV feature maps and do point-wise feature fusion with dense image feature maps.\"\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": \"MV3D projects the raw point cloud into bird's eye view to form a multi-channel BEV image.\",\n    \"b11\": \"AVOD uses a deep fusion based 2D CNN to extract features from BEV images and front camera images for 3D bounding box regression.\",\n    \"b12\": \"Pointfusion exploits mature 2D detectors to generate 2D proposals and narrow down the 3D processing domain.\"\n  },\n  \"Other Inspiration\": {\n    \"b4\": \"Uses voxels to encode the raw point cloud, and 3D CNNs are applied to learn voxel features for classification and bounding box regression.\",\n    \"b6\": \"PointNets used in PointPillars' encoder to represent point clouds organized in vertical columns followed with a 2D CNN detection head.\"\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of 3D object detection in autonomous driving systems, emphasizing the limitations of single-modality methods and the potential of multi-modal sensor fusion. It proposes a late fusion approach, Camera-LiDAR Object Candidates Fusion (CLOCs), to improve accuracy by leveraging cross-modality information and combining detection candidates before Non-Maximum Suppression (NMS).\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0,\n        \"b5\": 1.0,\n        \"b6\": 1.0,\n        \"b7\": 1.0,\n        \"b8\": 1.0,\n        \"b10\": 1.0,\n        \"b11\": 1.0,\n        \"b12\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.9,\n        \"b13\": 0.9,\n        \"b19\": 0.9,\n        \"b20\": 0.9,\n        \"b21\": 0.9,\n        \"b22\": 0.9,\n        \"b23\": 0.9,\n        \"b24\": 0.9,\n        \"b25\": 0.9\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is achieving accurate 3D object detection for autonomous driving systems by leveraging multi-modal sensor fusion, specifically combining camera images and LiDAR point clouds. The algorithm proposed by the authors, Camera-LiDAR Object Candidates Fusion (CLOCs), aims to improve the accuracy of 3D object detection by using a late fusion approach. This method combines detection candidates from both modalities before Non-Maximum Suppression (NMS) to retain potential correct detections that single-modality methods might miss.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b11\": 1.0,\n    \"b12\": 1.0,\n    \"b13\": 0.9,\n    \"b25\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.85,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.85,\n    \"b9\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.7,\n    \"b24\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to improve the accuracy of 3D object detection in autonomous driving systems using multi-modal sensor fusion, specifically combining camera and LiDAR data. The proposed algorithm, Camera-LiDAR Object Candidates Fusion (CLOCs), aims to leverage cross-modality information before Non-Maximum Suppression (NMS) to keep detection candidates that would otherwise be mistakenly suppressed by single-modality methods. The paper focuses on late fusion due to its simplicity and effectiveness in combining pre-trained single-modality detectors.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b11\": 1.0,\n    \"b12\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b25\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.75,\n    \"b9\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving accurate 3D object detection for autonomous driving systems by proposing a novel late fusion method, termed Camera-LiDAR Object Candidates Fusion (CLOCs). The inspiration primarily stems from the limitations of single-modal methods and the potential of multi-modal sensor fusion to improve upon these methods.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b11\": 1.0,\n    \"b12\": 1.0,\n    \"b13\": 1.0,\n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b21\": 0.8,\n    \"b22\": 0.8\n  }\n}\n```"], "5ce937225ced2477cb328bd9": ["```json\n{\n  \"Summary\": \"The paper addresses key challenges in designing FPGA-based CNN accelerators, specifically focusing on improving multiply-and-accumulate (MAC) operations in convolution layers. It proposes a novel ABM-SpConv scheme which separates accumulation and multiplication stages to enhance arithmetic intensity for accumulation, thus transforming the design space to be accumulator-bound. The paper introduces a heterogeneous hardware architecture with separate arrays of accumulators and multipliers, along with several optimization schemes for efficient data-path utilization and low memory bandwidth requirements.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b6\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b11\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing an FPGA-based CNN accelerator that efficiently utilizes on-chip computing resources to speed up MAC operations in convolution and fully-connected layers. The proposed solution introduces ABM-SpConv, which separates multiplication and accumulation operations into distinct stages, allowing a higher arithmetic intensity for accumulation and reducing the demand for DSP units. A heterogeneous hardware architecture with separate arrays for accumulators and multipliers is designed to match the ABM-SpConv computation flow. The implementation shows significant improvements in inference throughput compared to existing state-of-the-art designs.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b6\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.8,\n    \"b1\": 0.75,\n    \"b3\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing FPGA-based CNN accelerators. It introduces a new sparse convolution scheme called ABM-SpConv, which separates accumulation and multiplication stages to maximize computational efficiency. The proposed hardware architecture comprises heterogeneous arrays of accumulators and multipliers, optimizing data-path utilization and ensuring high throughput and low memory bandwidth requirements.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a novel FPGA-based CNN inference accelerator architecture that separates multiplication and accumulation operations to improve efficiency, named ABM-SpConv. The main challenges addressed include the efficient use of on-chip resources, handling irregular sparsity patterns, and optimizing memory access.\",\n  \"Direct Inspiration\": [\"b2\", \"b6\"],\n  \"Indirect Inspiration\": [\"b3\", \"b11\", \"b12\"],\n  \"Other Inspiration\": [\"b5\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Designing an FPGA-based CNN accelerator that efficiently utilizes computational resources for high-throughput inference, particularly targeting the imbalance in workload due to irregular sparsity, different arithmetic intensities of operations, and random memory access patterns.\",\n    \"inspirations\": \"The primary inspirations are derived from the need to improve arithmetic intensity and computational efficiency, leveraging sparse convolution techniques and developing a heterogeneous architecture to address the new computation flow.\"\n  },\n  \"Direct Inspiration\": [\"b2\", \"b6\"],\n  \"Indirect Inspiration\": [\"b5\", \"b7\"],\n  \"Other Inspiration\": [\"b1\", \"b3\", \"b11\", \"b12\"]\n}\n```"], "5f69d1e09fced0a24bc32bb1": ["```json\n{\n  \"Summary\": \"The paper addresses the memory wall challenge in deep neural networks (DNNs) by integrating computing logic inside memory dies through processing-in-memory (PIM) architectures. The authors propose a communication-aware and reliable solution for ImageNet-level tasks using PIM circuits, involving a full-stack co-design methodology encompassing algorithms, circuits, and architectures. They introduce a learned step-size quantization scheme and an in-memory entry-counting macro for energy-efficient and precise DNN computation.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b16\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b14\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the memory wall challenge in deep neural networks (DNNs) by proposing a communication-aware and reliable processing-in-memory (PIM) architecture for ImageNet-level tasks. The proposed design methodology includes algorithm-level co-design using low-bitwidth quantization, circuit-level co-design with an entry-counting macro, and architecture-level co-design with a complete PIM-based DNN accelerator.\",\n  \"Direct Inspiration\": [\"b20\", \"b16\"],\n  \"Indirect Inspiration\": [\"b11\", \"b12\", \"b27\"],\n  \"Other Inspiration\": [\"b14\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the memory wall challenge in DNN acceleration by proposing a communication-aware and reliable PIM-based solution for ImageNet-level tasks. Key contributions include an algorithm-level co-design employing low-bitwidth quantization, a circuit-level co-design of an in-memory entry-counting MAC, and an architecture-level co-design of a complete DNN accelerator for ResNet.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b11\": 0.9,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b14\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the memory wall challenge in DNN computations by proposing a communication-aware and reliable PIM solution for ImageNet-level tasks. The authors introduce a comprehensive design methodology covering algorithms, circuits, and architectures, including a silicon prototype.\",\n    \"Direct Inspiration\": {\n        \"b16\": 1.0,\n        \"b20\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b14\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b26\": 0.6,\n        \"b27\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\"Memory wall in DNNs\", \"Accuracy limitations in PIM architectures\", \"Communication bandwidth sensitivity in DNNs\"],\n    \"Inspirations\": [\"High-efficiency computing architectures\", \"In-memory and near-memory computing\", \"Low-bitwidth quantization\"]\n  },\n  \"Direct Inspiration\": [\"b20\", \"b16\"],\n  \"Indirect Inspiration\": [\"b14\", \"b10\", \"b11\"],\n  \"Other Inspiration\": [\"b25\", \"b26\", \"b27\"]\n}\n```"], "5d04eeba8607575390f83f3b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving instruction cache (i-cache) performance in Warehouse-Scale Computers (WSC) without invasive hardware modifications. It proposes a scalable and efficient software-based prefetching algorithm to reduce i-cache misses by up to 96% with minimal overhead. The paper introduces the Assembly Database (AsmDB) to collect and analyze instruction-level data across thousands of production binaries, enabling both manual and automated optimizations for i-cache behavior.\",\n  \"Direct Inspiration\": {\n    \"b29\": 0.9,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b8\": 0.6,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge is to reduce instruction cache misses in Warehouse-Scale Computers (WSC) which leads to front-end stalls and unrealized performance gains.\",\n    \"inspirations\": \"The proposed solution leverages a software-based prefetching approach, utilizing the Assembly Database (AsmDB) for collecting and analyzing CPU cycle data to optimize i-cache behavior and improve performance.\"\n  },\n  \"Direct Inspiration\": {\n    \"b29\": 0.95,\n    \"b15\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b11\": 0.65,\n    \"b13\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b6\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenge of instruction cache misses in Warehouse-Scale Computers (WSCs) by developing a software-based prefetching method that avoids invasive hardware modifications. The authors introduce the Assembly Database (AsmDB), which allows for detailed analysis and optimization of instruction cache behavior across a large fleet of servers. The novel contributions include a profile-driven optimization technique for injecting prefetch instructions and various compiler optimizations to reduce code bloat and fragmentation.\",\n  \"Direct Inspiration\": {\n    \"b29\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.85,\n    \"b4\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.75,\n    \"b6\": 0.70,\n    \"b11\": 0.65,\n    \"b13\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the significant challenge of instruction cache misses in Warehouse-Scale Computers (WSCs) due to the large instruction footprints of WSC workloads. It proposes a software-based prefetching approach to improve instruction cache behavior without invasive hardware modifications. The paper introduces the Assembly Database (AsmDB) to analyze instruction and basic block-level data for performance optimization.\",\n    \"Direct Inspiration\": {\n        \"b4\": 0.95,\n        \"b29\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.85,\n        \"b15\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.75,\n        \"b25\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving instruction cache behavior in Warehouse-Scale Computers (WSCs) by developing scalable software-based prefetching techniques that avoid invasive hardware modifications. It proposes the Assembly Database (AsmDB) for detailed instruction-level profiling, enabling analysis and optimization of i-cache utilization across a broad range of workloads. The novel methods include feedback-driven code layout optimizations and profile-driven software prefetching.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.5,\n    \"b11\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```"], "5c45b4d03a55ac25e7f0f55c": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in large-scale graph processing on ReRAMs, specifically energy consumption and memory inefficiency due to the sparsity of graphs. The proposed GraphSAR algorithm focuses on processing-in-memory and sparsity-aware graph partitioning to improve performance and reduce energy consumption.\",\n  \"Direct Inspiration\": [\"b0\"],\n  \"Indirect Inspiration\": [\"b17\", \"b18\", \"b19\"],\n  \"Other Inspiration\": [\"b20\", \"b21\", \"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include heavy writing overheads and lack of parallelism in existing ReRAM-based graph processing systems. The proposed algorithm, GraphSAR, introduces a sparsity-aware processing-in-memory graph processing accelerator that reduces memory space waste, energy consumption, and latency. Key contributions are processing-in-memory graph processing, sparsity-aware graph processing, fewer subgraphs, and lower cell bits through a lightweight graph clustering method.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high energy consumption, latency, and memory space inefficiency in ReRAM-based graph processing systems. It introduces GraphSAR, a sparsity-aware processing-in-memory graph processing accelerator on ReRAMs, which directly processes edges in memory and divides subgraphs with low densities to reduce memory space waste, achieving significant improvements in energy efficiency and speed.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b20\": 0.6,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in energy-efficient and high-speed graph processing using ReRAMs, focusing on overcoming the heavy writing overheads and lack of parallelism in existing methods. The proposed GraphSAR introduces a sparsity-aware processing-in-memory graph processing accelerator that directly processes edges in memory and uses a hybrid-centric model to improve memory space efficiency and reduce energy consumption.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b18\": 0.9,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b20\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of heavy writing overhead and lack of parallelism in ReRAM-based graph processing systems, specifically GraphR. It proposes GraphSAR, a sparsity-aware processing-in-memory accelerator for graph processing on ReRAMs, which directly processes edges in memory and utilizes sparsity-aware graph partitioning and lightweight graph clustering to improve memory efficiency and reduce energy consumption.\",\n  \"Direct Inspiration\": [\"b0\", \"b18\"],\n  \"Indirect Inspiration\": [\"b17\"],\n  \"Other Inspiration\": [\"b19\", \"b20\", \"b21\", \"b22\"]\n}\n```"], "5ce2d184ced107d4c6438f01": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the incorporation of edge features in Graph Neural Networks (GNNs) and the adaptability of edge features across layers. The proposed Edge-enhanced Graph Neural Network (EGNN) addresses these challenges by introducing multi-dimensional edge features, doubly stochastic normalization, and attention-based edge adaptiveness across neural network layers. The novel methods include a new framework for exploiting multi-dimensional edge features, doubly stochastic edge normalization, and encoding edge directions as multi-dimensional edge features.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b18\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.5,\n    \"b14\": 0.5,\n    \"b19\": 0.5,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of incorporating multi-dimensional edge features and adaptive edge features across layers in Graph Neural Networks (GNNs). The proposed Edge-enhanced GNNs (EGNNs) enhance current GNN models like GCNs and GAT by utilizing multi-dimensional edge features and introducing doubly stochastic normalization.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b18\": 1,\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.9,\n    \"b13\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in graph neural networks (GNN) related to the inadequate incorporation of edge features and the use of potentially noisy adjacency matrices. The proposed Edge-enhanced Graph Neural Networks (EGNN) introduce novel methods such as multi-dimensional edge features, doubly stochastic normalization, and attention-based edge adaptiveness to improve graph learning tasks.\",\n  \"Direct Inspiration\": [\"b11\", \"b18\", \"b28\"],\n  \"Indirect Inspiration\": [\"b30\"],\n  \"Other Inspiration\": [\"b20\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in graph neural networks (GNNs) by proposing enhanced GNN models (EGNN) that incorporate multi-dimensional edge features. The key contributions include a new framework for using multi-dimensional edge features, doubly stochastic edge normalization, attention-based edge adaptiveness across layers, and methods for handling directed edges. The main challenges include the limitations of existing GNN models like GCNs and GATs in handling edge features fully and the inefficacy of using a fixed adjacency matrix across layers.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b18\": 1.0,\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b30\": 0.8,\n    \"b23\": 0.7,\n    \"b20\": 0.7,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively incorporating multi-dimensional edge features in graph neural networks (GNNs). The proposed solution, Edge-enhanced Graph Neural Networks (EGNNs), introduces a new framework that adapts edge features across layers, utilizes doubly stochastic normalization, and encodes directed edges to enhance traditional GCN and GAT models.\",\n  \"Direct Inspiration\": [\"b11\", \"b18\", \"b28\"],\n  \"Indirect Inspiration\": [\"b30\"],\n  \"Other Inspiration\": [\"b23\", \"b20\", \"b27\"]\n}\n```"], "5e2ac357df1a9c0c41e7fa48": ["```json\n{\n  \"Summary\": \"The paper primarily addresses the challenges of QSAR modelling in drug discovery by proposing novel neural network architectures, specifically augmentations to the Message Passing Neural Network (MPNN) framework, called Attention MPNN (AMPNN) and Edge Memory Neural Network (EMNN). The inspirations for these architectures stem from existing graph neural network models and the need to improve prediction accuracy and efficiency in cheminformatics.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b30\": 1.0,\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.9,\n    \"b29\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.8,\n    \"b43\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in QSAR modeling, particularly the need for improved efficiency in drug discovery through better prediction models. The authors propose augmentations to the Message Passing Neural Network (MPNN) architecture, introducing Attention MPNN (AMPNN) and Edge Memory Neural Network (EMNN) to enhance performance on QSAR tasks.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b16\": 0.9,\n    \"b26\": 0.85,\n    \"b29\": 0.95,\n    \"b37\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.75,\n    \"b30\": 0.8,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.65,\n    \"b15\": 0.65,\n    \"b28\": 0.6,\n    \"b32\": 0.6,\n    \"b41\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents novel augmentations to the Message Passing Neural Network (MPNN) architecture, specifically the Attention MPNN (AMPNN) and Edge Memory Neural Network (EMNN). These augmentations aim to improve the prediction accuracy in QSAR modelling, which is crucial for drug development. The paper also compares these new architectures against existing benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.9,\n    \"b43\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.6,\n    \"b26\": 0.7,\n    \"b29\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b16\": 0.5,\n    \"b27\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in QSAR studies aimed at improving drug discovery efficiency using novel deep learning techniques. It proposes the use of graph neural networks (GNNs) and message passing neural networks (MPNNs) to automatically learn molecular descriptors, circumventing traditional feature engineering.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b27\": 1,\n    \"b29\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.9,\n    \"b37\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b10\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving the efficiency of QSAR (Quantitative Structure Activity Relationships) modeling by introducing novel deep learning architectures, specifically Attention Message Passing Neural Network (AMPNN) and Edge Memory Neural Network (EMNN). These architectures aim to enhance the prediction accuracy of chemical properties and streamline the drug development process.\",\n    \"Direct Inspiration\": [\n        \"b26\",\n        \"b29\",\n        \"b30\",\n        \"b37\"\n    ],\n    \"Indirect Inspiration\": [\n        \"b16\",\n        \"b27\",\n        \"b43\"\n    ],\n    \"Other Inspiration\": [\n        \"b41\"\n    ]\n}\n```"], "5ec6544a9fced0a24bd69aa3": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the inappropriate use of datasets, hidden ligand bias, and improper dataset splitting in chemogenomics-based CPI modeling. The paper introduces TransformerCPI, a novel transformer neural network model, to address these issues with new datasets and rigorous label reversal experiments to evaluate the generalization and interpretation capabilities of CPI models.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b41\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.8,\n    \"b14\": 0.8,\n    \"b36\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in compound-protein interaction (CPI) prediction, particularly focusing on the limitations of conventional methods and pitfalls in current deep learning approaches. The authors propose a novel model, TransformerCPI, inspired by transformer architecture, to improve CPI modeling with new datasets and rigorous testing methods.\",\n  \"Direct Inspiration\": [\n    \"b31\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b28\",\n    \"b29\",\n    \"b32\",\n    \"b38\",\n    \"b41\"\n  ],\n  \"Other Inspiration\": [\n    \"b7\",\n    \"b24\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting compound-protein interactions (CPI) without the need for protein 3D structures or large ligand datasets. It proposes a novel Transformer-based method (TransformerCPI) to improve the accuracy and generalization of CPI predictions. The paper also highlights issues such as hidden ligand bias and inappropriate dataset use, which can mislead CPI models.\",\n  \"Direct Inspiration\": [\"b31\"],\n  \"Indirect Inspiration\": [\"b2\", \"b28\", \"b29\", \"b38\"],\n  \"Other Inspiration\": [\"b32\", \"b33\", \"b7\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in predicting compound-protein interactions (CPI) without relying on protein 3D structures and the issues associated with current deep learning-based CPI models, such as hidden ligand bias and inappropriate dataset splitting. The authors propose a novel transformer-based neural network, TransformerCPI, to tackle these issues, achieving better performance and interpretability.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b38\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b28\": 0.75,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.65,\n    \"b19\": 0.6,\n    \"b17\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in compound-protein interaction (CPI) prediction, specifically the use of inappropriate datasets, hidden ligand bias, and improper data splitting. The authors propose a novel Transformer-based model, TransformerCPI, to tackle these challenges and achieve better performance and interpretability.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b41\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b28\": 0.7,\n    \"b38\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b20\": 0.4,\n    \"b26\": 0.4\n  }\n}\n```"], "5f86cae991e011dbc7eba2fa": ["```json\n{\n  \"Summary\": \"The paper introduces the Programmable Integrated Unified Memory Architecture (PIUMA) developed for the DARPA HIVE program, addressing the challenges of graph analytics due to irregular memory accesses, synchronization requirements, and large data sets. The PIUMA machine focuses on efficient memory utilization, synchronization, and remote memory access to achieve high performance and energy efficiency in processing massive graph datasets.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b3\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.85,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of performing graph analytics on existing computing architectures, particularly focusing on issues like cache and bandwidth utilization, irregular computation and memory intensity, synchronization, and handling massive datasets. The proposed solution is the Programmable Integrated Unified Memory Architecture (PIUMA) designed to improve performance and efficiency in graph processing.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.95,\n        \"b5\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b4\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.6,\n        \"b1\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of graph analytics on existing processor architectures, focusing on issues such as cache and bandwidth utilization, irregular computation and memory intensity, synchronization, and handling massive datasets. It introduces the Programmable Integrated Unified Memory Architecture (PIUMA) developed for the DARPA HIVE program to improve performance per watt for graph processing at massive scales.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b4\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of graph analytics on existing processor architectures, mainly focusing on cache and bandwidth utilization, irregular computation and memory intensity, synchronization, and handling massive datasets. It introduces the Programmable Integrated Unified Memory Architecture (PIUMA) designed for high-performance graph processing by optimizing network, memory, and compute architectures.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b1\": 0.5,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper revolve around the inefficiencies in handling sparse and irregular memory accesses inherent in graph analytics. These challenges include poor cache and bandwidth utilization, irregular computation and memory intensity, fine-and coarse-grained synchronization, and the management of massive datasets. The paper introduces the PIUMA architecture, specifically designed to address these issues by providing a high-performance, energy-efficient solution for large-scale graph processing.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.8,\n    \"b3\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7\n  }\n}\n```"], "5f576c1591e011f4c3d5dd7e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing Graph Neural Networks (GNNs), which often face issues with unstable optimization and slow convergence. It proposes a novel normalization method called Graph Normalization (GraphNorm), which normalizes feature values across all nodes in each graph with a learnable shift, improving the optimization and generalization performance of GNNs.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b11\": 0.7,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b12\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing Graph Neural Networks (GNNs), particularly focusing on the instability and slow convergence of existing methods. It introduces a novel normalization method called GraphNorm, which normalizes feature values across nodes in individual graphs with a learnable shift to improve optimization efficiency and generalization performance.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b36\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b12\": 0.6,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the optimization and convergence speed of Graph Neural Networks (GNNs) by introducing a novel normalization method called GraphNorm. This method normalizes the feature values across all nodes in each individual graph with a learnable shift, which avoids expressiveness degradation and enhances optimization efficiency.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1.0,\n    \"b15\": 0.9,\n    \"b3\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b6\": 0.6,\n    \"b12\": 0.6,\n    \"b0\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b33\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge in the paper is the unstable optimization and slow convergence of Graph Neural Networks (GNNs). To tackle this challenge, the authors propose a novel normalization method called GraphNorm, which normalizes feature values across nodes within individual graphs using a learnable shift. This method aims to improve optimization stability and convergence speed, and enhance generalization performance.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b36\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b35\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b12\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing Graph Neural Networks (GNNs), which often face unstable optimization and slow convergence. The authors propose a novel normalization method called GraphNorm, designed specifically for GNNs. This method applies normalization to each individual graph rather than across a batch, which is shown to smooth the optimization curvature and improve training efficiency and generalization performance.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.95,\n    \"b36\": 0.9,\n    \"b35\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b3\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b12\": 0.65\n  }\n}\n```"], "53e99cb5b7602d970256b3bb": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Achieving performance that scales with the number of cores in CMPs.\",\n      \"Providing explicit control over communication to optimize software.\",\n      \"Developing a light-weight threading and synchronization model.\"\n    ],\n    \"algorithm\": \"Mamba architecture with presence bits for synchronization and explicit communication control.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b6\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b7\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b8\", \"b9\", \"b10\", \"b11\", \"b12\", \"b13\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing efficient chip multiprocessors (CMP) by proposing the Mamba architecture. Mamba provides explicit control over communication, a light-weight threading, and synchronization model to improve performance and scalability with increasing core counts. The architecture includes features like presence bits for synchronization, and a round-robin scheduler for hardware threading.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.95,\n    \"b7\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b2\": 0.75,\n    \"b3\": 0.70,\n    \"b4\": 0.65,\n    \"b5\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.60,\n    \"b9\": 0.60,\n    \"b12\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of optimizing communication and synchronization in Chip Multiprocessor (CMP) systems, proposing the Mamba architecture which provides explicit control over communication and a lightweight threading and synchronization model.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1,\n    \"b3\": 1,\n    \"b6\": 1,\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the need for scalable performance with increasing core counts and efficient synchronization in multiprocessor systems. The proposed Mamba architecture addresses these challenges by giving explicit control over communication, providing a light-weight threading and synchronization model, and utilizing presence bits for synchronization and notification.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b3\": 0.7,\n    \"b4\": 0.6,\n    \"b5\": 0.7,\n    \"b7\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b9\": 0.5,\n    \"b10\": 0.4,\n    \"b11\": 0.4,\n    \"b12\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces Mamba, an architecture designed to exploit thread-level parallelism (TLP) along with instruction-level parallelism (ILP). It addresses the challenges of scaling performance with increasing core counts by providing explicit control over communication and a lightweight threading and synchronization model. The Mamba architecture includes presence bits for synchronization and a hardware scheduler to efficiently manage multiple threads.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.9,\n    \"b3\": 0.9,\n    \"b6\": 0.95,\n    \"b7\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7\n  }\n}\n```"], "5f2bd70491e011b36ba9ce89": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of procedural content generation (PCG) for game levels, specifically focusing on creating new levels from limited training data using TOAD-GAN. The main inspiration for this work is the SinGAN architecture, which learns a generative model from a single image. TOAD-GAN adapts this approach for 2D token maps in video games, such as Super Mario Bros., and introduces a downsampling algorithm to preserve important tokens.\",\n  \"Direct Inspiration\": [\"b15\"],\n  \"Indirect Inspiration\": [\"b4\", \"b17\", \"b21\"],\n  \"Other Inspiration\": [\"b0\", \"b7\", \"b9\", \"b8\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in procedural content generation for game levels, focusing on the scarcity of training data and the lack of high-level control in existing methods. TOAD-GAN is introduced as a novel generative model inspired by SinGAN, designed specifically for generating video game levels from a single example.\",\n  \"Direct Inspiration\": [\"b15\"],\n  \"Indirect Inspiration\": [\"b4\", \"b17\", \"b21\", \"b20\", \"b22\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the limited amount of training data available for Procedural Content Generation via Machine Learning (PCGML) for video game level design. The proposed algorithm, TOAD-GAN, aims to generate new game levels from very little training data by leveraging a Generative Adversarial Network (GAN) architecture inspired by SinGAN.\",\n    \"Direct Inspiration\": [\"b15\"],\n    \"Indirect Inspiration\": [\"b4\", \"b21\", \"b22\"],\n    \"Other Inspiration\": [\"b17\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating video game levels, specifically Super Mario Bros. (SMB) levels, using a limited amount of training data. The proposed method, TOAD-GAN, is inspired by SinGAN and aims to generate levels with a coherent style from a single example level, preserving important tokens and allowing for high-level control of the generated content.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b11\": 0.7,\n    \"b17\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating video game levels, specifically for Super Mario Bros. (SMB), with limited training data through a new generative model named TOAD-GAN. TOAD-GAN is inspired by SinGAN and aims to overcome limitations in existing Procedural Content Generation via Machine Learning (PCGML) methods, such as lack of high-level control and coherence in generated levels.\",\n  \"Direct Inspiration\": [\"b15\"],\n  \"Indirect Inspiration\": [\"b21\", \"b20\", \"b22\", \"b4\"],\n  \"Other Inspiration\": [\"b17\", \"b8\"]\n}\n```"], "53e9bafbb7602d9704734d1d": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of branch predictor warmup in architectural simulations, particularly in sampled simulations, where ensuring an accurate hardware state at the beginning of each sampling unit is crucial. The proposed solution, Branch History Matching (BHM), aims to improve the accuracy of branch predictor warmup by inspecting pre-sampling units for branch instances with similar global and local histories. The paper demonstrates the effectiveness of BHM in reducing warmup length and improving accuracy compared to fixed-length warmup and MRRL.\",\n    \"Direct Inspiration\": {\n        \"b4\": 0.9,\n        \"b6\": 0.8,\n        \"b8\": 0.8,\n        \"b9\": 0.95\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.7,\n        \"b20\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.6,\n        \"b1\": 0.6,\n        \"b2\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of branch predictor warmup in architectural simulations, proposing a novel Branch History Matching (BHM) method to improve accuracy and efficiency in simulations. BHM adjusts warmup lengths based on the locality of branch executions, aiming to reduce simulation time and disk space requirements.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b4\": 0.8,\n    \"b6\": 0.8,\n    \"b8\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurate branch predictor warmup in sampled simulation, proposing a novel Branch History Matching (BHM) method that improves accuracy and reduces warmup length. The method leverages global and local branch histories to create BHM distributions, which guide the allocation of warmup budgets across sampling units. This approach is microarchitecture-independent and aims to optimize simulation efficiency and accuracy.\",\n  \"Direct Inspiration\": [\"b0\", \"b9\"],\n  \"Indirect Inspiration\": [\"b1\", \"b4\", \"b6\", \"b8\", \"b16\"],\n  \"Other Inspiration\": [\"b2\", \"b5\", \"b7\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of warming up branch predictors in architectural simulations, particularly during sampled simulations. It proposes a novel method called Branch History Matching (BHM) to tackle the cold-start problem by examining branch histories and distributing warmup lengths based on the BHM distribution.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b9\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b16\", \"b0\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b6\", \"b4\", \"b8\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately simulating architectural designs, particularly focusing on the cold-start problem in branch predictor warmup during sampled simulations. The proposed Branch History Matching (BHM) method inspects pre-sampling units to identify similar branch instances and determines the appropriate warmup length based on a distribution of these instances, aiming to improve simulation accuracy and efficiency.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b9\"],\n    \"confidence_score\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b6\", \"b4\", \"b8\"],\n    \"confidence_score\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b2\", \"b1\", \"b16\"],\n    \"confidence_score\": 0.6\n  }\n}\n```"], "53e9b38fb7602d9703e70871": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in microprocessor design related to energy, power, and thermal management. The authors propose a method to identify stress patterns in typical workloads using representative sampling with threshold clustering, which is more effective than statistical sampling and k-means clustering for detecting extreme workload behaviors.\",\n  \"Direct Inspiration\": [\"b21\", \"b28\"],\n  \"Indirect Inspiration\": [\"b10\", \"b24\", \"b29\"],\n  \"Other Inspiration\": [\"b0\", \"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying stress patterns in typical workloads to improve energy, power, and thermal management in microprocessors. It proposes a novel method using representative sampling with threshold clustering, which is shown to be more effective than traditional statistical sampling and k-means clustering for identifying extreme behaviors in microprocessor workloads. The study demonstrates that this method can achieve significant simulation speedup while maintaining high accuracy in estimating stress patterns.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b29\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b10\": 0.65,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying stress patterns in typical workloads for microprocessors to optimize dynamic thermal management (DTM) techniques. It highlights the limitations of average workload behavior and hand-tuned stressmarks and proposes a novel approach using representative sampling with threshold clustering to find stress patterns with extreme workload behavior, achieving significant simulation speedup with minimal error.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b0\": 0.4,\n    \"b22\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying stress patterns in typical workloads with minimal simulation time, bridging the gap between average workload behavior and extreme stress behaviors. It introduces the use of representative sampling with threshold clustering as a more effective method for identifying extreme workload behaviors compared to statistical sampling and k-means clustering.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b28\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses significant challenges in microprocessor design, particularly in managing energy, power, thermal hotspots, and voltage variation. It proposes an approach using sampled simulation theory to identify stress patterns in typical workloads, focusing on representative sampling with threshold clustering. The paper demonstrates that this method is more effective in identifying extreme workload behaviors compared to statistical sampling and k-means clustering.\",\n    \"Direct Inspiration\": {\n        \"b21\": 0.95,\n        \"b28\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b24\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.7,\n        \"b29\": 0.65\n    }\n}\n```"], "5e2d653a3a55acc8374367eb": ["```json\n{\n  \"Summary\": \"The paper addresses privacy concerns in recommendation systems, particularly focusing on preventing private-attribute inference attacks while maintaining high utility in recommendations. It proposes a new framework called RAP (Recommender with Attribute Protection) that leverages adversarial learning through a Bayesian personalized ranking recommender and a private-attribute inference attacker, aiming to protect users' private attributes without compromising the quality of recommendations.\",\n  \"Direct Inspiration\": [\"b18\", \"b41\"],\n  \"Indirect Inspiration\": [\"b22\", \"b23\", \"b42\"],\n  \"Other Inspiration\": [\"b31\", \"b32\", \"b37\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of private-attribute inference attacks in recommendation systems. The proposed algorithm, RAP (Recommender with Attribute Protection), aims to recommend relevant items to users while concurrently protecting their private attribute information from being inferred by malicious attackers. The approach employs adversarial learning to optimize two conflicting objectives: maintaining recommendation utility and preventing private attribute leakage.\",\n  \"Direct Inspiration\": {\n    \"b41\": 0.9,\n    \"b12\": 0.8,\n    \"b34\": 0.8,\n    \"b18\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.75,\n    \"b32\": 0.75,\n    \"b23\": 0.7,\n    \"b42\": 0.7,\n    \"b10\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b31\": 0.6,\n    \"b37\": 0.6,\n    \"b44\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge in the paper is to develop a recommendation system that prevents private-attribute inference attacks while retaining high utility for users. The proposed solution, RAP (Recommendation with Attribute Protection), employs adversarial learning between a Bayesian personalized recommender and a private-attribute inference attacker to achieve privacy protection and recommendation relevance.\",\n  \"Direct Inspiration\": [\"b18\", \"b41\"],\n  \"Indirect Inspiration\": [\"b22\", \"b31\", \"b32\", \"b37\", \"b40\", \"b42\", \"b44\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are addressing the privacy concerns in recommendation systems, specifically focusing on private-attribute inference attacks. The proposed algorithm, RAP (Recommender with Attribute Protection), integrates adversarial learning to balance personalization and privacy by using a Bayesian personalized ranking recommender and a private-attribute inference attacker to prevent leakage of user private attributes while maintaining high utility.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.95,\n    \"b41\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.85,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.75,\n    \"b31\": 0.7,\n    \"b37\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed in the paper is protecting users' private attribute information from being inferred by adversaries while maintaining high-quality recommendations in recommendation systems.\",\n    \"inspirations\": \"The paper is inspired by adversarial learning techniques and aims to concurrently optimize recommendation utility and privacy protection.\"\n  },\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b41\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b42\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.5,\n    \"b34\": 0.5,\n    \"b44\": 0.5\n  }\n}\n```"], "599c797a601a182cd2641eda": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the miscalibration of modern neural networks, despite their improved accuracy. The authors propose various calibration methods, including a straightforward technique called temperature scaling, which they find to be surprisingly effective.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1.0,\n    \"b40\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b36\": 0.8,\n    \"b50\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b52\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of miscalibration in modern neural networks despite their high classification accuracy. It identifies the causes of miscalibration, proposes temperature scaling as a solution, and compares it with other calibration methods.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1.0,\n    \"b40\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9,\n    \"b36\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.75,\n    \"b20\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the miscalibration of modern neural networks, where the predicted confidence scores do not accurately represent the true probabilities. The authors propose several calibration methods, with a particular emphasis on temperature scaling, a simple and effective method for obtaining calibrated probabilities.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1,\n    \"b38\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b52\": 0.6,\n    \"b41\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of miscalibration in modern neural networks, which have increased in accuracy but often fail to produce reliable confidence estimates. It proposes temperature scaling as a straightforward and effective method for calibrating probabilistic outputs of neural networks.\",\n    \"Direct Inspiration\": {\n        \"b40\": 1.0,\n        \"b38\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.8,\n        \"b36\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.6,\n        \"b50\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of neural network miscalibration, which is the discrepancy between confidence scores and actual accuracy. It aims to understand the causes of this issue and proposes methods to alleviate it. The primary contribution is the introduction of temperature scaling as an effective post-processing calibration method.\",\n  \"Direct Inspiration\": {\n    \"b38\": 1.0,\n    \"b40\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9,\n    \"b36\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.7,\n    \"b50\": 0.7\n  }\n}\n```"], "5db9294247c8f766461f1f4a": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of vision-and-language reasoning, focusing on visual content understanding, language semantics, and cross-modal alignments. It proposes the LXMERT framework, which leverages a cross-modality encoder architecture to improve the interaction between vision and language. The model is pre-trained on various tasks to enhance its ability to understand and align visual and linguistic information, achieving state-of-the-art results in multiple datasets.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b33\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b26\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of vision-and-language reasoning by creating a cross-modality framework named LXMERT, which learns interactions between vision and language. The model is inspired by BERT-style architectures and incorporates multi-modality pre-training tasks to enhance performance on vision-and-language tasks.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b5\", \"b33\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b0\", \"b2\", \"b36\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b26\", \"b30\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of understanding and aligning visual content and language semantics in a cross-modality framework. The proposed algorithm, LXMERT, is a pre-trained vision-and-language model that uses three Transformer encoders to learn interactions between visual and language inputs. The model is pre-trained on five tasks to build both intra-modality and cross-modality relationships and is evaluated on multiple datasets, showing state-of-the-art performance.\",\n    \"Direct Inspiration\": {\n        \"b5\": 0.95,\n        \"b33\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.85,\n        \"b2\": 0.75,\n        \"b36\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.7,\n        \"b13\": 0.7,\n        \"b30\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of vision-and-language reasoning by developing a pre-trained cross-modality framework called LXMERT. This framework focuses on learning interactions between visual content and language semantics using a combination of self-attention and cross-attention layers. The model is pre-trained with tasks across both modalities and evaluated on datasets to demonstrate its effectiveness and generalizability.\",\n  \"Direct Inspiration\": {\n    \"Inspired by\": [\"b5\"],\n    \"Motivated by\": [\"b33\"]\n  },\n  \"Indirect Inspiration\": {\n    \"Following.. we adopt ... to solve the challenge/problem\": [\"b0\"],\n    \"We use... based on to achieve...\": [\"b26\"]\n  },\n  \"Other Inspiration\": [\"b1\", \"b13\", \"b30\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the underdevelopment of large-scale pretraining and finetuning studies for the modality-pair of vision and language. The authors propose the LXMERT framework, which focuses on learning vision-and-language interactions using a combination of self-attention and cross-attention layers. The model is pre-trained with five diverse tasks to better learn cross-modal alignments.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b33\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b13\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```"], "5e982cc591e0119e8a952209": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of learning cross-modal representations for vision-language tasks by introducing a novel Vision-Language Pretraining (VLP) method called Oscar. The primary challenges are the ambiguity in visual region features and the lack of explicit alignment between image regions and text. Oscar improves the learning of semantic alignments by using object tags detected in images as anchor points. The proposed method outperforms existing approaches in multiple vision-language benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b47\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.9,\n    \"b21\": 0.8,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b3\": 0.5,\n    \"b43\": 0.4,\n    \"b44\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in learning cross-modal representations for vision-language (V+L) tasks by proposing a new Vision-Language Pretraining (VLP) method called Oscar. The main challenges include ambiguity in visual region features and lack of grounding between image regions and text. The proposed method introduces object tags detected in images as anchor points to improve semantic alignments between images and texts, thereby addressing these challenges.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b47\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b21\": 0.8,\n    \"b43\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b3\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the ambiguity of visual region features due to over-sampling and the lack of explicit alignment information between image regions and text. The proposed algorithm, Oscar, introduces object tags as anchor points to improve the learning of semantic alignments between images and texts. The novel approach includes representing input pairs as triples (word sequence, object tags, and image region features) and using both masked token loss and contrastive loss for pre-training.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b47\": 0.9,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b5\": 0.65,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in vision-language pretraining (VLP) related to semantic alignment between image regions and text, and proposes a new method called Oscar. This method introduces object tags detected in images as anchor points to improve the learning of cross-modal representations. Oscar represents input image-text pairs as triples consisting of word sequences, object tags, and image region features. The novel approach aims to ease alignment learning and improve performance on various V+L tasks.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b47\": 0.9,\n    \"b5\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b19\": 0.75,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.65,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning cross-modal representations for vision-language tasks. It introduces Oscar, a method that uses object tags as anchor points to improve semantic alignments between images and texts. This approach aims to tackle issues such as ambiguity and lack of grounding in existing vision-language pretraining (VLP) methods.\",\n  \"Direct Inspiration\": {\n    \"b29\": 0.9,\n    \"b47\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b19\": 0.6,\n    \"b5\": 0.6,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b3\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```"], "5d04eeba8607575390f83f4d": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of balancing prefetcher coverage and accuracy in memory systems, particularly in the context of multicore processors. The authors propose Perceptron-based Prefetch Filtering (PPF), which enhances existing prefetchers by filtering out inaccurate prefetches to improve performance.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.8,\n        \"b20\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b4\": 0.6,\n        \"b21\": 0.6,\n        \"b22\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of balancing coverage and accuracy in prefetching algorithms, specifically in the context of aggressive prefetchers that can harm performance by wasting bandwidth and causing cache pollution. The authors propose Perceptron-based Prefetch Filtering (PPF) as an enhancement to existing state-of-the-art prefetchers, allowing them to speculate deeply to achieve high coverage while filtering out inaccurate prefetches using an online-trained neural model. The key contributions are an on-line neural model for hardware data prefetching, significant performance improvement by implementing PPF, and a methodology for determining an appropriate set of features for prediction.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b4\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.7,\n        \"b19\": 0.6,\n        \"b20\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"Challenges\": \"Balancing coverage and accuracy in prefetching to avoid bandwidth wastage and cache pollution, particularly in multicore systems.\",\n        \"Algorithm\": \"Perceptron-based Prefetch Filtering (PPF) to enhance existing prefetchers by filtering out inaccurate prefetches, thereby increasing both coverage and accuracy.\"\n    },\n    \"Direct Inspiration\": {\n        \"b1\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.8,\n        \"b20\": 0.8,\n        \"b21\": 0.7,\n        \"b22\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.5,\n        \"b2\": 0.5,\n        \"b3\": 0.6,\n        \"b4\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of balancing prefetcher coverage and accuracy to optimize performance in memory systems. It proposes the Perceptron-based Prefetch Filtering (PPF) as an enhancement to existing prefetchers like the Signature Path Prefetcher (SPP). PPF uses an online-trained neural model to filter out inaccurate prefetches, thereby improving both coverage and accuracy.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b19\": 0.9,\n        \"b20\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.7,\n        \"b4\": 0.7,\n        \"b21\": 0.6,\n        \"b22\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge is finding a balance between prefetching coverage and accuracy to minimize wasted bandwidth and performance degradation.\",\n    \"inspirations\": \"The paper introduces Perceptron-based Prefetch Filtering (PPF) to enhance state-of-the-art prefetchers by filtering out inaccurate prefetches, thereby improving both coverage and accuracy.\"\n  },\n  \"Direct Inspiration\": {\n    \"b1\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.65\n  }\n}\n```"], "53e9aca8b7602d97036868cf": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving branch prediction accuracy in modern computer architectures, which rely on speculation to boost instruction-level parallelism. The authors propose a novel approach by replacing traditional two-bit counters in two-level adaptive predictors with perceptrons, a type of simple neural network, to enhance prediction accuracy and reduce aliasing issues.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.9,\n    \"b3\": 0.85,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b15\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in branch prediction within modern computer architectures, particularly focusing on improving prediction accuracy to enhance performance. The authors propose a novel approach using perceptrons, a type of neural network, to replace traditional two-bit saturating counters in two-level adaptive branch predictors. This method aims to reduce aliasing and improve prediction accuracy while being feasible for hardware implementation.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b15\": 0.8,\n    \"b21\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving branch prediction accuracy in modern computer architectures by introducing a novel approach that replaces traditional two-level adaptive predictors' saturating counters with perceptrons, a simple form of neural network. The approach is motivated by the need to reduce aliasing and improve prediction accuracy without excessive hardware costs.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b17\": 0.9,\n    \"b20\": 0.9,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b21\": 0.7,\n    \"b24\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b18\": 0.5,\n    \"b19\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving branch prediction accuracy in modern computer architectures, which rely heavily on speculation to boost instruction-level parallelism. The proposed solution involves using perceptrons, a type of neural network, to replace traditional two-bit counters in branch predictors. This approach aims to enhance prediction accuracy by leveraging the predictive capabilities of perceptrons while maintaining hardware efficiency.\",\n    \"Direct Inspiration\": {\n        \"b25\": 0.9,\n        \"b16\": 0.8,\n        \"b15\": 0.8,\n        \"b21\": 0.8,\n        \"b3\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b24\": 0.6,\n        \"b7\": 0.6,\n        \"b6\": 0.6,\n        \"b17\": 0.6,\n        \"b22\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.5,\n        \"b18\": 0.5,\n        \"b1\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving branch prediction accuracy in modern microarchitectures by replacing two-bit saturating counters with perceptrons, a type of simple neural network. The paper proposes a novel two-level prediction scheme using perceptrons, which are efficient in hardware implementation and provide good predictive capabilities.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b18\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b15\": 0.6,\n    \"b21\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```"], "58437725ac44360f1082f992": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper revolve around ensuring the privacy of sensitive training data in machine learning models. The proposed algorithm, PATE (Private Aggregation of Teacher Ensembles), improves upon existing techniques by training an ensemble of teacher models on disjoint subsets of sensitive data, then using auxiliary, unlabeled non-sensitive data to train a student model on the aggregate output of the ensemble. This strategy ensures privacy by limiting the student's exposure to any single sensitive data point and uses differential privacy techniques for rigorous privacy guarantees.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b32\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of protecting the privacy of sensitive training data in machine learning models. It proposes the PATE (Private Aggregation of Teacher Ensembles) method, which improves privacy guarantees by using an ensemble of teacher models to label data for a student model, combined with techniques such as differential privacy and generative adversarial networks (GANs) for semi-supervised learning.\",\n  \"Direct Inspiration\": [\"b0\", \"b20\", \"b32\"],\n  \"Indirect Inspiration\": [\"b5\", \"b19\", \"b28\", \"b29\"],\n  \"Other Inspiration\": [\"b18\", \"b23\", \"b25\", \"b35\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Protecting the privacy of sensitive training data in machine learning applications.\",\n      \"Preventing overfitting and implicit memorization of specific training examples by machine learning models.\"\n    ],\n    \"inspirations\": [\n      \"Differential privacy techniques.\",\n      \"Semi-supervised learning with generative adversarial networks (GANs).\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b5\",\n    \"b20\",\n    \"b32\",\n    \"b0\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b18\",\n    \"b28\",\n    \"b29\",\n    \"b19\"\n  ],\n  \"Other Inspiration\": [\n    \"b13\",\n    \"b10\",\n    \"b16\",\n    \"b17\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge outlined in the paper is the protection of privacy in machine learning applications that use sensitive data. The paper proposes the PATE (Private Aggregation of Teacher Ensembles) strategy, which aims to improve privacy guarantees when training student models from sensitive data.\",\n    \"Inspirations\": \"The paper is inspired by previous work on knowledge aggregation and transfer, particularly focusing on differential privacy and semi-supervised learning techniques.\"\n  },\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b0\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b28\": 0.8,\n    \"b29\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b12\": 0.6,\n    \"b8\": 0.6,\n    \"b14\": 0.6,\n    \"b27\": 0.6,\n    \"b33\": 0.6,\n    \"b17\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge is to protect the privacy of sensitive training data in machine learning applications while maintaining high utility. The paper addresses the issue of implicit memorization by machine learning models, which can lead to privacy breaches.\",\n    \"inspirations\": \"The paper introduces the PATE (Private Aggregation of Teacher Ensembles) approach, which builds upon knowledge aggregation and transfer techniques. It incorporates semi-supervised learning and uses generative adversarial networks (GANs) to improve privacy guarantees. The moments accountant technique is also utilized for precise privacy analysis.\"\n  },\n  \"Direct Inspiration\": [\n    \"b20\",\n    \"b32\",\n    \"b0\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b19\",\n    \"b28\"\n  ],\n  \"Other Inspiration\": [\n    \"b13\"\n  ]\n}\n```"], "53e9aeebb7602d970391ac0a": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting the performance of an application on various platforms using a methodology based on microarchitecture-independent program similarity. It introduces a framework that uses standardized benchmark suites and evaluates performance prediction through normalization, principal components analysis, and a genetic algorithm. The genetic algorithm is found to be the most accurate in predicting performance differences across platforms.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b4\": 0.9,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting platform performance for applications of interest, especially when direct measurements are difficult or costly. The proposed methodology uses standardized benchmark suites and microarchitecture-independent characteristics to predict performance, leveraging normalization, principal components analysis, and a genetic algorithm to enhance accuracy.\",\n  \"Direct Inspiration\": [\"b1\", \"b4\"],\n  \"Indirect Inspiration\": [\"b6\", \"b9\"],\n  \"Other Inspiration\": [\"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is determining the best platform for a given application in terms of performance without relying on detailed cycle-accurate processor simulation, which is time-consuming. The authors propose a methodology based on microarchitecture-independent program characteristics to predict performance by using standardized benchmarks as proxies. They introduce three approaches to data transformation: normalization, principal components analysis (PCA), and a genetic algorithm, with the genetic algorithm proving to be the most accurate in predicting performance differences across various platforms.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b6\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting the performance of an application across different platforms without porting the application to each platform, using microarchitecture-independent characteristics of standardized benchmarks. The proposed methodology includes normalization, principal components analysis, and a genetic algorithm, with the genetic algorithm proving to be the most accurate.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.80,\n    \"b9\": 0.70,\n    \"b5\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting the best hardware platform for a given application without the need for extensive porting or simulation. The proposed solution leverages standardized benchmark suites and microarchitecture-independent characteristics to predict application performance. The methodology includes normalization, principal components analysis, and a genetic algorithm, with the genetic algorithm showing the most accuracy in performance prediction.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b4\": 0.9,\n    \"b6\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b9\": 0.65\n  },\n  \"Other Inspiration\": {}\n}\n```"], "5ea0166591e01173f5bbf71e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of using smartphone sensing data to detect psychological stress, specifically focusing on feature extraction from multimodality sensor data, creating a personalized generalizable model, and learning an accurate stress detection model with insufficient labeled training data. The proposed methods include combining multiple features into views, extracting both relative and absolute features, and employing a co-trainer-based semi-supervised learning approach.\",\n  \"Direct Inspiration\": [\"b12\", \"b16\"],\n  \"Indirect Inspiration\": [\"b10\", \"b11\", \"b14\", \"b18\"],\n  \"Other Inspiration\": [\"b6\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of detecting psychological stress using smartphone sensing data, emphasizing feature extraction from multimodal sensor data, integrating personalized components into a generalizable model, and handling insufficient labeled data through a co-training-based semi-supervised learning approach. Key contributions include the extraction of relevant features from smartphone data, the development of a personalized universal model, and the use of unlabeled data to improve classification accuracy.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting psychological stress using smartphone sensing data, focusing on feature extraction from multimodal sensor data, personalization of a generalized model, and learning with insufficient labeled data. The proposed approach combines absolute and relative features, uses a co-trainer-based semi-supervised learning approach, and is validated using the StudentLife dataset.\",\n  \"Direct Inspiration\": [\"b12\", \"b16\"],\n  \"Indirect Inspiration\": [\"b6\", \"b26\"],\n  \"Other Inspiration\": [\"b10\", \"b11\", \"b14\", \"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of detecting psychological stress using smartphone sensing data. The main challenges include feature extraction from multimodal data, developing a personalized yet generalizable model, and handling insufficient labeled training data. The authors propose a co-trainer-based semi-supervised learning approach to leverage unlabeled data and improve classification accuracy. They also integrate both absolute and relative features to personalize the detection model.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.95,\n    \"b6\": 0.9,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b16\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b18\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of detecting psychological stress using smartphone sensing data, focusing on feature extraction, personalization of the model, and handling insufficient labeled data. It proposes a co-trainer-based semi-supervised learning approach to improve classification accuracy.\",\n  \"Direct Inspiration\": [\"b12\", \"b16\"],\n  \"Indirect Inspiration\": [\"b6\", \"b26\"],\n  \"Other Inspiration\": [\"b15\", \"b17\", \"b18\"]\n}\n```"], "5d1eb9b7da562961f0af38c9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting human psychological states, specifically stress, using passive sensing data on a multi-class classification problem. The authors propose a novel algorithm, the Cross-personal Activity LSTM Multitask Auto-encoder Network (CALM-Net), which considers data as time-series and personalizes predictions to students. This method aims to overcome issues such as inter-subject variability and heterogeneity in granularity, achieving significant improvements over previous models.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b3\": 0.9,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of predicting human psychological states, specifically stress, using passive sensing data on a multi-class classification problem. It introduces the CALM-Net model, which incorporates time-series data and personalizes predictions to improve performance. The model achieves significant improvement over previous methods by capturing temporal patterns and learning personalized information about students.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b4\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b7\": 0.5,\n        \"b11\": 0.5,\n        \"b12\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting human psychological states, specifically stress, using passive sensing data on a multi-class classification problem. The proposed algorithm, CALM-Net, is a Cross-personal Activity LSTM Multitask Auto-encoder Network that incorporates time-series data and personalizes predictions for students. This approach aims to overcome issues such as inter-subject variability, heterogeneity in data granularity, and noisy raw sensor data.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting psychological states, such as stress, in students using passive sensing data from smartphones. It introduces the Cross-personal Activity LSTM Multitask Auto-encoder Network (CALM-Net), which models data as time-series to identify temporal patterns and personalize predictions. CALM-Net achieves a significant improvement in performance over previous models by incorporating time-series information and personalizing predictions.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b2\": 0.75,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.65,\n    \"b12\": 0.6,\n    \"b15\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting human psychological state (e.g., stress) using passive sensing data on a multi-class classification problem. The proposed Cross-personal Activity LSTM Multitask Auto-encoder Network (CALM-Net) can identify temporal patterns in student data and personalize predictions, achieving significant improvements over existing models.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b3\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```"], "53e9afd3b7602d9703a26d76": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of automating the feedback loop between coverage analysis and test generation in functional verification of hardware designs. It proposes a novel approach using Bayesian networks to model the relationship between coverage information and test directives, aiming to improve coverage progress rate, reach uncovered tasks, and provide diverse directives for given coverage tasks.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b12\": 0.85,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.65,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in functional verification of hardware designs, particularly the manual bottlenecks in coverage analysis and test generation. It proposes a novel approach using Bayesian networks to automate Coverage Directed Test Generation (CDG) by modeling the relationship between coverage information and test directives.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b8\": 0.9,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b5\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the bottleneck in hardware design verification, focusing on automating the feedback loop between coverage analysis and test generation. It proposes a novel approach using Bayesian networks to model the relationship between coverage information and test generator directives. This approach aims to improve coverage progress rate, target uncovered tasks, and provide various ways to reach coverage tasks. The paper describes experiments demonstrating the effectiveness of Bayesian networks in achieving these goals.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b10\": 1.0,\n        \"b12\": 1.0,\n        \"b13\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b5\": 0.8,\n        \"b11\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.6,\n        \"b9\": 0.6,\n        \"b14\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Functional verification bottleneck in hardware design\",\n      \"Manual bottlenecks in coverage analysis and test generation feedback loop\",\n      \"Need for automation in coverage directed test generation (CDG)\"\n    ],\n    \"inspirations\": [\n      \"Use of Bayesian networks to model the relationship between coverage information and test generation directives\",\n      \"Application of statistical inference and machine learning techniques to CDG\"\n    ]\n  },\n  \"Direct Inspiration\": [\"b1\", \"b8\", \"b10\"],\n  \"Indirect Inspiration\": [\"b7\", \"b12\"],\n  \"Other Inspiration\": [\"b2\", \"b5\", \"b11\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of automating the feedback loop in functional hardware verification through Coverage Directed Test Generation (CDG). It proposes a novel approach by incorporating Bayesian networks to model the relationship between coverage information and test directives. The Bayesian network model is designed to provide the most probable directives to achieve specific coverage tasks, thus increasing the efficiency and quality of the verification process.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b8\": 0.95,\n        \"b12\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b7\": 0.7,\n        \"b10\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b5\": 0.65,\n        \"b6\": 0.7,\n        \"b9\": 0.6,\n        \"b11\": 0.85\n    }\n}\n```"], "599e96ec9c05cae4992b4ef3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating plausible and expressive 3D facial animation from vocal audio tracks, focusing on overcoming the limitations of vision-based performance capture and improving the quality of audio-driven animation. The proposed method uses a deep convolutional neural network trained to account for phoneme coarticulation, lexical stress, and facial muscle interactions, incorporating a novel three-way loss function to ensure temporal stability and responsiveness.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b13\": 0.95,\n    \"b24\": 0.85,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b11\": 0.75,\n    \"b19\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b42\": 0.65,\n    \"b15\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Generating plausible and expressive 3D facial animation based exclusively on vocal audio tracks, addressing issues such as phoneme coarticulation, lexical stress, and facial muscle interactions.\",\n    \"Inspirations\": \"Adopting a data-driven approach with deep neural networks to handle the inherent ambiguities in audio-to-facial expression mapping.\"\n  },\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b36\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.75,\n    \"b42\": 0.7,\n    \"b31\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b24\": 0.55,\n    \"b2\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating expressive 3D facial animation from vocal audio tracks. It proposes a deep neural network architecture that accounts for phoneme coarticulation, lexical stress, and facial muscle interactions. The main contributions include a convolutional network architecture tailored for speech processing, a novel method for discovering variations in training data related to emotional states, and a three-way loss function for temporal stability and responsiveness.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b36\": 0.8,\n    \"b25\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.6,\n    \"b32\": 0.6,\n    \"b31\": 0.6,\n    \"b42\": 0.5,\n    \"b7\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.4,\n    \"b0\": 0.4,\n    \"b48\": 0.4,\n    \"b8\": 0.3,\n    \"b9\": 0.3,\n    \"b37\": 0.3,\n    \"b15\": 0.3,\n    \"b11\": 0.3,\n    \"b23\": 0.3,\n    \"b28\": 0.3,\n    \"b18\": 0.3,\n    \"b19\": 0.3,\n    \"b30\": 0.3,\n    \"b35\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating expressive 3D facial animation based solely on vocal audio tracks. The proposed method involves a data-driven approach using deep neural networks to overcome the inherent ambiguity in audio-based facial animation. Key contributions include a specialized convolutional network architecture, a novel approach to discover variations in training data, and a three-way loss function to ensure temporal stability and responsiveness.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.95,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.85,\n    \"b15\": 0.75,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b25\": 0.65,\n    \"b31\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on generating expressive 3D facial animation based on vocal audio tracks. The proposed method addresses challenges such as the high cost and complexity of vision-based performance capture systems, the variability in user voices and physical setups, and the stringent latency requirements of real-time applications. The authors introduce a convolutional network architecture tailored for human speech, a novel way to discover variations in training data that cannot be explained by audio alone, and a three-way loss function to ensure temporal stability and responsiveness.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.95,\n    \"b36\": 0.9,\n    \"b25\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b42\": 0.75,\n    \"b31\": 0.7,\n    \"b7\": 0.65,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.55,\n    \"b9\": 0.5,\n    \"b15\": 0.5,\n    \"b47\": 0.45\n  }\n}\n```"], "5f0d85c69fced0a24be4f04c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of hard-to-predict (H2P) branches in modern Out-of-Order (OOO) processors, which lead to performance and power inefficiencies due to pipeline flushes. The proposed solution, Auto-Predication of Critical Branches (ACB), dynamically predicates critical branches without requiring ISA or compiler support, thereby improving performance and reducing mispredictions.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b11\": 0.9,\n    \"b12\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of mitigating performance losses due to hard-to-predict (H2P) branches in Out-of-Order (OOO) processors. The authors propose Auto-Predication of Critical Branches (ACB), a micro-architectural solution that dynamically predicates critical branches to reduce pipeline flushes and improve performance without needing significant changes to the compiler or ISA.\",\n  \"Direct Inspiration\": [\"b6\", \"b11\", \"b12\", \"b13\", \"b14\"],\n  \"Indirect Inspiration\": [\"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b10\"],\n  \"Other Inspiration\": [\"b0\", \"b7\", \"b8\", \"b9\", \"b15\", \"b16\", \"b17\", \"b18\", \"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Hard-to-Predict (H2P) branches in Out-of-Order (OOO) processors, which cause significant performance and power overheads due to pipeline flushes and re-executions. The authors propose Auto-Predication of Critical Branches (ACB), a mechanism that intelligently disables speculation on performance-critical branches without requiring compiler or ISA changes. ACB uses a novel hardware mechanism to detect control flow convergence and dynamically predicates critical branches, reducing mispredictions and improving performance.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b10\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.85,\n    \"b12\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.75,\n    \"b15\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of mitigating branch mis-speculations in Out-of-Order (OOO) processors due to Hard-to-Predict (H2P) branches. The proposed solution, Auto-Predication of Critical Branches (ACB), aims to intelligently disable speculation on performance-critical branches without requiring compiler or ISA support. The ACB mechanism uses heuristics and novel hardware to detect control flow convergence and dynamically predicte critical branches, enhancing performance and reducing pipeline flushes.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b11\": 0.9,\n    \"b12\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of Hard-to-Predict (H2P) branches in modern Out-of-Order (OOO) processors, which cause performance and power overheads due to pipeline flushes and re-execution upon wrong speculation. The proposed solution, Auto-Predication of Critical Branches (ACB), intelligently disables speculation on performance-critical branches without needing compiler or ISA support, making it implementable in modern OOO processors.\",\n    \"Direct Inspiration\": [\"b6\", \"b11\", \"b12\", \"b13\"],\n    \"Indirect Inspiration\": [\"b10\", \"b14\"],\n    \"Other Inspiration\": [\"b5\"]\n}\n```"], "53e9ada5b7602d97037a48ea": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of inadequately assessing memory performance in multicore architectures using common benchmarks like STREAM. It proposes a set of benchmarks to determine memory performance characteristics in multicore, multiprocessor ccNUMA systems, focusing on on-chip and off-chip cache-to-cache transfers, and the influence of cache coherency protocols.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b5\": 0.8,\n    \"b9\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b8\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of assessing memory performance in multicore, multiprocessor ccNUMA systems, especially focusing on cache-to-cache transfers and cache coherency protocols. The authors propose a set of benchmarks to measure memory performance characteristics, including latency and bandwidth between processor cores and the impact of different cache architectures and coherency protocols.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b5\": 0.75,\n    \"b9\": 0.78\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b7\": 0.65,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in accurately benchmarking memory performance in multicore, multiprocessor ccNUMA systems, particularly with respect to on-chip and off-chip memory accesses. The authors propose a set of benchmarks to analyze memory bandwidth and latency, considering cache coherency protocols and different cache architectures. The benchmarks are designed to reveal undocumented architectural properties and performance characteristics of AMD and Intel processors.\",\n  \"Direct Inspiration\": [\"b11\"],\n  \"Indirect Inspiration\": [\"b7\", \"b2\", \"b5\", \"b9\"],\n  \"Other Inspiration\": [\"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately benchmarking memory performance in modern multicore, multiprocessor ccNUMA systems. It introduces a set of benchmarks designed to measure the performance characteristics of memory accesses, focusing on on-chip and off-chip cache-to-cache transfers and the influence of cache coherency protocols.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.95,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b2\": 0.5,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper presents a set of benchmarks to determine performance characteristics of memory accesses in multicore, multiprocessor ccNUMA systems, focusing on on-chip and off-chip cache-to-cache transfers and the influence of cache coherency protocols. The primary challenges include the inadequacy of common memory benchmarks for multicore architectures and the need to understand the impact of different cache architectures and coherency protocols on memory performance.\",\n    \"Direct Inspiration\": {\n        \"b11\": 0.9,\n        \"b10\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.75,\n        \"b8\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.7,\n        \"b2\": 0.7,\n        \"b5\": 0.65,\n        \"b9\": 0.65\n    }\n}\n```"], "5736982b6e3b12023e6fd154": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in throughput-oriented computing, particularly focusing on the bandwidth-wall and the high energy cost of off-chip memory access. It proposes the MORC architecture, a log-based, Manycore ORiented, compressed Last-Level Cache that emphasizes throughput by using extreme cache compression techniques.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b19\": 1.0,\n    \"b20\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b28\": 0.7,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of bandwidth limitations and high energy costs associated with off-chip memory access in manycore architectures. The proposed solution, MORC, is a log-based, inter-line compression architecture that aims to increase throughput by maximizing compression ratios, even at the cost of decompression latency. The architecture includes innovative features like tag compression and content-aware placement to compress similar cache lines together, which significantly reduces the need for off-chip memory access.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b18\": 0.9,\n    \"b19\": 0.9,\n    \"b20\": 0.8,\n    \"b25\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.6,\n    \"b21\": 0.5,\n    \"b22\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.4,\n    \"b27\": 0.4,\n    \"b28\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of bandwidth and energy cost in throughput-oriented manycore processors by proposing MORC, a novel log-based, inter-line compressed Last-Level Cache (LLC) architecture. The primary inspiration comes from the need to increase compression ratios to enhance throughput, even at the expense of single-stream performance. Key techniques involve content-aware placement, tag compression, and leveraging large-block encoding for inter-line compression.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b24\": 0.85,\n    \"b18\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b20\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of memory bandwidth and energy consumption in manycore architectures. It proposes a novel log-based, inter-line compression architecture called MORC to enhance throughput by increasing compression ratio while tolerating high decompression latency. The architecture leverages content-aware placement and tag compression to maximize compression efficiency and reduce off-chip memory accesses.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b19\": 0.9,\n    \"b20\": 0.8,\n    \"b24\": 0.85,\n    \"b25\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b28\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b26\": 0.65,\n    \"b27\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of increasing throughput and energy efficiency in throughput-oriented manycore processors by proposing a novel log-based, inter-line compression architecture called MORC. The main challenges include the bandwidth-wall problem and high energy costs associated with off-chip memory access. The proposed MORC architecture aims to maximize compression ratios, even at the expense of single-stream performance, by compressing similar cache lines together into logs, which improves throughput and reduces energy consumption.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b19\": 1,\n    \"b20\": 1,\n    \"b21\": 0.9,\n    \"b22\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7\n  }\n}\n```"], "58437735ac44360f10832276": ["```json\n{\n  \"Summary\": \"The paper discusses the design and implementation of Yet Another Compressed Cache (YACC), which aims to address the inefficiencies and complexities of existing compressed cache designs, such as Decoupled Compressed Cache (DCC) and Skewed Compressed Cache (SCC). The primary challenges tackled include reducing tag overhead, maintaining efficient tag-data mapping, eliminating recompaction overhead, and enabling advanced cache replacement policies. YACC inherits the benefits of DCC and SCC while simplifying the overall design, achieving similar or better performance and efficiency with lower complexity and area overhead.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.95,\n    \"Wood 2013a\": 0.95,\n    \"Wood 2013b\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b31\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b13\": 0.65,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces Yet Another Compressed Cache (YACC), aiming to improve cache performance and energy efficiency with a simpler design compared to existing methods like Decoupled Compressed Cache (DCC) and Skewed Compressed Cache (SCC). YACC utilizes super-block tags, compacts neighboring blocks with similar compression ratios in one data entry, and tracks them with a super-block tag. Unlike DCC and SCC, YACC maintains a conventional cache layout, enabling the use of modern replacement policies, reducing area overhead, and eliminating the need for complex alignment networks and skewing.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing an effective compressed cache that balances performance, energy efficiency, and simplicity. It proposes Yet Another Compressed Cache (YACC), which aims to improve upon previous designs like DCC and SCC by reducing complexity and area overheads while maintaining or improving performance and energy efficiency.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"Wood 2013a, 2013b\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of cache memory, specifically focusing on improving effective cache capacity through compression while minimizing area overhead and complexity. The proposed Yet Another Compressed Cache (YACC) design aims to achieve the benefits of previous designs (DCC and SCC) while simplifying the tag and data array structures and supporting modern replacement policies. YACC uses super-block tags, compacting neighboring blocks with similar compression ratios, and stores compressed blocks in one cache way to eliminate the need for an alignment network.\",\n    \"Direct Inspiration\": {\n        \"b26\": 1,\n        \"b16\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b31\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.7,\n        \"b23\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of compressed cache design, specifically aiming to improve the effective capacity of caches while maintaining low complexity and area overhead. The proposed YACC algorithm builds upon the principles of DCC and SCC but simplifies the design by avoiding skewed associativity and enabling the use of modern replacement policies. The key contributions are the use of super-block tags, in-place block expansion, and elimination of skewing, which collectively enhance performance and energy efficiency.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b13\": 0.7,\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b17\": 0.5,\n    \"b19\": 0.5\n  }\n}\n```"], "558c6c66e4b02b9f07a703d0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of eliminating branch mispredictions to improve performance and energy efficiency in processor architectures. The proposed technique, Control-Flow Decoupling (CFD), targets separable branches by decoupling the branch's predicate computation from its control-dependent instructions, reducing mispredictions and enhancing latency tolerance in large-window architectures.\",\n  \"Direct Inspiration\": [\"b1\", \"b3\"],\n  \"Indirect Inspiration\": [\"b27\", \"b28\"],\n  \"Other Inspiration\": [\"b22\", \"b30\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving single-thread performance while conserving energy in multi-core processors by eliminating branch mispredictions. The proposed solution, Control-Flow Decoupling (CFD), separates the computation of branch predicates from the execution of control-dependent instructions, thus enhancing performance and energy efficiency.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b27\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving single-thread performance and energy efficiency in multi-core processors by eliminating branch mispredictions, which waste time and energy. The authors propose a novel technique called control-flow decoupling (CFD) to handle hard-to-predict branches with large control-dependent regions, improving performance and energy efficiency.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.9,\n    \"b28\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b30\": 0.7,\n    \"b34\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of increasing single-thread performance and energy efficiency in multi-core processors by eliminating branch mispredictions. The proposed solution, Control-Flow Decoupling (CFD), separates the computation of branch predicates from the execution of control-dependent instructions, improving performance and reducing energy consumption.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0,\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving single-thread performance and energy efficiency in multi-core processors by eliminating branch mispredictions. The authors propose a novel technique called control-flow decoupling (CFD) to eradicate mispredictions of separable branches, which is achieved by separating the loop containing the branch into two loops, communicating branch outcomes through an architectural queue. This technique is evaluated on a microarchitecture similar to Intel's Sandy Bridge core, showing significant performance and energy improvements.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b28\": 0.7,\n    \"b30\": 0.7,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.5,\n    \"b31\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```"], "53e9b42fb7602d9703f2696f": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of effectively scheduling memory loads that block the reorder buffer (ROB) in multicore processors. It proposes a priority-based memory scheduler paired with a simple processor-side predictor to identify critical load instructions that stall the pipeline. By using small, simple per-core predictors, the authors aim to track blocking loads and prioritize them in the memory system, thereby improving performance.\",\n    \"Direct Inspiration\": {\n        \"b29\": 1.0,\n        \"b3\": 1.0,\n        \"b13\": 1.0,\n        \"b18\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b9\": 0.7,\n        \"b10\": 0.7,\n        \"b22\": 0.9\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing memory scheduling by utilizing processor-side information, specifically instruction criticality, to prioritize loads that block the reorder buffer (ROB) and potentially stall the pipeline. The proposed solution integrates a priority-based memory scheduler with simple per-core predictors to identify and prioritize these critical loads, demonstrating significant performance improvements using a sophisticated multicore simulator.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b13\": 0.9,\n    \"b18\": 0.9,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.4,\n    \"b20\": 0.4,\n    \"b21\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in memory scheduling by proposing a priority-based memory scheduler combined with a processor-side mechanism to predict load instructions that may block a core's reorder buffer (ROB) in a CMP, potentially stalling the pipeline. The approach aims to improve performance by prioritizing critical loads based on processor-side metrics rather than purely memory-side metrics.\",\n    \"Direct Inspiration\": {\n        \"b22\": 0.9,\n        \"b29\": 0.85,\n        \"b3\": 0.8,\n        \"b13\": 0.8,\n        \"b18\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b9\": 0.6,\n        \"b16\": 0.6,\n        \"b17\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.5,\n        \"b12\": 0.5,\n        \"b15\": 0.5,\n        \"b20\": 0.5,\n        \"b21\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in this paper is improving the performance of memory schedulers in multicore processors by leveraging processor-side information to predict and prioritize critical load instructions. The proposed solution involves combining a priority-based memory scheduler with a processor-side mechanism that predicts load instructions that could block the core's reorder buffer (ROB), potentially stalling the pipeline.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b13\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving memory scheduler performance by using processor-side information to predict and prioritize load instructions that may block the reorder buffer (ROB) in a multicore processor. The proposed method pairs a priority-based memory scheduler with a simple processor-side mechanism to predict blocking loads, reducing runtime and improving performance.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b3\": 1.0,\n    \"b13\": 1.0,\n    \"b18\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```"], "5a9cb66717c44a376ffb87ea": ["```json\n{\n  \"Summary\": \"The paper addresses the computational inefficiency of Neural Architecture Search (NAS) by introducing a method where child models share weights, named Efficient Neural Architecture Search (ENAS). This approach significantly reduces the GPU hours needed by over 1000x while achieving competitive performance on CIFAR-10 and Penn Treebank datasets.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b47\": 1.0,\n    \"b29\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.9,\n    \"b48\": 0.9,\n    \"b26\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b41\": 0.7,\n    \"b42\": 0.8,\n    \"b6\": 0.7,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of the high computational cost of Neural Architecture Search (NAS) by proposing Efficient Neural Architecture Search (ENAS), which forces child models to share weights to avoid retraining from scratch. The inspiration comes from transfer learning and multitask learning, which have shown that parameters learned for one model can be reused for others.\",\n  \"Direct Inspiration\": {\n    \"b35\": 0.9,\n    \"b47\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.7,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b21\": 0.5,\n    \"b41\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the computational inefficiency of Neural Architecture Search (NAS) by proposing Efficient Neural Architecture Search (ENAS), which forces child models to share weights. This significantly reduces the GPU-hours required, making it feasible to run on a single Nvidia GTX 1080Ti GPU. The proposed method achieves competitive performance on CIFAR-10 and Penn Treebank datasets.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1,\n    \"b47\": 0.9,\n    \"b29\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b41\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.9,\n    \"b48\": 0.85,\n    \"b5\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the computational expense and time consumption of Neural Architecture Search (NAS). The proposed algorithm, Efficient Neural Architecture Search (ENAS), addresses this challenge by enabling child models to share weights, thus avoiding the need to train each model from scratch to convergence. This approach is inspired by transfer learning and multitask learning, where parameters learned for a particular model on a particular task can be used for other models on other tasks.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1,\n    \"b47\": 1,\n    \"b29\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.9,\n    \"b48\": 0.9,\n    \"b26\": 0.9,\n    \"b42\": 0.7,\n    \"b21\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the computational expense and time consumption of Neural Architecture Search (NAS). The proposed solution, Efficient Neural Architecture Search (ENAS), improves efficiency by sharing weights among child models, inspired by transfer learning and multitask learning. This allows for a significant reduction in GPU hours while maintaining strong performance.\",\n  \"Direct Inspiration\": {\n    \"b35\": 0.9,\n    \"b47\": 0.9,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b46\": 0.8,\n    \"b48\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7,\n    \"b0\": 0.7,\n    \"b42\": 0.7,\n    \"b17\": 0.7,\n    \"b21\": 0.7,\n    \"b41\": 0.7,\n    \"b6\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```"], "5ce3af25ced107d4c65f15d4": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of protein function prediction from sequence data, proposing deep learning models (ProtENN and ProtCNN) to improve both accuracy and computational efficiency over traditional methods like BLASTp and HMMER. The authors construct a novel benchmark using Pfam seed sequences and demonstrate superior performance of their models in terms of error rates and speed. The work explores one-shot learning for annotating small families and highlights the potential of deep learning to provide general solutions for protein functional annotation.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b3\": 0.9,\n        \"b13\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b6\": 0.7,\n        \"b7\": 0.7,\n        \"b8\": 0.7,\n        \"b10\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.6,\n        \"b11\": 0.6,\n        \"b12\": 0.6,\n        \"b14\": 0.6,\n        \"b15\": 0.6,\n        \"b16\": 0.6,\n        \"b17\": 0.6,\n        \"b18\": 0.6,\n        \"b19\": 0.6,\n        \"b20\": 0.6,\n        \"b21\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"At least one-third of microbial proteins cannot be annotated through alignment to characterized sequences.\",\n      \"Run times of methods like BLASTp scale linearly with the size of the labeled database, which is growing exponentially.\",\n      \"Substitution matrices, sequence alignment, and hand-tuned scoring functions are required in traditional models, creating bottlenecks.\"\n    ],\n    \"inspirations\": [\n      \"Deep learning provides an opportunity to bypass bottlenecks and directly predict protein functional annotations from sequence data.\",\n      \"Few-shot learning allows identification of novel classes from just a few examples.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.85,\n    \"b10\": 0.8,\n    \"b13\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b17\": 0.7,\n    \"b19\": 0.65,\n    \"b20\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b24\": 0.55,\n    \"b25\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency and limitations of traditional protein function prediction methods, such as profile hidden Markov models (pHMMs) and BLASTp, particularly in handling the growing size of protein databases and the diversity within protein families. The authors propose using deep learning models, specifically ProtCNN and ProtENN, to predict protein functional annotations from sequence data, achieving higher accuracy and computational efficiency. The paper's core contributions include constructing a benchmark using Pfam sequences, demonstrating the superior performance of deep models in annotating protein sequences, and introducing one-shot learning for classifying sequences from small or novel protein families.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.95,\n    \"b26\": 0.85,\n    \"b27\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b19\": 0.75,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.65,\n    \"b22\": 0.6,\n    \"b28\": 0.55,\n    \"b29\": 0.55,\n    \"b30\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of protein function prediction from sequence data, proposing a deep learning approach to bypass traditional bottlenecks like sequence alignment and hand-tuned scoring functions. The authors develop ProtCNN and ProtENN models which are benchmarked against existing methods like BLASTp and HMMER, demonstrating significantly improved accuracy and computational efficiency.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b19\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the limitation of existing methods like BLASTp and profile HMMs in accurately and efficiently annotating protein functions from sequences. The proposed algorithm, ProtENN, uses deep learning models (ProtCNN) to overcome these limitations by directly predicting protein functional annotations from sequence data, offering substantial improvements in accuracy and computational efficiency.\",\n    \"Direct Inspiration\": {\n        \"b10\": 0.9,\n        \"b11\": 0.8,\n        \"b13\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.7,\n        \"b14\": 0.7,\n        \"b15\": 0.7,\n        \"b21\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b17\": 0.6,\n        \"b18\": 0.6,\n        \"b19\": 0.6,\n        \"b20\": 0.6\n    }\n}\n```"], "5ede0553e06a4c1b26a841e6": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of over-smoothing and over-fitting in Graph Neural Networks (GNNs) by introducing a general stochastic regularization technique called Graph DropConnect (GDC). The proposed method adaptively samples connections to prevent connected nodes from having the same learned representations, thus improving performance without serious over-smoothing. The paper also introduces a hierarchical beta-Bernoulli construction for Bayesian learnable GDC to further enhance uncertainty quantification (UQ) and regularization.\",\n    \"Direct Inspiration\": {\n        \"b35\": 1.0,\n        \"b36\": 1.0,\n        \"b3\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b40\": 0.9,\n        \"b16\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.7,\n        \"b26\": 0.7,\n        \"b24\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of over-smoothing and over-fitting in Graph Neural Networks (GNNs). The authors propose a stochastic regularization technique called Graph DropConnect (GDC), which adaptively samples connections to improve performance and reduce these issues. GDC incorporates techniques like DropOut, DropEdge, and node sampling but introduces a more flexible and adaptive approach.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1,\n    \"b35\": 1,\n    \"b3\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.8,\n    \"b8\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses key challenges in Graph Neural Networks (GNNs) related to over-smoothing and over-fitting. It proposes a novel stochastic regularization technique called Graph DropConnect (GDC) to mitigate these issues. GDC adaptively samples connections in GNNs, offering a flexible and efficient regularization mechanism. The paper positions GDC as a generalization of existing techniques such as DropOut, DropEdge, and node sampling, and demonstrates its effectiveness in improving GNN performance.\",\n  \"Direct Inspiration\": {\n    \"b35\": 0.95,\n    \"b36\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.85,\n    \"b24\": 0.80,\n    \"b40\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.70,\n    \"b16\": 0.65,\n    \"b8\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Over-smoothing in GNNs due to Laplacian smoothing\",\n      \"Over-fitting in GNNs\",\n      \"High computational cost and inconsistency in Bayesian extensions for GNNs\"\n    ],\n    \"algorithm\": \"Graph DropConnect (GDC) for adaptive connection sampling in GNNs to mitigate over-smoothing and over-fitting, with a hierarchical beta-Bernoulli construction for Bayesian learnable GDC.\"\n  },\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b36\": 1.0,\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b16\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of over-fitting and over-smoothing in Graph Neural Networks (GNNs) by introducing a general stochastic regularization technique called Graph DropConnect (GDC). The authors show that existing techniques like DropOut, DropEdge, and node sampling are special cases of GDC. The proposed method adaptively learns the connection sampling or drop rate to improve performance in semi-supervised node classification and uncertainty quantification (UQ).\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b36\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b40\": 0.7,\n    \"b8\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```"], "5736982b6e3b12023e6fd332": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of exploiting ordered irregular parallelism in parallel architectures. The proposed solution, Swarm, introduces a task-based execution model and a scalable microarchitecture to efficiently handle speculative task execution, conflict detection, and ordered task commits. The key contributions include a novel execution model with programmer-specified timestamps, a hardware task management scheme with speculative task creation, a scalable conflict detection scheme, and a distributed commit protocol.\",\n  \"Direct Inspiration\": [\"b32\", \"b33\", \"b47\", \"b78\"],\n  \"Indirect Inspiration\": [\"b27\", \"b59\", \"b65\", \"b67\"],\n  \"Other Inspiration\": [\"b54\", \"b24\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of exploiting ordered irregular parallelism in applications, where tasks must follow specific orders, have unknown data dependences, and dynamically create other tasks. The proposed solution, Swarm, consists of a task-based execution model and a microarchitecture designed to scale efficiently, leveraging speculative out-of-order task execution and distributed task queues.\",\n  \"Direct Inspiration\": [\"b47\", \"b78\"],\n  \"Indirect Inspiration\": [\"b32\", \"b33\", \"b59\", \"b65\"],\n  \"Other Inspiration\": [\"b27\", \"b60\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of exploiting ordered irregular parallelism in parallel architectures, which include dynamic task creation, unknown data dependencies, and ensuring proper execution order. The proposed solution, Swarm, introduces an architecture that supports speculative execution, distributed task management, and scalable conflict detection to efficiently utilize parallelism.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1,\n    \"b33\": 0.9,\n    \"b47\": 1,\n    \"b78\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b59\": 0.85,\n    \"b65\": 0.8,\n    \"b67\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.75,\n    \"b60\": 0.75,\n    \"b68\": 0.75,\n    \"b10\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the efficient exploitation of ordered irregular parallelism in applications. The paper introduces Swarm, an architecture that addresses these challenges with a task-based execution model and a microarchitecture that includes distributed task queues, speculative out-of-order task execution, and ordered task commits.\",\n  \"Direct Inspiration\": {\n    \"b32\": 0.9,\n    \"b33\": 0.8,\n    \"b47\": 0.85,\n    \"b78\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.7,\n    \"b59\": 0.7,\n    \"b65\": 0.7,\n    \"b67\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b24\": 0.6,\n    \"b28\": 0.6,\n    \"b60\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Efficient exploitation of ordered irregular parallelism\",\n      \"High runtime overheads in software implementations\",\n      \"Need for large speculation windows\"\n    ],\n    \"inspirations\": [\n      \"Previous work on ordered parallelism in software\",\n      \"Thread-level speculation schemes\"\n    ]\n  },\n  \"Direct Inspiration\": [\n    \"b32\",\n    \"b33\",\n    \"b47\",\n    \"b78\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b27\",\n    \"b59\",\n    \"b65\",\n    \"b67\"\n  ],\n  \"Other Inspiration\": [\n    \"b18\",\n    \"b34\",\n    \"b54\",\n    \"b58\"\n  ]\n}\n```"], "5ef96b048806af6ef2772036": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges in Grounded Video Description (GVD) by proposing a Hierarchical Attention based Spatial-Temporal Graph-to-Sequence Learning framework (HAST-Graph2Seq). The challenges include modeling spatial-temporal correlations and refining noisy graph structures. The novel approach involves constructing and refining a spatial-temporal sequence graph and employing hierarchical attention mechanisms for video description generation.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1.0,\n        \"b4\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b6\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b7\": 0.5,\n        \"Yang et al., 2019\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in Grounded Video Description (GVD) by proposing a Hierarchical Attention based Spatial-Temporal Graph-to-Sequence Learning framework (HAST-Graph2Seq). The main challenges include: 1) modeling relationships among region proposals and attending them for text generation, and 2) adapting graph-based methods to GVD considering spatial-temporal correlations and reducing noise in graph structures. The paper introduces a novel spatial-temporal sequence graph data structure, a refinement technique for noisy graph structures, and hierarchical attention mechanisms to improve the generation of grounded descriptions.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b2\": 0.7,\n    \"b6\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b3\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Modeling relationships among region proposals\",\n      \"Attending region proposals for text generation\",\n      \"Adapting graph-based approaches to GVD task\",\n      \"Handling temporal redundancy and noisy graph structures in videos\"\n    ],\n    \"proposed_algorithm\": \"Hierarchical Attention based Spatial-Temporal Graph-to-Sequence Learning framework (HAST-Graph2Seq) for Grounded Video Description\"\n  },\n  \"Direct Inspiration\": [\n    \"b8\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b4\"\n  ],\n  \"Other Inspiration\": [\n    \"b2\",\n    \"b3\",\n    \"b5\",\n    \"b6\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating grounded and accurate video descriptions by linking generated words with regions in video frames. Existing methods fail to model relationships among region proposals and attend to them during text generation. The proposed Hierarchical Attention based Spatial-Temporal Graph-to-Sequence Learning framework (HAST-Graph2Seq) aims to capture implicit correlations among region proposals using a spatial-temporal sequence graph and refine the graph structure to improve description generation.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b4\": 0.9,\n    \"b2\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b5\": 0.6,\n    \"b6\": 0.5\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the task of Grounded Video Description (GVD) by proposing a Hierarchical Attention based Spatial-Temporal Graph-to-Sequence Learning framework (HAST-Graph2Seq). The primary challenges include modeling spatial-temporal correlations in videos and refining noisy graph structures. The proposed method uses a spatial-temporal sequence graph and hierarchical attention mechanisms to generate more grounded and accurate video descriptions.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"Chen et al., 2019\": 0.7,\n    \"Vaswani et al., 2017\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"Yang et al., 2019\": 0.6,\n    \"b6\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```"], "53e99845b7602d9702072224": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include achieving high fetch bandwidth in superscalar processors while managing the complexity of the fetch architecture. The paper proposes the stream fetch engine based on the next stream predictor. The multiple stream predictor is introduced as a novel technique to concatenate frequently executed streams, thereby increasing instruction streams' length and performance without focusing on specific branch types.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b16\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b4\": 0.6,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are achieving high fetch bandwidth while maintaining low fetch architecture complexity and overcoming the limitations of branch prediction table access latency. The proposed solution is the multiple stream predictor, which concatenates instruction streams frequently executed as a sequence to hide prediction table access latency and reduce complexity.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b18\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of increasing fetch bandwidth in high-performance superscalar processors while controlling complexity. The authors propose the stream fetch engine, based on the next stream predictor, to achieve high fetch bandwidth. They present the multiple stream predictor to concatenate frequently executed streams, thus overcoming the limitations of branch prediction latency and avoiding the complexity of prediction overriding mechanisms.\",\n    \"Direct Inspiration\": {\n        \"b10\": 1.0,\n        \"b15\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b16\": 0.7,\n        \"b18\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.5,\n        \"b4\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper are achieving high fetch bandwidth while maintaining low complexity in the fetch engine, and reducing the latency caused by branch prediction table accesses in high frequency processors.\",\n    \"Inspirations\": \"The paper is inspired by previous work on accurate branch prediction mechanisms, stream fetch engines, and techniques to mitigate branch prediction latency such as prediction overriding, as well as research on code layout optimizations.\"\n  },\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b15\": 1,\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.8,\n    \"b11\": 0.8,\n    \"b19\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving high fetch bandwidth in superscalar processors while managing complexity. It proposes a stream fetch engine based on the next stream predictor, which uses instruction streams as the basic prediction unit. The paper introduces the multiple stream predictor to concatenate frequently executed streams, aiming to enlarge instruction streams and reduce the complexity introduced by prediction overriding mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b11\": 0.5,\n    \"b4\": 0.5,\n    \"b7\": 0.4\n  }\n}\n```"], "558bdcd2e4b02b9f07a3b092": ["```json\n{\n  \"Summary\": \"The paper addresses two main challenges in chip-multiprocessors (CMPs): the degradation of latency and energy in shared caches as systems scale up, and the interference among workloads in shared cache accesses. The proposed solution, Jigsaw, introduces a design that combines recent advancements in cache partitioning and non-uniform cache access (NUCA) architectures. Jigsaw's novel contributions include a hardware design that allows for fine-grained partitioning of cache banks and a software runtime that utilizes these partitions to optimize cache utilization and data placement.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1,\n    \"b33\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b10\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalability and interference in shared caches of chip-multiprocessors (CMPs). It proposes Jigsaw, a design that leverages fine-grained partitioning and novel resource management algorithms to efficiently manage cache utilization and access latency. The hardware design includes configurable bank partitions and dynamic adaptation mechanisms. The software side involves a runtime system that classifies data into shares and optimizes their configuration.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1.0,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b27\": 0.5,\n    \"b41\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalability and interference in shared caches of chip-multiprocessors (CMPs) by presenting Jigsaw, a design that combines efficient fine-grained partitioning with dynamic reconfiguration and monitoring. The paper leverages prior work on partitioning techniques and introduces new resource management algorithms.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1.0,\n    \"b33\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b10\": 0.7,\n    \"b32\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the scalability and interference problems of shared caches in chip-multiprocessors (CMPs), proposing the Jigsaw design. Jigsaw combines hardware and software innovations to partition and manage last-level caches efficiently, improving performance and providing quality-of-service (QoS) guarantees.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1.0,\n    \"b33\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b10\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalability and interference in shared caches within chip-multiprocessors (CMPs). The proposed solution, Jigsaw, leverages recent work on fine-grained partitioning to create dynamically reconfigurable, software-defined caches called shares. Jigsaw allows software to control data placement and cache capacity allocation, improving performance and managing multiple workloads efficiently. Novel hardware primitives and system-level runtime optimizations, such as the Peekahead algorithm, are introduced for effective resource management.\",\n  \"Direct Inspiration\": {\n    \"b36\": 1.0,\n    \"b33\": 1.0,\n    \"b13\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b32\": 0.7,\n    \"b24\": 0.6,\n    \"b27\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b8\": 0.5,\n    \"b41\": 0.5,\n    \"b43\": 0.5\n  }\n}\n```"], "558c9cd6e4b0cfb70a1eab8d": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving the performance of inclusive cache hierarchies in CMPs without sacrificing the benefits of inclusion, such as coherence and snoop filtering. The authors propose Temporal Locality Aware (TLA) cache management policies to reduce inclusion victims by taking into account the temporal locality of lines in core caches.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b4\", \"b25\", \"b15\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b7\", \"b8\", \"b27\", \"b0\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b3\", \"b20\", \"b21\", \"b2\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance issues in inclusive cache hierarchies, particularly the problem of 'inclusion victims' where frequently accessed lines are evicted due to the inclusion property. It introduces three Temporal Locality Aware (TLA) cache management policies: Temporal Locality Hints (TLH), Early Core Invalidation (ECI), and Query Based Selection (QBS) to mitigate this problem.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.95,\n    \"b25\": 0.9,\n    \"b15\": 0.85,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b0\": 0.7,\n    \"b8\": 0.65,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.55,\n    \"b17\": 0.5,\n    \"b12\": 0.45,\n    \"b2\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving inclusive cache performance without sacrificing its benefits, particularly focusing on reducing the number of inclusion victims. The proposed solutions are Temporal Locality Aware (TLA) inclusive cache management policies which include Temporal Locality Hints (TLH), Early Core Invalidation (ECI), and Query Based Selection (QBS). These methods aim to preserve hot lines in the core caches and extend their lifetime in the LLC by effectively managing the temporal locality.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b25\": 0.9,\n    \"b15\": 0.9,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b21\": 0.6,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b0\": 0.5,\n    \"b14\": 0.5,\n    \"b17\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of inclusive cache hierarchies without sacrificing the benefits of inclusion. The proposed solution, Temporal Locality Aware (TLA) cache management policies, aims to reduce the frequency of harmful inclusion victims by identifying lines with high temporal locality in the core caches and preventing their eviction from the LLC until their temporal locality is exhausted.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.9,\n    \"b15\": 0.8,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b8\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of inclusive cache hierarchies in CMPs without sacrificing the coherence benefits. It proposes Temporal Locality Aware (TLA) inclusive cache management policies to reduce the frequency of inclusion victims by preserving hot lines in the core caches and extending their lifetime in the LLC.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b25\": 0.9,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.7,\n    \"b7\": 0.6,\n    \"b8\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.4,\n    \"b21\": 0.3\n  }\n}\n```"], "5ce937225ced2477cb328bef": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving fast and accurate power estimation for reusable IP cores at RTL or behavioral level. The proposed algorithm, PRIMAL, leverages machine learning models to enable rapid power characterization and estimation, significantly reducing the time required for gate-level simulation cycles. The main contributions include the development of ML-based methodologies for power estimation, the use of CNNs for faster and more accurate power profiling, and the extension of the approach to SystemC designs.\",\n  \"Direct Inspiration\": [\n    \"b2\",\n    \"b3\",\n    \"b5\",\n    \"b6\",\n    \"b13\",\n    \"b19\",\n    \"b20\",\n    \"b22\",\n    \"b23\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b16\",\n    \"b11\",\n    \"b9\",\n    \"b15\"\n  ],\n  \"Other Inspiration\": [\n    \"b1\",\n    \"b10\",\n    \"b12\",\n    \"b7\",\n    \"b18\",\n    \"b14\",\n    \"b0\",\n    \"b21\",\n    \"b8\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving fast and accurate power estimation for VLSI designs at RTL or behavioral level. It introduces PRIMAL, a learning-based power inference framework that leverages machine learning models trained on gate-level power analysis data to estimate power consumption rapidly and accurately. The framework aims to enable power-directed design space exploration and optimization, offering significant speedups over traditional methods like Synopsys PrimeTime PX (PTPX) while maintaining high accuracy.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b20\": 0.7,\n    \"b13\": 0.7,\n    \"b6\": 0.65,\n    \"b9\": 0.6,\n    \"b11\": 0.6,\n    \"b16\": 0.6,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving fast and accurate high-level power estimation for ASIC designs, proposing PRIMAL, a machine learning-based power inference framework that significantly accelerates power analysis while maintaining accuracy. PRIMAL utilizes gate-level power analysis to train ML models on a subset of verification testbenches, which are then used to infer power profiles for different workloads at RTL or behavioral level.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of fast and accurate power estimation for VLSI designs at RTL or behavioral level. It proposes PRIMAL, a machine learning-based framework that leverages gate-level power analysis to train models for rapid power inference of reusable IP cores.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b6\": 0.8,\n    \"b13\": 0.7,\n    \"b22\": 0.7,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b5\": 0.6,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of fast and accurate power estimation for VLSI designs at RTL or behavioral levels. The proposed solution, PRIMAL, leverages machine learning models to infer power profiles from a subset of verification testbenches, significantly reducing the time required for gate-level simulations.\",\n  \"Direct Inspiration\": [\"b23\"],\n  \"Indirect Inspiration\": [\"b2\", \"b6\"],\n  \"Other Inspiration\": [\"b3\", \"b5\", \"b13\", \"b19\", \"b20\", \"b22\"]\n}\n```"], "57d063e0ac443673542947b3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing competition overhead (COH) in multi-threaded applications running on NoC-based CMPs. The novel contribution lies in the proposed software-hardware cooperative mechanism that maximizes the chance that a thread enters the critical section during the low-overhead spinning phase, using Remaining Times of Retry (RTR) monitoring and network-level prioritization.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b16\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.75,\n    \"b17\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b22\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing competition overhead in critical section access for multi-threaded applications running on NoC-based multi/many-cores. It proposes a software-hardware cooperative mechanism to maximize the chance that a thread enters the critical section in the low-overhead spinning phase by modifying queue spinlock primitives and prioritizing locking requests based on their Remaining Times of Retry (RTR) values.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b17\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing the competition overhead in critical sections for multi-threaded applications running on NoC-based CMPs. The proposed solution is a software-hardware cooperative mechanism that maximizes the chances of a thread entering a critical section during the low-overhead spinning phase.\",\n  \"Direct Inspiration\": [\"b4\", \"b16\"],\n  \"Indirect Inspiration\": [\"b11\", \"b12\", \"b13\", \"b14\", \"b17\", \"b21\", \"b22\", \"b23\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of reducing competition overhead in multi-threaded applications running on NoC-based multi/many-core processors. The proposed solution is a software-hardware cooperative mechanism that prioritizes threads based on their Remaining Times of Retry (RTR) in the queue spinlock phase to reduce blocking time and improve performance.\",\n    \"Direct Inspiration\": [\"b11\", \"b21\"],\n    \"Indirect Inspiration\": [\"b12\", \"b13\", \"b17\", \"b22\", \"b23\", \"b14\"],\n    \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing competition overhead (COH) in multithreaded applications on NoC-based CMPs. The proposed solution is a software-hardware cooperative mechanism that prioritizes thread locking requests based on Remaining Times of Retry (RTR) to maximize the chances of entering critical sections during the low-overhead spinning phase.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b17\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```"], "5d3c234c3a55acd386d4e112": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency and high failure rates of current bottom-up computational protein design methods, which rely on the accuracy of energy functions and sampling algorithms. The paper introduces a top-down framework using a conditional generative model that learns from a graph-based representation of protein structures. The model incorporates structured self-attention to capture higher-order dependencies, providing computational efficiency and representational flexibility. The approach is demonstrated to improve statistical performance and efficiency over existing methods such as Rosetta.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b7\": 0.9,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b1\": 0.8,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b29\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing protein sequences with defined structural and functional properties using a top-down framework that combines autoregressive self-attention with graph-based descriptions of 3D structures. The proposed Structured Transformer model improves computational efficiency, accuracy, and flexibility in representing protein structures compared to previous approaches.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b6\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b7\", \"b8\", \"b33\", \"b34\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b29\", \"b30\", \"b25\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a Structured Transformer model that integrates graph-based descriptions of 3D protein structure with autoregressive self-attention for protein design. It addresses challenges in current protein design methods which often involve trial-and-error and are limited by first-order effects in parameteric approaches.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b6\", \"b7\", \"b8\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b14\", \"b25\", \"b26\", \"b27\", \"b28\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b4\", \"b9\", \"b10\", \"b11\", \"b17\", \"b18\", \"b19\", \"b20\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in computational protein design, mainly the inefficiency and trial-and-error nature of current methods. It proposes a novel Structured Transformer model that integrates self-attention mechanisms with graph-based descriptions of 3D structures, enhancing the ability to capture higher-order dependencies between sequence and structure.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b7\": 0.9,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on computational protein design by proposing a novel top-down framework that learns a conditional generative model for protein sequences given a target structure represented as a graph over residues. This method aims to address challenges related to the accuracy of energy functions and efficiency of sampling algorithms in existing bottom-up approaches.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b33\": 0.9,\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b35\": 0.6,\n    \"b37\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b3\": 0.5,\n    \"b41\": 0.4\n  }\n}\n```"], "53e99fe3b7602d97028bddfb": ["```json\n{\n  \"Summary\": \"The primary challenges in the paper include optimizing DRAM scheduling to maximize memory bandwidth utilization and performance in CMPs. The proposed solution is a reinforcement learning-based memory controller that learns and adapts to changing workload demands, contrasting with conventional, ad-hoc memory controllers that lack long-term planning and adaptability.\",\n  \"Direct Inspiration\": {\n    \"b42\": 1.0,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.5,\n    \"b33\": 0.4,\n    \"b4\": 0.4,\n    \"b5\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge addressed by the paper is the inefficiency in DRAM scheduling due to the inability of conventional memory controllers to anticipate long-term consequences of their scheduling decisions and adapt to dynamically changing workload demands.\",\n    \"Inspirations\": \"The paper proposes a novel approach using reinforcement learning (RL) to design a self-optimizing, adaptive memory controller capable of planning, learning, and continuously adapting to changing workload demands.\"\n  },\n  \"Direct Inspiration\": [\"b42\"],\n  \"Indirect Inspiration\": [\"b6\", \"b28\"],\n  \"Other Inspiration\": [\"b37\", \"b36\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of DRAM scheduling in Chip Multiprocessors (CMPs) and proposes a machine learning-based approach using reinforcement learning (RL) to design a self-optimizing, adaptive memory controller. The RL-based controller can plan, learn, and adapt to changing workload demands, significantly improving performance over conventional memory controllers.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b42\": 0.9,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b22\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge in the paper is the need for an effective DRAM scheduling mechanism in Chip Multiprocessors (CMPs) to overcome the limitations of existing memory controllers. The novel method proposed is an RL-based memory controller that learns and adapts to changing workload demands to maximize data bus utilization and overall system performance.\",\n  \"Direct Inspiration\": [\"b42\", \"b37\"],\n  \"Indirect Inspiration\": [\"b6\", \"b28\"],\n  \"Other Inspiration\": [\"b4\", \"b5\", \"b33\", \"b46\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of DRAM scheduling in Chip Multiprocessors (CMPs) by proposing a reinforcement learning (RL) based memory controller. The key contribution is the design of an adaptive memory controller that can learn optimal scheduling policies autonomously to maximize long-term performance. It leverages the RL framework to overcome the limitations of conventional memory controllers, which are typically static and ad-hoc.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b28\": 0.85,\n    \"b42\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b33\": 0.6,\n    \"b46\": 0.6\n  }\n}\n```"], "5b1643998fbcbf6e5a9bc32d": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inefficiency of training vision tasks in isolation due to the lack of leveraging relationships among tasks. The proposed algorithm aims to create a computational framework for mapping the space of visual tasks, capturing relationships among them to improve data efficiency and performance. The main contribution is a fully computational approach using neural networks to discover task relationships and optimize task transfer policies.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1.0,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b77\": 0.8,\n    \"b16\": 0.8,\n    \"b53\": 0.7,\n    \"b42\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b72\": 0.6,\n    \"b98\": 0.6,\n    \"b57\": 0.5,\n    \"b29\": 0.5,\n    \"b36\": 0.5,\n    \"b41\": 0.5,\n    \"b96\": 0.5,\n    \"b82\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of solving a large set of vision tasks with less supervision by leveraging the relationships among tasks. The authors propose a computational approach to map the space of visual tasks and compute an affinity matrix to determine task transferability, leading to data-efficient learning. The method involves training task-specific networks, modeling feasible transfers, normalizing task affinities, and optimizing a global transfer policy using Boolean Integer Programming.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b77\": 0.9,\n    \"b72\": 0.9,\n    \"b98\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.7,\n    \"b36\": 0.7,\n    \"b41\": 0.7,\n    \"b96\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b56\": 0.6,\n    \"b73\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency and high data requirements of training vision tasks in isolation, ignoring their potentially useful relationships. The proposed algorithm introduces a fully computational framework to discover and exploit these task relationships to reduce supervision and computation demands. The framework computes an affinity matrix among tasks and extracts a globally efficient transfer policy using a Binary Integer Programming formulation.\",\n  \"Direct Inspiration\": [\"b16\", \"b42\", \"b57\", \"b77\"],\n  \"Indirect Inspiration\": [\"b24\", \"b29\", \"b36\", \"b41\", \"b53\", \"b82\", \"b96\"],\n  \"Other Inspiration\": [\"b1\", \"b12\", \"b14\", \"b15\", \"b18\", \"b21\", \"b25\", \"b27\", \"b31\", \"b33\", \"b34\", \"b35\", \"b39\", \"b46\", \"b50\", \"b51\", \"b55\", \"b58\", \"b61\", \"b64\", \"b67\", \"b68\", \"b69\", \"b70\", \"b75\", \"b78\", \"b79\", \"b80\", \"b83\", \"b84\", \"b85\", \"b91\", \"b94\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of developing a universal perception model by explicitly modeling the relationships among different visual tasks. The proposed approach uses a fully computational method to discover task relationships and create an efficient task transfer policy, which reduces the need for large amounts of labeled data. The paper introduces a task affinity matrix and employs a Binary Integer Programming formulation to optimize task transfer policies.\",\n    \"Direct Inspiration\": {\n        \"b16\": 0.9,\n        \"b77\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b72\": 0.8,\n        \"b98\": 0.75,\n        \"b57\": 0.7,\n        \"b29\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b36\": 0.65,\n        \"b41\": 0.65,\n        \"b96\": 0.6,\n        \"b82\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of leveraging the relationships among various computer vision tasks to minimize supervision and computational demands while maximizing performance. It proposes a computational approach to map the space of visual tasks using neural networks, forming a task taxonomy (taskonomy) that captures task transferability. This taxonomy aims to develop efficient perception models that can solve multiple tasks with reduced labeled data. The approach includes training task-specific networks, modeling task transfers, normalizing transfer affinities, and optimizing a global transfer policy using Boolean Integer Programming.\",\n  \"Direct Inspiration\": {\n    \"b72\": 0.9,\n    \"b98\": 0.9,\n    \"b30\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b77\": 0.7,\n    \"b16\": 0.7,\n    \"b53\": 0.7,\n    \"b42\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b57\": 0.6,\n    \"b29\": 0.6,\n    \"b36\": 0.6,\n    \"b41\": 0.6,\n    \"b96\": 0.6,\n    \"b82\": 0.6\n  }\n}\n```"], "5bdc315017c44a1f58a05ce0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in relation extraction by proposing a novel extension of the graph convolutional network (GCN) tailored for relation extraction. The main contributions include the development of GCNs to efficiently pool information over dependency structures, the introduction of a path-centric pruning technique to remove irrelevant information, and the presentation of a Contextualized GCN (C-GCN) to handle contextual information and parsing inaccuracies.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b16\": 0.9,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b9\": 0.7,\n    \"b11\": 0.6,\n    \"b21\": 0.6,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b6\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of relation extraction in sentences, particularly focusing on overcoming the limitations of traditional dependency-based models and neural models that rely on parse trees. The authors propose a novel extension of the graph convolutional network (GCN) tailored for relation extraction and introduce a path-centric pruning technique to improve performance and efficiency. The proposed model is tested on the SemEval 2010 Task 8 and TACRED datasets, showing significant improvements over existing models.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b16\": 0.7,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in relation extraction, particularly the inefficiency and limitations of existing dependency-based neural models. To overcome these, the authors propose a novel graph convolutional network (GCN) tailored for relation extraction, introducing efficient graph convolution operations and a path-centric pruning technique to improve robustness and performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b20\": 1,\n    \"b4\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b21\": 0.8,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b17\": 0.6,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of relation extraction between two entities in a sentence by proposing a novel extension of the graph convolutional network (GCN) tailored for this task. It incorporates dependency tree structures and introduces a new path-centric pruning technique to improve performance and efficiency. The proposed model outperforms existing dependency-based neural models and achieves significant speedups.\",\n  \"Direct Inspiration\": [\"b1\", \"b16\", \"b6\"],\n  \"Indirect Inspiration\": [\"b4\", \"b9\", \"b21\", \"b11\"],\n  \"Other Inspiration\": [\"b20\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in relation extraction by proposing a novel extension of the Graph Convolutional Network (GCN) tailored for this task. It introduces a path-centric pruning technique to enhance performance by removing irrelevant information while preserving crucial content. The model is tested on the SemEval 2010 Task 8 and TACRED datasets, showing significant improvements and achieving state-of-the-art performance.\",\n  \"Direct Inspiration\": [\"b1\", \"b16\", \"b6\"],\n  \"Indirect Inspiration\": [\"b3\", \"b11\", \"b4\", \"b9\", \"b21\"],\n  \"Other Inspiration\": [\"b23\", \"b17\", \"b25\"]\n}\n```"], "53e99e61b7602d97027252c8": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency of indirect branch prediction in high-performance processors, which is exacerbated by the use of modern object-oriented languages. The paper introduces the Virtual Program Counter (VPC) prediction algorithm, which leverages existing conditional branch prediction hardware to predict indirect branches without requiring additional hardware resources. This approach aims to provide a low-cost, effective solution to the problem of indirect branch mispredictions.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b24\": 1,\n    \"b20\": 1,\n    \"b5\": 1,\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of predicting indirect branch targets in object-oriented languages, which are essential for supporting polymorphism. The authors propose a novel Virtual Program Counter (VPC) prediction algorithm that leverages existing conditional branch prediction hardware to predict indirect branches, thereby reducing complexity and hardware cost.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b24\": 1.0,\n    \"b20\": 1.0,\n    \"b5\": 1.0,\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b33\": 0.8,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b47\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b37\": 0.6,\n    \"b11\": 0.6,\n    \"b46\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of inefficient indirect branch prediction in high-performance processors, exacerbated by the increased use of object-oriented languages. The proposed solution, Virtual Program Counter (VPC) prediction, uses existing conditional branch prediction hardware to predict indirect branches without requiring additional storage or complex structures, thereby improving performance and reducing energy consumption.\",\n    \"Direct Inspiration\": {\n        \"b10\": 1.0,\n        \"b24\": 1.0,\n        \"b20\": 1.0,\n        \"b5\": 1.0,\n        \"b28\": 1.0\n    },\n    \"Indirect Inspiration\": {},\n    \"Other Inspiration\": {\n        \"b9\": 0.8,\n        \"b33\": 0.8\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of efficiently predicting indirect branch targets in object-oriented programs without increasing complexity or hardware costs. It proposes a novel Virtual Program Counter (VPC) prediction algorithm which uses existing conditional branch prediction hardware to predict indirect branches by treating them as multiple conditional branches.\",\n  \"Direct Inspiration\": [\"b10\", \"b24\", \"b20\", \"b5\", \"b28\"],\n  \"Indirect Inspiration\": [\"b9\", \"b33\", \"b12\", \"b13\", \"b47\"],\n  \"Other Inspiration\": [\"b19\", \"b11\", \"b37\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of inefficient indirect branch prediction in object-oriented programming languages, which affects processor performance. The authors propose the Virtual Program Counter (VPC) prediction algorithm, which uses existing conditional branch prediction hardware to predict indirect branches by treating them as multiple virtual conditional branches.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b24\": 1.0,\n    \"b20\": 1.0,\n    \"b5\": 1.0,\n    \"b28\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b11\": 0.85,\n    \"b33\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b47\": 0.75\n  }\n}\n```"], "53e9a45cb7602d9702d79e76": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving microprocessor performance by optimizing cache memory, specifically comparing the efficiency of inclusive and exclusive cache hierarchies. The authors propose an exclusive cache system, highlighting its potential to increase effective cache size and detailing the implementation of a victim buffer to manage cache misses.\",\n  \"Direct Inspiration\": [\"b4\", \"b2\"],\n  \"Indirect Inspiration\": [\"b3\", \"b1\", \"b5\"],\n  \"Other Inspiration\": [\"b7\", \"b6\", \"b8\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the disparity between microprocessor speed and DRAM memory access latency by leveraging exclusive caching techniques to optimize cache performance. The main contributions include the evaluation of two-level exclusive cache hierarchies in uniprocessor systems and the development of a simulator integrated into the SimpleScalar toolset to assess the performance of exclusive versus inclusive caches.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b7\": 0.7,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the disparity between the improvement rates of microprocessor speed and DRAM memory access latency. It proposes a two-level exclusive cache hierarchy with a victim buffer to maximize effective cache size and improve performance. The paper evaluates this approach against inclusive caches using extensive simulations with various benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the disparity between microprocessor speed and DRAM memory access latency by exploring exclusive and inclusive cache hierarchies. It proposes the use of exclusive caching to maximize effective cache size and compares the performance of exclusive vs. inclusive caches using various benchmarks and configurations.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b2\": 0.9,\n    \"b4\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.65,\n    \"b5\": 0.6,\n    \"b13\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving cache hit rates and overall performance of microprocessors by comparing two-level exclusive and inclusive cache hierarchies. It proposes an exclusive cache system with a victim buffer to optimize performance, and uses simulation techniques to evaluate this system against traditional inclusive caches.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b5\": 0.5,\n    \"b8\": 0.4,\n    \"b12\": 0.4\n  }\n}\n```"], "53e99ae2b7602d97023691f2": ["```json\n{\n    \"Summary\": \"The paper addresses the critical bottleneck of L1 instruction-cache misses in commercial server workloads. It proposes a novel Temporal Instruction Fetch Streaming (TIFS) mechanism inspired by repetitive control flow graph traversals, which predicts L1 instruction-cache miss sequences directly. The challenges addressed include the limitations of existing instruction prefetchers, such as limited instruction prefetch lookahead, inefficiencies due to misprediction, and lack of cache content awareness.\",\n    \"Direct Inspiration\": [\"b6\", \"b7\", \"b20\", \"b29\", \"b36\"],\n    \"Indirect Inspiration\": [\"b23\", \"b4\", \"b30\", \"b31\"],\n    \"Other Inspiration\": [\"b21\", \"b22\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the performance bottleneck caused by L1 instruction-cache misses in commercial server workloads. The novel approach proposed is Temporal Instruction Fetch Streaming (TIFS), which records and predicts recurring sequences of instruction-cache misses, called temporal instruction streams, to prefetch instructions more effectively compared to existing methods. The algorithm leverages the repetitive nature of instruction-cache misses to improve fetch unit performance.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b7\": 0.8,\n    \"b20\": 0.85,\n    \"b29\": 0.9,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b22\": 0.75,\n    \"b26\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.65,\n    \"b23\": 0.7,\n    \"b30\": 0.7,\n    \"b31\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the critical performance bottleneck posed by L1 instruction-cache misses in commercial server workloads. The proposed Temporal Instruction Fetch Streaming (TIFS) algorithm predicts L1 instruction-cache miss sequences directly, inspired by repetitive data prefetching studies. TIFS records recurring instruction-cache miss sequences, called temporal instruction streams, and replays them to predict misses, thereby improving instruction-cache prefetching accuracy, bandwidth efficiency, and lookahead. The evaluation demonstrates significant performance improvements over existing prefetching mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b20\": 0.9,\n    \"b29\": 0.9,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b23\": 0.7,\n    \"b30\": 0.7,\n    \"b31\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b9\": 0.5,\n    \"b15\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the critical performance bottleneck caused by L1 instruction-cache misses in commercial server workloads. It proposes a novel hardware design called Temporal Instruction Fetch Streaming (TIFS) that predicts L1 instruction-cache miss sequences directly, inspired by data prefetching techniques. TIFS aims to eliminate inefficiencies in existing prefetchers that use branch predictions by leveraging repetitive control flow graph traversals.\",\n  \"Direct Inspiration\": [\n    \"b6\",\n    \"b7\",\n    \"b20\",\n    \"b29\",\n    \"b36\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b23\",\n    \"b4\",\n    \"b30\",\n    \"b31\"\n  ],\n  \"Other Inspiration\": [\n    \"b21\",\n    \"b22\",\n    \"b26\",\n    \"b5\",\n    \"b34\",\n    \"b35\",\n    \"b15\",\n    \"b9\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the critical performance bottlenecks caused by L1 instruction-cache misses in commercial server workloads. The proposed solution, Temporal Instruction Fetch Streaming (TIFS), predicts and prefetches recurring instruction-cache miss sequences, improving performance over existing prefetching techniques.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b7\": 1.0,\n    \"b20\": 1.0,\n    \"b29\": 1.0,\n    \"b36\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b23\": 0.7,\n    \"b30\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b26\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```"], "5b67b45517c44aac1c86084b": ["```json\n{\n    \"Summary\": \"The paper addresses challenges in multi-task learning (MTL) for deep neural networks, particularly the sensitivity to task differences and the computational cost of additional parameters. The proposed Multi-gate Mixture-of-Experts (MMoE) model aims to efficiently handle these issues by leveraging task-specific gating networks inspired by the Mixture-of-Experts (MoE) model, improving both model expressiveness and trainability.\",\n    \"Direct Inspiration\": {\n        \"b20\": 1,\n        \"b15\": 0.9,\n        \"b30\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b23\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b14\": 0.6,\n        \"b26\": 0.6,\n        \"b33\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of multi-task learning in deep neural networks, particularly in large-scale recommendation systems. The proposed solution is a Multi-gate Mixture-of-Experts (MMoE) structure, which is inspired by Mixture-of-Experts (MoE) models and aims to model task relationships explicitly while leveraging shared representations without adding many new parameters per task.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b20\": 0.9,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b23\": 0.7,\n    \"b14\": 0.6,\n    \"b33\": 0.6,\n    \"b26\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b27\": 0.5,\n    \"b34\": 0.5,\n    \"b2\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing multiple objectives simultaneously in recommendation systems using a novel Multi-gate Mixture-of-Experts (MMoE) structure. The MMoE model aims to better handle task differences by modeling task relationships explicitly and efficiently, avoiding the need for many additional parameters. The model is inspired by the Mixture-of-Experts (MoE) model and recent advances in MoE layers. Experimental results show that MMoE outperforms baseline methods, especially when tasks have low correlation, and improves both model expressiveness and trainability.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b15\": 0.9,\n    \"b30\": 0.9,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b26\": 0.7,\n    \"b33\": 0.7,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b18\": 0.5,\n    \"b2\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing multiple objectives simultaneously in large-scale recommendation systems using multi-task learning models. It proposes a novel Multi-gate Mixture-of-Experts (MMoE) structure inspired by the Mixture-of-Experts (MoE) model and recent MoE layer techniques to effectively capture task relationships and improve model performance.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b15\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b14\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a novel Multi-gate Mixture-of-Experts (MMoE) model to address the challenges in multi-task learning, particularly the task differences and the sensitivity of DNN-based models to such differences. The approach is inspired by the Mixture-of-Experts (MoE) model and aims to automatically allocate parameters to capture shared or task-specific information without adding many new parameters per task.\",\n  \"Direct Inspiration\": [\"b20\", \"b15\", \"b30\"],\n  \"Indirect Inspiration\": [\"b7\", \"b23\"],\n  \"Other Inspiration\": [\"b14\", \"b26\", \"b33\"]\n}\n```"], "53e9a9a9b7602d97033084f4": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of ultra-low latency, high bandwidth, and energy-efficient communication in on-chip networks. It proposes a novel flow control and router microarchitecture design called Express Virtual Channels (EVCs), which allow packets to bypass intermediate routers, thereby reducing delay, energy consumption, and improving throughput.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b9\": 0.9,\n    \"b10\": 0.85,\n    \"b11\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b14\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.65,\n    \"b17\": 0.6,\n    \"b15\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in on-chip networks, focusing on reducing latency, energy consumption, and enhancing throughput. It introduces Express Virtual Channels (EVCs) to bridge the performance gap between packet-switched networks and ideal interconnection fabrics by allowing packets to bypass intermediate routers, thus reducing contention and improving efficiency.\",\n\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b9\": 1.0,\n    \"b10\": 1.0\n  },\n\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.7\n  },\n\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the high latency, energy consumption, and complexity of state-of-the-art packet-switched on-chip networks. The authors propose Express Virtual Channels (EVCs) to virtually bypass intermediate routers, simulating the performance and energy efficiency of dedicated links while using shared physical channels.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.8,\n    \"b11\": 0.85,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.55,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the high energy, latency, and area overheads of current state-of-the-art packet-switched on-chip networks. The proposed solution, Express Virtual Channels (EVCs), allows packets to virtually bypass intermediate routers, thus reducing packet latency, energy consumption, and improving throughput.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of latency, energy consumption, and throughput in on-chip networks. The proposed solution is Express Virtual Channels (EVCs), which bypass intermediate routers to improve performance and efficiency. Both static and dynamic EVCs are introduced to enhance adaptability and reduce contention.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b9\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b18\": 0.5\n  }\n}\n```"], "5f0ed12691e011ead96652e9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of semi-supervised multi-modality learning, where each modality of data may only contain a small proportion of labeled data. The authors propose a novel information-theoretic approach called Total Correlation Gain Maximization (TCGM) to leverage unlabeled data. The TCGM framework maximizes the Total Correlation Gain among all modalities to infer ground-truth labels, outperforming existing methods by effectively utilizing information across all modalities.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b24\": 0.7,\n    \"b26\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of semi-supervised multi-modality learning, where annotating large-scale training data is expensive and time-consuming, leading to a small proportion of labeled data. The proposed algorithm, Total Correlation Gain Maximization (TCGM), maximizes the Total Correlation Gain (TCG) among all modalities to discover the information intersection (ground truth) across modalities. This method leverages both labeled and unlabeled data and demonstrates superior performance over baseline methods in various tasks.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.9,\n    \"b26\": 0.9,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7,\n    \"b11\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of semi-supervised multi-modal learning, proposing a novel information-theoretic approach (TCGM) to effectively utilize information across multiple modalities by maximizing Total Correlation Gain. The goal is to train accurate classifiers that can leverage both labeled and unlabeled data, with the proposed method showing superior performance across various tasks such as news classification, emotion recognition, and disease prediction.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b24\": 0.8,\n    \"b26\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b6\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of semi-supervised multi-modality learning, particularly in scenarios where large-scale labeled data is scarce and expensive to annotate. It proposes a novel information-theoretic framework called Total Correlation Gain Maximization (TCGM) to maximize the Total Correlation Gain (TCG) among different modalities, aiming to infer the ground-truth labels by discovering the information intersection among these modalities. The TCGM framework exploits both labeled and unlabeled data to train classifiers and prove that the optimal classifiers are equivalent to Bayesian posterior classifiers given each modality.\",\n  \"Direct Inspiration\": {\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b24\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b11\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of semi-supervised multi-modality learning, specifically how to effectively train classifiers using a combination of labeled and unlabeled data across different modalities. The authors propose a novel framework called Total Correlation Gain Maximization (TCGM), which leverages the concept of Total Correlation (TC) to identify the information intersection among modalities and improve classification accuracy.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b11\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b26\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```"], "5f1022a091e01168a7d6fc4f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving diagnostic accuracy in reading chest X-rays by proposing a novel module called Attend-and-Compare Module (ACM) that mimics the way radiologists compare regions in X-rays. The ACM module explicitly compares features of different regions to enhance recognition performance. The module is tested on multiple datasets, demonstrating superior performance in both chest X-ray and natural image tasks.\",\n  \"Direct Inspiration\": {\n    \"b35\": 0.9,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b37\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are the heavy workload of radiologists reading chest X-rays and the significant error rate in diagnoses. The proposed solution is a novel Attend-and-Compare Module (ACM) that mimics the way radiologists read X-rays by comparing different regions. This method enhances diagnostic accuracy by explicitly modeling the comparison process radiologists use during diagnosis.\",\n    \"Direct Inspiration\": {\n        \"b35\": 0.9,\n        \"b14\": 0.85,\n        \"b3\": 0.8,\n        \"b37\": 0.75,\n        \"b30\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b40\": 0.65,\n        \"b34\": 0.6,\n        \"b21\": 0.55\n    },\n    \"Other Inspiration\": {\n        \"b36\": 0.5,\n        \"b23\": 0.5,\n        \"b13\": 0.45,\n        \"b15\": 0.45,\n        \"b39\": 0.45\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the recognition performance of thoracic diseases in chest X-rays by modeling the way radiologists read X-rays. The proposed novel method, Attend-and-Compare Module (ACM), mimics the radiologists' comparison of different regions within X-rays to enhance feature extraction and improve diagnostic accuracy.\",\n  \"Direct Inspiration\": [\"b34\", \"b14\", \"b3\", \"b35\"],\n  \"Indirect Inspiration\": [\"b40\", \"b24\", \"b21\", \"b37\", \"b30\"],\n  \"Other Inspiration\": [\"b11\", \"b16\", \"b41\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving diagnostic accuracy in chest X-ray interpretation by radiologists, who often face high workloads and significant error rates. The authors propose a novel Attend-and-Compare Module (ACM) that mirrors the comparison process used by radiologists when reading X-rays, explicitly comparing different regions within an image to enhance recognition performance.\",\n  \"Direct Inspiration\": [\"b35\", \"b30\"],\n  \"Indirect Inspiration\": [\"b14\", \"b3\", \"b16\"],\n  \"Other Inspiration\": [\"b36\", \"b17\", \"b18\", \"b2\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of misdiagnosis in chest X-rays due to high workloads for radiologists. It proposes a novel Attend-and-Compare Module (ACM) inspired by how radiologists compare regions in X-rays to improve diagnostic accuracy. The ACM explicitly compares different regions in a data-driven way to enhance recognition performance.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b30\": 1.0,\n    \"b14\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b34\": 0.7,\n    \"b37\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b31\": 0.6,\n    \"b10\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```"], "53e9ae9cb7602d97038be67d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving instruction-level parallelism by parallelizing the front-end stages of the processing pipeline, specifically focusing on the fetch and rename units. The proposed algorithm involves using multiple sequencers to fetch instructions in parallel, improving throughput without the limitations of sequential fetch mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b7\": 1.0,\n    \"b10\": 1.0,\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of increasing instruction-level parallelism in processor front-ends by proposing techniques to parallelize the instruction fetching and renaming stages. The proposed parallel front-end aims to achieve higher throughput and better overall performance compared to high-performance sequential front-ends. The novel method involves using multiple sequencers to fetch different fragments of the program in parallel, thereby overcoming the limitations of sequential fetch mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the limitations of sequential front-ends in CPU architecture, particularly in instruction fetching, decoding, and renaming. The proposed algorithm introduces techniques to parallelize the front-end stages of the processing pipeline, specifically focusing on parallel fetch and rename units. These techniques aim to achieve better performance and address the inefficiencies of sequential fetch units by using multiple sequencers and fragment buffers. The paper evaluates the proposed design and highlights its advantages over traditional sequential front-ends.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b20\": 0.85,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b19\": 0.75,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving the instruction-level parallelism in processor front-ends, which have traditionally remained sequential despite parallel processing in back-ends. The authors propose techniques to parallelize the front-end, specifically focusing on the fetch and rename units, to achieve better performance than known high-performance sequential front-ends.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving instruction-level parallelism in processor front-ends, proposing a parallel fetch and rename unit to enhance front-end performance, which traditionally relies on sequential processes.\",\n  \"Direct Inspiration\": [\"b2\", \"b10\", \"b12\", \"b20\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b6\", \"b7\", \"b13\", \"b15\", \"b19\"]\n}\n```"], "5e539eca3a55ac4db70a53f1": ["```json\n{\n  \"Summary\": \"The paper presents a general framework for learning simulation from data using Graph Network-based Simulators (GNS). The primary challenges addressed include the high computational cost and effort required for traditional simulators, and the difficulty of scaling and approximating complex physics with machine learning. The proposed algorithm uses graphs to represent physical states and learned message-passing to approximate dynamics, achieving accurate simulations across various physical systems.\",\n  \"Direct Inspiration\": [\"b2\", \"b17\", \"b33\"],\n  \"Indirect Inspiration\": [\"b6\", \"b8\", \"b10\", \"b12\", \"b19\"],\n  \"Other Inspiration\": [\"b1\", \"b3\", \"b11\", \"b26\", \"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating efficient and general-purpose simulators for complex physical systems using machine learning. The proposed framework, Graph Network-based Simulators (GNS), represents physical states as graphs of interacting particles and approximates dynamics through learned message-passing among nodes. The GNS model demonstrates the ability to accurately simulate diverse physical systems and generalizes well across various scenarios.\",\n  \"Direct Inspiration\": [\"b17\", \"b33\", \"b2\"],\n  \"Indirect Inspiration\": [\"b1\", \"b4\", \"b22\", \"b25\"],\n  \"Other Inspiration\": [\"b8\", \"b9\", \"b16\", \"b35\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of creating realistic, scalable, and efficient simulators for complex physical systems using machine learning, specifically through a Graph Network-based Simulator (GNS) framework. This framework leverages graph-based representations and message-passing algorithms to model interactions between particles in diverse physical systems such as fluids, rigid solids, and deformable materials.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1.0,\n        \"b17\": 0.9,\n        \"b33\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.8,\n        \"b6\": 0.8,\n        \"b19\": 0.8,\n        \"b3\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.7,\n        \"b10\": 0.7,\n        \"b27\": 0.7,\n        \"b26\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating realistic simulators for complex physical systems, which are traditionally expensive and computationally intensive. The authors propose a Graph Network-based Simulator (GNS) framework that uses machine learning to train simulators directly from observed data. The GNS framework represents physical states as graphs of interacting particles, and complex dynamics are approximated through learned message-passing among nodes. The model can handle a wide range of physical systems and generalizes well beyond its training parameters.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b17\": 0.9,\n    \"b33\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b4\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper presents a general framework for learning simulation from data using Graph Network-based Simulators (GNS). It addresses the challenges of building traditional simulators which are computationally expensive, require substantial effort, and often trade-off generality for accuracy. The proposed GNS framework uses machine learning to train simulators directly from observed data, representing physical states with graphs of interacting particles and learning complex dynamics through message-passing among nodes.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1.0,\n        \"b17\": 1.0,\n        \"b33\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.8,\n        \"b3\": 0.8,\n        \"b11\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.5,\n        \"b13\": 0.5,\n        \"b9\": 0.5\n    }\n}\n```"], "5f3268fb91e011bc1612aeab": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating emotional talking faces from speech, conditioned on categorical emotions to provide direct and flexible control over visual emotion expression. The proposed method leverages a neural network system with a GAN framework, incorporating various discriminative losses to improve image quality and emotion expression.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b8\": 0.7,\n    \"b10\": 0.7,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating emotional talking faces from speech, conditioned on categorical emotions. The proposed method decouples speech and emotion conditions to provide flexible control over visual emotion expressions, enabling more personalized applications and facilitating behavioral psychology experiments.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b5\": 0.75,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.65,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating emotional talking faces from speech while providing independent control over the visual emotion expression. It proposes a neural network system that generates talking faces conditioned on categorical emotions, allowing for more personalized and flexible applications. The system employs a generative adversarial network (GAN) framework and introduces an emotion discriminative loss to classify rendered visual emotions.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b16\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b11\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the generation of talking face videos from speech that can express categorical emotions, providing a more flexible and independent control of visual emotion expressions. The proposed algorithm includes a neural network system that takes speech, a reference face image, and a categorical emotion condition as inputs to generate a talking face video with emotional expressions. The method introduces an emotion discriminative loss to classify rendered visual emotions and decouples speech and emotion conditions to allow independent manipulation of emotions during video generation.\",\n  \"Direct Inspiration\": [\"b20\"],\n  \"Indirect Inspiration\": [\"b16\", \"b21\"],\n  \"Other Inspiration\": [\"b4\", \"b5\", \"b11\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating emotional talking faces from speech, conditioned on categorical emotions to provide independent control of emotion expression. This is aimed at improving the realism and flexibility of automatic talking face generation systems, which have applications in various fields such as entertainment, education, and healthcare. The proposed neural network system includes a generator that accepts a speech utterance, a reference face image, and a categorical emotion input, and produces a synchronized talking face with the desired emotional expressions. Key contributions include a new method for emotion-conditioned talking face generation, an emotion discriminative loss for classifying visual emotions, and a pilot study on human emotion perception in mismatched audio-visual emotion conditions.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b21\": 0.8,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b8\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```"], "5ef0816891e0112aee042b88": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality samples using diffusion probabilistic models. It introduces a new parameterization that reveals an equivalence with denoising score matching and annealed Langevin dynamics, leading to improved sample quality. The paper also presents a simplified training objective and an efficient sampling procedure.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b51\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b49\": 0.9,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b19\": 0.7,\n    \"b31\": 0.6,\n    \"b48\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of demonstrating that diffusion probabilistic models are capable of generating high-quality samples, comparable or superior to other generative models. The proposed algorithm involves a parameterized Markov chain trained using variational inference, with a specific focus on a novel parameterization that reveals equivalences with denoising score matching and annealed Langevin dynamics. The primary contributions include demonstrating the sample quality of diffusion models and an analysis of their performance in terms of lossy compression.\",\n  \"Direct Inspiration\": {\n    \"b51\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b49\": 0.9,\n    \"b57\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality samples using diffusion probabilistic models. It demonstrates that these models can produce samples comparable to or better than other generative models. The key contributions include a parameterization revealing an equivalence with denoising score matching and annealed Langevin dynamics, and the development of a simplified training objective that improves sample quality.\",\n  \"Direct Inspiration\": [\"b10\", \"b51\"],\n  \"Indirect Inspiration\": [\"b49\", \"b30\", \"b13\"],\n  \"Other Inspiration\": [\"b62\", \"b56\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of demonstrating that diffusion probabilistic models can generate high-quality samples, which was previously not shown. The authors propose a specific parameterization of diffusion models that reveals an equivalence with denoising score matching and annealed Langevin dynamics, leading to improved sample quality.\",\n    \"Direct Inspiration\": {\n        \"b10\": 1.0,\n        \"b51\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b49\": 0.8,\n        \"b57\": 0.7,\n        \"b15\": 0.7,\n        \"b19\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b48\": 0.6,\n        \"b44\": 0.6,\n        \"b13\": 0.6,\n        \"b62\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-quality samples using diffusion probabilistic models. It proposes a parameterized Markov chain trained via variational inference, demonstrating that diffusion models can achieve sample quality comparable or superior to other generative models. The authors establish an equivalence with denoising score matching and annealed Langevin dynamics, which they identify as a primary contribution.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b51\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b49\": 0.8,\n    \"b13\": 0.7,\n    \"b57\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b62\": 0.6,\n    \"b48\": 0.6,\n    \"b15\": 0.5\n  }\n}\n```"], "5f61e88391e011fae8fd6b13": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of using large language models (LMs) like GPT-3, which require enormous computational resources and have limited scalability due to small context windows. The proposed solution modifies pattern-exploiting training (PET) to work for tasks requiring multi-token predictions, demonstrating superior performance with fewer parameters. The work combines PET with ALBERT and iteratively improves the training process through knowledge distillation.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b15\": 0.7,\n    \"b27\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of using pre-trained language models (LMs) for few-shot learning scenarios, particularly overcoming the limitations of GPT-3's large model size and context window constraints. It proposes modifications to Pattern-Exploiting Training (PET) to handle multi-token predictions and shows that these modifications outperform GPT-3 on the SuperGLUE benchmark with significantly fewer parameters.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.9,\n    \"b7\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b11\": 0.7,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the limitation of existing methods for using large language models (LMs) in few-shot learning scenarios, such as GPT-3's requirement for enormous computational resources and the inability of Pattern-Exploiting Training (PET) to handle tasks requiring multi-token predictions. The paper introduces a modified version of PET that can work with multi-token predictions and demonstrates its efficiency and effectiveness compared to GPT-3, using significantly fewer parameters. This is achieved by leveraging masked language models and refining the PET approach for better performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b15\": 0.9,\n    \"b26\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b27\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are the limitations of priming methods, such as the need for massive language models (LMs) and the inability to scale to more examples due to context window limitations. The proposed algorithm, a modified version of Pattern-Exploiting Training (PET), addresses these issues by allowing tasks that require predicting more than one token and demonstrating strong performance with significantly fewer parameters. The paper also highlights the importance of combining multiple task formulations and leveraging unlabeled data.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b30\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.7,\n        \"b15\": 0.7,\n        \"b27\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.5,\n        \"b35\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the limitations of GPT-3's priming method, which requires a large language model and does not scale well with more than a few examples. The paper proposes modifications to the Pattern-Exploiting Training (PET) method to support tasks requiring predictions of more than one token. The approach significantly improves performance on the SuperGLUE benchmark while using only a fraction of the parameters required by GPT-3.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.9,\n    \"b15\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b35\": 0.7\n  }\n}\n```"], "5de4e0b73a55ac2224ba53a6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the accuracy and breadth of knowledge retrieval from language models (LMs) using automatic prompt generation methods. The authors propose two methods: mining-based generation and paraphrasing-based generation, and also investigate lightweight ensemble methods to combine the answers from different prompts. The goal is to better query the knowledge stored in LMs, thereby increasing the accuracy of knowledge extraction.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b32\": 0.8,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.6,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the accuracy and breadth of knowledge retrieval from language models (LMs) by generating more effective prompts. The authors propose two main methods for prompt generation: mining-based methods and paraphrasing-based methods. They also introduce ensemble techniques to combine the effectiveness of multiple prompts.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b5\": 0.7,\n    \"b31\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately retrieving knowledge from language models (LMs) by improving the prompts used to query the LMs. The proposed solutions include mining-based and paraphrasing-based methods for prompt generation and ensemble methods for combining multiple prompts to enhance accuracy.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b32\": 0.7,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the effectiveness of language models (LMs) in retrieving factual knowledge through more effective prompt generation. The proposed methods include mining-based and paraphrasing-based prompt generation techniques, as well as ensemble methods to combine multiple prompts for better accuracy.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1.0,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively querying language models (LMs) to retrieve factual knowledge. It proposes two methods for generating prompts: mining-based and paraphrasing-based. The paper also explores ensemble methods to combine multiple prompts for better accuracy.\",\n  \"Direct Inspiration\": {\n    \"b29\": 1,\n    \"b34\": 0.9,\n    \"b31\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.75,\n    \"b32\": 0.75,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```"], "5fef1dfc91e0113b265a0220": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of few-shot learning in natural language processing (NLP) by leveraging prompt-based fine-tuning of smaller, moderately-sized language models like BERT and RoBERTa. Inspired by GPT-3's few-shot capabilities, the authors propose novel strategies such as automatic prompt generation using the T5 model and refined demonstration incorporation techniques to improve performance in both classification and regression tasks.\",\n  \"Direct Inspiration\": [\"b28\", \"b29\", \"b7\", \"b33\"],\n  \"Indirect Inspiration\": [\"b30\", \"b41\", \"b6\"],\n  \"Other Inspiration\": [\"b19\", \"b12\", \"b16\", \"b23\", \"b45\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of leveraging smaller language models like BERT and RoBERTa in few-shot learning scenarios, inspired by GPT-3's few-shot capabilities. It proposes novel strategies such as prompt-based prediction, automatic prompt generation using the T5 model, and refined demonstration incorporation techniques to improve few-shot learning performance for both classification and regression tasks.\",\n  \"Direct Inspiration\": {\n    \"1\": \"b7\",\n    \"2\": \"b28\",\n    \"3\": \"b29\",\n    \"4\": \"b33\"\n  },\n  \"Indirect Inspiration\": {\n    \"1\": \"b30\",\n    \"2\": \"b38\",\n    \"3\": \"b26\",\n    \"4\": \"b10\",\n    \"5\": \"b37\"\n  },\n  \"Other Inspiration\": {\n    \"1\": \"b11\",\n    \"2\": \"b21\",\n    \"3\": \"b19\",\n    \"4\": \"b35\",\n    \"5\": \"b17\",\n    \"6\": \"b12\",\n    \"7\": \"b20\",\n    \"8\": \"b16\",\n    \"9\": \"b23\",\n    \"10\": \"b45\",\n    \"11\": \"b0\",\n    \"12\": \"b2\",\n    \"13\": \"b27\",\n    \"14\": \"b41\",\n    \"15\": \"b6\"\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enabling few-shot learning in smaller language models like BERT or RoBERTa, inspired by GPT-3's few-shot capabilities. It introduces novel strategies for prompt-based prediction, automatic prompt generation using the T5 model, and refined demonstration incorporation. The focus is on making these methods practical and effective for both classification and regression tasks with minimal annotated data.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b28\": 1,\n    \"b29\": 1,\n    \"b33\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.8,\n    \"b21\": 0.8\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of applying few-shot learning in natural language processing with moderately-sized language models, such as BERT and RoBERTa. It proposes novel strategies like automatic prompt generation using T5, incorporating demonstrations in inputs, and refined sampling strategies to enhance few-shot learning performance.\",\n    \"Direct Inspiration\": {\n        \"b28\": 0.9,\n        \"b29\": 0.9,\n        \"b7\": 0.8,\n        \"b33\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.7,\n        \"b21\": 0.7,\n        \"b30\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.6,\n        \"b34\": 0.65,\n        \"b35\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of applying few-shot learning to moderately-sized language models like BERT and RoBERTa, as opposed to the massive GPT-3. The authors propose novel strategies for prompt-based fine-tuning and automatic prompt generation to improve model performance in few-shot settings.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b29\": 1,\n    \"b7\": 1,\n    \"b33\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b21\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.7,\n    \"b26\": 0.7,\n    \"b10\": 0.7,\n    \"b37\": 0.7\n  }\n}\n```"], "5d3ed25a275ded87f97deae1": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing the robustness of Graph Convolutional Networks (GCNs) against adversarial attacks. The authors propose a novel model called Robust GCN (RGCN) that uses Gaussian distributions as hidden representations of nodes and a variance-based attention mechanism to mitigate the effects of adversarial attacks.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b36\": 1,\n    \"b0\": 0.9,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b9\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing the robustness of Graph Convolutional Networks (GCNs) against adversarial attacks. The proposed solution, Robust GCN (RGCN), introduces Gaussian distributions for node representations and a variance-based attention mechanism to mitigate the effects of adversarial perturbations.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b17\": 0.9,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b35\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of enhancing the robustness of Graph Convolutional Networks (GCNs) against adversarial attacks. The proposed algorithm, Robust GCN (RGCN), introduces Gaussian distributions for node representations and a variance-based attention mechanism to mitigate the propagation of adversarial attacks.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1.0,\n        \"b35\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b17\": 0.8,\n        \"b36\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.7,\n        \"b9\": 0.7,\n        \"b34\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing the robustness of Graph Convolutional Networks (GCNs) against adversarial attacks. The proposed solution, Robust GCN (RGCN), uses Gaussian distributions for node representations and introduces a variance-based attention mechanism to mitigate the effects of adversarial attacks.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b36\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.9,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b0\": 0.7,\n    \"b35\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing the robustness of Graph Convolutional Networks (GCNs) against adversarial attacks. The authors propose a novel model called Robust GCN (RGCN) that uses Gaussian distributions for node representations and introduces a variance-based attention mechanism to mitigate the propagation of adversarial effects.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b36\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b17\": 0.8,\n    \"b9\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b0\": 0.6\n  }\n}\n```"], "5a260c3b17c44a4ba8a26007": ["```json\n{\n  \"Summary\": \"The paper identifies challenges in value prediction due to store instructions affecting predictor accuracy and proposes a novel Decoupled Load Value Prediction (DLVP) approach. DLVP predicts memory addresses instead of values to mitigate negative interactions between stores and value prediction. A Path-based Address Prediction (PAP) scheme is introduced to achieve high accuracy with less stringent confidence requirements.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b9\": 0.8,\n    \"b13\": 0.8,\n    \"b24\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in value prediction: (1) the issue of value mispredictions due to stale data in value predictors caused by store instructions, and (2) the stringent high-confidence requirements for value prediction accuracy. It proposes a novel approach called Decoupled Load Value Prediction (DLVP), which uses Path-based Address Prediction (PAP) to predict memory addresses instead of values, mitigating the negative interactions between stores and value prediction and providing higher accuracy with fewer occurrences.\",\n  \"Direct Inspiration\": [\"b26\", \"b27\"],\n  \"Indirect Inspiration\": [\"b2\", \"b18\", \"b24\", \"b35\"],\n  \"Other Inspiration\": [\"b9\", \"b12\", \"b13\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in value prediction: (1) the negative interactions between stores and value predictors due to stale values, and (2) the high performance cost of value mispredictions due to stringent confidence requirements. The proposed solution, Decoupled Load Value Prediction (DLVP), leverages Path-based Address Prediction (PAP) to predict memory addresses instead of values, thereby mitigating the identified challenges and improving prediction accuracy and efficiency.\",\n  \"Direct Inspiration\": [\"b2\", \"b26\", \"b27\"],\n  \"Indirect Inspiration\": [\"b24\", \"b35\"],\n  \"Other Inspiration\": [\"b10\", \"b12\", \"b18\", \"b30\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses two main challenges in value prediction for load instructions: stale values due to store instructions and high confidence requirements for prediction accuracy. It proposes a novel approach called Decoupled Load Value Prediction (DLVP), which uses Path-based Address Prediction (PAP) to predict memory addresses instead of values, aiming to improve prediction accuracy and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b26\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b24\": 0.6,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in value prediction for single-thread and multi-thread applications: handling store-induced stale values in value predictors and ensuring high prediction accuracy with less stringent confidence requirements. The proposed solution, Decoupled Load Value Prediction (DLVP), uses Path-based Address Prediction (PAP) to predict load memory addresses instead of values, thus mitigating negative interactions between stores and value prediction.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b12\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b35\": 0.7,\n    \"b10\": 0.7\n  }\n}\n```"], "5c9df4643cb210d271bea0dd": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of increasing the effectiveness of load value prediction in microarchitectures to break true data dependencies. The authors propose a composite predictor using state-of-the-art load value predictors, improving its performance through techniques such as Accuracy Monitor, heterogeneous table sizes, smart training algorithms, and dynamic table fusion. The composite predictor significantly outperforms individual predictors and the winner of the first championship value prediction.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b3\": 1.0,\n    \"b6\": 1.0,\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b5\": 0.8,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5,\n    \"b24\": 0.5,\n    \"b26\": 0.5,\n    \"b27\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in increasing the effectiveness of load value prediction in microarchitectures, which is limited by true data dependencies. The proposed solution involves a composite load value predictor that combines state-of-the-art predictors with various optimizations such as an Accuracy Monitor, heterogeneous table sizes, a smart training algorithm, and dynamic fusion of predictor tables.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b3\": 1,\n    \"b6\": 1,\n    \"b7\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of increasing the effectiveness of load value prediction in microarchitectures. It proposes a composite predictor that combines several state-of-the-art predictors and introduces optimizations like an Accuracy Monitor, heterogeneous table sizes, a smart training algorithm, and dynamic table fusion to improve prediction accuracy and performance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b3\": 1,\n    \"b6\": 1,\n    \"b7\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b25\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of true data dependencies in programs that limit performance, and proposes a composite load value predictor to enhance the effectiveness of value prediction. This composite predictor integrates four state-of-the-art predictors and incorporates several optimizations including an Accuracy Monitor, heterogeneous table sizes, a smart training algorithm, and dynamic table fusion.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b3\": 1.0,\n    \"b6\": 1.0,\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving load value prediction in microarchitectures to break true data dependencies and enhance performance. It proposes a composite predictor that combines four state-of-the-art load value predictors (LVP, SAP, CVP, CAP) and introduces several optimizations to improve accuracy and resource utilization.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b3\": 1,\n    \"b6\": 1,\n    \"b7\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b25\": 0.7\n  }\n}\n```"], "5d1eb9d5da562961f0b0fa03": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of adapting the whole word masking strategy in Chinese BERT to improve its performance on various Chinese NLP tasks. The authors propose a method to enhance BERT by using whole word masking, inspired by the original BERT model and ERNIE.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b7\": 0.8,\n    \"b22\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b17\": 0.7,\n    \"b3\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of applying whole word masking to Chinese BERT models to improve their performance on various NLP tasks. The authors adapt the whole word masking strategy to Chinese BERT, conduct extensive experiments, and provide useful tips for using these pre-trained models.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b8\", \"b19\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b14\", \"b17\", \"b3\", \"b10\", \"b11\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b5\", \"b7\", \"b22\", \"b16\", \"b20\", \"b21\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are adapting BERT's whole word masking strategy for Chinese language processing and verifying its effectiveness across various NLP tasks. The authors propose pre-training models using the whole word masking strategy on Chinese text and conduct extensive experiments to demonstrate the improved performance over BERT and ERNIE.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1, \n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b7\": 0.8,\n    \"b22\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b14\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing the BERT model using Whole Word Masking (WWM) for Chinese language processing. The authors adapt the WWM strategy from BERT to improve performance on Chinese NLP tasks, demonstrating its effectiveness through extensive experiments.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b8\", \"b19\"],\n    \"confidence_scores\": {\n      \"b8\": 1.0,\n      \"b19\": 0.9\n    }\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b10\", \"b14\", \"b17\", \"b20\"],\n    \"confidence_scores\": {\n      \"b10\": 0.7,\n      \"b14\": 0.8,\n      \"b17\": 0.8,\n      \"b20\": 0.7\n    }\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b3\", \"b11\", \"b16\"],\n    \"confidence_scores\": {\n      \"b3\": 0.6,\n      \"b11\": 0.6,\n      \"b16\": 0.6\n    }\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Chinese pre-trained language models by adapting the whole word masking strategy in BERT. It demonstrates the effectiveness of this approach through extensive experiments on various Chinese NLP tasks, providing new pre-trained models and useful tips for researchers.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b7\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b17\": 0.7,\n    \"b3\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7\n  }\n}\n```"], "53e99ab2b7602d970232a35e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of handling memory dependences in scalable computer architectures without compromising programmability or parallelism. It introduces two novel techniques: unordered late binding and lightweight overflow control, to design an LSQ that is more power and area-efficient, and scalable to thousands of in-flight instructions.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b12\": 1,\n    \"b14\": 1,\n    \"b24\": 1,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b10\": 0.9,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b8\": 0.7,\n    \"b6\": 0.6,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scalable memory dependence handling in computer architecture, specifically focusing on improving LSQ designs to support large-window microarchitectures efficiently. It introduces two novel techniques, unordered late binding and lightweight overflow control, to achieve power-efficient scaling for LSQs.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b12\": 0.9,\n    \"b14\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b10\": 0.8,\n    \"b22\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b19\": 0.7,\n    \"b16\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing Load/Store Queues (LSQs) that can handle memory dependences in a scalable manner without sacrificing programmability or parallelism. The proposed algorithm introduces two novel techniques: unordered late binding and lightweight overflow control, which allow LSQs to scale to thousands of instructions power efficiently.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b14\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b23\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scalable memory dependence handling without compromising on programmability or parallelism. It proposes an LSQ (Load/Store Queue) design that improves power and area efficiency for superscalar processors and scales to thousands of instructions using two key techniques: unordered late binding and lightweight overflow control.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b14\": 1,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b23\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of scalable memory dependence handling in computer architecture without compromising programmability or parallelism. The proposed algorithm involves an LSQ design featuring unordered late binding and lightweight overflow control, enabling efficient scaling to thousands of instructions.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b7\", \"b12\", \"b14\", \"b22\", \"b24\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b13\", \"b20\", \"b27\", \"b6\", \"b4\", \"b28\", \"b5\", \"b3\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b1\", \"b10\", \"b19\", \"b8\", \"b23\", \"b15\", \"b16\", \"b26\"]\n    }\n}\n```"], "5582ccf80cf2a81de266524a": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is increasing the instruction fetch bandwidth in modern processors, particularly beyond eight instructions per cycle. The proposed solution is a high-bandwidth fetch mechanism that fetches a small number of contiguous instructions from multiple points in the program using multiple narrow instruction sequencers, as opposed to fetching a large number from a single point.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b15\": 0.8,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of increasing instruction fetch bandwidth in modern processors without complicating the branch predictor or instruction cache. The authors propose a high-bandwidth fetch mechanism that fetches a small number of contiguous instructions from multiple points in the program, using multiple narrow instruction sequencers instead of one wide sequencer. This approach is inspired by the trace cache concept but aims for a more efficient and flexible instruction fetch mechanism.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.95,\n    \"b15\": 0.9,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.75,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.65,\n    \"b6\": 0.6,\n    \"b16\": 0.55\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of increasing instruction fetch bandwidth beyond eight instructions per cycle in modern processors. The proposed solution involves using multiple narrow instruction sequencers to fetch instructions from multiple points in the program concurrently, rather than fetching a large number of contiguous instructions from a single point. This method enhances fetch efficiency by reducing wasted fetch slots due to branches and cache-line boundaries, and it allows for more flexible use of the fetch unit in multithreaded processors.\",\n    \"Direct Inspiration\": {\n        \"b19\": 0.9,\n        \"b7\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.75,\n        \"b15\": 0.75,\n        \"b4\": 0.7,\n        \"b9\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b6\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Increasing instruction fetch bandwidth beyond eight instructions per cycle\",\n      \"Problems with fetching instructions from multiple points in the program\",\n      \"Inefficiency of wide fetch mechanisms for out-of-order processors\",\n      \"Need for a high-bandwidth fetch mechanism that fetches instructions concurrently from multiple points\"\n    ],\n    \"Proposed Algorithm\": \"A fetch mechanism that uses multiple narrow instruction sequencers to fetch multiple traces concurrently, optimizing bandwidth use and instruction supply delays.\"\n  },\n  \"Direct Inspiration\": {\n    \"b7\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b15\": 0.8,\n    \"b19\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b9\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of increasing the instruction fetch bandwidth in modern processors. The authors propose a high-bandwidth fetch mechanism that fetches a small number of contiguous instructions from multiple points in the program using multiple narrow instruction sequencers instead of one wide sequencer. This approach is intended to be more efficient for out-of-order processors by reducing wasted fetch slots and improving cache locality.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b10\": 0.9,\n    \"b15\": 0.9,\n    \"b6\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.6,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b9\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```"], "53e9b413b7602d9703f0894b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of data dependence limiting instruction-level parallelism (ILP) in superscalar microprocessors. It introduces a hardware-based mechanism for dynamic, cycle-by-cycle tracking of data dependences among in-flight instructions, utilizing a Data Dependence Table (DDT). The novel ARVI branch predictor is proposed, leveraging partial register values along data dependence chains for improved branch prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b10\": 0.9,\n    \"b25\": 0.95,\n    \"b28\": 0.85,\n    \"b29\": 0.85,\n    \"b2\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b11\": 0.75,\n    \"b21\": 0.8,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.4,\n    \"b18\": 0.5,\n    \"b34\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in exploiting instruction-level parallelism (ILP) due to data dependencies in microprocessors. It proposes a hardware-based mechanism for cycle-by-cycle tracking of data dependencies among in-flight instructions, which is utilized for various applications including dynamic scheduling, value prediction, and a novel branch prediction scheme called ARVI.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.9,\n    \"b21\": 0.8,\n    \"b5\": 0.75,\n    \"b2\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.6,\n    \"b29\": 0.6,\n    \"b10\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b32\": 0.45,\n    \"b1\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the limitations of instruction-level parallelism (ILP) due to data dependences, and the need for efficient data dependence tracking mechanisms in microprocessors. The paper proposes hardware-based mechanisms for cycle-by-cycle tracking of data dependences among in-flight instructions in a dynamic superscalar microprocessor. The novel contributions include the Data Dependence Table (DDT) for maintaining data dependence chains and the ARVI branch prediction scheme based on these chains.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b5\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.8,\n    \"b18\": 0.75,\n    \"b28\": 0.7,\n    \"b29\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b32\": 0.55,\n    \"b21\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are related to the limitations in instruction-level parallelism (ILP) due to data dependences, and the need for accurate and efficient hardware-based mechanisms for tracking data dependences among in-flight instructions in a dynamic superscalar microprocessor. The novel contributions include the development of the Data Dependence Table (DDT) for tracking data dependences and the ARVI branch predictor that leverages this data for improved branch prediction.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b5\": 0.9,\n    \"b10\": 0.85,\n    \"b28\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.75,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.65,\n    \"b21\": 0.6,\n    \"b34\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the primary challenge of instruction-level parallelism (ILP) performance gains slowing down due to data dependences in microprocessors. It proposes a novel hardware-based mechanism for cycle-by-cycle tracking of data dependences among in-flight instructions. The paper introduces the Data Dependence Table (DDT) and the ARVI branch predictor, leveraging dynamic data dependence information to improve branch prediction accuracy and overall processor performance.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b10\": 0.8,\n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b11\": 0.7,\n    \"b28\": 0.6,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b32\": 0.5\n  }\n}\n```"], "5c2c7a9217c44a4e7cf314f8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of sampling informative samples for training in metric learning, proposes a classification-based approach to learn high-dimensional binary embeddings, and achieves state-of-the-art retrieval performance. The key contributions include establishing classification as a strong baseline for deep metric learning, providing insights into performance effects of binarization and subsampling classes for scalable training, and proposing a new approach for high-dimensional binary embeddings.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b12\": 0.9,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b26\": 0.8,\n    \"b17\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b5\": 0.6,\n    \"b23\": 0.5,\n    \"b27\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in metric learning, particularly in sampling informative samples for training, and proposes a classification-based approach to learn high-dimensional binary embeddings. The work investigates if classification-based training can perform well for general image retrieval tasks, providing insights into binarization and subsampling classes for scalable training.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b12\": 0.95,\n    \"b13\": 0.95,\n    \"b15\": 0.9,\n    \"b16\": 0.85,\n    \"b22\": 0.95,\n    \"b26\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b27\": 0.75,\n    \"b28\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b21\": 0.7,\n    \"b24\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in metric learning, particularly focusing on sampling informative samples for training. The authors propose a classification-based approach to learn high-dimensional binary embeddings that achieve state-of-the-art retrieval performance. The approach includes techniques like L2 normalization, layer normalization, and class balanced sampling.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b15\": 0.9,\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b22\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b21\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of sampling informative triplets for metric learning and proposes a classification-based approach to learn high-dimensional binary embeddings for image retrieval tasks. The approach aims to achieve state-of-the-art retrieval performance with a balanced memory footprint.\",\n    \"Direct Inspiration\": {\n        \"b12\": 0.9,\n        \"b13\": 0.9,\n        \"b22\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.75,\n        \"b26\": 0.75,\n        \"b17\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.65,\n        \"b16\": 0.65,\n        \"b21\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of sampling informative samples for training in metric learning, aiming to improve generalization in image retrieval tasks. It proposes a classification-based approach to learn high-dimensional binary embeddings, achieving state-of-the-art retrieval performance with the same memory footprint as lower-dimensional float embeddings.\",\n    \"Direct Inspiration\": {\n        \"b12\": 0.9,\n        \"b13\": 0.9,\n        \"b22\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.8,\n        \"b26\": 0.8,\n        \"b17\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.7,\n        \"b28\": 0.7\n    }\n}\n```"], "53e9be72b7602d9704b34b8d": ["```json\n{\n    \"Summary\": \"The paper addresses the scheduling problem in single-ISA heterogeneous multi-core processors, where incorrect scheduling can lead to suboptimal performance and excess energy consumption. The proposed Performance Impact Estimation (PIE) algorithm estimates the expected performance for each core type for a given workload using CPI stack, MLP, and ILP profile information, dynamically adjusting the workload-to-core mapping.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1,\n        \"b2\": 1,\n        \"b8\": 1,\n        \"b16\": 1,\n        \"b17\": 1,\n        \"b18\": 1,\n        \"b21\": 1,\n        \"b29\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b3\": 0.7,\n        \"b32\": 0.6,\n        \"b6\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.5,\n        \"b25\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scheduling workloads on single-ISA heterogeneous multi-core processors to optimize performance and energy efficiency. It proposes the Performance Impact Estimation (PIE) algorithm, which estimates the expected performance of workloads on different core types based on CPI stack, MLP, and ILP profiles. The PIE method dynamically adjusts workload-to-core mapping to exploit time-varying execution behavior while requiring minimal hardware support.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1,\n    \"b8\": 1,\n    \"b16\": 1,\n    \"b17\": 1,\n    \"b18\": 1,\n    \"b29\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b6\": 0.7,\n    \"b9\": 0.9,\n    \"b21\": 0.8,\n    \"b24\": 0.6,\n    \"b25\": 0.9,\n    \"b32\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b11\": 0.4,\n    \"b15\": 0.4,\n    \"b19\": 0.5,\n    \"b20\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the efficient scheduling of workloads on single-ISA heterogeneous multi-core processors to improve performance and energy efficiency. The proposed solution, Performance Impact Estimation (PIE), estimates the performance impact of different core types on workloads by considering both Instruction Level Parallelism (ILP) and Memory Level Parallelism (MLP).\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b1\", \"b2\", \"b8\", \"b16\", \"b21\", \"b29\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b17\", \"b18\", \"b32\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b6\", \"b9\", \"b25\"]\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficient workload scheduling on single-ISA heterogeneous multi-core processors. It introduces the Performance Impact Estimation (PIE) algorithm, which estimates the performance of a given workload on different core types by considering CPI stacks, MLP, and ILP. This approach aims to improve scheduling decisions by dynamically adjusting workload-to-core mappings based on runtime profile information, thereby enhancing overall performance and energy efficiency.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1,\n        \"b2\": 1,\n        \"b8\": 1,\n        \"b16\": 1,\n        \"b21\": 1,\n        \"b29\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b17\": 0.9,\n        \"b18\": 0.9,\n        \"b32\": 0.9,\n        \"b6\": 0.8,\n        \"b3\": 0.8,\n        \"b4\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.7,\n        \"b25\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently scheduling workloads on single-ISA heterogeneous multi-core processors, aiming to optimize performance and energy consumption. It introduces the Performance Impact Estimation (PIE) mechanism to estimate the expected performance for each core type for a given workload, considering both memory-level parallelism (MLP) and instruction-level parallelism (ILP). The paper shows that PIE can dynamically adjust workload-to-core mapping with minimal hardware support, significantly improving performance over existing scheduling proposals.\",\n  \"Direct Inspiration\": [\"b1\", \"b2\", \"b8\", \"b16\", \"b21\", \"b29\"],\n  \"Indirect Inspiration\": [\"b17\", \"b18\", \"b32\"],\n  \"Other Inspiration\": []\n}\n```"], "53e9aa73b7602d97033eb1bd": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of evaluating the performance of Networks-on-Chip (NoCs) designs, which traditionally rely on slow and limited simulation methods. It proposes a new mathematical model for on-chip routers that provides fast and accurate performance estimates, enabling NoC design optimization. The model is based on a set of FIFO buffers interconnected by a switch and derives closed-form expressions for performance metrics under Poisson traffic arrival assumptions.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1.0,\n        \"b9\": 1.0,\n        \"b12\": 1.0,\n        \"b13\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.8,\n        \"b20\": 0.8,\n        \"b21\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.7,\n        \"b28\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing Network-on-Chip (NoC) designs by proposing a mathematical model for on-chip routers. This model is intended to provide accurate performance estimates and facilitate the NoC design process in an optimization loop. The novel contributions of the paper include a new router model based on FIFO buffers, a performance analysis methodology, and the formal proof that dedicated P2P links outperform router-based communication in terms of performance.\",\n  \"Direct Inspiration\": [\"b6\", \"b9\", \"b13\"],\n  \"Indirect Inspiration\": [\"b12\", \"b20\", \"b22\"],\n  \"Other Inspiration\": [\"b1\", \"b4\", \"b7\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in this paper is the performance evaluation of Networks-on-Chip (NoCs) which is traditionally based on slow and less insightful simulations. The authors propose a mathematical model for on-chip routers to facilitate fast and accurate performance analysis. This new model is aimed at improving the design process of NoCs within an optimization loop.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b12\", \"b20\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b6\", \"b8\", \"b13\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b9\", \"b21\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the inefficiency of current simulation-based performance evaluation methods for Networks-on-Chip (NoCs). The authors propose a new mathematical model for on-chip routers that provides fast and accurate performance estimates, which can be used in optimization loops for NoC design. This model is based on FIFO buffers interconnected by a switch and can generalize traditional M/G/1 queuing models.\",\n  \"Direct Inspiration\": [\"b12\", \"b13\"],\n  \"Indirect Inspiration\": [\"b6\", \"b9\", \"b20\"],\n  \"Other Inspiration\": [\"b8\", \"b21\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of optimizing performance evaluation for Networks-on-Chip (NoCs) by proposing a mathematical model for on-chip routers. The model aims to provide fast and accurate performance estimates to guide the NoC design process.\",\n    \"Direct Inspiration\": {\n        \"b12\": 0.9,\n        \"b20\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b6\": 0.6,\n        \"b9\": 0.6,\n        \"b13\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.5,\n        \"b24\": 0.5\n    }\n}\n```"], "53e9ad3bb7602d970372109a": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing an efficient Network-on-Chip (NoC) in the era of dark silicon, particularly focusing on issues like the impact of interconnect during computational sprinting and the need for flexible, fine-grained sprinting to balance performance and power consumption. The proposed solution, NoC-sprinting, includes a topological sprinting mechanism with deadlock-free routing and a heuristic thermal-aware floorplanning algorithm.\",\n  \"Direct Inspiration\": [\"b16\"],\n  \"Indirect Inspiration\": [\"b2\", \"b5\", \"b11\"],\n  \"Other Inspiration\": [\"b3\", \"b6\", \"b10\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the efficient design of Network-on-chip (NoC) for many-core processors in the dark silicon era and the optimization of power and thermal management during computational sprinting. The proposed algorithm, NoC-sprinting, introduces fine-grained sprinting that selectively activates a subset of cores depending on workload characteristics, along with a topological sprinting mechanism with deadlock-free routing and thermal-aware floorplanning to enhance power efficiency and system performance.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b5\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b6\": 0.7,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies the primary challenges of multicore scaling in the dark silicon era, focusing on the impact of Network-on-Chip (NoC) power consumption and the limitations of conventional computational sprinting. The proposed solution, NoC-sprinting, includes fine-grained sprinting, topological sprinting with deadlock-free routing, thermal-aware floorplanning, and an effective network power gating scheme.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b11\": 0.8,\n    \"b3\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing efficient Network-on-Chip (NoC) systems in the dark silicon era, where a significant portion of a chip cannot be fully utilized due to power constraints. It proposes a novel fine-grained sprinting mechanism that allows selective activation of cores to match workload characteristics, thereby improving power efficiency and performance. The paper introduces NoC-sprinting, which includes topological construction, deadlock-free routing, thermal-aware floorplanning, and network power gating to support this fine-grained sprinting approach.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.8,\n    \"b11\": 0.8,\n    \"b13\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b7\": 0.6,\n    \"b10\": 0.6,\n    \"b12\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the utilization wall challenge in technology scaling, particularly focusing on the design of Network-on-Chip (NoC) for many-core processors in the dark silicon era. The proposed solution, NoC-sprinting, introduces fine-grained sprinting to improve power efficiency and performance by selectively activating a subset of cores based on workload characteristics. The paper also presents a topological sprinting mechanism with deadlock-free routing, a thermal-aware floorplanning algorithm, and network power-gating techniques.\",\n  \"Direct Inspiration\": [\"b16\"],\n  \"Indirect Inspiration\": [\"b7\", \"b14\", \"b19\"],\n  \"Other Inspiration\": [\"b5\", \"b11\", \"b6\", \"b3\"]\n}\n```"], "5ecbc8889fced0a24b51eb0e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of object detection in videos, emphasizing the complexities due to motion blur, occlusion, and spatio-temporal variations. The proposed algorithm, Single Shot Video Object Detector (SSVD), integrates two-stream feature aggregation (motion stream and sampling stream) into a one-stage detection framework to leverage temporal coherence and boost detection accuracy.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b20\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b24\": 0.7,\n    \"b25\": 0.6,\n    \"b27\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b18\": 0.7,\n    \"b21\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of object detection in videos, which include large variations, complexities, and issues like motion blur and occlusion. It proposes a novel Single Shot Video Object Detector (SSVD) that integrates two-stream feature aggregation (motion and sampling streams) into a one-stage detection framework. The method leverages temporal coherence and feature sampling to enhance detection accuracy in videos.\",\n  \"Direct Inspiration\": [\"b18\", \"b20\", \"b23\"],\n  \"Indirect Inspiration\": [\"b11\", \"b12\", \"b7\", \"b24\", \"b25\"],\n  \"Other Inspiration\": [\"b15\", \"b16\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of object detection in videos, which is complicated by motion blur, occlusion, and the need to process large amounts of data. The authors propose a Single Shot Video Object Detector (SSVD) that integrates two-stream feature aggregation into a one-stage detection framework. The two streams are a motion stream, which uses optical flow for motion-aware calibration, and a sampling stream, which uses spatio-temporal sampling to hallucinate feature maps. The SSVD aims to leverage temporal coherence across frames to improve detection accuracy while maintaining computational efficiency.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b23\": 0.9,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b55\": 0.6,\n    \"b59\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of object detection in videos, which is complicated by factors such as motion blur and occlusion. The proposed algorithm, Single Shot Video Object Detector (SSVD), integrates two-stream feature aggregation (motion and sampling) into a one-stage detection framework to enhance detection accuracy and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b20\": 0.9,\n    \"b18\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b21\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.65,\n    \"b12\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of object detection in videos by proposing a Single Shot Video Object Detector (SSVD) that integrates two-stream feature aggregation via motion estimation and feature sampling into a one-stage detection framework. The primary contributions include leveraging temporal coherence across frames and combining motion and sampling streams to improve detection accuracy.\",\n  \"Direct Inspiration\": [\"b18\", \"b20\", \"b21\", \"b23\"],\n  \"Indirect Inspiration\": [\"b11\", \"b12\", \"b27\", \"b55\"],\n  \"Other Inspiration\": []\n}\n```"], "5ee7495191e01198a507f7ae": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in generating high-performance tensor programs for deep neural networks (DNNs) without extensive manual tuning. It introduces Ansor, a framework that automatically constructs a large search space with a hierarchical representation, performs fine-tuning using evolutionary search and a learned cost model, and dynamically prioritizes subgraphs critical to end-to-end performance.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b1\": 0.85,\n    \"b51\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.65,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of generating high-performance tensor programs for diverse hardware platforms without requiring significant manual tuning efforts. It proposes a novel search strategy, implemented in the Ansor framework, which automatically constructs a large search space and fine-tunes tensor programs using evolutionary search and a learned cost model. Ansor's hierarchical representation and dynamic prioritization of subgraphs improve the end-to-end performance of DNNs.\",\n  \"Direct Inspiration\": [\"b1\", \"b9\", \"b10\", \"b51\"],\n  \"Indirect Inspiration\": [\"b5\", \"b27\", \"b43\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of generating high-performance tensor programs for DNNs with a large and diverse search space. It proposes Ansor, a framework that uses a hierarchical representation to decouple high-level structures and low-level details, enabling efficient sampling and fine-tuning of tensor programs. Ansor leverages evolutionary search and a learned cost model to optimize performance and dynamically prioritize subgraphs critical to end-to-end DNN performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b51\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.75,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.65,\n    \"b30\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in generating high-performance tensor programs for deep neural networks (DNNs) across various hardware platforms. It proposes Ansor, a framework that constructs a large search space automatically, samples complete programs, and fine-tunes them using evolutionary search and a learned cost model. Key challenges include constructing a large search space, efficient search without comparing incomplete programs, and prioritizing critical subgraphs for optimizing end-to-end performance. Ansor's hierarchical representation and dynamic prioritization of subgraphs are novel contributions that outperform existing methods.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b10\": 1,\n    \"b51\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-performance tensor programs with low latency for DNNs across various hardware platforms. It introduces Ansor, a framework that automatically constructs a large search space, samples complete programs, and fine-tunes them using evolutionary search and a learned cost model. Ansor outperforms existing approaches by covering a broader search space and enabling efficient search without manual templates.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b9\": 1.0,\n    \"b10\": 1.0,\n    \"b51\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.5,\n    \"b19\": 0.4\n  }\n}\n```"], "53e9b9fbb7602d97045fabff": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of mitigating frontend stalls caused by instruction cache misses in lean-core server processors. It proposes the SHIFT (Shared History Instruction Fetch) design which leverages the commonality in instruction fetch streams across cores to share a single instruction history buffer, thereby reducing the high storage cost associated with previous prefetcher designs while maintaining performance benefits.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b26\": 0.6,\n    \"b40\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of instruction cache misses in lean-core server processors, which cause significant frontend stalls. The proposed solution, Shared History Instruction Fetch (SHIFT), leverages the commonality of instruction streams across cores to share instruction history, thereby reducing storage costs and improving performance.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b14\": 0.9,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b40\": 0.4,\n    \"b45\": 0.4,\n    \"b8\": 0.3\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed by the paper is the high instruction cache miss rate in lean-core server processors due to large instruction working sets, which causes significant frontend stalls. The proposed solution, SHIFT (Shared History Instruction Fetch), leverages the commonality in instruction streams across multiple cores running a homogeneous workload to share instruction history, thereby reducing storage overhead and improving performance density.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1.0,\n        \"b14\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.7,\n        \"b22\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b26\": 0.6,\n        \"b45\": 0.5,\n        \"b40\": 0.4,\n        \"b8\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is addressing instruction-fetch stalls in lean-core server processors due to large instruction working sets. The proposed algorithm, SHIFT, aims to mitigate this issue by sharing instruction history across multiple cores running a common server workload, thereby reducing the area overhead of existing instruction prefetcher designs while preserving their performance benefits.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b6\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b29\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of instruction cache misses in lean-core server processors by proposing the SHIFT (Shared History Instruction Fetch) design. SHIFT leverages the commonality in instruction fetch streams across multiple cores running homogeneous workloads to share a single instruction history, thereby reducing the storage overhead while maintaining high performance.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1.0,\n        \"b14\": 0.9,\n        \"b22\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.7,\n        \"b6\": 0.65,\n        \"b45\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b26\": 0.5,\n        \"b40\": 0.45\n    }\n}\n```"], "5b3d98cc17c44a510f801acc": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised learning in deep neural networks, specifically focusing on learning meaningful metrics that reflect apparent similarities among instances without relying on semantic annotations. The authors propose an instance-level discrimination approach and utilize noise-contrastive estimation (NCE) and proximal regularization methods to handle the computational complexity and stabilize the learning process. They highlight the advantages of their non-parametric model over traditional parametric models in terms of scalability and performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b8\": 1,\n    \"b10\": 1,\n    \"b28\": 1,\n    \"b45\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.9,\n    \"b22\": 0.8,\n    \"b30\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b42\": 0.75,\n    \"b48\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is learning meaningful feature representations in an unsupervised manner without human-provided labels. The novel method proposed involves an instance-level discrimination approach, utilizing a non-parametric softmax classifier and noise-contrastive estimation, with a memory bank to store features. The approach aims to create a metric space where visually similar images are mapped closer together.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b8\": 1,\n    \"b28\": 1,\n    \"b45\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b10\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b30\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised learning in computer vision, specifically for object recognition, without relying on annotated datasets. It proposes a novel instance-level discrimination approach that uses a non-parametric softmax classifier, noise-contrastive estimation (NCE), and a proximal regularization method to learn meaningful feature representations. The proposed method leverages the visual correlation among instances and evaluates the learned features using k-nearest neighbors (kNN) for classification.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b4\": 1,\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b17\": 0.8,\n    \"b28\": 0.9,\n    \"b45\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.7,\n    \"b42\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised learning for object recognition, proposing a novel approach that leverages instance-level discrimination. The authors utilize noise-contrastive estimation, proximal regularization, and a non-parametric classifier to manage the computational complexity and stabilize the learning process. The method is evaluated using both SVM and kNN classifiers, demonstrating significant improvements over state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b1\", \"b10\", \"b8\", \"b28\", \"b45\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b0\", \"b48\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b4\", \"b2\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning feature representations for object recognition without requiring annotated data, which is costly and sometimes infeasible. It proposes an unsupervised learning approach that leverages instance-level discrimination and non-parametric methods to learn a meaningful metric for visual similarity. The approach aims to overcome the computational challenges of extending softmax to a large number of classes using noise-contrastive estimation and proximal regularization.\",\n  \"Direct Inspiration\": [\"b8\", \"b28\", \"b45\"],\n  \"Indirect Inspiration\": [\"b1\", \"b10\"],\n  \"Other Inspiration\": [\"b4\"]\n}\n```"], "5cede0e5da562983788c40d8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised embedding learning by proposing a novel instance feature-based softmax embedding method. This method aims to learn discriminative features that maintain data augmentation invariance and instance spread-out properties without relying on labeled data. The proposed approach offers faster learning speeds and higher accuracy compared to existing methods.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.95,\n    \"b20\": 0.9,\n    \"b45\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.85,\n    \"b31\": 0.8,\n    \"b51\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b2\": 0.75\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of unsupervised embedding learning, which aims at learning discriminative embedding features without human annotated labels. The proposed method utilizes instance-wise supervision to approximate positive concentration and negative separation properties. A novel instance feature-based softmax embedding method is introduced to achieve data augmentation invariant and spread-out features for unsupervised embedding learning.\",\n    \"Direct Inspiration\": {\n        \"b7\": 0.9,\n        \"b11\": 0.85,\n        \"b20\": 0.95,\n        \"b45\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b31\": 0.75,\n        \"b51\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.7,\n        \"b15\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of unsupervised embedding learning by proposing a novel instance feature-based softmax embedding method. This method aims to achieve data augmentation invariant and instance spread-out properties without relying on category-wise labels.\",\n    \"Direct Inspiration\": {\n        \"b20\": 0.9,\n        \"b45\": 0.85,\n        \"b7\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b31\": 0.75,\n        \"b51\": 0.7,\n        \"b2\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.6,\n        \"b29\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised embedding learning in computer vision, aiming to learn discriminative embedding features without human-annotated labels. The proposed method utilizes instance-wise supervision to approximate positive concentration and negative separation properties, introducing a novel instance feature-based softmax embedding method. This advances the state-of-the-art by achieving faster learning speed and higher accuracy.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b45\": 1.0,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.7,\n    \"b29\": 0.7,\n    \"b51\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised embedding learning, aiming to learn discriminative embedding features without human annotated labels. It introduces a novel instance feature-based softmax embedding method to achieve data augmentation invariance and instance spread-out properties, improving the efficiency and accuracy of the learning process.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b45\": 0.9,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.7,\n    \"b51\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b26\": 0.6,\n    \"b48\": 0.6\n  }\n}\n```"], "5fc75d8591e0114897921043": ["```json\n{\n    \"Summary\": \"The paper addresses challenges in molecular property prediction using graph neural networks (GNNs). The main issues are overfitting due to small datasets and the limited expressiveness of Message Passing Neural Networks (MPNNs). The authors propose a variant of the standard GIN convolution to increase the effective receptive field and improve small cycle detection, without significantly increasing computational cost.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.9,\n        \"b24\": 0.85,\n        \"b16\": 0.8,\n        \"b3\": 0.75\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.7,\n        \"b18\": 0.65,\n        \"b20\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.55,\n        \"b21\": 0.5,\n        \"b19\": 0.45\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in molecular property prediction using graph neural networks (GNNs). The primary issues discussed are overfitting due to small datasets, non-universality of message passing neural networks (MPNN), and inefficiency in detecting small cycles in graphs. The authors propose a variant of the standard GIN convolution to increase the effective receptive field of each convolution step without increasing the global receptive field, thereby enhancing performance on larger datasets.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b24\": 0.85,\n    \"b16\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b9\": 0.75,\n    \"b22\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b21\": 0.7,\n    \"b0\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are overfitting due to small datasets in molecular property prediction, and the inability of message passing neural networks (MPNNs) to effectively detect small cycles in molecular graphs. The authors propose an alternative method to increase the effective receptive field of each convolution step without significantly increasing the computational cost or the number of parameters. They introduce a variant of the standard GIN convolution called GINE+, which is designed to better detect small cycles in molecular graphs.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b16\": 0.9,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b3\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b0\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of molecular property prediction using graph neural networks, particularly focusing on issues like overfitting due to small datasets and the limited discriminative power of message passing neural networks (MPNNs) for detecting small cycles in molecular graphs. The authors propose an alternative method to increase the effective receptive field of each convolution step to improve performance without significantly increasing computational costs or the number of parameters.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b24\", \"b16\", \"b3\", \"b6\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b9\", \"b22\", \"b19\", \"b7\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b18\", \"b20\", \"b21\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in molecular property prediction using graph neural networks (GNNs). It proposes a new variant of the GIN convolution to increase the effective receptive field for small cycle detection, improving performance with minimal computational cost.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b3\": 0.7,\n    \"b19\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```"], "5f7ef3f59fced0a24bebb79d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of ensuring proper implementation of path policies in current networks, focusing on source and path validation. The authors propose a new protocol, PSVM (Piece Split Validation Mechanism), which introduces a flexible authentication structure called PIC for dynamic and efficient packet verification.\",\n  \"Direct Inspiration\": [\"b2\", \"b3\", \"b11\"],\n  \"Indirect Inspiration\": [\"b1\", \"b5\", \"b12\"],\n  \"Other Inspiration\": [\"b6\", \"b7\", \"b13\", \"b16\", \"b17\", \"b18\", \"b19\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of ensuring source and path validation in networks, particularly under adversarial, high-speed, and variable-path conditions. The authors propose the PSVM (Piece Split Validation Mechanism) protocol, which uses a flexible authentication structure called PIC to validate path policies dynamically, aiming to reduce packet processing latency and communication costs.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1.0,\n        \"b3\": 1.0,\n        \"b11\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8,\n        \"b12\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are ensuring proper implementation of path policies in current networks, confirming source authenticity, and preventing packet detouring. The proposed algorithm, PSVM (Piece Split Validation Mechanism), aims to validate the correct forwarding of packets according to intended path policies. This involves a flexible authentication structure called PIC, which enables dynamic validation of packet transmission.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b3\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b12\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.8,\n    \"b17\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the verification of both source authenticity and the actual path of packets in network environments that are adversarial, high-speed, and variable-path. The proposed solution, PSVM (Piece Split Validation Mechanism), introduces a flexible authentication structure (PIC) that allows for dynamic validation and reduces packet processing latency and communication cost.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 1.0,\n    \"b3\": 0.9,\n    \"b11\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of ensuring source authenticity and path validation in network communications, specifically tackling issues related to path deviation, high processing overhead, and lack of adaptive routing node verification in dynamic environments. The proposed solution, PSVM (Piece Split Validation Mechanism), introduces a flexible authentication structure called PIC to dynamically validate packet forwarding according to intended path policies, significantly reducing packet processing latency and optimizing communication costs.\",\n    \"Direct Inspiration\": [\"b1\", \"b2\", \"b3\", \"b11\"],\n    \"Indirect Inspiration\": [\"b12\", \"b16\", \"b17\"],\n    \"Other Inspiration\": [\"b19\"]\n}\n```"], "5e6cacbe91e011436e692287": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the computational complexity and inefficiency of traditional resource allocation methods in wireless networks, particularly power control in K-user interference channels. The proposed solution is the Interference Graph Convolutional Networks (IGCNet), which utilizes Graph Neural Networks (GNNs) to model the interference channel as a complete graph and learn optimal power allocation. This approach aims to overcome the limitations of previous methods, such as MLP and CNN, by exploiting non-Euclidean data and incorporating instantaneous CSI.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b1\": 1.0,\n    \"b2\": 1.0,\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b16\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing power control in K-user interference channels in wireless networks. Traditional methods like MLP and CNN struggle with scalability and performance degradation in larger networks. The authors propose using Graph Neural Networks (GNNs), specifically the Interference Graph Convolutional Networks (IGCNet), to overcome these limitations by effectively exploiting non-Euclidean data and incorporating instantaneous channel state information (CSI). The proposed method shows superior performance and robustness compared to state-of-the-art algorithms and existing learning-based methods.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b1\": 1,\n    \"b2\": 1,\n    \"b3\": 0.9,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b11\": 0.7,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in resource allocation for wireless networks, particularly the power control in K-user interference channels. It proposes a novel algorithm, IGCNet, using Graph Neural Networks (GNNs) to overcome limitations of existing methods which fail to incorporate instantaneous CSI and struggle with scalability and heterogeneity.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.95,\n    \"b1\": 0.95,\n    \"b2\": 0.90,\n    \"b4\": 0.90,\n    \"b11\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.80,\n    \"b12\": 0.80,\n    \"b13\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.70,\n    \"b16\": 0.70\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing power control in K-user interference channels in wireless networks, focusing on overcoming the limitations of existing methods like MLP and CNN. Inspired by the success of deep learning, it proposes using Graph Neural Networks (GNNs), specifically designing Interference Graph Convolutional Networks (IGCNet) to handle the non-Euclidean nature of the data and improve performance under varying conditions, including instantaneous CSI and network size.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b1\": 1.0,\n    \"b2\": 1.0,\n    \"b3\": 1.0,\n    \"b4\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the significant challenge of resource allocation in wireless networks, focusing on power control in K-user interference channels. Traditional optimization methods fall short due to the increasing network density and latency requirements. The authors propose a novel approach using Graph Neural Networks (GNNs), specifically designing Interference Graph Convolutional Networks (IGCNet) to effectively exploit non-Euclidean data such as instantaneous Channel State Information (CSI). The proposed method aims to overcome the limitations of previous machine learning approaches (MLP and CNN) and optimization-based methods by improving scalability, robustness, and performance in fading channels and heterogeneous problems.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b1\": 1,\n    \"b2\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b11\": 0.8,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b16\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```"], "5f53599a91e0110c40a7bc91": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in predicting and deriving insights from the 3D structure of macromolecules, specifically proteins. The authors propose a novel architecture, the geometric vector perceptron (GVP), integrated into graph neural networks (GNNs), to leverage both geometric and relational aspects of protein structures. This approach is applied to model quality assessment (MQA) and computational protein design (CPD), demonstrating superior performance over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b16\": 0.85,\n    \"b31\": 0.8,\n    \"b1\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b23\": 0.7,\n    \"b27\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b20\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are predicting the quality of a structural model and determining whether two molecules will bind. The proposed algorithm introduces geometric vector perceptrons (GVPs) to unify geometric and relational aspects in protein structure learning. This method aims to bridge the gap between graph neural networks (GNNs) and convolutional neural networks (CNNs) by embedding geometric information at nodes and edges without reducing such information to scalars.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b16\": 0.85,\n    \"b31\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b13\": 0.7,\n    \"b5\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b24\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting or deriving insights from the structure of macromolecules such as proteins, RNA, or DNA. It introduces a unifying architecture called Geometric Vector Perceptrons (GVPs) that bridges geometric and relational aspects of protein structures by operating on both scalar and geometric features. This method is validated on model quality assessment (MQA) and computational protein design (CPD) tasks, demonstrating superior performance over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b2\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b3\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b9\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces a novel architecture, the Geometric Vector Perceptron-based Graph Neural Network (GVP-GNN), designed to leverage both geometric and relational aspects of macromolecular structures, specifically proteins. The main challenges addressed are the integration of geometric information in GNNs and the efficient representation of protein structures for tasks such as model quality assessment and computational protein design.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b2\", \"b16\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b5\", \"b17\", \"b22\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b23\", \"b9\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper are predicting structural properties of macromolecules and determining interactions or binding capabilities between molecules. The algorithm proposed by the author introduces geometric vector perceptrons (GVPs) to enhance graph neural networks (GNNs) by embedding geometric information, allowing the network to learn both geometric and relational features simultaneously. This approach is demonstrated on model quality assessment (MQA) and computational protein design (CPD).\",\n    \"Inspirations\": \"The authors are inspired by techniques in deep learning that leverage domain-specific problem structures, such as convolutions in computer vision and attention mechanisms in natural language processing. They also take inspiration from recent advances in GNNs and CNNs for learning from protein structures.\"\n  },\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b2\": 1,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```"], "5f7fdd328de39f0828397ac2": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of processing long texts with pretrained language models like BERT, which have a maximum token limit of 512. The proposed CogLTX framework draws inspiration from the human cognitive system, particularly the working memory, to handle long texts by reasoning over concatenated key sentences. Key components of CogLTX include MemRecall, which imitates working memory processes, and joint training of two BERT models to score and retrieve relevant text blocks.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b3\": 0.9,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b36\": 0.7,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.8,\n    \"b37\": 0.8,\n    \"b52\": 0.8,\n    \"b21\": 0.7,\n    \"b43\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of handling long texts in NLP tasks with BERT, which has a 512-token limit. The proposed CogLTX framework is inspired by human working memory to perform reasoning over key sentences. The core contributions include the MemRecall mechanism and joint training of two BERT models to handle long texts efficiently.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b37\": 0.8,\n    \"b52\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.6,\n    \"b45\": 0.7,\n    \"b3\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.4,\n    \"b36\": 0.4,\n    \"b7\": 0.4,\n    \"b41\": 0.4,\n    \"b34\": 0.4,\n    \"b26\": 0.3,\n    \"b8\": 0.3,\n    \"b30\": 0.3,\n    \"b2\": 0.3,\n    \"b51\": 0.3,\n    \"b43\": 0.3,\n    \"b21\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of handling long texts with BERT models, which are limited by a 512-token maximum input length. The proposed solution, CogLTX, draws inspiration from human cognitive processes, specifically working memory, to reason over key sentences. The methodology involves a process called MemRecall and the joint training of two BERT models.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b36\": 0.6,\n    \"b45\": 0.6,\n    \"b52\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of processing long texts with BERT, which has a maximum token length limit of 512 tokens. The authors propose the CogLTX framework inspired by human working memory to handle long texts. The framework includes a MemRecall process, which is designed to identify and retrieve relevant text blocks for efficient multi-step reasoning. The study demonstrates that CogLTX outperforms or achieves comparable performance with state-of-the-art models across various tasks, including question answering and text classification, while maintaining constant memory consumption regardless of text length.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b7\": 0.8,\n    \"b34\": 0.8,\n    \"b36\": 0.8,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b49\": 0.5,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of processing long texts with BERT, which is limited to 512 tokens. The authors propose the CogLTX framework, inspired by human working memory, to enable efficient reasoning over long texts. The key innovations include the MemRecall algorithm and the joint training of two BERT models.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b3\": 0.9,\n        \"b7\": 0.8,\n        \"b36\": 0.8,\n        \"b52\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.8,\n        \"b21\": 0.7,\n        \"b34\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.6,\n        \"b41\": 0.6,\n        \"b30\": 0.7\n    }\n}\n```"], "5f4f6ec291e0111f07b30a2b": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed in the paper is the difficulty in comparing the performance of various NAS algorithms due to differences in search spaces, training strategies, and validation sets. The proposed solution is NATS-Bench, which introduces both topology and size search spaces to improve the reproducibility and generality of NAS methods.\",\n    \"inspirations\": \"The authors are inspired by previous NAS benchmarks like NAS-Bench-101 and NAS-HPO-Bench but aim to overcome their limitations by providing a more extensive and applicable search space.\"\n  },\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b7\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.9,\n    \"b17\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.8,\n    \"b37\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reproducibility and generalization in NAS algorithms by proposing NATS-Bench, which introduces a new search space that includes both topology and architecture size. The goal is to provide a more comprehensive benchmark for evaluating NAS algorithms.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b7\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b17\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in reproducibility and generalization of Neural Architecture Search (NAS) algorithms due to different setups in search spaces, training strategies, and evaluation methods. The authors propose NATS-Bench, a comprehensive benchmark that includes topology and size search spaces to evaluate NAS algorithms more effectively.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1,\n        \"b7\": 1,\n        \"b10\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b17\": 0.9,\n        \"b19\": 0.8,\n        \"b18\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.7,\n        \"b20\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of standardizing the evaluation of neural architecture search (NAS) algorithms, which have inconsistent setups regarding search space, training strategies, and evaluation methods. The authors propose NATS-Bench, a benchmark that includes both topology and size search spaces to improve the reproducibility and generalization of NAS methods. NATS-Bench is algorithm-agnostic and provides comprehensive diagnostic information to facilitate better NAS algorithm designs.\",\n  \"Direct Inspiration\": [\"b4\", \"b7\", \"b10\"],\n  \"Indirect Inspiration\": [\"b17\", \"b19\"],\n  \"Other Inspiration\": [\"b3\", \"b6\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of reproducibility and generalization in Neural Architecture Search (NAS) by proposing NATS-Bench, which includes a topology search space and a size search space to evaluate NAS algorithms comprehensively. The paper emphasizes the need for consistent evaluation metrics and the provision of diagnostic information to inspire better NAS designs.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b7\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.8,\n    \"b20\": 0.8\n  }\n}\n```"], "5b67b46b17c44aac1c861fc4": ["```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the limitations of seq2seq models for grammatical error correction (GEC), specifically their reliance on limited training data and the difficulty of correcting sentences with multiple grammatical errors in a single round of inference. The proposed solution involves a novel fluency boost learning and inference mechanism that generates additional training data from less fluent sentences and allows for multi-round inference to incrementally improve sentence fluency.\",\n  \"Direct Inspiration\": {\n    \"b49\": 1,\n    \"b22\": 1,\n    \"b61\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b51\": 0.8,\n    \"b58\": 0.7,\n    \"b56\": 0.7,\n    \"b24\": 0.7,\n    \"b48\": 0.7,\n    \"b47\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b52\": 0.6,\n    \"b37\": 0.6,\n    \"b13\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in sequence-to-sequence (seq2seq) models for grammatical error correction (GEC): limited training data leading to poor generalization and the inability to correct sentences with multiple grammatical errors in a single round. To overcome these issues, the authors propose a novel fluency boost learning and inference mechanism. This involves generating new error-corrected sentence pairs from less fluent sentences during training and incrementally correcting sentences with multiple errors through multi-round inference.\",\n  \"Direct Inspiration\": {\n    \"b49\": 1,\n    \"b22\": 0.9,\n    \"b61\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b51\": 0.8,\n    \"b56\": 0.7,\n    \"b58\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b48\": 0.6,\n    \"b47\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in sequence-to-sequence (seq2seq) models for grammatical error correction (GEC): limited generalization due to restricted training data and difficulty in correcting sentences with multiple grammatical errors in a single round of inference. The proposed solution is a novel fluency boost learning and inference mechanism, which involves generating less fluent sentences to create additional training pairs and using multi-round inference to incrementally correct errors. The paper introduces back-boost, self-boost, and dual-boost learning strategies to enhance model training and achieve state-of-the-art results on multiple GEC benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b61\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b37\": 0.8,\n    \"b31\": 0.8,\n    \"b52\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.9,\n    \"b49\": 0.85\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two main challenges in seq2seq models for grammatical error correction (GEC): the limited size of training data and the inability to perfectly correct sentences with multiple grammatical errors in a single round. The proposed solution is a novel fluency boost learning and inference mechanism that generates less fluent sentences to create new error-corrected sentence pairs and allows multi-round inference to iteratively improve sentence fluency. This approach aims to improve neural seq2seq GEC models and achieve state-of-the-art results on benchmark datasets.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b49\": 1,\n    \"b61\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b24\": 0.8,\n    \"b51\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b13\": 0.7,\n    \"b31\": 0.7,\n    \"b37\": 0.7,\n    \"b52\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in seq2seq models for grammatical error correction (GEC): limited training data and the inability to correct sentences with multiple grammatical errors in a single round of inference. To tackle these, the authors propose a novel fluency boost learning and inference mechanism. This includes generating additional training data by creating less fluent sentence pairs and allowing multi-round inference to incrementally improve sentence fluency.\",\n  \"Direct Inspiration\": [\"b22\", \"b49\", \"b61\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b1\", \"b24\", \"b31\", \"b37\"]\n}\n```"], "5f8d00a29e795ea21aee8001": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge addressed in the paper is the scholar name ambiguity problem in digital libraries, which complicates tasks such as scholar search and document retrieval. This problem is exacerbated by limited information in bibliography data and the ambiguity of coauthor names.\",\n    \"Inspirations\": \"The paper introduces a novel iterative method via collective clustering (NDCC) to handle this scholar name disambiguation problem. It also develops a new metric for determining author similarity by combining coauthors, venues, titles, and coauthor names, and uses a word embedding method for semantic similarity.\"\n  },\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b42\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.7,\n    \"b23\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scholar name ambiguity in digital libraries, proposing an iterative method via collective clustering (NDCC) to disambiguate names using limited bibliography data. The solution involves a heterogeneous multipartite network model, a novel similarity metric, and comprehensive experimental validation against state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b9\": 0.95,\n    \"b17\": 0.85,\n    \"b16\": 0.8,\n    \"b42\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.6,\n    \"b23\": 0.65,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scholar name ambiguity in digital libraries. It proposes an iterative method via collective clustering, referred to as NDCC, which uses a heterogeneous multipartite network model to improve name disambiguation by considering the interconnections between different names. The paper introduces a novel metric for determining author similarity and conducts comprehensive experimental studies to validate the effectiveness and efficiency of the proposed method.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b9\": 0.9,\n    \"b30\": 0.85,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b42\": 0.65\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scholar name disambiguation in digital libraries by proposing an iterative method via collective clustering called NDCC. The method uses a heterogeneous multipartite network model and develops a novel metric for author similarity based on coauthors, venues, titles, and coauthor names.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b9\": 0.95,\n    \"b16\": 0.85,\n    \"b17\": 0.9,\n    \"b42\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b28\": 0.75,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b14\": 0.65,\n    \"b41\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses scholar name ambiguity in digital libraries using a novel iterative method called NDCC, which employs collective clustering on a heterogeneous multipartite network. This method considers the interconnectedness of author references and develops a new metric for author similarity. The proposed approach is evaluated against state-of-the-art methods and shows significant improvements in effectiveness and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b9\": 0.95,\n    \"b16\": 0.85,\n    \"b17\": 0.9,\n    \"b42\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": []\n}\n```"], "573696116e3b12023e52463f": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of generalizing convolutional neural networks (CNNs) to exploit larger groups of symmetries, including rotations and reflections, beyond just translations. The authors propose Group Convolutional Neural Networks (G-CNNs) which ensure equivariance to these additional transformations, thereby improving the model's efficiency and capability for tasks such as image recognition.\",\n    \"Direct Inspiration\": {\n        \"b26\": 0.9,\n        \"b1\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.7,\n        \"b34\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b14\": 0.6,\n        \"b18\": 0.6,\n        \"b20\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generalizing convolutional neural networks (CNNs) to exploit larger groups of symmetries beyond mere translations, such as rotations and reflections. The proposed algorithm, termed G-CNNs, is designed to maintain equivariance to these symmetries throughout the network layers, thereby improving predictive performance and generalization. The mathematical framework and experimental results on datasets like MNIST-rot and CIFAR10 demonstrate the efficacy of G-CNNs.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.7,\n    \"b16\": 0.7,\n    \"b32\": 0.7,\n    \"b35\": 0.7,\n    \"b29\": 0.7,\n    \"b19\": 0.7,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b18\": 0.5,\n    \"b36\": 0.5,\n    \"b33\": 0.5,\n    \"b26\": 0.5,\n    \"b0\": 0.5,\n    \"b1\": 0.5,\n    \"b42\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on generalizing convolutional neural networks (CNNs) to exploit larger groups of symmetries, including rotations and reflections, through the concept of equivariance. The proposed G-CNNs aim to improve performance by maintaining equivariance to these transformations. The paper presents a mathematical framework, implementation details, and experimental results demonstrating state-of-the-art performance on datasets like MNIST-rot and CIFAR10.\",\n  \"Direct Inspiration\": {\n    \"b26\": 1.0,\n    \"b0\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b18\": 0.75,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b14\": 0.6,\n    \"b31\": 0.55\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generalizing convolutional neural networks (CNNs) to exploit larger groups of symmetries, including rotations and reflections, which standard CNNs fail to handle efficiently. The proposed method, G-CNNs, aims to maintain equivariance to these transformations, thereby improving the network's ability to generalize and achieve state-of-the-art performance on tasks such as image classification.\",\n    \"Direct Inspiration\": {\n        \"b26\": 1.0,\n        \"b0\": 0.9,\n        \"b14\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b34\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b28\": 0.7,\n        \"b16\": 0.7,\n        \"b35\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generalizing convolutional neural networks (CNNs) to exploit larger groups of symmetries, including rotations and reflections. It introduces G-CNNs, a novel method that maintains equivariance to these transformations, thereby enhancing the efficiency and predictive performance of the networks.\",\n  \"Direct Inspiration\": {\n    \"b26\": 0.9,\n    \"b1\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.75,\n    \"b18\": 0.75,\n    \"b36\": 0.75,\n    \"b33\": 0.75,\n    \"b35\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b34\": 0.7,\n    \"b31\": 0.7\n  }\n}\n```"], "5fe1ccc591e0119a161edd6a": ["```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are the extension of group equivariance to self-attention, a non-linear map. The authors propose a new architecture, LieTransformer, which uses LieSelfAttention layers to address these challenges. This method is generalizable to Lie groups and their discrete subgroups, and it aims to improve performance and data efficiency in tasks such as shape counting on point clouds, molecular property regression, and modeling particle trajectories under Hamiltonian dynamics.\",\n    \"Direct Inspiration\": {\n        \"b5\": 0.95,\n        \"b48\": 0.90\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.75,\n        \"b13\": 0.80,\n        \"b29\": 0.70\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.60,\n        \"b8\": 0.65,\n        \"b20\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extending group equivariance to self-attention, a non-linear map, motivated by the success of group equivariant CNNs (G-CNNs) in handling symmetries such as rotations and reflections. The proposed LieTransformer is a group invariant Transformer built from equivariant LieSelfAttention layers, using a lifting-based approach to generalize permutation equivariance to other groups. The method is applicable to Lie groups and their discrete subgroups and is demonstrated on tasks such as shape counting on point clouds, molecular property regression, and modelling particle trajectories under Hamiltonian dynamics.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b48\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b7\": 0.75,\n    \"b13\": 0.7,\n    \"b21\": 0.68,\n    \"b29\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper include extending group equivariance to self-attention, developing a novel LieTransformer model, and demonstrating its application across various tasks. The paper is inspired by the successes of convolutional neural networks (CNNs) and group equivariant CNNs (G-CNNs), and aims to generalize these concepts to self-attention and Lie groups.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b48\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b13\": 0.8,\n    \"b29\": 0.8,\n    \"b8\": 0.75,\n    \"b7\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.7,\n    \"b51\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extending group equivariance to self-attention mechanisms, proposing the LieTransformer to handle various symmetries beyond translations. This involves using a lifting-based approach to relax constraints on the attention module and is applicable to Lie groups and their discrete subgroups.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b13\": 0.85,\n    \"b48\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b8\": 0.75,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b46\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of extending group equivariance to self-attention mechanisms, specifically proposing the LieTransformer, a group invariant Transformer built from equivariant LieSelfAttention layers. This is motivated by the success of group equivariant CNNs (G-CNNs) and the prominence of self-attention in various data modalities.\",\n    \"Direct Inspiration\": {\n        \"b5\": 1,\n        \"b48\": 1,\n        \"b13\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b29\": 0.8,\n        \"b8\": 0.8,\n        \"b2\": 0.8,\n        \"b7\": 0.7,\n        \"b42\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b46\": 0.6,\n        \"b20\": 0.6\n    }\n}\n```"], "58d82fced649053542fd692f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of training stochastic neural networks with discrete random variables, which are difficult to train due to the non-differentiable nature of these variables. The authors propose the Gumbel-Softmax distribution, a continuous distribution that can approximate categorical samples and whose gradients can be computed via the reparameterization trick. This method outperforms existing gradient estimators for both Bernoulli and categorical variables and can be used to efficiently train semi-supervised models.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b7\": 0.8,\n    \"b14\": 0.8,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b12\": 0.6,\n    \"b25\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is the difficulty in training stochastic neural networks with discrete random variables using the backpropagation algorithm. The paper proposes the Gumbel-Softmax distribution as a solution, which provides a continuous, differentiable approximation to categorical samples, making it easier to compute parameter gradients. The contributions include introducing the Gumbel-Softmax, demonstrating its superiority over existing single-sample gradient estimators, and showcasing its application in efficiently training semi-supervised models.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.95,\n    \"b13\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.85,\n    \"b8\": 0.80,\n    \"b12\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.75,\n    \"b14\": 0.70,\n    \"b6\": 0.70\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper tackles the challenge of training stochastic neural networks with discrete random variables, specifically focusing on gradient estimation for categorical variables. It introduces the Gumbel-Softmax distribution as a continuous approximation that allows for gradient computation via the reparameterization trick, and presents the Gumbel-Softmax estimator and Straight-Through Gumbel-Softmax estimator for efficient training. The method is shown to outperform existing estimators in various experimental setups.\",\n    \"Direct Inspiration\": [\"b13\"],\n    \"Indirect Inspiration\": [\"b10\", \"b0\", \"b7\", \"b15\"],\n    \"Other Inspiration\": [\"b8\", \"b12\", \"b25\", \"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of training stochastic neural networks with discrete random variables, which are difficult to train due to non-differentiable layers. The authors propose the Gumbel-Softmax distribution, which approximates categorical samples and allows for backpropagation. This estimator outperforms others for both Bernoulli and categorical variables and is useful for training semi-supervised models without costly marginalization over unobserved categorical latent variables.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b10\": 0.85,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b8\": 0.7,\n    \"b12\": 0.7,\n    \"b25\": 0.65,\n    \"b17\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b14\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the difficulty of training stochastic neural networks with discrete random variables due to the non-differentiable nature of these layers. The authors propose the Gumbel-Softmax estimator, a continuous distribution that approximates categorical samples and allows for efficient training using backpropagation. They demonstrate the effectiveness of this approach through various experiments, showing that it outperforms other gradient estimators and can be used in semi-supervised models without costly marginalization processes.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b13\": 0.85,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b14\": 0.7,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.65,\n    \"b12\": 0.6\n  }\n}\n```"], "58d82fcbd649053542fd6178": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing stochastic computation graphs (SCGs) with discrete nodes. It introduces the Concrete distribution as a novel method for creating continuous relaxations of discrete random variables, thus enabling gradient-based optimization. The work leverages and builds upon techniques such as the reparameterization trick and the Gumbel-Max trick.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b27\": 0.9,\n    \"b23\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.7,\n    \"b9\": 0.6,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing stochastic computation graphs (SCGs) with discrete nodes in the context of automatic differentiation libraries. It introduces the Concrete distribution as a continuous relaxation of discrete random variables, allowing for gradient-based optimization of discrete parameters. The proposed method aims to facilitate the optimization of large, complex stochastic architectures.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b27\": 0.8,\n    \"b23\": 0.85,\n    \"b40\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b34\": 0.65,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b24\": 0.55,\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing discrete stochastic computation graphs in automatic differentiation (AD) libraries. The authors propose the Concrete distribution as a novel method to relax discrete random variables, allowing gradient-based optimization using continuous relaxations.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.6,\n    \"b23\": 0.8,\n    \"b40\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.5,\n    \"b10\": 0.4,\n    \"b12\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in this paper involve optimizing stochastic computation graphs with discrete nodes, which conventional AD libraries struggle with. The paper introduces the Concrete distribution as a smooth relaxation of discrete random variables to facilitate optimization via gradients.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b27\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b34\": 0.8,\n    \"b4\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in optimizing stochastic computation graphs with discrete nodes, introducing the Concrete distribution as a relaxation technique to enable gradient-based optimization. The Concrete distribution allows for discrete variables to be treated as continuous during training, enabling the use of automatic differentiation and gradient descent methods.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b40\": 0.6,\n    \"b4\": 0.6,\n    \"b24\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```"], "5d04eeba8607575390f83f53": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the diverse and complex architectural requirements of different microservices at Facebook, which manifest in various bottlenecks and performance constraints. The proposed solution, ?SKU, is a design tool that automates the search for optimal server configurations ('soft SKUs') tailored to specific microservices, leveraging configurable server parameters to enhance performance without requiring new hardware.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b16\": 0.9,\n    \"b18\": 0.7,\n    \"b20\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.6,\n    \"b6\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b30\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b31\": 0.5,\n    \"b33\": 0.5,\n    \"b38\": 0.5,\n    \"b40\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the diverse and complex performance requirements of Facebook's microservices running on dedicated bare metal servers. The primary challenges include identifying system-level and architectural bottlenecks and optimizing server configurations for varied microservice needs without increasing hardware diversity. The novel approach involves creating 'soft SKUs' by tuning server parameters and using an automated design tool, ?SKU, to optimize configurations through A/B testing in a production environment.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b5\": 0.8,\n    \"b11\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.75,\n    \"b30\": 0.7,\n    \"b31\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.6,\n    \"b38\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Diverse performance constraints and bottlenecks across microservices require conflicting optimization choices.\",\n      \"Need for strategies enabling limited server CPU architectures (SKUs) to provide performance and energy efficiency across diverse microservices.\"\n    ],\n    \"proposed_solution\": [\n      \"Comprehensive system-level and architectural characterizations of key microservices.\",\n      \"Development of ?SKU, a design tool to automatically tune server parameters for microservice-specific soft SKUs.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b10\", \"b15\", \"b16\", \"b20\", \"b22\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b33\", \"b38\", \"b39\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b4\", \"b11\", \"b17\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the diverse and complex performance requirements of microservices in large-scale data centers, specifically focusing on Facebook's production systems. The primary challenge is to optimize server CPU architectures to meet the distinct needs of various microservices without diversifying hardware platforms. The proposed solution, \\u03b7SKU, automates the tuning of server parameters to create microservice-specific 'soft SKUs' that improve performance and energy efficiency without additional hardware requirements.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.8,\n    \"b4\": 0.7,\n    \"b7\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.6,\n    \"b15\": 0.7,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b17\": 0.4,\n    \"b20\": 0.5,\n    \"b33\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of diverse system and CPU architectural requirements among Facebook's microservices, proposing 'soft SKUs' to tailor server configurations without diversifying hardware platforms. The authors introduce ?SKU, an automated tool to optimize server parameters, achieving significant performance improvements.\",\n  \"Direct Inspiration\": [\"b6\", \"b10\", \"b16\", \"b17\", \"b22\"],\n  \"Indirect Inspiration\": [\"b15\", \"b20\", \"b28\", \"b31\", \"b33\"],\n  \"Other Inspiration\": [\"b11\", \"b25\", \"b26\", \"b27\", \"b29\"]\n}\n```"], "5efcb8cd91e0115203245887": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the development of an unsupervised clustering method for Graph Neural Networks (GNNs) that can optimize cluster assignments in an end-to-end differentiable way. The proposed method, DMON, bridges traditional graph clustering objectives with deep neural networks, addressing issues present in previous methods such as lack of end-to-end optimization and poor performance on hierarchical structures. DMON leverages spectral modularity maximization and introduces a novel regularization method to prevent trivial solutions.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b68\": 0.85,\n    \"b37\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b60\": 0.75,\n    \"b30\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b25\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised clustering in Graph Neural Networks (GNNs) by introducing DMON, a method that allows end-to-end optimization of cluster assignments. It bridges the gap between traditional graph clustering objectives and deep neural networks, proposing a differentiable unsupervised clustering objective inspired by spectral modularity maximization. The work also includes empirical studies and experimental evaluations demonstrating the effectiveness of DMON over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.95,\n    \"b68\": 0.9,\n    \"b60\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.75,\n    \"b30\": 0.7,\n    \"b19\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.6,\n    \"b37\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of unsupervised clustering in Graph Neural Networks (GNNs), focusing on the limitations of existing methods that do not optimize clustering in an end-to-end differentiable manner. The proposed method, DMON, allows for such optimization by leveraging spectral modularity maximization and introducing a novel regularization technique to avoid trivial solutions.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b68\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b60\": 0.9,\n    \"b19\": 0.8,\n    \"b30\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b28\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised graph clustering in Graph Neural Networks (GNNs). It introduces DMON, a method that bridges traditional graph clustering objectives with deep neural networks, optimizing cluster assignments in an end-to-end differentiable manner.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b68\": 1.0,\n    \"b60\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b30\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unsupervised clustering in Graph Neural Networks (GNNs), proposing the DMON (Deep Modularity Networks) method that allows for end-to-end differentiable optimization of cluster assignments. This method bridges the gap between traditional graph clustering objectives and GNNs, providing a novel approach that optimizes a spectral modularity maximization objective with a collapse regularization to ensure informative clusters.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b68\": 0.85,\n    \"b60\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.75,\n    \"b55\": 0.7,\n    \"b17\": 0.65,\n    \"b36\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b37\": 0.6,\n    \"b28\": 0.55\n  }\n}\n```"], "5aed146117c44a44381527f9": ["```json\n{\n  \"Summary\": \"The paper addresses the front-end bottleneck in server workloads caused by large instruction working sets overwhelming private instruction caches (L1-I) and branch prediction structures. The proposed solution, Shotgun, is a BTB-directed front-end prefetcher that aims to mitigate this bottleneck by using a new BTB organization specialized for effective prefetching. Shotgun focuses on tracking unconditional branches and their targets' spatial footprints, enabling accurate and timely BTB-directed prefetching while maintaining a practical-sized BTB for conditional branches.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b9\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the front-end bottleneck in server workloads caused by large instruction working sets. It introduces 'Shotgun', a BTB-directed instruction prefetcher that uses a new BTB organization specialized for effective prefetching. Shotgun tracks global control flow with unconditional branches and spatial footprints of target regions to improve prefetch accuracy and reduce stalls.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b5\", \"b9\", \"b12\", \"b14\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b2\", \"b8\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b7\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the front-end bottleneck in servers caused by large instruction working sets, which overwhelms private instruction caches and branch prediction structures. The proposed algorithm, Shotgun, aims to improve BTB-directed instruction prefetching by focusing on unconditional branches and their targets' spatial footprints to track the application's instruction working set at a cache block granularity. This approach allows for more effective and timely prefetching compared to previous methods like Boomerang and Confluence.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b14\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.8,\n    \"b8\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is the front-end bottleneck in servers caused by large instruction working sets, which overwhelm private instruction caches and branch prediction structures, leading to performance degradation. The proposed solution is Shotgun, a BTB-directed front-end prefetcher that leverages a new BTB organization specialized for effective prefetching by focusing on unconditional branches and their spatial footprints to track the application's instruction working set.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b12\", \"b9\", \"b14\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b5\", \"b8\", \"b7\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b2\", \"b11\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the front-end bottleneck in servers caused by large instruction cache misses and BTB misses. It introduces 'Shotgun,' a BTB-directed front-end prefetcher that utilizes a new BTB organization to effectively track and prefetch instruction working sets by focusing on unconditional branches and their spatial footprints.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b12\", \"b9\", \"b14\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b5\", \"b2\", \"b8\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b7\"]\n  }\n}\n```"], "5aed147c17c44a4438153a60": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of performance isolation and resource utilization in public cloud environments, specifically focusing on managing the Last-Level Cache (LLC) in Intel x86 processors. The authors propose a dynamic cache allocation technology (dCat) built on top of Intel's Cache Allocation Technology (CAT) to dynamically manage cache partitions, ensuring baseline performance while optimizing cache usage in real-time.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1.0,\n        \"b26\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b19\": 0.8,\n        \"b20\": 0.8,\n        \"b24\": 0.8,\n        \"b33\": 0.8,\n        \"b34\": 0.8,\n        \"b38\": 0.8,\n        \"b39\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.7,\n        \"b6\": 0.7,\n        \"b35\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": [\n    \"The primary challenges outlined in the paper include managing performance interference in shared resources, such as CPU caches, in public cloud environments. The paper proposes a dynamic cache allocation technology (dCat) to provide performance isolation and improve resource utilization by dynamically adjusting cache allocation based on workload behavior.\"\n  ],\n  \"Direct Inspiration\": [\n    \"b17\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b9\",\n    \"b10\",\n    \"b19\",\n    \"b20\",\n    \"b24\",\n    \"b33\",\n    \"b34\",\n    \"b38\",\n    \"b39\"\n  ],\n  \"Other Inspiration\": [\n    \"b7\",\n    \"b23\",\n    \"b26\",\n    \"b35\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of managing the last-level cache (LLC) in Intel x86 processors for multi-tenant performance isolation while increasing utilization. The authors propose a dynamic cache allocation technology (dCat) built on top of Intel's Cache Allocation Technology (CAT) to dynamically partition the LLC based on runtime behavior, ensuring consistent baseline performance while optimizing resource utilization.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.9,\n        \"b10\": 0.9,\n        \"b20\": 0.8,\n        \"b24\": 0.8,\n        \"b33\": 0.8,\n        \"b34\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b26\": 0.7,\n        \"b19\": 0.7,\n        \"b38\": 0.7,\n        \"b39\": 0.7,\n        \"b23\": 0.6,\n        \"b5\": 0.6,\n        \"b6\": 0.6,\n        \"b35\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of performance interference in shared resources, particularly the last-level cache (LLC) in Intel x86 processors, within public cloud IaaS environments. It proposes a dynamic cache allocation technology (dCat) that builds on Intel's Cache Allocation Technology (CAT) to provide performance isolation and improve resource utilization. The main contributions include the design of dCat, which dynamically adjusts cache allocation based on workload behavior to ensure a consistent baseline performance while optimizing cache usage for workloads that can benefit from it.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.8,\n        \"b10\": 0.8,\n        \"b26\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.6,\n        \"b19\": 0.6,\n        \"b20\": 0.6,\n        \"b24\": 0.6,\n        \"b33\": 0.6,\n        \"b34\": 0.6,\n        \"b38\": 0.6,\n        \"b39\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Performance interference due to competition for shared resources in public clouds.\",\n      \"Need for performance isolation to ensure consistent application performance while maintaining resource efficiency.\",\n      \"Limitations of Intel CAT in providing static cache partitioning leading to suboptimal performance for memory-intensive applications.\"\n    ],\n    \"inspirations\": [\n      \"Leveraging dynamic resource adjustments to improve physical resource utilization and cloud efficiency.\",\n      \"Implementation of dCat on top of Intel CAT to dynamically manage cache allocation.\"\n    ]\n  },\n  \"Direct Inspiration\": [\"b17\"],\n  \"Indirect Inspiration\": [\"b7\", \"b19\", \"b20\", \"b24\", \"b33\", \"b34\", \"b38\", \"b39\"],\n  \"Other Inspiration\": [\"b5\", \"b6\", \"b23\", \"b26\", \"b35\"]\n}\n```"], "558c9cd8e4b0cfb70a1eab9c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of high miss rates in last-level caches (LLCs) by introducing a sampling-based dead block predictor. This new technique aims to improve cache efficiency by accurately predicting and replacing dead blocks with live data while consuming less power and area than previous methods.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b14\": 1.0,\n    \"b18\": 1.0,\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b10\": 0.8,\n    \"b0\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b20\": 0.7,\n    \"b8\": 0.7,\n    \"b7\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in this paper is reducing the miss rate in the last-level cache (LLC) by efficiently predicting and replacing dead blocks. The proposed solution is a sampling-based dead block predictor that offers higher accuracy, lower power consumption, and reduced metadata overhead compared to previous methods. This new predictor leverages consistent memory access patterns and a skewed organization inspired by branch prediction research.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b0\": 0.7,\n    \"b11\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inefficiency caused by dead blocks in the last-level cache (LLC). The paper proposes a novel sampling-based dead block predictor that aims to improve cache performance by replacing dead blocks with live ones more efficiently, while consuming less power and requiring fewer resources compared to previous predictors.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b14\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b18\": 0.8,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of reducing miss rates in the last-level cache (LLC) by proposing a sampling-based dead block predictor. The new technique aims to improve cache efficiency by replacing dead blocks with live ones more effectively. It addresses the limitations of existing dead block predictors which are unsuitable for LLCs due to their substantial overhead, reliance on LRU replacement algorithms, and other issues. The sampling predictor uses a small fraction of references to sets to perform accurate predictions with less power and area requirements.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to reduce the miss rate in the last-level cache (LLC) by effectively predicting and replacing dead blocks with live blocks. The proposed algorithm introduces a novel sampling dead block predictor that leverages consistent memory access patterns across sets, decouples the prediction mechanism from the cache structure, and minimizes power and area overhead compared to previous predictors.\",\n  \"Direct Inspiration\": [\"b12\", \"b14\", \"b18\"],\n  \"Indirect Inspiration\": [\"b4\", \"b0\", \"b6\"],\n  \"Other Inspiration\": [\"b9\", \"b19\"]\n}\n```"], "5ce3cd34e1cd8e3f7932b9af": ["```json\n{\n  \"Summary\": \"The paper addresses the limitations of deterministic Spike-Time-Dependent-Plasticity (STDP) in Spiking Neural Networks (SNN) and proposes a stochastic STDP algorithm to improve learning accuracy, speed, and robustness in unsupervised learning tasks. The proposed GPU-accelerated SNN simulator, ParallelSpikeSim, supports various precision levels and rounding options to optimize performance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on developing a GPU-accelerated SNN simulator, ParallelSpikeSim, for high-accuracy, fast, and low-precision unsupervised learning using stochastic STDP. The main challenges addressed include the limitations of deterministic STDP in existing SNN simulators and the need for efficient simulation of complex tasks like Fashion-MNIST. Key innovations include the use of stochastic STDP, precision control, and frequency of input spike trains.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b11\": 0.5,\n    \"b12\": 0.5,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve addressing the limitations of deterministic STDP in SNN simulators, such as lower accuracy for complex tasks and limited opportunities for fast, low-precision simulation. The algorithm proposed by the authors introduces a GPU-accelerated SNN simulator called ParallelSpikeSim, which employs stochastic STDP to improve learning accuracy and efficiency. The authors also provide precision control and different rounding options to further enhance the performance of unsupervised learning in SNN.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b13\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.75,\n    \"b7\": 0.7,\n    \"b9\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges addressed in this paper are the limitations of deterministic STDP in achieving high accuracy and fast low-precision learning in SNNs. The paper proposes a GPU-accelerated SNN simulator, ParallelSpikeSim, which uses stochastic STDP to overcome these limitations, offering better performance in both accuracy and learning speed.\",\n  \"Direct Inspiration\": [\"b2\", \"b13\"],\n  \"Indirect Inspiration\": [\"b11\", \"b12\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving high-accuracy, fast, and low-precision unsupervised learning in Spiking Neural Networks (SNN) using Spike-Time-Dependent-Plasticity (STDP). It introduces a novel GPU-accelerated SNN simulator, ParallelSpikeSim, that employs stochastic STDP instead of deterministic STDP algorithms used in prior simulators. This approach improves learning accuracy for complex tasks, reduces learning time, and supports low-precision learning.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b8\": 0.5,\n    \"b9\": 0.5,\n    \"b11\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```"], "5ce3cd34e1cd8e3f7932b9ee": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of the aging effect in memristor-based crossbars used for neural network computations. The proposed algorithm aims to mitigate aging by skewing weights towards smaller values during software training and incorporating aging-aware mapping to reduce online tuning iterations.\",\n    \"Direct Inspiration\": [\"b8\", \"b9\", \"b11\"],\n    \"Indirect Inspiration\": [\"b3\", \"b12\", \"b13\"],\n    \"Other Inspiration\": [\"b10\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the aging effect of memristors used in neural network implementations. The authors propose a new framework combining software training and hardware mapping to counteract this aging effect. The core contributions include skewed-weight training to reduce current through memristors and aging-aware mapping to adjust weights based on the aging status of the memristors.\",\n  \"Direct Inspiration\": [\"b8\", \"b9\"],\n  \"Indirect Inspiration\": [\"b6\", \"b15\"],\n  \"Other Inspiration\": [\"b10\", \"b11\", \"b16\", \"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of aging in memristor-based crossbars used for neural network computations, proposing a framework combining software training and hardware mapping to counteract this effect. By skewing weights towards smaller values, the aging effect is alleviated, and the framework adjusts the conductances considering the current aging status, extending the lifetime of the crossbars significantly without additional hardware cost.\",\n  \"Direct Inspiration\": [\"b8\", \"b9\"],\n  \"Indirect Inspiration\": [\"b10\", \"b11\"],\n  \"Other Inspiration\": [\"b16\", \"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of the aging effect in memristor-based crossbars used for neural network computations. It proposes a new framework combining software training and hardware mapping to counter this aging effect without incurring extra hardware costs. The key contributions include a skewed-weight training method and an aging-aware mapping process to extend the lifetime of memristor crossbars.\",\n  \"Direct Inspiration\": [\"b8\", \"b9\"],\n  \"Indirect Inspiration\": [\"b10\", \"b11\", \"b16\", \"b17\"],\n  \"Other Inspiration\": [\"b6\", \"b12\", \"b13\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed in the paper is the aging effect of memristors during repetitive tuning in memristor-based neural networks. This aging effect degrades the valid conductance range of memristors, leading to a decrease in classification accuracy and the need for more programming iterations.\",\n    \"inspirations\": \"The paper introduces a novel framework to counter aging through skewed-weight training and aging-aware mapping, aiming to extend the lifetime of memristor-based crossbars without incurring extra hardware costs.\"\n  },\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b9\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b16\": 0.9,\n    \"b17\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b15\": 0.6\n  }\n}\n```"], "599c7ea4601a182cd28b8342": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of energy and latency in cache hierarchies by proposing a novel split cache hierarchy that separates data and metadata. This separation allows for more efficient data tracking and retrieval, supporting a range of cache optimizations like direct data access, dynamic coherence, and cache bypassing. The methodology builds on the Direct-to-Data (D2D) framework and extends it to coherent multicore processors.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.5,\n    \"b20\": 0.5\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of energy and latency costs incurred by associative searches in cache hierarchies. It proposes a novel cache hierarchy that separates data and metadata into two distinct hierarchies. This separation allows for optimized metadata handling and more efficient data access, leading to improvements in various cache optimizations such as direct data access, data placement, dynamic coherence, cache bypassing, and dynamic indexing.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b1\": 0.9,\n    \"b2\": 0.9,\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies in current cache hierarchies which couple data and metadata, resulting in high energy and latency costs. It proposes a novel cache hierarchy that separates metadata from data, enabling more efficient and flexible cache management. The proposed design builds upon the Direct-to-Data (D2D) cache hierarchy and extends it to support coherent multicore processors with a deterministic metadata hierarchy. This separation simplifies the implementation of various cache optimizations, including direct data access, dynamic coherence, data placement, cache bypassing, and dynamic indexing.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.5,\n    \"b20\": 0.5\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper are the inefficiencies and significant costs in energy and latency due to the explicit coupling of data and metadata in today's cache hierarchies. The paper proposes a novel cache hierarchy that separates data and metadata into two distinct hierarchies, enhancing efficiency in data access and enabling various cache optimizations.\",\n    \"Inspirations\": \"The proposed solution builds upon the Direct-to-Data (D2D) cache hierarchy to support coherent multicore processors and generalizes it to develop an explicit separation of metadata and data hierarchies.\"\n  },\n  \"Direct Inspiration\": {\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.5,\n    \"b20\": 0.5\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Significant costs in energy and latency due to associative searches for data and metadata in cache hierarchies.\",\n      \"Limited design flexibility caused by the explicit coupling of data and metadata.\"\n    ],\n    \"Inspirations\": [\n      \"Direct-to-Data (D2D) cache hierarchy for efficient data location tracking.\",\n      \"Extension of D2D to support coherent multicore processors and general separation of metadata and data hierarchies.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b0\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b6\", \"b7\", \"b8\", \"b9\", \"b10\", \"b11\", \"b12\", \"b13\", \"b14\", \"b15\", \"b16\", \"b17\", \"b18\", \"b19\", \"b20\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": []\n  }\n}\n```"], "5fbe5cf091e011e6e11b3cf5": ["```json\n{\n    \"Summary\": \"The paper addresses the problem of false negatives in contrastive self-supervised learning, which hinders learning good semantic representations. It proposes novel strategies for identifying and mitigating the effects of false negatives to improve the performance of contrastive learning methods.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1,\n        \"b21\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b33\": 0.9,\n        \"b38\": 0.8,\n        \"b24\": 0.8,\n        \"b11\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b43\": 0.7,\n        \"b22\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of identifying and mitigating the impact of false negatives in contrastive learning for self-supervised visual representation learning. It proposes two strategies: false negative elimination and false negative attraction, and demonstrates their effectiveness in improving model performance.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b8\": 0.85,\n    \"b33\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b38\": 0.65,\n    \"b24\": 0.65,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5,\n    \"b41\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of false negatives in contrastive self-supervised learning, where different images with similar semantic content are treated as negative pairs. This issue hinders the learning of good semantic representations. The authors propose methods to identify and handle false negatives, namely false negative elimination and attraction, which significantly improve contrastive learning outcomes.\",\n    \"Direct Inspiration\": {\n        \"b21\": 0.9,\n        \"b8\": 0.85,\n        \"b33\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b38\": 0.75,\n        \"b24\": 0.7,\n        \"b11\": 0.65,\n        \"b25\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.5,\n        \"b43\": 0.5,\n        \"b41\": 0.5,\n        \"b23\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge highlighted in the paper is the issue of false negatives in contrastive self-supervised learning, which can hinder the learning of good semantic representations. The authors propose novel strategies to identify and mitigate the effects of false negatives, including false negative elimination and attraction.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b8\": 0.85,\n    \"b33\": 0.8,\n    \"b25\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.7,\n    \"b22\": 0.65,\n    \"b23\": 0.6,\n    \"b41\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b38\": 0.45,\n    \"b24\": 0.4,\n    \"b6\": 0.35\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the issue of false negatives in contrastive self-supervised learning, which hampers the quality of learned representations by discarding semantic information and slowing convergence. The paper proposes novel strategies to identify and mitigate false negatives, including false negative elimination and attraction, to improve the contrastive loss and enhance the performance of contrastive learning-based methods.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b8\": 0.8,\n    \"b33\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.75,\n    \"b38\": 0.7,\n    \"b24\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b39\": 0.55\n  }\n}\n```"], "5bbacb1e17c44aecc4eaac69": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of accurately predicting survival outcomes using whole slide pathological images (WSIs). It proposes a novel method called DeepGraphSurv, which constructs survival-specific graphs from WSI patches and performs graph convolution to learn topological features relevant to survival analysis. The key innovations include the use of survival-specific graphs and an attention mechanism to focus on regions of interest within the WSIs.\",\n    \"Direct Inspiration\": {\n        \"b4\": 0.9,\n        \"b21\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.75,\n        \"b12\": 0.7,\n        \"b14\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b10\": 0.6,\n        \"b13\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of accurately predicting survival probabilities using whole slide images (WSIs) in clinical settings. The proposed methodology, DeepGraphSurv, constructs a graph-based model to capture global topological representations of WSIs. Key innovations include the use of survival-specific graphs and the integration of a graph attention mechanism to focus on regions of interest. The approach is validated on multiple cancer datasets, demonstrating improved performance over existing methods.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1.0,\n        \"b21\": 1.0,\n        \"b4\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.8,\n        \"b11\": 0.7,\n        \"b12\": 0.6,\n        \"b14\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.5,\n        \"b10\": 0.4,\n        \"b7\": 0.4,\n        \"b9\": 0.3,\n        \"b18\": 0.3,\n        \"b17\": 0.3,\n        \"b16\": 0.3\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in survival analysis by proposing a novel model, DeepGraphSurv, which leverages graph convolutional networks (GCNs) to learn survival-specific topological representations from whole slide images (WSIs) of pathological data. The primary challenge is integrating topological properties of WSIs into survival prediction, overcoming limitations of previous models that either oversimplified survival probabilities or failed to utilize the topological structure of WSIs effectively.\",\n  \"Direct Inspiration\": [\n    \"b8\",\n    \"b21\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b4\",\n    \"b12\",\n    \"b14\"\n  ],\n  \"Other Inspiration\": [\n    \"b2\",\n    \"b13\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of survival analysis using whole slide images (WSIs). It proposes a novel method, DeepGraphSurv, which models WSIs as graphs to learn global topological representations for survival prediction. The method involves constructing graphs from patch-wise features, fine-tuning these features with survival-specific graphs under supervision, and utilizing attention mechanisms to focus on regions of interest on WSIs.\",\n  \"Direct Inspiration\": [\"b21\", \"b4\"],\n  \"Indirect Inspiration\": [\"b2\", \"b8\", \"b11\"],\n  \"Other Inspiration\": [\"b5\", \"b12\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in survival analysis using whole slide images (WSIs), focusing on the need for efficient survival prediction models that can utilize the topological structures of WSIs. The primary inspiration for the proposed model, DeepGraphSurv, includes methods for graph construction, spectral graph convolution, and supervised graph learning tailored to survival analysis.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b11\": 0.6,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.4,\n    \"b12\": 0.5,\n    \"b14\": 0.5\n  }\n}\n```"], "5d79a6ff3a55ac5b650357fb": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include inter-core interference caused by sharing LLC and off-chip memory bandwidth when more cores are deployed in modern multicore processors. The proposed algorithm is a multi-resource management framework (CMM) combining prefetch throttling and cache partitioning (CP) to address performance isolation and fairness across processes/cores. The framework is implemented as a Linux kernel module and targets Intel multicore processors but can be applied to any processor with similar capabilities.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b16\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b14\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b23\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of destructive inter-core interference in modern multicore processors caused by sharing LLC and off-chip memory bandwidth, exacerbated by prefetch traffic from multiple hardware prefetchers. The authors propose a coordinated multi-resource management (CMM) framework that combines prefetch throttling and cache partitioning to improve performance isolation and fairness across processes/cores. The framework is implemented as a Linux kernel module and targets Intel multicore processors.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b16\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b14\": 0.7,\n    \"b21\": 0.65,\n    \"b22\": 0.65,\n    \"b23\": 0.6,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b2\": 0.5,\n    \"b3\": 0.5,\n    \"b4\": 0.5,\n    \"b6\": 0.5,\n    \"b8\": 0.5,\n    \"b9\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of inter-core interference caused by prefetch traffic in modern multicore processors. It proposes a coordinated multi-resource management (CMM) framework that combines prefetch throttling and cache partitioning to improve performance isolation and fairness across processes. The framework is designed to be low-overhead and implemented as a Linux kernel module, targeting Intel multicore processors but applicable to any processor with similar capabilities.\",\n    \"Direct Inspiration\": {\n        \"b5\": 0.9,\n        \"b7\": 0.8,\n        \"b16\": 0.95\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.7,\n        \"b19\": 0.65,\n        \"b20\": 0.6,\n        \"b21\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b23\": 0.5,\n        \"b24\": 0.55\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of destructive inter-core interference caused by sharing LLC and memory bandwidth in multicore processors, exacerbated by multiple hardware prefetchers. It proposes the CMM framework, which combines cache partitioning (CP) and prefetch throttling to improve performance isolation and fairness. The framework is implemented as a Linux kernel module and evaluated on an Intel multicore processor.\",\n    \"Direct Inspiration\": {\n        \"b5\": 0.95,\n        \"b16\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.85,\n        \"b24\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.75,\n        \"b22\": 0.7,\n        \"b23\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of destructive inter-core interference in multicore processors due to shared LLC and off-chip memory bandwidth, exacerbated by prefetch traffic. The proposed solution combines cache partitioning (CP) and prefetch throttling into a software-based framework called CMM, which is implemented as a Linux kernel module and evaluated on Intel processors.\",\n  \"Direct Inspiration\": [\n    \"b16\",\n    \"b7\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b14\",\n    \"b15\",\n    \"b17\"\n  ],\n  \"Other Inspiration\": [\n    \"b5\",\n    \"b23\",\n    \"b24\"\n  ]\n}\n```"], "53e9ac5bb7602d970362bd46": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are detecting program phase changes accurately and developing metrics for comparing different phase detection techniques. The paper proposes and evaluates three schemes for detecting phase changes: working set signatures, basic block vectors, and a conditional branch counter. The goal is to improve power/performance characteristics through dynamic adaptation based on program phases.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b10\": 1.0,\n    \"b16\": 1.0,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of dynamically detecting program phase changes to optimize power and performance in adaptable hardware and software environments. It focuses on three major techniques: working set signatures, basic block vectors, and conditional branch counters, and compares them using various performance metrics.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b10\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting program phase changes to optimize power and performance in dynamically adaptable systems. It proposes three phase detection techniques based on working set signatures, basic block vectors, and conditional branch counters. The paper also defines metrics for comparing these techniques and evaluates their performance with unbounded hardware resources.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b10\": 0.9,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting program phase changes in dynamically adaptable systems, which is crucial for optimizing performance and power efficiency. The authors examine three primary techniques for phase detection: working set signatures, basic block vectors, and conditional branch counters. They propose metrics for comparing these techniques and evaluate their performance under various conditions.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b10\": 1,\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b11\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting program phase changes to optimize power and performance in dynamically adaptable systems. The authors propose and compare three phase detection techniques: working set signatures, basic block vectors (BBVs), and conditional branch counters. The paper introduces metrics for comparing these techniques and discusses the implications of phase detection on power and performance optimization.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b10\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.9,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```"], "573697316e3b12023e621b53": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing cache usage in multicore systems through a novel concept called partition-sharing, combining aspects of both partitioning and sharing. The key contributions include the introduction of Natural Cache Partition (NCP) for theoretical reduction, and an optimal partitioning algorithm that handles various program types and objective functions, such as fairness and quality of service.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b15\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.80,\n    \"b10\": 0.75\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimal cache partition-sharing for multicore processors, combining both partitioning and sharing techniques to maximize throughput and fairness. The proposed solution introduces the concept of Natural Cache Partition and an optimization algorithm that generalizes previous methods, allowing for any miss ratio curve and multiple objective functions. The approach is validated through theoretical derivations and empirical evaluations on benchmark programs.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of cache partitioning and sharing in multicore systems. It introduces the concept of partition-sharing to optimize both throughput and fairness. The proposed algorithm reduces partition-sharing to partitioning and provides an optimal solution using dynamic programming. Key contributions include the natural cache partition, the optimal partitioning algorithm, and evaluations demonstrating improved performance and fairness.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b10\": 0.75\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing cache partitioning and sharing in multicore systems. It proposes a novel 'partition-sharing' scheme, which combines both partitioning and sharing strategies to maximize throughput and fairness. The solution has two main components: reducing the problem to partitioning using the concept of Natural Cache Partition, and developing an optimization algorithm that handles various objective functions, such as fairness and quality of service.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b15\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing cache partition-sharing for multicore systems to improve both throughput and fairness. It introduces the concept of Natural Cache Partition (NCP) and proposes a dynamic programming algorithm to find the optimal partitioning. The approach generalizes previous methods by allowing for non-convex miss ratio curves and multiple objective functions. The paper evaluates its method through a set of benchmark programs, demonstrating significant improvements over existing solutions.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b15\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": []\n}\n```"], "5a260c8417c44a4ba8a31186": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently generating custom instructions for a deep learning hardware accelerator. It introduces a compiler that converts high-level model representations into instruction streams for the Snowflake architecture. The primary contributions include a software framework for instruction generation, optimization for bandwidth usage, and communication load balancing. Key references include previous work on deep learning models, compiler design for CNN accelerators, and specific hardware architectures.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.95,\n    \"b6\": 0.9,\n    \"b24\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b21\": 0.75,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.65,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in efficiently generating custom instructions for deep learning hardware accelerators, particularly targeting CNN models. The primary contributions include a software framework for custom instruction generation, optimizing data bandwidth usage, and load balancing for better memory bandwidth utilization.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b8\": 0.9,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b11\": 0.6,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the cumbersome and error-prone process of manually crafting assembly-like instructions for deep learning models, and the need for customization on both hardware and software sides. The paper introduces a software framework and compiler, Snowball, which generates custom instructions for a CNN-targeted hardware accelerator, Snowflake, to address these issues. Key contributions include optimizing memory bandwidth usage, communication load balancing, and using a high-level model representation to generate instruction streams for Snowflake.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b6\": 1.0,\n    \"b8\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b11\": 0.7,\n    \"b24\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of manually crafting assembly-like instructions for custom deep learning accelerators, which is cumbersome and error-prone. The proposed solution is a compiler that generates instructions and manages data for the Snowflake hardware accelerator, optimizing performance and memory bandwidth usage.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b8\": 0.8,\n    \"b12\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.6,\n    \"b7\": 0.6,\n    \"b10\": 0.5,\n    \"b21\": 0.4\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.3,\n    \"b24\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in designing a compiler for custom deep learning hardware accelerators. It introduces a software framework for generating custom instructions, optimizing memory bandwidth usage, and balancing communication loads. The core contributions include generating instructions for CNN-targeted hardware, optimizing data transfer strategies, and implementing communication load balancing.\",\n  \"Direct Inspiration\": [\n    \"b6\",\n    \"b8\",\n    \"b12\",\n    \"b24\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b7\",\n    \"b10\",\n    \"b13\",\n    \"b17\"\n  ],\n  \"Other Inspiration\": [\n    \"b0\",\n    \"b11\",\n    \"b18\"\n  ]\n}\n```"], "5aed14d617c44a4438158d78": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of the memory wall in von Neumann computers by proposing a novel sequence-based neural network approach for prefetching. The authors are inspired by recent works in image and audio generation, specifically PixelRNN and Wavenet, and aim to apply these sequence learning techniques to the problem of prefetching in microarchitectural systems.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.8,\n    \"b33\": 0.7,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.5,\n    \"b13\": 0.4,\n    \"b14\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of the memory wall in modern microprocessors by proposing the use of sequence-based neural networks for prefetching. The core contribution is leveraging sequence learning, particularly LSTM-based models, to improve prediction accuracy in memory access patterns, inspired by methods in image and audio generation.\",\n  \"Direct Inspiration\": [\"b37\"],\n  \"Indirect Inspiration\": [\"b38\"],\n  \"Other Inspiration\": [\"b1\", \"b31\", \"b13\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of the memory wall in computer architecture, where computation is much faster than memory access, leading to delays. The authors propose using sequence-based neural networks, particularly LSTMs, to improve prefetching by predicting future memory accesses. They draw inspiration from recent works in image and audio generation (e.g., PixelRNN and Wavenet) to discretize the space and make prefetching analogous to neural language models.\",\n  \"Direct Inspiration\": [\n    \"b37\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b38\",\n    \"b31\",\n    \"b14\"\n  ],\n  \"Other Inspiration\": [\n    \"b8\",\n    \"b17\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge in the paper is to mitigate the memory wall issue in computer architecture by improving memory prefetching techniques. The authors propose using sequence-based neural networks, specifically LSTMs, to predict memory access patterns. By treating the prefetching problem as a classification task rather than a regression problem, the authors leverage recent advancements in image and audio generation models, particularly PixelRNN and Wavenet, to discretize the output space effectively.\",\n  \"Direct Inspiration\": [\"b37\"],\n  \"Indirect Inspiration\": [\"b13\", \"b14\", \"b31\", \"b38\"],\n  \"Other Inspiration\": [\"b0\", \"b6\", \"b10\", \"b11\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of mitigating the memory wall in computer architecture by using neural networks for sequence-based prediction in prefetching. The authors propose using sequence learning algorithms, particularly LSTM-based models, to predict memory access patterns more effectively than traditional hardware-based prefetchers.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1.0,\n    \"b38\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b17\": 0.5,\n    \"b33\": 0.4,\n    \"b28\": 0.3\n  }\n}\n```"], "5d1b2f5a3a55ac071793c55c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating classical Chinese poetry that meets strict form and content requirements. It proposes a method based on the pre-trained GPT model, which is fine-tuned with a classical Chinese poetry corpus. The method is simple yet effective, requiring no additional human-defined rules or features, and employs a truncated top-k sampling strategy for diverse and artistic poem generation.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b4\": 1,\n    \"b3\": 1,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is generating classical Chinese poetry that meets both form and content requirements using a simplified approach based on the pre-trained GPT model. The proposed method focuses on model conciseness, well-formedness, poetry diversity, and artistry in the generated poems. The authors fine-tune a GPT model with a classical Chinese poetry corpus and employ truncated top-k sampling for diverse text generation.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.9,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b1\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating classical Chinese poetry that meets both form and content requirements using a simplified GPT-based model. The proposed method eliminates the need for human-defined rules or specially designed neural networks, relying instead on a pre-trained GPT language model fine-tuned with a classical Chinese poetry corpus.\",\n  \"Direct Inspiration\": {\n    \"ref1\": \"b5\",\n    \"ref2\": \"b2\",\n    \"ref3\": \"b4\",\n    \"ref4\": \"b3\"\n  },\n  \"Indirect Inspiration\": {\n    \"ref1\": \"b8\",\n    \"ref2\": \"b9\",\n    \"ref3\": \"b6\",\n    \"ref4\": \"b7\"\n  },\n  \"Other Inspiration\": {\n    \"ref1\": \"b11\",\n    \"ref2\": \"b10\",\n    \"ref3\": \"b13\",\n    \"ref4\": \"b12\"\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of generating classical Chinese poetry that meets strict form and content requirements using a pre-trained GPT model. The novel method proposed eliminates the need for human-defined rules or features, and instead leverages fine-tuning on a classical Chinese poetry corpus to achieve well-formed, coherent, and diverse poetry generation.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b3\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b4\": 0.7,\n        \"b5\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.4,\n        \"b11\": 0.4,\n        \"b12\": 0.4,\n        \"b13\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges of generating classical Chinese poetry, which must meet both form and content requirements. The authors propose a method based on a pre-trained GPT model fine-tuned with a classical Chinese poetry corpus. Key features of their approach include model conciseness, well-formedness, poetry diversity, and artistry.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.85,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```"], "5c6a37d03a69b1c9e12a9fc4": ["```json\n{\n    \"Summary\": \"The main challenge addressed in this paper is the efficient and accurate computation of graph similarity. The authors propose a novel approach called SimGNN, which leverages neural networks to transform the graph similarity computation problem into a learning problem. The paper introduces two key strategies: a global context-aware attention mechanism for graph-level embedding and a pairwise node comparison method to supplement the embeddings with fine-grained node-level information.\",\n    \"Direct Inspiration\": {\n        \"b37\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b21\": 0.9,\n        \"b14\": 0.8,\n        \"b16\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.7,\n        \"b5\": 0.7,\n        \"b8\": 0.7,\n        \"b27\": 0.7,\n        \"b34\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenging problem of graph similarity computation by proposing a novel neural network-based approach called SimGNN. The key challenges include the NP-completeness of computing exact graph distances and the inefficiency of existing approximate methods. SimGNN aims to provide an efficient and effective solution by transforming the problem into a learning task using graph neural networks. Two main strategies are proposed: a graph-level embedding interaction mechanism and a pairwise node comparison method. Extensive experiments demonstrate the effectiveness and efficiency of the proposed approach.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b5\": 0.7,\n    \"b8\": 0.7,\n    \"b27\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of graph similarity computation, which is crucial yet difficult due to the NP-complete nature of exact graph distance metrics like Graph Edit Distance (GED). The authors propose SimGNN, a neural network-based approach, to map pairs of graphs into similarity scores using graph embeddings and pairwise node comparisons.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b5\": 0.8,\n    \"b8\": 0.85,\n    \"b27\": 0.95,\n    \"b34\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b4\": 0.75,\n    \"b47\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.65,\n    \"b48\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently computing graph similarity by proposing a novel neural network-based approach called SimGNN. The primary inspiration is from existing methods that either prune exact computations or find approximate solutions. SimGNN transforms the problem into a learning task, leveraging graph neural networks to map graph pairs into similarity scores. Two strategies are proposed: graph-level embeddings with attention mechanisms and pairwise node comparison.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b5\": 0.9,\n    \"b8\": 0.9,\n    \"b27\": 0.9,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b47\": 0.7,\n    \"b48\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient graph similarity computation by proposing a neural network-based approach called SimGNN. The two core strategies are: (1) a global context-aware attention mechanism for graph-level embeddings and (2) a pairwise node comparison method to supplement the graph-level information. The key contributions are the novel attention mechanism and the combined use of graph-level and node-level embeddings for effective and efficient similarity computation.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b5\": 0.7,\n    \"b8\": 0.7,\n    \"b27\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b4\": 0.5,\n    \"b47\": 0.5,\n    \"b48\": 0.5\n  }\n}\n```"], "5736982b6e3b12023e6fd21d": ["```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies in server-side JavaScript applications, particularly focusing on the asynchronous event-driven programming model. It identifies severe microarchitecture inefficiencies, such as instruction cache and TLB misses and branch misprediction rate, which hinder single-core performance. The paper proposes coordinating the cache insertion policy and instruction prefetcher to exploit inter-event code reuse and improve front-end efficiency.\",\n  \"Direct Inspiration\": [\"b11\", \"b12\"],\n  \"Indirect Inspiration\": [\"b6\", \"b7\", \"b8\", \"b9\"],\n  \"Other Inspiration\": [\"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper include severe microarchitecture inefficiencies in server-side event-driven JavaScript applications, particularly front-end bottlenecks such as high instruction cache and TLB misses and branch misprediction rates. The paper focuses on improving single-core performance for such applications by optimizing instruction cache efficiency.\",\n    \"inspirations\": \"The paper draws inspiration from recent studies on client-side event-driven applications, as well as existing research on garbage collection, type checking, exploiting parallelisms, and leveraging hardware heterogeneity in dynamic languages.\"\n  },\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b11\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of server-side asynchronous event-driven programming in JavaScript, specifically focusing on the front-end inefficiencies arising from the single-threaded event loop model. The authors propose optimizing the instruction cache by coordinating the cache insertion policy and instruction prefetcher to exploit inter-event code reuse.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b10\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies in server-side event-driven programming, particularly focusing on the front-end bottlenecks caused by the single-threaded event loop in JavaScript applications. It proposes optimizing the instruction cache through coordinated cache insertion policy and prefetching strategy to mitigate these inefficiencies.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b2\": 0.5,\n    \"b3\": 0.5,\n    \"b4\": 0.5,\n    \"b5\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include inefficiencies in single-threaded event-driven server-side JavaScript applications, particularly in front-end bottlenecks such as high instruction cache and TLB misses, and branch misprediction rates. The proposed solution involves coordinating cache insertion policies and prefetching strategies to exploit inter-event code reuse to reduce these inefficiencies.\",\n  \"Direct Inspiration\": [\"b6\", \"b7\", \"b10\"],\n  \"Indirect Inspiration\": [\"b1\", \"b2\", \"b3\", \"b4\", \"b5\"],\n  \"Other Inspiration\": [\"b8\", \"b9\", \"b11\", \"b12\", \"b19\", \"b20\"]\n}\n```"], "5f803c8f91e01119a5df749b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of slow convergence and high computational complexity in the DETR model for object detection. It proposes Deformable DETR, which combines deformable convolution and Transformer attention mechanisms to create a more efficient and faster-converging object detector. Key contributions include the deformable attention module and multi-scale deformable attention module, which help mitigate the issues of handling high-resolution feature maps and slow training convergence.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b25\": 0.5,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces Deformable DETR to address the slow convergence and high complexity issues of DETR, which arise from its use of Transformer attention modules for processing image feature maps. The proposed approach utilizes deformable attention modules inspired by deformable convolution to attend to a sparse set of spatial locations, thereby reducing computational and memory complexities while improving performance, especially for small object detection. The paper also explores iterative bounding box refinement and a two-stage variant to further enhance detection accuracy.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.95,\n    \"b2\": 0.9,\n    \"b17\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.75,\n    \"b25\": 0.7,\n    \"b16\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b28\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issues of slow convergence and high complexity in the DETR model for object detection. It proposes Deformable DETR, which combines the advantages of deformable convolution and Transformer attention to achieve faster convergence and better performance, especially on small objects. Key innovations include the deformable attention module and multi-scale deformable attention module.\",\n  \"Direct Inspiration\": [\"b2\", \"b5\"],\n  \"Indirect Inspiration\": [\"b17\", \"b33\"],\n  \"Other Inspiration\": [\"b25\", \"b16\", \"b8\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of slow convergence and high complexity in the DETR object detector. The proposed solution, Deformable DETR, combines the sparse spatial sampling of deformable convolution with the relation modeling capability of Transformers. The paper introduces a deformable attention module to efficiently process image feature maps and a multi-scale deformable attention module to handle multi-scale features.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b5\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b17\": 0.7,\n    \"b25\": 0.6,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.5,\n    \"b42\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of the DETR object detector, specifically its slow convergence and poor performance on small objects. The authors propose Deformable DETR, which combines deformable convolution with Transformer attention to create a more efficient and fast-converging object detector.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b5\": 1,\n    \"b42\": 1,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b16\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```"], "5aed14d617c44a4438158cff": ["```json\n{\n  \"Summary\": \"The paper addresses three main challenges in Taobao's recommendation system: scalability, sparsity, and cold start. It proposes a two-stage framework with matching and ranking stages. In the matching stage, it constructs an item graph from user behavior history and uses graph embedding methods to learn item embeddings. The novel contributions include Base Graph Embedding (BGE), Graph Embedding with Side information (GES), and Enhanced Graph Embedding with Side information (EGES). The framework is implemented on Taobao\u2019s XTensorflow platform, showing significant improvements in recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b10\": 0.7,\n    \"b15\": 0.7,\n    \"b12\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three major technical challenges in Taobao's recommendation system: scalability, sparsity, and cold start. It proposes a two-stage recommending framework with a matching stage using graph embedding methods, including BGE, GES, and EGES, to compute item similarities and generate candidate sets. The paper emphasizes the importance of incorporating side information to enhance embeddings, particularly for cold-start items.\",\n  \"Direct Inspiration\": [\"b14\"],\n  \"Indirect Inspiration\": [\"b7\", \"b16\"],\n  \"Other Inspiration\": [\"b8\", \"b10\", \"b15\", \"b12\", \"b13\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three primary challenges in Taobao's recommendation system: scalability, sparsity, and cold start problems. It proposes a two-stage recommendation framework (matching and ranking), focusing on the matching stage where pairwise similarities between items are computed using graph embedding methods. The authors introduce three embedding methods: Base Graph Embedding (BGE), Graph Embedding with Side Information (GES), and Enhanced Graph Embedding with Side Information (EGES), and demonstrate their effectiveness in improving recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b12\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b10\": 0.8,\n    \"b15\": 0.8,\n    \"b16\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b21\": 0.7,\n    \"b7\": 0.75,\n    \"b1\": 0.75,\n    \"b11\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses major technical challenges in Taobao's recommendation system, including scalability, sparsity, and cold start problems. It proposes a two-stage recommendation framework focusing on the matching stage with novel graph embedding techniques: Base Graph Embedding (BGE), Graph Embedding with Side information (GES), and Enhanced Graph Embedding with Side information (EGES). These methods aim to improve recommendation accuracy by leveraging user behavior and side information.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b8\": 0.6,\n    \"b10\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses three major technical challenges in Taobao's recommendation system: Scalability, Sparsity, and Cold Start. To tackle these, the authors propose a two-stage recommendation framework consisting of matching and ranking stages. They introduce three embedding methods: Base Graph Embedding (BGE), Graph Embedding with Side information (GES), and Enhanced Graph Embedding with Side information (EGES). The proposed framework leverages graph embedding techniques to capture higher-order similarities and incorporates side information to improve embeddings, especially for cold-start items.\",\n    \"Direct Inspiration\": {\n        \"b12\": 0.9,\n        \"b13\": 0.9,\n        \"b14\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.7,\n        \"b10\": 0.7,\n        \"b15\": 0.7,\n        \"b16\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.6,\n        \"b5\": 0.6,\n        \"b21\": 0.6\n    }\n}\n```"], "5ed7796e91e011e6e91120f0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of detecting political ideology on Twitter by leveraging social network links rather than just text. It introduces a new model, TIMME (Twitter Ideology-detection via Multi-task Multi-relational Embedding), which captures interactions between different types of relations on Twitter to improve ideology detection. The primary challenges include handling large and dense datasets, sparse labels, incomplete features, and heterogeneous links.\",\n  \"Direct Inspiration\": [\n    \"b12\",\n    \"b9\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b13\",\n    \"b18\",\n    \"b32\"\n  ],\n  \"Other Inspiration\": [\n    \"b4\",\n    \"b7\",\n    \"b14\",\n    \"b15\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting ordinary citizens' ideology using social network data, specifically Twitter. This involves dealing with large, dense datasets, sparse labels, incomplete features, and heterogeneous links. The authors propose TIMME (Twitter Ideology-detection via Multi-task Multi-relational Embedding) to handle these challenges by using a GNN-based approach that captures interactions between different types of relations.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b12\": 1,\n    \"b13\": 1,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of ideology detection on Twitter by developing TIMME (Twitter Ideology-detection via Multi-task Multi-relational Embedding). The primary challenges include handling large, densely linked datasets, sparse labels, incomplete features, and the need to model heterogeneous relationships effectively. The proposed TIMME model aims to capture the interactions between different relations and treats them separately while measuring their importance for ideology detection. The paper highlights the inadequacies of existing models and positions TIMME as a superior alternative.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b13\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b16\": 0.7,\n    \"b9\": 0.65,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.55,\n    \"b36\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of detecting ideology on Twitter by leveraging social network links, which have been underutilized in prior works. The proposed TIMME model uses multi-task multi-relational embedding to handle sparsely-labeled large graphs with heterogeneous links, overcoming issues such as high data density, sparse labels, incomplete features, and heterogeneity.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b12\": 0.85,\n    \"b36\": 0.8,\n    \"b39\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b3\": 0.65,\n    \"b31\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.55,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include dealing with large-scale and dense Twitter datasets, sparse labels, incomplete features, and modeling the heterogeneity of link types. The proposed algorithm, TIMME (Twitter Ideology-detection via Multi-task Multi-relational Embedding), addresses these challenges by utilizing a multi-relational GCN-based model that captures the interactions between different relations and treats them separately for ideology detection.\",\n  \"Direct Inspiration\": [\"b13\", \"b12\"],\n  \"Indirect Inspiration\": [\"b4\", \"b7\", \"b14\", \"b15\", \"b16\"],\n  \"Other Inspiration\": [\"b18\", \"b36\", \"b39\"]\n}\n```"], "5736986b6e3b12023e72fc0c": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of accurately labeling large datasets using crowdsourcing, which often involves noisy labels from workers with varying reliability. The proposed algorithm, Max-Margin Majority Voting (M3V), enhances the discriminative ability of majority voting by incorporating max-margin principles and Bayesian techniques to combine generative and discriminative approaches.\",\n    \"Direct Inspiration\": [\"b4\", \"b26\"],\n    \"Indirect Inspiration\": [\"b10\", \"b3\"],\n    \"Other Inspiration\": [\"b6\", \"b1\", \"b7\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The main challenge addressed is the accurate aggregation of noisy labels from crowdsourced data, considering the varying reliability of workers.\",\n    \"algorithm\": \"The paper proposes a max-margin majority voting (M3V) estimator and a Bayesian generalization that integrates both generative and discriminative approaches to improve label aggregation accuracy.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b4\", \"b10\", \"b26\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b3\", \"b7\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b1\", \"b9\", \"b11\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving label accuracy in crowdsourced datasets by presenting a max-margin formulation of majority voting and a Bayesian generalization. It leverages both generative and discriminative approaches to better infer true labels from noisy observations.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b26\": 1,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b1\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving labeling accuracy in crowdsourcing tasks by proposing a max-margin formulation of majority voting and a Bayesian generalization that combines generative and discriminative approaches. The authors refine the majority voting estimator to maximize the margin between true and alternative labels and couple it with a generative model under Bayesian inference, leading to improved performance in label aggregation tasks.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the accuracy of crowdsourced labeling, which is often noisy due to the varying reliability of workers. It proposes a novel max-margin majority voting (M3V) method and a Bayesian generalization that combines generative and discriminative approaches to enhance label aggregation accuracy.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b10\": 0.7,\n    \"b3\": 0.7,\n    \"b1\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```"], "53e9adc7b7602d97037cd6f8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatically detecting F-formations in crowded social scenes, which are clusters of individuals engaged in focused interactions. It improves upon the baseline method by Yu et al. by formulating the problem as one of identifying dominant sets in edge-weighted graphs and introduces a modification to Pavan and Pelillo's peeling off strategy.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b19\", \"b16\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b4\", \"b9\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b15\", \"b21\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatically detecting F-formations in crowded social scenes, which are specific types of focused encounters where individuals maintain equal access to a shared space. The proposed method improves upon previous work by formulating the problem as identifying dominant sets in edge-weighted graphs and introducing a stopping criterion to better detect singletons.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b19\": 1,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b4\": 0.75,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.65,\n    \"b2\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatically estimating relationships between humans in crowded social scenes, specifically focusing on identifying F-formations. The authors propose a novel framework by extending prior work in psychology and social sciences and improving upon existing methods using graph-theoretic approaches, particularly dominant sets.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b19\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b15\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.8,\n    \"b21\": 0.7,\n    \"b2\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of detecting F-formations in crowded social scenes, which are groups of individuals maintaining a shared interaction space. The proposed method improves upon previous work by formulating the problem as identifying dominant sets in edge-weighted graphs, allowing for a more accurate detection of these formations compared to the modularity cut approach.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b3\": 0.5,\n    \"b21\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of estimating relationships between humans in freely formed groups by focusing on detecting F-formations in crowded social scenes. It proposes a new method based on identifying dominant sets, improving upon the modularity cut algorithm.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b4\": 0.8,\n    \"b16\": 0.85,\n    \"b15\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b13\": 0.55,\n    \"b20\": 0.5\n  }\n}\n```"], "5b3d98cc17c44a510f80212a": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the manual optimization of tensor operations for diverse hardware back-ends, which requires significant engineering effort. The proposed algorithm is a machine learning framework that uses statistical cost models to predict program run time and guide the optimization of tensor operator programs. The framework leverages transferable representations to generalize across different workloads and accelerate search using transfer learning.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b4\": 0.9,\n    \"b41\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.8,\n    \"b32\": 0.8,\n    \"b34\": 0.8,\n    \"b12\": 0.7,\n    \"b16\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b37\": 0.6,\n    \"b26\": 0.6,\n    \"b25\": 0.6,\n    \"b20\": 0.6,\n    \"b0\": 0.5,\n    \"b3\": 0.5,\n    \"b7\": 0.5,\n    \"b1\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of optimizing tensor programs for different hardware platforms using machine learning models. The key contributions include formalizing the problem, proposing a machine learning framework, and accelerating the optimization using transfer learning.\",\n    \"Direct Inspiration\": {\n        \"b31\": 1.0,\n        \"b4\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b33\": 0.8,\n        \"b32\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b19\": 0.7,\n        \"b41\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing tensor programs for diverse hardware platforms using machine learning techniques. The authors propose a statistical cost model to predict run time and use transfer learning to accelerate optimization.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b4\": 0.9,\n    \"b41\": 0.9,\n    \"b40\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b33\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b37\": 0.5,\n    \"b26\": 0.5,\n    \"b25\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing tensor operator programs for various hardware platforms using machine learning. The authors propose a machine learning framework that includes statistical cost models to predict program runtime and utilize transfer learning to accelerate optimization. Key contributions include formalizing the problem of learning to optimize tensor programs, proposing a machine learning framework, and improving optimization efficiency.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b12\": 0.7,\n    \"b32\": 0.7,\n    \"b33\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.8,\n    \"b36\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on leveraging machine learning to optimize tensor operator programs, addressing the challenge of manually optimizing high-performance tensor operation libraries for diverse hardware back-ends. The proposed solution includes statistical cost models to predict runtime and a machine learning framework that accelerates optimization using transfer learning.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.95,\n    \"b4\": 0.90,\n    \"b41\": 0.90,\n    \"b40\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.75,\n    \"b32\": 0.75,\n    \"b34\": 0.75,\n    \"b12\": 0.75,\n    \"b16\": 0.75,\n    \"b24\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.60,\n    \"b37\": 0.60,\n    \"b26\": 0.60,\n    \"b25\": 0.60,\n    \"b20\": 0.60\n  }\n}\n```"], "53e9ba39b7602d970464970c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of inclusive and exclusive last-level caches (LLCs) in terms of performance and efficiency. The main focus is on designing bypass and insertion algorithms for exclusive LLCs to improve performance by reducing unnecessary cache fills and optimizing cache utilization.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in inclusive LLC designs, specifically the inefficiencies caused by data replication across cache levels and premature evictions. The authors propose novel bypass and insertion age algorithms for exclusive LLCs to minimize these inefficiencies and improve performance.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b3\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b19\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7,\n    \"b24\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of inclusive last-level cache (LLC) designs, specifically focusing on premature evictions and inefficient use of silicon estate due to cross-level replication. The proposed algorithms aim to improve cache performance through selective bypass and optimized insertion age algorithms in exclusive LLCs.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b14\": 1,\n    \"b3\": 1,\n    \"b26\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b5\": 0.8,\n    \"b11\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of inclusive last-level caches (LLCs) in terms of coherence simplification and performance degradation due to cross-level replication. It proposes novel bypass and insertion algorithms for exclusive LLCs to improve performance by reducing bandwidth demand and allocating LLC capacity more efficiently.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b3\": 0.7,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b4\": 0.6,\n    \"b6\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper explores the challenges of implementing LLC bypass algorithms in exclusive last-level caches (LLCs) to improve performance. It focuses on designing effective bypass and insertion age algorithms for exclusive LLCs to reduce LLC misses and improve overall cache performance.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b24\": 0.7,\n    \"b6\": 0.6,\n    \"b19\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```"], "5dea04309e795e693620e97c": ["```json\n{\n    \"Summary\": \"This paper addresses the challenges of author disambiguation in digital libraries by proposing a heterogeneous graph convolutional network embedding method and an efficient clustering strategy that does not require specifying the number of clusters. The proposed solution integrates multi-layer relationships and semantic information of publications to generate high-quality representations.\",\n    \"Direct Inspiration\": {\n        \"b15\": 0.9,\n        \"b16\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.7,\n        \"b4\": 0.7,\n        \"b3\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of author name disambiguation in digital libraries by proposing a framework that integrates a heterogeneous network embedding method with a graph convolutional network, efficient clustering strategies, and incremental disambiguation approaches.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b3\": 0.75,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b21\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of author name disambiguation in digital libraries, focusing on learning effective publication representations, efficient clustering without pre-set parameters, and incremental processing of new publications. The proposed framework utilizes a heterogeneous graph convolutional network embedding method, a novel clustering strategy, and incremental disambiguation techniques to improve accuracy and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b15\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b17\": 0.7,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Learning effective representations for publications by extracting and combining different types of features.\",\n      \"Efficiently determining the assignment of publications when the number of distinct persons is unknown.\",\n      \"Efficiently processing new published publications that may contain potentially ambiguous names.\"\n    ],\n    \"Inspirations\": [\n      \"Constructing a heterogeneous network of publications.\",\n      \"Using graph convolutional networks for heterogeneous network embedding.\",\n      \"Developing efficient clustering strategies without requiring the number of clusters.\",\n      \"Introducing incremental disambiguation strategies.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on addressing three main challenges in author name disambiguation: (1) Learning effective representations for publications by combining different types of features. (2) Efficient clustering of publications without knowing the number of distinct authors. (3) Incrementally processing new publications without reprocessing the entire dataset. The proposed solution involves constructing a heterogeneous network of publications, using a graph convolutional network for embedding, and developing an efficient clustering strategy.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b4\": 1,\n    \"b15\": 1,\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b7\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```"], "57aa28de0a3ac518da9896d5": ["```json\n{\n  \"Summary\": \"The paper introduces node2vec, a semi-supervised algorithm for scalable feature learning in networks. The primary challenges addressed include the need for flexible and scalable feature learning methods that can generalize across various network structures and prediction tasks. The algorithm draws inspiration from natural language processing techniques, specifically the Skip-gram model, and extends these ideas to network data. Node2vec uses random walks to generate network neighborhoods and learns feature representations that preserve these neighborhoods, allowing for effective multi-label classification and link prediction tasks.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.95,\n    \"b23\": 0.9,\n    \"b27\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b6\": 0.75,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.65,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b30\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scalable and flexible feature learning in networks for tasks like node classification and link prediction. It introduces node2vec, a semi-supervised algorithm that optimizes a custom graph-based objective function using stochastic gradient descent (SGD). The novelty lies in the use of biased random walks to generate network neighborhoods, allowing the algorithm to capture both community-based and structural role-based node similarities.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b23\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.6,\n    \"b6\": 0.5,\n    \"b9\": 0.5,\n    \"b35\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.4,\n    \"b24\": 0.4,\n    \"b36\": 0.4,\n    \"b17\": 0.4,\n    \"b1\": 0.4,\n    \"b33\": 0.4,\n    \"b3\": 0.4,\n    \"b2\": 0.4,\n    \"b26\": 0.4,\n    \"b29\": 0.4,\n    \"b34\": 0.4,\n    \"b22\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scalable and flexible feature learning in networks for node and edge prediction tasks. The proposed algorithm, node2vec, uses a semi-supervised approach with biased random walks to capture both community structures and structural roles within networks. This method aims to balance computational efficiency and predictive accuracy, overcoming limitations of previous rigid approaches.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b23\": 1.0,\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b6\": 0.8,\n    \"b9\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b36\": 0.6,\n    \"b17\": 0.6,\n    \"b33\": 0.6,\n    \"b3\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces node2vec, a semi-supervised algorithm for scalable feature learning in networks, addressing challenges in defining an objective function for feature learning, balancing computational efficiency and predictive accuracy, and generalizing across different prediction tasks. The approach combines ideas from natural language processing, specifically the Skip-gram model, to maximize the likelihood of preserving network neighborhoods. Key contributions include a flexible notion of network neighborhood using biased random walks, scalability, and extending node representations to pairs of nodes for edge-based prediction tasks.\",\n    \"Direct Inspiration\": [\"b20\", \"b23\"],\n    \"Indirect Inspiration\": [\"b27\"],\n    \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to develop a scalable and flexible algorithm for feature learning in networks that can efficiently optimize a neighborhood-preserving objective using stochastic gradient descent (SGD). The paper introduces node2vec, a semi-supervised algorithm that generalizes previous work by using a flexible notion of network neighborhood through biased random walks, allowing it to learn node representations that capture both community and structural equivalences.\",\n  \"Direct Inspiration\": {\n    \"b20\": 0.9,\n    \"b23\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b6\": 0.6,\n    \"b9\": 0.6,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```"], "53e9b54ab7602d97040825b6": ["```json\n{\n  \"Summary\": \"The main challenges addressed by the paper include ensuring the integrity and consistency of file system data even when malicious parties control the server, and reducing the need for highly trusted administrators and secure server environments. The SUNDR protocol and implementation aim to provide a secure network file system that guarantees these properties using digital signatures and collision-resistant hash functions. The paper also discusses the performance optimization and practical implementation of the protocol.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b25\": 0.9,\n    \"b8\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b20\": 0.75,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b10\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of data security in network file systems, introducing SUNDR, a secure system that ensures file integrity and consistency even when servers are compromised. It uses digital signatures and collision-resistant hash functions to achieve this without relying on trusted parties.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b20\": 0.7,\n    \"b8\": 0.6,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5,\n    \"b3\": 0.5,\n    \"b10\": 0.4,\n    \"b0\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is ensuring the integrity and consistency of file system data even when the server is controlled by malicious parties. The algorithm proposed by the author involves using digital signatures and collision-resistant hash functions to guarantee these properties. A significant aspect of the proposed solution is that it does not require any online trusted parties, unlike previous Byzantine-fault-tolerant file systems. The SUNDR protocol includes mechanisms for secure data storage, consistency protocols, and optimization techniques to enhance performance.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b8\": 0.6,\n    \"b13\": 0.6,\n    \"b10\": 0.4,\n    \"b0\": 0.4,\n    \"b16\": 0.4,\n    \"b9\": 0.4,\n    \"b15\": 0.4,\n    \"b26\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents SUNDR, a secure network file system designed to ensure the integrity and consistency of file system data even when malicious parties control the server. The key challenges addressed are the limitations of traditional security methods that rely on trusted servers and the inconvenience they cause. The proposed algorithm uses digital signatures and a collision-resistant hash function to guarantee data integrity, without requiring any online trusted parties.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b8\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents SUNDR, a secure network file system designed to ensure the integrity and consistency of file system data even when malicious parties control the server. Unlike previous Byzantine-fault-tolerant file systems, SUNDR assumes no online trusted parties and relies on digital signatures and collision-resistant hash functions. The paper addresses performance optimization and the protocol's security properties, with a focus on blocking unauthorized tampering and providing flexibility in data management.\",\n  \"Direct Inspiration\": [\"b12\", \"b8\", \"b13\"],\n  \"Indirect Inspiration\": [\"b2\", \"b20\", \"b25\"],\n  \"Other Inspiration\": [\"b10\", \"b0\", \"b3\", \"b16\"]\n}\n```"], "53e9a396b7602d9702ca5bc7": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of memory dependence prediction in out-of-order processors and explores the idea of bypassing memory dependences using store sets. The authors evaluate the performance gains from memory bypassing and conclude that the gains are relatively small. They investigate the reasons for these meager gains and suggest that there are few opportunities for bypassing and that the bypassed references are often not on the critical path.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b11\": 0.7,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b5\": 0.5,\n    \"b13\": 0.4,\n    \"b15\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the accurate prediction of memory dependences between load and store instructions in out-of-order processors to improve performance. The paper explores the idea of bypassing memory dependences in load-store (RISC) architectures using store sets, evaluating the performance gains from this technique.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b6\": 0.9,\n        \"b8\": 0.7,\n        \"b11\": 0.6,\n        \"b14\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.7,\n        \"b15\": 0.7,\n        \"b13\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is improving performance by predicting memory dependences between load and store instructions to enable more aggressive out-of-order issue of memory instructions. The paper explores the idea of memory bypassing in load-store architectures, specifically evaluating a store sets-based version of this technique and simulating different types of ideal processors to assess performance limits.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b8\": 0.7,\n    \"b11\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b13\": 0.55,\n    \"b14\": 0.5,\n    \"b15\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of memory dependence prediction to improve the performance of out-of-order processors by exploring the idea of bypassing memory dependences. It evaluates the performance gains from memory bypassing using store sets and concludes that the performance gains are relatively small. The paper also discusses the methodology, experimental results, and reasons for the small performance gains.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.5,\n    \"b5\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of memory dependence predictions in out-of-order processors and explores a technique called memory bypassing to improve performance. The main inspiration and approaches are derived from store sets for memory dependence prediction, which is a key focus of the paper.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b8\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.6,\n        \"b11\": 0.5,\n        \"b15\": 0.5\n    }\n}\n```"], "599c797a601a182cd2642797": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of factual inaccuracies, out-of-vocabulary (OOV) words, and repetition in abstractive summarization models. The authors propose a hybrid pointer-generator network with a novel coverage mechanism to tackle these issues, applying it to the CNN/Daily Mail dataset and achieving significant improvements in ROUGE scores.\",\n  \"Direct Inspiration\": [\"b6\", \"b15\", \"b25\", \"b26\"],\n  \"Indirect Inspiration\": [\"b3\", \"b17\", \"b20\", \"b28\"],\n  \"Other Inspiration\": [\"b14\", \"b24\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of factual inaccuracies, out-of-vocabulary (OOV) words, and repetition in abstractive text summarization. The authors propose a hybrid pointer-generator network with a novel coverage mechanism to tackle these issues, achieving better performance on the CNN/Daily Mail dataset.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.9,\n        \"b15\": 0.9,\n        \"b25\": 0.9,\n        \"b26\": 0.8,\n        \"b17\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.7,\n        \"b16\": 0.7,\n        \"b8\": 0.7\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of factual inaccuracies, repetition, and handling out-of-vocabulary (OOV) words in abstractive summarization models. The proposed solution involves a hybrid pointer-generator network enhanced with a coverage mechanism.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b25\": 0.85,\n    \"b26\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b15\": 0.75,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of generating accurate and non-repetitive multi-sentence abstractive summaries. It proposes a hybrid pointer-generator network with a novel coverage mechanism to handle out-of-vocabulary words, factual accuracy, and repetition.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b6\": 1.0,\n    \"b15\": 1.0,\n    \"b17\": 1.0,\n    \"b25\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b16\": 0.8,\n    \"b24\": 0.8,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b10\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in abstractive summarization, specifically issues with factual inaccuracies, handling out-of-vocabulary (OOV) words, and repetition in generated summaries. The authors propose a hybrid pointer-generator network with a novel coverage mechanism to address these challenges, applied to the CNN/Daily Mail dataset.\",\n    \"Direct Inspiration\": {\n        \"b26\": 1.0,\n        \"b25\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b15\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b17\": 0.7,\n        \"b16\": 0.7,\n        \"b8\": 0.6,\n        \"b3\": 0.6\n    }\n}\n```"], "5e5e190893d709897ce48240": ["```json\n{\n  \"Summary\": \"The primary challenges addressed in this paper are the inadequacy and inconvenience caused by author name ambiguity in digital academic libraries. The proposed solution is a unified framework combining a discriminative module and a generative module to handle both content information and relation information in an adversarial way. The framework eliminates the need for complicated feature engineering and labeled samples.\",\n  \"Direct Inspiration\": [\"b4\"],\n  \"Indirect Inspiration\": [\"b18\", \"b10\", \"b14\"],\n  \"Other Inspiration\": [\"b6\", \"b8\", \"b9\", \"b0\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of author name disambiguation in academic contexts, proposing a unified framework combining discriminative and generative modules. The framework effectively integrates content and relation information from heterogeneous information networks, eliminating the need for labeled samples and complex feature engineering.\",\n  \"Direct Inspiration\": [\"b4\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b6\", \"b8\", \"b9\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed in the paper is the author name disambiguation (AND) problem, specifically the inadequacy of existing methods to integrate content information and relation information effectively. Existing methods either rely on supervised learning without high-order connection processing or model high-order connections with the need for supervision.\",\n    \"inspirations\": \"The paper is inspired by generative adversarial networks (GANs) to combine content and relation information in an adversarial way to improve author name disambiguation.\"\n  },\n  \"Direct Inspiration\": {\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b8\": 0.6,\n    \"b9\": 0.5,\n    \"b0\": 0.5,\n    \"b10\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of author name disambiguation by proposing a unified framework that combines content and relation information using a generative adversarial network (GAN) approach. It aims to eliminate the need for labeled samples and complicated feature engineering by integrating a discriminative module and a generative module to enhance high-order connection awareness and topology understanding of the networks.\",\n  \"Direct Inspiration\": [\"b4\"],\n  \"Indirect Inspiration\": [\"b18\", \"b14\"],\n  \"Other Inspiration\": [\"b5\", \"b21\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of author name disambiguation by proposing a novel unified framework that combines content information and relation information in an adversarial way, inspired by generative adversarial networks. This framework includes a discriminative module that distinguishes whether two papers are written by the same author and a generative module that selects homogeneous papers within a heterogeneous information network.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b6\": 0.6,\n    \"b8\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```"], "558bfbace4b00c3c48df9828": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient execution of short threads in manycore systems, focusing on the high cost of thread migration due to cache state transfer. The authors propose a working set migration (WSM) mechanism that includes capturing access patterns, summarizing this data, and using it to prefetch data at the new core. They demonstrate significant performance improvements using simple, low-cost hardware additions.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b13\": 0.9,\n    \"b26\": 0.9,\n    \"b33\": 0.9,\n    \"b6\": 0.9,\n    \"b10\": 0.9,\n    \"b39\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.7,\n    \"b18\": 0.7,\n    \"b0\": 0.7,\n    \"b21\": 0.7,\n    \"b4\": 0.7,\n    \"b16\": 0.7,\n    \"b5\": 0.7,\n    \"b30\": 0.7,\n    \"b28\": 0.7,\n    \"b19\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b37\": 0.6,\n    \"b7\": 0.6,\n    \"b15\": 0.6,\n    \"b23\": 0.6,\n    \"b3\": 0.6,\n    \"b1\": 0.6,\n    \"b9\": 0.6,\n    \"b22\": 0.6,\n    \"b20\": 0.6,\n    \"b38\": 0.6,\n    \"b8\": 0.6,\n    \"b36\": 0.6,\n    \"b14\": 0.6,\n    \"b32\": 0.6,\n    \"b24\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiency of executing short threads in manycore systems due to the overhead of migrating cache states between cores. It proposes a three-step working set migration (WSM) approach: capturing access behavior, summarizing and transferring this behavior, and prefetching at the new core based on the summary. The method aims to improve performance by reducing the cost of thread migration and is tested on a speculative multithreading architecture.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b30\": 0.85,\n    \"b39\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b19\": 0.7,\n    \"b35\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently executing short threads on manycore architectures by introducing a mechanism for predicting and prefetching the future working set of threads as they migrate between cores. The proposed three-step approach includes capturing access behavior, summarizing this behavior, and applying it with a prefetcher at the new core. This method is evaluated for its ability to boost performance with minimal complexity and cost.\",\n  \"Direct Inspiration\": {\n    \"b28\": 0.9,\n    \"b9\": 0.85,\n    \"b30\": 0.8,\n    \"b39\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b15\": 0.65,\n    \"b10\": 0.6,\n    \"b3\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b35\": 0.45,\n    \"b7\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficiently executing short threads in manycore systems due to the high overhead of migrating their cache state between cores. It proposes a three-step mechanism for working set migration (WSM) that captures, summarizes, and prefetches thread access behavior to predict future working sets and improve performance post-migration.\",\n    \"Direct Inspiration\": {\n        \"b38\": 0.9,\n        \"b20\": 0.85,\n        \"b9\": 0.8,\n        \"b28\": 0.75\n    },\n    \"Indirect Inspiration\": {\n        \"b39\": 0.7,\n        \"b10\": 0.7,\n        \"b30\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b35\": 0.6,\n        \"b19\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge outlined in the paper is the efficient execution of short threads in manycore environments due to the high overhead of copying a thread's cache state between cores. This challenge results in a significant barrier to exploiting thread-level parallelism.\",\n    \"algorithm\": \"The authors propose a three-step approach to Working Set Migration (WSM) which includes augmenting each core with hardware to capture access behavior, summarizing this behavior during thread deactivation or forking, and using a prefetcher at the new core to apply the summary data, aiming to predict and prefetch the future working set of threads.\"\n  },\n  \"Direct Inspiration\": {\n    \"b30\": 1.0,\n    \"b28\": 1.0,\n    \"b9\": 1.0,\n    \"b39\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b37\": 0.8,\n    \"b7\": 0.7,\n    \"b15\": 0.7,\n    \"b23\": 0.7,\n    \"b3\": 0.7,\n    \"b1\": 0.7,\n    \"b22\": 0.7,\n    \"b19\": 0.7,\n    \"b35\": 0.7,\n    \"b20\": 0.6,\n    \"b38\": 0.6,\n    \"b8\": 0.6,\n    \"b24\": 0.6,\n    \"b34\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.5,\n    \"b13\": 0.5,\n    \"b26\": 0.5,\n    \"b33\": 0.5,\n    \"b6\": 0.5,\n    \"b17\": 0.5,\n    \"b18\": 0.5,\n    \"b0\": 0.5,\n    \"b21\": 0.5,\n    \"b4\": 0.5,\n    \"b16\": 0.5,\n    \"b5\": 0.5,\n    \"b15\": 0.5,\n    \"b23\": 0.5,\n    \"b30\": 0.5,\n    \"b32\": 0.5,\n    \"b36\": 0.5,\n    \"b14\": 0.5\n  }\n}\n```"], "5a9cb60d17c44a376ffb35be": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of map matching for low-sampling-rate trajectory data, proposing a novel algorithm called MSTM. The key contributions include a map-searching tree (MS-Tree) structure, a measure method to infer optimal paths, and an evaluation of MSTM's efficiency and effectiveness on real datasets.\",\n  \"Direct Inspiration\": [ \"b3\", \"b4\" ],\n  \"Indirect Inspiration\": [ \"b5\", \"b6\" ],\n  \"Other Inspiration\": [ \"b16\", \"b10\", \"b11\" ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of map matching on low-sampling-rate trajectory data, proposing a novel algorithm called MSTM. The algorithm introduces a new map-searching tree structure (MS-Tree) and incorporates both temporal and spatial features of trajectories to infer the optimal path. Key contributions include the definition of extend and prune operations to build the MS-Tree and a measure method to select the best path.\",\n  \"Direct Inspiration\": [\"b3\", \"b4\"],\n  \"Indirect Inspiration\": [\"b5\", \"b6\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is map matching on low-sampling-rate trajectory data, which is more common in practical scenarios but more difficult to handle than high-sampling-rate data. The proposed algorithm, MSTM, introduces a novel map-searching tree (MS-Tree) structure along with extend and prune operations to maintain and select the optimal candidate paths. The paper aims to improve accuracy and performance compared to existing methods like ST-Matching and IVMM.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of map matching for low-sampling-rate trajectory data, which is often sparse and error-prone. The proposed MSTM algorithm introduces a novel map-searching tree (MS-Tree) structure that maintains all candidate paths and uses extend and prune operations to control the generation of the MS-Tree. The algorithm considers both temporal and spatial features of trajectories to infer the optimal path from all possible paths in the MS-Tree, showing improved accuracy and performance compared to existing methods.\",\n  \"Direct Inspiration\": [\"b3\", \"b4\"],\n  \"Indirect Inspiration\": [\"b5\", \"b6\", \"b10\", \"b11\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the difficulty of map matching on low-sampling-rate trajectory data, which is common in practice due to energy constraints and app limitations. The authors propose a novel map-searching tree (MS-Tree) structure and an MSTM algorithm to improve accuracy and performance in this context.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9,\n        \"b4\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.6,\n        \"b6\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.5\n    }\n}\n```"], "5c757586f56def9798a004b1": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve the modeling and prediction of temporal point patterns using stochastic processes. The authors propose using conditional intensity functions to define and analyze these point processes, including marked point processes. The paper introduces various examples such as the Poisson process, Hawkes process, and self-correcting process, and discusses their applications in modeling real-world phenomena like earthquakes.\",\n  \"Direct Inspiration\": [\"b3\", \"b4\", \"b5\", \"b6\", \"b11\", \"b12\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of modeling temporal point processes and explores evolutionary point processes using conditional intensity functions. Various types of point processes, including the Poisson process, Hawkes process, and self-correcting process, are discussed along with their conditional intensity functions. The paper introduces novel contributions in defining and interpreting these point processes, particularly through the use of conditional intensity functions and marks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.8,\n    \"b11\": 0.85,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the modeling of temporal point patterns using evolutionary point processes, focusing on the conditional intensity function as a key concept. It explores various examples like the Poisson process, Hawkes process, and self-correcting process. The paper aims to extend these models to the marked case and discusses methods for parameter estimation using likelihood functions.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b4\": 1,\n    \"b5\": 1,\n    \"b6\": 1,\n    \"b11\": 1,\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of modeling temporal point processes, particularly focusing on the concept of evolutionary point processes and the use of conditional intensity functions. It explores different models like the Poisson process, Hawkes process, and self-correcting processes, and provides methodologies for parameter estimation using likelihood functions.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b1\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in modeling temporal point processes, specifically focusing on evolutionary processes defined by conditional intensity functions. Various examples are provided, including the Poisson process, Hawkes process, and self-correcting process. The paper also discusses the estimation of parameters using likelihood functions and the generalization to marked point processes.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b4\": 1,\n    \"b5\": 1,\n    \"b6\": 1,\n    \"b11\": 1,\n    \"b12\": 1\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```"], "5e6cae3493d709897ccff2dd": ["```json\n{\n  \"Summary\": \"The paper introduces HEAX, a novel high-performance architecture for computing on homomorphically encrypted data. The primary challenges addressed include the computational intensity of modular arithmetic on large integers and the complex data dependencies in homomorphic operations. The architecture focuses on optimizing core computation blocks for fast modular arithmetic and designing a high-throughput Number-Theoretic Transform (NTT).\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b41\": 0.9,\n    \"b44\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b31\": 0.7,\n    \"b18\": 0.7,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b37\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces HEAX, a novel high-performance architecture for computing on encrypted data using homomorphic encryption, specifically focusing on the CKKS scheme. It addresses the computational challenges in modular arithmetic and data dependency in FHE, proposing optimized core computation blocks for fast modular arithmetic and high-throughput Number-Theoretic Transform (NTT).\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.95,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b31\": 0.7,\n    \"b18\": 0.7,\n    \"b41\": 0.6,\n    \"b44\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b37\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performing computations on encrypted data using Fully Homomorphic Encryption (FHE), with a novel focus on accelerating such computations using a high-performance architecture called HEAX. The primary inspiration comes from the inherent computational overhead in FHE, especially with the CKKS scheme. The novel methods introduced include optimized core computation blocks for fast modular arithmetic and a high-throughput Number-Theoretic Transform (NTT) architecture.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1,\n    \"b17\": 1,\n    \"b41\": 0.9,\n    \"b44\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b31\": 0.7,\n    \"b18\": 0.7,\n    \"b45\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b30\": 0.5,\n    \"b40\": 0.5,\n    \"b61\": 0.5,\n    \"b52\": 0.4,\n    \"b54\": 0.4,\n    \"b55\": 0.4,\n    \"b8\": 0.6,\n    \"b37\": 0.6,\n    \"b9\": 0.6,\n    \"b56\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the computationally intensive nature of homomorphic operations on ciphertexts, the convoluted data dependency within these operations, and the prohibitive storage requirements for intermediate results on FPGA chips. The authors propose HEAX, a novel high-performance architecture for homomorphic encryption, focusing on optimized core computation blocks for fast modular arithmetic and a new architecture for high-throughput Number-Theoretic Transform (NTT). The design aims to overcome these challenges and is applicable to a broad range of FHE schemes, with specific emphasis on CKKS for approximate computation.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b37\": 0.8,\n    \"b41\": 0.7,\n    \"b44\": 0.7,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b31\": 0.5,\n    \"b18\": 0.5,\n    \"b24\": 0.5,\n    \"b36\": 0.5,\n    \"b52\": 0.4,\n    \"b54\": 0.4,\n    \"b55\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper involve the computationally intensive nature of homomorphic operations on ciphertexts, specifically modular arithmetic on large integers and handling large polynomial degrees. The authors propose HEAX, a high-performance architecture to accelerate computations on homomorphically encrypted data using the CKKS scheme. Key contributions include optimized core computation blocks for modular arithmetic and a new high-throughput Number-Theoretic Transform architecture.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b18\": 0.8,\n    \"b31\": 0.8,\n    \"b41\": 0.7,\n    \"b44\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b37\": 0.5,\n    \"b45\": 0.5\n  }\n}\n```"], "5ede0553e06a4c1b26a83ff5": ["```json\n{\n  \"Summary\": \"The paper addresses the high computational and memory costs associated with Transformer models, particularly in handling long sequences. The authors propose a linear transformer model that reduces these costs by using a kernel-based formulation of self-attention and the associative property of matrix products. This approach significantly improves memory efficiency and inference speed while maintaining performance levels.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b13\": 0.9,\n    \"b35\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.5,\n    \"b8\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the high computational and memory cost associated with the global receptive field of self-attention in transformer models. The authors propose a linear transformer model that reduces memory footprint and scales linearly with respect to context length using a kernel-based formulation of self-attention and the associative property of matrix products. This leads to faster autoregressive inference and efficient handling of long sequences in tasks like image generation and automatic speech recognition.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b13\": 0.8,\n    \"b35\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.6,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b30\": 0.5,\n    \"b4\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the high computational and memory costs of transformer models, particularly due to the global receptive field of self-attention. To tackle this, the authors propose the linear transformer model, which uses a kernel-based formulation of self-attention and the associative property of matrix products to reduce complexity and memory usage, thus enabling faster autoregressive inference.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b13\": 1,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b30\": 0.7,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges associated with the high computational and memory costs of transformer models, particularly due to the quadratic complexity of self-attention mechanisms. The authors propose a linear transformer model that reduces memory footprint and scales linearly with context length by using a kernel-based formulation of self-attention and the associative property of matrix products.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b13\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b3\": 0.6,\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenge of high computational and memory costs in transformer models due to the quadratic complexity of self-attention mechanisms. It proposes a novel linear transformer model that reduces memory footprint and scales linearly with context length by using a kernel-based formulation of self-attention and the associative property of matrix products.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b13\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b30\": 0.6,\n    \"b3\": 0.6,\n    \"b15\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.4\n  }\n}\n```"], "573697316e3b12023e6217ec": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges related to the impact of the coherence protocol on latencies and bandwidths of core-to-core transfers and memory accesses in contemporary multi-socket x86 servers. Specifically, it analyzes different configurations in a dual socket Haswell-EP system using micro-benchmarks and investigates their influence on the performance of parallel applications.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 1.0,\n    \"b3\": 1.0,\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b6\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the impact of cache coherence protocols on latencies and bandwidths in multi-socket Haswell-EP systems. It compares different snooping modes and their influence on core-to-core and memory access performance, using micro-benchmarks and parallel applications.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.9,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b10\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the impact of cache coherence protocols on the performance of core-to-core transfers and memory accesses in dual socket Haswell-EP systems. It delves into different coherence protocols, such as MESIF, and their implementations using source snooping and home snooping, as well as the influence of NUMA behavior and Cluster-on-Die (COD) mode.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.8,\n    \"b3\": 0.85,\n    \"b4\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.65,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of cache coherence and memory access latencies in multi-socket x86 servers, specifically analyzing the Haswell-EP architecture. It proposes and evaluates the impact of different snooping modes and coherence protocols on core-to-core transfers and memory accesses using micro-benchmarks and parallel applications.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1,\n    \"b3\": 0.9,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b9\": 0.6,\n    \"b10\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper investigates the impact of the coherence protocol on latencies and bandwidths of core-to-core transfers and memory accesses in Haswell-EP systems. It compares three configurations using micro-benchmarks and examines the influence on parallel applications' performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.85,\n    \"b3\": 0.9,\n    \"b4\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```"], "53e99d12b7602d97025c4239": ["```json\n{\n  \"Summary\": \"The paper addresses the influence of cache sharing in modern CMPs on the performance of multithreaded applications. It presents a systematic measurement revealing novel observations and explores cache-sharing-aware program transformations to improve performance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b7\": 0.9,\n    \"b23\": 0.9,\n    \"b6\": 0.9,\n    \"b19\": 0.9,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b11\": 0.7,\n    \"b9\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b1\": 0.6,\n    \"b3\": 0.6,\n    \"b18\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge in the paper is the insignificant influence of cache sharing on the performance of contemporary multithreaded applications due to a mismatch between the generated programs and the CMP cache architecture. The proposed solution involves cache-sharing-aware program transformations to exert the potential of shared cache management. The novel methods include increasing the amount of shared data among sibling threads and appropriate placement of threads on cores.\",\n  \"Direct Inspiration\": [\"b2\"],\n  \"Indirect Inspiration\": [\"b7\", \"b23\", \"b6\", \"b19\", \"b22\"],\n  \"Other Inspiration\": [\"b8\", \"b24\", \"b25\", \"b11\", \"b15\", \"b9\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limited influence of cache sharing on the performance of modern Chip Multiprocessors (CMP) and proposes cache-sharing-aware program transformations to improve performance. It identifies the mismatch between program development/compilation and CMP cache architecture as a key challenge and demonstrates the potential benefits of cache-sharing-aware transformations through experiments.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b23\": 0.8,\n    \"b6\": 0.8,\n    \"b19\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b9\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies the limited influence of cache sharing on the performance of multithreaded applications in the PARSEC benchmark suite. It attributes this to uniform data sharing, limited shared cache line accesses, and large working sets. The paper demonstrates that cache-sharing-aware program transformations can significantly improve performance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b23\": 0.8,\n    \"b6\": 0.8,\n    \"b19\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b9\": 0.6,\n    \"b12\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding the performance influence of cache sharing in modern Chip Multiprocessors (CMP) on multithreaded applications. It finds that cache sharing has limited influence on performance due to uniform data sharing, limited shared cache line accesses, and large working sets. The paper proposes cache-sharing-aware program transformations to better exploit cache sharing and demonstrates significant performance improvements through these transformations.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b19\": 0.8,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b6\": 0.7,\n    \"b23\": 0.65,\n    \"b24\": 0.6,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.55,\n    \"b3\": 0.5,\n    \"b8\": 0.5,\n    \"b9\": 0.45,\n    \"b11\": 0.45,\n    \"b12\": 0.45,\n    \"b15\": 0.45,\n    \"b17\": 0.4,\n    \"b18\": 0.4\n  }\n}\n```"], "5bdc315817c44a1f58a05e88": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of animating a still image of a face in a controllable, lightweight manner without explicitly modeling the face. The proposed solution, X2Face, is a novel self-supervised network architecture that learns to control a face using different driving modalities such as a frame, pose, or audio. The network operates by factorizing the problem into an embedding network and a driving network, trained in a self-supervised manner.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1.0,\n    \"b0\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b14\": 0.8,\n    \"b36\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are animating a still image of a face in a controllable, lightweight manner without explicit face representation and mapping control modalities (e.g., expression, pose, audio) back onto the face representation. The proposed algorithm, X2Face, addresses these challenges using a novel self-supervised network architecture that implicitly learns face representation from a large collection of video data and uses this implicit representation to control the face with different modalities.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1.0,\n    \"b0\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b31\": 0.8,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of animating a still face image in a controllable manner without explicit face representation. It introduces X2Face, a self-supervised network architecture that can control a source face using a driving vector derived from multiple modalities, such as audio or pose. The network architecture involves embedding and driving networks that learn to factorize the problem, and it is trained using a combination of self-supervised and identity loss functions.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b44\": 0.85,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.75,\n    \"b14\": 0.7,\n    \"b36\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b10\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Animating a still image of a face in a controllable and lightweight manner without explicit face representation.\",\n    \"inspirations\": \"Developing a self-supervised network architecture (X2Face) for face puppeteering using driving vectors from various modalities such as frames, pose, or audio.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b0\", \"b44\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b14\", \"b31\", \"b36\", \"b8\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b5\", \"b12\", \"b22\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of animating a still image of a face in a controllable and lightweight manner without explicitly modeling the face in 3D. It introduces a novel self-supervised network architecture called X2Face, which can control a source face using a driving vector derived from different modalities such as video frames, pose information, or audio data. The method is trained in a self-supervised manner, and its performance is evaluated against state-of-the-art self-supervised and supervised methods.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b44\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b12\": 0.8,\n    \"b31\": 0.7,\n    \"b14\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```"], "5cede0fada562983788d93ae": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating the flexibility of imperative programming with the performance benefits of declarative programming in machine learning libraries. The proposed solution, TensorFlow Eager, allows developers to switch between imperative and staged computations within a single Python-embedded DSL, facilitating rapid prototyping and optimized performance for scalable deployment.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b0\": 0.8,\n    \"b23\": 0.8,\n    \"b32\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.7,\n    \"b31\": 0.6,\n    \"b20\": 0.5,\n    \"b29\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.4,\n    \"b14\": 0.4,\n    \"b13\": 0.4,\n    \"b4\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the difficulty of balancing the flexibility and accessibility of imperative DSLs with the performance benefits of declarative DSLs in differentiable programming. The paper proposes TensorFlow Eager, a Python-embedded DSL that allows developers to interpolate between imperative and staged computations. It offers a multi-stage programming model, enabling rapid prototyping and selective staging for performance optimization.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b4\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.9,\n    \"b33\": 0.8,\n    \"b20\": 0.7,\n    \"b0\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b22\": 0.5,\n    \"b2\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in machine learning libraries related to the limitations of imperative and declarative DSLs, proposing TensorFlow Eager as a solution that combines the benefits of both. It introduces a multi-stage programming model that allows for both rapid prototyping and efficient execution.\",\n  \"Direct Inspiration\": {\n    \"b32\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b4\": 0.7,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b33\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of combining the flexibility of imperative execution with the performance benefits of declarative programming in machine learning. The proposed solution is TensorFlow Eager, a Python-embedded DSL that allows developers to switch between imperative and staged computations seamlessly. The primary contributions include an elegant implementation that integrates with TensorFlow and a multi-stage programming model contextualized within the differentiable programming community.\",\n    \"Direct Inspiration\": {\n        \"b32\": 1.0,\n        \"b23\": 1.0,\n        \"b4\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.8,\n        \"b20\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b0\": 0.6,\n        \"b31\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents TensorFlow Eager, a Python-embedded DSL for differentiable programming that allows developers to switch between imperative and staged computations. The primary challenges addressed include the performance bottlenecks of imperative DSLs embedded in Python and the steep learning curves of declarative DSLs. TensorFlow Eager aims to combine the flexibility of imperative execution with the performance benefits of declarative programming.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b4\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b0\": 0.8,\n    \"b19\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.7,\n    \"b25\": 0.7,\n    \"b20\": 0.6\n  }\n}\n```"], "5dc9327d3a55acc1042498de": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of memory consumption and training time in pre-training and fine-tuning large language models like BERT. The proposed solution, BlockBERT, introduces sparse block substructures into the attention matrix to reduce memory usage and floating point operations (FLOPs), aiming for a more efficient model capable of handling long sequences.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"Vaswani et al., 2017\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b11\": 0.6,\n    \"b9\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces BlockBERT, a model designed to reduce memory consumption and training time for BERT-like models by sparsifying the attention layers using sparse block substructures. The primary challenges addressed include high memory consumption and long training times of existing models like BERT and RoBERTa. BlockBERT aims to enable efficient handling of long sequences while maintaining or improving performance on various NLP tasks.\",\n  \"Direct Inspiration\": [\"b1\", \"b4\", \"b19\"],\n  \"Indirect Inspiration\": [\"b0\", \"b11\", \"b9\"],\n  \"Other Inspiration\": [\"b15\", \"b25\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high memory consumption in training BERT-family models due to the quadratic nature of dot-product self-attention. It proposes BlockBERT, which introduces sparse block substructures in the attention matrix to reduce memory usage and training time while maintaining or improving performance.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b19\": 0.95,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b14\": 0.75,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.65,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of high memory consumption and long training times associated with pre-training and fine-tuning large language models like BERT. The proposed solution, BlockBERT, introduces sparse block structures into the attention matrix to reduce memory usage and the number of floating point operations, enabling the modeling of long sequences in a memory-efficient manner. Experimental results show significant reductions in memory usage and training time while maintaining or improving performance compared to existing models.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1,\n        \"b19\": 1,\n        \"Vaswani et al., 2017\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8,\n        \"b11\": 0.6,\n        \"b0\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b27\": 0.5,\n        \"b12\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high memory consumption in training large language models like BERT and proposes a novel algorithm called BlockBERT. BlockBERT introduces sparse block substructures into the attention matrix to reduce memory consumption and the number of floating point operations, while still capturing long-distance dependencies effectively.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b0\": 0.6,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b23\": 0.4,\n    \"b17\": 0.4\n  }\n}\n```"], "53e9b49bb7602d9703fa7aed": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are cache pollution and cache thrashing, which degrade cache performance and consequently reduce system performance. The proposed algorithm, the Evicted-Address Filter (EAF), aims to address both cache pollution and thrashing by predicting the reuse behavior of missed cache blocks based on their own past behavior. The EAF keeps track of recently evicted blocks using a Bloom filter and uses this information to decide the priority of inserting missed blocks into the cache.\",\n  \"Direct Inspiration\": {\n    \"b34\": 0.9,\n    \"b54\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b18\": 0.7,\n    \"b33\": 0.7,\n    \"b52\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of cache pollution and cache thrashing in modern processors. The proposed solution, the Evicted-Address Filter (EAF), tracks recently evicted block addresses to predict the reuse behavior of missed cache blocks and uses this prediction to optimize cache insertion policy. The EAF is implemented using a Bloom filter to reduce storage and power overhead and to mitigate thrashing by periodically clearing the EAF when it becomes full.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.9,\n    \"b18\": 0.9,\n    \"b33\": 0.9,\n    \"b54\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b35\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in cache management: cache pollution and cache thrashing. The proposed solution is the Evicted-Address Filter (EAF), which predicts the reuse behavior of cache blocks based on their eviction history and uses a Bloom filter to mitigate both issues with low overhead.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b34\": 0.95,\n    \"b54\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b16\": 0.85,\n    \"b33\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.7,\n    \"b35\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of cache pollution and thrashing in modern processors where multiple applications share the last-level cache. It proposes a novel mechanism called the Evicted-Address Filter (EAF), which predicts the reuse behavior of cache blocks based on their eviction history. The EAF aims to reduce both pollution and thrashing using a low-cost implementation with a Bloom filter, leading to improved system performance.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1.0,\n    \"b54\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b16\": 0.8,\n    \"b18\": 0.8,\n    \"b33\": 0.8,\n    \"b52\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of cache pollution and thrashing in modern processor designs with shared on-chip last-level caches. It proposes a novel mechanism called the Evicted-Address Filter (EAF) that predicts the reuse behavior of missed cache blocks based on their own past behavior, reducing both cache pollution and thrashing with low storage and power overhead.\",\n  \"Direct Inspiration\": {\n    \"b34\": 1,\n    \"b54\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b16\": 0.8,\n    \"b18\": 0.8,\n    \"b33\": 0.7,\n    \"b52\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b8\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```"], "5736982b6e3b12023e6fd099": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of high power consumption in Out-of-Order (OoO) cores compared to In-Order (InO) cores by proposing Dynamic Migration of Out-of-order Schedule (DynaMOS). The key idea is to record and replay schedules from a big OoO core to an energy-efficient little InO core, aiming to achieve similar performance with reduced energy consumption.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.95,\n    \"b5\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b10\": 0.75,\n    \"b11\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b14\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of high power consumption in Out-of-Order (OoO) cores compared to In-Order (InO) cores by proposing a new architecture, DynaMOS. This architecture records and memoizes instruction schedules on a big (OoO) core and replays them on a little (InO) core to achieve similar high performance with greater energy efficiency.\",\n    \"Direct Inspiration\": {\n        \"b8\": 0.9,\n        \"b9\": 0.8,\n        \"b10\": 0.8,\n        \"b11\": 0.9,\n        \"b12\": 0.8,\n        \"b5\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.7,\n        \"b4\": 0.75,\n        \"b14\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b3\": 0.6,\n        \"b6\": 0.6,\n        \"b7\": 0.6,\n        \"b13\": 0.6,\n        \"b15\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the high power consumption of Out-of-Order (OoO) cores compared to In-Order (InO) cores, despite their higher performance. The proposed DynaMOS algorithm aims to optimize energy efficiency in heterogeneous multi-core processors by recording and replaying instruction schedules from the OoO core on the InO core, effectively maintaining performance while reducing power consumption.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b11\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b4\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high power consumption in out-of-order (OoO) cores while maintaining performance levels. The proposed DynaMOS algorithm records schedules of instruction traces on an OoO core and replays them on an in-order (InO) core, thus leveraging the efficiency of InO cores without compromising performance. Key design challenges include handling false dependencies, speculative execution, and precise interrupt handling.\",\n  \"Direct Inspiration\": [\"b5\", \"b8\"],\n  \"Indirect Inspiration\": [\"b4\", \"b10\", \"b11\"],\n  \"Other Inspiration\": [\"b12\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high power consumption in Out-of-Order (OoO) cores compared to In-Order (InO) cores while maintaining high performance. It proposes Dynamic Migration of Out-of-Order Schedule (DynaMOS) for a fine-grain, tightly coupled heterogeneous processor. The key idea is to memoize a schedule by executing an instruction trace on a big core and replaying it on a little core, thereby achieving similar performance with lower energy consumption.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b11\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```"], "53e9b365b7602d9703e48f3c": ["```json\n{\n  \"Summary\": \"This paper addresses the performance trade-offs between in-order and out-of-order (OOO) instruction scheduling in modern CPUs. The primary contributions include quantifying the performance benefits of OOO scheduling, especially the role of speculation and dynamism, and providing recommendations for optimizing in-order designs. The study uses a simulator framework to analyze the relative importance of these factors.\",\n  \"Direct Inspiration\": {\n    \"b46\": 0.9,\n    \"b40\": 0.8,\n    \"b19\": 0.8,\n    \"b15\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b36\": 0.7,\n    \"b50\": 0.7,\n    \"b35\": 0.7,\n    \"b42\": 0.7,\n    \"b48\": 0.7,\n    \"b9\": 0.7,\n    \"b8\": 0.7,\n    \"b44\": 0.7,\n    \"b28\": 0.7,\n    \"b18\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b27\": 0.6,\n    \"b41\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance trade-offs between in-order and out-of-order (OOO) instruction scheduling in modern CPUs. The authors aim to quantify the contributions of dynamism and speculation to the performance advantage of OOO schedulers. They propose a study to better understand these differences, providing insights and recommendations for improving in-order designs based on current technology and workload trends.\",\n  \"Direct Inspiration\": {\n    \"b46\": 1,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b42\": 0.7,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b27\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper investigates the key performance differences between in-order and out-of-order (OOO) instruction scheduling in CPUs. It aims to quantify the contributions of dynamism and speculation to OOO performance and provides recommendations for improving in-order designs.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b46\", \"b40\", \"b19\", \"b15\", \"b21\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b27\", \"b50\", \"b35\", \"b42\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b48\", \"b9\", \"b8\", \"b44\", \"b28\", \"b18\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding the performance advantage of out-of-order (OOO) vs. in-order instruction scheduling in CPUs. It quantifies the contributions of speculation and dynamism in OOO performance and provides recommendations for competitive in-order designs.\",\n  \"Direct Inspiration\": [\"b46\"],\n  \"Indirect Inspiration\": [\"b19\", \"b15\", \"b21\", \"b27\", \"b8\", \"b44\"],\n  \"Other Inspiration\": [\"b9\", \"b48\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper are the trade-offs in CPU design between in-order and out-of-order (OOO) instruction scheduling, the need to quantify the contributions of dynamism in OOO execution, and the performance gap between in-order and OOO designs. The paper proposes a study to characterize the relative importance of dynamism in OOO and to understand the schedule differences between in-order and OOO schedulers.\",\n    \"Inspirations\": \"The paper is inspired by previous work on execution caching, theoretical limitations of static scheduling, and the observed performance advantages of OOO designs in exploiting memory-level parallelism and handling cache misses and branch mispredictions.\"\n  },\n  \"Direct Inspiration\": {\n    \"b46\": 1,\n    \"b21\": 0.9,\n    \"b8\": 0.85,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b15\": 0.65,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b27\": 0.45,\n    \"b28\": 0.4,\n    \"b40\": 0.35,\n    \"b42\": 0.3,\n    \"b48\": 0.25\n  }\n}\n```"], "53e9a2c8b7602d9702bd0573": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing a memory system for texture mapping in computer graphics. The authors propose using a texture cache to handle the high memory bandwidth and low-latency access requirements. They explore the representation of textures in memory, rasterization order, and cache organization to exploit temporal and spatial locality, demonstrating that small caches can offer significant performance benefits.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b6\": 0.7,\n    \"b8\": 0.7,\n    \"b19\": 0.7,\n    \"b20\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing a memory system for texture mapping in real-time graphics. It proposes using an SRAM cache with a fragment generator to improve memory bandwidth and rendering performance by exploiting spatial and temporal locality in texture accesses. The study evaluates different memory representations and rasterization orders to optimize cache behavior.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.7,\n    \"b21\": 0.7,\n    \"b20\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the design of a memory system for texture mapping in computer graphics. The proposed solution is to use a small SRAM texture cache backed by a shared pool of DRAM, instead of dedicated DRAM memory, to exploit temporal and spatial locality and improve cost-effectiveness, rendering performance, and memory bandwidth.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b19\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in designing a memory system for texture mapping in computer graphics. The main focus is on employing an SRAM texture cache to improve memory bandwidth and rendering performance by exploiting spatial and temporal locality in texture mapping.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b13\", \"b14\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b21\", \"b22\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b20\", \"b8\", \"b6\", \"b19\", \"b24\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing a memory system for texture mapping in computer graphics, specifically focusing on using an SRAM cache to improve memory bandwidth and rendering performance. The primary issues tackled include the representation of textures in memory, rasterization order, and cache organization to exploit temporal and spatial locality.\",\n  \"Direct Inspiration\": [\"b13\", \"b14\"],\n  \"Indirect Inspiration\": [\"b19\", \"b20\"],\n  \"Other Inspiration\": [\"b4\", \"b8\", \"b6\", \"b21\"]\n}\n```"], "5f0423a69e795e06bbe12b1e": ["```json\n{\n  \"Summary\": \"The paper tackles the challenge of bias in large-scale deep candidate generation (DCG) for recommender systems. It proposes a novel framework called CLRec based on contrastive learning, which has a debiasing effect by establishing a theoretical connection with inverse propensity weighting (IPW). The paper also introduces Multi-CLRec, which uses multiple queues for more accurate user-intention aware bias reduction.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b18\": 0.9,\n    \"b38\": 0.85,\n    \"b24\": 0.8,\n    \"b40\": 0.8,\n    \"b47\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b25\": 0.7,\n    \"b5\": 0.7,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b41\": 0.6,\n    \"b37\": 0.6,\n    \"b34\": 0.6,\n    \"b42\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of bias and inefficiency in large-scale industrial recommender systems, specifically in the candidate generation stage. It introduces a novel Contrastive Learning framework (CLRec) and its extension Multi-CLRec, which leverage theoretical connections between contrastive learning and inverse propensity weighting for debiasing. The methods use queue-based designs to efficiently manage negative sampling and improve the quality of the learned item representations, leading to better recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b24\": 1.0,\n    \"b40\": 1.0,\n    \"b47\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9,\n    \"b5\": 0.8,\n    \"b25\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.7,\n    \"b37\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in large-scale industrial recommender systems, particularly focusing on debiasing deep candidate generation (DCG) models. The authors introduce CLRec, a Contrastive Learning framework, which utilizes self-supervised tasks to enhance the model's discriminative ability and reduce exposure bias. The framework leverages theoretical connections between contrastive learning and inverse propensity weighting for efficient bias reduction.\",\n  \n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b18\": 0.9,\n    \"b24\": 0.9,\n    \"b40\": 0.9,\n    \"b47\": 0.9\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b38\": 0.8,\n    \"b42\": 0.8,\n    \"b41\": 0.7\n  },\n  \n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b25\": 0.6,\n    \"b27\": 0.6,\n    \"b31\": 0.6,\n    \"b35\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of bias in large-scale industrial recommender systems. It introduces a novel Contrastive Learning framework (CLRec) to reduce exposure bias and improve the representation quality of under-explored items. The paper further develops a Multi-CLRec model for more accurate user-intention aware bias reduction.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b24\": 1.0,\n    \"b40\": 1.0,\n    \"b47\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b31\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in large-scale industrial recommender systems, particularly focusing on the biases in training data and the computational inefficiencies in candidate generation. The authors propose a novel Contrastive Learning framework (CLRec) to debias Deep Candidate Generation (DCG) models and improve recommendation quality. They establish a theoretical connection between contrastive learning and inverse propensity weighting (IPW) for bias reduction and introduce a queue-based implementation to enhance efficiency.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b24\": 1.0,\n    \"b40\": 1.0,\n    \"b47\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b25\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b10\": 0.6,\n    \"b31\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```"], "5ee9f15b91e01152af022eb9": ["```json\n{\n  \"Summary\": \"The paper addresses the limitations of traditional Graph Neural Networks (GNNs) in exploiting graph structures, particularly substructures, for tasks like graph isomorphism testing and real-world network analysis. The authors propose Graph Substructure Networks (GSNs), which enhance message passing by incorporating structural information based on substructure counts.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1.0,\n    \"b15\": 0.9,\n    \"b17\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b24\": 0.7,\n    \"b25\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of traditional Graph Neural Networks (GNNs) in capturing structural characteristics of graphs, such as counting substructures, and proposes a new architecture called Graph Substructure Network (GSN) to enhance expressivity by incorporating structural information into message passing.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b18\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.9,\n    \"b16\": 0.8,\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses primary challenges in graph neural networks (GNNs) related to isotropic aggregation functions, structural awareness, and generalization without sacrificing isomorphism invariance. The authors propose a Graph Substructure Network (GSN) that incorporates structural information in the aggregation function, enhancing expressivity and addressing the aforementioned challenges.\",\n  \"Direct Inspiration\": [\"b17\", \"b18\", \"b32\"],\n  \"Indirect Inspiration\": [\"b15\", \"b16\", \"b21\", \"b22\"],\n  \"Other Inspiration\": [\"b4\", \"b19\", \"b20\", \"b33\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses three main challenges in designing Graph Neural Networks (GNNs): (a) going beyond isotropic aggregation functions, (b) ensuring GNNs are aware of the structural characteristics of the graph, and (c) achieving these without sacrificing invariance to isomorphism. The proposed solution is a novel architecture called Graph Substructure Network (GSN), which incorporates structural information in the aggregation function by counting substructures. This method improves the expressivity of GNNs while retaining their locality, making them more powerful than traditional GNNs and comparable to higher-order Weisfeiler-Leman tests.\",\n    \"Direct Inspiration\": {\n        \"b32\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b17\": 0.8,\n        \"b18\": 0.8,\n        \"b15\": 0.8,\n        \"b16\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.6,\n        \"b22\": 0.6,\n        \"b23\": 0.6,\n        \"b24\": 0.6,\n        \"b25\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of traditional Graph Neural Networks (GNNs) in capturing graph structural characteristics, specifically substructures, and proposes a novel architecture called Graph Substructure Network (GSN). The challenges include going beyond symmetric aggregation functions, ensuring GNNs are aware of structural characteristics, and retaining graph isomorphism invariance. The proposed GSN introduces structural information in the aggregation function by counting substructures, significantly improving the expressivity of GNNs.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1,\n    \"b17\": 1,\n    \"b18\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.9,\n    \"b4\": 0.8,\n    \"b19\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```"], "5550424345ce0a409eb411c6": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the lack of a comprehensive dataset for studying multimodal emotion expressions, which allows for separating and analyzing the contributions of visual and auditory modalities. The paper introduces the Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D) to tackle this gap. CREMA-D includes a large number of clips with diverse actors and raters and provides ratings in three different modalities (audio-only, visual-only, and audio-visual). This dataset aims to facilitate studies on the perception of emotions through different modalities and the interaction between them.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b38\": 0.85,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.75,\n    \"b40\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of creating a comprehensive dataset, CREMA-D, for the study of multimodal expression and perception of basic acted emotions. The inspiration is drawn from the need to understand the interplay between audio and visual modalities in emotion communication, and how these modalities contribute to perceived emotion. The dataset includes a large number of clips from a diverse group of actors, with ratings collected via crowd-sourcing.\",\n    \"Direct Inspiration\": {\n        \"b32\": 0.9,\n        \"b38\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b24\": 0.75,\n        \"b33\": 0.7,\n        \"b42\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b36\": 0.6,\n        \"b40\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents the Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D), a dataset for the study of multimodal expression and perception of basic acted emotions. The key challenges addressed include the lack of large, diverse datasets with multimodal ratings and the need for a dataset that separates visual and auditory modalities. The dataset includes 7,442 clips from 91 actors expressing six universal emotions, with ratings from 2,443 crowd-sourced raters. The paper aims to provide a comprehensive resource for studying the interplay between audio and visual modalities in emotion perception.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b38\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b32\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.5,\n    \"b53\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of understanding the interplay between audio and visual modalities in the perception of emotions. It introduces the CREMA-D (Crowd-sourced Emotional Multimodal Actors Dataset), which is a comprehensive dataset designed to study multimodal emotion expression and perception. The dataset includes a large number of clips from actors expressing six basic emotions, with ratings collected through crowd sourcing in three modalities: audio-only, visual-only, and audio-visual.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b7\", \"b32\", \"b38\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b6\", \"b8\", \"b9\", \"b10\", \"b11\", \"b12\", \"b13\", \"b14\", \"b15\", \"b16\", \"b17\", \"b18\", \"b19\", \"b20\", \"b21\", \"b22\", \"b23\", \"b24\", \"b25\", \"b26\", \"b27\", \"b28\", \"b30\", \"b31\", \"b33\", \"b34\", \"b35\", \"b36\", \"b37\", \"b39\", \"b40\", \"b41\", \"b42\", \"b43\", \"b44\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b45\", \"b46\", \"b47\", \"b48\", \"b49\", \"b50\", \"b51\", \"b52\", \"b53\", \"b54\", \"b55\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of understanding the interplay between facial and vocal expressions in conveying emotions. It introduces the Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D), which includes a large number of clips from diverse actors expressing basic emotions in audio-only, visual-only, and audio-visual modalities. The dataset is unique in its size, the number of ratings per clip, and the separation of modalities, allowing for a detailed study of multimodal emotion perception.\",\n  \"Direct Inspiration\": {\n    \"1\": \"b32\",\n    \"2\": \"b38\"\n  },\n  \"Indirect Inspiration\": {\n    \"1\": \"b45\",\n    \"2\": \"b31\"\n  },\n  \"Other Inspiration\": {\n    \"1\": \"b33\",\n    \"2\": \"b34\",\n    \"3\": \"b35\",\n    \"4\": \"b24\"\n  }\n}\n```"], "5d04e8ffda56295d08dd1a2f": ["```json\n{\n    \"Summary\": \"The paper proposes a novel text-based editing approach for talking-head video, addressing challenges in modifying speech content while maintaining seamless audio-visual flow without jump cuts. The method involves annotating input videos with phonemes, visemes, 3D face pose, and other parameters, and uses a dynamic programming optimization strategy to select and stitch video segments, followed by a recurrent video generation network to produce photorealistic output.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b2\", \"b32\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b6\", \"b58\", \"b63\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b27\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of editing talking-head videos to modify speech content while maintaining seamless audio-visual flow. The novel method involves automatic annotation of phonemes, visemes, 3D face pose and geometry, reflectance, expression, and scene illumination per frame. The core contributions include a text-based editing tool, a dynamic programming-based strategy for video synthesis, a parameter blending scheme, and a recurrent video generation network.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b58\": 0.8,\n    \"b63\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b27\": 0.6,\n    \"b52\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a novel text-based editing approach for talking-head video that produces realistic output videos with modified dialogues while maintaining seamless audio-visual flow. The method involves automatic annotation of input video with various parameters, dynamic programming-based optimization for segment selection, and a recurrent video generation network for photorealistic rendering.\",\n  \"Direct Inspiration\": {\n    \"Inspired by the Deep Video Portraits approach of\": \"b32\",\n    \"Inspired by the pioneering work of\": \"b6\"\n  },\n  \"Indirect Inspiration\": {\n    \"Synthesizing Obama project\": \"b58\",\n    \"Performance-driven puppeteering and dubbing methods like VDub and Face2Face\": [\"b17\", \"b63\"]\n  },\n  \"Other Inspiration\": {\n    \"Text-based video editing tools\": \"b2\"\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of editing talking-head videos based on transcript edits to produce realistic outputs without jump cuts. The proposed method seamlessly stitches segments of the input video based on phoneme and viseme annotations, uses a 3D face model, and employs a recurrent video generation network to create photorealistic videos.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1,\n    \"b2\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.9,\n    \"b58\": 0.9,\n    \"b63\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of editing talking-head videos to modify the dialogue seamlessly without jump cuts. The proposed method uses a novel text-based approach that annotates input videos with various parameters and employs a dynamic programming optimization strategy and a recurrent video generation network to produce realistic output videos.\",\n  \"Direct Inspiration\": [\"b2\", \"b32\"],\n  \"Indirect Inspiration\": [\"b6\", \"b58\", \"b63\"],\n  \"Other Inspiration\": []\n}\n```"], "5cf48a3cda56291d5829eb69": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating photo-realistic talking faces from speech audio while maintaining temporal consistency and handling various visual dynamics. The proposed solution is a novel temporal GAN structure with a multi-modal convolutional-RNN-based generator and a regression-based discriminator. Additionally, a hierarchical structure using facial landmarks and a dynamically adjustable loss with attention mechanisms are introduced to improve the performance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.95,\n    \"b26\": 0.9,\n    \"b25\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b7\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b0\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating photo-realistic talking face videos from speech audio while maintaining natural lip synchronization and smooth facial transitions. The proposed solution includes a novel temporal GAN structure that leverages a multi-modal convolutional-RNN-based generator and a regression-based discriminator to model temporal dependencies and improve video quality. Key techniques include the use of high-level facial landmarks, dynamically adjustable pixel-wise loss, and an attention mechanism to handle visual dynamics unrelated to speech audio.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b26\": 0.9,\n    \"b25\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b30\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of generating photo-realistic talking faces synchronized with arbitrary speech audio while maintaining smooth transitions between frames. The proposed algorithm includes a novel temporal GAN structure with a multi-modal convolutional-RNN-based generator and a regression-based discriminator. Key innovations include utilizing high-level facial landmarks to bridge audio with pixel image generation, an attention mechanism, and a dynamically adjustable pixel-wise loss.\",\n  \"Direct Inspiration\": [\"b2\", \"b7\", \"b25\"],\n  \"Indirect Inspiration\": [\"b26\", \"b27\", \"b30\"],\n  \"Other Inspiration\": [\"b0\", \"b11\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating a photo-realistic talking face from an arbitrary speech audio recording while maintaining natural lip synchronization and smooth facial transitions. The proposed solution involves a novel temporal GAN structure with a multi-modal convolutional-RNN-based generator and a regression-based discriminator. The model also utilizes high-level facial landmarks to bridge the audio signal with pixel image generation, combined with a dynamically adjustable loss and attention mechanism to focus on audiovisual-correlated regions.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b7\": 0.95,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.85,\n    \"b27\": 0.8,\n    \"b30\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b23\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating photo-realistic talking faces from speech audio while maintaining natural lip synchronization and smooth transitions between facial images over time. The proposed solution is a novel temporal GAN structure with a multi-modal convolutional-RNN-based generator (MMCRNN) and a regression-based discriminator. Additionally, the paper introduces a hierarchical structure using facial landmarks to bridge audio signals with pixel images, dynamically adjustable loss functions, and attention mechanisms to enhance audiovisual synchronization and reduce temporal discontinuities.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b26\": 0.9,\n    \"b7\": 0.85,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.75,\n    \"b28\": 0.7,\n    \"b33\": 0.65,\n    \"b30\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.55,\n    \"b0\": 0.5,\n    \"b31\": 0.45,\n    \"b15\": 0.4\n  }\n}\n```"], "5c0f8548da562944ac906a71": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of dense 3D face reconstruction from monocular video and single face images using convolutional neural networks (CNNs). It highlights the limitations of previous methods and introduces a novel framework, 3DFaceNet, which includes CoarseNet and FineNet for real-time face reconstruction and tracking. The key contributions include generating realistic training datasets, handling fine-scale details, and achieving real-time performance.\",\n  \"Direct Inspiration\": [\"b41\"],\n  \"Indirect Inspiration\": [\"b40\", \"b52\", \"b17\"],\n  \"Other Inspiration\": [\"b23\", \"b50\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The paper addresses the complex problem of dense 3D face reconstruction from monocular video and single images, highlighting the challenges of real-time, fine-detailed reconstruction robust to pose, expression, and lighting variations.\",\n    \"inspirations\": \"The paper leverages convolutional neural networks (CNNs) and draws significant inspiration from state-of-the-art single-image based face reconstruction methods, particularly those using cascaded CNNs.\"\n  },\n  \"Direct Inspiration\": {\n    \"b41\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.8,\n    \"b52\": 0.7,\n    \"b17\": 0.7,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of dense 3D face reconstruction from monocular video and single face image, presenting a framework with a novel network architecture called 3DFaceNet. The primary challenges include real-time reconstruction, handling fine-scale details, and generating realistic training data. The proposed method is inspired by earlier works using convolutional neural networks (CNNs) for face reconstruction, particularly focusing on novel data generation techniques and coherent video frame tracking.\",\n  \"Direct Inspiration\": [\"b41\"],\n  \"Indirect Inspiration\": [\"b40\", \"b52\", \"b17\"],\n  \"Other Inspiration\": [\"b7\", \"b6\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of dense 3D face reconstruction and tracking from monocular video and single face images using convolutional neural networks (CNNs). The authors develop a framework called 3DFaceNet that utilizes three CNNs for online real-time dense face reconstruction and an optimization-based inverse rendering method to generate large-scale training datasets. They highlight the difficulties posed by insufficient training data and propose novel methods to generate realistic face images and detailed face geometry datasets. The main contributions include the development of the 3DFaceNet architecture, the creation of realistic training datasets, and the real-time reconstruction of fine-scale facial details.\",\n  \"Direct Inspiration\": [\"b41\"],\n  \"Indirect Inspiration\": [\"b40\", \"b52\", \"b17\"],\n  \"Other Inspiration\": [\"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of dense 3D face reconstruction from monocular video and single face images. The proposed method utilizes convolutional neural networks (CNNs) to achieve real-time, fine-detailed face reconstruction and tracking. Key inspiration comes from existing CNN-based single-image face reconstruction methods, particularly focusing on overcoming limitations like lack of fine-scale detail recovery and the need for extensive labeled training data.\",\n  \"Direct Inspiration\": [\"b41\"],\n  \"Indirect Inspiration\": [\"b40\", \"b53\", \"b50\"],\n  \"Other Inspiration\": [\"b7\", \"b6\", \"b17\", \"b52\"]\n}\n```"], "5f44e5bd91e011872f85ed90": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are lip-syncing talking face videos with arbitrary identities and voices in unconstrained videos. The algorithm proposed includes a novel lip-synchronization network (Wav2Lip), a new evaluation framework, and the collection of a new dataset (ReSyncED). The work builds upon existing speaker-independent approaches and aims to address their limitations using a powerful lip-sync discriminator and improved evaluation metrics.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b16\", \"b17\", \"b8\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b12\", \"b22\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": []\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating accurate lip-sync for videos with arbitrary identities and voices in unconstrained environments. The proposed method, Wav2Lip, incorporates a powerful pre-trained lip-sync discriminator to enforce realistic lip motion and introduces new benchmarks and evaluation metrics for lip synchronization. The paper's contributions include the development of a novel lip-synchronization network, an evaluation framework, and the release of the ReSyncED dataset.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b17\": 0.9,\n    \"b8\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurate lip-syncing for talking face videos with diverse, dynamic content. The proposed Wav2Lip model aims to improve lip-sync accuracy for arbitrary identities and voices in unconstrained videos. Key innovations include a powerful pre-trained lip-sync discriminator and a new evaluation framework with rigorous benchmarks and metrics.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b16\": 0.9,\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b2\": 0.6,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of lip-syncing talking face videos in diverse, unconstrained environments. It introduces a novel lip-synchronization network, Wav2Lip, which utilizes a pre-trained, accurate lip-sync discriminator to achieve superior lip-sync accuracy. The paper also proposes new evaluation benchmarks and metrics, and releases a dataset, ReSyncED, for real-world lip-sync evaluation.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.9,\n        \"b17\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.7,\n        \"b14\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating a speaker-independent model for lip-syncing talking face videos, which accurately morphs lip shapes in unconstrained, real-world videos. It proposes a novel lip-synchronization network, Wav2Lip, which uses a pre-trained expert lip-sync discriminator to enforce accurate lip motion generation.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b18\": 0.5,\n    \"b21\": 0.5\n  }\n}\n```"], "5736977f6e3b12023e66632b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of embedding large-scale information networks into low-dimensional spaces, focusing on preserving both local (first-order) and global (second-order) network structures. The LINE model introduced is capable of scaling efficiently to networks with millions of nodes and billions of edges, and the proposed edge-sampling algorithm enhances the optimization process, particularly when using stochastic gradient descent.\",\n  \"Direct Inspiration\": [\"b19\", \"b28\", \"b42\"],\n  \"Indirect Inspiration\": [\"b27\", \"b30\", \"b39\"],\n  \"Other Inspiration\": [\"b41\", \"b43\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of embedding large information networks into low-dimensional spaces while preserving both local and global network structures. The proposed model, LINE, introduces a new objective function to preserve first-order and second-order proximities and an edge-sampling method to optimize the objective efficiently.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b42\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.6,\n    \"b43\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of embedding large-scale information networks into low-dimensional spaces while preserving both local (first-order) and global (second-order) network structures. The proposed LINE model introduces a novel edge-sampling method to optimize the objective function efficiently, making it scalable for networks with millions of nodes and billions of edges.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1,\n    \"b31\": 1,\n    \"b39\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b28\": 0.8,\n    \"b42\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.75,\n    \"b41\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of embedding large-scale information networks into low-dimensional spaces to improve visualization, node classification, link prediction, and recommendation. The proposed LINE model is designed to handle networks with millions of nodes and billions of edges, preserving both first-order and second-order proximities. Additionally, the paper introduces a novel edge-sampling method to enhance the optimization process, making it more efficient and effective.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b28\": 0.9,\n    \"b27\": 0.8,\n    \"b42\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.7,\n    \"b39\": 0.7,\n    \"b43\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.5,\n    \"b32\": 0.5,\n    \"b40\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper include embedding large-scale information networks with millions of nodes and billions of edges into low-dimensional spaces, preserving both local and global network structures, and optimizing the embedding efficiently for large networks with high variance in edge weights.\",\n    \"Inspirations\": \"The authors propose a network embedding model called 'LINE' that preserves both first-order and second-order proximities and introduces an edge-sampling algorithm to improve the efficiency and effectiveness of the optimization process.\"\n  },\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b28\": 1.0,\n    \"b42\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b43\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.5,\n    \"b32\": 0.5\n  }\n}\n```"], "5d3c233f3a55acd386d4ddc1": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning dynamic node representations in graph-structured data, focusing on modeling the intertwined evolution of topological changes and node interactions over time. The proposed method, DyRep, uses a two-time scale temporal point process model combined with a deep inductive representation network to capture these dynamics and improve node representation learning.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b14\": 0.8,\n    \"b54\": 0.8,\n    \"b42\": 0.8,\n    \"b31\": 0.8,\n    \"b50\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b55\": 0.6,\n    \"b22\": 0.6,\n    \"b49\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of learning node representations in dynamic graphs, focusing on modeling two distinct dynamic processes (topological evolution and node interactions) and proposes a novel framework called DyRep, which utilizes a two-time scale deep temporal point process approach to capture continuous-time fine-grained temporal dynamics.\",\n  \"Direct Inspiration\": [\"b14\", \"b31\", \"b42\", \"b54\"],\n  \"Indirect Inspiration\": [\"b5\", \"b17\"],\n  \"Other Inspiration\": [\"b55\", \"b50\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in representation learning over dynamic graphs, emphasizing the need for models that can capture the interleaved evolution of structural and interaction dynamics at different time scales. The proposed DyRep framework introduces a two-time scale deep temporal point process model to effectively learn node representations over time, leveraging a novel Temporal Attention Mechanism to handle the complex temporal and structural dynamics.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b54\": 0.8,\n    \"b42\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b55\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning node representations for dynamic graphs, which exhibit complex temporal properties. It introduces a novel framework, DyRep, to model the interleaved evolution of two fundamental processes: topological evolution and node interactions. The proposed framework employs a two-time scale deep temporal point process approach and a novel temporal attention mechanism to capture continuous-time fine-grained temporal dynamics and effectively learn richer node representations over time.\",\n  \"Direct Inspiration\": {\n    \"references\": [\n      \"b14\",\n      \"b31\",\n      \"b42\",\n      \"b54\"\n    ],\n    \"confidence\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\n      \"b5\",\n      \"b17\",\n      \"b50\",\n      \"b55\"\n    ],\n    \"confidence\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"references\": [\n      \"b1\",\n      \"b29\",\n      \"b43\",\n      \"b53\"\n    ],\n    \"confidence\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include modeling highly dynamic graph data with complex temporal properties and learning node representations that effectively capture evolving graph information over time. The authors propose a novel framework, DyRep, to model the interleaved evolution of two fundamental processes in dynamic graphs\u2014topological evolution and node interactions\u2014through a latent mediation process.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b31\": 0.7,\n    \"b42\": 0.7,\n    \"b54\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b55\": 0.6\n  }\n}\n```"], "558c2b08e4b00c3c48e0a105": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of designing a general-purpose processor core that balances high single-thread performance and high energy efficiency. It proposes a novel heterogeneous block architecture (HBA) that leverages fine-grained heterogeneity and atomic blocks to dynamically form, specialize, and execute code blocks on the most suitable execution backend (out-of-order, VLIW, or in-order). This approach aims to maximize energy efficiency while maintaining high performance.\",\n    \"Direct Inspiration\": {\n        \"b37\": 1.0,\n        \"b61\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8,\n        \"b2\": 0.8,\n        \"b5\": 0.8,\n        \"b19\": 0.8,\n        \"b22\": 0.8,\n        \"b33\": 0.8,\n        \"b36\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.6,\n        \"b43\": 0.6,\n        \"b48\": 0.6,\n        \"b45\": 0.6,\n        \"b54\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing a general-purpose core that balances high single-thread performance and high energy efficiency. The proposed solution, the Heterogeneous Block Architecture (HBA), exploits fine-grained heterogeneity by dynamically forming code into atomic blocks and assigning them to the most appropriate execution backend (out-of-order, VLIW, or in-order) based on the observed instruction schedule stability and instruction-level parallelism (ILP) of each block.\",\n  \"Direct Inspiration\": {\n    \"b20\": 1.0,\n    \"b37\": 0.9,\n    \"b61\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.8,\n    \"b43\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving both high performance and energy efficiency in general-purpose processor core design. It proposes a fine-grained heterogeneous core that dynamically forms code into blocks and assigns them to the most appropriate execution backend. The core design leverages concepts such as block atomicity, memoized scheduling, and dynamic pipeline narrowing to optimize performance and energy efficiency.\",\n  \"Direct Inspiration\": [\"b20\", \"b37\", \"b45\"],\n  \"Indirect Inspiration\": [\"b33\", \"b36\", \"b43\"],\n  \"Other Inspiration\": [\"b1\", \"b2\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing a general-purpose processor core that achieves both high single-thread performance and high energy efficiency. The proposed solution, the Heterogeneous Block Architecture (HBA), exploits fine-grained heterogeneity by dynamically forming code into blocks and assigning them to the most appropriate execution backend (out-of-order, VLIW, or in-order). The core contributions are the introduction of HBA, implementation details of a fine-grained heterogeneous core, mechanisms for switching between execution backends, and extensive evaluation showing superior energy efficiency with minimal performance loss.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.9,\n    \"b5\": 0.9,\n    \"b19\": 0.9,\n    \"b22\": 0.9,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.8,\n    \"b61\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.7,\n    \"b20\": 0.6,\n    \"b33\": 0.6,\n    \"b45\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are achieving high single-thread performance and high energy efficiency in general-purpose processor core design, which are typically conflicting goals. The paper proposes a fine-grained heterogeneous core design that forms atomic blocks of code and dynamically assigns them to the most appropriate execution backend (out-of-order, VLIW, in-order) based on block characteristics. This approach aims to exploit fine-grained heterogeneity in code behavior to achieve better energy efficiency while maintaining performance.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9,\n    \"b61\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b5\": 0.7,\n    \"b19\": 0.7,\n    \"b22\": 0.7,\n    \"b33\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.6\n  }\n}\n```"], "53e9b043b7602d9703aa6a22": ["```json\n{\n  \"Summary\": \"The paper proposes a representation of residue classes to speed up modular multiplication without affecting modular addition and subtraction algorithms. The focus is on leveraging a radix R coprime to N to facilitate efficient computations. The paper details the algorithm REDC for computing TR-1 mod N and validates its correctness. The proposed method aims to improve computational efficiency in modular arithmetic.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b1\": 0.6,\n    \"b3\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of speeding up modular multiplication without affecting modular addition and subtraction algorithms. It introduces a novel representation of residue classes and proposes an algorithm (REDC) for quick computation in this representation.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.95,\n    \"b1\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.80,\n    \"b5\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.70,\n    \"b4\": 0.65\n  }\n}\n```", "{\n  \"Summary\": \"The paper addresses the challenge of speeding up modular multiplication without affecting modular addition and subtraction algorithms. The authors propose a representation of residue classes to achieve this goal, particularly focusing on the efficient computation of TR-1 mod N using a specific algorithm (REDC).\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b1\": 1,\n    \"b3\": 1,\n    \"b4\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {}\n}", "{\n  \"Summary\": \"The paper addresses the challenge of speeding up modular multiplication without affecting modular addition and subtraction algorithms. It proposes a representation of residue classes that leverages specific properties of radices and their relationship with the modulus to achieve this goal. The primary contribution is the REDC algorithm, which allows for efficient computation of modular multiplication.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b1\": 1.0,\n    \"b3\": 1.0,\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {}\n}", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge addressed in the paper is speeding up modular multiplication without affecting the modular addition and subtraction algorithms in modular arithmetic computations.\",\n    \"Inspirations\": \"The paper proposes a novel representation of residue classes to achieve this goal, inspired by recent algorithms and mathematical properties of modular arithmetic.\"\n  },\n  \"Direct Inspiration\": [\n    \"b0\",\n    \"b1\",\n    \"b3\",\n    \"b4\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b2\",\n    \"b5\"\n  ],\n  \"Other Inspiration\": []\n}\n```"], "5c04967517c44a2c7470927f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of personalized product search by proposing an Attentive Long Short-Term Preference model (ALSTP). The key challenges include accurately modeling both long-term and short-term user preferences and effectively integrating them with the current query. The proposed solution involves an attention mechanism to weight the importance of different factors in both types of preferences and a pairwise learning-to-rank method to optimize the ranking of product search results.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b2\": 0.9,\n    \"b55\": 0.85,\n    \"b62\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.75,\n    \"b28\": 0.7,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b44\": 0.5,\n    \"b56\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of personalized product search by proposing a neural network model, ALSTP, which integrates long-term and short-term user preferences with the current query using attention mechanisms. The primary contributions include: 1) Jointly integrating long-term and short-term user preferences, 2) Applying attention mechanisms to weight different factors, and 3) Conducting extensive experiments to validate the model's effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b33\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.7,\n    \"b55\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b57\": 0.6,\n    \"b60\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are the effective modeling of both long-term and short-term user preferences and their integration with the current query to accurately capture the user's search intention for personalized product search. The proposed algorithm, ALSTP (Attentive Long Short-Term Preference model), addresses these challenges by employing attention mechanisms to weight the relevance of different factors in both long-term and short-term user preferences with respect to the current query, and then fusing these preferences with the query for better representation of the user's intention.\",\n    \"Direct Inspiration\": [\"b0\"],\n    \"Indirect Inspiration\": [\"b2\", \"b35\", \"b55\"],\n    \"Other Inspiration\": [\"b33\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in this paper are accurately modeling long-term and short-term user preferences and effectively integrating them with the current query for personalized product search.\",\n    \"Inspirations\": \"The work is inspired by the necessity to incorporate both long-term and short-term user preferences in personalized product search to improve search accuracy and user experience.\"\n  },\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b57\": 0.9,\n    \"b60\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b28\": 0.7,\n    \"b54\": 0.7,\n    \"b35\": 0.8,\n    \"b55\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b58\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving personalized product search by integrating long-term and short-term user preferences with the current query. The proposed algorithm, Attentive Long Short-Term Preference model (ALSTP), uses attention mechanisms to weight the importance of different factors in both long-term and short-term preferences, and then fuses these with the current query to better represent the user's specific intention.\",\n  \"Direct Inspiration\": [\"b0\", \"b1\", \"b33\"],\n  \"Indirect Inspiration\": [\"b2\", \"b35\", \"b57\"],\n  \"Other Inspiration\": [\"b10\", \"b12\", \"b41\"]\n}\n```"], "5ef3247091e0110c353da5ff": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of semantic matching in search engines, especially for Facebook search, by proposing a unified embedding model that incorporates user and context information along with query text. The model leverages triplet loss and various feature engineering techniques to optimize recall in retrieval tasks. The authors also integrate embedding-based retrieval with traditional term matching using a hybrid retrieval framework, utilizing libraries like Faiss for embedding vector quantization.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1, \n    \"b12\": 0.9, \n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8, \n    \"b10\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7, \n    \"b4\": 0.65, \n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving search quality in search engines, particularly focusing on embedding-based retrieval (EBR) techniques. The authors propose a unified embedding model that incorporates text, user, and context to address the unique challenges of Facebook search. Key methods include triplet loss for training, feature engineering, and hybrid retrieval frameworks to integrate embedding KNN and term matching.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b6\": 0.7,\n    \"b7\": 0.65,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.55,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are accurate semantic matching in search engines, integrating embedding-based retrieval (EBR) with term matching, and handling the scale of data in search engines like Facebook. The paper proposes a unified embedding model, hybrid retrieval framework, and various optimization techniques to address these challenges.\",\n  \"Direct Inspiration\": [\"b1\", \"b8\"],\n  \"Indirect Inspiration\": [\"b6\", \"b7\", \"b10\"],\n  \"Other Inspiration\": [\"b4\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of embedding-based retrieval (EBR) in Facebook's search engine, particularly focusing on the integration of user and contextual data into embeddings, and the efficient serving of these embeddings. It introduces a unified embedding model that combines text, user, and contextual information to improve retrieval performance. The model uses triplet loss for training and incorporates various feature engineering techniques, such as location and social embedding features. The paper also discusses the implementation of approximate nearest neighbor (ANN) search using the Faiss library and the development of a hybrid retrieval framework that integrates embedding-based KNN with Boolean matching.\",\n  \"Direct Inspiration\": [\"b8\"],\n  \"Indirect Inspiration\": [\"b1\", \"b6\", \"b7\", \"b10\", \"b12\", \"b13\", \"b14\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in search engines regarding accurate semantic matching and retrieval, particularly within Facebook search. It proposes a unified embedding model that incorporates query text, user data, and contextual information to improve retrieval performance. The study employs techniques such as triplet loss for training, feature engineering for better embeddings, and hybrid retrieval frameworks integrating embedding-based and term matching retrieval.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.7,\n    \"b10\": 0.7,\n    \"b12\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b13\": 0.8\n  }\n}\n```"], "5d245bb6da56295a28fcd54f": ["```json\n{\n  \"Summary\": \"The paper discusses the challenges of using geometric scattering features for graph data analysis, emphasizing stability and capacity. The primary inspiration comes from extending the Euclidean scattering transform to graphs using a wavelet approach. The novel contributions include defining the graph scattering transform, comparing it to other graph ConvNet methods, and demonstrating its application in various graph data analysis tasks.\",\n  \"Direct Inspiration\": [\"b23\", \"b15\", \"b40\"],\n  \"Indirect Inspiration\": [\"b11\", \"b37\", \"b4\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of using geometric scattering transforms for effective graph data analysis. It explores whether these transforms can serve as task-independent feature extractors and investigates their stability and capacity in representing graphs.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b15\": 0.9,\n    \"b40\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b37\": 0.75,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.65,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of extracting task-independent features from graph data using a geometric scattering transform. It proposes a method that leverages graph wavelets and scattering transforms to capture multiscale geometric information from graphs. The approach is validated through applications in graph classification, dimensionality reduction, and data exploration in social networks and biochemistry datasets.\",\n  \"Direct Inspiration\": [\"b23\"],\n  \"Indirect Inspiration\": [\"b40\", \"b15\"],\n  \"Other Inspiration\": [\"b11\", \"b4\", \"b12\", \"b37\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of utilizing geometric scattering transforms to extract effective and stable task-independent features from graph data. The algorithm proposed aims to leverage graph wavelets and the geometric scattering transform to provide a rich feature space for graph data analysis, ensuring stability to deformations and retaining significant information for various machine learning tasks.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b40\": 0.9,\n    \"b15\": 0.85,\n    \"b11\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b12\": 0.7,\n    \"b37\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of feature extraction from graph-structured data, specifically focusing on the geometric scattering transform as a task-independent feature extractor. The authors explore the stability and capacity of geometric scattering features, comparing their effectiveness to other graph-based methods, and demonstrate their application in various data analysis tasks including graph classification and dimensionality reduction.\",\n    \"Direct Inspiration\": [\"b23\", \"b40\", \"b15\"],\n    \"Indirect Inspiration\": [\"b5\", \"b11\", \"b37\"],\n    \"Other Inspiration\": [\"b4\", \"b12\"]\n}\n```"], "5c20b1fcda5629702063afe6": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the performance degradation caused by branch misprediction in superscalar processors. The authors propose a novel algorithm called PUBS (Prioritizing Unconfident Branch Slices) to minimize the misspeculation penalty by prioritizing the issue of instructions in unconfident branch slices. The scheme uses confidence estimation and linking mechanisms to identify unconfident branch slices and prioritizes their execution to reduce waiting cycles in the issue queue.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is reducing the penalty caused by branch misprediction in superscalar processors, specifically focusing on reducing the cycles consumed during useless speculative execution. The proposed algorithm, PUBS (Prioritizing Unconfident Branch Slices), aims to minimize the misspeculation penalty by issuing instructions in unconfident branch slices with the highest priority from the issue queue.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of branch misprediction in superscalar processors by proposing a scheme called PUBS (Prioritizing Unconfident Branch Slices) to reduce the misspeculation penalty. The PUBS scheme prioritizes the issue of instructions in unconfident branch slices to minimize the waiting cycles in the issue queue, thereby reducing the overall misspeculation penalty.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing the penalty caused by branch misprediction in superscalar processors, specifically focusing on minimizing the misspeculation penalty. The proposed solution, PUBS (Prioritizing Unconfident Branch Slices), improves performance by issuing instructions in unconfident branch slices with the highest priority to reduce their waiting cycles in the issue queue.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the penalty caused by branch misprediction in superscalar processors, specifically focusing on the misspeculation penalty. The proposed algorithm, PUBS (Prioritizing Unconfident Branch Slices), aims to minimize the misspeculation penalty by prioritizing the issue of instructions in unconfident branch slices.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n  },\n  \"Other Inspiration\": {\n  }\n}\n```"], "573698426e3b12023e70c039": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of slow and time-consuming architectural simulation in computer architecture by proposing LiveSim, a simulation framework that provides near real-time simulation results. LiveSim is designed to improve the productivity of computer architects by enabling interactive microarchitecture design space exploration.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b3\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b13\": 0.7,\n        \"b8\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.6,\n        \"b14\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the slow, expensive, and time-consuming process of architectural simulation. The authors propose LiveSim, a simulation framework that provides near real-time simulation results to improve the productivity of computer architects by enabling interactive microarchitecture design space exploration. LiveSim uses in-memory checkpoints, checkpoint clustering, parallel checkpoint execution, and a fast cache warmup technique to achieve its goal.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b3\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the slow, expensive, and time-consuming process of system design, specifically architectural simulation. The proposed solution, LiveSim, is a simulation framework that provides near real-time results, significantly improving the productivity of computer architects by offering initial results within 5 seconds. LiveSim integrates several techniques including statistical sampling, in-memory checkpoints, checkpoint clustering, parallel checkpoint execution, and a fast cache warmup technique.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b3\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of slow and time-consuming architectural simulation in computer architecture. It proposes LiveSim, an interactive simulation framework that provides near real-time simulation results, enabling more efficient design space exploration. The novel contributions include introducing LiveSim, demonstrating its ability to provide accurate results quickly, and producing confidence intervals within user-defined error bounds.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b3\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b11\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b10\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the slow and time-consuming nature of architectural simulation by proposing LiveSim, a new framework that provides near real-time simulation results. LiveSim leverages systematic sampling, in-memory checkpoints, checkpoint clustering, parallel checkpoint execution, and a fast cache warmup technique to achieve interactive simulation. Key contributions include accurate simulation results within 5 seconds (LiveSample) and bounded error results within 41 seconds on average (LiveCI).\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.8,\n    \"b13\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.6,\n    \"b15\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.4,\n    \"b14\": 0.3\n  }\n}\n```"], "5f4e198191e011084df59c0b": ["```json\n{\n  \"Summary\": \"The paper addresses the name disambiguation problem in digital libraries, where multiple authors with the same name can cause confusion. The authors propose a novel Multi-view Attention-based Pair Recurrent Neural Network (MA-PairRNN) framework to solve this problem. The main challenges include handling the heterogeneity of academic networks, ensuring inductive capability for new papers, and dealing with the uncertain number of authors with the same name. The proposed algorithm uses meta-path based views to capture semantic information, a semantic attention layer to fuse these representations, and a Pseudo-Siamese recurrent neural network to learn the similarity of paper sequences.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b4\": 0.9,\n    \"b5\": 0.95,\n    \"b6\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b13\": 0.75,\n    \"b23\": 0.72\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b19\": 0.68,\n    \"b22\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of name disambiguation in academic networks, which involves identifying unique persons with the same name, particularly when discriminative attributes like email and affiliation change over time. The proposed solution, MA-PairRNN, employs a novel Multi-view Attention-based Pair Recurrent Neural Network framework that leverages the stability of co-author sets and research areas over time. Key components of the proposed method include multi-view graph embedding, semantic attention, and a Pseudo-Siamese recurrent neural network layer for pairwise paper set classification.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 0.9,\n    \"b4\": 0.8,\n    \"b5\": 0.9,\n    \"b6\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b13\": 0.7,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the name disambiguation problem in academic databases, focusing on overcoming challenges like heterogeneity of academic networks, inductive capability, and uncertain number of authors. The proposed solution, MA-PairRNN, combines multi-view graph embedding, semantic attention, and Pseudo-Siamese recurrent neural network layers to classify paper sets and merge them based on pairwise similarity.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b4\": 0.85,\n        \"b5\": 0.8,\n        \"b6\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.7,\n        \"b9\": 0.75,\n        \"b13\": 0.75,\n        \"b23\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.65\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of name disambiguation in academic networks, focusing on issues like the heterogeneity of academic networks, inductive capability, and the uncertain number of authors with the same name. The proposed solution, MA-PairRNN, utilizes a multi-view attention-based pair recurrent neural network framework to classify pairs of paper sets without needing to estimate the number of authors. The core contributions include a multi-view graph embedding layer, a semantic attention layer, and a pseudo-Siamese recurrent neural network layer to capture and fuse rich semantic information for name disambiguation.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b2\": 0.8,\n        \"b5\": 0.9,\n        \"b6\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.7,\n        \"b13\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the name disambiguation problem in academic networks, which involves identifying unique authors among multiple authors with the same name. The main challenges include handling the heterogeneity of academic networks, developing inductive capabilities for new papers, and dealing with an uncertain number of authors. The proposed solution is a Multi-view Attention-based Pair Recurrent Neural Network (MA-PairRNN) framework, which leverages co-authorship and research interests consistency to perform pairwise classification of paper sets, using multi-view graph embeddings, semantic-level attention, and a Pseudo-Siamese recurrent neural network to achieve this.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.85,\n    \"b4\": 0.8,\n    \"b5\": 0.75,\n    \"b6\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.65,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.55,\n    \"b23\": 0.5\n  }\n}\n```"], "53e9a415b7602d9702d30d91": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of scalable computation on massive graph data, particularly focusing on the balanced graph partitioning problem and efficient processing of dynamic graphs. The proposed algorithm, Fennel, introduces a general framework for graph partitioning that relaxes hard cardinality constraints and provides an interpolation between existing heuristics to achieve high-quality graph partitions.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.9,\n    \"b28\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.85,\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.75,\n    \"b6\": 0.7,\n    \"b21\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalable graph partitioning for massive graph data and proposes a new framework for streaming graph partitioning. The primary contributions include the introduction of a general framework that relaxes cardinality constraints, the evaluation of the proposed method (Fennel) on various datasets, and the demonstration of significant performance gains compared to existing methods.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b25\", \"b28\"],\n    \"confidence\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b2\", \"b19\"],\n    \"confidence\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b18\", \"b22\", \"b26\"],\n    \"confidence\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalable computation on massive graph data, particularly focusing on balanced graph partitioning and efficient processing of dynamic graphs. It introduces Fennel, a new framework for graph partitioning that relaxes hard cardinality constraints and provides high-quality partitions while bridging the gap between theory and practice.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b19\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalable graph partitioning and efficient processing of dynamic graphs. It introduces a new framework for graph partitioning that relaxes hard cardinality constraints and unifies popular heuristics used for streaming balanced graph partitioning. The authors propose and evaluate a new streaming graph partitioning method, Fennel, demonstrating its improved performance over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b15\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalable computation on massive graph data, particularly focusing on the balanced graph partitioning problem and its dynamic version, streaming graph partitioning. The authors propose a new framework and algorithm called Fennel for streaming balanced graph partitioning, which relaxes hard cardinality constraints and interpolates between existing heuristics.\",\n  \"Direct Inspiration\": [\"b25\", \"b28\"],\n  \"Indirect Inspiration\": [\"b19\", \"b2\"],\n  \"Other Inspiration\": [\"b18\"]\n}\n```"], "5992a2ed5ba2006b76482df8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently partitioning large-scale graphs in distributed systems, focusing on edge partitioning to minimize vertex replication and communication overhead. The authors propose a novel heuristic called Neighbor Expansion (NE) based on a new partitioning model, providing theoretical bounds on replication factors and demonstrating significant improvements over existing methods in practical applications.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b5\": 1,\n    \"b14\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of large-scale graph partitioning, specifically focusing on edge partitioning to improve workload balance and minimize communication between machines. The proposed NE (Neighbor Expansion) algorithm aims to maximize edge locality and reduce the replication factor in distributed graph systems.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b2\", \"b5\", \"b14\", \"b16\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b6\", \"b12\", \"b15\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b7\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of edge partitioning in distributed graph engines to handle large-scale graphs more efficiently. The authors propose a heuristic called NE (Neighbor Expansion) that aims to minimize vertex replication while ensuring balanced workload distribution across machines. The paper establishes theoretical foundations for the edge partitioning problem, proves its NP-hardness, and provides a worst-case upper bound for the replication factor.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b5\": 1,\n    \"b14\": 1,\n    \"b16\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the efficient partitioning of large-scale graphs to minimize vertex replication while ensuring balanced workloads across machines. The authors propose a novel edge partitioning heuristic called NE (Neighbor Expansion) that maximizes edge locality. They provide theoretical guarantees for the replication factor and demonstrate significant improvements in system performance through extensive experiments.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b5\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.6,\n    \"b12\": 0.5,\n    \"b16\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently partitioning large-scale graphs to optimize distributed computation. The authors propose a novel heuristic called Neighbor Expansion (NE) that improves edge locality and reduces replication factors, which are crucial for minimizing communication costs and ensuring balanced workloads across machines.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b2\", \"b5\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b12\", \"b14\", \"b16\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b6\", \"b15\"]\n  }\n}\n```"], "5a260c0c17c44a4ba8a1e113": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting protein interfaces using graph convolutional networks (GCNs). The main inspiration comes from the success of deep learning in image classification and the need for novel methodologies in a saturated field of protein interface prediction. The proposed method leverages graph convolution to handle the irregular 3D structure of proteins, outperforming previous SVM-based approaches.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b8\": 0.85,\n    \"b18\": 0.85,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.75,\n    \"b4\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of predicting protein interfaces using a novel graph convolutional approach. Inspired by successful methods in computer vision and image classification, the authors generalize the convolution operator to graph data. They propose several graph convolution operators to improve prediction accuracy and demonstrate that their methods outperform state-of-the-art approaches.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b15\": 1.0,\n    \"b4\": 0.9,\n    \"b8\": 0.9,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b1\": 0.75,\n    \"b18\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting protein interfaces by proposing a graph convolution approach. This method generalizes convolutional neural networks to graph structures, allowing for prediction tasks on proteins represented as graphs. The paper highlights the need for new methodologies in interface prediction, inspired by successful image classification techniques.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b7\": 0.9,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b4\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting protein interfaces using a novel graph convolution approach. Inspired by successful image classification techniques, the authors propose multiple graph convolution operators to handle the irregular structure of protein data. The method is designed to learn local features important for protein interactions without imposing an order on the nodes, achieving state-of-the-art accuracy compared to previous methods.\",\n  \"Direct Inspiration\": [\n    \"b15\", \n    \"b1\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b7\", \n    \"b8\", \n    \"b18\"\n  ],\n  \"Other Inspiration\": [\n    \"b20\", \n    \"b4\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is predicting protein interfaces using graph convolution methods. The authors propose novel graph convolution operators that are order-independent and can handle arbitrary graph sizes and structures. They aim to improve upon traditional methods that rely on hand-crafted features by leveraging convolutional techniques inspired by image classification.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b1\": 0.9,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b18\": 0.7,\n    \"b20\": 0.7,\n    \"b4\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```"], "53e99c66b7602d9702514d51": ["```json\n{\n  \"Summary\": \"The paper addresses the emerging memory capacity wall in future commodity systems due to increased core counts, virtual machine consolidation, and growing memory footprints. The proposed solution is a disaggregated memory architecture using memory blades, enabling transparent memory expansion and sharing across compute blades. This approach leverages virtualization for managing memory dynamically without extensive changes to commodity systems.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.95,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.7,\n    \"b38\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b13\": 0.6,\n    \"b18\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of the impending memory capacity wall for future commodity systems, driven by increasing core counts, virtual machines, and memory footprints. It proposes a novel architectural solution involving disaggregated memory designs to enhance memory capacity and sharing across systems. The proposed design includes a memory blade that can be accessed by multiple compute blades, providing transparent memory expansion and sharing. The paper evaluates the effectiveness of this approach through simulations, demonstrating significant performance benefits and cost efficiency.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b13\": 0.8,\n    \"b17\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of memory capacity constraints in future commodity systems, exacerbated by increasing cores per socket, more virtual machines per core, and growing memory footprints per VM. The proposed solution involves a new architectural building block for transparent memory expansion and sharing via a disaggregated memory design, which separates memory modules into a shared memory blade accessible by multiple compute blades through a shared interconnect.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b9\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b13\": 0.6,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b21\": 0.5,\n    \"b23\": 0.5,\n    \"b25\": 0.5,\n    \"b27\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges addressed in the paper include the imbalance between memory capacity and compute power, the need for transparent memory expansion and sharing, and the cost and power consumption of memory in future systems.\",\n    \"inspirations\": \"The paper proposes a disaggregated memory design to address these challenges, inspired by trends in blade servers, virtualization, and commodity-based solutions.\"\n  },\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b9\": 0.85,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b12\": 0.65,\n    \"b13\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b21\": 0.45\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of the increasing memory capacity demands in commodity systems due to trends like higher core counts and VM usage. It proposes a novel architectural building block\u2014a disaggregated memory blade that can provide transparent memory expansion and sharing across compute blades with minimal changes to existing hardware. Two system architectures are introduced: one using virtualization support for page-level access and another with minimal hardware changes for fine-grained remote memory access.\",\n    \"Direct Inspiration\": [\"b7\", \"b9\", \"b10\"],\n    \"Indirect Inspiration\": [\"b11\", \"b15\", \"b18\"],\n    \"Other Inspiration\": [\"b27\", \"b36\"]\n}\n```"], "558bbd9ce4b00c3c48de3915": ["```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges outlined in the paper include supporting multithreaded x86 64 code in binary instrumentation, ensuring thread safety, efficiently managing thread-local data, and reducing runtime overhead in multithreaded HPC programs.\",\n    \"Inspirations\": \"The paper is inspired by the need to handle complex parallelization techniques in HPC programs and aims to improve upon existing binary instrumentation tools like Pin and DyninstAPI by adding support for multithreaded codes and introducing novel optimizations for thread-specific data management and state preservation.\"\n  },\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of instrumenting multithreaded HPC programs while minimizing runtime overhead. It introduces enhancements to PEBIL, a static binary instrumentation toolkit, to support threading models such as pthreads and OpenMP. The paper provides comparisons with other instrumentation platforms like Pin and DyninstAPI, and demonstrates the effectiveness of PEBIL in capturing memory address traces with reduced overhead through sampling techniques.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b3\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in analyzing complex parallelization techniques in HPC programs, particularly focusing on multithreaded code support in the PEBIL binary instrumentation toolkit. It introduces new threading support for pthreads and OpenMP, compares PEBIL with Pin and DyninstAPI, and discusses optimizations for efficient memory address trace collection.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of analyzing multithreaded HPC programs using binary instrumentation, specifically focusing on minimizing runtime overhead while providing accurate memory address traces. The proposed solution is an extension of the PEBIL framework to support pthreads and OpenMP, compared to existing tools like Pin and DyninstAPI.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b2\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.5,\n        \"b5\": 0.5,\n        \"b6\": 0.5,\n        \"b7\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adding support for multithreaded programs to the PEBIL binary instrumentation toolkit. It compares PEBIL's threading model to those of Pin and DyninstAPI and evaluates their performance in memory address trace collection for multithreaded programs.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.7,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```"], "5ee3527191e011cb3bff7700": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of link prediction for Knowledge Graphs (KGs) that are highly incomplete and dynamically evolving. It introduces a novel meta-learning framework called Graph Extrapolation Networks (GENs) for few-shot Out-Of-Graph (OOG) link prediction, which predicts links between both seen-to-unseen and unseen-to-unseen entities. GENs use inductive and transductive GNNs to handle emerging entities and account for uncertainty with stochastic embedding.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b48\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b37\": 0.8,\n    \"b55\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.7,\n    \"b39\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Link prediction for dynamically evolving knowledge graphs with emerging entities.\",\n      \"Handling long-tail distribution where many entities have few associative triplets.\"\n    ],\n    \"Inspirations\": [\n      \"Existing limitations of embedding-based methods for unseen entities.\",\n      \"Motivation to develop a meta-learning framework to handle few-shot out-of-graph link prediction.\"\n    ]\n  },\n  \"Direct Inspiration\": [\"b16\", \"b48\"],\n  \"Indirect Inspiration\": [\"b37\", \"b55\"],\n  \"Other Inspiration\": [\"b4\", \"b56\", \"b30\", \"b29\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot out-of-graph (OOG) link prediction in multi-relational graphs, where new entities emerge and only have a few associative triplets. The authors propose Graph Extrapolation Networks (GENs), a meta-learning framework designed to handle both seen-to-unseen and unseen-to-unseen link predictions. The framework employs two GNNs: an inductive GEN for predicting embeddings of unseen entities and a transductive GEN for predicting links between unseen entities. The method is validated on knowledge graph completion and drug-to-drug interaction prediction tasks, showing superior performance over relevant baselines.\",\n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b48\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b37\": 0.8,\n    \"b55\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b35\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Link prediction for dynamically evolving knowledge graphs with emerging entities.\",\n      \"Handling long-tail distribution where many entities have few triplets.\"\n    ],\n    \"novel_methods\": [\n      \"Proposed a novel meta-learning framework, Graph Extrapolation Networks (GENs), for few-shot out-of-graph (OOG) link prediction.\",\n      \"Introduced inductive and transductive GNNs to predict embeddings and links for unseen entities.\",\n      \"Utilized stochastic embedding to model uncertainty in link predictions.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b16\", \"b48\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b4\", \"b37\", \"b55\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b35\", \"b56\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenges addressed in this paper are the dynamic evolution of knowledge graphs with emerging entities and the long-tail distribution problem where many entities have only a few triplets.\",\n    \"Inspirations\": \"The authors propose a novel meta-learning framework called Graph Extrapolation Networks (GENs) to tackle the few-shot out-of-graph (OOG) link prediction for emerging entities.\"\n  },\n  \"Direct Inspiration\": [\n    \"b16\",\n    \"b48\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b37\",\n    \"b55\"\n  ],\n  \"Other Inspiration\": [\n    \"b4\",\n    \"b56\",\n    \"b35\",\n    \"b30\"\n  ]\n}\n```"], "5736974d6e3b12023e638aca": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of knowledge base completion by proposing a new method called subgraph feature extraction (SFE), which aims to improve both the efficiency and expressivity of the existing Path Ranking Algorithm (PRA). SFE simplifies the process by eliminating the computationally intensive second step of PRA, which involves calculating random walk probabilities. Instead, SFE uses a single search over the graph and constructs binary feature matrices. This method not only reduces computational complexity but also allows for the inclusion of more expressive features that are not representable in PRA.\",\n  \"Direct Inspiration\": [\"b14\", \"b6\", \"b10\"],\n  \"Indirect Inspiration\": [\"b15\", \"b8\", \"b9\", \"b20\"],\n  \"Other Inspiration\": [\"b16\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of knowledge base (KB) completion by proposing a new method called subgraph feature extraction (SFE), which aims to improve both the efficiency and expressivity of the current Path Ranking Algorithm (PRA). The key inspiration for this work includes the inefficiencies and limited expressivity of PRA, and the need for more computationally feasible methods for generating feature matrices over node pairs in a graph.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the computational inefficiency and limited expressivity of the Path Ranking Algorithm (PRA) in knowledge base completion tasks. The authors propose a new method called subgraph feature extraction (SFE), which aims to generate feature matrices over node pairs in a graph more efficiently and expressively than PRA.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency and limited expressivity of the Path Ranking Algorithm (PRA) for knowledge base completion. The authors propose a new technique called subgraph feature extraction (SFE) that simplifies the process and improves both efficiency and expressivity by eliminating the need for computationally intensive random walk probabilities. SFE constructs feature matrices over node pairs using a single search over the graph, allowing for more expressive features and significantly reducing computation time while improving prediction performance.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.9,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of knowledge base completion by proposing a novel algorithm called subgraph feature extraction (SFE) to improve both the efficiency and expressivity of the Path Ranking Algorithm (PRA). SFE avoids the computationally intensive second step of PRA and uses local subgraph searches and feature extraction to generate feature matrices for node pairs in a graph.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b14\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```"], "5ecb57199e795ec6f2ba59cc": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of Entity Matching (EM) using pre-trained Transformer-based language models. The proposed solution, D, leverages these models to enhance language understanding and improve matching capabilities. Key optimizations include injecting domain knowledge, summarizing long strings, and augmenting training data with difficult examples.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b28\": 0.8,\n    \"b44\": 0.7,\n    \"b30\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.6,\n    \"b24\": 0.6,\n    \"b38\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b47\": 0.5,\n    \"b8\": 0.5,\n    \"b49\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Entity Matching (EM) using pre-trained Transformer-based language models. The primary contributions include casting EM as a sequence-pair classification problem, fine-tuning pre-trained language models for better language understanding and matching accuracy, and introducing three optimization techniques: domain knowledge injection, summarization of long strings, and data augmentation.\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b28\": 0.85,\n    \"b44\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b33\": 0.75,\n    \"b25\": 0.65,\n    \"b24\": 0.6,\n    \"b38\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Entity Matching (EM) by proposing a novel solution named D, which leverages pre-trained Transformer-based language models for improved language understanding and matching capability. The solution includes optimizations such as domain knowledge injection, summarization of long strings, and data augmentation to enhance performance.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b28\": 0.8,\n    \"b44\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b24\": 0.7,\n    \"b38\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents a novel Entity Matching (EM) solution named D based on pre-trained Transformer-based language models. The primary challenges addressed include the need for substantial language understanding and domain-specific knowledge to correctly match candidate pairs. The algorithm improves matching capability through optimizations such as injecting domain knowledge, summarizing long strings, and augmenting training data with difficult examples.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b24\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.9,\n    \"b50\": 0.8,\n    \"b47\": 0.6,\n    \"b36\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents a novel Entity Matching (EM) solution called D, which leverages pre-trained Transformer-based language models to improve the matching capability. The primary challenges addressed include the need for substantial language understanding and domain-specific knowledge, handling long strings, and reducing the amount of training data required. The proposed solution introduces three optimizations: injecting domain knowledge, summarizing long strings, and augmenting training data with difficult examples.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b28\": 1.0,\n    \"b44\": 1.0,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b24\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b47\": 0.7,\n    \"b31\": 0.6,\n    \"b41\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```"], "5dbebb7447c8f766462c21c0": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the difficulty of incorporating emotional factors into dialogue systems, modeling human emotion from a given sentence due to semantic sparsity, deciding the optimal response emotion, and generating plausible emotional sentences without sacrificing grammatical fluency and semantic coherence. The proposed algorithm, Emotion-Aware Chat Machine (EACM), addresses these challenges using a unified Seq2seq architecture with a self-attention enhanced emotion selector and an emotion-biased response generator.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1,\n    \"b47\": 1,\n    \"b10\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b35\": 0.8,\n    \"b17\": 0.8,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b36\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating emotional factors into dialogue systems, proposing an emotion-aware chat machine (EACM) based on Seq2seq architecture with a self-attention enhanced emotion selector and an emotion-biased response generator. It aims to generate responses that are both semantically reasonable and emotionally appropriate.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1,\n    \"b47\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b24\": 0.7,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5,\n    \"b39\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating emotional factors into dialogue systems, particularly focusing on generating emotionally appropriate responses automatically without manual intervention. The proposed solution, EACM, utilizes a Seq2seq architecture enhanced with self-attention and emotion selectors to generate responses that are semantically coherent and emotionally appropriate.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b24\": 0.8,\n    \"b47\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b35\": 0.7,\n    \"b32\": 0.7,\n    \"b40\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of incorporating emotional factors into dialogue systems, specifically in the context of response generation. The authors propose a novel emotion-aware chat machine (EACM) based on a Seq2seq architecture with a self-attention enhanced emotion selector and an emotion-biased response generator. This approach aims to generate emotionally appropriate responses by leveraging both the emotional and semantic information of the input post.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1.0,\n    \"b47\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b25\": 0.7,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses three main challenges in emotion-aware dialogue systems: (1) modeling the complex emotions from a given sentence, (2) determining the optimal response emotion, and (3) maintaining grammatical fluency and semantic coherence. The authors propose a novel emotion-aware chat machine (EACM) based on a unified Seq2seq architecture with a self-attention enhanced emotion selector and an emotion-biased response generator to tackle these challenges.\",\n  \"Direct Inspiration\": [\"b45\", \"b47\"],\n  \"Indirect Inspiration\": [\"b10\", \"b24\", \"b25\"],\n  \"Other Inspiration\": [\"b4\", \"b35\"]\n}\n```"], "5f8d6be69fced0a24bbab005": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of predicting Click-Through-Rate (CTR) in advertising systems, where input variables are mostly discrete and categorical, leading to a large and sparse feature space. The proposed solution is a novel Dimension Relation Module (DRM) based on self-attention mechanisms to model relations among latent fields in the embedding space, thereby improving CTR prediction accuracy.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b8\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.9,\n        \"b4\": 0.9,\n        \"b10\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.8,\n        \"b6\": 0.7,\n        \"b9\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Click-Through-Rate (CTR) prediction, particularly focusing on the transformation of sparse and discrete input variables into dense embedding features. The proposed solution is the Dimension Relation Module (DRM) based on self-attention mechanisms, which improves CTR prediction by modeling the relations among latent fields in the embedding space.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b7\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge in CTR prediction of dealing with discrete and categorical input variables leading to a large and sparse feature space. It introduces a novel Dimension Relation Module (DRM) based on dimension recalibration and self-attention mechanisms to model the relations among latent fields in embedding space. The proposed module improves CTR prediction accuracy by capturing extra useful information and boosting the performance of existing state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b8\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.85,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.75,\n    \"b3\": 0.7,\n    \"b7\": 0.65,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed is improving the accuracy of Click-Through-Rate (CTR) prediction by modeling relations among latent fields in embedding space. The novel methods introduced include the Dimension Relation Module (DRM) based on self-attention mechanisms and the FED network, which combines field-wise and element-wise modeling.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b4\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.8,\n    \"b10\": 0.8,\n    \"b6\": 0.7,\n    \"b9\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper include dealing with large and sparse feature spaces in CTR prediction, and the difficulty of capturing high-order feature interactions using traditional linear models. The proposed algorithm, DRM (Dimension Relation Module), aims to explicitly model the relations among latent fields in the embedding feature space to improve CTR prediction accuracy. The paper introduces DRM, which uses a self-attention mechanism to recalibrate dimensions and enhance embeddings. The FED network, built on DRM, models complex interactions among features at multiple levels (field-wise and element-wise).\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0,\n        \"b8\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.9,\n        \"b4\": 0.9,\n        \"b10\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.7,\n        \"b6\": 0.7,\n        \"b9\": 0.7\n    }\n}\n```"], "5e5794b791e011545375102b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing high-quality DNN accelerators, which often require extensive time and expertise due to the large design space and need for algorithm/hardware co-design. The proposed solution, DNN-Chip Predictor, is an analytical performance predictor that can efficiently estimate DNN accelerators' energy, throughput, and latency, aiding in fast evaluation and design optimization.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b21\": 0.9,\n    \"b23\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.65,\n    \"b22\": 0.6,\n    \"b10\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the high complexity and extensive design space of DNN accelerators, which require significant time and expertise to develop. The paper proposes the DNN-Chip Predictor, an analytical performance predictor that efficiently and accurately predicts DNN accelerators' performance, enabling fast evaluation and optimization of DNN accelerator designs.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b10\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b26\": 0.7,\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.5,\n    \"b25\": 0.5,\n    \"b29\": 0.5,\n    \"b28\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of developing high-quality DNN accelerators, which are complex and time-consuming to design. The authors propose the DNN-Chip Predictor, an analytical performance predictor that can efficiently and accurately predict the performance of DNN accelerators in terms of energy, throughput, and latency. This tool is designed to be fast, cover both ASIC and FPGA accelerators, and validated against various DNN models and designs.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.8,\n        \"b21\": 0.7,\n        \"b22\": 0.7,\n        \"b23\": 0.7,\n        \"b24\": 0.7,\n        \"b26\": 0.7\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of developing high-quality DNN accelerators, which involve large design spaces, algorithm/hardware co-design requirements, and extensive time and expertise. It proposes the DNN-Chip Predictor, an analytical performance predictor for efficiently and accurately estimating DNN accelerators' performance, validated across various DNN models and accelerator designs.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b21\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing high-quality DNN accelerators, specifically the complexity and time-consuming nature of their development. It proposes DNN-Chip Predictor, an analytical performance predictor that can efficiently and accurately predict DNN accelerators' performance, enabling fast evaluation and design space exploration of DNN accelerators.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b21\": 0.9,\n    \"b26\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.8,\n    \"b12\": 0.75,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.65,\n    \"b16\": 0.65,\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b23\": 0.55,\n    \"b24\": 0.5\n  }\n}\n```"], "555048f045ce0a409eb72c02": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of graph partitioning (GP) in distributed graph computing frameworks, particularly focusing on power-law graphs. It proposes a novel vertex-cut method called degree-based hashing (DBH) that effectively exploits power-law degree distributions, offering theoretical guarantees on communication cost and workload balance, and demonstrating empirical improvements over existing methods.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b5\", \"b7\"],\n    \"confidence_score\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b3\"],\n    \"confidence_score\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b12\", \"b14\", \"b6\"],\n    \"confidence_score\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in distributed graph computing (DGC) for power-law graphs, proposing a novel vertex-cut graph partitioning method called degree-based hashing (DBH). The DBH method effectively exploits power-law degree distributions to minimize communication costs and balance workloads better than existing methods. Key contributions include theoretical bounds on communication cost and workload balance, implementation compatibility with PowerGraph, and empirical results demonstrating superior performance.\",\n  \"Direct Inspiration\": [\"b3\", \"b5\"],\n  \"Indirect Inspiration\": [\"b7\", \"b8\", \"b14\", \"b18\", \"b19\"],\n  \"Other Inspiration\": [\"b2\", \"b0\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of graph partitioning (GP) for distributed graph computing (DGC) frameworks, focusing on power-law graphs. It proposes a novel vertex-cut GP method called degree-based hashing (DBH) that effectively exploits power-law degree distributions to minimize communication cost and balance workload. Theoretical analysis and empirical results demonstrate DBH's superior performance compared to existing methods.\",\n  \"Direct Inspiration\": [\"b5\"],\n  \"Indirect Inspiration\": [\"b3\", \"b7\"],\n  \"Other Inspiration\": [\"b12\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of graph partitioning (GP) in distributed graph-computing (DGC) frameworks, particularly focusing on minimizing communication costs and balancing workloads when dealing with power-law distributions. It proposes a novel vertex-cut method called degree-based hashing (DBH) that leverages power-law degree distributions to improve performance over existing methods such as PowerGraph and GraphBuilder.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in distributed graph-computing (DGC) frameworks, specifically focusing on optimizing graph partitioning (GP) for power-law graphs. The proposed method, degree-based hashing (DBH), aims to reduce communication costs and balance workloads effectively by leveraging the power-law degree distribution of graphs. The theoretical and empirical results demonstrate that DBH outperforms existing methods such as those used in PowerGraph and GraphBuilder.\",\n  \"Direct Inspiration\": [\"b5\", \"b7\"],\n  \"Indirect Inspiration\": [\"b3\"],\n  \"Other Inspiration\": [\"b4\"]\n}\n```"], "5bbacbad17c44aecc4eb007e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of completing knowledge graphs (KGs) with long-tail relations and dynamic updates. It proposes a novel one-shot learning model that leverages entity embeddings and local graph structures to predict new triples based on a single reference triple. The model uses a permutation-invariant network and a recurrent neural network for multi-step matching, and it demonstrates improved performance on newly constructed datasets.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b33\": 0.9,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b31\": 0.7,\n    \"b38\": 0.7,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.5,\n    \"b19\": 0.4,\n    \"b32\": 0.4,\n    \"b14\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of knowledge graph (KG) completion, particularly focusing on long-tail relations and dynamic, evolving KGs where new relations are continuously added. The proposed model leverages entity embeddings and local graph structures to predict missing triples using a permutation-invariant network and a recurrent neural network. The key contributions include the first consideration of long-tail relations in link prediction, an effective one-shot learning framework for relational data, and the creation of two new datasets for KG completion tasks.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b33\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.9,\n    \"b31\": 0.9,\n    \"b38\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.8,\n    \"b35\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of completing knowledge graphs (KGs) with sparse long-tail relations and dynamically evolving KGs. The proposed algorithm is a one-shot learning framework that leverages entity embeddings and local graph structures to predict new triples based on a learnable matching metric. The contributions include introducing few-shot relational learning, an effective one-shot framework, and newly constructed datasets for evaluation.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b33\": 1,\n    \"b23\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b38\": 0.8,\n    \"b20\": 0.7,\n    \"b14\": 0.7,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b31\": 0.5,\n    \"b25\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incomplete knowledge graphs (KGs) by proposing a one-shot learning framework for link prediction, specifically targeting long-tail and dynamically evolving relations. The model leverages entity embeddings and local graph structures to predict new triples with minimal training data, outperforming traditional embedding-based methods.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b33\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b31\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include handling long-tail relations with limited training data and adapting to dynamic, evolving knowledge graphs. The proposed algorithm focuses on a one-shot learning framework that leverages entity embeddings and local graph structures to predict new triples. The model uses a permutation-invariant network for encoding one-hop neighbors and a recurrent neural network for multi-step matching.\",\n  \"Direct Inspiration\": {\n    \"b40\": 0.95,\n    \"b33\": 0.9,\n    \"b23\": 0.85,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b2\": 0.7,\n    \"b38\": 0.65,\n    \"b36\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.55,\n    \"b31\": 0.5,\n    \"b19\": 0.45,\n    \"b32\": 0.4\n  }\n}\n```"], "5db92a0d47c8f766461ff307": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot link prediction in knowledge graphs, introducing a novel framework called Meta Relational Learning (MetaR). MetaR leverages relation-specific meta information to improve the prediction of new triples based on a few existing ones. Key inspirations include knowledge graph embedding methods and meta-learning techniques.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b12\": 1,\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b16\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed in the paper is the few-shot link prediction in knowledge graphs, where the goal is to predict new triples given only a few existing examples for a specific relation.\",\n    \"algorithm\": \"The authors propose a novel framework called Meta Relational Learning (MetaR), which uses relation-specific meta information (relation meta and gradient meta) to address the few-shot link prediction problem. The framework aims to transfer common and shared knowledge from a few existing instances to incomplete triples, enhancing the learning process and achieving state-of-the-art results.\"\n  },\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b12\": 0.9,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b13\": 0.7,\n    \"b16\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot link prediction in knowledge graphs by proposing a novel framework called Meta Relational Learning (MetaR). The main contributions include the introduction of relation-specific meta information (relation meta and gradient meta) to transfer knowledge and accelerate learning, achieving state-of-the-art results on few-shot link prediction tasks.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b16\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot link prediction in knowledge graphs by proposing a novel framework called Meta Relational Learning (MetaR). It introduces two types of relation-specific meta information: relation meta and gradient meta, which help transfer common relation information and accelerate the learning process. MetaR is evaluated on few-shot link prediction datasets and achieves state-of-the-art results.\",\n  \"Direct Inspiration\": [\"b22\", \"b12\", \"b5\"],\n  \"Indirect Inspiration\": [\"b1\", \"b16\", \"b23\"],\n  \"Other Inspiration\": [\"b3\", \"b18\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of few-shot link prediction in knowledge graphs, where the goal is to predict new triples given only a few examples of a relation. The proposed solution, Meta Relational Learning (MetaR), introduces relation-specific meta information to improve prediction by transferring common relation information and accelerating learning within tasks. The framework includes a relation-meta learner and an embedding learner, achieving state-of-the-art results.\",\n    \"Direct Inspiration\": [\"b22\"],\n    \"Indirect Inspiration\": [\"b1\", \"b5\", \"b12\"],\n    \"Other Inspiration\": [\"b18\", \"b3\", \"b13\", \"b16\", \"b23\"]\n}\n```"], "5e5e191893d709897ce4ff17": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot relation learning in knowledge graphs (KGs), focusing on inferring missing relations with limited reference entity pairs. The authors propose a model called Few-Shot Relation Learning (FSRL), which introduces a relation-aware heterogeneous neighbor encoder and a recurrent autoencoder aggregation network to improve the representation and matching of entity pairs.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b0\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot knowledge graph completion by proposing a Few-Shot Relation Learning model (FSRL). The model aims to effectively infer relations with limited reference entity pairs using a relation-aware heterogeneous neighbor encoder, a recurrent autoencoder aggregation network, and a matching network. The key contributions are the novel heterogeneous neighbor encoding and the aggregation of few-shot reference pairs.\",\n  \"Direct Inspiration\": [\"b10\", \"b4\"],\n  \"Indirect Inspiration\": [\"b0\", \"b8\"],\n  \"Other Inspiration\": [\"b11\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of few-shot learning for knowledge graph completion. It proposes a Few-Shot Relation Learning model (FSRL) that includes a relation-aware heterogeneous neighbor encoder, a recurrent autoencoder aggregation network, and a matching network. The model is aimed at effectively inferring true entity pairs given a few-shot reference set for each relation.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the issue of few-shot relation learning in knowledge graphs (KGs), proposing the Few-Shot Relation Learning model (FSRL) which uses a relation-aware heterogeneous neighbor encoder and a recurrent autoencoder aggregation network to handle limited reference entity pairs per relation. The main challenge addressed is the inability of previous models to deal with relations that have very few entity pairs, and the proposed model aims to improve the representation and interaction of these few-shot reference instances.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of completing large-scale knowledge graphs (KGs) with few-shot reference entity pairs. It introduces a Few-Shot Relation Learning model (FSRL) to infer entity pairs effectively by employing a relation-aware heterogeneous neighbor encoder, a recurrent autoencoder aggregation network, and a matching network.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6\n  }\n}\n```"], "5550414745ce0a409eb39ec8": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of identifying representative big data analytics applications and understanding their performance characteristics on modern processors. It proposes a pragmatic experiment approach to characterize big data analytics workloads, focusing on the micro-architecture level and the impact of modern big data software stacks.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b39\": 0.9,\n    \"b17\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.7,\n    \"b23\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.6,\n    \"b18\": 0.65,\n    \"b11\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of characterizing big data analytics workloads in data centers to identify bottlenecks and optimization points. It also aims to understand the performance characteristics of these workloads on modern processors, comparing them with traditional workloads and analyzing the impact of modern big data software stacks.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b2\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b18\": 0.8,\n    \"b11\": 0.7,\n    \"b10\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.6,\n    \"b12\": 0.6,\n    \"b38\": 0.6,\n    \"b8\": 0.6,\n    \"b44\": 0.6,\n    \"b40\": 0.6,\n    \"b35\": 0.6,\n    \"b21\": 0.6,\n    \"b23\": 0.6,\n    \"b39\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of characterizing big data analytics workloads and identifying optimization points for improving data analytics systems' performance. It primarily focuses on the comparison of big data analytics workloads with traditional workloads, correlation analysis of CPI performance with micro-architecture metrics, and the impact of modern big data software stacks on application behaviors.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b39\": 0.7,\n    \"b17\": 0.7,\n    \"b31\": 0.6,\n    \"b45\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of characterizing big data analytics workloads to improve the performance of data center systems. It identifies the bottlenecks and optimization points and investigates the impacts of a typical big data software stack on application behaviors.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.8,\n    \"b31\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b17\": 0.6,\n    \"b26\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in identifying representative big data analytics workloads, characterizing their performance on modern processors, and optimizing system performance by understanding bottlenecks and the impacts of modern big data software stacks. It proposes experiments with larger data sets to analyze performance metrics and correlations, emphasizing the importance of cache and TLB optimizations.\",\n    \"Direct Inspiration\": {\n        \"b16\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b30\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.7,\n        \"b39\": 0.7,\n        \"b17\": 0.7,\n        \"b29\": 0.6,\n        \"b26\": 0.6,\n        \"b20\": 0.6\n    }\n}\n```"], "5eabf3cd91e011664efc496f": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the limited performance and high energy consumption of Out-of-Order (OoO) cores due to complex scheduling logic and power-hungry structures. The paper proposes a novel approach called SpecInO, which enhances In-Order (InO) core performance by speculatively issuing ready-to-execute instructions from a cascaded InO IQ, reducing stalls caused by long-latency operations. The CASINO core microarchitecture is introduced to implement this approach, achieving near-OoO performance with higher energy efficiency.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the energy inefficiency and complexity of Out-of-Order (OoO) cores by proposing a new microarchitecture, CASINO, which speculatively issues instructions in an In-Order (InO) pipeline to achieve OoO-like performance gains. The main challenges include the high power consumption of conventional OoO cores and the limited performance of InO cores. The CASINO core utilizes a SpecInO scheduling mechanism and introduces cost-effective register renaming and memory disambiguation techniques.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b14\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing the performance of in-order (InO) cores while maintaining energy efficiency. The proposed solution involves the CASINO core microarchitecture, which uses speculative in-order (SpecInO) scheduling. This approach dynamically generates out-of-order (OoO) issue schedules optimized for the underlying InO pipeline by examining dynamic instructions and issuing ready-to-execute instructions speculatively. Key contributions include novel register renaming and memory disambiguation schemes to support this speculative scheduling.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b5\": 0.7,\n    \"b8\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of energy-efficient and high-performance out-of-order (OoO) execution in processors. It introduces the SpecInO scheduling mechanism, which combines in-order (InO) and speculative out-of-order scheduling to dynamically generate OoO schedules in a cost-effective manner. The proposed CASINO core microarchitecture implements this approach to achieve near-OoO performance while maintaining energy efficiency.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b18\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b14\": 0.8,\n    \"b15\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the energy efficiency and performance of in-order (InO) cores by enabling out-of-order (OoO) execution capabilities. The proposed approach, called SpecInO, involves speculatively issuing ready-to-execute instructions by examining dynamic instructions in program order, which prevents stalling on long-latency operations and exposes both instruction-level parallelism (ILP) and memory-level parallelism (MLP). The CASINO core microarchitecture implements this concept with a speculative instruction queue (S-IQ) and a normal IQ, alongside optimized register renaming and memory disambiguation schemes.\",\n  \n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b8\": 0.85,\n    \"b14\": 0.8,\n    \"b17\": 0.95,\n    \"b18\": 0.9,\n    \"b24\": 0.85\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b15\": 0.75,\n    \"b26\": 0.7,\n    \"b27\": 0.7\n  },\n  \n  \"Other Inspiration\": {\n    \"b0\": 0.65,\n    \"b4\": 0.65,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b19\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b25\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6,\n    \"b36\": 0.6,\n    \"b37\": 0.6\n  }\n}\n```"], "5fef22c691e0113b265a0289": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of compressing large pretrained Transformer models to make them more suitable for fine-tuning and online serving by proposing a task-agnostic knowledge distillation method. It introduces a novel approach named multi-head self-attention relation distillation, which generalizes and simplifies the deep self-attention distillation used in MINILM. This method eliminates the restriction on the number of attention heads in the student model and uses more fine-grained self-attention knowledge for improved performance.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b30\": 0.7,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b28\": 0.5,\n    \"b33\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of fine-tuning and online serving of large pretrained Transformers due to computational resource constraints and latency. It introduces a novel method for task-agnostic compression of pretrained Transformers using multi-head self-attention relation distillation. This method eliminates the restriction on the number of attention heads in student models and transfers more fine-grained self-attention knowledge from teacher to student models. The approach is validated through extensive experiments, showing improved performance over state-of-the-art models.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.8,\n    \"b14\": 0.8,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b28\": 0.7,\n    \"b37\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is compressing large pretrained Transformers for natural language processing tasks, which are computationally expensive to fine-tune and serve online. The authors propose a novel method of task-agnostic knowledge distillation using multi-head self-attention relations to train a student model that mimics the teacher's self-attention module without being restricted to the same number of attention heads. Additionally, they explore the effectiveness of transferring self-attention knowledge from different layers of large-size teacher models.\",\n    \"Direct Inspiration\": {\n        \"b40\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b30\": 0.8,\n        \"b14\": 0.8,\n        \"b34\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.7,\n        \"b28\": 0.7,\n        \"b36\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge discussed in the paper is the compression of large pretrained Transformers to make them more efficient for fine-tuning and online serving, given the computational constraints. The paper proposes a method to generalize and simplify deep self-attention distillation by introducing multi-head self-attention relations. This method allows for the alignment of queries, keys, and values of different attention heads between teacher and student models, thereby eliminating the restriction on the number of attention heads in the student models and improving performance.\",\n  \"Direct Inspiration\": {\n    \"b40\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.9,\n    \"b34\": 0.9,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b28\": 0.7,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the compression of large pretrained Transformer models to reduce computational resources and latency while maintaining performance. The authors propose a method called multi-head self-attention relation distillation, which generalizes and simplifies deep self-attention distillation by using multi-head self-attention relations computed by scaled dot-product of pairs of queries, keys, and values. This method eliminates the restriction on the number of attention heads in student models and improves performance by transferring more fine-grained self-attention knowledge.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.95,\n    \"b30\": 0.9,\n    \"b34\": 0.9,\n    \"b40\": 0.98\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b28\": 0.8,\n    \"b37\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b9\": 0.7,\n    \"b19\": 0.7,\n    \"b25\": 0.7,\n    \"b21\": 0.7\n  }\n}\n```"], "5f7fdd328de39f0828397fae": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the accurate forecasting of multivariate time-series data by jointly modeling intra-series temporal patterns and inter-series correlations. The proposed algorithm, StemGNN, combines the advantages of Graph Fourier Transform (GFT) and Discrete Fourier Transform (DFT) to operate in the spectral domain, improving the effectiveness of both temporal and structural dependencies.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b16\": 0.85,\n    \"b30\": 0.8,\n    \"b28\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.7,\n    \"b22\": 0.65,\n    \"b2\": 0.6,\n    \"b18\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b24\": 0.45\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of multivariate time-series forecasting by proposing the StemGNN model, which jointly models intra-series temporal patterns and inter-series correlations in the spectral domain using GFT and DFT. The method avoids the need for pre-defined topologies and demonstrates state-of-the-art performance across various datasets.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b12\": 0.9,\n        \"b22\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.7,\n        \"b30\": 0.7,\n        \"b28\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b31\": 0.5,\n        \"b18\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurate multivariate time-series forecasting by proposing a novel model called StemGNN, which jointly models intra-series temporal patterns and inter-series correlations in the spectral domain using Graph Fourier Transform (GFT) and Discrete Fourier Transform (DFT).\",\n  \"Direct Inspiration\": {\n    \"b30\": 0.9,\n    \"b16\": 0.85,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.75,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.65,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the accurate forecasting of multivariate time-series by jointly modeling intra-series temporal patterns and inter-series correlations. The proposed algorithm, StemGNN, combines the advantages of Graph Fourier Transform (GFT) and Discrete Fourier Transform (DFT) to model multivariate time-series data entirely in the spectral domain, making the spectral representations easier to be recognized by convolution and sequential modeling layers.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b31\": 0.9,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b30\": 0.75,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.65,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving multivariate time-series forecasting by jointly modeling intra-series temporal patterns and inter-series correlations in the spectral domain. The proposed method, StemGNN, leverages Graph Fourier Transform (GFT) and Discrete Fourier Transform (DFT) to achieve this goal, introducing a novel approach that does not require pre-defined topology and learns latent correlations automatically.\",\n  \"Direct Inspiration\": [\"b31\", \"b22\", \"b12\", \"b30\", \"b16\"],\n  \"Indirect Inspiration\": [\"b2\", \"b18\", \"b24\", \"b28\"],\n  \"Other Inspiration\": [\"b9\", \"b5\", \"b6\"]\n}\n```"], "5e54f1813a55acae32a25da5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of author name disambiguation (AND) in digital libraries where similar or identical names can lead to ambiguity. The proposed solution combines content information and relation information using a unified framework inspired by generative adversarial networks (GANs). This framework consists of a discriminative module that distinguishes whether two papers are written by the same author and a generative module that selects papers viewed as homogeneous pairs. The method integrates content features using Doc2Vec and relation features using Node2Vec, and it employs a novel adversarial approach to improve the accuracy of the AND task without requiring extensive labeled data or feature engineering.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b4\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of author name disambiguation by proposing a unified framework that integrates both content and relation information through a generative adversarial network approach. The key contributions include the construction of a heterogeneous information network, a combined discriminative and generative module, and the creation of a large benchmark dataset.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b12\": 0.8,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b13\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of author name disambiguation in academic digital libraries. The proposed algorithm combines content information and relation information using a generative adversarial network (GAN) framework with a discriminative module and a generative module. This unified framework aims to eliminate the need for labeled samples and complicated feature engineering while effectively handling high-order connections among papers.\",\n  \"Direct Inspiration\": [\"b1\", \"b9\"],\n  \"Indirect Inspiration\": [\"b3\", \"b4\", \"b12\"],\n  \"Other Inspiration\": [\"b2\", \"b5\", \"b13\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of author name disambiguation (AND) in academic digital libraries, where similar names cause confusion in identifying authors. The proposed solution combines content and relational information within a unified framework using a generative adversarial network (GAN). This model includes a discriminative module that learns to distinguish whether two papers are authored by the same individual and a generative module that selects likely paper pairs. The framework leverages high-order connections and eliminates the need for complex feature engineering or labeled samples, demonstrating superior performance on benchmark datasets.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of author name disambiguation in digital libraries, which is crucial for accurate information retrieval and scholarly communication. It proposes a novel unified framework combining a discriminative module and a generative module to integrate both content and relation information effectively. Inspired by generative adversarial networks, the model self-trains to distinguish papers by the same author without requiring labeled samples or complicated feature engineering. Experimental results on benchmark datasets demonstrate the superiority of this approach over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.9,\n    \"b2\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.75,\n    \"b5\": 0.7,\n    \"b12\": 0.65\n  }\n}\n```"], "53e999cab7602d970220f327": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of creating an effective branch predictor, specifically the cbp1.5, which is derived from the GPPM family of predictors. The primary focus is on understanding the performance trade-offs and degradations from an ideal GPPM predictor to a practical implementation. Key inspirations include previous work on PPM for text compression and branch prediction, as well as techniques from other branch predictors like YAGS and gshare.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1,\n        \"b2\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.8,\n        \"b3\": 0.8,\n        \"b4\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b6\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper outlines the challenges in branch prediction and proposes the cbp1.5 predictor, a variant of the GPPM family. It describes how cbp1.5 operates, the degradation path from GPPM-ideal to cbp1.5, and methods used for improving prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 0.9,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper outlines the challenges of designing an effective branch predictor and introduces the cbp1.5 predictor, which is a global-history based predictor derived from PPM. The key contributions include the structure and operation of the cbp1.5 predictor, the degradation path from the ideal GPPM predictor to cbp1.5, and various optimizations such as the use of 3-bit counters and history folding mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 0.9,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b6\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper describes the cbp1.5 predictor, derived from the GPPM (global-history PPM-like) predictor family, and outlines its design, operation, and performance. The primary challenge addressed is enhancing branch prediction accuracy under real-life constraints, which is achieved by progressively degrading the ideal GPPM predictor to meet practical limitations.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.9,\n    \"b2\": 0.8,\n    \"b3\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces the cbp1.5 predictor, a global-history PPM-like branch predictor, and compares it with an ideal GPPM predictor called GPPM-ideal. The main challenges addressed include improving prediction accuracy by utilizing global history and managing real-life constraints leading to 'degradations' from the ideal model.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1, \n        \"b2\": 0.9,\n        \"b3\": 0.8 \n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.7,\n        \"b5\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.5\n    }\n}\n```"], "5ce3aebeced107d4c65ebaaf": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating diverse, high-quality image samples from a single natural image without relying on a large dataset. The proposed algorithm, SinGAN, leverages the internal statistics of patches within a single image, using a multi-scale pyramid of fully convolutional GANs to capture both fine details and global structures. The method allows for various image manipulation tasks and produces high-quality results that preserve the internal patch statistics of the training image.\",\n  \"Direct Inspiration\": {\n    \"b47\": 1.0,\n    \"b43\": 0.9,\n    \"b44\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.8,\n    \"b26\": 0.7,\n    \"b2\": 0.7,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b61\": 0.4,\n    \"b11\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces SinGAN, a novel single-image generative adversarial network (GAN) model that leverages the internal statistics of patches within a single natural image to generate high-quality, diverse image samples. This method addresses the challenge of generating realistic images without relying on large datasets by using a multi-scale architecture of fully convolutional GANs.\",\n  \"Direct Inspiration\": {\n    \"b47\": 1.0,\n    \"b43\": 0.9,\n    \"b44\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b30\": 0.8,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the difficulty of capturing the distribution of highly diverse datasets with multiple object classes using GANs. The proposed SinGAN model tackles this by learning an unconditional generative model from a single natural image that can produce diverse high-quality samples. This is achieved through a pyramid of fully convolutional light-weight GANs, each capturing the distribution of patches at different scales.\",\n  \"Direct Inspiration\": {\n    \"b47\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.9,\n    \"b44\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.8,\n    \"b30\": 0.7,\n    \"b25\": 0.7,\n    \"b19\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces SinGAN, a generative adversarial network (GAN) that can learn to generate diverse high-quality image samples from a single natural image. The primary challenges addressed include capturing the internal statistics of patches within a single image at multiple scales and generating realistic, complex images without relying on a large dataset. The novel approach involves a pyramid of fully convolutional light-weight GANs, each responsible for different scales of image patches.\",\n  \"Direct Inspiration\": [\"b47\", \"b43\", \"b44\"],\n  \"Indirect Inspiration\": [\"b18\", \"b30\", \"b25\"],\n  \"Other Inspiration\": [\"b19\", \"b34\", \"b58\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is generating high-quality, diverse image samples from a single natural image without relying on a database of class-specific images. The proposed method, SinGAN, leverages the internal statistics of patches within a single image to perform various image manipulation tasks through a multi-scale generative model consisting of a pyramid of fully convolutional GANs.\",\n  \"Direct Inspiration\": {\n    \"b47\": 0.9,\n    \"b43\": 0.85,\n    \"b44\": 0.85,\n    \"b30\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b26\": 0.7,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b61\": 0.6,\n    \"b21\": 0.6,\n    \"b56\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```"], "53e9bb2fb7602d970476a406": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing high-performance cache architectures for modern processors, focusing on issues like mapping, searching, and movement of cache lines. The authors propose novel cache designs including S-NUCA-1, S-NUCA-2, and D-NUCA to optimize performance by reducing access times and improving data locality.\",\n  \"Direct Inspiration\": [\"b0\", \"b13\"],\n  \"Indirect Inspiration\": [\"b26\", \"b29\"],\n  \"Other Inspiration\": [\"b5\", \"b12\", \"b17\", \"b32\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper involve optimizing large multi-level cache hierarchies in high-performance processors, focusing on the mapping, searching, and movement of cache lines. The paper proposes and evaluates different cache architectures, including Uniform Cache Architecture (UCA), Static Non-Uniform Cache Architecture (S-NUCA-1 and S-NUCA-2), and Dynamic Non-Uniform Cache Architecture (D-NUCA), to improve performance. The novel contributions include the introduction of S-NUCA-2 using a two-dimensional switched network and D-NUCA with dynamic data migration to faster banks.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b5\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are related to optimizing the performance of large, multi-level caches in high-performance processors. The paper proposes several non-uniform cache architectures (NUCA) that address issues of bank mapping, search strategies, and data movement within the cache to improve performance. The paper introduces static NUCA designs (S-NUCA-1 and S-NUCA-2) and a dynamic NUCA design (D-NUCA) which dynamically migrates data to faster banks based on usage frequency.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b32\": 1.0,\n    \"b26\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b9\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b29\": 0.7,\n    \"b20\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include determining the optimal number of addressable banks in future caches, the strategy for searching potential locations for a line, and how lines should be moved within the cache. The paper proposes various static and dynamic non-uniform cache architectures (S-NUCA-1, S-NUCA-2, D-NUCA) to address these challenges, focusing on improving performance by reducing access times and managing data more effectively within the cache hierarchy.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b15\": 0.85,\n    \"b17\": 0.75,\n    \"b18\": 0.7,\n    \"b25\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b7\": 0.4,\n    \"b9\": 0.4,\n    \"b11\": 0.5,\n    \"b12\": 0.4,\n    \"b20\": 0.5,\n    \"b26\": 0.6,\n    \"b29\": 0.4,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"How to addressable banks should future caches contain, and how should lines be mapped into those banks?\",\n      \"What is the right strategy for searching the set of possible locations for a line?\",\n      \"Should a line always be placed in the same bank? If not, how is a line moved, either while resident in the cache or across different lifetimes in the cache?\"\n    ],\n    \"inspirations\": [\n      \"The paper explores different types of organizations for large level-two (L2) caches.\",\n      \"It proposes new static and dynamic non-uniform cache architectures (S-NUCA and D-NUCA) to improve performance in wire-delay-dominated technologies.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b5\": 0.9,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b18\": 0.7,\n    \"b25\": 0.7\n  }\n}\n```"], "53e9bc6eb7602d97048eba78": ["```json\n{\n  \"Summary\": \"The paper addresses the performance disparity between microprocessor and memory speed. It proposes a dedicated prefetching cache structure called Data Access History Cache (DAHC) to improve data prefetching accuracy, coverage, and aggressiveness. The DAHC tracks recent data access histories and maintains correlations to support various prefetching algorithms, including stride prefetching, Markov prefetching, and aggressive strategies like the Multi-Level Difference Table (MLDT) algorithm.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of the expanding performance disparity between processors and memory by proposing a dedicated prefetching cache structure named Data Access History Cache (DAHC). The DAHC tracks recent data access histories and maintains correlations to improve prefetching strategies. The paper introduces methodologies for stride prefetching, Markov prefetching, and aggressive prefetching strategies using the DAHC.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b9\": 0.85,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b3\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of the growing performance gap between processor and memory speeds. It proposes a novel data prefetching cache structure called Data Access History Cache (DAHC) to improve data access latency by tracking data access histories and maintaining correlations from different perspectives. The DAHC supports various prefetching strategies, including stride prefetching, Markov prefetching, and aggressive strategies like the Multi-Level Difference Table (MLDT).\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of the widening performance gap between processors and memory speeds, proposing a dedicated prefetching cache structure named Data Access History Cache (DAHC). This structure aims to enhance data prefetching mechanisms by tracking recent data access histories and maintaining correlations from different perspectives to improve prefetching accuracy, coverage, and aggressiveness.\",\n  \"Direct Inspiration\": [\"b2\", \"b9\"],\n  \"Indirect Inspiration\": [\"b8\", \"b20\"],\n  \"Other Inspiration\": [\"b5\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of the increasing performance gap between microprocessors and memory by proposing a dedicated prefetching cache structure, named Data Access History Cache (DAHC). The DAHC aims to enhance data prefetching accuracy, coverage, and aggressiveness by tracking recent data access histories and maintaining correlations from different perspectives. The paper discusses the design and methodologies of DAHC and demonstrates its application in various prefetching strategies including stride prefetching, Markov prefetching, and aggressive prefetching strategies.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b9\": 0.85,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b5\": 0.75\n  }\n}\n```"], "53e99822b7602d9702041bcd": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving memory system performance in high-end servers, particularly for commercial applications with complex memory access patterns. It proposes Spatial Memory Streaming (SMS) as a novel method to predict and stream spatially-correlated memory access patterns to enhance memory-level parallelism (MLP) and reduce memory latency.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b16\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b23\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b17\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of poor memory system performance in commercial server applications due to memory system-related stalls. It proposes the Spatial Memory Streaming (SMS) algorithm to improve memory-level parallelism (MLP) and hide latency by predicting and streaming spatially-correlated access patterns into the primary cache ahead of demand misses.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b16\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.75,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.65,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of poor memory system behavior in commercial applications, particularly in high-end servers, which significantly affects performance. The proposed algorithm, Spatial Memory Streaming (SMS), aims to predict and stream spatially-correlated access patterns to improve memory-level parallelism (MLP) and hide latency in memory accesses. This approach is targeted at reducing primary cache and off-chip misses in multiprocessor servers by exploiting repetitive access patterns.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9,\n        \"b16\": 0.9,\n        \"b19\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b20\": 0.7,\n        \"b29\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b17\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of poor memory system behavior in commercial applications due to memory stalls. It proposes the Spatial Memory Streaming (SMS) algorithm to improve memory-level parallelism (MLP) and hide memory latency by predicting and streaming spatially-correlated access patterns.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b20\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving memory system performance in commercial server applications by proposing the Spatial Memory Streaming (SMS) algorithm, which predicts and streams spatially-correlated data blocks to enhance memory-level parallelism and hide memory latency. The key innovations include effective spatial correlation and prediction, accurate tracking of spatial correlation, and significant performance enhancement over existing methods.\",\n  \"Direct Inspiration\": {\n    \"high_score_references\": [\"b3\", \"b16\"],\n    \"medium_score_references\": []\n  },\n  \"Indirect Inspiration\": {\n    \"high_score_references\": [],\n    \"medium_score_references\": [\"b19\", \"b20\", \"b29\"]\n  },\n  \"Other Inspiration\": {\n    \"high_score_references\": [\"b5\"],\n    \"medium_score_references\": []\n  }\n}\n```"], "5e5e19c093d709897ce87ab9": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of automatically generating high-performance schedules for Halide programs, which are crucial for efficient image processing pipelines. The proposed algorithm leverages function bounds analysis and interval-based techniques to perform program transformations similar to those used by expert developers, without requiring costly auto-tuning.\",\n    \"Direct Inspiration\": {\n        \"b11\": 1,\n        \"b12\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b7\": 0.7,\n        \"b8\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the difficulty of designing schedules that perform well on modern hardware for Halide programs, which requires expertise in optimization techniques and hardware architectures. The proposed algorithm automatically generates high-performance schedules for Halide programs by leveraging function bounds analysis and performing locality-enhancing global program transformations, without the need for auto-tuning.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b7\": 0.7,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.9\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of automatically generating high-performance schedules for Halide programs. It proposes an algorithm leveraging Halide's function bounds analysis to perform locality enhancing global program transformations without requiring costly auto-tuning. The approach aims to simplify the laborious process of manually creating efficient schedules for complex image processing pipelines.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b2\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.95\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the difficulty of designing efficient schedules for Halide programs, which require expertise in optimization techniques and hardware architectures. The proposed algorithm automatically generates high-performance schedules for Halide programs by leveraging function bounds analysis to perform locality-enhancing global program transformations.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9, \n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b7\": 0.65,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generating high-performance schedules for Halide programs, which is currently a laborious and expertise-dependent process. The proposed algorithm leverages function bounds analysis to automatically perform locality enhancing global program transformations without the need for auto-tuning, achieving competitive performance with expert-created schedules.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b11\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b2\", \"b7\", \"b8\", \"b12\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": []\n  }\n}\n```"], "53e99845b7602d9702071dfc": ["```json\n{\n    \"Summary\": \"The primary challenges outlined in the paper are the scalability and efficiency issues in the wakeup and picker sides of the scheduler due to process constraints and the large number of entries. The proposed algorithm introduces modifications to the wakeup matrix by subscribing columns on-demand and an indirection technique for the picker side to reduce matrix size, thereby improving performance and reducing delay.\",\n    \"Direct Inspiration\": {\n        \"b14\": 1.0,\n        \"b5\": 0.9,\n        \"b8\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.7,\n        \"b16\": 0.7,\n        \"b20\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b25\": 0.6,\n        \"b28\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses scalability issues in the wakeup and picker logic of traditional schedulers in modern processors. The authors propose modifications to wakeup matrices and introduce an indirection technique for picker logic to support larger instruction windows and reduce critical path latency.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.95,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.85,\n    \"b22\": 0.85,\n    \"b9\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.75,\n    \"b3\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalability in the wakeup and picker logic of schedulers in modern processors. It proposes a new design for wakeup matrices that conservatively allocates columns based on demand, and an indirection technique for picker matrices to reduce complexity and size, thereby improving performance and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b17\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b5\": 0.8,\n    \"b8\": 0.7,\n    \"b21\": 0.6,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b7\": 0.5,\n    \"b16\": 0.4,\n    \"b23\": 0.5,\n    \"b25\": 0.4,\n    \"b27\": 0.4,\n    \"b30\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalability and efficiency in the wakeup and picker sides of schedulers in modern processors. The key innovations include a modified wakeup matrix that conservatively allocates columns based on demand and an indirection technique for the picker side to reduce the matrix size and complexity.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b2\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the scalability and efficiency of the scheduler in modern processors, specifically addressing the wakeup and picker sides. The novel methods introduced include a subscription-based wakeup matrix and an indirection technique for the picker to reduce complexity and improve performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b17\": 0.9,\n    \"b2\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b7\": 0.75,\n    \"b16\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b8\": 0.7,\n    \"b22\": 0.65\n  }\n}\n```"], "5ee355c49fced0a24b90bd2f": ["```json\n{\n  \"Summary\": \"The primary challenge of the paper is to develop a fast and accurate efficiency estimation framework for DNN accelerators considering the high-dimensional design space involving DNN layers, hardware resources, and mapping strategies. The proposed solution, MAESTRO, is a data-centric analytical cost-benefit analysis framework that leverages systematic data reuse analysis to provide precise and fast estimations of various performance metrics.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of efficiently estimating the performance and energy efficiency of DNN accelerators. It introduces MAESTRO, a cost-benefit analysis framework that uses a data-centric representation of mapping to provide precise descriptions and analyses of data reuse in DNN accelerators.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.95,\n    \"b5\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently modeling the high-dimensional design space of DNN accelerators, focusing on data reuse patterns to improve latency and energy efficiency. It proposes MAESTRO, a cost-benefit analysis framework based on a data-centric representation of mappings, which allows for precise descriptions of data reuse opportunities in DNN accelerators.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b1\": 0.6,\n    \"b2\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges of efficiently estimating the performance and energy efficiency of DNN accelerators. It introduces an analytical cost model, MAESTRO, which considers the interaction of DNN layers, hardware, and mapping strategies. MAESTRO provides fast and accurate cost-benefit analysis by leveraging a data-centric representation of mappings, enabling comprehensive performance statistics and facilitating hardware design space exploration.\",\n  \"Direct Inspiration\": [\"b4\", \"b5\"],\n  \"Indirect Inspiration\": [\"b10\", \"b11\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of efficiently estimating the performance and energy efficiency of DNN accelerators, focusing on the interaction of DNN layers, hardware resources, and mapping strategies. The authors propose MAESTRO, a data-centric analytical framework to model data reuse patterns and conduct cost-benefit analysis, enabling rapid and precise efficiency estimations.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.7\n  }\n}\n```"], "5cf48a30da56291d58292a2f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating photorealistic personalized talking head models with minimal effort, using few-shot learning from a handful of photographs. The proposed method leverages deep convolutional networks, adversarial training, and meta-learning to achieve high-quality results.\",\n  \"Direct Inspiration\": [\n    \"b14\",\n    \"b18\",\n    \"b35\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b11\",\n    \"b21\",\n    \"b30\",\n    \"b13\",\n    \"b9\",\n    \"b41\",\n    \"b39\",\n    \"b17\"\n  ],\n  \"Other Inspiration\": [\n    \"b38\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating personalized photorealistic talking head models with a few-shot learning approach. It proposes a deep convolutional network-based system that can generate realistic head images from a small set of face landmarks and photographs. The system uses extensive pre-training (meta-learning) on a large corpus of talking head videos, followed by adversarial fine-tuning to achieve high-quality personalized video synthesis.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b18\": 0.9,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b27\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b21\": 0.6,\n    \"b30\": 0.6,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper include the high photometric, geometric, and kinematic complexity of human heads, and the sensitivity of human perception to minor mistakes in appearance modeling. The proposed algorithm is a meta-learning-based few-shot learning system that can create personalized photorealistic talking heads from a handful of photographs. The system employs deep convolutional networks trained with adversarial learning to synthesize video frames directly without warping, leveraging pre-trained networks to enable rapid fine-tuning for new individuals.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b18\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b39\": 0.7,\n    \"b41\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b13\": 0.5,\n    \"b21\": 0.4,\n    \"b30\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating personalized photorealistic talking head models with minimal input data and training time. The proposed system employs adversarially-trained deep convolutional networks and meta-learning to achieve few-shot and even one-shot learning capabilities for generating realistic talking head sequences.\",\n  \"Direct Inspiration\": [\"b14\", \"b18\", \"b35\"],\n  \"Indirect Inspiration\": [\"b10\", \"b27\", \"b38\"],\n  \"Other Inspiration\": [\"b11\", \"b13\", \"b21\", \"b30\", \"b9\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of creating personalized, photorealistic talking head models from a minimal set of photographs, using a meta-learning framework that incorporates adversarial training. Key inspirations include works on generative adversarial networks (GANs), meta-learning techniques, and few-shot learning methods.\",\n  \"Direct Inspiration\": [\"b14\", \"b18\", \"b35\", \"b21\", \"b30\"],\n  \"Indirect Inspiration\": [\"b11\", \"b13\", \"b9\", \"b39\", \"b41\"],\n  \"Other Inspiration\": [\"b38\", \"b36\", \"b20\", \"b23\"]\n}\n```"], "5a260c0917c44a4ba8a1e00e": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of visual recognition in long-tailed distributions, where head classes have abundant data and tail classes have very few examples. The proposed solution is MetaModelNet, a meta-level network that transfers knowledge from head to tail classes through a series of recursive residual transformations. Key inspirations include head-to-tail knowledge transfer and meta-learning techniques.\",\n    \"Direct Inspiration\": [\"b21\"],\n    \"Indirect Inspiration\": [\"b10\", \"b12\"],\n    \"Other Inspiration\": [\"b53\", \"b4\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of long-tail visual recognition by proposing a novel meta-learning approach called MetaModelNet. This approach transfers meta-knowledge from data-rich head classes to data-poor tail classes, using a continuous strategy of chaining multiple regressors with residual connections. The goal is to predict model parameters for varying sample sizes, effectively capturing the model dynamics from few-shot to many-shot learning.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b53\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b12\": 0.8,\n    \"b54\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b13\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of long-tail visual recognition by proposing a method to transfer meta-knowledge from data-rich head classes to data-poor tail classes using a recursive residual network called MetaModelNet. This network is inspired by recent work on meta-learning, particularly focusing on learning model parameter transformations that allow effective few-shot to many-shot training parameter regression.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b10\": 0.9,\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b53\": 0.8,\n    \"b54\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b36\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the long-tailed distribution in real-world visual recognition tasks, where the number of training examples per class varies significantly. The paper proposes a method called MetaModelNet, which transfers meta-level knowledge from data-rich head classes to data-poor tail classes using a recursive residual network strategy inspired by meta-learning techniques.\",\n  \"Direct Inspiration\": [\"b21\", \"b53\"],\n  \"Indirect Inspiration\": [\"b10\", \"b12\", \"b54\"],\n  \"Other Inspiration\": [\"b18\", \"b22\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of long-tailed visual recognition by proposing MetaModelNet, a meta-learning approach that transfers meta-level knowledge from head classes to tail classes. This method utilizes a recursive residual network to predict how model parameters will change with increasing training data, effectively capturing the dynamics of transferring meta-knowledge from data-rich to data-poor regimes.\",\n    \"Direct Inspiration\": {\n        \"b21\": 1.0,\n        \"b53\": 0.95\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.85,\n        \"b22\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.7,\n        \"b12\": 0.65,\n        \"b4\": 0.6\n    }\n}\n```"], "5dbebb7447c8f766462c22a6": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges outlined in the paper include the limitation of CNNs to handle graph-structured data and the necessity to capture higher-order structures in graphs for tasks such as semi-supervised node classification.\",\n    \"inspirations\": \"The paper introduces a general class of graph convolution networks utilizing weighted multi-hop motif adjacency matrices to capture higher-order neighborhoods in graphs. A novel attention mechanism is proposed to allow each node to select the most relevant motif-induced neighborhood to integrate information from.\"\n  },\n  \"Direct Inspiration\": {\n    \"b19\": 1,\n    \"b41\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b45\": 0.8,\n    \"b27\": 0.7,\n    \"b35\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b25\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of capturing higher-order structures in graphs for node classification tasks. The authors introduce a novel model called Motif Convolutional Networks (MCN), which utilizes weighted motif-induced adjacencies and an attention mechanism to select the best neighborhood for each node. The approach is demonstrated to outperform existing methods, particularly on graphs that do not exhibit homophily.\",\n  \"Direct Inspiration\": [\"b19\", \"b32\", \"b41\"],\n  \"Indirect Inspiration\": [\"b27\", \"b33\", \"b45\"],\n  \"Other Inspiration\": [\"b35\"]\n}\n```", "```json\n{\n    \"Summary\": \"The main challenges addressed in this paper are the limitations of Convolutional Neural Networks (CNNs) in handling graph-structured data and the need for higher-order structures in Graph Convolutional Networks (GCNs). The proposed solution is a general class of graph convolution networks called Motif Convolutional Networks (MCNs) that utilize weighted multi-hop motif adjacency matrices to capture higher-order neighborhoods in graphs. The MCNs also include a novel attention mechanism to select the most relevant motif-induced neighborhood for each node, trained using reinforcement learning.\",\n    \"Direct Inspiration\": {\n        \"references\": [\"b19\", \"b32\", \"b45\", \"b41\"],\n        \"confidence_scores\": {\n            \"b19\": 1,\n            \"b32\": 1,\n            \"b45\": 0.9,\n            \"b41\": 0.9\n        }\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b27\", \"b33\", \"b35\"],\n        \"confidence_scores\": {\n            \"b27\": 0.8,\n            \"b33\": 0.8,\n            \"b35\": 0.7\n        }\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b24\", \"b31\", \"b29\"],\n        \"confidence_scores\": {\n            \"b24\": 0.6,\n            \"b31\": 0.5,\n            \"b29\": 0.5\n        }\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing Graph Convolutional Networks (GCNs) in capturing higher-order structures in graphs. It introduces Motif Convolutional Networks (MCN) that utilize weighted motif-induced adjacency matrices and a novel attention mechanism to select the most relevant neighborhood for each node. The main contributions include the generalization of GCNs through higher-order structures, the introduction of an attention mechanism, and the demonstration of the method's superiority in performance on various graph-based tasks.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b19\", \"b32\", \"b41\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b27\", \"b33\", \"b45\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b24\", \"b35\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the limitation of traditional Convolutional Neural Networks (CNNs) when applied to graph-structured data. The proposed solution, Motif Convolutional Networks (MCN), introduces a novel attention mechanism that allows nodes in a graph to select the most relevant motif-induced neighborhoods to integrate information from. This approach generalizes existing Graph Convolutional Networks (GCNs) by incorporating higher-order structures and motifs, thus enhancing performance on tasks such as node classification.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b41\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b27\": 0.7,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b35\": 0.5,\n    \"b33\": 0.5\n  }\n}\n```"], "5f8eab4c91e01153024c4ba2": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of explainability and interpretability in deep learning models, particularly in the context of healthcare and education. It proposes the infusion of knowledge graphs (KG) into deep learning models to enhance their ability to provide human-understandable explanations and to interpret the model's internal mechanisms.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1,\n        \"b3\": 1,\n        \"b10\": 1,\n        \"b15\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.8,\n        \"b17\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b4\": 0.6,\n        \"b7\": 0.6,\n        \"b6\": 0.6,\n        \"b8\": 0.6,\n        \"b9\": 0.6,\n        \"b11\": 0.6,\n        \"b13\": 0.6,\n        \"b14\": 0.6,\n        \"b16\": 0.6,\n        \"b18\": 0.6,\n        \"b19\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the need for explainability and interpretability in Deep Learning (DL) models, particularly in healthcare and education domains. The authors propose integrating knowledge graphs (KG) into DL models (termed knowledge infusion) to achieve better interpretability and explainability. This approach aims to make model behavior more transparent and decisions more understandable by leveraging domain-specific knowledge.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b3\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b11\": 0.7,\n    \"b15\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b9\": 0.55,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of explainability and interpretability in deep learning (DL) models, particularly in the context of healthcare and education. It highlights the limitations of current DL models in explaining their decisions, especially when trained with limited labeled data. The proposed solution involves the infusion of knowledge graphs (KG) to enhance the interpretability and explainability of these models.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b3\": 0.85,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.65,\n    \"b7\": 0.65,\n    \"b15\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of explainability and interpretability in deep learning (DL) models, particularly in the context of healthcare and education. The authors propose infusing domain knowledge into DL models using knowledge graphs (KG) to improve their ability to generate explanations and interpretations. The paper outlines various methods for knowledge infusion and emphasizes the need for explainable AI systems to foster trust and adoption in real-world domains.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b4\": 0.75,\n    \"b6\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b7\": 0.65,\n    \"b11\": 0.65,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of enhancing the explainability and interpretability of deep learning (DL) models, especially in the context of tasks like question answering and domain-specific applications such as healthcare and education. The authors propose the infusion of knowledge graphs (KG) to improve model understanding, decision-making, and generating human-comprehensible explanations.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b15\": 0.6,\n    \"b17\": 0.5\n  }\n}\n```"], "58d82fcbd649053542fd67e0": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of author identification under double-blind review settings in heterogeneous bibliographic networks. The authors propose a task-guided and path-augmented network embedding method to learn better feature representations for this task. The novel contributions include the use of task-specific embedding guidance and meta-path selection to improve embedding effectiveness.\",\n    \"Direct Inspiration\": {\n        \"b18\": 0.9,\n        \"b17\": 0.9,\n        \"b19\": 0.8,\n        \"b28\": 0.8,\n        \"b27\": 0.8,\n        \"b25\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b23\": 0.6,\n        \"b31\": 0.4,\n        \"b24\": 0.4,\n        \"b22\": 0.3,\n        \"b21\": 0.3,\n        \"b3\": 0.3\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.2,\n        \"b6\": 0.2,\n        \"b7\": 0.2,\n        \"b11\": 0.2,\n        \"b14\": 0.2,\n        \"b15\": 0.2,\n        \"b35\": 0.2,\n        \"b36\": 0.2\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are embedding the network under the guidance of the author identification task and selecting the best type of information in heterogeneous networks. The proposed algorithm addresses these challenges by introducing a task-guided and path-augmented network embedding method. This method jointly trains embeddings for the author identification task and general heterogeneous network embedding, utilizing meta paths derived from heterogeneous networks.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.85,\n    \"b19\": 0.80,\n    \"b28\": 0.80,\n    \"b27\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.70,\n    \"b17\": 0.70\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of author identification in anonymized papers within heterogeneous bibliographic networks. The authors propose a task-guided and path-augmented network embedding method that combines task-specific embedding for author identification and general heterogeneous network embedding using meta paths.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b17\": 0.9,\n    \"b25\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b28\": 0.8,\n    \"b27\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.7,\n    \"b31\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of author identification in a double-blind review setting using heterogeneous bibliographic networks. It introduces a task-guided and path-augmented network embedding method to improve the performance of author identification by incorporating meta paths and task-specific objectives.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b28\": 0.7,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of author identification under a double-blind review setting in heterogeneous bibliographic networks. The primary challenge is to effectively embed the network under the guidance of the author identification task and to select the best type of information due to the network's heterogeneity. The authors propose a task-guided and path-augmented network embedding method to tackle these challenges.\",\n    \"Direct Inspiration\": {\n        \"b19\": 0.9,\n        \"b28\": 0.85,\n        \"b27\": 0.8,\n        \"b25\": 0.75\n    },\n    \"Indirect Inspiration\": {\n        \"b18\": 0.7,\n        \"b17\": 0.65,\n        \"b23\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b24\": 0.5,\n        \"b21\": 0.45\n    }\n}\n```"], "5e4672c93a55ac14f595d7f3": ["```json\n{\n  \"Summary\": \"The paper discusses the challenges in improving Airbnb's search ranking using deep learning models. The primary challenges include optimizing the deep learning architecture, making price more interpretable, and aligning search results with user preferences. The paper describes several iterations of deep learning models, including attempts to enforce price preferences and the development of a two-tower architecture.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b2\": 0.9,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b9\": 0.7,\n    \"b4\": 0.6\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.5,\n    \"b13\": 0.4,\n    \"b15\": 0.4\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.3,\n    \"b18\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses challenges in optimizing Airbnb's search ranking system using deep learning, particularly focusing on price relevance and user preferences. The authors explore various DNN architectures to address these challenges, highlighting their iterative process and insights gained from failed attempts and successful implementations.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b9\": 0.7,\n    \"b11\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.6,\n    \"b4\": 0.6,\n    \"b13\": 0.6,\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges faced in improving Airbnb's search ranking using deep learning, especially beyond initial implementations. It emphasizes the need for specialized architectures and iterative improvements driven by user problems, rather than simply adopting successful techniques from other domains.\",\n  \"Direct Inspiration\": [\"b5\", \"b6\", \"b7\"],\n  \"Indirect Inspiration\": [\"b2\", \"b15\", \"b18\"],\n  \"Other Inspiration\": [\"b4\", \"b9\", \"b11\", \"b13\", \"b16\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing the search ranking system at Airbnb using deep learning techniques. The primary inspiration comes from their previous work and iterations on deep learning models, emphasizing user-driven model development. The paper introduces various novel methods such as enforcing price interpretability, generalized monotonicity, and a two-tower architecture to balance relevance and price, thereby improving the booking rate and search result relevance.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b6\": 0.9,\n    \"b7\": 0.8,\n    \"b18\": 0.8,\n    \"b9\": 0.9,\n    \"b11\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.6,\n    \"b15\": 0.5,\n    \"b4\": 0.5,\n    \"b16\": 0.4\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges faced in improving Airbnb's search ranking using deep learning models. It outlines the journey from initial deep learning implementations to more advanced architectures. Key challenges include the failure of deeper networks, the need for specialized architectures, and the necessity to balance relevance and price in the ranking model. The paper introduces several novel approaches, including a generalized monotonicity model and a two tower architecture, to overcome these challenges.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b13\": 0.6,\n    \"b15\": 0.7,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b11\": 0.5,\n    \"b16\": 0.4\n  }\n}\n```"], "5aed148b17c44a4438154efb": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning higher-order network representations by proposing a framework called higher-order network embedding (HONE). The framework incorporates various motif-based matrix formulations to capture the higher-order connectivity patterns in networks. The paper demonstrates the effectiveness of HONE by comparing it with state-of-the-art methods in node embeddings and link prediction tasks.\",\n  \"Direct Inspiration\": [\"b3\", \"b4\", \"b8\"],\n  \"Indirect Inspiration\": [\"b5\", \"b6\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include learning node embeddings that capture higher-order connectivity patterns in networks. The authors propose a general framework called higher-order network embedding (HONE) which utilizes higher-order motif-based matrices to learn these embeddings. The algorithm leverages various motif matrix formulations such as Motif Weighted Graph, Motif Transition Matrix, Motif Laplacian, and Normalized Motif Laplacian.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b4\": 0.9,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n    \"Summary\": \"The paper tackles the challenge of learning higher-order network representations and proposes the HONE (higher-order network embedding) framework. The key contributions include motif-based matrix formulations, k-step motif-based matrix derivations, and learning node embeddings through optimization problems. The paper evaluates the HONE variants against several state-of-the-art methods for tasks such as link prediction.\",\n    \"Direct Inspiration\": {\n        \"b3\": 1.0,\n        \"b4\": 0.9,\n        \"b8\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.7,\n        \"b5\": 0.6\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning higher-order network representations using a general framework called Higher-Order Network Embedding (HONE). The primary contributions include the formulation of motif-weighted adjacency matrices, motif transition matrices, motif Laplacians, and normalized motif Laplacians to capture higher-order connectivity patterns. The paper proposes several variants of HONE and compares their performance against state-of-the-art methods using link prediction tasks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b4\": 0.85,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b8\": 0.75\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of learning higher-order network embeddings by proposing a framework called HONE, which leverages motif-based adjacency matrices to capture higher-order connectivity patterns. The experimental results show that HONE outperforms existing state-of-the-art methods in link prediction tasks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b8\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b6\": 0.7\n  }\n}\n```"], "5e5e186d93d709897ce02914": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of reducing the feature engineering process in video-based stress recognition by comparing Feed-Forward Neural Network (FFNN) and Long Short-Term Memory (LSTM) models. The primary contributions include evaluating feature selection techniques (magnitude measure and -1 norm regularisation) to reduce parameters in FFNN and comparing these with LSTM for performance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b0\": 1.0,\n    \"b7\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b4\": 0.9,\n    \"b6\": 0.85,\n    \"b9\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges of video-based stress recognition by comparing several neural network models, including FFNN and LSTM, to reduce the reliance on feature engineering, which is often time-consuming and expensive. The primary contributions are the evaluation of feature selection techniques (magnitude measure and -1 norm regularization) to reduce parameters in FFNN models and comparisons with LSTM models designed for time-series data.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b0\": 0.9,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b4\": 0.7,\n    \"b9\": 0.7,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b10\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing the feature engineering process in video-based stress recognition tasks by evaluating and comparing neural network models, specifically FFNN and LSTM. It introduces two key feature selection techniques, magnitude measure and L1 norm regularization, to enhance FFNN performance while reducing parameters.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b8\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b7\": 0.8,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performing video-based stress recognition with neural networks while reducing the need for expensive and time-consuming feature engineering. The authors propose improvements to a feed-forward neural network (FFNN) using feature selection techniques like magnitude measure and -1 norm regularization, and compare these models with a Long Short-Term Memory (LSTM) network.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b4\": 0.9,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b3\": 0.7,\n    \"b5\": 0.6,\n    \"b9\": 0.8,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performing video-based stress recognition without the costly and time-consuming feature engineering process. It compares feed-forward neural networks (FFNN) and long short-term memory networks (LSTM). The main contributions include evaluating feature selection techniques such as magnitude measure and L1 norm regularization to reduce the number of parameters in FFNN models and comparing these with an LSTM model.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b8\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b7\": 0.8,\n    \"b4\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b9\": 0.7,\n    \"b6\": 0.65,\n    \"b11\": 0.65\n  }\n}\n```"], "53e9b31cb7602d9703de96fd": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficiency of modern processors for scale-out workloads in data centers. The paper proposes a detailed micro-architectural study to highlight the mismatch between scale-out workload demands and current processor designs, identifying inefficiencies in instruction-cache miss rates, instruction- and memory-level parallelism, data working sets, and bandwidth requirements. The authors then introduce a benchmark suite, CloudSuite, to evaluate these workloads and the impact on processor design.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b27\": 0.7,\n    \"b36\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the inefficiencies of modern processors for scale-out workloads in cloud data centers. The proposed algorithm involves detailed micro-architectural studies of scale-out workloads to demonstrate significant mismatches between these workloads and the predominant processor microarchitecture. The study shows that scale-out workloads suffer from high instruction-cache miss rates, low instruction-and memory-level parallelism, and inefficient use of large last-level caches, among other issues.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b27\": 1,\n    \"b36\": 1,\n    \"b37\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b16\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b19\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inefficiency of modern processors in running scale-out workloads typically found in data centers. The proposed solution involves a detailed micro-architectural study to identify inefficiencies and leverage the characteristics of scale-out workloads to enhance area and energy efficiency in future servers.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b2\": 0.85,\n    \"b8\": 0.8,\n    \"b4\": 0.8,\n    \"b9\": 0.75,\n    \"b41\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.7,\n    \"b12\": 0.7,\n    \"b27\": 0.7,\n    \"b37\": 0.7,\n    \"b22\": 0.65,\n    \"b6\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b24\": 0.6,\n    \"b13\": 0.55,\n    \"b26\": 0.55,\n    \"b25\": 0.5,\n    \"b38\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the inefficiency of modern processors in handling scale-out workloads typical of cloud data centers. It identifies key mismatches between scale-out workload demands and current processor architectures, such as high instruction-cache miss rates, low instruction and memory-level parallelism, and inadequate on-chip and off-chip bandwidth requirements. The proposed solution involves leveraging the unique characteristics of scale-out workloads to design more area and energy-efficient processors.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1.0,\n        \"b8\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b9\": 0.7,\n        \"b41\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b6\": 0.6,\n        \"b36\": 0.6,\n        \"b38\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies of modern processors for scale-out workloads in cloud data centers. It identifies the primary challenges, such as high instruction-cache miss rates, low instruction and memory-level parallelism, and the mismatch between data working sets and cache capacities. The paper introduces CloudSuite, a benchmark suite for emerging scale-out workloads, and proposes leveraging the unique characteristics of scale-out workloads to design more efficient future servers.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b2\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b9\": 0.7,\n    \"b41\": 0.7,\n    \"b5\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b22\": 0.6,\n    \"b38\": 0.6,\n    \"b36\": 0.65\n  }\n}\n```"], "58d82fc8d649053542fd590e": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of analyzing temporal networks, which are networks where edges change over time. It extends the concept of static network motifs to temporal networks, introducing the notion of \\u03c4-temporal motifs. The paper proposes efficient algorithms for counting these motifs, which are crucial for understanding the structural patterns in temporal networks.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0,\n        \"b19\": 0.9,\n        \"b29\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.7,\n        \"b6\": 0.7,\n        \"b23\": 0.7,\n        \"b27\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5,\n        \"b10\": 0.5,\n        \"b17\": 0.5,\n        \"b13\": 0.4,\n        \"b30\": 0.4,\n        \"b5\": 0.4,\n        \"b24\": 0.4\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of analyzing temporal networks, where the relationships between nodes change over time. The authors propose extending static network motifs to temporal networks and introduce an efficient algorithm to count these motifs. The methodology accounts for edge ordering and time windows, making it suitable for various types of temporal data.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0,\n        \"b19\": 1.0,\n        \"b29\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b13\": 0.7,\n        \"b30\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.5,\n        \"b17\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in modeling and characterizing temporal networks, where links between objects dynamically change over time. It proposes a methodology for analyzing temporal networks using temporal motifs, extending the concept of static network motifs to temporal networks. The authors develop algorithms for counting temporal motifs efficiently, enabling new analyses in various domains.\",\n    \"Direct Inspiration\": {\n        \"b4\": 0.9,\n        \"b10\": 0.8,\n        \"b17\": 0.8,\n        \"b19\": 0.9,\n        \"b29\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.7,\n        \"b1\": 0.7,\n        \"b6\": 0.7,\n        \"b23\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.6,\n        \"b12\": 0.6,\n        \"b14\": 0.6,\n        \"b25\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in analyzing temporal networks, proposing an algorithm for counting temporal motifs efficiently. The novel contribution is a general framework for counting temporal motifs and specialized algorithms for certain patterns.\",\n  \"Direct Inspiration\": [\"b4\", \"b19\", \"b29\"],\n  \"Indirect Inspiration\": [\"b6\", \"b13\", \"b30\"],\n  \"Other Inspiration\": [\"b2\", \"b10\", \"b17\", \"b1\", \"b23\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of modeling and characterizing temporal networks, proposing a methodology that extends static network motifs to temporal networks. It introduces algorithms for counting temporal motifs efficiently, particularly focusing on 2-node, 3-edge motifs like stars and triangles, and demonstrates their application across various datasets to reveal communication patterns and other insights.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0,\n        \"b19\": 1.0,\n        \"b29\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b13\": 0.8,\n        \"b30\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b10\": 0.6,\n        \"b17\": 0.6,\n        \"b1\": 0.6,\n        \"b23\": 0.6\n    }\n}\n```"], "5c96086e3cb210d2716c4a08": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing last-level cache (LLC) usage in Intel processors to reduce memory access latency and improve application performance. It proposes a slice-aware memory management scheme and introduces CacheDirector, an extension to Intel Data Direct I/O Technology (DDIO), to place packet headers into the appropriate LLC slice, thereby reducing packet processing latency and improving predictability in systems.\",\n  \"Direct Inspiration\": {\n    \"b43\": 0.9,\n    \"b54\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b56\": 0.7,\n    \"b15\": 0.7,\n    \"b52\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b70\": 0.6,\n    \"b1\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the Memory Wall problem by proposing slice-aware memory management for Last Level Cache (LLC) in Intel processors. The proposed CacheDirector aims to optimize packet processing latency by leveraging Non-Uniform Cache Architecture (NUCA) characteristics.\",\n  \"Direct Inspiration\": [\"b43\"],\n  \"Indirect Inspiration\": [\"b36\", \"b86\", \"b52\"],\n  \"Other Inspiration\": [\"b15\", \"b54\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Mitigating the memory wall by optimizing cache memory usage, specifically focusing on slice-aware memory management in Intel's non-uniform cache architecture (NUCA) to improve application performance and predictability.\",\n    \"inspirations\": \"Exploiting undocumented Complex Addressing techniques used by Intel processors, leveraging Intel Data Direct I/O Technology (DDIO), and achieving significant performance improvements in NFV service chains.\"\n  },\n  \"Direct Inspiration\": {\n    \"b54\": 1,\n    \"b43\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b86\": 0.85,\n    \"b36\": 0.8,\n    \"b15\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b38\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges include exploiting Intel's undocumented Complex Addressing for better LLC slice management, adapting existing data structures, and balancing performance gains with overhead.\",\n    \"inspirations\": \"The paper introduces slice-aware memory management and CacheDirector, which improves packet processing latency by placing packet headers into the correct LLC slice.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\n      \"b43\"\n    ]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\n      \"b36\",\n      \"b86\",\n      \"b54\"\n    ]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\n      \"b8\",\n      \"b59\",\n      \"b52\",\n      \"b15\"\n    ]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving computer system performance by optimizing Last Level Cache (LLC) management. Specifically, it proposes a slice-aware memory management system, 'CacheDirector,' to leverage the Non-Uniform Cache Architecture (NUCA) characteristics of Intel processors. The solution aims to reduce packet processing latency in high-speed networks by mapping packet headers to the LLC slice closest to the processing core.\",\n  \"Direct Inspiration\": {\n    \"b36\": 0.9,\n    \"b43\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b54\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.65,\n    \"b52\": 0.6,\n    \"b0\": 0.6\n  }\n}\n```"], "5f857a2791e011ff32809698": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of characterizing cloud applications' system resource consumption and understanding the impact on overall performance, with a focus on latency-critical workloads, load levels, hyper-threading, and resource sharing. The research aims to help cloud providers manage resources to avoid QoS degradation. The paper introduces new workload characterization and taxonomy, investigates the impact of hyper-threading, and explores the effects of constraining major system resources.\",\n  \"Direct Inspiration\": [\"b2\", \"b3\"],\n  \"Indirect Inspiration\": [\"b7\", \"b8\"],\n  \"Other Inspiration\": [\"b10\", \"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenges of cloud performance, particularly focusing on tail latency, hyper-threading effects, and resource sharing. It provides insights into how performance is affected by varying load levels, hyper-threading configurations, and constraints on major system resources, aiming to help cloud providers improve system performance.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.85,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b8\": 0.7,\n    \"b9\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b11\": 0.55,\n    \"b19\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of performance degradation in public cloud systems due to resource sharing among VMs. It proposes an analysis of cloud application behavior from a system resource consumption perspective, focusing on latency-critical workloads. The paper studies the impact of load levels, hyper-threading, and resource constraints on performance, particularly tail latency, and suggests the need for new resource allocation strategies.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.85,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b8\": 0.65,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of inter-VM interference in public cloud systems, focusing on latency-critical workloads. It proposes characterizing cloud applications from a system resource consumption perspective and studies how performance is affected by varying load levels, hyper-threading, and resource sharing. The aim is to help cloud providers understand performance variations and optimize resource allocation.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.85,\n    \"b6\": 0.8,\n    \"b7\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b9\": 0.65,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.5,\n    \"b19\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses primary challenges in public cloud systems related to resource sharing among tenant applications, focusing on latency-critical workloads. It presents a workload characterization study, examines the impact of CPU utilization, hyper-threading, and resource sharing on performance, and proposes further research into performance degradation modeling.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b7\": 0.85,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b6\": 0.7,\n    \"b8\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b19\": 0.5\n  }\n}\n```"], "5fe31b9491e01125d4b5b744": ["```json\n{\n    \"Summary\": \"The paper focuses on developing a Medical Dialogue System (MDS) aimed at addressing low-resource medical dialogue generation. The primary challenges include handling data imbalance among diseases, effectively transferring diagnostic experience between diseases, and evolving disease-symptom relationships. The proposed solution integrates a hierarchical context encoder, a meta-knowledge graph reasoning (MGR) network, and a graph-guided response generator. The novel Graph-Evolving Meta-Learning (GEML) framework further enhances the system by enabling fast adaptation to new diseases with limited data.\",\n    \"Direct Inspiration\": {\n        \"b33\": 0.9,\n        \"b35\": 0.8,\n        \"b16\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.75,\n        \"b25\": 0.7,\n        \"b29\": 0.7,\n        \"b1\": 0.65,\n        \"b5\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b30\": 0.6,\n        \"b31\": 0.6,\n        \"b17\": 0.55,\n        \"b15\": 0.55,\n        \"b32\": 0.5,\n        \"b8\": 0.5,\n        \"b39\": 0.45,\n        \"b40\": 0.45,\n        \"b22\": 0.4,\n        \"b26\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of medical dialogue systems (MDS) in generating responses for low-resource medical dialogue scenarios. It proposes an end-to-end dialogue system that integrates a hierarchical context encoder, a meta-knowledge graph reasoning (MGR) network, and a graph-guided response generator. The system also employs a Graph-Evolving Meta-Learning (GEML) framework to transfer diagnostic experience across different diseases.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b22\": 0.8,\n    \"b26\": 0.75,\n    \"b32\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b16\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b25\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of low-resource medical dialogue generation (MDG). The primary contributions include an end-to-end dialogue system integrating a hierarchical context encoder, a meta-knowledge graph reasoning (MGR) network, and a graph-guided response generator. Additionally, the paper introduces a Graph-Evolving Meta-Learning (GEML) framework to transfer diagnostic experience across diseases and adapt to new diseases with limited data. The methods proposed aim to improve the efficiency and accuracy of symptom request and disease diagnosis in medical dialogue systems.\",\n  \"Direct Inspiration\": {\n    \"b32\": 0.9,\n    \"b22\": 0.85,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.75,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b25\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing a low-resource medical dialogue generation (MDG) model that can handle the data imbalance and scarcity of examples for various diseases. The authors propose an end-to-end dialogue system incorporating a hierarchical context encoder, a meta-knowledge graph reasoning (MGR) network, and a graph-guided response generator. They also introduce a novel Graph-Evolving Meta-Learning (GEML) framework to transfer diagnostic experience from high-resource diseases to low-resource ones and adapt to the evolving nature of disease-symptom correlations.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b32\": 1.0,\n    \"b26\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b3\": 0.7,\n    \"b25\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b33\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an end-to-end medical dialogue system (MDS) for low-resource scenarios, focusing on enhancing dialogue generation with medical knowledge incorporation and meta-learning. The proposed model, which integrates a hierarchical context encoder, a meta-knowledge graph reasoning network, and a graph-guided response generator, aims to transfer diagnostic experience from high-resource diseases to low-resource ones. Additionally, a novel Graph-Evolving Meta-Learning (GEML) framework is introduced to improve the adaptability and accuracy of the model in low-resource settings.\",\n  \n  \"Direct Inspiration\": {\n    \"b16\": 0.9,\n    \"b33\": 0.85,\n    \"b35\": 0.85,\n    \"b22\": 0.8\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b25\": 0.7,\n    \"b29\": 0.7\n  },\n  \n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b15\": 0.6,\n    \"b32\": 0.6\n  }\n}\n```"], "53e9b350b7602d9703e268f6": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficiently partitioning the shared largest-level on-chip cache among multiple competing applications in multicore processors. The authors propose a Utility-Based Cache Partitioning (UCP) approach, which allocates cache based on the utility of cache resources for each application rather than the demand. The UCP scheme includes a novel, low-overhead utility monitoring (UMON) circuit and a scalable Lookahead Algorithm for partitioning decisions.\",\n    \"Direct Inspiration\": [\"b3\", \"b9\", \"b11\"],\n    \"Indirect Inspiration\": [\"b14\"],\n    \"Other Inspiration\": [\"b1\", \"b5\"]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the efficient partitioning of the shared largest-level on-chip cache among multiple competing applications to reduce off-chip accesses and improve performance. The authors propose Utility-Based Cache Partitioning (UCP) to address this challenge, leveraging utility monitoring circuits (UMON) to obtain runtime information about the utility of cache resources for different applications. The UCP scheme includes a novel partitioning algorithm and replacement policy adjustments to optimize cache allocation based on utility rather than demand.\",\n    \"Direct Inspiration\": {\n        \"b11\": 1.0,\n        \"b14\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.7,\n        \"b9\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b1\": 0.5,\n        \"b5\": 0.5,\n        \"b18\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently partitioning the shared largest-level on-chip cache among multiple competing applications in multicore processors. The authors propose a Utility-Based Cache Partitioning (UCP) method, which allocates cache resources based on the utility or benefit that applications derive from the cache, rather than their demand. This approach is supported by monitoring circuits (UMON) that gather utility information with low hardware overhead and a scalable Lookahead Algorithm to manage partitioning decisions.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b5\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently partitioning the largest level of on-chip cache among multiple concurrent applications in multicore processors. The proposed solution, Utility-Based Cache Partitioning (UCP), aims to allocate cache resources based on the utility or benefit derived by each application, rather than their demand. This approach is facilitated by a novel, low-overhead utility monitoring (UMON) circuit and a scalable Lookahead Algorithm for partitioning decisions.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b5\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently partitioning the shared largest-level on-chip cache among multiple competing applications in multicore processors. It proposes a novel Utility-Based Cache Partitioning (UCP) scheme that uses low-overhead utility monitoring circuits (UMON) to allocate cache resources based on the utility rather than demand. This approach aims to improve overall system performance by reducing misses and ensuring cache resources are allocated where they are most beneficial.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b5\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```"], "53e99fa2b7602d9702875e25": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing simulation time in detailed microarchitecture simulation models by proposing a self-monitored adaptive (SMA) warm-up scheme for processor caches. The SMA technique monitors the warm-up process and decides when the caches are sufficiently warmed up based on a heuristic, aiming for both accuracy and reduced warm-up overhead.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b8\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of reducing simulation time in detailed microarchitecture simulations by proposing a self-monitored adaptive (SMA) warm-up scheme for processor caches. This scheme aims to achieve accurate cache state initialization with minimal warm-up instructions, adapting to both benchmark characteristics and cache configurations.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.8,\n        \"b9\": 0.7,\n        \"b8\": 0.6,\n        \"b7\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.5,\n        \"b4\": 0.4,\n        \"b6\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing simulation time in microarchitecture simulation by proposing a self-monitored adaptive (SMA) warm-up scheme for processor caches. The SMA technique adapts to both workload characteristics and cache configurations to achieve accurate warm-up with reduced overhead compared to previous methods.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b11\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b13\": 0.6,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.4,\n    \"b7\": 0.4,\n    \"b8\": 0.4,\n    \"b9\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing the simulation time of modern microarchitecture simulations by focusing on the warm-up process of processor caches. The authors propose a self-monitored adaptive (SMA) warm-up scheme that monitors the cache warm-up process and decides when the caches are warmed up based on a simple heuristic. This method aims to achieve high accuracy with lower warm-up overhead compared to previous techniques.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b9\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing simulation time for detailed microarchitecture simulation models by proposing a self-monitored adaptive warm-up scheme for processor caches. This scheme aims to achieve high accuracy with lower warm-up overhead compared to previous techniques.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b13\": 0.6,\n    \"b14\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b8\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```"], "58d83045d649053542fe853e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of modern microprocessors by proposing a novel path confidence-based lookahead prefetcher named Signature Path Prefetcher (SPP). The authors introduce a signature mechanism for memory access patterns, develop a path confidence-based prefetch throttling mechanism, and evaluate SPP's performance improvements over existing prefetchers.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b11\": 0.85,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently prefetching data in modern microprocessors to bridge the gap between processor speed and memory latency. It introduces the Signature Path Prefetcher (SPP), which uses a path confidence-based lookahead prefetching mechanism. Key contributions include a signature mechanism for compressed memory access patterns, a path confidence-based throttling mechanism, and operation purely in the physical memory space.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b11\": 0.8,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b5\": 0.4,\n    \"b6\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of memory latency and cache hierarchy inefficiencies by proposing a novel Signature Path Prefetcher (SPP) that uses a lookahead mechanism and path confidence-based throttling to improve prefetching accuracy and coverage. The main contributions include a signature mechanism for compressing memory access patterns, a path confidence-based prefetch throttling mechanism, and a design that operates purely in the physical memory space.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b11\": 0.9,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b9\": 0.75,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.65,\n    \"b6\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of bridging the performance gap between processor execution speed and memory latency through an innovative prefetching technique called the Signature Path Prefetcher (SPP). SPP leverages a signature mechanism to store and predict memory access patterns, introduces a path confidence-based prefetch throttling mechanism, and operates purely based on the physical memory access stream to improve prefetching accuracy and coverage.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b11\": 1.0,\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b9\": 0.8,\n    \"b10\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b14\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of prefetching in modern microprocessors, focusing on improving both prefetch coverage and accuracy. The proposed Signature Path Prefetcher (SPP) introduces a signature mechanism that stores memory access patterns in a compressed form to initiate lookahead prefetching. SPP employs a path confidence-based prefetch throttling mechanism to dynamically balance prefetch coverage with accuracy. Unlike prior lookahead based prefetchers, SPP does not require deep hooks into the core microarchitecture and operates purely based on the physical memory access stream.\",\n    \"Direct Inspiration\": {\n        \"b11\": 0.9,\n        \"b8\": 0.9,\n        \"b12\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.75,\n        \"b9\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b10\": 0.6,\n        \"b14\": 0.55\n    }\n}\n```"], "5fc61cdb91e0118947381abc": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of author disambiguation in large paper databases by proposing an unsupervised, incremental approach called IUAD. The paper critiques existing top-down methods and introduces a novel bottom-up method, involving the construction of a stable collaboration network (SCN) and a global collaboration network (GCN). The SCN captures stable collaborative relations to ensure high precision, while the GCN uses a probabilistic generative model to improve recall.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b22\": 0.8,\n    \"b27\": 0.75,\n    \"b29\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b26\": 0.65,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is accurately disambiguating authors who share the same name in large paper databases. The proposed algorithm, IUAD, introduces a novel incremental and unsupervised approach in a bottom-up manner, which focuses on identifying stable collaborative relations and using a probabilistic generative model to reconstruct a global collaboration network.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.8,\n    \"b29\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b34\": 0.5,\n    \"b35\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of author disambiguation, which identifies distinct authors sharing the same name in paper databases. The proposed method, IUAD, is an incremental and unsupervised approach that works in a bottom-up manner. It constructs a stable collaboration network (SCN) based on frequent co-author relationships and further refines this using a probabilistic generative model to form a global collaboration network (GCN). The method aims to improve both precision and recall in author disambiguation.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b22\": 0.9,\n    \"b27\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b32\": 0.7,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b30\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in author disambiguation, particularly focusing on unsupervised methods to improve accuracy and handle large-scale data. It proposes a novel incremental and unsupervised author disambiguation method called IUAD, which operates in two stages: constructing a stable collaboration network (SCN) based on frequent co-author relationships, and then using a probabilistic generative model to create a global collaboration network (GCN). The method aims to overcome limitations of existing top-down approaches by emphasizing bottom-up construction and integration of diverse information.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.7,\n    \"b29\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.5,\n    \"b33\": 0.5,\n    \"b34\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is author disambiguation in online bibliography systems, particularly focusing on the limitations of existing unsupervised methods. The proposed algorithm, IUAD, is a novel incremental and unsupervised approach consisting of two stages: constructing a Stable Collaboration Network (SCN) based on frequent co-author relationships, and enhancing it to a Global Collaboration Network (GCN) using a probabilistic generative model to integrate diverse information.\",\n  \"Direct Inspiration\": [\"b21\", \"b22\"],\n  \"Indirect Inspiration\": [\"b27\", \"b29\", \"b33\"],\n  \"Other Inspiration\": [\"b26\", \"b34\", \"b35\"]\n}\n```"], "5db1765a3a55ac101c887e97": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving NLP model performance through a unified text-to-text framework. The authors propose using a consistent model and training procedure across various NLP tasks, leveraging large-scale unsupervised pre-training on a new dataset (C4) and utilizing the Transformer architecture.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b68\": 1,\n    \"b82\": 1,\n    \"b108\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.9,\n    \"b64\": 0.9,\n    \"b65\": 0.9,\n    \"b95\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.8,\n    \"b109\": 0.8,\n    \"b111\": 0.8,\n    \"b120\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of making natural language processing (NLP) models more capable of handling diverse tasks by unifying them under a text-to-text framework. The authors propose using the Text-to-Text Transfer Transformer (T5) and develop the Colossal Clean Crawled Corpus (C4) dataset to support large-scale training. The goal is to systematically study and push the limits of transfer learning techniques for NLP.\",\n  \"Direct Inspiration\": {\n    \"b68\": 1,\n    \"b82\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.9,\n    \"b64\": 0.9,\n    \"b108\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b95\": 0.8,\n    \"b65\": 0.8,\n    \"b36\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a unified approach to transfer learning for NLP by treating every NLP problem as a 'text-to-text' problem, leveraging the Transformer architecture. The primary challenges addressed include the complexity of comparing different transfer learning techniques and the scalability of models. The novel contributions include the 'Text-to-Text Transfer Transformer' (T5) framework and the creation of the 'Colossal Clean Crawled Corpus' (C4) dataset.\",\n  \"Direct Inspiration\": {\n    \"b68\": 1.0,\n    \"b82\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b108\": 0.9,\n    \"b23\": 0.8,\n    \"b64\": 0.8,\n    \"b65\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b95\": 0.7,\n    \"b36\": 0.7,\n    \"b25\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of transfer learning in NLP through a unified 'text-to-text' framework, leveraging the Transformer architecture. It proposes the 'Text-to-Text Transfer Transformer' (T5) model and introduces the 'Colossal Clean Crawled Corpus' (C4) dataset. The paper primarily focuses on providing a comprehensive perspective on existing techniques and achieving state-of-the-art results through systematic study and scaling.\",\n    \"Direct Inspiration\": {\n        \"b68\": 0.9,\n        \"b82\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b23\": 0.7,\n        \"b64\": 0.6,\n        \"b25\": 0.6,\n        \"b65\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b108\": 0.5,\n        \"b95\": 0.5,\n        \"b36\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of unifying various NLP tasks under a single framework by treating all tasks as a text-to-text problem. This approach allows for the use of the same model, training procedure, and evaluation method across different NLP tasks, enhancing the ability to compare and improve transfer learning techniques. The paper introduces the Text-to-Text Transfer Transformer (T5) and a new dataset, the Colossal Clean Crawled Corpus (C4), to advance the scalability and performance of NLP models.\",\n  \"Direct Inspiration\": {\n    \"b68\": 1.0,\n    \"b82\": 1.0,\n    \"b108\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.9,\n    \"b64\": 0.85,\n    \"b25\": 0.85,\n    \"b65\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.8,\n    \"b109\": 0.75,\n    \"b111\": 0.75,\n    \"b95\": 0.7\n  }\n}\n```"], "5a4aef9e17c44a2190f7a6fb": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing the redundancy in CNNs by proposing the Neuron Importance Score Propagation (NISP) algorithm. This algorithm propagates neuron importance scores from the final response layer backwards to earlier layers and prunes neurons with low importance scores. The pruning process is formulated as a binary integer programming problem and solved with an efficient closed-form solution. The method is demonstrated to achieve significant compression and acceleration of CNNs with minimal accuracy loss.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b28\": 1,\n    \"b33\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b13\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing the high computational cost and number of parameters in CNNs during both training and testing phases. The proposed Neuron Importance Score Propagation (NISP) algorithm measures the importance of neurons in the final response layer and propagates it backwards to prune neurons with lower importance scores across the entire network. The approach is evaluated on multiple datasets and demonstrates superior performance compared to existing pruning methods.\",\n  \"Direct Inspiration\": {\n    \"b24\": 0.9,\n    \"b28\": 0.85,\n    \"b33\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.75,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b5\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high computational cost and excessive parameters in CNNs by proposing a novel pruning algorithm called Neuron Importance Score Propagation (NISP). This algorithm measures neuron importance across the entire network, rather than layer-by-layer, to minimize reconstruction errors and maintain predictive accuracy.\",\n  \"Direct Inspiration\": {\n    \"References\": [\"b33\"]\n  },\n  \"Indirect Inspiration\": {\n    \"References\": [\"b13\", \"b28\", \"b24\"]\n  },\n  \"Other Inspiration\": {\n    \"References\": [\"b10\", \"b22\", \"b21\", \"b15\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of pruning neurons and filters in Convolutional Neural Networks (CNNs) to reduce computational cost and parameters while maintaining accuracy. The authors propose the Neuron Importance Score Propagation (NISP) algorithm, which measures and propagates neuron importance across all layers of the network, rather than layer-by-layer, to minimize reconstruction error and maintain predictive accuracy.\",\n    \"Direct Inspiration\": {\n        \"b24\": 1.0,\n        \"b28\": 0.9,\n        \"b33\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b13\": 0.8,\n        \"b10\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.5,\n        \"b20\": 0.5,\n        \"b4\": 0.5,\n        \"b21\": 0.5,\n        \"b36\": 0.5,\n        \"b15\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high computational costs and redundancy in CNNs by introducing the Neuron Importance Score Propagation (NISP) algorithm. This algorithm globally measures neuron importance across different layers and prunes neurons based on their propagated importance scores, minimizing reconstruction errors and preserving predictive accuracy.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.8,\n    \"b33\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```"], "58d82fb2d649053542fd184a": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently counting graphlets in large networks. The proposed algorithm is a fast, efficient, and parallel method leveraging combinatorial arguments to significantly improve the scalability of graphlet counting. The approach is shown to be 460x faster than current methods and is applicable to large-scale networks with hundreds of millions of nodes and billions of edges.\",\n  \"Direct Inspiration\": {\n    \"b32\": 0.9,\n    \"b21\": 0.8,\n    \"b39\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.7,\n    \"b7\": 0.7,\n    \"b31\": 0.6,\n    \"b30\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5,\n    \"b11\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently counting graphlets (small subgraph patterns) in large networks. The authors propose a fast, efficient, and parallel algorithm that leverages combinatorial arguments to significantly improve the scalability of graphlet counting, achieving an average runtime improvement of 460x over state-of-the-art methods. The algorithm is tested on over 300 networks from various domains, showing its effectiveness and scalability.\",\n  \"Direct Inspiration\": {\n    \"b32\": 0.9,\n    \"b21\": 0.85,\n    \"b39\": 0.85,\n    \"b14\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.75,\n    \"b25\": 0.75,\n    \"b4\": 0.75,\n    \"b6\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.7,\n    \"b5\": 0.7,\n    \"b1\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently counting graphlets in large networks, which is computationally intensive with existing methods. The authors propose a fast, efficient, and parallel algorithm that leverages combinatorial arguments to improve scalability and performance, achieving significant runtime improvements compared to state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b39\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.7,\n    \"b25\": 0.7,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficiently counting graphlets, which are small subgraph patterns recurring in real-world networks. The authors propose a novel fast, efficient, and parallel algorithm for graphlet counting that leverages combinatorial arguments to achieve significant improvements in scalability and runtime compared to existing methods.\",\n    \"Direct Inspiration\": {\n        \"b32\": 1.0,\n        \"b21\": 0.9,\n        \"b39\": 0.85,\n        \"b14\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b27\": 0.75,\n        \"b25\": 0.7,\n        \"b37\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b26\": 0.55,\n        \"b31\": 0.5,\n        \"b30\": 0.45,\n        \"b7\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently and accurately counting graphlets in large networks. The authors propose a novel fast, efficient, and parallel algorithm for counting graphlets of size k = {3, 4}-nodes, leveraging combinatorial arguments and edge neighborhoods. This new method shows significant improvements in scalability and speed over existing approaches, enabling the analysis of much larger networks and various applications in graph anomaly detection, community detection, and graph classification.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b39\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b25\": 0.6,\n    \"b11\": 0.5\n  }\n}\n```"], "5e5e18e493d709897ce3a0f2": ["```json\n{\n  \"Summary\": \"The paper focuses on addressing challenges in NLP model pretraining, particularly memory limitations due to large model sizes. Key contributions include parameter reduction techniques, cross-layer parameter sharing, and a novel sentence-order prediction loss to improve language representation efficiency.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b27\": 0.9,\n    \"b53\": 0.8,\n    \"b59\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.6,\n    \"b16\": 0.6,\n    \"b18\": 0.5,\n    \"b35\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.4,\n    \"b11\": 0.4,\n    \"b21\": 0.4,\n    \"b43\": 0.3,\n    \"b55\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in improving NLP models by reducing memory consumption and training time for large-scale models. It introduces ALBERT, a model that incorporates parameter-reduction techniques and cross-layer parameter sharing to enhance parameter efficiency. The paper also proposes a sentence-order prediction (SOP) loss for better inter-sentence coherence modeling.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b5\": 0.9,\n    \"b16\": 0.9,\n    \"b27\": 0.8,\n    \"b53\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.7,\n    \"b43\": 0.65,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.55,\n    \"b0\": 0.55,\n    \"b10\": 0.55,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b24\": 0.5,\n    \"b29\": 0.5,\n    \"b23\": 0.5,\n    \"b14\": 0.5,\n    \"b38\": 0.5,\n    \"b59\": 0.45,\n    \"b55\": 0.4,\n    \"b22\": 0.4,\n    \"b28\": 0.35,\n    \"b30\": 0.35,\n    \"b46\": 0.3,\n    \"b48\": 0.3,\n    \"b31\": 0.25,\n    \"b50\": 0.25,\n    \"b52\": 0.25,\n    \"b61\": 0.2\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of memory limitations and training speed in large NLP models, proposing ALBERT as a solution with novel techniques such as cross-layer parameter sharing and factorized embedding parameterization to improve parameter efficiency and model performance. The paper also introduces a sentence-order prediction loss for better inter-sentence coherence modeling.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b35\": 0.9,\n    \"b53\": 0.9,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of computational constraints posed by large NLP models, particularly memory limitations and training speed. It proposes the ALBERT model, which introduces cross-layer parameter sharing and a sentence-order prediction loss to improve parameter efficiency and performance over BERT. The paper takes inspiration from previous work on pre-trained language models, parameter sharing techniques, and sentence embedding objectives.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b27\": 0.9,\n    \"b53\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b18\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of memory limitations and training speed in large NLP models with hundreds of millions or billions of parameters. The proposed solution, ALBERT, introduces innovations such as cross-layer parameter sharing and factorized embedding parameterization to improve parameter efficiency and training speed. The paper also explores a new pretraining loss based on sentence-order prediction (SOP) compared to BERT's next-sentence prediction (NSP).\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b27\": 0.9,\n    \"b5\": 0.9,\n    \"b16\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.7,\n    \"b21\": 0.7,\n    \"b53\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.6,\n    \"b31\": 0.5,\n    \"b43\": 0.5\n  }\n}\n```"], "5aed14e217c44a4438159cc1": ["```json\n{\n  \"Summary\": \"The paper addresses the lack of a comprehensive taxonomy and classification of task-based parallel programming environments. It introduces a new taxonomy to classify and categorize existing APIs and runtimes, focusing on API characteristics and runtime mechanisms such as scheduling, performance monitoring, and fault handling.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b22\": 0.7,\n    \"b27\": 0.7,\n    \"b1\": 0.7,\n    \"b16\": 0.6,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b23\": 0.5,\n    \"b24\": 0.5,\n    \"b6\": 0.5,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the lack of a comprehensive taxonomy and classification of existing task-based parallel programming environments, which are crucial for HPC software development. The authors propose a taxonomy for task-based parallel programming environments, defining important API characteristics and runtime mechanisms, and apply this taxonomy to classify various existing environments.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b4\": 0.8,\n    \"b9\": 0.7,\n    \"b16\": 0.6,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b7\": 0.5,\n    \"b22\": 0.4,\n    \"b23\": 0.4,\n    \"b24\": 0.4,\n    \"b26\": 0.4,\n    \"b27\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge addressed in the paper is the lack of an up-to-date and comprehensive taxonomy and classification of existing task-based parallel programming environments. The authors aim to provide a taxonomy of task-based parallel programming environments and validate its applicability by classifying various task-based environments.\",\n    \"Inspirations\": \"The paper is inspired by the need to provide a clear overview of task-based programming environments, particularly in the context of modern HPC, which includes considerations for heterogeneity and resilience management.\"\n  },\n  \"Direct Inspiration\": {\n    \"b18\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.85,\n    \"b9\": 0.85,\n    \"b22\": 0.85,\n    \"b27\": 0.85,\n    \"b1\": 0.80,\n    \"b2\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.75,\n    \"b7\": 0.75,\n    \"b16\": 0.75,\n    \"b17\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the lack of an up-to-date and comprehensive taxonomy and classification of existing task-based parallel programming environments. The authors propose a taxonomy to classify various task-based programming environments, focusing on their APIs and runtime systems, and then apply this taxonomy to categorize these environments. They address aspects such as scheduling, performance monitoring, and fault handling.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b9\": 0.6,\n    \"b22\": 0.6,\n    \"b27\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b6\": 0.5,\n    \"b7\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5,\n    \"b2\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of providing a comprehensive taxonomy and classification of task-based parallel programming environments, which is crucial for researchers and developers focused on HPC software development. It introduces a taxonomy for classifying task-based APIs and runtimes, validated by applying it to various existing environments. The paper's main contribution is the structured classification of APIs based on key features relevant to modern HPC, such as heterogeneity and resilience management.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b9\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.7,\n    \"b26\": 0.7,\n    \"b27\": 0.7\n  }\n}\n```"], "5aed146117c44a4438152803": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing a flexible and efficient DNN accelerator that can handle a wide variety of dataflows due to the rapid evolution of DNNs. The proposed solution, Maeri, leverages a reconfigurable interconnect to support diverse dataflows, enhancing efficiency and reducing overhead compared to rigid accelerators.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b15\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b19\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the lack of programmability in DNN accelerator ASIC designs to support evolving DNN dataflows, including various layer types, input/filter sizes, and optimizations like sparsity. The proposed solution, Maeri, introduces a reconfigurable interconnect within the accelerator to adapt to different dataflows efficiently, using a collection of multiply-accumulate engines augmented with configurable switches.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b15\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b19\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b18\": 0.65,\n    \"b22\": 0.6,\n    \"b31\": 0.55,\n    \"b32\": 0.55,\n    \"b33\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the need for programmability in DNN accelerator ASIC design to handle the evolving and diverse dataflows resulting from various DNN layers, dense and sparse connections, and different partitioning approaches. The proposed solution, Maeri, introduces a reconfigurable interconnect within the accelerator, allowing for efficient mapping of myriad dataflows using a suite of plug-and-play building blocks rather than a monolithic design.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b13\": 0.6,\n    \"b16\": 0.6,\n    \"b17\": 0.5,\n    \"b18\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.4,\n    \"b21\": 0.4,\n    \"b22\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in this paper is the programmability and flexibility of DNN accelerator ASIC designs to handle a variety of dataflows resulting from different DNN layers, dense and sparse connections, and various partitioning approaches. The proposed solution is Maeri, a design methodology for DNN accelerators that uses a suite of plug-and-play building blocks with reconfigurable interconnects to support different dataflows efficiently.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b15\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing a flexible and efficient DNN accelerator ASIC that can handle various dataflows resulting from different types of layers, dense and sparse connections, and partitioning approaches. The proposed solution, Maeri, introduces reconfigurable interconnects among multiply-accumulate engines to support different dataflows efficiently.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b7\": 1.0,\n    \"b15\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b13\": 0.7,\n    \"b14\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```"], "5bdc31b817c44a1f58a0bee9": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of predicting the probabilities of users clicking on ads or items (click-through rate prediction), focusing on handling sparse and high-dimensional input features and modeling high-order feature interactions. The authors propose a novel approach based on the multi-head self-attention mechanism to explicitly model different orders of feature combinations, represent features in low-dimensional spaces, and provide good model explainability.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b7\": 0.8,\n    \"b10\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b37\": 0.6,\n    \"b31\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of predicting click-through rates (CTR) by dealing with sparse, high-dimensional input features and the need for modeling high-order feature interactions. The proposed approach leverages a multi-head self-attention mechanism to learn effective low-dimensional representations of input features and capture meaningful high-order feature interactions with good explainability.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b35\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b7\", \"b10\", \"b12\", \"b18\", \"b25\", \"b31\", \"b37\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b3\", \"b11\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the effective prediction of click-through rates (CTR) using sparse and high-dimensional input features. The authors propose a novel approach based on the multi-head self-attention mechanism to explicitly model different orders of feature combinations, representing features in low-dimensional spaces while offering good model explainability.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b10\": 0.9,\n    \"b12\": 0.8,\n    \"b31\": 0.8,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b18\": 0.5,\n    \"b37\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of predicting click-through rates (CTR) in online advertising and recommender systems. The main challenges include sparse and high-dimensional input features, the necessity of modeling high-order feature interactions, and ensuring model explainability. The proposed algorithm, AutoInt, leverages a multi-head self-attention mechanism to automatically learn high-order feature interactions, embed features into low-dimensional spaces, and provide model explainability through attention mechanisms.\",\n    \"Direct Inspiration\": {\n        \"b35\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.9,\n        \"b10\": 0.85,\n        \"b25\": 0.8,\n        \"b31\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.75,\n        \"b11\": 0.9,\n        \"b18\": 0.8\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of click-through rate (CTR) prediction, which is critical for applications like online advertising and recommender systems. The primary challenges include handling extremely sparse and high-dimensional input features and effectively modeling high-order feature interactions. The proposed solution involves a novel approach using a multi-head self-attention mechanism to learn low-dimensional representations of features and model their interactions while offering good model explainability.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b10\": 0.7,\n    \"b25\": 0.6,\n    \"b31\": 0.6,\n    \"b37\": 0.5,\n    \"b18\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.4,\n    \"b3\": 0.4,\n    \"b1\": 0.4,\n    \"b4\": 0.4\n  }\n}\n```"], "5bdc315017c44a1f58a05a1d": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Lack of interpretability in deep neural networks for security applications.\",\n      \"Existing explanation methods are not suitable for security applications involving RNN and MLP.\",\n      \"Low explanation fidelity in current methods for security applications.\"\n    ],\n    \"inspirations\": [\n      \"Developing a high-fidelity explanation method specifically for security applications.\",\n      \"Introducing specialized designs to address interpretability and explanation fidelity issues.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b33\", \"b45\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b26\", \"b34\", \"b64\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b2\", \"b50\", \"b53\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the lack of transparency and interpretability of deep neural networks in security applications. Existing explanation methods are either not applicable or have low fidelity when applied to models like RNN and MLP used in security domains. The paper proposes LEMNA, a novel explanation method that works under a black-box setting and improves explanation fidelity by using a mixture regression model enhanced with fused lasso.\",\n  \"Direct Inspiration\": [\"b33\", \"b45\", \"b64\"],\n  \"Indirect Inspiration\": [\"b26\", \"b34\"],\n  \"Other Inspiration\": [\"b2\", \"b50\", \"b53\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the lack of transparency in deep neural networks (DNNs) used in security applications, particularly focusing on the challenges of low interpretability and poor explanation fidelity of existing methods. The proposed solution, LEMNA, is a novel explanation method tailored for security applications, utilizing a mixture regression model enhanced by fused lasso to provide high-fidelity explanations under a black-box setting.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b45\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b34\": 0.8,\n    \"b64\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b50\": 0.6,\n    \"b53\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of explaining deep neural network decisions in security applications. It introduces LEMNA, a novel explanation method that improves fidelity by using a mixture regression model enhanced with fused lasso to handle non-linear decision boundaries and feature dependencies. The method is designed to work under a black-box setting and is evaluated on malware classification and binary reverse-engineering tasks.\",\n  \"Direct Inspiration\": [\"b33\", \"b45\"],\n  \"Indirect Inspiration\": [\"b26\", \"b34\", \"b64\"],\n  \"Other Inspiration\": [\"b14\", \"b20\", \"b52\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the transparency and interpretability of deep neural networks in security applications. The proposed algorithm, LEMNA, uses a mixture regression model enhanced by fused lasso to approximate non-linear decision boundaries and handle feature dependencies, aiming to provide high-fidelity explanations of classifier decisions.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b45\": 1.0,\n    \"b64\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b20\": 0.8,\n    \"b52\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.7,\n    \"b34\": 0.7\n  }\n}\n```"], "5b67b47917c44aac1c8639a8": ["```json\n{\n  \"Summary\": \"The paper presents Widar2.0, a device-free passive tracking system using a single pair of COTS WiFi devices. It addresses challenges such as joint estimation of multiple parameters (AoA, ToF, DFS, and attenuation) from noisy CSI measurements, eliminating phase noises, and deriving precise locations from low-resolution parameters. Widar2.0 introduces novel algorithms for multidimensional parameter estimation, CSI cleaning via conjugate multiplication, and a graph-based algorithm for precise localization.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.9,\n    \"b11\": 0.9,\n    \"b14\": 0.9,\n    \"b15\": 0.9,\n    \"b19\": 0.9,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b6\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b20\": 0.6,\n    \"b37\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of passive human tracking using a single pair of COTS WiFi devices without additional infrastructure. The proposed system, Widar2.0, overcomes issues related to noisy CSI measurements and joint estimation of multiple parameters (AoA, ToF, DFS, and attenuation). The key contributions include a unified model for multidimensional parameter estimation, a novel CSI cleaning technique, and algorithms for precise location derivation.\",\n  \"Direct Inspiration\": [\"b11\", \"b15\", \"b19\", \"b30\"],\n  \"Indirect Inspiration\": [\"b14\", \"b23\"],\n  \"Other Inspiration\": [\"b8\", \"b17\", \"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the difficulty of passive WiFi tracking due to weak reflected signals and the complexities of estimating multiple parameters (AoA, ToF, DFS) from noisy CSI measurements on commercial WiFi devices. The algorithm proposed, Widar2.0, aims to address these challenges by using a single pair of COTS WiFi devices to jointly estimate multiple signal parameters, clean phase noises, and derive precise locations through novel algorithms.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b15\", \"b19\", \"b30\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b14\", \"b11\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b0\", \"b4\", \"b39\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of passive localization using WiFi signals, specifically focusing on achieving accurate tracking with minimal infrastructure. The proposed system, Widar2.0, leverages multiple signal parameters (AoA, ToF, DFS, and attenuation) from a single WiFi link, overcoming significant challenges such as noisy CSI measurements and phase noise. Key contributions include a unified model for joint parameter estimation, a novel CSI cleaning method, and algorithms for precise localization.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b14\": 0.9,\n    \"b19\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b30\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b5\": 0.65,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": {\n        \"Challenges\": [\n            \"Estimating multiple parameters (AoA, ToF, DFS, attenuation) from noisy CSI measurements\",\n            \"Cleaning unpredictable phase noises in CSI measurements\",\n            \"Deriving fine-grained locations from low-resolution single link parameters\"\n        ],\n        \"Inspirations\": [\n            \"Using a single pair of COTS WiFi devices for passive human tracking\",\n            \"Unified model for joint estimation of multiple parameters\",\n            \"Novel algorithms for parameter estimation and fusion\",\n            \"Conjugate multiplication for phase noise elimination\"\n        ]\n    },\n    \"Direct Inspiration\": {\n        \"References\": [\"b15\", \"b19\", \"b30\"]\n    },\n    \"Indirect Inspiration\": {\n        \"References\": [\"b1\", \"b11\", \"b14\"]\n    },\n    \"Other Inspiration\": {\n        \"References\": [\"b4\", \"b39\"]\n    }\n}\n```"], "5d0616bd8607575390f86730": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Primitive signal features are domain-dependent.\",\n      \"Difficulty in describing gestures with radio signals from limited links.\",\n      \"Cross-domain generalization requires complex models.\"\n    ],\n    \"inspirations\": [\n      \"Developing a domain-independent feature, Body-Coordinate Velocity Profile (BVP)\",\n      \"Leveraging compressive sensing techniques\",\n      \"Building a cross-domain recognition model with high accuracy and zero training effort in new domains.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b31\": 0.8,\n    \"b32\": 0.8,\n    \"b43\": 0.85,\n    \"b44\": 0.85,\n    \"b49\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b34\": 0.7,\n    \"b38\": 0.75,\n    \"b40\": 0.7,\n    \"b45\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.65,\n    \"b37\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of cross-domain gesture recognition using Wi-Fi signals. Traditional methods rely on domain-specific features, which degrade performance across different environments. Widar3.0 proposes a novel domain-independent feature called body-coordinate velocity profile (BVP) to achieve accurate and zero-effort cross-domain gesture recognition. The system uses channel state information (CSI) from off-the-shelf Wi-Fi devices and employs compressive sensing techniques and a hybrid deep learning model to recognize gestures.\",\n    \"Direct Inspiration\": {\n        \"b43\": 0.9,\n        \"b19\": 0.85,\n        \"b49\": 0.85,\n        \"b31\": 0.8,\n        \"b32\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.7,\n        \"b25\": 0.7,\n        \"b34\": 0.7,\n        \"b40\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b44\": 0.6,\n        \"b45\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are the need for secure, device-free, and ubiquitous gesture recognition that can generalize across different domains (locations, orientations, and environments) without additional training. The proposed solution, Widar3.0, introduces a domain-independent feature called body-coordinate velocity profile (BVP) to achieve cross-domain gesture recognition using Wi-Fi signals without extra training efforts.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b49\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b43\": 0.75,\n    \"b38\": 0.7,\n    \"b31\": 0.65,\n    \"b32\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b44\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of cross-domain gesture recognition using Wi-Fi signals. The proposed system, Widar3.0, introduces a novel domain-independent feature called body-coordinate velocity profile (BVP) to achieve zero-effort cross-domain gesture recognition. The system is designed to overcome the limitations of existing methods that require additional training and data collection for new domains by moving generalization ability downward to the signal level.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b32\": 0.9,\n    \"b43\": 0.8,\n    \"b44\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b49\": 0.7,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b40\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Lack of spatial resolution in Wi-Fi signals causing domain-specific gesture recognition.\",\n      \"Difficulty in describing human gestures using radio signals from limited links.\",\n      \"Need for sophisticated learning models slowing down training and making models less explainable.\"\n    ],\n    \"Inspirations\": [\n      \"Develop a one-fits-all model able to recognize gestures in different domains without extra data collection or retraining.\",\n      \"Extract domain-independent features from raw Wi-Fi signals to improve cross-domain generalization.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b32\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.75,\n    \"b49\": 0.7,\n    \"b43\": 0.65,\n    \"b44\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b38\": 0.55,\n    \"b12\": 0.5\n  }\n}\n```"], "5f0d85c69fced0a24be4f052": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the high processing cost and resulting low throughput for compression-intensive workloads in enterprise and cloud environments. The paper proposes a novel on-chip compression and decompression accelerator called NXU, implemented in IBM POWER9 and z15 processors, to address these challenges. Key contributions include a hybrid LZ77 encoder, a dynamic Huffman encoder, and various other optimizations to improve throughput, compression ratio, and area efficiency.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b3\": 0.85,\n    \"b6\": 0.95,\n    \"b14\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.75,\n    \"b19\": 0.7,\n    \"b21\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b18\": 0.55,\n    \"b24\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high processing cost, low throughput, and high elapsed time for compression-intensive workloads in computing systems. It introduces a novel on-chip compression accelerator (NXU) in IBM's POWER9 and z15 processors, which implements the Deflate standard with significantly higher throughput than existing solutions. Key techniques include a CAM and pseudo-CAM based hybrid encoder, a dynamic Huffman encoder, and a speculative decoder.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b3\": 0.85,\n    \"b5\": 0.8,\n    \"b6\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.65,\n    \"b8\": 0.7,\n    \"b9\": 0.6,\n    \"b14\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b15\": 0.45,\n    \"b19\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high processing costs and low throughput in lossless data compression, particularly in enterprise and cloud environments. It introduces a novel on-chip compression accelerator (NXU) for IBM POWER9 and z15 processors that implements the Deflate standard with significantly higher throughput than existing solutions. The paper details various innovative techniques, including a hybrid LZ77 encoder and a dynamic Huffman encoder, to improve compression ratio and performance.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b5\": 1.0,\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b13\": 0.8,\n    \"b19\": 0.8,\n    \"b21\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b18\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in lossless data compression, particularly the high processing cost and low throughput of compression-heavy workloads. It introduces the NXU compression accelerator in IBM z15 and POWER9, which implements the Deflate standard with novel techniques to enhance throughput and compression ratio.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b3\": 0.8,\n    \"b6\": 0.85,\n    \"b14\": 0.8,\n    \"b19\": 0.75,\n    \"b21\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.6,\n    \"b22\": 0.65,\n    \"b24\": 0.55,\n    \"b25\": 0.55,\n    \"b26\": 0.55,\n    \"b27\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b20\": 0.45,\n    \"b28\": 0.4,\n    \"b29\": 0.4,\n    \"b30\": 0.4,\n    \"b31\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenges of high processing costs, low throughput, and high elapsed time associated with lossless data compression in enterprise and cloud environments. It proposes a novel on-chip compression accelerator, NXU, integrated into IBM's POWER9 and z15 processors. The paper highlights several new techniques for improving throughput and compression ratio, including a hybrid LZ77 encoder, a dynamic Huffman encoder, and a speculative variable length code decoder.\",\n  \"Direct Inspiration\": [\"b1\", \"b3\", \"b14\"],\n  \"Indirect Inspiration\": [\"b6\", \"b7\", \"b8\", \"b18\"],\n  \"Other Inspiration\": [\"b19\", \"b21\", \"b22\", \"b24\", \"b25\"]\n}\n```"], "5f2e715791e011ecdac9c1bc": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an efficient and effective BERT-based ranking model for search systems. Traditional ranking approaches struggle with capturing deep semantic information, while interaction-based models like BERT are computationally expensive. The proposed solution uses a representation-based structure to independently compute query and document embeddings, allowing for pre-computation and reducing online serving latency. The paper introduces the DeText framework, which supports various deep NLP components and balances effectiveness and efficiency for industry use cases.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b21\": 0.9,\n    \"b12\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b10\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating BERT into search ranking models for efficient and effective performance. The authors propose a representation-based structure instead of an interaction-based one, which allows pre-computation of document embeddings and thus reduces online serving latency. They developed the DeText framework to support various NLP components and shared practical lessons for deploying deep ranking models in search systems.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b21\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b17\": 0.75,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.65,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an efficient and effective BERT-based ranking model for industry use cases. Traditional ranking approaches struggle with capturing deep semantic information, while interaction-based models using BERT are computationally expensive for real-time applications. The proposed solution, DeText-BERT, uses a representation-based structure to independently generate embeddings for queries and documents, allowing for pre-computation of document embeddings. This balances the trade-off between effectiveness and efficiency. The framework is designed to be flexible, supporting various neural network components like CNN and LSTM, and incorporates traditional hand-crafted features for improved ranking relevance.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b17\": 0.9,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b12\": 0.7,\n    \"b25\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b10\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of developing an effective and efficient BERT-based ranking model for search systems. Traditional ranking approaches have limitations in capturing deep semantic information, and existing BERT-based approaches are computationally expensive for real-world applications. The proposed approach uses a representation-based structure to independently generate query and document embeddings, which allows for pre-computing document embeddings and reduces online computation. The paper introduces the DeText framework, which supports various deep NLP components in addition to BERT, and aims to balance relevance performance and low latency in industry search systems.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1.0,\n    \"b5\": 0.9,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b25\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b6\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on improving search system ranking by using a BERT-based representation model, which balances effectiveness and efficiency in real-world applications. The authors address the challenge of heavy computational cost associated with BERT models and propose a representation-based structure to enable pre-computing document embeddings, thereby reducing online computation time. The framework, DeText, extends to support other deep NLP components like CNN and LSTM, providing flexibility for various industry use cases.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b12\": 0.95,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b17\": 0.8,\n    \"b25\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```"], "5c8d1d8b4895d9cbc63cdd4f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of environmental perception in 5G cellular systems, focusing on device-free activity recognition, presence detection, and walking speed detection. The proposed algorithms leverage the GSM-band and a prototype 5G system, emphasizing the use of OFDM carriers and signal-strength time series for resource-limited IoT devices.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.95,\n    \"b5\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.85,\n    \"b2\": 0.80,\n    \"b3\": 0.80,\n    \"b6\": 0.75,\n    \"b11\": 0.75,\n    \"b13\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.65,\n    \"b8\": 0.65,\n    \"b9\": 0.65,\n    \"b10\": 0.65,\n    \"b12\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of environmental perception in 5G cellular systems, focusing on device-free activity recognition, presence detection, and walking speed detection. The authors propose algorithms for crowd-size detection in GSM-bands, presence detection using a 5G prototype system with OFDM carriers, and walking speed detection using multiple OFDM channels. The work is tailored for IoT and resource-limited mobile devices, avoiding phase-information and instead using signal-strength time series.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b3\": 0.85,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.75,\n    \"b6\": 0.7,\n    \"b11\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.65,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of environmental perception in 5G cellular systems with a focus on device-free activity recognition, crowd size detection, presence detection, and walking speed detection. The novel methods proposed include the use of OsmocomBB firmware for GSM-band crowd-size detection, a 5G prototype system for presence and walking speed detection, and a comparison to accelerometer-based methods, all without using phase information.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b2\": 0.9,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b4\": 0.8,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses environmental perception in 5G cellular systems, focusing on device-free activity recognition, presence detection, and walking speed detection using a 5G prototype system and GSM-band. The novel contributions include the use of a prototype 5G OFDM system for walking speed detection and the evaluation of device-free activity recognition accuracy in existing cellular systems exploiting OsmocomBB firmware.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1,\n    \"b2\": 1,\n    \"b3\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.8,\n    \"b13\": 0.8,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b12\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of environmental perception using 5G cellular systems, focusing specifically on recognizing crowd size, presence detection, and walking speed detection using signal strength information from GSM and 5G bands. It introduces novel methods for device-free activity recognition tailored for IoT and other resource-limited mobile devices, avoiding the use of phase information and instead exploiting signal-strength time series.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.9,\n    \"b11\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b6\": 0.75,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.65,\n    \"b8\": 0.65,\n    \"b9\": 0.65\n  }\n}\n```"], "5dc5488edf1a9c0c41511e7e": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the disparity in access latency between modern multi-core processors and main memory, also known as the Memory Wall. The paper focuses on evaluating replacement policies (RPs) for different cache inclusion policies (inclusive, non-inclusive, and exclusive) to understand their impact on performance, especially in exclusive caches that have been less explored. The contributions include a comprehensive evaluation of various cache configurations and a discussion on designing cache management techniques tailored to specific inclusion policies.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b9\": 0.85,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.75,\n    \"b27\": 0.7,\n    \"b33\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b21\": 0.55,\n    \"b35\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the disparity between main memory and processor performance, often referred to as the Memory Wall [b37]. The paper's focus is on cache management techniques, specifically in the context of inclusive, non-inclusive, and exclusive last-level caches (LLCs). The novel contribution is a comprehensive evaluation of multiple cache configurations, including various replacement policies (RPs) and prefetchers (PFs), to understand the gaps in cache management design and improve performance across different inclusion policies.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b25\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b21\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b35\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing cache management techniques suitable for exclusive caches, focusing on evaluating the performance of existing replacement policies (RPs) and prefetchers (PFs) across different inclusion policies (inclusive, non-inclusive, and exclusive caches). The main inspiration is to understand the gaps in cache management designs and their impact on performance under various inclusion policies.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b25\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b9\": 0.8,\n    \"b27\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b20\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in managing last-level caches (LLCs) in modern multi-core processors, specifically focusing on the limitations of inclusive and non-inclusive caches and evaluating the performance of various replacement policies (RPs) under different inclusion policies. It aims to determine if exclusive caches offer advantages in terms of effective cache capacity and performance.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b4\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b9\": 0.7,\n    \"b27\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b22\": 0.5,\n    \"b3\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving cache management for modern multi-core processors, specifically focusing on the inclusion policies of last-level caches (LLCs) and their impact on replacement policies (RPs) and prefetchers (PFs). The main contribution is a comprehensive evaluation of multiple cache configurations to determine the need for different cache management techniques based on the inclusion policy, with a special focus on exclusive caches.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b19\": 0.85,\n    \"b21\": 0.8,\n    \"b25\": 0.9,\n    \"b27\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b16\": 0.7,\n    \"b35\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.65,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b20\": 0.7,\n    \"b24\": 0.7,\n    \"b32\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```"], "5eede0b091e0116a23aafcd3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient data annotation in deep learning, specifically for tasks like image classification and 3D Hand Pose Estimation. It proposes a novel task-agnostic Active Learning framework using Graph Convolutional Networks (GCNs) to improve sample selection. The key contributions include the introduction of UncertainGCN and CoreGCN sampling methods, which leverage higher-order graph node information and latent space distancing for better performance in selecting meaningful samples.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1.0,\n    \"b41\": 1.0,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b37\": 0.8,\n    \"b5\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b19\": 0.6,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently selecting meaningful samples for data annotation in deep learning, particularly in domains where annotation is expensive and time-consuming. It introduces a sequential Graph Convolutional Network (GCN) for Active Learning (AL) in a task-agnostic manner, proposing two novel sampling methods: UncertainGCN and CoreGCN. These methods leverage GCNs to propagate uncertainties and geometric information between labelled and unlabelled examples to improve sample selection.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1.0,\n    \"b41\": 1.0,\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b37\": 0.7,\n    \"b10\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes a sequential Graph Convolutional Network (GCN) for Active Learning in a task-agnostic manner to address issues related to the scalability and lack of communication between learner and sampler in existing active learning frameworks.\",\n  \"Direct Inspiration\": {\n    \"b32\": 0.9,\n    \"b41\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.8,\n    \"b37\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.7,\n    \"b0\": 0.65,\n    \"b5\": 0.65,\n    \"b10\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in Active Learning (AL) by proposing a task-agnostic method using Graph Convolutional Networks (GCNs) to improve the sampling strategy. The key methods introduced are UncertainGCN and CoreGCN, which leverage GCNs to better represent the relationship between labelled and unlabelled data.\",\n  \"Direct Inspiration\": [\"b30\", \"b32\", \"b41\"],\n  \"Indirect Inspiration\": [\"b0\", \"b5\", \"b10\", \"b37\"],\n  \"Other Inspiration\": [\"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in Active Learning (AL) for image classification and 3D Hand Pose Estimation by proposing a task-agnostic sequential Graph Convolutional Network (GCN) method. The proposed method introduces UncertainGCN and CoreGCN sampling techniques to improve data selection for annotation. The method aims to overcome limitations of previous task-dependent methods by effectively utilizing information sharing between labelled and unlabelled data through GCNs.\",\n  \"Direct Inspiration\": {\n    \"b32\": 1.0,\n    \"b41\": 1.0,\n    \"b30\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b5\": 0.7,\n    \"b10\": 0.7,\n    \"b37\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b14\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```"], "5c7572b7f56def97988385ce": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing data labeling for deep convolutional neural networks (CNNs) in the context of active learning, particularly focusing on batch sampling to mitigate correlation issues. The authors propose a method that defines active learning as a core-set selection problem and provides a theoretical bound for this problem using the k-Center combinatorial optimization problem.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1.0,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b36\": 0.7,\n    \"b42\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5,\n    \"b41\": 0.6,\n    \"b38\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing the selection of data points for labeling in active learning, specifically for CNNs, by redefining it as a core-set selection problem and providing theoretical guarantees and empirical validation for the proposed method.\",\n  \"Direct Inspiration\": {\n    \"b45\": 0.9,\n    \"b42\": 0.85,\n    \"b25\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b36\": 0.7,\n    \"b38\": 0.65,\n    \"b41\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.55,\n    \"b10\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inefficacy of traditional active learning heuristics when applied to CNNs due to the correlation caused by batch acquisition. The authors propose defining active learning as a core-set selection problem and provide a theoretical bound between average loss over any subset and the remaining data points. They solve the core-set selection as a k-Center problem and propose an efficient approximate solution to this combinatorial optimization problem. The empirical analysis demonstrates state-of-the-art performance for image classification.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b36\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5,\n    \"b42\": 0.5,\n    \"b38\": 0.4,\n    \"b41\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is optimizing the selection of data points to label in a deep convolutional neural network (CNN) setting to maximize accuracy given a fixed labeling budget. The authors propose defining active learning as a core-set selection problem and provide a bound between an average loss over any given subset of the dataset and the remaining data points via the geometry of the data points. They adopt an efficient approximate solution to the k-Center problem to achieve this.\",\n  \"Direct Inspiration\": {\n    \"b45\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b25\": 0.7,\n    \"b42\": 0.7,\n    \"b41\": 0.6,\n    \"b38\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying active learning to deep convolutional neural networks (CNNs) in a batch setting. It proposes a novel algorithm that redefines active learning as a core-set selection problem, aiming to select a subset of data points that minimizes a bound on the core-set loss. This approach is equivalent to solving the k-Center problem, for which the authors provide an efficient approximate solution.\",\n  \"Direct Inspiration\": [\"b45\", \"b25\", \"b42\"],\n  \"Indirect Inspiration\": [\"b4\", \"b36\", \"b41\", \"b38\"],\n  \"Other Inspiration\": [\"b43\", \"b1\", \"b10\"]\n}\n```"], "5dc5488edf1a9c0c41511e82": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of increasing memory capacity in computing systems. It discusses limitations in physical hardware design, semiconductor manufacturing, and emerging technologies like non-volatile memories. The paper also explores NUMA architectures and their potential to increase memory capacity, alongside the issues of latency, coherence, and scalability in multi-socket systems. The Unimem architecture and its application in the EuroExa project are highlighted as contributions towards solving these challenges.\",\n  \"Direct Inspiration\": {\n    \"b74\": 0.9,\n    \"b40\": 0.8,\n    \"b73\": 0.8,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b4\": 0.6,\n    \"b5\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b66\": 0.5,\n    \"b1\": 0.5,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges related to increasing memory capacity in computing systems. It explores various limitations such as die size, packaging design, and signal integrity, as well as the potential of new memory technologies like non-volatile memories and advanced interfaces like HBM and HMC. The paper also discusses the limitations and potential of NUMA architectures and introduces the Unimem architecture as a solution for large-scale memory capacity issues.\",\n  \"Direct Inspiration\": {\n    \"b40\": 0.9,\n    \"b41\": 0.8,\n    \"b73\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b35\": 0.7,\n    \"b36\": 0.6,\n    \"b57\": 0.7,\n    \"b59\": 0.7,\n    \"b65\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.5,\n    \"b42\": 0.55,\n    \"b48\": 0.6,\n    \"b49\": 0.55,\n    \"b53\": 0.5,\n    \"b64\": 0.6,\n    \"b66\": 0.55,\n    \"b68\": 0.55,\n    \"b74\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the limitation in increasing memory capacity, which affects the performance of computing systems. The paper discusses various approaches, technologies, and architectures to overcome these challenges, including NUMA architectures, new memory interfaces like HBM and HMC, and the Unimem architecture. The paper also explores the constraints of single socket memory capacity and potential bypassing solutions.\",\n  \"Direct Inspiration\": {\n    \"b35\": 0.95,\n    \"b57\": 0.9,\n    \"b59\": 0.9,\n    \"b65\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b48\": 0.75,\n    \"b66\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b12\": 0.7,\n    \"b41\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper revolve around increasing memory capacity to meet the demands of modern applications while overcoming physical and architectural limitations. The paper discusses various approaches such as NUMA architectures, new memory technologies, and the Unimem architecture to address these challenges. The authors are particularly focused on the limitations of DRAM technology and the need for innovative interconnects and protocols to enable larger memory capacities.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b48\": 0.8,\n    \"b66\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b40\": 0.75,\n    \"b59\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b73\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of increasing memory capacity in computing systems, focusing on the limitations and potential solutions for single socket processors, Non-Uniform Memory Access (NUMA) architectures, and Distributed Shared Memory (DSM) systems.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.95,\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b12\": 0.85,\n    \"b48\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b40\": 0.75,\n    \"b45\": 0.8,\n    \"b52\": 0.75,\n    \"b64\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.7,\n    \"b57\": 0.7,\n    \"b59\": 0.7,\n    \"b65\": 0.7,\n    \"b66\": 0.7,\n    \"b68\": 0.7,\n    \"b73\": 0.7,\n    \"b74\": 0.7\n  }\n}\n```"], "5f69cfbb91e011a2f02706bb": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving branch prediction accuracy in microprocessors by proposing a new Load Driven Branch Predictor (LDBP). This predictor combines the use of a stride address predictor and a new branch predictor to trigger loads ahead of time and feed the load data to the branch predictor, significantly improving prediction accuracy even with random data.\",\n    \"Direct Inspiration\": [\"b7\", \"b6\", \"b10\"],\n    \"Indirect Inspiration\": [\"b0\", \"b3\", \"b12\", \"b13\", \"b14\"],\n    \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of branch mispredictions in modern microprocessors by proposing a Load Driven Branch Predictor (LDBP). The LDBP leverages predictable load addresses to prefetch data and improve branch prediction accuracy, particularly for branches dependent on recent loads with random data. The proposed predictor integrates with a baseline system, the 256-Kbit TAGE-GSC + IMLI, to achieve significant improvements in prediction accuracy and IPC.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b14\": 0.8,\n    \"b17\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.6,\n    \"b12\": 0.5,\n    \"b13\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.4,\n    \"b7\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of branch mispredictions and data cache misses in modern microprocessors. It introduces the Load Driven Branch Predictor (LDBP), which combines a stride address predictor with a new type of branch predictor to trigger loads ahead of time and feed the load data to the branch predictor, achieving higher accuracy even with random data. The key challenge is ensuring the timeliness of trigger load execution before the corresponding load-dependent branch reaches fetch.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is improving branch prediction accuracy in modern microprocessors to enhance single-thread performance. The novel approach proposed by the authors is the Load Driven Branch Predictor (LDBP), which combines a stride address predictor with a new type of branch predictor to prefetch load data ahead of time and use it in branch prediction. This approach aims to tackle branches that are dependent on predictable load addresses, thereby achieving near-perfect branch prediction accuracy even with random data.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b7\": 0.9,\n    \"b10\": 0.9,\n    \"b14\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b15\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving branch prediction accuracy in microprocessors to enhance single-thread performance. The authors propose a novel Load Driven Branch Predictor (LDBP) that combines load addresses and branch predictors to prefetch load data ahead of time, significantly improving prediction accuracy for hard-to-predict branches with random data patterns.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b11\": 0.7,\n    \"b14\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```"], "5f75aa6a9fced0a24b64599c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of high misprediction rates in branch prediction for data-dependent branches in OOO cores. It introduces the 3D-Branch Overrider, a technique that improves branch prediction accuracy by leveraging the value of feeder loads to override baseline branch predictions. This method is effective for Direct Data Dependent (3D) branches, which constitute a significant portion of mispredictions.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b23\": 0.6,\n    \"b32\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b31\": 0.5,\n    \"b34\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of branch mispredictions in Out-of-Order (OOO) processors, particularly focusing on data-dependent branches. It proposes the 3D-Branch Overrider, a novel technique designed to override the default branch prediction when the feeder load's value is available, thus reducing mispredictions and improving performance. The approach emphasizes timely load value availability and integrates multiple enhancements, including load address prediction and prefetching.\",\n    \"Direct Inspiration\": {\n        \"b13\": 0.9,\n        \"b14\": 0.95,\n        \"b31\": 0.85,\n        \"b32\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.8,\n        \"b17\": 0.8,\n        \"b34\": 0.85\n    },\n    \"Other Inspiration\": {\n        \"b23\": 0.7,\n        \"b26\": 0.7,\n        \"b36\": 0.75\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of branch mispredictions in modern processors, specifically focusing on data-dependent branches, which are poorly correlated to prior branch history. The authors propose the 3D-Branch Overrider, a technique designed to override default predictions for Direct Data Dependent (3D) branches, where the branch direction can be computed as soon as the feeder load's value is available. The proposed method aims to re-steer the pipeline early, minimizing the performance penalties due to mispredictions.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b17\": 1.0,\n    \"b23\": 0.9,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing mispredictions in conditional branches, particularly focusing on data-dependent branches. It proposes the 3D-Branch Overrider, a technique that overrides the default branch prediction based on the feeding load's value to re-steer the processor's pipeline early, thus reducing the performance penalties associated with mispredictions. The paper highlights the significant opportunity presented by 3D-branches, which are characterized by having only one feeder load instruction and simple operations to compute the branch direction.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b17\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.75,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.6,\n    \"b39\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenge of mispredictions from data-dependent branches in modern processors. It proposes the 3D-Branch Overrider technique to override default branch predictions using feeder load values to re-steer the pipeline early, reducing the impact of mispredictions. The paper highlights the prevalence of Direct Data Dependent (3D) branches and introduces enhancements to improve prediction accuracy and performance.\",\n  \n  \"Direct Inspiration\": {\n    \"b8\": 0.9,\n    \"b17\": 0.8,\n    \"b32\": 0.7\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b23\": 0.6,\n    \"b31\": 0.6\n  },\n  \n  \"Other Inspiration\": {\n    \"b13\": 0.4,\n    \"b14\": 0.4,\n    \"b34\": 0.5\n  }\n}\n```"], "5ec49a639fced0a24b4de922": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of social biases in sentence-level representations in NLP models like BERT and ELMo. It proposes SENT-DEBIAS, a method to debias sentence representations through a four-step process: defining bias attributes, contextualizing words into sentences, estimating the bias subspace, and debiasing general sentences by removing the projection onto this bias subspace.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b24\": 0.9,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b31\": 0.7,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.5,\n    \"b48\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of debiasing sentence representations in NLP models, specifically focusing on post-hoc debiasing techniques due to the impracticality of retraining large models like BERT and GPT. The proposed method, SENT-DEBIAS, extends the HARD-DEBIAS method to sentence-level representations by contextualizing bias-attribute words using diverse sentence templates. This approach aims to reduce biases while preserving performance in downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b31\": 0.8,\n    \"b24\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the difficulty of debiasing sentence representations due to the extensive training required for state-of-the-art models and the complex variety in sentence compositions. The proposed algorithm, SENT-DEBIAS, addresses this challenge by contextualizing bias-attribute words into sentences using diverse sentence templates and then debiasing these sentence representations. Key inspirations and methods include the HARD-DEBIAS method and various sentence encoding methods such as BERT and ELMo.\",\n  \"Direct Inspiration\": [\"b6\", \"b10\"],\n  \"Indirect Inspiration\": [\"b23\", \"b24\", \"b31\", \"b33\"],\n  \"Other Inspiration\": [\"b8\", \"b30\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of debiasing sentence-level representations in NLP models, focusing on post-hoc debiasing techniques due to the impracticality of retraining large models like BERT and GPT. The proposed solution, SENT-DEBIAS, extends the HARD-DEBIAS method to sentence representations by contextualizing bias-attribute words into sentences using diverse templates from various text corpora. Experimental results show that SENT-DEBIAS reduces bias while preserving performance on downstream tasks.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1,\n        \"b24\": 0.9,\n        \"b23\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b10\": 0.7,\n        \"b31\": 0.7,\n        \"b8\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b47\": 0.5,\n        \"b28\": 0.5,\n        \"b11\": 0.5,\n        \"b7\": 0.4,\n        \"b16\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of debiasing sentence representations in NLP models, a task that is more complex than word-level debiasing due to the variety of sentence structures and the computational cost of retraining large models. The proposed method, SENT-DEBIAS, contextualizes bias-attribute words into sentences using diverse templates from various text corpora and applies a post-hoc debiasing algorithm to the sentence representations.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b10\": 1.0,\n    \"b24\": 0.9,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.7,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.5,\n    \"b35\": 0.4\n  }\n}\n```"], "5edb32399e795ec54fd81737": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of oversmoothing in deep graph convolutional networks (GCNs) and proposes a novel approach, AdaGCN, to efficiently aggregate high-order neighbor information using the AdaBoost framework. The primary motivation is to overcome the limitations of existing shallow GCN architectures and improve computational efficiency.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.9,\n    \"b22\": 0.85,\n    \"b34\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.75,\n    \"b38\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of oversmoothing in deep Graph Convolutional Networks (GCNs) and proposes AdaGCN, a novel architecture that incorporates the AdaBoost framework to efficiently extract and combine high-order neighbor information for improved prediction performance.\",\n  \"Direct Inspiration\": [\"b14\", \"b22\", \"b34\"],\n  \"Indirect Inspiration\": [\"b20\", \"b36\", \"b16\"],\n  \"Other Inspiration\": [\"b19\", \"b11\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of oversmoothing in deep Graph Convolutional Networks (GCNs) by incorporating AdaBoost into their design, resulting in a novel RNN-like architecture called AdaGCN. The proposed method efficiently extracts and combines information from different orders of neighbors, maintaining computational efficiency and improving performance over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b14\": 1,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b34\": 0.7,\n    \"b36\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b38\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of oversmoothing in deep graph convolutional networks (GCNs) and proposes a novel architecture, AdaGCN, which incorporates the AdaBoost algorithm to effectively aggregate information from different orders of neighbors. The approach refines graph convolutions, employs a two-layer fully-connected neural network instead of deep GCN layers, and leverages the SAMME algorithm for multi-class classification.\",\n    \"Direct Inspiration\": {\n        \"b10\": 0.9,\n        \"b14\": 0.9,\n        \"b22\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b12\": 0.7,\n        \"b13\": 0.7,\n        \"b19\": 0.8,\n        \"b34\": 0.7,\n        \"b20\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b25\": 0.6,\n        \"b36\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of oversmoothing in deep Graph Convolutional Networks (GCNs) and proposes a novel architecture called AdaGCN. AdaGCN incorporates the AdaBoost framework to efficiently explore and combine information from various orders of neighbors, thus achieving better classification performance and computational efficiency.\",\n    \"Direct Inspiration\": {\n        \"b10\": 0.9,\n        \"b14\": 0.8,\n        \"b20\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.7,\n        \"b11\": 0.65,\n        \"b34\": 0.75\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.6,\n        \"b36\": 0.55\n    }\n}\n```"], "53e9a232b7602d9702b3a1a9": ["```json\n{\n    \"Summary\": \"The paper focuses on reducing the startup costs of very short threads in speculative multithreading architectures, particularly by improving the Global History Register (GHR) management for better branch prediction accuracy. The proposed solution uses program counter bits to initialize the GHR, ensuring repeatable state while distinguishing between different threads.\",\n    \"Direct Inspiration\": {\n        \"b22\": 0.9,\n        \"b51\": 0.9,\n        \"b5\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b39\": 0.7,\n        \"b31\": 0.7,\n        \"b25\": 0.6,\n        \"b48\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.5,\n        \"b35\": 0.5,\n        \"b36\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing the startup cost of branch predictors for very short threads in multicore processors. It proposes a novel solution that initializes the Global History Register (GHR) using program counter bits from the new thread's starting PC, improving branch prediction accuracy without compromising performance for longer threads.\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b51\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b48\": 0.6,\n    \"b36\": 0.7,\n    \"b35\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.5,\n    \"b33\": 0.5,\n    \"b34\": 0.5,\n    \"b39\": 0.4,\n    \"b31\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of reducing startup costs for very short threads in multi-core processors, specifically focusing on the optimization of branch prediction. The primary issue is the initialization of the global history register (GHR) which impacts the effectiveness of branch predictors when threads are frequently spawned or migrated. The paper introduces techniques to initialize the GHR using the program counter (PC) of the new thread's starting point, among other methods, to improve prediction accuracy for short threads.\",\n    \"Direct Inspiration\": {\n        \"b22\": 0.9,\n        \"b51\": 0.9,\n        \"b5\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b39\": 0.7,\n        \"b31\": 0.7,\n        \"b36\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b48\": 0.5,\n        \"b25\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the high startup cost of branch predictors for very short threads in speculative multithreading architectures due to the global history register (GHR) containing irrelevant or noisy data. The proposed algorithm introduces simple architectural changes to initialize the GHR with program counter bits to improve branch prediction accuracy without compromising performance for longer threads.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b51\": 1.0,\n    \"b5\": 1.0,\n    \"b39\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b48\": 0.8,\n    \"b25\": 0.8,\n    \"b36\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b45\": 0.5,\n    \"b37\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of branch prediction in short threads within speculative multithreading architectures. It proposes using program counter bits to initialize the global history register (GHR) to improve prediction accuracy without compromising long thread prediction.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b51\": 1.0,\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b39\": 0.8,\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b48\": 0.6,\n    \"b25\": 0.6\n  }\n}\n```"], "5ebbc75d91e0119bc4e43623": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of image upscaling from downscaled images, which is an ill-posed problem due to the loss of high-frequency information during downscaling. The authors propose a novel invertible model called Invertible Rescaling Net (IRN) to capture the lost high-frequency information in the form of a latent variable z, which follows a fixed distribution. This model significantly improves the quality of upscaled images and reduces the ill-posedness of the task.\",\n  \"Direct Inspiration\": [\"b25\", \"b33\", \"b48\"],\n  \"Indirect Inspiration\": [\"b14\", \"b15\", \"b28\", \"b49\"],\n  \"Other Inspiration\": [\"b2\", \"b46\", \"b52\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of image upscaling, particularly the ill-posed nature of recovering high-resolution (HR) images from low-resolution (LR) images after downscaling. The novel method proposed is the Invertible Rescaling Net (IRN), which captures lost high-frequency information in the form of its distribution and embeds it into model parameters to facilitate better HR image reconstruction.\",\n    \"Direct Inspiration\": [\"b25\", \"b33\", \"b48\"],\n    \"Indirect Inspiration\": [\"b14\", \"b15\", \"b28\"],\n    \"Other Inspiration\": [\"b2\", \"b3\", \"b49\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the ill-posed problem of image upscaling, particularly when high-resolution images are downscaled for storage and need to be upscaled back to high resolution. The proposed solution is an invertible model called Invertible Rescaling Net (IRN), which captures the distribution of lost high-frequency information during downscaling and embeds it into the model's parameters. This novel approach leverages invertible neural networks and introduces a compact and effective objective function combining HR reconstruction loss, LR guidance loss, and distribution matching loss.\",\n\n  \"Direct Inspiration\": {\n    \"b25\": 0.9,\n    \"b33\": 0.8,\n    \"b48\": 0.9\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b2\": 0.6\n  },\n  \n  \"Other Inspiration\": {\n    \"b49\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the ill-posed nature of the image upscaling task, which arises from the non-injective nature of image downscaling. The paper proposes a novel invertible model called Invertible Rescaling Net (IRN) to mitigate this issue by capturing lost high-frequency information and embedding it into the model's parameters. The model aims to produce visually pleasing low-resolution (LR) images and use case-agnostic latent variables for efficient upscaling.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1,\n    \"b33\": 1,\n    \"b48\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b28\": 0.8,\n    \"b49\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b35\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the ill-posed nature of image upscaling, which results from the loss of high-frequency information during the downscaling process. The authors propose a novel invertible model called Invertible Rescaling Net (IRN) to embed the lost high-frequency information into a latent variable, thus mitigating the ill-posedness. This model is designed to be both lightweight and efficient, with specific training objectives to ensure high-quality reconstruction of high-resolution (HR) images from low-resolution (LR) images.\",\n  \n  \"Direct Inspiration\": {\n    \"b25\": 0.9,\n    \"b33\": 0.8,\n    \"b48\": 0.85\n  },\n  \n  \"Indirect Inspiration\": {\n    \"b14\": 0.75,\n    \"b15\": 0.7,\n    \"b2\": 0.65\n  },\n  \n  \"Other Inspiration\": {\n    \"b49\": 0.6,\n    \"b3\": 0.55\n  }\n}\n```"], "5eede0b791e0116a23aafe75": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"Developing efficient and effective methods for learning on continuous-time dynamic graphs, which are more complex due to their evolving nature compared to static or discrete-time dynamic graphs.\",\n    \"inspirations\": \"The paper introduces Temporal Graph Networks (TGNs) to handle continuous-time dynamic graphs, proposing novel training strategies for efficient learning and memory modules to capture long-term dependencies.\"\n  },\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b65\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.9,\n    \"b3\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.8,\n    \"b58\": 0.8,\n    \"b45\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning on continuous-time dynamic graphs by introducing the Temporal Graph Network (TGN) framework. TGNs allow for efficient and scalable processing of temporal data by incorporating memory modules, novel training strategies, and various embedding techniques. The core contributions include a generic inductive framework, a novel training strategy for sequential data, a detailed ablation study, and state-of-the-art performance on multiple tasks and datasets.\",\n  \"Direct Inspiration\": {\n    \"b31\": 0.9,\n    \"b35\": 0.9,\n    \"b65\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b46\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b58\": 0.6,\n    \"b45\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of learning on continuous-time dynamic graphs, which are essential for modeling real-life systems such as social networks and biological interactomes. The proposed Temporal Graph Networks (TGNs) framework provides an efficient way to handle continuous-time dynamic graphs by using a novel memory module, message functions, and message aggregation strategies. The paper also introduces a novel training strategy that significantly speeds up the learning process while maintaining accuracy.\",\n    \"Direct Inspiration\": {\n        \"b35\": 1,\n        \"b65\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b31\": 0.9,\n        \"b46\": 0.8,\n        \"b2\": 0.8,\n        \"b58\": 0.8,\n        \"b45\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.7,\n        \"b26\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the primary challenge of learning on continuous-time dynamic graphs, which are more representative of real-world systems compared to static or discrete-time dynamic graphs. The key contributions include the proposal of Temporal Graph Networks (TGNs), a novel training strategy for efficient parallel processing, and demonstrating state-of-the-art performance on various tasks while being faster than previous methods.\",\n  \"Direct Inspiration\": {\n    \"b35\": 0.9,\n    \"b65\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b46\": 0.6,\n    \"b45\": 0.6,\n    \"b58\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of learning on continuous-time dynamic graphs by proposing a generic inductive framework called Temporal Graph Networks (TGNs). This framework innovatively combines memory modules, message functions, and temporal embedding techniques to handle evolving graphs efficiently. The framework leads to significant performance improvements in speed and accuracy over previous methods.\",\n  \"Direct Inspiration\": {\n    \"b35\": 1.0,\n    \"b65\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b45\": 0.7,\n    \"b58\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b46\": 0.6\n  }\n}\n```"], "558b3e9384ae84d265c24c24": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges in scaling cache coherence directories for chip-multiprocessors (CMPs) with hundreds to thousands of cores. The proposed Scalable Coherence Directory (SCD) scheme aims to efficiently scale to thousands of cores while maintaining exact sharer representation and incurring negligible invalidations. Key inspirations include leveraging highly-associative cache techniques from ZCache and Cuckoo Directory.\",\n    \"Direct Inspiration\": [\"b9\", \"b24\"],\n    \"Indirect Inspiration\": [\"b0\", \"b5\", \"b12\", \"b17\"],\n    \"Other Inspiration\": [\"b1\", \"b29\", \"b30\", \"b34\", \"b35\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of implementing scalable and efficient directory-based cache coherence in chip-multiprocessors (CMPs) with hundreds to thousands of cores. It introduces the Scalable Coherence Directory (SCD), which leverages efficient highly-associative caches to provide an exact and scalable representation of sharer sets, reducing area, energy, and latency overheads while minimizing directory-induced invalidations.\",\n  \"Direct Inspiration\": [\n    \"b9\",\n    \"b24\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b17\",\n    \"b30\",\n    \"b32\",\n    \"b34\"\n  ],\n  \"Other Inspiration\": [\n    \"b12\",\n    \"b35\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is implementing scalable and efficient coherent cache hierarchies in CMPs with hundreds or thousands of cores. The proposed solution, Scalable Coherence Directory (SCD), leverages highly-associative cache designs (ZCache and Cuckoo Directory) and introduces a novel directory scheme to efficiently track sharers, reduce invalidations, and maintain performance and energy efficiency.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b24\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b30\": 0.8,\n    \"b34\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of implementing scalable directory-based cache coherence protocols for chip-multiprocessors (CMPs) with hundreds to thousands of cores. The proposed solution, the Scalable Coherence Directory (SCD), aims to maintain efficient sharer information representation while minimizing area, energy, and latency overheads. The SCD leverages recent advancements in highly-associative caches and introduces a variable-size sharer set representation to achieve these goals.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b24\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b35\": 0.6,\n    \"b34\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b30\": 0.5,\n    \"b32\": 0.5,\n    \"b26\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of implementing scalable directory-based cache coherence protocols for chip-multiprocessors (CMPs) with hundreds to thousands of cores. The proposed Scalable Coherence Directory (SCD) aims to efficiently track sharers while minimizing overheads and invalidations, leveraging recent innovations such as ZCache and Cuckoo Directory.\",\n  \"Direct Inspiration\": [\"b24\", \"b9\"],\n  \"Indirect Inspiration\": [],\n  \"Other Inspiration\": [\"b17\", \"b1\", \"b12\", \"b30\", \"b34\"]\n}\n```"], "5a260c8117c44a4ba8a30adf": ["```json\n{\n    \"Summary\": \"The paper discusses the challenges of incorporating graph-structured information into machine learning models. The primary focus is on representation learning approaches that embed nodes or subgraphs into low-dimensional vector spaces, optimizing these embeddings to reflect the structure of the original graph. The paper introduces an encoder-decoder framework to unify various node embedding methods and highlights the need for scalable approaches inspired by recent advancements in deep learning.\",\n    \"Direct Inspiration\": {\n        \"b27\": 0.9,\n        \"b46\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.75,\n        \"b3\": 0.7,\n        \"b8\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b32\": 0.6,\n        \"b42\": 0.55\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is the difficulty in incorporating information about graph structure into a machine learning model. The proposed solution is to use representation learning approaches to learn embeddings that encode structural information about the graph. These embeddings can then be used as feature inputs for downstream machine learning tasks. The paper reviews various recent advancements in representation learning on graphs and develops a unified conceptual framework for describing these approaches.\",\n    \"Direct Inspiration\": {\n        \"b27\": 1.0,\n        \"b46\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.8,\n        \"b8\": 0.8,\n        \"b3\": 0.7,\n        \"b44\": 0.7\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of incorporating graph-structured information into machine learning models, particularly emphasizing representation learning approaches that embed nodes or subgraphs into low-dimensional vector spaces. The primary motivation is to move beyond traditional hand-engineered feature methods by using data-driven techniques that optimize embeddings to reflect the structural information of graphs.\",\n    \"Direct Inspiration\": [\"b3\", \"b27\", \"b46\"],\n    \"Indirect Inspiration\": [\"b5\", \"b39\", \"b57\"],\n    \"Other Inspiration\": [\"b0\", \"b8\", \"b36\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating graph structure information into machine learning models, particularly focusing on learning low-dimensional node embeddings that encode structural information about the graph. The proposed solution involves using an encoder-decoder framework to learn these embeddings through optimization techniques.\",\n  \"Direct Inspiration\": {\n    \"reference_number\": [\"b27\", \"b46\"]\n  },\n  \"Indirect Inspiration\": {\n    \"reference_number\": [\"b5\", \"b57\", \"b39\"]\n  },\n  \"Other Inspiration\": {\n    \"reference_number\": [\"b32\", \"b42\", \"b37\", \"b6\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges of incorporating graph-structured information into machine learning models, highlighting the limitations of traditional methods and the rise of representation learning approaches. It introduces an encoder-decoder framework for node and subgraph embeddings, focusing on methods that are scalable and inspired by advancements in deep learning.\",\n  \"Direct Inspiration\": [\"b27\", \"b46\"],\n  \"Indirect Inspiration\": [\"b0\", \"b8\"],\n  \"Other Inspiration\": [\"b5\", \"b57\", \"b39\"]\n}\n```"], "53e9b6c4b7602d970424e729": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving last-level cache (LLC) performance and efficiency in modern processors with minimal hardware cost. The authors propose a novel bypass technique named Optimal Bypass Monitor (OBM) to dynamically learn and predict the behavior of the optimal bypass, aiming to retain blocks with high temporal locality in the cache and avoid thrashing caused by distant reuse blocks.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b12\": 0.9,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b28\": 0.5,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper identifies key challenges in LLC management, specifically the poor performance of LRU policy and the potential for bypass techniques to improve cache performance and efficiency. The proposed algorithm, Optimal Bypass Monitor (OBM), dynamically learns and predicts optimal bypass behavior to make bypass decisions with minimal hardware modification.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1.0,\n    \"b12\": 0.9,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.6,\n    \"b32\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving Last-Level Cache (LLC) performance and efficiency with minimal hardware cost. It proposes a technique called Optimal Bypass Monitor (OBM) which dynamically learns and predicts the behavior of the optimal bypass to make efficient bypass decisions. The key contributions include the introduction of the Replacement History Table (RHT) and Bypass Decision Counter Table (BDCT) to predict bypass decisions, making OBM both thread-aware and prefetch-aware.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1,\n        \"b12\": 1,\n        \"b19\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b20\": 0.8,\n        \"b21\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b27\": 0.7,\n        \"b28\": 0.7,\n        \"b32\": 0.7\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving the performance and efficiency of Last-Level Caches (LLCs) with minimal hardware cost. It proposes a novel bypass technique called Optimal Bypass Monitor (OBM) that learns and predicts the behavior of the optimal bypass to make bypass decisions. OBM is evaluated with several replacement policies, demonstrating significant performance improvements over existing methods while requiring minimal additional storage.\",\n    \"Direct Inspiration\": [\"b2\", \"b12\", \"b19\"],\n    \"Indirect Inspiration\": [\"b10\", \"b20\", \"b21\"],\n    \"Other Inspiration\": [\"b6\", \"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving last-level cache (LLC) performance and efficiency with minimal hardware cost through a new bypass technique called Optimal Bypass Monitor (OBM). The proposed technique dynamically learns and predicts the behavior of the optimal bypass to make bypass decisions.\",\n  \"Direct Inspiration\": [\"b2\"],\n  \"Indirect Inspiration\": [\"b12\", \"b19\"],\n  \"Other Inspiration\": [\"b6\", \"b10\", \"b27\", \"b28\", \"b32\"]\n}\n```"], "558a49f584ae84d265bcb7dc": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is reducing capacity misses in cache memory to improve application data locality. The paper introduces StatCache, a novel probabilistic approach to estimate miss ratios of fully-associative caches, which is more efficient than traditional methods like time sampling and set sampling.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inefficiency in locating and explaining cache-related performance bottlenecks, particularly capacity misses, in modern high-performance computers. The proposed algorithm, StatCache, is a novel probabilistic approach to estimate miss ratios of fully-associative caches, reducing the need for time-consuming simulations. StatCache focuses on improving application data locality by using a probabilistic model of the cache and sampling reuse distances of memory references.\",\n  \"Direct Inspiration\": [\"b10\", \"b19\"],\n  \"Indirect Inspiration\": [\"b4\", \"b5\"],\n  \"Other Inspiration\": [\"b1\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing capacity cache misses in memory-intensive applications by improving data locality. It proposes StatCache, a novel probabilistic approach to estimate miss ratios of fully-associative caches. StatCache uses a probabilistic model based on run-time statistics to achieve accurate results with minimal sampling.\",\n  \"Direct Inspiration\": [\"b10\", \"b19\"],\n  \"Indirect Inspiration\": [\"b4\", \"b5\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving application data locality to reduce capacity misses in cache memories. It proposes StatCache, a novel probabilistic approach to estimate miss ratios of fully-associative caches. The method involves collecting a cache-size-independent statistic at run-time and applying a probabilistic model during post-processing to estimate cache miss ratios. This approach is designed to be more efficient and accurate compared to traditional simulation methods.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b5\": 1,\n    \"b10\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b1\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing capacity misses in cache memory by improving data locality, which is critical for enhancing the performance of memory-intensive applications. The authors propose StatCache, a novel probabilistic approach to estimate miss ratios of fully-associative caches, which significantly reduces simulation time compared to traditional methods like time sampling and set sampling.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1.0,\n    \"b19\": 1.0,\n    \"b4\": 1.0,\n    \"b5\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b1\": 0.8\n  }\n}\n```"], "5edf5ddc91e011bc656defd7": ["```json\n{\n  \"Summary\": \"The paper addresses the efficiency bottleneck in Transformer models caused by the self-attention mechanism, which incurs O(n^2) complexity with respect to sequence length. The paper proposes a novel approach inspired by the observation that self-attention is low rank, introducing a mechanism that reduces self-attention to an O(n) operation through linear projections and low-rank factorization.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b12\": 0.85,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b5\": 0.75,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing Transformer models to avoid the quadratic complexity of self-attention while maintaining strong performance. The authors propose a novel approach inspired by the observation that self-attention can be approximated by a low-rank matrix, thereby reducing self-attention to an O(n) operation through linear projections.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b12\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.75,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b16\": 0.55,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the efficiency bottleneck in Transformer models due to the self-attention mechanism, which has quadratic complexity with respect to sequence length. The authors propose a novel approach that leverages the low-rank nature of self-attention to reduce its complexity to O(n) by decomposing the attention into multiple smaller attentions through linear projections.\",\n  \"Direct Inspiration\": [\"b5\", \"b12\"],\n  \"Indirect Inspiration\": [\"b6\", \"b22\", \"b15\"],\n  \"Other Inspiration\": [\"b16\", \"b30\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the inefficiency of the self-attention mechanism in Transformer models, which has a quadratic complexity with respect to sequence length. The novel contribution is a low-rank approximation approach that reduces self-attention to an O(n) operation in both space and time complexity by decomposing the original attention into multiple smaller attentions through linear projections.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b12\": 1.0,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.8,\n    \"b1\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the inefficiency of the self-attention mechanism in Transformer models, which leads to high computational complexity (O(n^2)) in time and space. The authors propose a novel approach inspired by the observation that self-attention is low rank, which allows them to reduce the complexity to O(n) through linear projections. This method is theoretically and empirically validated to show comparable or better performance with significant training and inference speedups.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b12\": 0.9,\n    \"b6\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b1\": 0.4\n  }\n}\n```"], "53e9a4b1b7602d9702dd0241": ["```json\n{\n    \"Summary\": \"The paper discusses the formal verification methodology applied to the Intel Core i7 execution cluster, replacing traditional simulation-based approaches. The key challenges include verifying over 2700 distinct microinstructions, managing the complexity of different verification tasks, and ensuring the timely feedback and robustness of the verification code.\",\n    \"Direct Inspiration\": {\n        \"b2\": 1,\n        \"b11\": 1,\n        \"b12\": 1,\n        \"b14\": 1,\n        \"b20\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.9,\n        \"b10\": 0.8,\n        \"b19\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.7,\n        \"b17\": 0.7,\n        \"b18\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper discusses the challenges and the novel methodology of using formal verification as the primary validation approach for the Intel Core i7 execution cluster. The key challenges addressed are the limitations of traditional testing and simulation, and the paper introduces the use of symbolic simulation and inductive invariants to extend formal verification to control logic and state verification.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b11\": 0.9,\n        \"b12\": 0.9,\n        \"b14\": 0.9,\n        \"b20\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b16\": 0.8,\n        \"b5\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.7,\n        \"b18\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the use of formal verification as the primary validation vehicle for the execution cluster of Intel Core i7, including full datapath, control, and state verification. The authors highlight the challenges of replacing traditional testing with formal verification, emphasizing the role of human verifiers and symbolic simulation, particularly in dealing with feedback-intensive problems using inductive invariants.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b11\": 1,\n    \"b12\": 1,\n    \"b14\": 1\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b5\": 0.8,\n    \"b20\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges and inspirations in using formal verification as the primary validation method for the Intel Core i7 execution cluster. The primary challenges include replacing traditional simulation-based testing with formal verification, managing the complexity of symbolic simulation for various micro-instructions, and ensuring timely feedback and maintenance of the verification code. Key inspirations come from previous works on formal verification of specific datapaths and the use of inductive invariants and symbolic simulation.\",\n  \"Direct Inspiration\": [\"b11\", \"b12\", \"b14\"],\n  \"Indirect Inspiration\": [\"b2\", \"b3\", \"b5\"],\n  \"Other Inspiration\": [\"b20\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper discusses the formal verification of the execution cluster (EXE) in Intel Core i7 processors, replacing traditional testing methods with formal verification. The primary challenges include verifying the large number of distinct microinstructions and ensuring the correctness of control, state, and bypass mechanisms. The authors use symbolic simulation and inductive invariants to tackle these challenges.\",\n    \"Direct Inspiration\": [\"b11\", \"b12\", \"b14\", \"b20\"],\n    \"Indirect Inspiration\": [\"b2\", \"b5\"],\n    \"Other Inspiration\": [\"b7\", \"b10\", \"b17\"]\n}\n```"], "57a4e91dac44365e35c9886f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating a lightweight CPU with a hardware accelerator within an FPGA overlay framework to enhance performance and maintain software compatibility. The proposed solution is a 4-stage pipeline RISC-V soft processor designed for efficiency and portability, tightly coupled with the accelerator to minimize control switching overhead. Key contributions include the novel architecture design and the demonstration of its benefits through various real-life application benchmarks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b4\": 0.9,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b28\": 0.6,\n    \"b29\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b25\": 0.4,\n    \"b26\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently integrating hardware accelerators with software systems through a tightly-coupled architecture, using a lightweight, open-source soft processor based on the RISC-V RV32I ISA. The main contribution lies in demonstrating the benefits of this integration within an FPGA overlay framework to improve design productivity, resource consumption, and compatibility across different FPGA platforms.\",\n  \"Direct Inspiration\": {\n    \"reference\": [\"b3\", \"b4\"]\n  },\n  \"Indirect Inspiration\": {\n    \"reference\": [\"b5\", \"b6\"]\n  },\n  \"Other Inspiration\": {\n    \"reference\": [\"b9\", \"b10\", \"b11\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": [\n      \"Limited flexibility of hardware accelerators\",\n      \"Need for efficient CPU to control accelerators within an overlay framework\",\n      \"Maintaining compatibility with the software system\",\n      \"Reducing resource consumption while maintaining efficiency\"\n    ],\n    \"Inspirations\": [\n      \"Use of FPGA overlays to improve productivity\",\n      \"Tightly-coupling a lightweight CPU with hardware accelerator\",\n      \"RISC-V's openness and simplicity for instruction set architecture\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b4\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b6\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b0\": 0.7,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of integrating a flexible and efficient CPU with hardware accelerators within an FPGA overlay framework. It proposes the use of a small, open-source RISC-V soft processor tightly-coupled with the hardware accelerator to enhance control and maintain compatibility with software systems. The primary contributions include the design of a 4-stage pipeline soft processor and the tightly-coupled architecture, which are evaluated for performance and resource consumption.\",\n    \"Direct Inspiration\": {\n        \"b3\": 1.0,\n        \"b4\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b6\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b11\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of integrating a flexible and efficient CPU with a hardware accelerator in an FPGA overlay framework. It proposes a 4-stage RISC-V soft processor tightly-coupled with an accelerator to achieve fine-grained control and portability across various FPGA platforms. Key contributions include the design of the soft processor, custom instruction-set extensions, and the tightly-coupled architecture utilizing the MURAC model.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b4\": 1.0,\n    \"b5\": 1.0,\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b2\": 0.8,\n    \"b28\": 0.8,\n    \"b29\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```"], "5ea6adfa91e011a546871d52": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in Chinese Named Entity Recognition (NER), particularly the complexity introduced by word segmentation. The proposed solution, FLAT (Flat Lattice Transformer), leverages a novel position encoding method for lattice structures, allowing for the effective use of transformers to model long-distance dependencies and interactions between characters and potential words in the lattice structure.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b6\": 0.75,\n    \"b17\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b16\": 0.7,\n    \"b11\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of Chinese Named Entity Recognition (NER), particularly the issues related to word segmentation and long-distance dependencies. The authors propose a novel model called FLAT (Flat Lattice Transformer) that utilizes a unique position encoding to handle the lattice structure of Chinese characters and words, leveraging the self-attention mechanism of Transformers to improve performance and inference speed.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.9,\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of Chinese Named Entity Recognition (NER), which is complicated by the need for word segmentation. The authors propose FLAT (Flat LAttice Transformer) to leverage a lattice structure using a novel position encoding method inspired by the Transformer model. The lattice structure allows the model to avoid error propagation from word segmentation and to fully utilize GPU parallel computation.\",\n  \"Direct Inspiration\": [\"b19\", \"b22\"],\n  \"Indirect Inspiration\": [\"b25\", \"b6\", \"b17\"],\n  \"Other Inspiration\": [\"b5\", \"b11\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the complexity of Chinese Named Entity Recognition (NER) due to the need for word segmentation and proposes a novel solution using a Flat Lattice Transformer (FLAT). The primary challenge is to design a model that can leverage word information without the error propagation of word segmentation and efficiently handle long-distance dependencies. The proposed FLAT model uses an ingenious position encoding for the lattice structure, allowing the Transformer to fully model the lattice input. This enables characters to interact directly with any potential word, improving performance and inference speed over existing methods.\",\n  \"Direct Inspiration\": [\"b19\", \"b22\"],\n  \"Indirect Inspiration\": [\"b25\", \"b6\", \"b17\"],\n  \"Other Inspiration\": [\"b5\", \"b16\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of Chinese Named Entity Recognition (NER) by proposing a novel model, FLAT (Flat Lattice Transformer). The primary challenges include handling the complexities of word segmentation in Chinese and modeling long-distance dependencies. The proposed algorithm leverages a lattice structure and introduces a position encoding scheme for the lattice, enabling the use of Transformer architecture to effectively model interactions in the lattice.\",\n  \"Direct Inspiration\": [\"b19\", \"b22\"],\n  \"Indirect Inspiration\": [\"b6\", \"b17\", \"b25\"],\n  \"Other Inspiration\": [\"b5\"]\n}\n```"], "5da1a6d447c8f7664606888c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of temporal data prefetching by introducing Triage, a prefetcher that does not use off-chip metadata. It repurposes a portion of the last-level cache (LLC) for storing prefetcher metadata and uses a dynamic cache partitioning scheme to manage on-chip metadata efficiently. The paper highlights significant reductions in off-chip traffic, energy consumption, and hardware complexity while maintaining or improving performance, especially in bandwidth-constrained environments.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b46\": 1,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.7,\n    \"b44\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b43\": 0.6,\n    \"b0\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high metadata storage requirements and off-chip metadata traffic in temporal data prefetching. The proposed Triage prefetcher repurposes a portion of the LLC to store metadata on-chip, using a compact and efficient table-based organization, and employs the Hawkeye replacement policy to manage metadata dynamically.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b24\": 0.9,\n    \"b46\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.85,\n    \"b44\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents the Triage prefetcher, a novel temporal data prefetcher that repurposes a portion of the last-level cache (LLC) for storing metadata, avoiding off-chip metadata. It introduces a dynamic cache partitioning scheme and uses the Hawkeye replacement policy to manage on-chip metadata effectively. Triage aims to reduce off-chip traffic, energy consumption, and hardware complexity, offering significant performance benefits in bandwidth-constrained environments.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b46\": 1.0,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b33\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b44\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of hiding long latencies of DRAM accesses in a bandwidth-constrained environment by proposing Triage, a temporal data prefetcher that eliminates the need for off-chip metadata. Triage repurposes a portion of the LLC for storing prefetcher metadata and uses an adaptive policy for dynamically provisioning the size of the metadata store. The key inspiration comes from the state-of-the-art in temporal prefetching, particularly focusing on reducing off-chip metadata traffic, energy consumption, and hardware complexity.\",\n  \"Direct Inspiration\": [\"b23\", \"b46\"],\n  \"Indirect Inspiration\": [\"b24\", \"b44\"],\n  \"Other Inspiration\": [\"b26\", \"b33\", \"b39\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of high metadata traffic, energy consumption, and hardware complexity associated with temporal data prefetching by introducing Triage, a novel prefetcher that stores all metadata on-chip. It repurposes a portion of the LLC for metadata storage, utilizes the Hawkeye replacement policy for efficient metadata management, and employs a dynamic cache partitioning scheme to optimize metadata storage.\",\n  \"Direct Inspiration\": [\"b23\", \"b24\", \"b46\"],\n  \"Indirect Inspiration\": [\"b26\", \"b44\"],\n  \"Other Inspiration\": []\n}\n```"], "5def6ca63a55ac6095fe0607": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"handling very large graphs\",\n      \"limitations of DRAM on most machines\",\n      \"improving system support for non-vertex programs\",\n      \"efficient graph partitioning for triangle counting in distributed systems\"\n    ],\n    \"inspirations\": [\n      \"developing a novel application-agnostic graph partitioning strategy\",\n      \"using distributed computing to address triangle counting\",\n      \"eliminating almost all communication for distributed triangle counting\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b15\", \"b27\", \"b28\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b4\", \"b20\", \"b25\", \"b26\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b6\", \"b13\", \"b18\", \"b29\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the efficient implementation of triangle counting on large graphs, dealing with memory limitations, and the lack of effective systems support for non-vertex programs. The paper proposes a novel application-agnostic graph partitioning strategy for distributed triangle counting that minimizes communication between hosts. This strategy involves distributing edges among hosts, creating proxy vertices, and employing a static partitioning policy to perform local triangle counting independently on each host, followed by aggregating local counts to get the total triangle count.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b27\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.8,\n    \"b36\": 0.7,\n    \"b35\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in efficiently counting triangles in very large graphs using distributed computing. It introduces a novel application-agnostic graph partitioning strategy that minimizes communication between machines during distributed triangle counting.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b27\": 0.9,\n    \"b28\": 0.9,\n    \"b4\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b18\": 0.7,\n    \"b20\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in triangle counting within large undirected graphs by proposing a novel application-agnostic graph partitioning strategy for distributed computing. This strategy minimizes communication between machines, allowing each machine to count triangles independently and aggregate results at the end. The method is implemented on a distributed multi-GPU platform and shows significant improvement over existing solutions.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b27\": 0.9,\n    \"b28\": 0.9,\n    \"b4\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b20\": 0.7,\n    \"b21\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b18\": 0.5,\n    \"b19\": 0.5,\n    \"b25\": 0.5,\n    \"b26\": 0.5,\n    \"b29\": 0.4,\n    \"b30\": 0.4,\n    \"b31\": 0.4,\n    \"b32\": 0.4,\n    \"b33\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of triangle counting in large undirected graphs using distributed computing. It introduces a novel application-agnostic graph partitioning strategy that eliminates almost all communication for distributed triangle counting, enabling efficient processing on a distributed multi-GPU platform.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b27\": 1,\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b36\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.8\n  }\n}\n```"], "5cf48a33da56291d5829579e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of neural architecture search (NAS) by proposing a generic optimization framework based on stochastic relaxation. Key contributions include a step-size adaptation mechanism for stochastic natural gradient ascent, a robust framework for arbitrary types of architecture variables, and improvements in optimization speed and robustness.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b22\": 0.8,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the optimization of neural network architectures for previously unseen tasks via Neural Architecture Search (NAS). The proposed solution is a generic optimization framework for one-shot NAS based on stochastic relaxation. This framework generalizes existing work to handle various types of architecture variables and introduces a step-size adaptation mechanism to improve robustness and optimization speed.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b1\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b16\": 0.7,\n    \"b22\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b19\": 0.4,\n    \"b6\": 0.3,\n    \"b4\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of robust neural architecture search (NAS), focusing on optimizing the architecture and weights simultaneously through a stochastic relaxation framework. The proposed method generalizes previous works to handle various types of architecture variables and introduces a step-size adaptation mechanism for the stochastic natural gradient ascent to improve optimization speed and robustness.\",\n  \"Direct Inspiration\": {\n    \"b18\": 1.0,\n    \"b9\": 0.9,\n    \"b22\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper revolve around developing a robust and efficient neural architecture search (NAS) framework that can handle various types of architecture variables, including categorical, ordinal, and their mixtures. The proposed framework, based on stochastic relaxation and adaptive stochastic natural gradient ascent, aims to reduce sensitivity to input parameters, improve optimization speed, and leverage parallel computing architectures.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b1\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.75,\n    \"b9\": 0.7,\n    \"b22\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.55,\n    \"b3\": 0.5,\n    \"b8\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing neural architecture search (NAS) by proposing a generic optimization framework for one-shot NAS based on stochastic relaxation. The framework generalizes previous work to handle various types of architecture variables and introduces a step-size adaptation mechanism to improve robustness and optimization speed.\",\n  \"Direct Inspiration\": {\n    \"b18\": 0.9,\n    \"b1\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b16\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b3\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```"], "53e99967b7602d97021ac42b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of memory disambiguation in superscalar processors. It proposes a store distance based feedback-directed mechanism to accurately determine memory dependences at runtime without requiring large on-chip memory. The novel method focuses on profiling store distances for load instructions and uses these distances to enhance load speculation accuracy and performance.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.7,\n    \"b8\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b11\": 0.5,\n    \"b15\": 0.5,\n    \"b16\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of memory disambiguation in modern superscalar processors by proposing a store distance-based mechanism. This mechanism aims to accurately determine memory dependences without requiring large on-chip memory, thus improving load speculation performance.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the memory disambiguation problem in superscalar processors by proposing a store distance-based mechanism. This technique aims to improve load speculation accuracy without requiring large on-chip memory. It leverages store distance, a dynamic memory metric, to predict memory dependences, outperforming previous access distance-based methods and store set techniques with significantly less chip space.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b5\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of memory disambiguation in out-of-order superscalar processors. It proposes a feedback-directed mechanism based on memory distance analysis, specifically using a concept called store distance. This approach aims to improve load speculation without requiring large on-chip memory. The proposed method shows superior performance compared to previous access distance-based and store set techniques, using significantly less chip space.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b26\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the problem of memory disambiguation in superscalar processors, specifically scheduling load instructions early without causing memory order violations. The proposed solution is a compiler/microarchitecture cooperative scheme based on a novel concept called store distance. This method aims to accurately determine memory dependences at runtime with minimal on-chip memory requirements.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.9,\n    \"b15\": 0.8,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b13\": 0.6,\n    \"b28\": 0.6\n  }\n}\n```"], "5ddcf7f53a55ac1c5e8cce13": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating meta-learning with neural architecture search (NAS) to effectively handle few-shot learning tasks. The proposed method, METANAS, allows for rapid adaptation to new tasks by learning task-specific architectures with minimal data and computation.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b24\": 1.0,\n    \"b32\": 1.0,\n    \"b36\": 1.0,\n    \"b29\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b49\": 0.6,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.5,\n    \"b38\": 0.5,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the integration of Neural Architecture Search (NAS) and meta-learning to adapt neural architectures to novel tasks with few data points and minimal computational cost. The proposed algorithm, METANAS, combines gradient-based meta-learning with NAS to generate task-specific architectures efficiently. The novel contributions include combining meta-learning with NAS, a soft-pruning mechanism to avoid retraining, and achieving state-of-the-art results on few-shot learning benchmarks.\",\n  \"Direct Inspiration\": [\"b15\", \"b32\", \"b36\"],\n  \"Indirect Inspiration\": [\"b24\", \"b29\"],\n  \"Other Inspiration\": [\"b13\", \"b20\", \"b49\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of combining Neural Architecture Search (NAS) with meta-learning to create a method that quickly adapts both architectures and weights to new tasks with limited data. The proposed method, METANAS, integrates gradient-based NAS and meta-learning to generate task-specific architectures without requiring extensive retraining.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b24\": 0.85,\n    \"b29\": 0.8,\n    \"b32\": 0.9,\n    \"b36\": 0.85\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {\n    \"b26\": 0.75,\n    \"b45\": 0.75,\n    \"b49\": 0.75,\n    \"b20\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying Neural Architecture Search (NAS) to few-shot learning by integrating NAS with meta-learning to rapidly adapt architectures to new tasks using limited data. The proposed method, METANAS, combines gradient-based meta-learning methods with NAS to generate task-specific architectures without requiring retraining, improving efficiency and performance.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b32\": 1.0,\n    \"b36\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.8,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b45\": 0.5,\n    \"b49\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying Neural Architecture Search (NAS) in few-shot learning settings. The proposed algorithm, METANAS, combines model-agnostic, gradient-based meta-learning methods with gradient-based NAS methods to generate task-specific architectures based on few data points, without requiring extensive retraining.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b32\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.9,\n    \"b36\": 0.9,\n    \"b29\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b26\": 0.7,\n    \"b45\": 0.7\n  }\n}\n```"], "599c7ea4601a182cd28b81a7": ["```json\n{\n  \"Summary\": \"The paper addresses the significant challenge of managing last-level caches efficiently, considering both theoretical and practical approaches. The proposed EVA replacement policy is designed to bridge the gap between theory and practice by integrating insights from planning theory and economic value added (EVA) to make optimal cache replacement decisions under uncertainty.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1.0,\n    \"b26\": 1.0,\n    \"b34\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b20\": 0.8,\n    \"b15\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b9\": 0.6,\n    \"b24\": 0.6,\n    \"b29\": 0.6,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently managing last-level caches, which consume significant resources. It proposes the EVA replacement policy, which aims to make optimal use of limited information and reconcile hit probability with time spent in the cache, drawing on planning theory and cost-benefit analysis.\",\n  \"Direct Inspiration\": {\n    \"Inspired by MIN\": [\"b16\", \"b26\", \"b34\"]\n  },\n  \"Indirect Inspiration\": {\n    \"Hawkeye\": [\"b15\"],\n    \"SHiP\": [\"b38\"],\n    \"DRRIP\": [\"b16\"],\n    \"PDP\": [\"b13\"],\n    \"IRGD\": [\"b34\"]\n  },\n  \"Other Inspiration\": [\n    \"b1\",\n    \"b10\",\n    \"b20\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge outlined in the paper is managing last-level caches efficiently given their significant resource consumption. The algorithm proposed by the authors, EVA, addresses this challenge by using planning theory to develop a practical cache replacement policy that balances hit probability and time spent in the cache.\",\n    \"Direct Inspiration\": {\n        \"b16\": 0.9,\n        \"b19\": 0.85,\n        \"b26\": 0.8,\n        \"b34\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.75,\n        \"b10\": 0.7,\n        \"b13\": 0.7,\n        \"b15\": 0.7,\n        \"b20\": 0.7,\n        \"b29\": 0.7,\n        \"b38\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b9\": 0.6,\n        \"b24\": 0.6,\n        \"b25\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Efficient last-level cache management under uncertainty\",\n    \"Inspirations\": \"Draws from both theoretical models such as Belady's MIN and empirical observations of dynamic access patterns\"\n  },\n  \"Direct Inspiration\": [\"b1\", \"b9\", \"b16\", \"b19\", \"b20\", \"b26\", \"b34\"],\n  \"Indirect Inspiration\": [\"b10\", \"b13\", \"b15\", \"b25\", \"b38\"],\n  \"Other Inspiration\": [\"b3\", \"b8\", \"b14\", \"b17\", \"b23\", \"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently managing last-level caches in processors, highlighting the limitations of both theoretical and empirical cache replacement policies. The proposed EVA (Economic Value Added) replacement policy aims to reconcile hit probability and time spent in the cache by using a probabilistic approach to make informed replacement decisions.\",\n  \"Direct Inspiration\": {\n    \"b16\": 1,\n    \"b26\": 1,\n    \"b34\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b10\": 0.6,\n    \"b13\": 0.7,\n    \"b15\": 0.7,\n    \"b20\": 0.6,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b25\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```"], "5d3044363a55ac8b59feaf5b": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of mitigating Spectre variant 2 (Branch Target Injection) in processors, focusing on the performance overhead caused by retpolines. The authors propose JumpSwitches, an alternative mechanism leveraging indirect call promotion to mitigate Spectre attacks while improving performance. JumpSwitches dynamically learn and adapt to execution targets using a lightweight mechanism within the Linux kernel, thereby reducing overhead and maintaining security.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b8\": 0.9,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.7,\n    \"b37\": 0.7,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b39\": 0.5,\n    \"b53\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of mitigating Spectre variant 2 attacks, particularly focusing on the performance overheads caused by retpolines. The proposed solution, JumpSwitches, leverages indirect call promotion to transform indirect calls into conditional direct calls, thereby optimizing performance while maintaining security against Spectre attacks.\",\n  \"Direct Inspiration\": {\n    \"b48\": 1.0,\n    \"b29\": 0.9,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b19\": 0.7,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b39\": 0.6,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed by the paper is mitigating Spectre variant 2 attacks while optimizing the performance of indirect branches. The proposed solution, JumpSwitch, leverages indirect call promotion to transform indirect calls into conditional direct calls, thereby improving efficiency and security. Key inspirations and previous works mentioned include retpolines and various compiler-based approaches to indirect call promotions.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b8\": 0.8,\n    \"b19\": 0.7,\n    \"b48\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.6,\n    \"b39\": 0.7,\n    \"b53\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b29\": 0.5,\n    \"b38\": 0.5,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the high overhead of existing Spectre variant 2 mitigations, particularly retpolines, and proposes JumpSwitches as a more efficient alternative. JumpSwitches leverage indirect call promotion and runtime learning to optimize indirect calls in the Linux kernel while maintaining security against Spectre-style attacks.\",\n  \"Direct Inspiration\": {\n    \"b37\": 1,\n    \"b48\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b8\": 0.8,\n    \"b19\": 0.7,\n    \"b29\": 0.7,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b26\": 0.5,\n    \"b44\": 0.5,\n    \"b45\": 0.5,\n    \"b53\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance impact of mitigating Spectre variant 2 attacks, specifically targeting the costly use of retpolines. It proposes JumpSwitches, an alternative mechanism utilizing indirect call promotion to convert indirect calls into conditional direct calls, thereby improving performance while maintaining security against Spectre attacks. The paper highlights the novelty of dynamically learning branch targets at runtime and integrating with the Linux kernel's build system.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b8\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.8,\n    \"b37\": 0.9,\n    \"b48\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b45\": 0.7,\n    \"b53\": 0.6\n  }\n}\n```"], "5f0d85c69fced0a24be4f03a": ["```json\n{\n    \"Summary\": \"The paper details the design and performance considerations of the IBM z15 mainframe processor, focusing on its branch prediction mechanisms. The primary challenges addressed include balancing single-thread performance with overall throughput, optimizing cache hierarchy, and improving branch prediction accuracy to minimize pipeline delays. The z15's novel contributions involve a sophisticated multi-level BTB design, the use of TAGE PHT tables for direction prediction, and the integration of a neural-network-based perceptron predictor for hard-to-predict branches.\",\n    \"Direct Inspiration\": {\n        \"b8\": 0.95,\n        \"b9\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b7\": 0.75,\n        \"b10\": 0.7,\n        \"b12\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b11\": 0.6,\n        \"b13\": 0.55,\n        \"b14\": 0.5,\n        \"b15\": 0.45\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper outlines the challenges of designing a high-performance branch predictor for the IBM z15 mainframe processor. The key challenges include balancing capacity, latency, throughput, and accuracy in branch prediction to achieve optimal performance. The paper discusses the novel multi-level BTB design, the use of perceptron-based neural networks, and other advanced techniques to enhance branch prediction accuracy.\",\n    \"Direct Inspiration\": {\n        \"b7\": 1,\n        \"b8\": 1,\n        \"b9\": 1,\n        \"b10\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b11\": 0.8,\n        \"b12\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b13\": 0.7,\n        \"b14\": 0.7,\n        \"b15\": 0.6,\n        \"b17\": 0.6,\n        \"b18\": 0.7,\n        \"b19\": 0.7,\n        \"b20\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper discusses the design considerations and implementation of the branch predictor in the IBM z15 mainframe processor. It emphasizes the challenges of balancing capacity, latency, throughput, and accuracy in branch prediction to improve performance. The paper outlines the multi-level BTB design, direction prediction using BHT, PHT, and perceptron, and target prediction with CTB and call/return stack. The primary innovations include the asynchronous lookahead branch predictor, SKOOT predictor, and enhanced perceptron-based direction prediction.\",\n    \"Direct Inspiration\": {\n        \"b7\": 1,\n        \"b8\": 0.9,\n        \"b9\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b10\": 0.6,\n        \"b11\": 0.6,\n        \"b12\": 0.6,\n        \"b18\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the z15 mainframe processor by IBM, focusing on the branch prediction mechanisms and various design considerations such as capacity, latency, throughput, and accuracy. Key innovations include a multi-level branch target buffer (BTB), pattern history table (PHT), and perceptron-based branch direction prediction to improve performance and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b8\": 0.95,\n    \"b9\": 0.9,\n    \"b11\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b12\": 0.75,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.65,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of balancing single-thread performance and overall throughput for IBM's z15 mainframe processor, focusing on branch prediction accuracy, latency, and capacity. The authors propose a sophisticated multi-level branch prediction mechanism, including BTB1 and BTB2 structures, to improve direction and target prediction for branches, leveraging techniques like TAGE PHT, perceptron predictors, and the CTB for multi-target branches.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b8\": 0.9,\n    \"b9\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b12\": 0.8,\n    \"b14\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7\n  }\n}\n```"], "5fae6daad4150a363cec035c": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of maintaining structural information in CNNs while applying attention mechanisms. The novel method proposed is the Spatial Pyramid Attention (SPA) module, which integrates structural information into the attention mechanism using a spatial pyramid structure instead of global average pooling. This approach includes multiple pooling scales and integrates channel dependencies with a lightweight design.\",\n    \"Direct Inspiration\": {\n        \"b5\": 1.0,\n        \"b8\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b9\": 0.7,\n        \"b7\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b10\": 0.6,\n        \"b11\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the loss of structural information in intermediate feature maps when using global average pooling in CNNs. The authors propose a Spatial Pyramid Attention (SPA) module to incorporate structural information into channel-wise attention blocks. The SPA module utilizes a spatial pyramid structure to encode intermediate features and improve performance without introducing additional parameters.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b10\": 0.6,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of incorporating structural information into channel-wise attention blocks in convolutional neural networks (CNNs) to mitigate the limitations of global average pooling. It proposes the Spatial Pyramid Attention (SPA) module, which uses a spatial pyramid structure to encode intermediate features. The SPA module is lightweight and does not introduce additional parameters. The proposed method is evaluated on CIFAR-100 and a down-sampled ImageNet dataset, showing significant performance improvements over existing methods.\",\n  \"Direct Inspiration\": [\"b5\", \"b7\", \"b8\"],\n  \"Indirect Inspiration\": [\"b2\", \"b3\", \"b10\", \"b18\", \"b19\"],\n  \"Other Inspiration\": [\"b4\", \"b12\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of loss of structural information in intermediate feature maps when using global average pooling (GAP) in convolutional neural networks (CNNs). The authors propose the Spatial Pyramid Attention (SPA) module, which introduces a spatial pyramid structure to better encode intermediate features. The SPA module consists of a spatial pyramid structure and a combination of fully-connected layers and a sigmoid-based activation layer. The paper is inspired by the self-attention mechanism and explores three variations of the SPA module: SPANet-A, SPANet-B, and SPANet-C.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b10\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents the Spatial Pyramid Attention (SPA) module for convolutional neural networks (CNNs), addressing the loss of structural information caused by global average pooling (GAP) layers in traditional attention mechanisms. The SPA module includes a spatial pyramid structure and fully connected layers to enhance channel-wise attention while retaining structural information. The primary challenges tackled are the limitations of GAP and the need for improved attention mechanisms in CNNs.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b7\": 0.7,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b11\": 0.4,\n    \"b12\": 0.3\n  }\n}\n```"], "5dbab2523a55acea3c05b02b": ["```json\n{\n  \"Summary\": \"The paper presents BART, a denoising autoencoder built with a sequence-to-sequence model combining Bidirectional and Auto-Regressive Transformers, applicable to a wide range of end tasks. BART's key innovations include its flexibility in noising functions and its effectiveness across various NLP tasks, such as text generation, comprehension, and machine translation.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b11\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b6\": 0.7,\n    \"b9\": 0.7,\n    \"b21\": 0.7,\n    \"b28\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.5,\n    \"b20\": 0.5,\n    \"b23\": 0.5,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper introduces BART, a denoising autoencoder combining Bidirectional and Auto-Regressive Transformers, which generalizes BERT and GPT by using a flexible noising approach. The primary challenges addressed include the limitations of existing self-supervised methods in handling diverse end tasks. BART uses a sequence-to-sequence model to reconstruct text corrupted by arbitrary noising functions, achieving state-of-the-art results across various NLP tasks.\",\n    \"Direct Inspiration\": {\n        \"b11\": 1.0,\n        \"b2\": 0.95,\n        \"b15\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b21\": 0.75,\n        \"b6\": 0.7,\n        \"b28\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b17\": 0.6,\n        \"b20\": 0.55,\n        \"b23\": 0.5,\n        \"b29\": 0.45,\n        \"b18\": 0.4,\n        \"b9\": 0.35\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents BART, a denoising autoencoder based on sequence-to-sequence models combining Bidirectional and Auto-Regressive Transformers. It addresses challenges of limited applicability in existing self-supervised methods by introducing a flexible noising approach applicable to a wide range of NLP tasks, including text generation, comprehension, and machine translation. BART's architecture generalizes BERT and GPT while introducing novel noising schemes like text infilling and sentence permutation.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b11\": 1,\n    \"b15\": 1,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b6\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.6,\n    \"b28\": 0.6,\n    \"b29\": 0.6,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper presents BART, a denoising autoencoder for NLP tasks that combines bidirectional and auto-regressive transformers. Key challenges addressed include the limitations of existing masked language models in handling diverse tasks and the need for flexibility in noising functions. The paper introduces a novel approach to text corruption and reconstruction, achieving state-of-the-art results in several benchmarks.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b11\": 0.8,\n        \"b15\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.7,\n        \"b6\": 0.75,\n        \"b21\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b28\": 0.6,\n        \"b29\": 0.65\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge of the paper is to develop a versatile pre-training model that can be effectively fine-tuned for a wide range of NLP tasks. The authors propose BART, a denoising autoencoder built with a sequence-to-sequence model combining Bidirectional and Auto-Regressive Transformers. The paper focuses on the flexibility of the noising schemes during pre-training and evaluates the model's performance on various tasks.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b11\": 0.8,\n    \"b15\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.6,\n    \"b20\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b28\": 0.75,\n    \"b29\": 0.7\n  }\n}\n```"], "555048d145ce0a409eb71b05": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying deep learning techniques to graph clustering, specifically leveraging the similarity between autoencoders and spectral clustering. The proposed method, GraphEncoder, uses stacked sparse autoencoders to perform graph clustering, which is computationally more efficient and flexible in incorporating sparsity constraints compared to spectral clustering.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1, \n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying deep learning techniques to graph clustering, particularly focusing on the use of stacked sparse autoencoders to perform graph clustering. The authors are motivated by the similarity between autoencoder and spectral clustering methods, aiming to leverage the efficiency and flexibility of autoencoders in large-scale graph clustering problems.\",\n  \"Direct Inspiration\": [\"b10\", \"b19\", \"b23\"],\n  \"Indirect Inspiration\": [\"b12\", \"b18\"],\n  \"Other Inspiration\": [\"b14\", \"b15\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the lack of investigation into the adoption of deep learning, specifically autoencoders, for graph clustering tasks. The authors propose using a stacked sparse autoencoder to perform graph clustering, motivated by the theoretical similarities and practical advantages over spectral clustering.\",\n  \"Direct Inspiration\": [\"b10\", \"b23\"],\n  \"Indirect Inspiration\": [\"b12\", \"b18\", \"b19\"],\n  \"Other Inspiration\": [\"b0\", \"b3\", \"b4\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of applying deep learning techniques to graph clustering. It proposes a method called GraphEncoder, which uses stacked sparse autoencoders to perform graph clustering more efficiently and flexibly compared to traditional spectral clustering methods.\",\n    \"Direct Inspiration\": [\"b10\", \"b23\"],\n    \"Indirect Inspiration\": [\"b15\", \"b14\"],\n    \"Other Inspiration\": [\"b12\", \"b19\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of applying deep learning techniques to graph clustering, particularly using stacked sparse autoencoders. The motivation stems from the similarity between autoencoder and spectral clustering, with the proposed method, GraphEncoder, providing a more efficient and flexible solution for large-scale graph clustering.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.95,\n    \"b19\": 0.90\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.85,\n    \"b18\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.70,\n    \"b6\": 0.75,\n    \"b23\": 0.65\n  }\n}\n```"], "5b67b45517c44aac1c8607aa": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of applying regular convolutional operations on generic graphs, specifically the variability in the number of neighboring nodes and the lack of ordering information among them. The authors propose a learnable graph convolutional layer (LGCL) to transform graphs into grid-like data, allowing the use of regular convolutional neural networks (CNNs). Additionally, they introduce a sub-graph training method to handle large-scale graph data effectively.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b27\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the difficulty of applying regular convolutional operations to generic graphs due to the varying number of adjacent nodes and the lack of ranking information among these nodes. The proposed algorithm, learnable graph convolutional layer (LGCL), transforms graphs into grid-like data to enable the use of regular convolutional operations. Additionally, a sub-graph training method is developed to handle large-scale graph data efficiently.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b26\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in applying convolutional neural networks (CNNs) to generic graph data: varying numbers of adjacent nodes and the lack of ranking information among neighboring nodes. The authors propose the learnable graph convolutional layer (LGCL) to transform graphs into grid-like data, enabling the use of regular convolutional operations. They also introduce a sub-graph training method to handle large-scale graphs efficiently.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b9\": 0.5,\n    \"b21\": 0.5,\n    \"b28\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of applying convolutional neural networks (CNNs) to graph data, specifically the need for a fixed number of ordered neighboring nodes. It introduces the learnable graph convolutional layer (LGCL) and a sub-graph training method to overcome these challenges.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b27\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b0\": 0.75,\n    \"b26\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of applying convolutional operations on generic graphs, specifically the varying number of neighboring nodes and lack of ordering among neighbors. It proposes the learnable graph convolutional layer (LGCL) to transform graphs into grid-like data, enabling the use of regular convolutional operations. Additionally, it introduces a sub-graph training method to handle large-scale graph data efficiently.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b27\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b0\": 0.6,\n    \"b26\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.5,\n    \"b16\": 0.5,\n    \"b23\": 0.5,\n    \"b25\": 0.5\n  }\n}\n```"], "5b1643998fbcbf6e5a9bc447": ["```json\n{\n  \"Summary\": \"The paper addresses challenges of open vocabulary in Neural Machine Translation (NMT) by introducing a new regularization method called subword regularization. This method employs multiple subword segmentations to improve accuracy and robustness of NMT models. The core contributions include a simple NMT training algorithm integrating multiple segmentation candidates and a new subword segmentation algorithm based on a language model that provides multiple segmentations with probabilities.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b19\": 0.9,\n    \"b31\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.6,\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the accuracy and robustness of Neural Machine Translation (NMT) models in an open vocabulary setting by proposing a new regularization method called subword regularization. This method integrates multiple subword segmentations and employs a novel subword segmentation algorithm based on a language model, providing significant improvements over single subword sequence methods.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b31\": 0.8,\n    \"b26\": 0.7,\n    \"b4\": 0.6,\n    \"b7\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b15\": 0.4,\n    \"b30\": 0.4,\n    \"b20\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of handling open vocabulary settings in Neural Machine Translation (NMT) by proposing a method called subword regularization. This method employs multiple subword segmentations to enhance the accuracy and robustness of NMT models. The key contributions include an NMT training algorithm integrating multiple segmentation candidates and a new subword segmentation algorithm based on a language model.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b26\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.6,\n    \"b27\": 0.6,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of handling open-vocabulary settings in Neural Machine Translation (NMT) by proposing a novel regularization method called subword regularization. This method involves two main contributions: an NMT training algorithm that integrates multiple segmentation candidates through on-the-fly data sampling and a new subword segmentation algorithm based on a language model that provides multiple segmentations with probabilities. The proposed methods aim to improve the accuracy and robustness of NMT models.\",\n    \"Direct Inspiration\": {\n        \"b19\": 0.9,\n        \"b21\": 0.9,\n        \"b31\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.7,\n        \"b26\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.6,\n        \"b23\": 0.6,\n        \"b27\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving Neural Machine Translation (NMT) models by tackling the issue of fixed word vocabularies which lead to increased unknown words and inaccuracies in translation. The authors propose a new regularization method called subword regularization to enhance accuracy and robustness by integrating multiple subword segmentations.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b31\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.5,\n    \"b15\": 0.5,\n    \"b20\": 0.4\n  }\n}\n```"], "53e9a034b7602d970291ce1a": ["```json\n{\n  \"Summary\": \"The paper tackles the challenge of finding cuts with small conductance in graphs using a novel local graph partitioning algorithm based on personalized PageRank vectors. The proposed method improves upon previous techniques by simplifying the process and allowing for greater flexibility in computing approximations. The primary contributions include an improved algorithm for computing approximate PageRank vectors and a local version of the Cheeger inequality for PageRank vectors, which leads to the PageRank-Nibble algorithm.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b0\": 0.8,\n    \"b14\": 0.7,\n    \"b8\": 0.6,\n    \"b9\": 0.6\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.5,\n    \"b13\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.4,\n    \"b7\": 0.4,\n    \"b12\": 0.4,\n    \"b10\": 0.4,\n    \"b11\": 0.3,\n    \"b2\": 0.3,\n    \"b5\": 0.3,\n    \"b4\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of finding a cut with small conductance in a graph and proposes a local graph partitioning algorithm using personalized PageRank vectors. The challenges include improving running time and approximation ratios compared to existing methods like Nibble. The algorithm is inspired by and builds upon previous work on PageRank vectors and local partitioning algorithms.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b0\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.7,\n    \"b4\": 0.7,\n    \"b11\": 0.6,\n    \"b2\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the problem of finding a cut with small conductance in graphs, proposing a local graph partitioning algorithm using personalized PageRank vectors. The primary challenges addressed include improving running time and approximation ratio for local partitioning, compared to existing methods such as the Nibble algorithm.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b0\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b8\": 0.7,\n    \"b9\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Finding a cut with small conductance in graph partitioning, improving running time and approximation ratio of existing algorithms.\",\n    \"Inspirations\": \"PageRank vectors, mixing results, and local partitioning techniques.\"\n  },\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b0\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.7,\n    \"b8\": 0.6,\n    \"b9\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b11\": 0.4,\n    \"b2\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge of the paper is to find cuts with small conductance in a graph, improving over existing spectral and local partitioning algorithms. The algorithm proposed by the author, PageRank-Nibble, uses personalized PageRank vectors to produce cuts efficiently and with better approximation ratios compared to previous methods like the Nibble algorithm. The key inspirations and methods include the Cheeger inequality, local spectral techniques, and the algorithms of Jeh-Widom and Berkhin for computing personalized PageRank vectors.\",\n    \"Direct Inspiration\": {\n        \"b0\": 1.0,\n        \"b6\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b1\": 0.8,\n        \"b7\": 0.8,\n        \"b12\": 0.8,\n        \"b13\": 0.8,\n        \"b8\": 0.9,\n        \"b9\": 0.9,\n        \"b14\": 0.9\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.7,\n        \"b4\": 0.7,\n        \"b5\": 0.7,\n        \"b10\": 0.7,\n        \"b11\": 0.7\n    }\n}\n```"], "53e9bd92b7602d9704a386c6": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is improving cache efficiency by identifying dead blocks early and accurately. The authors propose a new class of dead-block predictors using cache burst history, which groups contiguous references a block receives in the MRU position. This approach aims to balance the tradeoff between prediction timeliness and accuracy/coverage. The paper evaluates these predictors in comparison to existing counting-based and trace-based predictors, showing significant improvements in prediction accuracy and coverage.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b13\": 1.0,\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b3\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving cache efficiency by predicting and managing dead blocks in the L1 and L2 caches. It proposes a novel dead-block prediction method based on 'cache bursts', which groups contiguous references a block receives in the MRU position. This approach aims to improve prediction accuracy and coverage, ultimately enhancing cache performance and system efficiency.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b6\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving cache efficiency by accurately predicting dead blocks in the L1 and L2 caches. It proposes a novel approach called cache burst prediction, which groups contiguous references in the MRU position to predict dead blocks more effectively than existing methods.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b13\": 1.0,\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.9,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b3\": 0.6,\n    \"b25\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the challenge of improving cache efficiency by predicting dead blocks accurately and timely. The authors propose new dead-block predictors based on cache burst history, which offer better prediction accuracy and coverage compared to existing methods. These predictors are particularly effective for L1 caches and are evaluated for their improvements in cache efficiency and overall system performance.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b6\": 1,\n    \"b15\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b3\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving cache efficiency by accurately predicting dead blocks in the L1 and L2 caches. It introduces a new class of dead-block predictors based on cache burst history, which proves to be more regular and predictable than individual reference history. These predictors aim to maximize cache utilization and reduce cache misses, thereby enhancing overall system performance.\",\n  \"Direct Inspiration\": [\"b6\", \"b13\", \"b15\"],\n  \"Indirect Inspiration\": [\"b0\", \"b14\"],\n  \"Other Inspiration\": [\"b1\", \"b3\"]\n}\n```"], "5e6cae3493d709897ccff2f9": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the processor-memory performance gap exacerbated by the increasing data set sizes and slowing transistor scaling. The authors propose a novel methodology for understanding and classifying memory access patterns of applications to optimize prefetchers. This approach involves a combination of static binary analysis and dynamic profiling to perform data dependency analysis and classify delinquent load addresses into prefetch kernels. The methodology aims to improve the generality and performance of prefetchers while minimizing their implementation burden.\",\n  \"Direct Inspiration\": {\n    \"b27\": 0.9,\n    \"b16\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b22\": 0.7,\n    \"b24\": 0.7,\n    \"b41\": 0.7,\n    \"b43\": 0.7,\n    \"b26\": 0.7,\n    \"b37\": 0.7,\n    \"b44\": 0.7,\n    \"b11\": 0.7,\n    \"b14\": 0.7,\n    \"b20\": 0.7,\n    \"b29\": 0.7,\n    \"b31\": 0.7,\n    \"b35\": 0.7,\n    \"b36\": 0.7,\n    \"b38\": 0.7,\n    \"b53\": 0.7,\n    \"b47\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b5\": 0.6,\n    \"b21\": 0.6,\n    \"b15\": 0.6,\n    \"b2\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of the processor-memory performance gap by developing a methodology to understand and classify memory access patterns of applications. This approach enables the evaluation and optimization of existing prefetchers and the development of new prefetcher techniques.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b21\": 0.8,\n    \"b27\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b6\": 0.75,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b16\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing an effective prefetcher that balances performance, generality, and cost. The proposed methodology involves understanding and classifying memory access patterns from application binaries to evaluate and optimize existing prefetchers and to develop new prefetcher techniques. This approach aims to provide insights into the performance and utility of different prefetching techniques and to inform new prefetcher designs.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b21\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.7,\n    \"b27\": 0.7,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"This paper addresses the challenge of bridging the processor-memory performance gap due to increased memory access latency despite exponential processor performance improvements. It proposes a novel methodology to understand and classify memory access patterns in applications to evaluate and optimize existing prefetchers and develop new prefetcher techniques. The approach uses static binary analysis and dynamic profiling information to perform data dependency analysis, forming prefetch kernels which are then classified. This technique is scalable, allows for accurate prediction of prefetching utility, and informs the design of new prefetcher capabilities.\",\n    \"Direct Inspiration\": {\n        \"b19\": 0.9,\n        \"b12\": 0.85,\n        \"b41\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b27\": 0.75,\n        \"b47\": 0.7,\n        \"b26\": 0.65,\n        \"b22\": 0.65,\n        \"b24\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b14\": 0.6,\n        \"b20\": 0.55,\n        \"b29\": 0.55,\n        \"b31\": 0.55,\n        \"b38\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of the processor-memory performance gap in Von-Neuman architectures, focusing on designing a prefetcher that improves performance, generality, and is implementable. The authors propose a novel methodology to understand and classify memory access patterns in applications using static binary analysis and dynamic profiling. This approach aims to evaluate, optimize existing prefetchers, and develop new prefetcher techniques.\",\n  \"Direct Inspiration\": {\n    \"b11\": 0.9,\n    \"b14\": 0.9,\n    \"b20\": 0.9,\n    \"b29\": 0.9,\n    \"b35\": 0.9,\n    \"b36\": 0.9,\n    \"b38\": 0.9,\n    \"b53\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b27\": 0.8,\n    \"b16\": 0.8,\n    \"b19\": 0.8,\n    \"b21\": 0.7,\n    \"b5\": 0.7,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b22\": 0.6,\n    \"b24\": 0.6,\n    \"b41\": 0.6,\n    \"b43\": 0.6,\n    \"b26\": 0.6,\n    \"b37\": 0.6,\n    \"b44\": 0.6,\n    \"b47\": 0.5,\n    \"b6\": 0.5,\n    \"b15\": 0.5\n  }\n}\n```"], "53e9ba8ab7602d97046ac03f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in object-based storage systems, particularly focusing on achieving high performance and availability while maintaining strong consistency. The proposed algorithm, CRAQ (Chain Replication with Apportioned Queries), improves the basic chain replication method by enabling multiple nodes to handle read operations, thus reducing latency and increasing throughput.\",\n  \"Direct Inspiration\": {\n    \"b46\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b47\": 0.6,\n    \"b28\": 0.5,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of achieving high performance and availability in object-based storage systems, specifically focusing on strong consistency and load balancing. The proposed algorithm, CRAQ (Chain Replication with Apportioned Queries), aims to maintain strong consistency while improving read throughput and reducing latency by allowing any node in the chain to handle read operations.\",\n  \"Direct Inspiration\": {\n    \"b46\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.8,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.5,\n    \"b12\": 0.5,\n    \"b14\": 0.6,\n    \"b15\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper presents CRAQ (Chain Replication with Apportioned Queries), an object storage system designed to improve read throughput and lower latency while maintaining strong consistency. CRAQ allows any node in the chain to handle read operations, thereby supporting load balancing and reducing potential hotspots. The system also supports eventual consistency and accommodates geographically-distributed datacenters. The primary challenges addressed include maintaining strong consistency with high throughput, load balancing across nodes, and supporting read operations across multiple datacenters.\",\n  \"Direct Inspiration\": [\n    \"b46\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b21\",\n    \"b17\"\n  ],\n  \"Other Inspiration\": [\n    \"b47\"\n  ]\n}\n```", "```json\n{\n    \"Summary\": \"The main challenges addressed in the paper are improving the read throughput and reducing latency in object storage systems while maintaining strong consistency. The proposed solution, CRAQ (Chain Replication with Apportioned Queries), distributes read operations across all nodes in a chain and introduces mechanisms to manage multiple versions of objects. This approach aims to enhance performance in read-mostly workloads and provides flexibility in consistency guarantees.\",\n    \"Direct Inspiration\": [\"b46\"],\n    \"Indirect Inspiration\": [\"b21\", \"b28\", \"b47\"],\n    \"Other Inspiration\": [\"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces CRAQ (Chain Replication with Apportioned Queries) to address the limitations of basic chain replication, such as potential hotspots and inefficiencies in multi-datacenter deployments. CRAQ enhances read throughput and supports strong consistency by allowing any node in the chain to handle read operations, distributing the load more effectively across nodes.\",\n  \"Direct Inspiration\": {\n    \"b46\": 1.0,\n    \"b21\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b28\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```"], "5aed147c17c44a4438153ea5": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the effectiveness of temporal prefetchers in server workloads by proposing a new prefetching technique called Domino. The main innovation lies in the lookup mechanism that uses both one and two miss addresses to identify streams, aiming to increase the accuracy and coverage of prefetches.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b12\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b21\": 0.8,\n    \"b34\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of frequent cache misses in server workloads, which hinder peak processor performance. The novel algorithm proposed, Domino, combines single and double miss address lookups to improve the effectiveness of temporal prefetching, thereby bridging the gap between the opportunities and the performance of current state-of-the-art prefetchers like STMS and ISB.\",\n    \"Direct Inspiration\": {\n        \"b20\": 1.0,\n        \"b9\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.8,\n        \"b17\": 0.8,\n        \"b12\": 0.7,\n        \"b21\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b34\": 0.6,\n        \"b10\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of frequent cache misses in server workloads which prevent processors from reaching peak performance. It proposes a novel temporal prefetcher named Domino that improves the prefetching mechanism by using both one and two previous miss addresses for lookup, leveraging an Enhanced Index Table (EIT). The goal is to bridge the gap between the potential of temporal prefetching and the performance of existing state-of-the-art prefetchers like STMS and ISB.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b12\": 0.7,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.6,\n    \"b17\": 0.6,\n    \"b21\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of frequent cache misses in server workloads, which prevent processors from reaching peak performance. It proposes a novel temporal prefetcher named Domino that combines one and two previous miss addresses in the lookup mechanism to achieve higher accuracy and longer useful streams without imposing fewer prefetch requests per stream.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b12\": 0.8,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b17\": 0.75,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b34\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the frequent cache misses in server workloads that prevent processors from reaching peak performance. The proposed solution is a novel temporal prefetcher named Domino, which uses a combination of one and two last misses in the lookup mechanism to improve the accuracy and timeliness of prefetching.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1.0,\n    \"b20\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b6\": 0.7,\n    \"b17\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b34\": 0.6\n  }\n}\n```"], "5f0d85c69fced0a24be4f019": ["```json\n{\n  \"Summary\": \"The paper discusses the development of a lightweight spatial L1-D prefetcher to address the challenges of hardware overhead, address generation latency, bandwidth limitations, and accuracy. The proposed solution, Instruction Pointer Classification based spatial Prefetching (IPCP), classifies IPs into three classes based on access patterns to optimize prefetching and improve performance significantly while maintaining low hardware overhead.\",\n  \"Direct Inspiration\": {\n    \"b32\": 0.9,\n    \"b46\": 0.85,\n    \"b10\": 0.85,\n    \"b12\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b44\": 0.75,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b53\": 0.65,\n    \"b11\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on improving L1-D hardware prefetchers to overcome challenges like hardware overhead, lookup latency, bandwidth limitations, and low accuracy. The proposed method, IPCP, classifies instruction pointers (IPs) into three categories and uses this classification for spatial prefetching at L1-D and L2 levels, aiming to provide high performance with minimal hardware overhead.\",\n  \"Direct Inspiration\": [\"b46\", \"b10\"],\n  \"Indirect Inspiration\": [\"b37\", \"b32\"],\n  \"Other Inspiration\": [\"b13\", \"b44\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of designing an effective and lightweight hardware prefetcher for the L1-D cache to improve performance by reducing costly off-chip DRAM accesses. The proposed method, IPCP, classifies instruction pointers (IPs) into three classes based on their access patterns and uses this classification for better prefetching. The paper highlights key contributions including the development of a lightweight spatial prefetching framework and demonstrating its effectiveness in improving performance with significantly lower hardware overhead compared to existing spatial prefetchers.\",\n  \"Direct Inspiration\": {\n    \"b10\": 0.9,\n    \"b46\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b44\": 0.7,\n    \"b12\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.5,\n    \"b11\": 0.5,\n    \"b57\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing a lightweight and efficient L1-D cache prefetcher to improve performance without significant hardware overhead. The proposed method, Instruction Pointer Classification based Prefetching (IPCP), classifies instruction pointers into three classes based on their access patterns and uses a bouquet of tiny prefetchers to handle different patterns. This approach aims to reduce the memory wall problem by improving L1-D prefetching accuracy and coverage while maintaining low hardware overhead.\",\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b46\": 0.9,\n    \"b32\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b44\": 0.8,\n    \"b37\": 0.8,\n    \"b12\": 0.7,\n    \"b48\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.6,\n    \"b13\": 0.6,\n    \"b23\": 0.5,\n    \"b57\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\"hardware overhead\", \"lookup latency\", \"bandwidth limitations\", \"limited hardware resources\", \"accuracy to avoid pollution\"],\n    \"algorithm\": \"Instruction Pointer Classification based spatial Prefetching (IPCP)\"\n  },\n  \"Direct Inspiration\": {\n    \"b10\": 1,\n    \"b46\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b13\": 0.9,\n    \"b44\": 0.9,\n    \"b32\": 0.8,\n    \"b37\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.7,\n    \"b23\": 0.7,\n    \"b54\": 0.7,\n    \"b57\": 0.7,\n    \"b43\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```"], "53e99f3bb7602d970280aaa9": ["```json\n{\n  \"Summary\": \"The paper addresses the critical challenge of mitigating memory access latency in commercial server workloads. It proposes a novel algorithm, Sampled Temporal Memory Streaming (STMS), which leverages hash-based lookups and probabilistic updates to improve the practicality of address-correlating prefetchers. The key innovations include minimizing lookup latency, bandwidth-efficient metadata updates, and amortized lookups to enhance prefetch efficiency.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b26\": 1.0,\n    \"b20\": 0.9,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b22\": 0.6,\n    \"b25\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b12\": 0.4,\n    \"b15\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper outlines the challenges posed by memory access latency in commercial server workloads and proposes a novel prefetching algorithm called Sampled Temporal Memory Streaming (STMS). The key requirements for effective temporal streaming include minimal lookup latency, bandwidth-efficient index table updates, and amortized lookups. The proposed algorithm leverages hash-based lookups and probabilistic updates to achieve these goals, addressing the limitations of existing address-correlating prefetchers.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b26\": 1,\n    \"b20\": 0.9,\n    \"b25\": 0.9,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b22\": 0.7,\n    \"b12\": 0.6,\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b4\": 0.4,\n    \"b17\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of memory access latency in commercial server workloads by proposing a novel prefetching algorithm, Sampled Temporal Memory Streaming (STMS). This algorithm aims to minimize off-chip meta-data lookup latency, ensure bandwidth-efficient meta-data updates, and amortize off-chip lookup over many accurate prefetches.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b26\": 1,\n    \"b20\": 1,\n    \"b25\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b22\": 0.8,\n    \"b12\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b9\": 0.6,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of memory access latency in commercial server workloads. It proposes a novel prefetching algorithm called Sampled Temporal Memory Streaming (STMS) to tackle this issue. STMS leverages prior work in temporal streaming and aims to minimize off-chip meta-data lookup latency, improve bandwidth efficiency, and achieve practical deployment by using hash-based lookup and probabilistic updates.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b26\": 1,\n    \"b20\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.9,\n    \"b9\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.7,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the high memory access latency in commercial server workloads, specifically the inefficiency of existing address-correlating prefetchers due to high on-chip storage requirements, long lookup latencies, and increased memory bandwidth pressure. The paper proposes a new prefetching algorithm, Sampled Temporal Memory Streaming (STMS), which uses hash-based lookup and probabilistic updates to reduce off-chip metadata lookup latency and bandwidth consumption while maintaining effective prefetching performance.\",\n  \"Direct Inspiration\": [\"b5\", \"b26\"],\n  \"Indirect Inspiration\": [\"b8\", \"b20\", \"b22\", \"b25\"],\n  \"Other Inspiration\": []\n}\n```"], "5b8c9f4a17c44af36f8b6ef6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of limited operation and processing capabilities of edge devices and the lack of intelligence in behavior feedback, automatic networking, load balance, and data-driven network optimization in edge computing. It proposes a novel Edge Cognitive Computing (ECC) architecture that combines edge computing and cognitive computing to provide dynamic and elastic storage and computing services. The architecture integrates communication, computation, storage, and application on edge networks to achieve data and resource cognition, enabling deeper, human-centered cognitive intelligence.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1,\n    \"b16\": 1,\n    \"b17\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in edge computing and cognitive computing by proposing a new Edge Cognitive Computing (ECC) paradigm. The main contributions include a new ECC architecture and a dynamic cognitive service migration mechanism to enhance service delivery and user experience in various AI-related applications.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b16\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b13\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b2\": 0.5,\n    \"b18\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in edge computing and cognitive computing, proposing a new paradigm called Edge Cognitive Computing (ECC). ECC aims to deploy cognitive computing at the edge of the network to provide dynamic and elastic storage and computing services. Key contributions include a new ECC architecture and a dynamic cognitive service migration mechanism to optimize user mobility and resource allocation.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b16\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of integrating edge computing with cognitive computing to improve real-time service, data optimization, and application intelligence. It proposes a new paradigm called Edge Cognitive Computing (ECC), which aims to combine the strengths of both computing paradigms to provide dynamic and elastic storage and computing services at the network edge.\",\n    \"Direct Inspiration\": {\n        \"b15\": 0.9,\n        \"b16\": 0.85,\n        \"b17\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.75,\n        \"b4\": 0.75,\n        \"b5\": 0.75,\n        \"b12\": 0.7,\n        \"b13\": 0.7\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the integration of edge computing and cognitive computing to form a new paradigm called Edge Cognitive Computing (ECC). The primary challenges include limited operation and processing capabilities of edge devices, missing intelligence in behavior feedback, automatic networking, load balance, and data-driven network optimization. The proposed ECC architecture aims to provide dynamic and elastic storage and computing services by deploying cognitive computing at the edge of the network. A dynamic cognitive service migration mechanism is also introduced to offer mobility-aware, elastic cognitive services based on user mobility and resource availability.\",\n  \"Direct Inspiration\": {\n    \"b15\": 1.0,\n    \"b16\": 1.0,\n    \"b17\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b2\": 0.5,\n    \"b3\": 0.5,\n    \"b4\": 0.5,\n    \"b5\": 0.5\n  }\n}\n```"], "53e9a42bb7602d9702d44c0b": ["```json\n{\n  \"Summary\": \"The paper addresses the performance bottleneck caused by off-chip memory accesses in commercial server applications, particularly those with pointer-based data structures that exhibit non-strided access patterns. It introduces a hardware-independent study of temporal streams across single-chip and multi-chip multiprocessors, identifying repetitive access sequences without specific prefetching implementation assumptions. It also analyzes the specific functions and modules leading to cache misses and temporal streams in commercial applications and the Solaris kernel.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b6\": 0.9,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b18\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b11\": 0.7,\n    \"b3\": 0.6,\n    \"b17\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b5\": 0.5,\n    \"b10\": 0.5,\n    \"b12\": 0.5,\n    \"b13\": 0.5,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of off-chip memory access bottlenecks in commercial server applications, specifically focusing on pointer-based data structures with non-strided access patterns. It proposes a hardware-independent study of temporal streams across single-chip and multi-chip multiprocessors, using an information-theoretic analysis to identify repetitive access sequences without assuming specific prefetching implementations.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b20\": 0.75,\n    \"b18\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of off-chip memory access bottlenecks in commercial server applications, particularly those with complex, non-strided access patterns. It proposes a hardware-independent study of temporal streams across single-chip and multi-chip multiprocessors to analyze repetitive access sequences and their origins. Key contributions include miss classification, information-theoretic analysis of temporal streams, and identification of application-level origins of temporal streams.\",\n    \"Direct Inspiration\": {\n        \"b24\": 1.0,\n        \"b6\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b7\": 0.8,\n        \"b8\": 0.7,\n        \"b20\": 0.7,\n        \"b18\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b2\": 0.6,\n        \"b11\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the critical performance bottleneck caused by off-chip memory accesses in commercial server applications, particularly those dominated by pointer-based data structures with non-strided access patterns. It proposes a hardware-independent study of temporal streams across single-chip and multi-chip multiprocessors, using an information-theoretic approach to identify repetitive access sequences. The study aims to understand miss behavior changes in different system organizations and the application-level origins of temporal streams.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b24\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.85,\n    \"b20\": 0.75,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b11\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of off-chip memory access bottlenecks in commercial server applications, particularly focusing on pointer-based data structures. It proposes a hardware-independent study of temporal streams across single-chip and multi-chip multiprocessors. The novel approach involves identifying temporal streams using information-theoretic analysis and profiling specific code modules leading to cache misses. The paper demonstrates miss classification across system organizations, analyzes temporal streams' characteristics, and investigates the application-level origins of these streams.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b24\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b7\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```"], "558c8f2684ae6766fdf3c27c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of encoding hint bits in instruction sets without changing the ISA, proposing a technique that uses register names to encode these hints. The method allows for the introduction and removal of hints without breaking binary compatibility, and it integrates this encoding into the register allocation process.\",\n  \"Direct Inspiration\": [\"b0\", \"b1\", \"b2\", \"b3\"],\n  \"Indirect Inspiration\": [\"b4\", \"b5\", \"b6\", \"b7\"],\n  \"Other Inspiration\": [\"b8\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of encoding hint bits in instruction sets without changing the ISA. It proposes a novel technique using register names to encode hints, which enhances the cooperation between compilers and architectures while maintaining binary compatibility. The key contributions include an algorithm for encoding hints in register names, extending register allocation to accommodate hints, and proposing architectural support for decoding these hints.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b2\": 0.9,\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper outlines the challenge of enhancing cooperation between compilers and architectures without altering the ISA, and proposes an algorithm to encode hints in register names, enabling dynamic and backward-compatible hint allocation.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b7\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adding hint bits to an existing ISA without causing binary compatibility issues. It proposes a novel technique to encode hints using the choice of register names, which eliminates the need for ISA changes. The key contributions include a hint encoding algorithm integrated into register allocation, architectural support for decoding hint bits, and experimental evaluations on the Alpha and x86-64 ISAs.\",\n  \"Direct Inspiration\": {\n    \"b0\": 0.9,\n    \"b1\": 0.85,\n    \"b2\": 0.85,\n    \"b3\": 0.85,\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of adding hint bits to an existing ISA without breaking binary compatibility. It proposes a technique to encode hints using existing instruction sets by leveraging the choice of register names. The technique integrates hint encoding with the register allocation process, ensuring that the encoding of hints does not degrade the quality of register allocation. The paper presents an algorithm for encoding hints, discusses necessary architectural support, and evaluates the proposed technique on different ISAs.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b5\": 0.6,\n    \"b6\": 0.6,\n    \"b7\": 0.6,\n    \"b8\": 0.5\n  }\n}\n```"], "53e9a775b7602d97030b16b6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of hardware data prefetching, focusing on mitigating its negative impact on performance and energy consumption due to memory bandwidth contention and cache pollution. The proposed solution dynamically adjusts the prefetcher's aggressiveness and insertion points in the cache based on runtime estimations of prefetch accuracy, timeliness, and cache pollution.\",\n  \"Direct Inspiration\": [\"b12\", \"b10\", \"b23\"],\n  \"Indirect Inspiration\": [\"b9\", \"b0\"],\n  \"Other Inspiration\": [\"b21\", \"b7\", \"b17\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of hardware data prefetching including memory bandwidth contention and cache pollution. It proposes dynamic feedback mechanisms to adjust the aggressiveness of the prefetcher and the insertion point of prefetched data in the cache based on run-time estimations of accuracy, timeliness, and cache pollution.\",\n  \"Direct Inspiration\": {\n    \"b23\": 0.9,\n    \"b10\": 0.85,\n    \"b9\": 0.75,\n    \"b0\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b18\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b5\": 0.55,\n    \"b7\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of hardware data prefetching, which can lead to performance degradation and increased energy consumption due to inaccurate predictions and cache pollution. The proposed solution is a feedback-directed prefetching mechanism that dynamically adjusts the aggressiveness of the prefetcher based on real-time metrics of accuracy, lateness, and cache pollution, thereby improving performance and reducing memory bandwidth consumption.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1.0,\n    \"b10\": 0.9,\n    \"b18\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b0\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6,\n    \"b5\": 0.65,\n    \"b7\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of hardware data prefetching, including increased memory bandwidth contention and cache pollution. It proposes a dynamic feedback mechanism to adjust the aggressiveness of prefetching and the cache insertion points based on runtime estimations of prefetch accuracy, lateness, and cache pollution. This approach aims to enhance performance while reducing negative impacts.\",\n  \"Direct Inspiration\": {\n    \"b23\": 1,\n    \"b10\": 0.9,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b0\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b21\": 0.4,\n    \"b12\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses two primary challenges in hardware data prefetching: bandwidth contention and cache pollution. It proposes mechanisms for dynamically adjusting the aggressiveness of the prefetcher based on prefetch accuracy, lateness, and cache pollution. The novel methods include dynamic feedback mechanisms that estimate these metrics and adjust prefetcher behavior accordingly. This approach aims to preserve the performance benefits of aggressive prefetching while mitigating its negative impacts.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1.0,\n    \"b23\": 1.0,\n    \"b12\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b17\": 0.8,\n    \"b5\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.7,\n    \"b0\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```"], "5d04e8d7da56295d08daef06": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in modeling users' on-task search behaviors to improve retrieval performance. It proposes a context-aware neural retrieval solution called Context Attentive document-Ranking and query-Suggestion (CARS) that encodes past search activities using a hierarchical recurrent neural network and applies attention mechanisms to model the development of users' search intent. The proposed model jointly optimizes document ranking and query suggestion tasks.\",\n  \"Direct Inspiration\": [\"b1\", \"b16\", \"b26\", \"b33\", \"b40\"],\n  \"Indirect Inspiration\": [\"b3\", \"b12\", \"b17\", \"b23\", \"b29\", \"b50\"],\n  \"Other Inspiration\": [\"b9\", \"b42\", \"b48\", \"b52\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper revolve around the limitations of existing methods in effectively modeling users' on-task search activities, particularly considering the interdependencies between various retrieval tasks (e.g., document ranking and query suggestion). The proposed solution, Context Attentive document-Ranking and query-Suggestion (CARS), addresses these challenges by employing a context-aware neural retrieval model that uses a hierarchical recurrent neural network and attention mechanisms to model search context and optimize both document ranking and query suggestion tasks.\",\n  \"Direct Inspiration\": [\"b1\", \"b16\", \"b26\", \"b33\", \"b40\"],\n  \"Indirect Inspiration\": [\"b3\", \"b12\", \"b50\", \"b52\"],\n  \"Other Inspiration\": [\"b9\", \"b23\", \"b48\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of modeling users' on-task search behaviors, particularly the limitations of existing solutions in capturing contextual information and sequential dependencies. It proposes a context-aware neural retrieval solution, Context Attentive document-Ranking and query-Suggestion (CARS), which employs a two-level hierarchical recurrent neural network to encode search context and optimize document ranking and query suggestion tasks jointly.\",\n  \"Direct Inspiration\": [\"b1\", \"b16\", \"b26\", \"b33\", \"b40\"],\n  \"Indirect Inspiration\": [\"b3\", \"b12\", \"b25\", \"b29\", \"b50\"],\n  \"Other Inspiration\": [\"b6\", \"b9\", \"b23\", \"b48\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in modeling users' on-task search behaviors for improving retrieval performance. It proposes a context-aware neural retrieval solution, Context Attentive document-Ranking and query-Suggestion (CARS), which uses a two-level hierarchical recurrent neural network to encode search context representations and employs attention mechanisms to model variable dependency structures. CARS jointly optimizes tasks of document ranking and query suggestion.\",\n  \"Direct Inspiration\": [\"b1\", \"b16\", \"b26\", \"b33\", \"b40\"],\n  \"Indirect Inspiration\": [\"b3\", \"b23\", \"b48\", \"b50\", \"b12\", \"b29\"],\n  \"Other Inspiration\": [\"b9\", \"b6\", \"b41\", \"b14\", \"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the effective modeling of users' on-task search behaviors to improve retrieval performance in search engines. The proposed algorithm, Context Attentive document-Ranking and query-Suggestion (CARS), leverages a two-level hierarchical recurrent neural network to encode search context from user queries and clicks, and uses attention mechanisms to capture variable dependencies within search tasks. CARS jointly optimizes document ranking and query suggestion tasks through multi-task learning.\",\n  \"Direct Inspiration\": [\"b1\", \"b16\", \"b26\", \"b33\", \"b40\"],\n  \"Indirect Inspiration\": [\"b3\", \"b12\", \"b23\", \"b29\", \"b50\"],\n  \"Other Inspiration\": [\"b6\", \"b9\", \"b17\", \"b18\", \"b32\"]\n}\n```"], "53e9a23eb7602d9702b4428f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving replacement algorithms for inclusive last-level caches (LLC) by leveraging information from L2 cache evictions. The proposed algorithm, CHAR, aims to identify dead blocks from L2 cache evictions to improve LLC performance by marking them as potential victim candidates, thereby retaining blocks with shorter reuse distances in the LLC.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b8\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b5\": 0.7,\n    \"b21\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b2\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving replacement algorithms for inclusive last-level caches (LLCs) by utilizing eviction hints from L2 caches. The proposed CHAR (cache hierarchy-aware replacement) algorithm aims to identify and evict dead blocks early to retain blocks with shorter reuse distances in the LLC, enhancing cache performance.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b6\": 0.95,\n    \"b8\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b21\": 0.7,\n    \"b24\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.65,\n    \"b17\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving last-level cache (LLC) replacement policies by using hints from L2 cache evictions. It proposes a cache hierarchy-aware replacement (CHAR) algorithm that identifies dead blocks at the L2 cache level to optimize LLC performance. The CHAR algorithm aims to bridge the gap between baseline SRRIP policy and Belady's optimal policy by accurately predicting which L2 cache evictions can be used to update the re-reference prediction value (RRPV) in the LLC.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b6\": 0.9,\n    \"b1\": 0.8,\n    \"b20\": 0.8,\n    \"b5\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.6,\n    \"b4\": 0.6,\n    \"b24\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b11\": 0.5,\n    \"b25\": 0.4,\n    \"b23\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving last-level cache (LLC) replacement policies by utilizing information from L2 cache evictions. The core idea is to use the reuse patterns observed in L2 cache to predict the liveness of blocks and make better replacement decisions in the LLC, thereby reducing cache misses and improving performance.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b4\": 0.7,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b5\": 0.7,\n    \"b21\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving replacement policies for the last-level cache (LLC) by incorporating eviction information from the L2 cache. The proposed algorithm, CHAR, uses an oracle-assisted experiment to determine the reuse patterns of blocks evicted from the L2 cache and thereby improve the LLC's replacement decisions. The paper aims to bridge the gap between the baseline SRRIP policy and Belady's optimal algorithm by identifying and early evicting dead blocks from the LLC.\",\n    \"Direct Inspiration\": {\n        \"b6\": 0.9,\n        \"b8\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.75,\n        \"b21\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b26\": 0.6,\n        \"b12\": 0.55,\n        \"b4\": 0.5\n    }\n}\n```"], "5fdb279d91e0118a02c4f4ef": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are related to predicting gene expression levels based on histone modifications. The authors propose two algorithms, SimpleChrome and DeepNeighbors, to address these challenges. SimpleChrome focuses on reducing the dimensionality of the input data using autoencoders, while DeepNeighbors incorporates neighboring gene information using a two-step training approach. The paper aims to achieve competitive predictive performance with reduced sample size and model complexity.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.6,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.5,\n    \"b3\": 0.5,\n    \"b4\": 0.5,\n    \"b14\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the prediction of gene expression levels based on histone modifications, while managing the high dimensionality of data which complicates the modeling process. The proposed algorithm, SimpleChrome, focuses on reducing data dimensionality using autoencoders before applying machine learning models, whereas DeepNeighbors also considers neighboring genes' data for prediction.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1,\n    \"b13\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b4\": 0.7,\n    \"b6\": 0.6,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.9,\n    \"b2\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting gene expression based on histone modifications, proposing SimpleChrome and DeepNeighbors as solutions. These methods aim to reduce model complexity while retaining predictive accuracy by leveraging unsupervised learning and lower-dimensional data representations.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is to improve the prediction of gene expression levels based on histone modifications, while addressing limitations in existing models such as DeepChrome and AttentiveChrome, which only consider data around the Transcription Start Site and utilize complex models with many parameters. The proposed algorithm, SimpleChrome and DeepNeighbors, aims to reduce model complexity and computational requirements by using unsupervised learning to derive lower dimensional representations of histone modifications and incorporating neighboring genes in the predictions.\",\n  \"Direct Inspiration\": [\"b12\", \"b13\"],\n  \"Indirect Inspiration\": [\"b3\", \"b4\", \"b15\", \"b10\", \"b9\"],\n  \"Other Inspiration\": [\"b2\", \"b8\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting gene expression based on histone modifications using deep learning. It introduces SimpleChrome and DeepNeighbors to overcome the limitations of previous models that only consider limited data and have high computational costs. The proposed methods utilize unsupervised learning for dimensionality reduction and simple models for prediction, demonstrating competitive performance with reduced complexity.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b8\": 0.8,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b14\": 0.6\n  }\n}\n```"], "57a4e92bac44365e35c9ab55": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of using SRAM for large caches in modern processors due to its low density and high leakage power. It proposes leveraging DRAM, particularly stacked DRAM, for cache design to overcome these challenges. The paper surveys various techniques for managing DRAM caches, classifying them based on key parameters to highlight similarities and differences. The goal is to provide a comprehensive view of DRAM cache research to guide future developments.\",\n  \"Direct Inspiration\": {\n    \"b7\": 0.9,\n    \"b11\": 0.85,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b3\": 0.75,\n    \"b4\": 0.75,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.65,\n    \"b30\": 0.65,\n    \"b33\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of integrating large DRAM caches into modern processor architectures, primarily driven by the limitations of SRAM in terms of density, leakage power, and scalability. It explores various techniques and strategies to efficiently manage DRAM caches, including cache line granularity, metadata management, and hybrid cache designs. The paper is motivated by the need to overcome power and bandwidth walls, and to allow more cores on-chip for multicore scaling.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.95,\n    \"b7\": 0.9,\n    \"b27\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.75,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.65,\n    \"b19\": 0.6,\n    \"b51\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of using DRAM caches in modern processors to overcome limitations of SRAM in terms of density, leakage power, and scalability. It proposes novel techniques for DRAM cache management, focusing on architecture and system-level solutions rather than device-level techniques.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b11\": 1,\n    \"b27\": 1,\n    \"b30\": 1,\n    \"b33\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.9,\n    \"b49\": 0.9,\n    \"b51\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.8,\n    \"b3\": 0.8,\n    \"b4\": 0.8,\n    \"b5\": 0.8,\n    \"b8\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges related to the inefficiencies of SRAM caches in modern processors, particularly focusing on the potentials of DRAM caches. It discusses the higher density and energy efficiency of DRAM, advancements in die-stacking technology, and various techniques for managing DRAM caches. The paper aims to provide a comprehensive survey of DRAM cache management techniques, highlighting key research insights and future directions.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b7\": 0.85,\n    \"b27\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b19\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.65,\n    \"b32\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of using DRAM caches in modern processors, focusing on issues such as latency, bandwidth, and tag management. It proposes various techniques for optimizing DRAM caches and highlights the need for novel approaches to fully leverage DRAM's potential in mitigating power and bandwidth barriers.\",\n  \"Direct Inspiration\": [\"b7\", \"b27\", \"b30\", \"b11\", \"b33\"],\n  \"Indirect Inspiration\": [\"b5\", \"b8\", \"b19\", \"b23\"],\n  \"Other Inspiration\": [\"b1\", \"b3\", \"b4\", \"b10\", \"b51\"]\n}\n```"], "53e9b79fb7602d970434793d": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving DRAM Bank-Level Parallelism (BLP) to enhance system performance by proposing two techniques: BLP-Aware Prefetch Issue (BAPI) and BLP-Preserving Multi-core Request Issue (BPMRI). These techniques aim to optimize the prefetch issue policy and minimize destructive interference in BLP in multi-core systems.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b11\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b15\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving DRAM Bank-Level Parallelism (BLP) by proposing two techniques: BLP-Aware Prefetch Issue (BAPI) and BLP-Preserving Multi-core Request Issue (BPMRI). These techniques aim to maximize the number of different DRAM banks accessed and minimize destructive interference in BLP when multiple applications run on a CMP system.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b15\": 0.8,\n    \"b7\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.75,\n    \"b11\": 0.7,\n    \"b24\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b4\": 0.5,\n    \"b19\": 0.55,\n    \"b23\": 0.55,\n    \"b8\": 0.6,\n    \"b20\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of exploiting DRAM Bank-Level Parallelism (BLP) to achieve high system performance. It proposes new techniques, namely BLP-Aware Prefetch Issue (BAPI) and BLP-Preserving Multi-core Request Issue (BPMRI), to improve the parallel handling of memory requests and minimize destructive interference in multi-core systems.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b22\": 0.9,\n    \"b11\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.7,\n    \"b23\": 0.7,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.5,\n    \"b15\": 0.5,\n    \"b8\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on improving DRAM Bank-Level Parallelism (BLP) to enhance system performance. The primary challenges addressed include inefficient servicing of memory requests due to FIFO issue policies and destructive interference in multi-core systems. The proposed solutions, BLP-Aware Prefetch Issue (BAPI) and BLP-Preserving Multi-core Request Issue (BPMRI), aim to maximize BLP by modifying prefetch request handling and L2-to-DRAM request arbitration.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b11\": 0.85,\n    \"b22\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b8\": 0.75,\n    \"b24\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b19\": 0.65,\n    \"b20\": 0.65,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of maximizing DRAM Bank-Level Parallelism (BLP) to enhance system performance by improving memory request issue policies. It proposes two techniques: BLP-Aware Prefetch Issue (BAPI) and BLP-Preserving Multi-core Request Issue (BPMRI), to increase and preserve DRAM BLP by modifying the order in which prefetch and regular memory requests are sent to Miss Status Holding Registers (MSHRs) and DRAM request buffers.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b3\": 0.8,\n    \"b24\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.75,\n    \"b11\": 0.7,\n    \"b19\": 0.65,\n    \"b23\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b8\": 0.6,\n    \"b15\": 0.55,\n    \"b22\": 0.6\n  }\n}\n```"], "5cede104da562983788e4508": ["```json\n{\n  \"Summary\": \"The primary challenges addressed in the paper are achieving precise keypoint localization for 2D human pose estimation while maintaining high-resolution representations throughout the entire network process. The authors propose the High-Resolution Net (HRNet), which connects high-to-low resolution subnetworks in parallel and performs repeated multi-scale fusions to create high-resolution, rich representations for keypoint detection.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b71\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b76\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.7,\n    \"b35\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces a novel architecture, High-Resolution Net (HRNet), for 2D human pose estimation. The key challenge addressed is maintaining high-resolution representations throughout the entire process, as opposed to the conventional high-to-low and low-to-high frameworks. The HRNet connects high-to-low resolution subnetworks in parallel and performs repeated multi-scale fusions to boost high-resolution representations, leading to more accurate keypoint detection.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1,\n    \"b71\": 1,\n    \"b10\": 1,\n    \"b26\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b76\": 0.8,\n    \"b68\": 0.7,\n    \"b50\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.5,\n    \"b82\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge tackled in the paper is improving the spatial accuracy and efficiency of 2D human pose estimation. The paper introduces the High-Resolution Net (HRNet), which maintains high-resolution representations throughout the entire process by connecting high-to-low resolution subnetworks in parallel and performing repeated multi-scale fusions.\",\n  \"Direct Inspiration\": [\"b39\", \"b71\", \"b10\"],\n  \"Indirect Inspiration\": [\"b26\", \"b76\", \"b68\"],\n  \"Other Inspiration\": [\"b23\", \"b82\", \"b55\", \"b54\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Primary Challenges\": \"The primary challenge addressed in the paper is the localization of human anatomical keypoints for single-person pose estimation using a novel architecture that maintains high-resolution representations throughout the process.\",\n    \"Algorithm Proposed\": \"The paper introduces the High-Resolution Net (HRNet), which maintains high-resolution representations through parallel multi-resolution subnetworks and repeated multi-scale fusions.\"\n  },\n  \"Direct Inspiration\": {\n    \"References\": [\"b39\", \"b71\", \"b10\", \"b26\", \"b76\"]\n  },\n  \"Indirect Inspiration\": {\n    \"References\": [\"b55\", \"b82\", \"b23\", \"b54\"]\n  },\n  \"Other Inspiration\": {\n    \"References\": [\"b35\", \"b1\", \"b13\", \"b68\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the accurate localization of human anatomical keypoints for 2D human pose estimation using a novel architecture called High-Resolution Net (HRNet). The HRNet maintains high-resolution representations throughout the whole process by connecting high-to-low resolution subnetworks in parallel and conducting repeated multi-scale fusions.\",\n  \"Direct Inspiration\": {\n    \"b39\": 1.0,\n    \"b71\": 1.0,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b26\": 0.8,\n    \"b76\": 0.8,\n    \"b35\": 0.7,\n    \"b1\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b55\": 0.6,\n    \"b23\": 0.6,\n    \"b82\": 0.6,\n    \"b54\": 0.6\n  }\n}\n```"], "53e9a73bb7602d9703070242": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving DBMS performance by reducing instruction cache misses using a hardware instruction prefetching technique called Call Graph Prefetching (CGP). CGP leverages the predictability in function call sequences within the layered architecture of DBMSs to prefetch instructions and reduce cache miss stalls.\",\n    \"Direct Inspiration\": {\n        \"b12\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.7,\n        \"b19\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.6,\n        \"b11\": 0.6,\n        \"b14\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the performance bottleneck due to poor cache performance in memory-bound database systems. The paper proposes Call Graph Prefetching (CGP), a hardware instruction prefetching technique, to reduce instruction cache misses by analyzing the call graph of an application and prefetching instructions. CGP uses a Call Graph History Cache (CGHC) to store sequences of functions invoked during execution and prefetches instructions at function boundaries using next-N-line (NL) prefetching within function boundaries.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7,\n    \"b11\": 0.7,\n    \"b14\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b15\": 0.6,\n    \"b17\": 0.6,\n    \"b20\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance of memory-bound database systems by reducing instruction cache misses through a technique called Call Graph Prefetching (CGP). This technique dynamically analyzes the call graph of an application to prefetch instructions, thereby reducing cache misses and improving performance.\",\n  \"Direct Inspiration\": {\n    \"b12\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b11\": 0.8,\n    \"b14\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b20\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving memory-bound DBMS performance by reducing instruction cache misses. It proposes Call Graph Prefetching (CGP), a hardware instruction prefetching technique that leverages call graph history to prefetch instructions, thereby reducing instruction cache misses and improving performance.\",\n    \"Direct Inspiration\": {\n        \"b12\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.8,\n        \"b3\": 0.8,\n        \"b11\": 0.8,\n        \"b14\": 0.8,\n        \"b19\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.7,\n        \"b23\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing instruction cache (I-cache) misses in memory-bound database management systems (DBMSs). The authors propose a novel hardware instruction prefetching technique called Call Graph Prefetching (CGP), which leverages the predictable function call sequences in DBMSs to prefetch instructions and reduce I-cache misses.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b15\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b14\": 0.7,\n    \"b19\": 0.7,\n    \"b11\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6,\n    \"b23\": 0.6,\n    \"b20\": 0.5\n  }\n}\n```"], "5ccef1b26558b90bfac1ef8f": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting adverse drug reactions (ADRs) in combined medication scenarios using pharmacologic databases. The proposed method, HCNS-ADR, improves upon existing techniques by generating credible negative samples and employing advanced machine learning models for accurate prediction.\",\n  \"Direct Inspiration\": {\n    \"b21\": 0.9,\n    \"b22\": 0.8,\n    \"b6\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b18\": 0.65,\n    \"b19\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting adverse drug reactions (ADRs) for combined medication, which is underexplored in post-marketing surveillance. The authors propose a method called HCNS-ADR that leverages multiple pharmacologic databases. They develop a novel scoring method for selecting credible negative samples based on a drug-disease-gene tripartite network and use advanced machine learning techniques for ADR prediction.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b17\": 0.85,\n    \"b22\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting adverse drug reactions (ADRs) from combined medications using pharmacologic databases. The authors propose a novel method called HCNS-ADR which uses credible negative samples selected from pharmacologic databases. The method involves representing drugs as multi-dimensional vectors, generating credible negative samples through a drug-disease-gene tripartite network, and applying machine learning techniques for prediction.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b6\": 0.85,\n    \"b17\": 0.8,\n    \"b21\": 0.75,\n    \"b22\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.65,\n    \"b7\": 0.6,\n    \"b18\": 0.55,\n    \"b19\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b23\": 0.45,\n    \"b25\": 0.4,\n    \"b26\": 0.35,\n    \"b27\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting adverse drug reactions (ADRs) from combined medication, focusing on drug-drug interactions (DDIs) using pharmacologic databases. The authors propose a method called HCNS-ADR which utilizes credible negative samples and multi-dimensional drug representations for improved machine learning predictions. Key innovations include drug representation through heterogeneous features and a novel scoring method for selecting negative samples using a drug-disease-gene tripartite network.\",\n  \"Direct Inspiration\": [\"b1\", \"b17\"],\n  \"Indirect Inspiration\": [\"b6\", \"b21\", \"b22\"],\n  \"Other Inspiration\": [\"b23\", \"b24\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting adverse drug reactions (ADRs) from combined medication using pharmacologic databases. It proposes a method called HCNS-ADR that uses credible negative samples generated from a drug-disease-gene network to improve prediction accuracy through advanced machine learning techniques.\",\n  \"Direct Inspiration\": [\"b6\", \"b24\"],\n  \"Indirect Inspiration\": [\"b3\", \"b17\", \"b18\", \"b19\"],\n  \"Other Inspiration\": [\"b21\", \"b22\"]\n}\n```"], "5e5e18b193d709897ce27ccd": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving both prediction quality and prediction speed for tasks requiring multi-sentence scoring, such as retrieval and dialogue tasks. It introduces the Poly-encoder architecture, which combines the advantages of Bi-encoders and Cross-encoders, and proposes pre-training on data similar to downstream tasks to enhance performance.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b16\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.9,\n    \"b20\": 0.9,\n    \"b28\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b18\": 0.7,\n    \"b26\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The paper addresses the challenge of improving both prediction quality and speed in multi-sentence scoring tasks, particularly in retrieval and dialogue tasks. The primary difficulty lies in the trade-off between the high accuracy of Cross-encoder models and the speed of Bi-encoder models.\",\n    \"Inspirations\": \"The proposed Poly-encoder architecture takes inspiration from both Bi-encoder and Cross-encoder models, aiming to combine their strengths by introducing a novel attention mechanism for better performance.\"\n  },\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b16\": 0.9,\n    \"b20\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.85,\n    \"b19\": 0.8,\n    \"b28\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b26\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of improving prediction quality and prediction speed in tasks that require multi-sentence scoring, such as retrieval and dialogue tasks. The proposed solution is the Poly-encoder architecture, which combines the benefits of Bi-encoders and Cross-encoders by introducing an additional learnt attention mechanism. This new architecture aims to provide performance gains over Bi-encoders and speed gains over Cross-encoders. Additionally, the paper explores pre-training strategies using data more similar to downstream tasks to achieve further improvements.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.95,\n    \"b16\": 0.90,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.75,\n    \"b6\": 0.70,\n    \"b28\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.60,\n    \"b13\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving both prediction quality and speed in tasks requiring multi-sentence scoring, such as retrieval and dialogue tasks. The authors propose the Poly-encoder architecture, which combines the advantages of Bi-encoders (fast prediction times) and Cross-encoders (high accuracy) by introducing an additional learnt attention mechanism. The study also explores pre-training strategies using data more similar to downstream tasks, resulting in significant performance gains.\",\n  \"Direct Inspiration\": [\"b5\", \"b16\", \"b20\"],\n  \"Indirect Inspiration\": [\"b6\", \"b19\", \"b28\"],\n  \"Other Inspiration\": [\"b21\", \"b23\", \"b27\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving both the quality and speed of multi-sentence scoring tasks by proposing a novel architecture called Poly-encoder. The main contributions include the introduction of the Poly-encoder, which combines the strengths of Bi-encoders and Cross-encoders, and pre-training strategies using data more similar to downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b16\": 0.9,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b19\": 0.7,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.5,\n    \"b13\": 0.4\n  }\n}\n```"], "53e99998b7602d97021da64b": ["```json\n{\n  \"Summary\": \"The paper presents a novel sampling-based framework for Feedback-Directed Optimization (FDO) using the Performance Monitoring Unit (PMU) of modern processors to overcome the shortcomings of traditional instrumentation-based profiling, such as high overhead and tight coupling between builds. It introduces methods to collect edge/basic block frequency profiles and value profiles, and suggests heuristics and algorithms to improve profile accuracy.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b11\": 0.6,\n    \"b12\": 0.6,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5,\n    \"b2\": 0.5,\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b16\": 0.4,\n    \"b21\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of traditional profile-guided optimization (FDO) which include the requirement of double compilation, tight coupling between builds, difficulty in creating representative execution environments, and high overhead of profile collection. To overcome these, the paper proposes a framework using hardware performance monitoring units (PMU) to collect execution profiles through sampling, thereby reducing overhead and decoupling profile collection from FDO builds. The framework focuses on collecting edge/basic block frequency profiles and value profiles using various sampling techniques and mitigates hardware-induced inaccuracies through heuristics and algorithms.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b23\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b16\": 0.8,\n    \"b21\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.7,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"This paper addresses the limitations of traditional profile-guided optimizations in compilers, particularly the high overhead and tight coupling between builds. It proposes a novel method using performance monitoring units (PMU) to collect execution profiles through sampling rather than instrumentation, aiming to reduce profiling overhead and decouple the profile collection from the optimization build.\",\n  \"Direct Inspiration\": [\"b3\", \"b23\"],\n  \"Indirect Inspiration\": [\"b0\", \"b4\", \"b5\"],\n  \"Other Inspiration\": [\"b2\", \"b10\", \"b11\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of traditional profile-guided optimization (FDO) which include the need for dual compilation, tight coupling between builds, difficulty in setting up profiling environments, and significant overhead during profile collection. It proposes using sampling events generated by modern processors' performance monitoring units (PMU) to collect execution profiles, thereby eliminating the need for instrumentation and reducing overhead. The paper introduces a framework for sampling-based FDO, identifies hardware effects that impact sample accuracy, proposes heuristics to mitigate these effects, and evaluates the approach's effectiveness.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b23\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b11\": 0.6,\n    \"b16\": 0.6,\n    \"b21\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of traditional profile-guided optimization, particularly the requirement of double compilation, tight coupling between builds, difficulty in representative profiling environments, and high overhead of instrumentation. The proposed solution is a sampling-based framework using performance monitoring units (PMUs) to collect execution profiles, aiming for lower overhead and decoupling profile collection from optimization builds.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.8,\n    \"b3\": 0.85,\n    \"b23\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.75,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.65,\n    \"b21\": 0.65,\n    \"b2\": 0.6\n  }\n}\n```"], "5ec3b1479fced0a24bfa3f1d": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"Predicting DDI-associated events using diverse drug features and deep learning.\",\n    \"inspirations\": \"Existing methods and datasets, such as those from DrugBank and previous machine learning models.\"\n  },\n  \"Direct Inspiration\": [\"b24\", \"b14\", \"b6\"],\n  \"Indirect Inspiration\": [\"b5\", \"b1\", \"b25\"],\n  \"Other Inspiration\": [\"b26\", \"b34\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting DDI-associated events by developing a computational method named DDIMDL that combines diverse drug features with deep learning. The authors emphasize the need for a comprehensive study by considering various drug features and efficiently combining them to predict DDI events.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b4\": 0.85,\n    \"b16\": 0.8,\n    \"b24\": 0.9,\n    \"b26\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b10\": 0.7,\n    \"b27\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.65,\n    \"b23\": 0.65,\n    \"b28\": 0.6,\n    \"b34\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"Predicting DDI-associated events efficiently and accurately using a method that combines diverse drug features and deep learning.\",\n    \"Inspirations\": \"Existing methods for DDI prediction and event prediction using various drug features, the need for better integration and combination of features for improved performance.\"\n  },\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b14\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b16\": 0.8,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b29\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting DDI-associated events, emphasizing the limitations of existing methods which mainly predict whether two drugs interact without considering the biological consequences of these interactions. The proposed DDIMDL method leverages diverse drug features (chemical substructures, targets, enzymes, pathways) and deep learning to predict fine-grained DDI events from DrugBank data.\",\n  \"Direct Inspiration\": [\"b24\", \"b14\"],\n  \"Indirect Inspiration\": [\"b6\", \"b16\", \"b26\"],\n  \"Other Inspiration\": [\"b7\", \"b17\", \"b20\", \"b28\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is predicting drug-drug interaction (DDI) events using computational methods to reduce time and cost associated with traditional wet experiments. The authors propose a multimodal deep learning framework named DDIMDL, which combines diverse drug features (chemical substructures, targets, enzymes, and pathways) to predict DDI events. The paper defines a standard protocol for analyzing DDI events in DrugBank and constructs a joint DNN framework to learn cross-modality representations of drug-drug pairs.\",\n  \"Direct Inspiration\": {\n    \"b24\": 1.0,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b34\": 0.5,\n    \"b7\": 0.4,\n    \"b28\": 0.4\n  }\n}\n```"], "5f6f14c49fced0a24bb647ec": ["```json\n{\n  \"Summary\": \"The main challenge addressed in the paper is the identification of drug-drug interactions (DDIs), which is crucial due to the increasing use of combinational drug treatments. The proposed algorithm, DPDDI, leverages a Graph Convolution Network (GCN) to learn low-dimensional feature representations of drugs in a DDI network and uses a Deep Neural Network (DNN) to predict potential interactions. The paper highlights the limitations of existing text mining-based and machine learning-based methods and demonstrates the superior performance of DPDDI in predicting DDIs compared to state-of-the-art methods.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1.0,\n    \"b18\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b12\": 0.8,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b16\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b20\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the identification of drug-drug interactions (DDIs) in the context of combinational treatments, which are crucial yet expensive and time-consuming to detect. The authors propose a novel method, DPDDI, that leverages a Graph Convolution Network (GCN) to learn low-dimensional feature representations of drugs and a Deep Neural Network (DNN) to predict potential DDIs. The method aims to address limitations in existing computational approaches by effectively capturing drug properties and interactions without requiring extensive manual feature engineering.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b20\": 0.8,\n    \"b21\": 0.8,\n    \"b22\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the identification of drug-drug interactions (DDIs) in a cost-effective and time-efficient manner. The paper introduces a novel prediction method, DPDDI, which utilizes a graph convolution network (GCN) to learn low-dimensional feature representations of drugs and a deep neural network (DNN) to predict potential DDIs. The approach aims to overcome the limitations of existing text mining-based and machine learning-based methods, especially the reliance on handcrafted features and the absence of certain drug properties.\",\n  \"Direct Inspiration\": [\"b17\", \"b18\"],\n  \"Indirect Inspiration\": [\"b5\", \"b6\", \"b10\", \"b12\", \"b16\", \"b19\", \"b21\", \"b22\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": {\n        \"challenges\": \"The primary challenges outlined in the paper are the urgent need to identify drug-drug interactions (DDIs) efficiently and accurately due to the increasing use of combinational treatments for complex diseases. Existing methods, particularly those based on text mining and machine learning, have limitations such as dependency on handcrafted features and inability to detect unannotated DDIs.\",\n        \"inspirations\": \"The paper is inspired by the limitations of existing computational approaches and the potential of Graph Convolutional Networks (GCNs) to overcome these limitations. The authors propose a novel method, DPDDI, which integrates GCNs for feature extraction and Deep Neural Networks (DNN) for prediction.\"\n    },\n    \"Direct Inspiration\": {\n        \"references\": [\"b17\", \"b18\", \"b19\", \"b20\", \"b21\", \"b22\"]\n    },\n    \"Indirect Inspiration\": {\n        \"references\": [\"b1\", \"b5\", \"b6\", \"b10\", \"b12\", \"b14\", \"b16\"]\n    },\n    \"Other Inspiration\": {\n        \"references\": [\"b2\", \"b3\", \"b4\", \"b7\", \"b8\", \"b9\", \"b11\", \"b13\", \"b15\", \"b23\", \"b24\", \"b25\", \"b26\", \"b27\", \"b28\", \"b29\", \"b30\"]\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting drug-drug interactions (DDIs), which is crucial for combinational drug treatments but difficult due to the time-consuming and costly nature of traditional detection methods. The proposed solution, DPDDI, leverages a graph convolution network (GCN) to extract low-dimensional features of drugs and a deep neural network (DNN) to predict potential DDIs. This method aims to overcome the limitations of existing methods by effectively capturing the topological relationships between drugs in a network without relying heavily on handcrafted features.\",\n  \"Direct Inspiration\": {\n    \"b17\": 1,\n    \"b18\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b5\": 0.7,\n    \"b6\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b21\": 0.5\n  }\n}\n```"], "5dc3eb4e3a55ac3c4bb65817": ["```json\n{\n    \"Summary\": \"The primary challenge outlined in this paper is the speed of incremental inference in the Transformer model, specifically due to memory bandwidth limitations caused by reloading large 'keys' and 'values' tensors. The authors propose an architectural variation called multi-query attention to improve inference speed with minor quality degradation.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b3\": 0.7,\n        \"b5\": 0.7\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the speed of incremental inference in the Transformer model, particularly related to the memory bandwidth required for reloading large tensors. The proposed solution is an architectural variation called multi-query attention, which improves inference speed with minor quality degradation.\",\n  \"Direct Inspiration\": [\"b4\"],\n  \"Indirect Inspiration\": [\"b2\", \"b5\", \"b3\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the speed of incremental inference in Transformer neural sequence models by proposing an architectural variation called multi-query attention. This method aims to reduce the memory bandwidth limitations associated with reloading the large 'keys' and 'values' tensors in attention layers.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b5\": 0.75,\n    \"b3\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of slow incremental inference speed in Transformer models caused by memory bandwidth limitations. It proposes an architectural variation called multi-query attention to enhance inference speed with minimal quality degradation.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b5\": 0.8,\n    \"b3\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of speeding up the incremental inference of the Transformer neural sequence model, which is limited by memory bandwidth due to the large 'keys' and 'values' tensors. The authors propose a variation called multi-query attention, which improves inference speed with minor quality degradation. The paper reviews the multi-head-attention layers, provides a performance analysis, and introduces the multi-query attention architecture.\",\n    \"Direct Inspiration\": [\"b4\"],\n    \"Indirect Inspiration\": [\"b0\", \"b2\", \"b3\", \"b5\"],\n    \"Other Inspiration\": []\n}\n```"], "5de7997c9e795e77580692f9": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in constrained reinforcement learning (RL), focusing on the non-convexity of the problem and the computational difficulties associated with hyper-parameter tuning. The authors propose a novel approach that reformulates the problem to have zero duality gap and shows that this can be extended to parametrized policies such as neural networks. The main contributions are theoretical guarantees for the duality gap and a primal-dual algorithm for solving constrained RL problems.\",\n  \"Direct Inspiration\": [\"b6\", \"b10\"],\n  \"Indirect Inspiration\": [\"b1\", \"b2\", \"b3\", \"b5\", \"b11\"],\n  \"Other Inspiration\": [\"b21\", \"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The main challenges addressed in this paper involve the complexity and computational intensity of selecting penalty coefficients in multi-objective reinforcement learning, and the interference between goals leading to training plateaus. The authors propose a novel approach to solve constrained reinforcement learning problems by establishing zero duality gap for policies in a general distribution class and extending this result to parametrized policies, showing that primal-dual algorithms converge to the optimal solution under mild assumptions.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.8,\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b4\": 0.8\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in constrained reinforcement learning (RL), particularly the multi-objective problem and the need for manually selecting penalty coefficients. It proposes a theoretical framework showing that constrained RL can be solved exactly in the dual domain, even for non-convex problems, and extends these results to parametrized policies, demonstrating convergence to optimal solutions with primal-dual algorithms under mild assumptions.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b10\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b11\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b3\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in constrained reinforcement learning (RL) by proposing a dual domain approach to solve non-convex RL problems. It establishes that constrained RL for policies in a general distribution class has zero duality gap and extends this result to parametrized policies, e.g., neural networks.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b21\": 0.7,\n    \"b22\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b3\": 0.6,\n    \"b18\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of balancing multiple conflicting requirements in reinforcement learning (RL). The proposed algorithm leverages constrained reinforcement learning and primal-dual methods to automatically choose parameters, ensuring satisfying behavior without manual selection of penalty coefficients. The key theoretical contributions include proving zero duality gap for general distribution policies and showing that the suboptimality bound holds for parametrized policies, such as neural networks.\",\n  \"Direct Inspiration\": [\"b6\", \"b10\"],\n  \"Indirect Inspiration\": [\"b11\", \"b21\"],\n  \"Other Inspiration\": [\"b1\", \"b2\", \"b3\", \"b18\"]\n}\n```"], "5c0f87a5da562944ac95a190": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the development of new drugs using computational methods, specifically virtual screening (VS) for drug-target interaction (DTI) prediction. The paper explores various data resources and novel machine learning approaches, emphasizing recent applications of deep learning techniques.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.85,\n    \"b7\": 0.75,\n    \"b9\": 0.80,\n    \"b16\": 0.70,\n    \"b28\": 0.65,\n    \"b32\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b36\": 0.60,\n    \"b40\": 0.55,\n    \"b48\": 0.50,\n    \"b51\": 0.45\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the identification of novel bioactive compounds for target proteins, identification of new targets for known bioactive compounds, and the high cost and time consumption of high-throughput screening experiments. The paper introduces virtual screening (VS) methods, particularly focusing on the use of machine learning and deep learning techniques to predict drug-target interactions (DTI).\",\n  \"Direct Inspiration\": {\n    \"b22\": 0.95,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.85,\n    \"b27\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b32\": 0.7,\n    \"b33\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on the challenges and advancements in computational drug discovery with a specific emphasis on Virtual Screening (VS) methods. The key challenges include identifying novel bioactive compounds and new targets for known compounds. The paper covers various computational methods and machine learning approaches, particularly deep learning, that have shown promise in improving the efficiency and effectiveness of VS. It discusses the benefits, limitations, and recent advancements in different VS methodologies, including structure-based, ligand-based, and proteochemometric modeling (PCM). The authors also explore various data resources, libraries, and toolkits essential for VS, as well as gold-standard datasets for benchmarking and evaluating these methods.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b23\": 0.9,\n    \"b36\": 0.85,\n    \"b37\": 0.85,\n    \"b40\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.75,\n    \"b33\": 0.75,\n    \"b28\": 0.7,\n    \"b35\": 0.7,\n    \"b120\": 0.7,\n    \"b121\": 0.7,\n    \"b122\": 0.7,\n    \"b123\": 0.7,\n    \"b124\": 0.7,\n    \"b125\": 0.7,\n    \"b47\": 0.7,\n    \"b48\": 0.7,\n    \"b49\": 0.7,\n    \"b41\": 0.65,\n    \"b42\": 0.65,\n    \"b43\": 0.65,\n    \"b44\": 0.65,\n    \"b45\": 0.65,\n    \"b46\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b24\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b27\": 0.6,\n    \"b29\": 0.6,\n    \"b30\": 0.6,\n    \"b31\": 0.6,\n    \"b34\": 0.6,\n    \"b38\": 0.6,\n    \"b39\": 0.6,\n    \"b50\": 0.6,\n    \"b51\": 0.6,\n    \"b52\": 0.6,\n    \"b53\": 0.6,\n    \"b54\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"New drug development, high cost and time of high-throughput screening, identification of bioactive compounds and targets.\",\n    \"inspirations\": \"Computational methods like virtual screening, machine learning approaches, deep learning techniques.\"\n  },\n  \"Direct Inspiration\": {\n    \"b22\": 0.9,\n    \"b23\": 0.8,\n    \"b16\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.7,\n    \"b33\": 0.7,\n    \"b36\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.65,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenges are identifying novel bioactive compounds for target proteins and new targets for known bioactive compounds.\",\n    \"inspirations\": \"The paper takes inspiration from the advancements in computational methods such as machine learning and deep learning for virtual screening (VS) and drug-target interaction (DTI) prediction.\"\n  },\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b23\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b28\": 0.8,\n    \"b32\": 0.8,\n    \"b33\": 0.8,\n    \"b36\": 0.8,\n    \"b40\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b41\": 0.7,\n    \"b42\": 0.7,\n    \"b43\": 0.7,\n    \"b44\": 0.7,\n    \"b45\": 0.7,\n    \"b46\": 0.7\n  }\n}\n```"], "53e9b11db7602d9703b99bc4": ["```json\n{\n    \"Summary\": \"The paper addresses the memory gap challenge in microprocessors by proposing a novel hardware prefetching technique called Stream Chaining. This technique aims to improve the accuracy and timeliness of prefetches by chaining localized streams, partially reconstructing chronological information and building longer streams. The proposed Miss Graph prefetching scheme is demonstrated to outperform existing PC/DC and G/DC prefetchers in various benchmarks.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.9,\n        \"b5\": 0.9,\n        \"b14\": 0.8,\n        \"b20\": 0.8,\n        \"b23\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.7,\n        \"b16\": 0.7,\n        \"b22\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges posed by the memory gap, specifically the increasing disparity between microprocessor execution speed and memory access latency. It introduces a novel prefetching algorithm called Stream Chaining, which aims to improve the accuracy and timeliness of prefetches by chaining localized streams of memory misses. The proposed method is evaluated against state-of-the-art prefetchers, demonstrating improved performance in terms of accuracy and memory bandwidth utilization.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b5\": 0.75,\n    \"b14\": 0.85,\n    \"b20\": 0.8,\n    \"b23\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the memory gap issue caused by the disparity between microprocessor speed and memory access latency. It proposes a novel prefetching algorithm called Stream Chaining, which enhances the prediction accuracy and timeliness by chaining localized streams, particularly focusing on PC localized prefetching. The proposed method is evaluated against state-of-the-art prefetchers and shows significant performance improvement.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.7,\n    \"b5\": 0.7,\n    \"b14\": 0.7,\n    \"b20\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the memory gap challenge, specifically the latency in memory access compared to processor execution speed. It proposes a novel algorithm called Stream Chaining to improve the prediction accuracy and timeliness of hardware prefetchers. By chaining localized streams, the algorithm reconstructs chronological information and builds longer, more accurate streams. The proposed method, Miss Graph prefetching, is evaluated against existing prefetchers and shows significant performance improvements.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b3\": 0.8,\n        \"b5\": 0.8,\n        \"b14\": 0.8,\n        \"b20\": 0.8,\n        \"b23\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b10\": 0.7,\n        \"b16\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the memory gap challenge in modern microprocessors, particularly focusing on the limitations and issues of hardware prefetching. It introduces the concept of stream chaining to improve the accuracy and timeliness of prefetching by combining localized streams into longer, more predictable sequences. The novel Miss Graph prefetching scheme is proposed and evaluated against existing methods, showing improved performance.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b5\": 0.8,\n    \"b14\": 0.8,\n    \"b20\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```"], "53e9af67b7602d97039a85ee": ["```json\n{\n  \"Summary\": \"The paper addresses the memory-access performance bottleneck in microprocessors by proposing Last-Touch Correlated Data Streaming (LT-cords), an address-correlating predictor design. The key challenge is achieving both long lookahead and high coverage in memory access predictions without requiring impractically large on-chip storage. LT-cords combines off-chip storage for long sequences of last-touch signatures with on-chip storage for timely predictions, showing significant performance improvements over existing techniques.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b19\": 0.7,\n    \"b23\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.5,\n    \"b17\": 0.4,\n    \"b25\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of bridging the performance gap between microprocessors and memory by proposing Last-Touch Correlated Data Streaming (LT-cords). LT-cords combines the prediction timeliness of on-chip lookup with the high coverage enabled by off-chip predictor storage, aiming to prefetch data accurately and timely into the L1D cache. This method seeks to overcome the limitations of previous predictors like DBCP and delta-correlating GHB by using off-chip storage for long repetitive sequences of last-touch signatures while maintaining practicality with reduced on-chip storage requirements.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0,\n    \"b14\": 0.9,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.6,\n    \"b23\": 0.6,\n    \"b18\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.4,\n    \"b16\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the performance gap between microprocessors and memory due to long-latency memory accesses. It proposes a new prefetching algorithm called Last-Touch Correlated Data Streaming (LT-cords) that combines on-chip and off-chip storage to achieve high prediction timeliness and coverage. LT-cords aims to mitigate the inefficiencies of existing prefetchers, particularly the Dead-Block Correlating Prefetcher (DBCP), by using off-chip storage to record long repetitive sequences of last-touch signatures while keeping only a minimal amount of data on chip.\",\n  \"Direct Inspiration\": [\"b11\", \"b23\"],\n  \"Indirect Inspiration\": [\"b14\", \"b19\"],\n  \"Other Inspiration\": [\"b6\", \"b13\", \"b15\", \"b20\", \"b21\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the performance gap between microprocessors and memory, particularly the limitations of existing prefetching techniques to mitigate long-latency memory accesses. The paper proposes the Last-Touch Correlated Data Streaming (LT-cords) algorithm, which combines the prediction timeliness of last-touch on-chip lookup with the high coverage enabled by off-chip predictor storage.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b19\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.6,\n    \"b17\": 0.6,\n    \"b4\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the memory-access performance bottleneck faced by modern processors, specifically the latency issues with long-latency memory accesses and the limited effectiveness of existing prefetching techniques. The paper proposes the Last-Touch Correlated Data Streaming (LT-cords) algorithm, which aims to combine the prediction timeliness of on-chip lookup with the high coverage enabled by off-chip predictor storage. The key innovation is using off-chip storage to record long repetitive sequences of last-touch signatures, drastically reducing on-chip storage requirements while maintaining prediction accuracy and timeliness.\",\n  \"Direct Inspiration\": {\n    \"b11\": 1,\n    \"b19\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```"], "5b8c9f2b17c44af36f8b4fc1": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting drug-drug interactions (DDIs) using a novel two-layered meta-learning framework that combines positive-unlabeled (PU) learning and network embedding algorithms. The primary inspiration for the paper comes from the need for more efficient and accurate computational methods to predict DDIs, given the limitations of traditional methods like clinical trials and adverse drug event reports.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.8,\n    \"b20\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b42\": 0.6,\n    \"b43\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting drug-drug interactions (DDIs) using a computational approach due to the complexity and high cost of experimental methods. The authors propose a two-layered meta-learning framework combining semi-supervised learning-based classifiers and a network embedding algorithm to enhance DDI prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1.0,\n    \"b17\": 0.9,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.6,\n    \"b24\": 0.6,\n    \"b42\": 0.7,\n    \"b43\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.4,\n    \"b32\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of predicting drug-drug interactions (DDIs) using a novel two-layered meta-learning framework that integrates various data sources and leverages Positive-Unlabeled (PU) learning. The proposed method aims to improve the accuracy of DDI predictions using a bagging SVM classifier and a network embedding algorithm.\",\n  \"Direct Inspiration\": {\n    \"b25\": 0.95,\n    \"b11\": 0.90,\n    \"b20\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.80,\n    \"b23\": 0.75,\n    \"b24\": 0.70\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.65,\n    \"b42\": 0.60\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is predicting drug-drug interactions (DDIs) to improve drug safety and reduce adverse effects. The authors propose a two-layered meta-learning framework using a bagging SVM classifier to enhance prediction accuracy by learning from different data sources and generating meta-knowledge. This framework addresses the limitations of traditional binary classification by leveraging Positive-Unlabeled (PU) learning and network embedding to handle the high-dimensional feature representation of drug pairs.\",\n  \"Direct Inspiration\": {\n    \"b25\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b17\": 0.9,\n    \"b20\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.8,\n    \"b23\": 0.75,\n    \"b24\": 0.7,\n    \"b42\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of predicting drug-drug interactions (DDIs) using a computational approach due to the limitations of conventional methods such as clinical trials and adverse drug event reports. The proposed solution includes a two-layered meta-learning framework utilizing Positive-Unlabeled (PU) learning and a bagging SVM classifier. The framework integrates multiple data sources to create feature networks and employs a network embedding algorithm for better representation learning.\",\n    \"Direct Inspiration\": {\n        \"b11\": 0.9,\n        \"b25\": 0.95,\n        \"b17\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b23\": 0.7,\n        \"b20\": 0.75,\n        \"b43\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.6,\n        \"b24\": 0.65,\n        \"b19\": 0.5\n    }\n}\n```"], "53e9afd3b7602d9703a24a4b": ["```json\n{\n  \"Summary\": \"The paper addresses challenges in bridging the gap between processor and memory performance, particularly focusing on cache miss rates and prefetching inefficiencies. It proposes Dead-Block Predictors (DBPs) and Dead-Block Correlating Prefetchers (DBCPs) to improve cache performance by predicting when a cache block becomes evictable and using this prediction to prefetch data more effectively.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b10\": 0.6,\n    \"b18\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed in the paper is the performance gap between processor and memory, exacerbated by high miss rates in higher cache levels and the limited capacity of conventional cache hierarchies. The paper critiques existing Miss Correlating Prefetchers (MCPs) for their lack of timeliness and limited prediction accuracy and coverage.\",\n    \"inspirations\": \"The paper proposes Dead-Block Predictors (DBPs) and Dead-Block Correlating Prefetchers (DBCPs) to tackle these challenges by predicting when a cache block becomes evictable and prefetching subsequent addresses accordingly. The approach leverages trace-based prediction mechanisms to improve the timeliness and accuracy of data prefetching.\"\n  },\n  \"Direct Inspiration\": {\n    \"b6\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b2\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.7,\n    \"b10\": 0.7,\n    \"b16\": 0.6,\n    \"b14\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges of high miss rates in conventional cache hierarchies and the limitations of existing prefetching techniques in mitigating memory latency. It introduces Dead-Block Predictors (DBPs) and Dead-Block Correlating Prefetchers (DBCPs) as novel mechanisms to predict when a cache block becomes evictable and to prefetch data more effectively based on dead-block traces.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b2\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b16\": 0.6,\n        \"b3\": 0.6,\n        \"b5\": 0.6,\n        \"b19\": 0.5,\n        \"b10\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the high miss rates and reduced performance due to limited capacity and simple data placement mechanisms in conventional cache hierarchies. The proposed algorithm, Dead-Block Predictors (DBPs) and Dead-Block Correlating Prefetchers (DBCPs), aims to predict when a cache block becomes evictable and use address correlation to enable timely prefetching of data, thus mitigating these issues.\",\n  \"Direct Inspiration\": [\"b6\"],\n  \"Indirect Inspiration\": [\"b4\", \"b2\"],\n  \"Other Inspiration\": [\"b19\", \"b10\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the performance gap between processor and memory due to high miss rates in conventional cache hierarchies. The paper proposes novel hardware mechanisms called Dead-Block Predictors (DBPs) and Dead-Block Correlating Prefetchers (DBCPs) to predict when a cache block becomes evictable and prefetch subsequent data to improve cache performance.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b4\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.6,\n    \"b16\": 0.6,\n    \"b14\": 0.6,\n    \"b19\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```"], "599c782b601a182cd25a765e": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of predicting drug-drug interactions (DDIs) using structural similarities and integrated pharmacokinetic (PK) and pharmacodynamic (PD) networks. The proposed algorithm assumes that a query drug (Dq) and a drug to be examined (De) tend to interact if Dq is structurally similar to the drugs in De's network that interact with the enzymes/transporters/target proteins of De.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b7\": 0.9,\n    \"b8\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.7,\n    \"b13\": 0.6,\n    \"b15\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b9\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in this paper is predicting drug-drug interactions (DDIs) using structural similarities of drugs and integrated pharmacokinetics (PK) and pharmacodynamics (PD) information. The authors propose a novel model that utilizes a network-based approach to combine various similarity scores, including structural, enzyme-related, transporter-related, and target-related scores, to predict potential DDIs.\",\n    \"Direct Inspiration\": {\n        \"b5\": 1.0,\n        \"b7\": 0.9,\n        \"b10\": 0.8,\n        \"b14\": 0.7\n    },\n    \"Indirect Inspiration\": {\n        \"b6\": 0.6,\n        \"b13\": 0.5,\n        \"b15\": 0.5\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.4,\n        \"b11\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"Challenges\": \"The primary challenge addressed in the paper is the prediction of drug-drug interactions (DDIs) to reduce adverse effects and prioritize drug candidates during development. The authors propose a model that uses structural similarities of drugs within pharmacokinetic (PK) and pharmacodynamic (PD) networks to predict DDIs.\",\n    \"Inspirations\": \"The authors were inspired by various approaches including structural similarity analysis, network modeling, and integration of multiple similarity measures from prior research.\"\n  },\n  \"Direct Inspiration\": {\n    \"b5\": 1.0,\n    \"b6\": 0.9,\n    \"b7\": 0.9,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b11\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b14\": 0.5,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting drug-drug interactions (DDI) based on structural similarities and interaction networks involving pharmacokinetics (PK) and pharmacodynamics (PD). The authors propose models that integrate structural similarity scores from both PK and PD networks to predict DDIs more effectively. They highlight the importance of combining multiple types of information for better prediction accuracy.\",\n  \"Direct Inspiration\": {\n    \"b5\": 1,\n    \"b6\": 0.9,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b8\": 0.7,\n    \"b10\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b9\": 0.5,\n    \"b15\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting drug-drug interactions (DDIs) by developing models based on the structural similarities of drugs within pharmacokinetic (PK) and pharmacodynamic (PD) networks. The proposed approach integrates various similarity scores and demonstrates the importance of combining PK and PD information for accurate DDI prediction.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.95,\n    \"b9\": 0.85,\n    \"b7\": 0.80\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.75,\n    \"b6\": 0.70,\n    \"b13\": 0.65,\n    \"b4\": 0.60\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.55,\n    \"b15\": 0.50,\n    \"b12\": 0.45\n  }\n}\n```"], "5ecc763e9e795e81e9307559": ["```json\n{\n  \"Summary\": \"The paper identifies challenges in existing pre-training models for NLP, specifically the limitations of BERT's Masked Language Modeling (MLM) and XLNet's Permuted Language Modeling (PLM). It proposes MPNet, which unifies MLM and PLM to address their issues while retaining their advantages. MPNet models token dependencies and leverages full positional information, improving performance on NLP tasks.\",\n  \"Direct Inspiration\": [\"b4\", \"b30\"],\n  \"Indirect Inspiration\": [\"b11\", \"b17\"],\n  \"Other Inspiration\": [\"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing pre-training models, specifically BERT and XLNet, by proposing a new method called MPNet. MPNet aims to combine the benefits of masked language modeling (MLM) and permuted language modeling (PLM) while mitigating their respective drawbacks. The main challenges are improving the modeling of dependencies among predicted tokens and reducing the discrepancy between pre-training and fine-tuning. MPNet is designed to take both the dependency among predicted tokens and the position information of all tokens into account, using a unified view of MLM and PLM and incorporating position compensation.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b30\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b14\": 0.6,\n    \"b23\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) in pre-training models like BERT and XLNet. The proposed MPNet model unifies MLM and PLM to capture both the dependency among predicted tokens and the position information of all tokens in a sequence. This leads to improved performance on various NLP tasks by reducing the discrepancy between pre-training and fine-tuning.\",\n  \"Direct Inspiration\": [\"b4\", \"b30\"],\n  \"Indirect Inspiration\": [\"b11\", \"b14\", \"b23\"],\n  \"Other Inspiration\": [\"b16\", \"b17\", \"b6\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is improving the pre-training of language models by combining the advantages of Masked Language Modeling (MLM) and Permuted Language Modeling (PLM) while mitigating their respective limitations. The proposed method, MPNet, aims to model the dependency among predicted tokens and reduce the discrepancy between pre-training and fine-tuning by utilizing position information and two-stream self-attention with position compensation.\",\n  \"Direct Inspiration\": [\"b4\", \"b30\"],\n  \"Indirect Inspiration\": [\"b11\", \"b26\"],\n  \"Other Inspiration\": [\"b14\", \"b23\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the limitations of existing pre-training methods for language models, specifically Masked Language Modeling (MLM) as used in BERT and Permuted Language Modeling (PLM) as used in XLNet. The proposed algorithm, MPNet, aims to combine the advantages of MLM and PLM while addressing their respective drawbacks. MPNet incorporates the dependency among predicted tokens and full positional information to improve pre-training efficiency and performance on downstream tasks.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b30\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b11\": 0.8,\n    \"b26\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b23\": 0.6,\n    \"b16\": 0.5,\n    \"b6\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```"], "573697636e3b12023e64a731": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the inefficiency of traditional thread-agnostic cache replacement schemes in managing shared last-level caches (LLC) for task-based parallel applications on multicore processors. The proposed algorithm leverages task dependency information from the runtime to create a hardware-software technique for more efficient LLC management by tracking inter-task data reuse and optimizing cache partitioning based on future task needs.\",\n  \"Direct Inspiration\": [\"b3\", \"b11\", \"b2\"],\n  \"Indirect Inspiration\": [\"b5\", \"b28\", \"b27\"],\n  \"Other Inspiration\": [\"b25\", \"b26\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the inefficiency of traditional thread-agnostic LRU-based cache replacement schemes in managing shared LLCs for multicore processors, particularly for task-based parallel programming models. It proposes a novel hardware-software technique that leverages the dependency-aware nature of task-parallel models to improve cache management and overall performance.\",\n  \"Direct Inspiration\": [\"b3\", \"b5\", \"b11\", \"b2\", \"b28\"],\n  \"Indirect Inspiration\": [\"b23\", \"b27\"],\n  \"Other Inspiration\": [\"b19\", \"b31\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies of traditional thread-agnostic cache replacement schemes for shared last-level caches (LLC) in multicore processors, focusing on task-based parallelism. It proposes a novel hardware-software technique that leverages runtime-managed task dependencies to improve cache utilization by tracking inter-task data reuse and creating dynamic partitions based on future task usage.\",\n  \"Direct Inspiration\": [\"b3\", \"b11\"],\n  \"Indirect Inspiration\": [\"b2\", \"b5\"],\n  \"Other Inspiration\": [\"b23\", \"b27\", \"b28\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of managing shared last-level caches (LLC) for task-parallel applications in multicore processors. The proposed solution is a hardware-software technique that leverages runtime information to efficiently partition the LLC based on inter-task data reuse, leading to improved cache utilization and performance.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b5\": 1,\n    \"b11\": 1,\n    \"b2\": 1,\n    \"b28\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b27\": 0.8,\n    \"b6\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b31\": 0.6,\n    \"b25\": 0.6,\n    \"b26\": 0.6,\n    \"b19\": 0.6,\n    \"b12\": 0.5,\n    \"b20\": 0.5,\n    \"b30\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies of traditional thread-agnostic cache replacement schemes for shared last-level caches (LLCs) in multicore processors, especially in the context of task-based parallelism. The proposed technique leverages runtime-managed task dependency information to create task-based partitions for the LLC, aiming to optimize cache utilization and improve performance.\",\n  \"Direct Inspiration\": [\"b3\", \"b5\", \"b11\", \"b2\", \"b28\"],\n  \"Indirect Inspiration\": [\"b23\", \"b27\"],\n  \"Other Inspiration\": [\"b1\", \"b4\", \"b8\"]\n}\n```"], "53e9afa5b7602d97039f09be": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of high leakage power in traditional SRAM caches by proposing a combined static and dynamic optimization approach for hybrid caches consisting of SRAM and STT-RAM. The proposed strategy aims to leverage the advantages of both static compiler hints and dynamic hardware adjustments to improve cache endurance and reduce energy consumption.\",\n  \"Direct Inspiration\": [\"b4\", \"b5\", \"b10\"],\n  \"Indirect Inspiration\": [\"b2\", \"b7\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of high leakage power in traditional SRAM caches by proposing a hybrid cache architecture combining SRAM and STT-RAM. The key innovation is a combined static and dynamic optimization approach for data placement, which aims to improve the endurance and reduce the energy consumption of the hybrid cache while maintaining performance.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b5\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.6,\n    \"b3\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in the paper is the endurance limitation and energy consumption of STT-RAM in hybrid cache designs. The proposed solution is a combined approach of static and dynamic optimizations, where the compiler provides initial data placement hints and the hardware dynamically corrects these placements based on runtime behavior.\",\n    \"Direct Inspiration\": {\n        \"b4\": 1.0,\n        \"b5\": 1.0,\n        \"b10\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b7\": 0.8,\n        \"b8\": 0.8\n    },\n    \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the endurance and reducing the energy consumption of hybrid SRAM and STT-RAM caches. It proposes a combined approach that leverages both static (compiler-generated hints) and dynamic (runtime cache behavior) optimizations to achieve this goal.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1.0,\n    \"b5\": 1.0,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b10\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.7,\n    \"b8\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of high leakage power in traditional SRAM caches by introducing a combined static and dynamic optimization approach for hybrid caches using both SRAM and STT-RAM. The proposed method seeks to improve endurance, reduce system energy, and maintain performance by integrating compiler hints with runtime hardware adjustments.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b5\": 1,\n    \"b7\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.65\n  }\n}\n```"], "5c757416f56def979890a549": ["```json\n{\n  \"Summary\": \"The paper challenges the widely accepted Triadic Closure Principle (TCP) in predicting protein-protein interactions (PPIs) and proposes a new principle, the L3 principle, which considers paths of length three. The new method outperforms existing PPI prediction algorithms and helps uncover novel interactions related to diseases such as retinitis pigmentosa.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b18\": 0.7,\n    \"b19\": 0.7,\n    \"b20\": 0.6,\n    \"b21\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately predicting human protein-protein interactions (PPIs), highlighting the limitations of the Triadic Closure Principle (TCP) in this context. The authors propose a novel link prediction principle based on paths of length three (L3), which aligns with structural and evolutionary constraints. This principle significantly outperforms existing methods and has broad applicability in understanding biological functions and disease mechanisms.\",\n  \"Direct Inspiration\": {\n    \"references\": [\"b6\", \"b9\", \"b10\", \"b11\", \"b12\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b4\", \"b5\", \"b7\", \"b8\", \"b13\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b14\", \"b15\", \"b16\", \"b17\", \"b18\", \"b19\", \"b20\", \"b21\", \"b22\", \"b23\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the inadequacy of the Triadic Closure Principle (TCP) in predicting protein-protein interactions (PPIs) in biological networks. The authors propose a novel link prediction principle, based on paths of length three (L3), which accounts for structural and evolutionary constraints and significantly outperforms existing methods. The L3 principle is experimentally validated and shown to provide new insights into diseases like retinitis pigmentosa.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b14\": 0.9,\n    \"b15\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b16\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.6,\n    \"b23\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper challenges the validity of the Triadic Closure Principle (TCP) in predicting protein-protein interactions (PPIs) and proposes a new principle based on paths of length three (L3) that better aligns with structural and evolutionary constraints. The L3 principle demonstrates superior predictive power compared to existing methods and is experimentally validated to uncover novel protein interactions, particularly in the context of diseases like Retinitis Pigmentosa.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b14\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b18\": 0.8,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b23\": 0.6,\n    \"b33\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting protein-protein interactions (PPIs) by highlighting the limitations of the Triadic Closure Principle (TCP) in biological contexts and introducing a new principle based on paths of length three (L3). The authors provide empirical evidence that TCP is invalid for most protein pairs and propose the L3 principle, which is rooted in both structural and evolutionary arguments. The L3 principle significantly outperforms existing link prediction methods and is experimentally validated to uncover novel protein interactions, including those linked to diseases like retinitis pigmentosa.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b16\": 0.9,\n    \"b19\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.8,\n    \"b15\": 0.8,\n    \"b17\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b10\": 0.7,\n    \"b18\": 0.7\n  }\n}\n```"], "55a49d7565ceb7cb02d42720": ["```json\n{\n    \"Summary\": \"The paper addresses the significant issue of adverse drug events (ADEs), particularly the limitations of current signal detection algorithms in handling sampling variance and reporting biases. The authors propose a new method called SCRUB, which adapts propensity score matching (PSM) to address these biases without requiring stratification. This method is inspired by case-control approaches in clinical studies and aims to dampen or remove the effect of covariates, making it suitable for systematic application and routine analysis. The method is evaluated against several standards and shows improved performance in predicting ADEs, drug targets, and drug indications.\",\n    \"Direct Inspiration\": {\n        \"b9\": 0.9,\n        \"b10\": 0.9,\n        \"b11\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.7,\n        \"b6\": 0.7,\n        \"b7\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b12\": 0.5,\n        \"b13\": 0.5,\n        \"b14\": 0.5\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of identifying and correcting synthetic associations in adverse drug event (ADE) reporting systems. Traditional signal detection algorithms are limited by sampling variance and reporting biases. The authors propose a new approach called SCRUB, inspired by case-control methods and propensity score matching (PSM), to remove biases without predefined stratification. The SCRUB method adapts PSM to use co-reported drugs and indications, successfully removing synthetic associations from indications, co-prescriptions, and hidden covariates.\",\n    \"Direct Inspiration\": [\"b9\", \"b10\", \"b11\"],\n    \"Indirect Inspiration\": [\"b6\", \"b7\"],\n    \"Other Inspiration\": [\"b13\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the identification and correction of biases in adverse drug event (ADE) reporting systems, specifically sampling variance and selection biases. The proposed algorithm, SCRUB, is inspired by the case-control approach to cohort selection in observational clinical studies and aims to remove synthetic associations without the need for stratification. The paper leverages propensity score matching (PSM) and adapts it to work within the constraints of adverse event reporting systems. The method significantly enhances the predictive power of ADE signal detection algorithms.\",\n  \"Direct Inspiration\": {\n    \"b9\": 1,\n    \"b10\": 1,\n    \"b11\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b6\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.7,\n    \"b14\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the issue of adverse drug events (ADEs), focusing on the limitations of current signal detection algorithms due to sampling variance and selection biases. It introduces a new method, SCRUB, inspired by case-control approaches and adapted propensity score matching (PSM) to correct synthetic associations without needing predefined covariates. SCRUB's effectiveness is demonstrated through its application to real-world data, showing improved performance in detecting true drug-event associations and constructing comprehensive off-label and polypharmacy side-effect databases.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b7\": 0.8,\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b8\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of adverse drug events (ADEs), especially those that are rare or occur in specific populations, and the limitations of current signal detection algorithms due to sampling variance and reporting biases. The authors propose a new method called SCRUB (Statistical Correction of Uncharacterized Bias) that adapts propensity score matching to address these biases without the need for stratification. This new method is designed to complement existing signal detection approaches and improve the reliability of drug-event association data.\",\n  \"Direct Inspiration\": {\n    \"b9\": 0.9,\n    \"b10\": 0.9,\n    \"b11\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.8,\n    \"b4\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b13\": 0.7,\n    \"b14\": 0.7\n  }\n}\n```"], "573695886e3b12023e4a8d45": ["```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge addressed is predicting new drug-target associations using a network-based approach without prior training datasets.\",\n    \"algorithm\": \"The authors propose a random walk-based link prediction algorithm on a combined heterogeneous network of drug-drug, target-target, and drug-target interactions.\"\n  },\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b4\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b12\": 0.6,\n    \"b13\": 0.6,\n    \"b14\": 0.6,\n    \"b15\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.4,\n    \"b5\": 0.4,\n    \"b7\": 0.3,\n    \"b8\": 0.3,\n    \"b9\": 0.3,\n    \"b10\": 0.3,\n    \"b11\": 0.3\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting drug-target interactions using a network-based approach. It leverages a random walk-based link prediction algorithm, inspired by previous work, to evaluate its performance on a heterogeneous network of drugs and targets. The method incorporates multiple similarity measures and explores various parameters to optimize prediction performance.\",\n  \"Direct Inspiration\": [\"b2\"],\n  \"Indirect Inspiration\": [\"b4\", \"b6\"],\n  \"Other Inspiration\": [\"b1\", \"b3\", \"b5\", \"b12\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of predicting drug-target interactions using a network-based approach. It introduces a random walk-based link prediction algorithm applied to a heterogeneous network combining drug-drug, target-target, and drug-target interactions. The paper evaluates the approach using an external dataset and explores various parameters to optimize the prediction performance.\",\n  \"Direct Inspiration\": [\"b2\"],\n  \"Indirect Inspiration\": [\"b0\", \"b1\", \"b3\", \"b4\", \"b5\"],\n  \"Other Inspiration\": [\"b6\", \"b7\", \"b8\", \"b9\", \"b10\", \"b11\"]\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The primary challenge is to predict new drug-target interactions using a network-based approach without relying on training datasets or target preference.\",\n    \"algorithm\": \"The authors propose a random walk-based link prediction algorithm applied to a heterogeneous drug-target network, inspired by Chen et al.\"\n  },\n  \"Direct Inspiration\": [\"b2\"],\n  \"Indirect Inspiration\": [\"b4\", \"b6\"], \n  \"Other Inspiration\": [\"b1\", \"b5\", \"b10\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of predicting drug-target interactions using a network-based approach that combines drug-drug, target-target, and drug-target networks. The authors use a random walk with restart algorithm inspired by previous work (Chen et al.) to generate proximity scores and identify potential drug-target interactions.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b4\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b5\": 0.6,\n    \"b6\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.4,\n    \"b3\": 0.4\n  }\n}\n```"], "5a260c0217c44a4ba8a1d1dd": ["```json\n{\n  \"Summary\": \"The main challenges addressed in the paper are the variety of performance goals, the complexity of performance surfaces across different systems, and the overhead involved in configuration tuning. The proposed algorithm, BestConfig, adopts a sampling method with wide space coverage and a search-based optimization algorithm to handle these challenges within resource limits. The architecture is designed to be flexible and extensible, allowing easy integration with different systems and workloads.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b10\": 0.9,\n    \"b18\": 0.9,\n    \"b39\": 0.9,\n    \"b41\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b7\": 0.8,\n    \"b13\": 0.8,\n    \"b15\": 0.8,\n    \"b20\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.7,\n    \"b6\": 0.7,\n    \"b12\": 0.7,\n    \"b19\": 0.7,\n    \"b27\": 0.7,\n    \"b30\": 0.7,\n    \"b35\": 0.7,\n    \"b43\": 0.7,\n    \"b44\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges of configuration tuning in cloud systems, which include the variety, complexity, and overhead of tuning processes. The proposed solution, BestConfig, aims to optimize performance goals by adjusting configuration parameters. It adopts a sampling method with wide space coverage and a search-based optimization algorithm. Key contributions include an automatic configuration tuning system, a flexible architecture, and novel sampling and optimization methods.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b18\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b10\": 0.8,\n    \"b39\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.65,\n    \"b26\": 0.65,\n    \"b32\": 0.65,\n    \"b33\": 0.65,\n    \"b3\": 0.7,\n    \"b41\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of automatic configuration tuning for general systems in the cloud, focusing on the variety of performance goals, complexity of performance surfaces, overhead of sample collection, and high-dimensional parameter space. It introduces BestConfig, an automatic configuration tuning system that optimizes performance by adjusting configuration parameters within a resource limit using a novel sampling method (DDS) and optimization algorithm (RBS).\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.9,\n    \"b18\": 0.85,\n    \"b10\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.75,\n    \"b5\": 0.7,\n    \"b39\": 0.7,\n    \"b41\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.65,\n    \"b13\": 0.6,\n    \"b20\": 0.55,\n    \"b37\": 0.55,\n    \"b44\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of automatic configuration tuning for general systems in the cloud, focusing on variety, complexity, and overhead. It proposes BestConfig, an automatic configuration tuning system that optimizes performance goals by adjusting configuration parameters within a given resource limit. BestConfig uses a novel combination of a sampling method (Divide and Diverge Sampling) and an optimization algorithm (Recursive Bound and Search) to efficiently navigate high-dimensional parameter spaces and limited samples.\",\n  \"Direct Inspiration\": {\n    \"b15\": 0.95,\n    \"b10\": 0.9,\n    \"b18\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b39\": 0.75,\n    \"b41\": 0.75,\n    \"b20\": 0.7,\n    \"b7\": 0.7,\n    \"b13\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b4\": 0.6,\n    \"b26\": 0.6,\n    \"b32\": 0.6,\n    \"b33\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the complexity of configuration tuning in cloud systems due to the variety of systems, performance goals, and high-dimensional parameter spaces. The proposed BestConfig system aims to automate configuration tuning to optimize performance within resource limits by employing a divide-and-diverge sampling method and a recursive-bound-and-search optimization algorithm.\",\n  \"Direct Inspiration\": {\n    \"b5\": 0.9,\n    \"b10\": 0.85,\n    \"b18\": 0.8,\n    \"b39\": 0.75\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.65,\n    \"b7\": 0.6,\n    \"b13\": 0.6,\n    \"b41\": 0.55\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.5,\n    \"b19\": 0.5,\n    \"b20\": 0.45,\n    \"b27\": 0.4,\n    \"b30\": 0.4,\n    \"b35\": 0.4\n  }\n}\n```"], "599c7cdf601a182cd27e33f3": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of conducting black-box attacks on DNN classifiers where the adversary has no information about the DNN's structure or parameters and lacks access to a large training dataset. The proposed algorithm involves training a local substitute DNN with a synthetic dataset generated by the adversary. This substitute is used to craft adversarial examples that are misclassified by both the substitute and the target DNN due to similar decision boundaries.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b8\": 1,\n    \"b13\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.9,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of black-box attacks against DNN classifiers, where the adversary has no knowledge of the model's architecture or training data. The proposed algorithm trains a local substitute DNN using a synthetic dataset generated by the adversary, which is then used to craft adversarial examples that are misclassified by both the substitute and target DNNs.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b8\": 1.0,\n    \"b13\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b17\": 0.8,\n    \"b11\": 0.8,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b10\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the vulnerability of DNN classifiers to black-box adversarial attacks. The proposed algorithm trains a local substitute DNN using a synthetic dataset generated by the adversary and labeled by observing the target DNN's output. This substitute is then used to craft adversarial examples that can mislead the target DNN.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b8\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b13\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.8,\n    \"b17\": 0.7,\n    \"b11\": 0.7,\n    \"b14\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in this paper is the vulnerability of DNN classifiers to black-box adversarial attacks where the adversary has no information about the model's structure or parameters and does not have access to any large training dataset. The authors propose a novel attack strategy that involves training a local substitute DNN with a synthetic dataset and crafting adversarial examples using the substitute, which are then misclassified by both the substitute and target DNN due to similar decision boundaries.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b8\": 1,\n    \"b13\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b11\": 0.7,\n    \"b14\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b10\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of black-box attacks against DNN classifiers, where adversaries have no knowledge of the model's architecture or training data. The authors propose a novel attack strategy that involves training a local substitute DNN with a synthetic dataset generated by the adversary, using only the labels assigned by the target DNN. This approach allows for crafting adversarial examples that can mislead the target DNN.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b8\": 0.85,\n    \"b13\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b17\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b10\": 0.6,\n    \"b11\": 0.6\n  }\n}\n```"], "5cede0f6da562983788d5a5f": ["```json\n{\n  \"Summary\": \"The paper focuses on advancing Relational Graph Convolutional Networks (RGCNs) by integrating attention mechanisms, resulting in Relational Graph Attention Networks (RGATs). The primary challenge addressed is the limitations of spectral methods in handling relational graphs. The authors propose two RGAT variants: Within-Relation Graph Attention (WIRGAT) and Across-Relation Graph Attention (ARGAT), each employing either additive or multiplicative attention mechanisms. Extensive experiments are conducted to evaluate these models against benchmarks, with the goal of improving performance on transductive node classification and inductive graph classification tasks.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1.0,\n    \"b33\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b32\": 0.8,\n    \"b38\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the generalization of Relational Graph Convolutional Networks (RGCNs) away from their spectral origins to address their limitations. The paper proposes a novel approach, Relational Graph Attention Networks (RGATs), extending attention mechanisms to relational graph domains with two variants: Within-Relation Graph Attention (WIRGAT) and Across-Relation Graph Attention (ARGAT). The paper also performs extensive hyperparameter searches and evaluates these models on node classification and graph classification tasks.\",\n  \"Direct Inspiration\": [\"b28\", \"b33\"],\n  \"Indirect Inspiration\": [\"b12\", \"b32\", \"b38\"],\n  \"Other Inspiration\": [\"b22\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generalizing Neural Networks to non-Euclidean domains, specifically focusing on Relational Graph Convolutional Networks (RGCNs) and proposing Relational Graph Attention Networks (RGATs) as an extension. The key contribution is the introduction of attention mechanisms to the relational graph domain, with variants such as Within-Relation Graph Attention (WIRGAT) and Across-Relation Graph Attention (ARGAT). The paper performs comprehensive evaluations and presents detailed results to aid further research.\",\n  \"Direct Inspiration\": {\n    \"b28\": 1,\n    \"b33\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.6,\n    \"b22\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenge addressed in this paper is the generalization of Relational Graph Convolutional Networks (RGCNs) away from their spectral origins to develop Relational Graph Attention Networks (RGATs). The authors propose two variants, WIRGAT and ARGAT, using either additive or multiplicative attention mechanisms. The motivation is to overcome the limitations of spectral methods and to enhance performance on node classification and graph classification tasks.\",\n    \"Direct Inspiration\": {\n        \"b28\": 1,\n        \"b33\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b38\": 0.8,\n        \"b32\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b22\": 0.7,\n        \"b12\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of generalizing Relational Graph Convolutional Networks (RGCNs) to overcome their spectral limitations. The authors propose Relational Graph Attention Networks (RGATs), incorporating attention mechanisms into the relational graph domain. They introduce two variants, WIRGAT and ARGAT, with additive and multiplicative attention, and evaluate these models on node classification and graph classification tasks.\",\n  \"Direct Inspiration\": [\"b33\", \"b28\"],\n  \"Indirect Inspiration\": [\"b38\"],\n  \"Other Inspiration\": [\"b32\"]\n}\n```"], "573698426e3b12023e70bf59": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the efficient scheduling of threads on SMT cores, specifically for the IBM POWER8 architecture. This is complicated by the high level of interference among threads sharing core resources. The proposed solution is an online scheduler leveraging existing CPI stack mechanisms to predict interference and optimize scheduling without additional hardware or offline profiling.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b19\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b17\": 0.5,\n    \"b13\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficient scheduling in manycore/manythread systems, particularly focusing on SMT cores within CMP architectures like IBM POWER8. The proposed solution is an online scheduler that leverages CPI stack accounting to predict thread interference and optimize scheduling without the need for additional hardware or sampling.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"The paper addresses scheduling challenges in manycore/manythread systems, particularly for chip multicore processors (CMP) with simultaneous multithreading (SMT) cores. The goal is to optimize performance by reducing thread interference and improving resource sharing.\",\n    \"algorithm\": \"The paper proposes an online scheduler for IBM POWER8 architecture, leveraging CPI stacks to predict thread interference and dynamically adjust scheduling without requiring additional hardware or offline profiling.\"\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b6\", \"b19\", \"b23\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b5\", \"b7\", \"b10\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b15\"]\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenges in scheduling for manycore/manythread systems, particularly focusing on the IBM POWER8 architecture that supports simultaneous multithreading (SMT). The authors propose an online model-based scheduler that leverages CPI stack measurements to predict interference among threads, aiming to optimize performance without additional hardware or offline profiling.\",\n    \"Direct Inspiration\": {\n        \"b6\": 1,\n        \"b19\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b23\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.6,\n        \"b18\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scheduling in manycore/manythread systems, particularly for the IBM POWER8 architecture, which involves significant sharing of resources among threads. The authors propose an online scheduler that leverages the CPI stack mechanism to predict interference among threads, aiming to optimize performance without requiring additional hardware or sampling.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.9,\n    \"b23\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.8,\n    \"b22\": 0.7\n  }\n}\n```"], "57d063e0ac44367354294777": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of fine-grain configurability in IaaS systems by proposing the CASH architecture and runtime. The main challenges include increased complexity for customers and non-convex optimization spaces. The CASH architecture, inspired by the Sharing Architecture, provides a configurable hardware-software interface and uses control theory and reinforcement learning to meet QoS guarantees while minimizing cost.\",\n  \"Direct Inspiration\": [\"b64\"],\n  \"Indirect Inspiration\": [\"b40\", \"b57\"],\n  \"Other Inspiration\": [\"b3\", \"b36\", \"b59\", \"b18\", \"b30\", \"b42\", \"b14\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in fine-grain configurability for IaaS customers, emphasizing the need for a simple interface and automated runtime management to optimize cost while meeting QoS guarantees. The proposed CASH (Cost-aware Adaptive Software and Hardware) architecture and runtime system integrate control theory and reinforcement learning to navigate non-convex optimization spaces and adapt to application phases, inspired by and improving upon the Sharing Architecture.\",\n  \"Direct Inspiration\": {\n    \"b64\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b22\": 0.8,\n    \"b27\": 0.8,\n    \"b43\": 0.8,\n    \"b51\": 0.8,\n    \"b14\": 0.8,\n    \"b40\": 0.8,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b36\": 0.6,\n    \"b59\": 0.6,\n    \"b18\": 0.6,\n    \"b30\": 0.6,\n    \"b42\": 0.6,\n    \"b4\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of fine-grain configurability in IaaS systems, proposing the CASH architecture and runtime to optimize cost and meet QoS guarantees. The novel contributions include a configurable architecture with a simple user interface and an adaptive runtime system using control theory and reinforcement learning.\",\n  \"Direct Inspiration\": [\"b64\"],\n  \"Indirect Inspiration\": [\"b40\", \"b57\"],\n  \"Other Inspiration\": [\"b19\", \"b22\", \"b27\", \"b43\", \"b51\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing fine-grain configurable resources in IaaS systems to meet QoS guarantees while minimizing cost. The proposed CASH architecture and runtime system dynamically reconfigures resources based on customer needs. The architecture is inspired by the Sharing Architecture but improves on it with faster reconfiguration, a well-defined software-hardware interface, and remote core performance monitoring. The runtime uses control theory and reinforcement learning for optimization.\",\n  \"Direct Inspiration\": {\n    \"b64\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b40\": 0.8,\n    \"b57\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b36\": 0.6,\n    \"b59\": 0.6,\n    \"b18\": 0.6,\n    \"b30\": 0.6,\n    \"b42\": 0.6,\n    \"b14\": 0.6,\n    \"b19\": 0.6,\n    \"b22\": 0.6,\n    \"b27\": 0.6,\n    \"b43\": 0.6,\n    \"b51\": 0.6,\n    \"b20\": 0.6,\n    \"b4\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of fine-grain configurability in IaaS systems by proposing the CASH architecture and runtime. The main challenges include the complexity of optimization due to non-convex spaces and the need for simple interfaces for customers. The CASH system combines a configurable hardware architecture and an optimizing runtime to meet QoS guarantees while minimizing costs.\",\n    \"Direct Inspiration\": {\n        \"b64\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b40\": 0.8,\n        \"b57\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.7,\n        \"b20\": 0.7\n    }\n}\n```"], "55a5d3fd65ce60f99bf6743c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in drug development, particularly the inefficacy of the 'one drug-one target' approach and the complexities of predicting drug safety. It proposes using network analysis, machine learning, and dynamical modeling to harness high-dimensional data for developing combination drug therapies (CDD). The paper highlights the potential of integrating systems biology and computational methods to identify effective drug combinations, emphasizing the need for innovative methods to predict adverse side effects and the importance of leveraging large-scale biological and medical data.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b15\": 0.9,\n    \"b16\": 0.9,\n    \"b35\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b26\": 0.7,\n    \"b27\": 0.7,\n    \"b46\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b8\": 0.5,\n    \"b9\": 0.5,\n    \"b10\": 0.5,\n    \"b12\": 0.5,\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5,\n    \"b30\": 0.5,\n    \"b31\": 0.5,\n    \"b32\": 0.5,\n    \"b33\": 0.5,\n    \"b34\": 0.5,\n    \"b36\": 0.5,\n    \"b37\": 0.5,\n    \"b38\": 0.5,\n    \"b42\": 0.5,\n    \"b43\": 0.5,\n    \"b44\": 0.5,\n    \"b47\": 0.5,\n    \"b48\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in drug development, particularly focusing on the ineffectiveness of the 'one drug-one target' paradigm, the limitations of traditional experimental model systems, and the difficulty of anticipating drug safety profiles. It proposes the use of combination drug design (CDD) and the integration of systems-level understanding, network analysis, machine learning, and high-dimensional data to develop and evaluate drug combinations.\",\n  \"Direct Inspiration\": {\n    \"inspired by\": [\"b30\"],\n    \"motivated by\": [\"b38\"],\n    \"take inspiration\": [\"b42\"]\n  },\n  \"Indirect Inspiration\": {\n    \"following\": [\"b35\", \"b32\", \"b36\"],\n    \"we use\": [\"b43\"]\n  },\n  \"Other Inspiration\": {\n    \"pioneering work\": [\"b7\", \"b28\", \"b41\"]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the stagnation in drug development, focusing on challenges in efficacy and safety of drugs. It proposes leveraging high-dimensional data and computational approaches, such as network analysis, machine learning, and dynamical modeling, to improve combinatorial drug design (CDD) and efficacy.\",\n  \"Direct Inspiration\": {\n    \"b30\": 1,\n    \"b35\": 1,\n    \"b36\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b25\": 0.8,\n    \"b26\": 0.8,\n    \"b27\": 0.8,\n    \"b41\": 0.8,\n    \"b43\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b12\": 0.6,\n    \"b32\": 0.6,\n    \"b45\": 0.6,\n    \"b47\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the stagnation in drug development and the challenges in efficacy and safety of experimental drugs. It proposes the use of network analysis, machine learning, and dynamical modeling to leverage high-dimensional data for the development of combinatorial drug design (CDD). The main focus is on using computational approaches to identify effective drug combinations, taking into account systems-level understanding and quantifiable outcomes.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b7\": 0.8,\n    \"b10\": 0.8,\n    \"b35\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.7,\n    \"b26\": 0.7,\n    \"b32\": 0.75,\n    \"b36\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.65,\n    \"b27\": 0.65,\n    \"b28\": 0.65,\n    \"b43\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges in drug development, focusing on the inefficacy of the 'one drug-one target' paradigm and the high failure rates due to efficacy and safety issues. It proposes leveraging network analysis, machine learning, and dynamical modeling to harness high-dimensional data for combinatorial drug design (CDD). The goal is to increase efficacy and reduce toxicity of drug combinations by understanding biological networks and interactions.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b32\": 0.9,\n    \"b35\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b36\": 0.7,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b16\": 0.6\n  }\n}\n```"], "53e99c2fb7602d97024dc163": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of dynamically determining the optimal hardware configuration for adaptive superscalar microarchitectures to maximize energy efficiency. The authors propose a machine learning-based control mechanism using runtime hardware counters to predict the best configuration for each program phase.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1.0,\n    \"b4\": 1.0,\n    \"b5\": 1.0,\n    \"b6\": 1.0,\n    \"b16\": 0.9,\n    \"b17\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7,\n    \"b18\": 0.6,\n    \"b19\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b0\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The main challenge addressed in this paper is dynamically determining the optimal hardware configuration for adaptive superscalar microarchitectures to maximize energy efficiency. The proposed solution involves a machine learning model that predicts the best configuration based on runtime hardware counters, significantly improving energy/performance efficiency without the need for extensive online searching.\",\n  \"Direct Inspiration\": [\"b3\", \"b4\", \"b5\", \"b6\"],\n  \"Indirect Inspiration\": [\"b0\", \"b1\", \"b2\"],\n  \"Other Inspiration\": [\"b16\", \"b17\", \"b18\", \"b19\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of dynamically determining optimal hardware configurations in adaptive superscalar microarchitectures to maximize energy efficiency and performance. It proposes a novel runtime resource management scheme using a soft-max machine learning model to predict the best hardware configurations based on runtime hardware counters.\",\n    \"Direct Inspiration\": {\n        \"b0\": 0.95,\n        \"b1\": 0.85,\n        \"b3\": 0.9,\n        \"b5\": 0.9,\n        \"b16\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.75,\n        \"b4\": 0.75,\n        \"b6\": 0.75,\n        \"b17\": 0.7,\n        \"b18\": 0.7,\n        \"b19\": 0.7\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of designing high-performance, power-efficient microprocessors through adaptive superscalar microarchitectures. It proposes a novel runtime resource management scheme using a soft-max machine learning model to predict the optimal hardware configuration for any program phase, based on dynamically gathered hardware counters.\",\n  \"Direct Inspiration\": {\n    \"b1\": 1.0,\n    \"b2\": 1.0,\n    \"b3\": 1.0,\n    \"b4\": 1.0,\n    \"b5\": 1.0,\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b18\": 0.7,\n    \"b19\": 0.7\n  },\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of dynamically determining the optimal hardware configuration for adaptive superscalar microarchitectures to maximize energy efficiency and performance. It proposes a runtime resource management scheme using a soft-max machine learning model based on runtime hardware counters to predict the best configuration for each program phase.\",\n  \"Direct Inspiration\": {\n    \"b0\": 1.0,\n    \"b3\": 0.9,\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.7,\n    \"b2\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b16\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6,\n    \"b19\": 0.6\n  }\n}\n```"], "5f7d9bfd91e011346ad27f0c": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of leveraging limited labeled data for multivariate time series (MTS) tasks by proposing a novel transformer-based methodology for unsupervised pre-training. The transformer encoder is used to create dense vector representations through an input denoising objective, which are then fine-tuned for various downstream tasks such as regression and classification. The method demonstrates superior performance over both traditional and deep learning models, even with limited labeled data.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b8\": 0.95,\n    \"b4\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.8,\n    \"b10\": 0.8,\n    \"b29\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.7,\n    \"b19\": 0.7,\n    \"b7\": 0.65,\n    \"b14\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of limited labeled data availability for multivariate time series (MTS) analysis and proposes a transformer-based unsupervised pre-training methodology to extract dense vector representations of MTS. This approach leverages unlabeled data and achieves state-of-the-art performance on regression and classification tasks with minimal labeled data.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b8\": 0.9,\n    \"b4\": 0.85,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b14\": 0.75,\n    \"b12\": 0.7,\n    \"b18\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b10\": 0.55,\n    \"b19\": 0.5,\n    \"b7\": 0.45,\n    \"b9\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of leveraging unlabeled multivariate time series data for unsupervised representation learning and subsequent regression and classification tasks. The authors propose using a transformer encoder, inspired by its success in NLP, to develop a methodology that can pre-train on unlabeled data and fine-tune on a limited amount of labeled data. The novel contribution lies in adapting transformers for time series data and demonstrating their effectiveness across various datasets and tasks.\",\n  \"Direct Inspiration\": [\n    \"b31\",\n    \"b8\",\n    \"b26\"\n  ],\n  \"Indirect Inspiration\": [\n    \"b4\",\n    \"b14\",\n    \"b18\"\n  ],\n  \"Other Inspiration\": [\n    \"b28\",\n    \"b19\",\n    \"b7\"\n  ]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of using limited labeled multivariate time series data by leveraging a transformer encoder for unsupervised representation learning. Inspired by the success of transformers in NLP, the authors develop a methodology that uses unsupervised pre-training to extract dense vector representations, which are then applied to tasks like regression and classification.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b8\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b26\": 0.6,\n    \"b14\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of leveraging unlabeled data for multivariate time series (MTS) to improve accuracy in regression and classification tasks using transformer models. Inspired by successes in NLP with unsupervised pre-training, the authors propose a framework that adapts transformer encoders to MTS for unsupervised representation learning and downstream tasks.\",\n  \"Direct Inspiration\": [\"b31\", \"b8\"],\n  \"Indirect Inspiration\": [\"b4\", \"b26\", \"b14\"],\n  \"Other Inspiration\": [\"b10\", \"b28\", \"b19\", \"b7\", \"b9\"]\n}\n```"], "5d04e908da56295d08dd9fcb": ["```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the accurate representation and implementation of Markov chains for distance prefetching, especially when dealing with interleaving patterns and page limits. The proposed algorithm focuses on tracking deltas per page and using an efficient Markov chain representation to improve accuracy in predicting complex access patterns.\",\n  \"Direct Inspiration\": {\n    \"b1\": 0.9,\n    \"b4\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.7,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b6\": 0.75,\n    \"b7\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The primary challenges outlined in this paper are dealing with interleaving page access patterns that obfuscate delta streams in distance prefetching. The proposed solution is an efficient representation of a Markov chain for distance prefetching that tracks deltas per page instead of globally, using an LFU replacement policy to maintain transition probabilities and reconstruct obfuscated delta transitions.\",\n    \"Direct Inspiration\": {\n        \"b1\": 1,\n        \"b4\": 1,\n        \"b6\": 1\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.8,\n        \"b3\": 0.7,\n        \"b5\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in delta prefetching using Markov models, specifically focusing on overcoming invalid delta transitions due to interleaving page access patterns. It proposes a novel, efficient Markov-chain-based delta prefetching algorithm that tracks deltas per page and uses a set-associative cache to retain accurate transition probabilities.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.95,\n    \"b6\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b2\": 0.75,\n    \"b3\": 0.7,\n    \"b5\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving accuracy in Markov-chain based distance prefetching by introducing a more-efficient representation of Markov chains. The primary contributions include tracking deltas per page instead of globally, and using a novel set-associative cache structure for implementing the Markov chain with Least Frequently Used (LFU) replacement policy.\",\n    \"Direct Inspiration\": {\n        \"b1\": 0.9,\n        \"b4\": 0.9,\n        \"b6\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b2\": 0.7,\n        \"b3\": 0.7,\n        \"b5\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b7\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately prefetching data using Markov models by introducing an efficient representation of a Markov chain to improve delta-transition probability metrics. The proposed algorithm handles the complexities of delta transitions and memory page constraints, aiming to enhance the accuracy and efficiency of prefetching in modern systems.\",\n  \"Direct Inspiration\": [\"b1\", \"b4\", \"b6\"],\n  \"Indirect Inspiration\": [\"b2\", \"b3\", \"b5\"],\n  \"Other Inspiration\": []\n}\n```"], "5e85c28491e0114016e821dc": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of effectively predicting drug-target binding affinity (DTA) by proposing a novel end-to-end learning framework, DeepGS. This framework uniquely combines local chemical context and topological structure information, leveraging techniques like Smi2Vec and Prot2Vec for embedding, Graph Attention Network (GAT) for molecular structure, and BiGRU for sequence context. The main contributions are the integration of these methods to predict DTA without needing expert knowledge or 3D structures, demonstrating improved performance over existing methods.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b25\": 0.9,\n    \"b30\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b22\": 0.7,\n    \"b1\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b35\": 0.5,\n    \"b32\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces DeepGS, a novel end-to-end learning framework for drug-target binding affinity prediction, which integrates both the local chemical context and topological structure of drugs and targets. The framework leverages CNN, BiGRU, and GAT to extract relevant features for binding affinity prediction. The main challenges addressed include capturing the complementary nature of topological structure and local chemical context, and the novel approach of using Smi2Vec and Prot2Vec embeddings inspired by Word2Vec.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.8,\n    \"b25\": 0.8,\n    \"b30\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b32\": 0.6,\n    \"b4\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper tackles the challenge of predicting drug-target binding affinity (DTA) by introducing a novel framework called DeepGS. This framework combines local chemical context and molecular structure information using advanced techniques like Smi2Vec and Prot2Vec, and includes convolutional neural networks (CNN), graph attention networks (GAT), and bi-directional gated recurrent units (BiGRU). The authors aim to improve upon existing methods by leveraging both the topological structure and the local chemical context of molecules and protein sequences.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b25\": 0.9,\n    \"b30\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b1\": 0.75,\n    \"b32\": 0.7,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in predicting drug-target binding affinity (DTA) by proposing a novel model, DeepGS. The primary challenge is integrating both local chemical context and topological structure information of drugs and targets. The authors introduce a three-step framework utilizing CNN, BiGRU, and GAT models, along with embedding techniques (Smi2Vec and Prot2Vec), to encode and analyze the sequences and structures of drugs and proteins.\",\n  \"Direct Inspiration\": {\n    \"b19\": 0.9,\n    \"b25\": 0.85,\n    \"b1\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.75,\n    \"b32\": 0.75,\n    \"b4\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.65,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the problem of predicting drug-target binding affinity (DTA) by proposing a novel end-to-end learning framework called DeepGS. The primary challenge is to consider both the local chemical context and the molecular structure of drugs and targets. The main contributions include the integration of CNN, GAT, and BiGRU for extracting local chemical context and topological structure, respectively, and the use of Smi2Vec and Prot2Vec for encoding amino acids and atoms into distributed representations.\",\n  \"Direct Inspiration\": {\n    \"b19\": 1.0,\n    \"b25\": 0.9,\n    \"b1\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b10\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```"], "573697796e3b12023e66024b": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of limited Translation Lookaside Buffer (TLB) reach in virtual memory systems due to stagnant TLB sizes and growing memory capacities. The proposed solution, Redundant Memory Mappings (RMM), is a novel hardware/software co-design that improves translation performance by creating redundant mappings for ranges of contiguous virtual and physical pages. RMM includes a range TLB, range table, and eager paging allocation mechanism, which together significantly reduce page-walk overheads.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1.0,\n        \"b10\": 1.0,\n        \"b21\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.9,\n        \"b19\": 0.8,\n        \"b29\": 0.85\n    },\n    \"Other Inspiration\": {\n        \"b0\": 0.7,\n        \"b20\": 0.75\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the limited TLB reach problem caused by stagnant TLB sizes and increasing memory capacities. The authors propose Redundant Memory Mappings (RMM), a novel hardware/software co-design that enhances virtual memory translation performance by adding a redundant mapping mechanism to page tables, which efficiently translates contiguous virtual and physical page ranges.\",\n    \"Direct Inspiration\": {\n        \"b8\": 1.0,\n        \"b10\": 0.9,\n        \"b21\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b19\": 0.7,\n        \"b29\": 0.7,\n        \"b36\": 0.6,\n        \"b37\": 0.6,\n        \"b45\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b20\": 0.5,\n        \"b0\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": \"Limited TLB reach due to stagnant TLB sizes and growing memory capacities.\",\n    \"algorithm\": \"Redundant Memory Mappings (RMM), a hardware/software co-design that uses range translations to map contiguous virtual and physical pages, leveraging natural contiguity in address spaces.\"\n  },\n  \"Direct Inspiration\": [\"b4\", \"b8\", \"b10\", \"b21\"],\n  \"Indirect Inspiration\": [\"b0\", \"b19\", \"b29\", \"b36\", \"b37\", \"b45\"],\n  \"Other Inspiration\": [\"b20\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of limited TLB reach due to stagnant TLB sizes and growing memory capacities. It proposes Redundant Memory Mappings (RMM), a hardware/software co-design that leverages range translations to efficiently map contiguous virtual pages to contiguous physical pages, thus improving translation performance and reducing page-walks.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1.0,\n    \"b21\": 1.0,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b20\": 0.7,\n    \"b36\": 0.6,\n    \"b37\": 0.6,\n    \"b45\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b19\": 0.5,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the limited TLB reach due to stagnant TLB sizes in the face of growing modern memory capacities. The proposed solution, Redundant Memory Mappings (RMM), is a novel hardware/software co-design that improves translation performance by adding a redundant mapping for ranges of contiguous pages. This approach is designed to be transparent to applications and aims to reduce the overhead of virtual memory significantly.\",\n  \"Direct Inspiration\": {\n    \"b8\": 1,\n    \"b10\": 1,\n    \"b21\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b36\": 0.8,\n    \"b37\": 0.8,\n    \"b45\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.7,\n    \"b19\": 0.7,\n    \"b20\": 0.7,\n    \"b29\": 0.7\n  }\n}\n```"], "5f0d85c69fced0a24be4f028": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the limited TLB reach and the inadequacy of coarse granularity in conventional page sizes for modern processors. The proposed solution, Tailored Page Sizes (TPS), introduces support for pages sized at any power-of-two larger than the base page size to significantly reduce translation overhead, lower page walk memory references, and reduce L1 TLB misses. TPS involves changes to the operating system, ISA, and microarchitecture to leverage intermediate page sizes.\",\n  \"Direct Inspiration\": [\"b33\", \"b37\"],\n  \"Indirect Inspiration\": [\"b4\", \"b29\", \"b41\"],\n  \"Other Inspiration\": [\"b15\", \"b16\", \"b45\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the high overhead of virtual-to-physical address translations in modern computer systems due to limited TLB reach and the inadequacy of conventional, coarse-grained page sizes. The proposed algorithm, Tailored Page Sizes (TPS), aims to reduce translation overhead, lower page walk memory references, and reduce L1 TLB misses by introducing support for pages sized at any power-of-two larger than the base page size. TPS involves changes to the operating system software, the ISA, and the microarchitecture.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b37\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8,\n    \"b12\": 0.8,\n    \"b22\": 0.7,\n    \"b30\": 0.7,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b16\": 0.6,\n    \"b45\": 0.6,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the performance degradation caused by limited TLB reach in systems with increasing physical memory capacity. The proposed solution, Tailored Page Sizes (TPS), introduces support for pages sized at any power-of-two larger or equal to the base page size, aiming to reduce translation overhead, lower page walk memory references, and reduce L1 TLB misses in TLB intensive workloads.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1.0,\n    \"b37\": 1.0,\n    \"b45\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.9,\n    \"b22\": 0.8,\n    \"b30\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b12\": 0.7,\n    \"b31\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper include the inefficiency of current page-based virtual memory systems in handling large physical memory capacities due to limited TLB reach and the resulting high overhead of page walks. The novel method introduced is Tailored Page Sizes (TPS), which allows for pages of any power-of-two size larger than the base page size to reduce translation overhead, lower page walk memory references, and reduce L1 TLB misses. The algorithm includes architectural changes to support new page sizes, updates to TLB design, and modifications to OS paging strategies to leverage these new sizes effectively.\",\n  \"Direct Inspiration\": {\n    \"b7\": 1,\n    \"b33\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b37\": 0.8,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b48\": 0.5,\n    \"b42\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper proposes Tailored Page Sizes (TPS) to address the challenges of high virtual-to-physical translation overheads in systems with large physical memory. TPS allows for pages of any power-of-two size larger than the base page size, reducing translation overhead, page walk memory references, and L1 TLB misses. The paper outlines architectural and OS changes necessary to implement TPS and evaluates its performance using simulations and real system measurements.\",\n  \"Direct Inspiration\": [\"b33\"],\n  \"Indirect Inspiration\": [\"b7\", \"b12\", \"b22\", \"b30\", \"b31\"],\n  \"Other Inspiration\": [\"b4\", \"b6\", \"b14\", \"b20\"]\n}\n```"], "5b67b4b917c44aac1c867dbc": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is the flat nature of existing GNN architectures, which are incapable of inferring and aggregating information hierarchically, especially for graph classification tasks. The proposed algorithm, DIFFPOOL, introduces a differentiable graph pooling module to create hierarchical representations of graphs, enabling the construction of deeper GNN models. This method is inspired by spatial pooling operations in CNNs and aims to overcome the challenges of lacking natural spatial locality and varying graph sizes.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.95,\n    \"b15\": 0.9,\n    \"b20\": 0.85,\n    \"b28\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.75,\n    \"b24\": 0.7,\n    \"b39\": 0.65,\n    \"b34\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.55,\n    \"b16\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of integrating hierarchical structure into Graph Neural Networks (GNNs) for tasks like graph classification. It proposes DIFFPOOL, a differentiable pooling module that enables hierarchical, end-to-end learning within GNNs, improving their performance and interpretability.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b6\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.75,\n    \"b15\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b39\": 0.65,\n    \"b28\": 0.6,\n    \"b24\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of hierarchical information propagation in Graph Neural Networks (GNNs). It introduces DIFFPOOL, a differentiable graph pooling module that allows GNNs to operate hierarchically, improving graph classification tasks. The paper draws inspiration from methods that handle graph data and convolutional neural networks.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.8,\n    \"b6\": 0.7,\n    \"b10\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.6,\n    \"b39\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the limitation of current GNN architectures in propagating information in a hierarchical manner. To tackle this, the authors propose DIFFPOOL, a differentiable graph pooling module that operates hierarchically and end-to-end, which is inspired by spatial pooling in CNNs but adapted for the complex topological structure of graphs. The novel methods introduced include a hierarchical multi-layer model that learns a differentiable soft assignment for clustering nodes and combining it with GNN layers to generate hierarchical graph representations.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b15\": 0.9,\n    \"b20\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b10\": 0.7,\n    \"b24\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b22\": 0.6,\n    \"b28\": 0.6,\n    \"b39\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of hierarchical information aggregation in graph neural networks (GNNs). It proposes DIFFPOOL, a differentiable graph pooling module that enables hierarchical and end-to-end training of GNNs by learning soft cluster assignments.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b10\": 0.9,\n    \"b14\": 1,\n    \"b20\": 0.85,\n    \"b39\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.7,\n    \"b24\": 0.65,\n    \"b28\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b7\": 0.5,\n    \"b22\": 0.5,\n    \"b31\": 0.5,\n    \"b34\": 0.5\n  }\n}\n```"], "5d245bb5da56295a28fcca5f": ["```json\n{\n  \"Summary\": \"The paper identifies the limitations of existing GC models, particularly their inability to capture Gabor-like filters, which are crucial for hierarchical object representations. The authors propose a novel method, MixHop, which allows full linear mixing of neighborhood information at every message passing step, thus overcoming the limitations of previous models.\",\n  \"Direct Inspiration\": [\"b13\", \"b7\"],\n  \"Indirect Inspiration\": [\"b4\", \"b5\", \"b6\", \"b14\"],\n  \"Other Inspiration\": [\"b18\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing graph convolutional network (GCN) models in capturing graph analogues of Gabor filters due to their computational inefficiency and limited representational capacity. The proposed MixHop model allows linear mixing of neighborhood information at multiple distances, enhancing the model's expressiveness without increasing computational complexity.\",\n  \"Direct Inspiration\": [\"b13\"],\n  \"Indirect Inspiration\": [\"b7\", \"b4\"],\n  \"Other Inspiration\": [\"b8\", \"b9\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of current graph convolutional methods in capturing the graph analogue of Gabor filters. It proposes MixHop, a new Graph Convolutional layer that mixes powers of the adjacency matrix, allowing for full linear mixing of neighborhood information at every message passing step. The proposed method aims to enhance the representational capacity of graph convolutional networks without increasing the computational complexity.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b7\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b8\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b15\": 0.5,\n    \"b5\": 0.5,\n    \"b6\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of existing Graph Convolutional Network (GCN) approximations, which prevent models from capturing the graph analogue of Gabor filters. The proposed method, MixHop, allows full linear mixing of neighborhood information at every message-passing step, enhancing the expressiveness and representational capacity of graph convolution models.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.85,\n    \"b7\": 0.80\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.75,\n    \"b9\": 0.70,\n    \"b18\": 0.65\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses limitations in existing graph convolution models, particularly their inability to capture graph analogues of Gabor filters. The proposed MixHop model allows for full linear mixing of neighborhood information at every step, enhancing the representational capacity without increasing computational complexity.\",\n    \"Direct Inspiration\": {\n        \"b13\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b4\": 0.8,\n        \"b7\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b5\": 0.6,\n        \"b6\": 0.6,\n        \"b8\": 0.7,\n        \"b9\": 0.7,\n        \"b14\": 0.5\n    }\n}\n```"], "5ca72ae8181a2f3597ce6e45": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges of optimizing intra-layer parallelism and inter-layer pipelining in tiled neural network (NN) accelerators. It proposes Tangram, a scalable tiled accelerator with novel dataflow optimizations to improve performance and energy efficiency. Tangram introduces buffer sharing dataflow (BSD) to reduce data duplication and alternate layer loop ordering (ALLO) to optimize the forwarding of intermediate data, extending the applicability of inter-layer pipelining to complex DAG structures in advanced CNNs and LSTMs.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b43\": 0.9,\n    \"b39\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b21\": 0.7,\n    \"b24\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b45\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is how to scale and optimize neural network (NN) accelerators for larger and more complex NNs while improving performance and energy efficiency. The novel contribution of the paper is the Tangram architecture, which introduces optimized dataflow techniques such as Buffer Sharing Dataflow (BSD) and Alternate Layer Loop Ordering (ALLO) to address inefficiencies in existing tiled NN architectures.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b43\": 0.9,\n    \"b39\": 0.8,\n    \"b24\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.6,\n    \"b21\": 0.6,\n    \"b5\": 0.5,\n    \"b6\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b45\": 0.4,\n    \"b16\": 0.4,\n    \"b41\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of scalable and energy-efficient neural network (NN) acceleration, specifically focusing on the inefficiencies in existing dataflow schemes for coarse-grained parallelism in tiled NN architectures. The proposed Tangram architecture introduces novel dataflow optimizations, namely Buffer Sharing Dataflow (BSD) and Alternate Layer Loop Ordering (ALLO), to enhance intra-layer parallelism and inter-layer pipelining respectively. These optimizations reduce data duplication and buffer requirements, improving performance and energy efficiency.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b43\": 0.85,\n    \"b24\": 0.8,\n    \"b39\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.75,\n    \"b21\": 0.7,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.65,\n    \"b16\": 0.65,\n    \"b17\": 0.65,\n    \"b41\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of scalability and efficiency in neural network (NN) accelerators, specifically focusing on intra-layer and inter-layer parallelism. The proposed solution, Tangram, introduces optimized dataflow techniques, such as Buffer Sharing Dataflow (BSD) and Alternate Layer Loop Ordering (ALLO), to enhance performance and energy efficiency.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b43\": 1.0,\n    \"b39\": 0.9,\n    \"b24\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.8,\n    \"b21\": 0.8,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.6,\n    \"b6\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the performance and energy efficiency of neural network (NN) accelerators, particularly for large and complex networks. It introduces Tangram, a scalable tiled accelerator with novel dataflow optimizations for intra-layer parallelism and inter-layer pipelining. Key innovations include Buffer Sharing Dataflow (BSD) and Alternate Layer Loop Ordering (ALLO). These optimizations reduce data duplication, improve buffer utilization, and support complex Directed Acyclic Graph (DAG) structures in advanced Convolutional Neural Networks (CNNs) and Long Short-Term Memories (LSTMs).\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b39\": 0.85,\n    \"b43\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.75,\n    \"b24\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.7,\n    \"b45\": 0.7\n  }\n}\n```"], "53e99c53b7602d9702504c04": ["```json\n{\n  \"Summary\": \"The paper addresses the challenges in traditional processor instruction sets and proposes a Virtual Instruction Set Architecture (V-ISA) to provide flexibility and efficiency in hardware-software codesign. The V-ISA aims to decouple program representation from hardware implementation, enabling sophisticated compiler optimizations and cooperative hardware/software mechanisms.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1.0,\n    \"b13\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b22\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b20\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper discusses the challenges of traditional processor instruction sets and proposes a design for a Virtual Instruction Set Architecture (V-ISA) to overcome these limitations. The proposed V-ISA aims to support sophisticated compiler analyses and transformations, allow offline translation and caching, and provide a low-level, language-independent interface for arbitrary user and operating system software.\",\n    \"Direct Inspiration\": {\n        \"b31\": 1.0,\n        \"b13\": 0.9,\n        \"b10\": 0.9,\n        \"b25\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.7,\n        \"b23\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.6,\n        \"b22\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges of traditional processor instruction sets and proposes a Virtual Instruction Set Architecture (V-ISA) and an accompanying compilation strategy. It highlights the limitations of existing ISAs and presents a novel V-ISA design that supports sophisticated compiler analyses and transformations while being low-level enough for native hardware instruction sets. The paper also describes a translation strategy for offline translation and caching, leveraging aggressive optimization techniques.\",\n  \"Direct Inspiration\": {\n    \"b31\": 1,\n    \"b13\": 0.9,\n    \"b10\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b25\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b20\": 0.5,\n    \"b28\": 0.5,\n    \"b30\": 0.5,\n    \"b32\": 0.5,\n    \"b8\": 0.6,\n    \"b22\": 0.6,\n    \"b23\": 0.6,\n    \"b18\": 0.4,\n    \"b34\": 0.4,\n    \"b26\": 0.4,\n    \"b29\": 0.4,\n    \"b9\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in traditional processor instruction sets by proposing a Virtual Instruction Set Architecture (V-ISA) that decouples program representation from hardware interface. It aims to support sophisticated compiler analyses and transformations while being low-level enough for efficient hardware implementation. The design includes simple, RISC-like operations and a Static Single Assignment (SSA) representation, excluding execution-oriented features that obscure program behavior.\",\n    \"Direct Inspiration\": {\n        \"b31\": 1.0,\n        \"b13\": 0.9,\n        \"b10\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b14\": 0.75,\n        \"b25\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b4\": 0.6,\n        \"b20\": 0.65,\n        \"b8\": 0.7\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the limitations of traditional processor instruction sets and proposes a design for a Virtual Instruction Set Architecture (V-ISA). It highlights the challenges of designing a V-ISA that provides high-level program information and supports sophisticated compiler optimizations while being low-level enough for efficient hardware implementation. The proposed V-ISA aims to decouple program representation from the hardware interface, enabling cooperative hardware/software design and flexibility in processor implementations.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b31\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.7,\n    \"b20\": 0.7,\n    \"b28\": 0.7,\n    \"b30\": 0.7,\n    \"b32\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.6,\n    \"b22\": 0.6,\n    \"b8\": 0.5,\n    \"b23\": 0.5,\n    \"b18\": 0.5,\n    \"b34\": 0.5,\n    \"b26\": 0.5,\n    \"b29\": 0.5,\n    \"b25\": 0.4,\n    \"b9\": 0.4\n  }\n}\n```"], "5c234870da562935fc1d4da6": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately measuring performance events in modern processors with a limited number of hardware counters, particularly in cloud computing environments. It proposes CounterMiner, a methodology that leverages data mining and machine learning techniques to improve the accuracy of multiplexed hardware counter measurements by cleaning data, ranking the importance of events, and quantifying interactions between events.\",\n  \"Direct Inspiration\": {\n    \"b33\": 1,\n    \"b34\": 1,\n    \"b38\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b30\": 0.8,\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.7,\n    \"b15\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of accurately and efficiently measuring a large number of events with a small number of hardware counters in modern processors, especially in the context of cloud computing. The proposed solution, CounterMiner, employs data mining and machine learning techniques to improve the quality of counter data and quantify the importance and interaction of events. Key components include data cleaning, importance ranking, and interaction ranking.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b34\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b38\": 0.8,\n    \"b30\": 0.8,\n    \"b4\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.6,\n    \"b15\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in accurately measuring and understanding big performance data in modern cloud computing platforms using hardware counters. The proposed methodology, CounterMiner, leverages data mining and machine learning techniques to improve data quality and quantify the importance and interactions of events. The primary challenges include handling measurement errors in multiplexing (MLPX) and dealing with the high dimensionality of performance data.\",\n  \"Direct Inspiration\": [\"b33\", \"b34\"],\n  \"Indirect Inspiration\": [\"b38\"],\n  \"Other Inspiration\": [\"b7\", \"b29\", \"b30\", \"b41\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of measuring and understanding performance data from hardware counters in modern processors, particularly in cloud computing environments. It proposes CounterMiner, a methodology that leverages data mining and machine learning techniques to improve data quality, quantify event importance, and analyze event interactions.\",\n  \"Direct Inspiration\": {\n    \"b33\": 0.9,\n    \"b34\": 0.9,\n    \"b38\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.7,\n    \"b29\": 0.7,\n    \"b30\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b41\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Accuracy vs. efficiency trade-off in using hardware counters for performance measurement.\",\n      \"Measurement errors in multiplexing (MLPX) approach, especially in cloud computing environments.\",\n      \"Difficulty in extracting insights from large volumes of performance data.\"\n    ],\n    \"inspirations\": [\n      \"Need to handle MLPX measurement errors.\",\n      \"Importance of efficiently identifying and ranking significant performance events.\"\n    ],\n    \"novel_methods\": [\n      \"CounterMiner methodology with data cleaning, importance ranking, and interaction ranking components.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\"b30\", \"b33\", \"b34\"]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\"b7\", \"b29\", \"b38\"]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\"b15\", \"b40\", \"b42\"]\n  }\n}\n```"], "5c87a964da56296d04a90b97": ["```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the high tail latency in microsecond-scale tasks for cloud applications. The authors propose Shinjuku, a single-address space operating system that implements preemptive scheduling at the microsecond-scale to improve tail latency and throughput for both light and heavy-tailed service time distributions. Shinjuku leverages hardware support for virtualization and optimizes context switches to achieve efficient and frequent preemption.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1.0,\n    \"b44\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b34\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7,\n    \"b42\": 0.7,\n    \"b29\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving low tail latency and high throughput for latency-critical cloud services and microservices. It presents Shinjuku, a single-address space operating system that implements preemptive scheduling at the microsecond scale to optimize tail latency and throughput across a wide range of service time distributions.\",\n  \"Direct Inspiration\": [\"b14\", \"b44\"],\n  \"Indirect Inspiration\": [\"b12\", \"b21\", \"b34\", \"b35\"],\n  \"Other Inspiration\": [\"b20\", \"b42\", \"b29\", \"b37\"]\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper revolve around achieving low tail latency and high throughput for cloud applications with diverse workload distributions. The proposed solution, Shinjuku, is a single-address space operating system that implements preemptive scheduling at the microsecond scale. Shinjuku aims to improve tail latency and throughput by using centralized scheduling and leveraging hardware support for virtualization to enable fast preemption and context switching.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b44\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b20\": 0.8,\n    \"b29\": 0.8,\n    \"b37\": 0.8,\n    \"b42\": 0.8,\n    \"b43\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.7,\n    \"b24\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of achieving low tail latency for cloud services and microservices at the microsecond scale. It introduces Shinjuku, a single-address space operating system that implements preemptive scheduling to improve tail latency and throughput for various service time distributions. Shinjuku uses centralized scheduling and hardware support for virtualization to achieve low overheads for preemption and context switching.\",\n  \"Direct Inspiration\": {\n    \"b14\": 0.9,\n    \"b44\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.7,\n    \"b21\": 0.7,\n    \"b34\": 0.7,\n    \"b35\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.6,\n    \"b42\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of reducing tail latency for cloud applications, microservices, and in-memory databases by proposing Shinjuku, a single-address space operating system. Shinjuku uses preemptive scheduling at the microsecond scale, centralized queuing, and low-overhead mechanisms to improve tail latency and throughput for varied service time distributions.\",\n  \"Direct Inspiration\": {\n    \"b14\": 1,\n    \"b44\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.9,\n    \"b19\": 0.8,\n    \"b34\": 0.8,\n    \"b35\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b20\": 0.7,\n    \"b29\": 0.7,\n    \"b37\": 0.7,\n    \"b43\": 0.7\n  }\n}\n```"], "53e9ac54b7602d9703622ebc": ["```json\n{\n  \"Summary\": \"The paper proposes the Sharing Architecture, a novel manycore architecture designed for IaaS Cloud providers. The primary challenges addressed include the need for efficient matching of customer needs with computational resources, customization of micro-architecture hardware, and reuse of idle hardware resources. The architecture allows for fine-grain resource allocation and dynamic reconfiguration of Virtual Cores to optimize performance and cost.\",\n  \"Direct Inspiration\": [\"b22\", \"b62\", \"b9\", \"b15\", \"b47\", \"b48\", \"b51\", \"b61\", \"b63\"],\n  \"Indirect Inspiration\": [\"b31\", \"b32\"],\n  \"Other Inspiration\": []\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the inefficiencies of current commodity multicore processors in IaaS Cloud environments. It proposes the Sharing Architecture, a manycore architecture that enables dynamic reconfiguration, customization, and leasing of sub-core resources to better match customer workloads and optimize resource utilization.\",\n    \"Direct Inspiration\": {\n        \"b22\": 1.0,\n        \"b62\": 1.0,\n        \"b9\": 1.0,\n        \"b15\": 1.0,\n        \"b47\": 1.0,\n        \"b48\": 1.0,\n        \"b51\": 1.0,\n        \"b61\": 1.0,\n        \"b63\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b31\": 0.8,\n        \"b32\": 0.8\n    },\n    \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the inefficiencies of current IaaS Cloud systems by proposing a novel 'Sharing Architecture' that allows for fine-grain, sub-core allocation and customization of computational resources. This architecture aims to optimize the matching of computational resources with customer needs, thereby maximizing efficiency and profitability for Cloud providers.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b62\": 1.0,\n    \"b9\": 1.0,\n    \"b15\": 1.0,\n    \"b47\": 1.0,\n    \"b48\": 1.0,\n    \"b51\": 1.0,\n    \"b61\": 1.0,\n    \"b63\": 1.0\n  },\n  \"Indirect Inspiration\": {},\n  \"Other Inspiration\": {}\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing computational resources in IaaS Cloud systems by proposing the 'Sharing Architecture.' This architecture allows for dynamic reconfiguration and fine-grain resource allocation within multicore processors. The Sharing Architecture is particularly inspired by the need to match customer-specific application requirements with computational resources efficiently, thus maximizing both performance and profit for Cloud providers.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0,\n    \"b62\": 1.0,\n    \"b9\": 1.0,\n    \"b15\": 1.0,\n    \"b47\": 1.0,\n    \"b48\": 1.0,\n    \"b51\": 1.0,\n    \"b61\": 1.0,\n    \"b63\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b31\": 0.8,\n    \"b32\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b38\": 0.6,\n    \"b50\": 0.6,\n    \"b57\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the inefficiencies in current IaaS Cloud computing systems, proposing the Sharing Architecture to enable fine-grain resource allocation and efficient market pricing. The architecture allows for customizable Virtual Cores by reconfiguring sub-core resources like ALUs and cache, thus optimizing both Cloud providers' and users' needs.\",\n    \"Direct Inspiration\": {\n        \"b22\": 1.0,\n        \"b62\": 1.0,\n        \"b9\": 1.0,\n        \"b15\": 1.0,\n        \"b47\": 1.0,\n        \"b48\": 1.0,\n        \"b51\": 1.0,\n        \"b61\": 1.0,\n        \"b63\": 1.0\n    },\n    \"Indirect Inspiration\": {},\n    \"Other Inspiration\": {\n        \"b31\": 0.8,\n        \"b32\": 0.8,\n        \"b6\": 0.7,\n        \"b57\": 0.7,\n        \"b50\": 0.7\n    }\n}\n```"], "53e9b0e6b7602d9703b63850": ["```json\n{\n    \"Summary\": \"The paper addresses the challenge of integrating reconfigurable logic with general-purpose processors to improve performance and power efficiency while managing the complexity of multithreaded execution. The proposed solution involves dynamically managed multithreaded reconfigurable fabrics shared among several cores of a CMP. The paper introduces algorithms for managing thread assignments and spatial partitioning of SPL fabrics to optimize performance and utilization.\",\n    \"Direct Inspiration\": {\n        \"b44\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b23\": 0.8,\n        \"b45\": 0.8\n    },\n    \"Other Inspiration\": {\n        \"b3\": 0.6,\n        \"b4\": 0.5,\n        \"b42\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of integrating reconfigurable logic with microprocessors to improve performance and power efficiency while minimizing area costs. The proposed solution involves dynamically managed multithreaded reconfigurable fabrics shared among multiple cores of a CMP, managed through various approaches including machine learning and heuristic techniques.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b45\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b43\": 0.6,\n    \"b24\": 0.5,\n    \"b42\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the integration and management of reconfigurable logic within multi-core processors to improve performance and power efficiency. The paper proposes dynamically managed multithreaded reconfigurable fabrics that are shared among several cores of a CMP (Chip Multi-Processor). The authors present various management policies, including heuristic approaches and advanced machine learning techniques, to optimize thread assignment and spatial partitioning of the reconfigurable fabrics.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1.0,\n    \"b3\": 0.9,\n    \"b23\": 0.9,\n    \"b45\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.7,\n    \"b42\": 0.7,\n    \"b4\": 0.7,\n    \"b29\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b43\": 0.6,\n    \"b25\": 0.6,\n    \"b1\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is the efficient management of multithreaded reconfigurable fabrics in CMPs to save area and increase fabric utilization while maintaining performance. The paper proposes dynamically managed multithreaded SPL clusters, exploring approaches ranging from heuristic methods to advanced techniques like machine learning and stability analysis to manage thread assignment and spatial partitioning.\",\n  \"Direct Inspiration\": {\n    \"b44\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b23\": 0.75,\n    \"b45\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.5,\n    \"b42\": 0.5,\n    \"b4\": 0.5,\n    \"b29\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of managing reconfigurable fabrics in a CMP environment to optimize performance and power efficiency. It proposes dynamically managed multithreaded reconfigurable fabrics shared among multiple cores, inspired by the need to balance resource utilization and minimize area and power costs. Key inspirations include techniques for fabric architecture, virtualization, and dynamic management policies.\",\n  \"Direct Inspiration\": {\n    \"b44\": 0.9,\n    \"b3\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.7,\n    \"b45\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b42\": 0.5\n  }\n}\n```"], "5550415645ce0a409eb3a69e": ["```json\n{\n  \"Summary\": \"The paper focuses on improving the depth of Convolutional Networks (ConvNets) to achieve higher accuracy in large-scale image recognition tasks. Inspired by prior works, the authors propose several configurations of ConvNets, with varying depths, and demonstrate that deeper architectures with smaller convolution filters can significantly enhance performance.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b21\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.7,\n    \"b12\": 0.6,\n    \"b31\": 0.65\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b29\": 0.55,\n    \"b33\": 0.55\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of increasing the depth of Convolutional Networks (ConvNets) for improved image recognition accuracy. The authors propose a novel architecture with very small convolution filters (3x3) applied throughout the network, and they investigate the impact of depth by adding more convolutional layers. They achieve state-of-the-art accuracy on various image recognition tasks and provide detailed configurations and training methods for their models.\",\n  \"Direct Inspiration\": {\n    \"b21\": 1,\n    \"b3\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b29\": 0.6,\n    \"b33\": 0.6,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving the accuracy of Convolutional Networks (ConvNets) for large-scale image classification by increasing the depth of the network. The key inspiration comes from the use of small 3x3 convolution filters, which allows for deeper architectures without increasing the number of parameters excessively. The paper proposes several configurations of ConvNets with varying depths and evaluates their performance on the ILSVRC dataset.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b21\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b23\": 0.8,\n    \"b29\": 0.85,\n    \"b31\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.75,\n    \"b12\": 0.75,\n    \"b33\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper primarily addresses the challenge of improving ConvNet architecture by increasing its depth while using small convolution filters. This approach led to significantly more accurate ConvNet architectures, achieving state-of-the-art accuracy on ILSVRC classification and localization tasks and being applicable to other image recognition datasets. Key inspirations cited include the use of small convolution filters and multi-scale training and testing frameworks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b21\": 1,\n    \"b29\": 0.9,\n    \"b11\": 0.8,\n    \"b31\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.5,\n    \"b12\": 0.4,\n    \"b22\": 0.4\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.3,\n    \"b4\": 0.3,\n    \"b33\": 0.2,\n    \"b26\": 0.2,\n    \"b28\": 0.2,\n    \"b23\": 0.2,\n    \"b20\": 0.1,\n    \"b17\": 0.1\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is improving the depth of ConvNet architectures while maintaining or enhancing performance on large-scale image recognition tasks. The authors propose a novel approach involving smaller convolution filters (3\u00d73), increased network depth, and the elimination of Local Response Normalisation (LRN), leading to significant accuracy improvements on the ILSVRC classification and localization tasks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 1,\n    \"b21\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b29\": 0.8,\n    \"b31\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b33\": 0.6,\n    \"b12\": 0.5,\n    \"b23\": 0.4\n  }\n}\n```"], "5ac1829d17c44a1fda917eab": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of accurately estimating the fundamental frequency (f0) of a monophonic audio signal, particularly under noisy conditions and for diverse timbres. The proposed solution, CREPE, is a data-driven method based on a deep convolutional neural network that operates directly on the time-domain signal. CREPE demonstrates superior performance over traditional heuristic approaches such as pYIN and SWIPE, achieving over 90% raw pitch accuracy and robustness to noise.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b12\": 0.9,\n    \"b18\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.6,\n    \"b19\": 0.7,\n    \"b20\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b21\": 0.5,\n    \"b22\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of monophonic pitch estimation in audio signals, proposing a novel data-driven method called CREPE based on a deep convolutional neural network. This method shows superior performance compared to heuristic-based approaches like pYIN and SWIPE, especially in noisy environments and with complex timbres.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1.0,\n    \"b18\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b19\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.7,\n    \"b28\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of monophonic pitch tracking in audio signals, introducing a novel data-driven method called CREPE, which utilizes a deep convolutional neural network to outperform traditional heuristic methods like pYIN and SWIPE. The method is designed to provide more robust and precise pitch estimation, especially in noisy environments and for complex timbres.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b12\": 1,\n    \"b18\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b19\": 0.8,\n    \"b20\": 0.7,\n    \"b21\": 0.7,\n    \"b22\": 0.7,\n    \"b23\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b28\": 0.6,\n    \"b24\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of monophonic pitch estimation, proposing a novel data-driven method called CREPE based on a deep convolutional neural network. CREPE outperforms existing heuristic methods like pYIN and SWIPE, showing robustness to noise and maintaining high accuracy in pitch estimation.\",\n  \"Direct Inspiration\": {\n    \"b13\": 0.9,\n    \"b12\": 0.8,\n    \"b18\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.75,\n    \"b10\": 0.75,\n    \"b11\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b24\": 0.6,\n    \"b28\": 0.65,\n    \"b26\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of monophonic pitch tracking (estimating the fundamental frequency, f0) in audio signals, which is crucial for tasks in music signal processing and speech analysis. Traditional methods rely on candidate-generating functions and heuristics, but the novel contribution of this paper is a data-driven approach using a deep convolutional neural network (CREPE) that operates directly on the time-domain audio signal. CREPE achieves state-of-the-art results and is more robust to noise compared to existing methods like pYIN and SWIPE.\",\n  \"Direct Inspiration\": {\n    \"b13\": 1,\n    \"b12\": 1,\n    \"b18\": 1,\n    \"b19\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b24\": 0.9,\n    \"b20\": 0.8,\n    \"b21\": 0.8,\n    \"b22\": 0.8,\n    \"b23\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.7,\n    \"b7\": 0.7,\n    \"b8\": 0.7,\n    \"b9\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b14\": 0.7,\n    \"b15\": 0.7,\n    \"b16\": 0.7,\n    \"b17\": 0.7,\n    \"b25\": 0.7,\n    \"b26\": 0.7,\n    \"b27\": 0.7,\n    \"b28\": 0.7\n  }\n}\n```"], "5d1eb9e4da562961f0b1eb04": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning from graph-structured data by proposing two novel graph neural network (GNN) models: Relational Graph Dynamic Convolutional Networks (RGDCN) and Graph Neural Networks with Feature-wise Linear Modulation (GNN-FiLM). The paper highlights the importance of incorporating both source and target node representations in message passing, and introduces the use of hypernetworks to dynamically compute neural message passing functions.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b10\": 0.9,\n    \"b16\": 0.8,\n    \"b17\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.7,\n    \"b12\": 0.7,\n    \"b8\": 0.6,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper focuses on learning from graph-structured data using novel methods to improve neural message passing in Graph Neural Networks (GNNs). The primary challenges include the need for complex interactions between different information sources and computational feasibility. The proposed methods, RGDCN and GNN-FiLM, introduce dynamic computation of message passing functions and feature-wise linear modulations, respectively.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b18\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper introduces two novel formalisms for graph neural networks: Relational Graph Dynamic Convolutional Networks (RGDCN) and Graph Neural Networks with Feature-wise Linear Modulation (GNN-FiLM). The main challenges addressed include the need for non-trivial interaction between different information sources in neural networks and the computational expense of hypernetworks. The proposed methods aim to improve the expressiveness and computational feasibility of neural message passing in graphs.\",\n  \"Direct Inspiration\": {\n    \"b6\": 0.9,\n    \"b10\": 0.8,\n    \"b16\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.6,\n    \"b17\": 0.6,\n    \"b18\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.5,\n    \"b12\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses challenges in learning from graph-structured data, specifically the need for non-trivial interaction between different information sources in Graph Neural Networks (GNNs). The paper proposes two new formalisms: Relational Graph Dynamic Convolutional Networks (RGDCN) and Graph Neural Networks with Feature-wise Linear Modulation (GNN-FiLM). These methods aim to improve neural message passing by dynamically computing the message transformation function or using feature-wise affine transformations.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.8,\n    \"b17\": 0.8,\n    \"b18\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.5,\n    \"b8\": 0.6,\n    \"b12\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of learning from graph-structured data, particularly focusing on the need for efficient message passing mechanisms in Graph Neural Networks (GNNs). It proposes two novel models: Relational Graph Dynamic Convolutional Networks (RGDCN) and Graph Neural Networks with Feature-wise Linear Modulation (GNN-FiLM). These models aim to enhance the interaction between source and target nodes in the graph to improve the accuracy and efficiency of learning tasks.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0,\n    \"b10\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b16\": 0.9,\n    \"b18\": 0.85,\n    \"b17\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b5\": 0.75,\n    \"b8\": 0.7,\n    \"b12\": 0.7\n  }\n}\n```"], "5c20b1fcda5629702063aff8": ["```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Improving single-thread performance in the context of limited power budgets.\",\n      \"Reducing the complexity and power consumption of out-of-order (OoO) execution architectures.\"\n    ],\n    \"inspirations\": [\n      \"The need to eliminate inefficient register renaming in OoO cores.\",\n      \"Adopting a unique instruction set architecture (ISA) that bypasses register renaming.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\n      \"b6\"\n    ]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": [\n      \"b4\",\n      \"b5\",\n      \"b7\"\n    ]\n  },\n  \"Other Inspiration\": {\n    \"references\": [\n      \"b8\",\n      \"b10\",\n      \"b12\",\n      \"b13\"\n    ]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge outlined in the paper is improving single-thread performance while maintaining power efficiency in modern processors. The proposed solution is the STRAIGHT architecture, which eliminates the need for register renaming by using a unique instruction set architecture (ISA) that ensures each logical register is written only once. This simplifies the out-of-order (OoO) execution structure and improves performance and power efficiency.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b7\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.5,\n    \"b5\": 0.5,\n    \"b8\": 0.7,\n    \"b10\": 0.6,\n    \"b11\": 0.6,\n    \"b12\": 0.4,\n    \"b13\": 0.4\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in the paper is improving single-thread performance in CPU architectures, particularly by reducing the amount of power-intensive indirect operations such as register renaming. The proposed solution is a novel Out-of-Order (OoO) execution architecture called STRAIGHT, which eliminates the need for register renaming by utilizing a unique instruction set architecture (ISA). This ISA ensures each logical register is written only once and is discarded after a fixed period, simplifying the hardware and enhancing performance and power efficiency.\",\n  \"Direct Inspiration\": {\n    \"b6\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.9\n  },\n  \"Other Inspiration\": {\n    \"b7\": 0.8,\n    \"b8\": 0.7,\n    \"b10\": 0.7,\n    \"b11\": 0.7,\n    \"b12\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving single-thread performance in processors by proposing a novel OoO execution architecture called STRAIGHT, which eliminates register renaming. The STRAIGHT ISA is designed to simplify hardware and enhance power efficiency by using a unique instruction set that guarantees each logical register will be written only once. The architecture includes a new microarchitecture and compiler to support this ISA. A cycle-accurate simulator and RTL description were developed to evaluate the performance and power efficiency of the STRAIGHT architecture compared to conventional OoO superscalar processors.\",\n  \"Direct Inspiration\": {\n    \"b4\": 0.9,\n    \"b5\": 0.9,\n    \"b6\": 0.95\n  },\n  \"Indirect Inspiration\": {\n    \"b2\": 0.8,\n    \"b7\": 0.85\n  },\n  \"Other Inspiration\": {\n    \"b8\": 0.75,\n    \"b10\": 0.75,\n    \"b13\": 0.75\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of improving single-thread performance in modern CPUs by introducing the novel STRAIGHT architecture. This architecture eliminates register renaming by adopting a unique instruction set architecture (ISA) that uses instruction distances for operand referencing, leading to simpler and more power-efficient hardware. The paper outlines the development of the ISA, microarchitecture, compilation algorithm, and evaluation of the STRAIGHT processor, demonstrating significant performance and power efficiency improvements over conventional out-of-order (OoO) superscalar processors.\",\n  \"Direct Inspiration\": {\n    \"b4\": 1,\n    \"b5\": 1,\n    \"b6\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b0\": 0.8,\n    \"b2\": 0.8,\n    \"b7\": 0.8,\n    \"b8\": 0.8,\n    \"b12\": 0.8,\n    \"b13\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b1\": 0.5,\n    \"b10\": 0.5,\n    \"b11\": 0.5\n  }\n}\n```"], "5e524da993d709897cb69fde": ["```json\n{\n  \"Summary\": \"This paper addresses the challenge of antibiotic resistance and the declining rate of new antibiotic discovery. It proposes a novel method using a directed message passing neural network to predict the antibacterial properties of molecules. The model was trained on a dataset of 2,335 molecules and applied to large chemical libraries, identifying halicin and several other compounds as potential new antibiotics.\",\n  \"Direct Inspiration\": {\n    \"b63\": 1,\n    \"b10\": 0.9,\n    \"b20\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.8,\n    \"b0\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b2\": 0.6,\n    \"b27\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the critical challenge of discovering new antibiotics to combat antibiotic resistance, leveraging machine learning to identify new compounds. The novel approach involves using a directed message passing neural network (D-MPNN) model to predict antibacterial properties of molecules in silico, enabling the identification of structurally unique antibacterial compounds, including halicin, from large chemical libraries.\",\n  \"Direct Inspiration\": {\n    \"b63\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b10\": 0.85,\n    \"b20\": 0.85,\n    \"b7\": 0.8,\n    \"b55\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.75,\n    \"b43\": 0.7\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of discovering new antibiotics due to the increasing antibiotic resistance and the difficulty of finding new antibiotic candidates through traditional methods. The authors propose a novel approach using a directed message passing neural network (D-MPNN) to predict antibacterial compounds in silico. Their model was trained on a dataset of 2,335 molecules and applied to large chemical libraries, leading to the discovery of halicin, a potent broad-spectrum antibiotic.\",\n  \"Direct Inspiration\": {\n    \"b63\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b3\": 0.9,\n    \"b10\": 0.8,\n    \"b20\": 0.8,\n    \"b26\": 0.7,\n    \"b27\": 0.7,\n    \"b38\": 0.7,\n    \"b60\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.6,\n    \"b34\": 0.6,\n    \"b35\": 0.6,\n    \"b36\": 0.6,\n    \"b41\": 0.6,\n    \"b43\": 0.6,\n    \"b47\": 0.6\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of discovering new antibiotics amidst rising antibiotic resistance and limited economic incentives leading to reduced private sector development. It proposes a novel approach utilizing a directed message passing neural network to predict molecular properties and identify potential antibiotics from vast chemical libraries.\",\n    \"Direct Inspiration\": {\n        \"b63\": 1.0,\n        \"b10\": 0.9,\n        \"b20\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b26\": 0.8,\n        \"b27\": 0.8,\n        \"b40\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b17\": 0.6,\n        \"b60\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper discusses the challenges in discovering new antibiotics due to antibiotic resistance and the lack of economic incentives for new antibiotic development. The authors propose a machine learning-based approach using a directed message passing neural network to predict antibacterial activity, leading to the discovery of new compounds like halicin.\",\n  \"Direct Inspiration\": {\n    \"b63\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b10\": 0.8,\n    \"b20\": 0.8,\n    \"b3\": 0.7,\n    \"b43\": 0.7,\n    \"b60\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b26\": 0.6,\n    \"b65\": 0.6\n  }\n}\n```"], "5e943b8091e0113448664118": ["```json\n{\n    \"Summary\": {\n        \"challenges\": \"The primary challenge is improving the retrieval component in open-domain question answering (QA) systems to enhance end-to-end QA accuracy. Traditional retrieval methods like TF-IDF/BM25 often fail to capture semantic meaning effectively, leading to performance degradation. The proposed dense passage retriever (DPR) aims to address these limitations by leveraging dense vector representations and optimizing them through a dual-encoder architecture without additional pretraining.\",\n        \"inspirations\": \"The inspiration for the proposed DPR comes from advancements in reading comprehension models, the limitations of traditional retrieval methods, and previous work such as ORQA and BERT.\"\n    },\n    \"Direct Inspiration\": {\n        \"b22\": 1.0,\n        \"b8\": 0.9,\n        \"b3\": 0.85\n    },\n    \"Indirect Inspiration\": {\n        \"b5\": 0.8,\n        \"b36\": 0.75,\n        \"b38\": 0.7,\n        \"b11\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.65,\n        \"b20\": 0.6,\n        \"b41\": 0.6\n    }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of improving retrieval in open-domain question answering (QA) systems, which traditionally use methods like TF-IDF or BM25. It proposes a dense passage retriever (DPR) that leverages BERT and a dual-encoder architecture to improve retrieval accuracy and ultimately the end-to-end QA performance. The novel approach involves optimizing embeddings for maximum inner product similarity between question and passage vectors, and it shows significant improvements over traditional methods.\",\n    \"Direct Inspiration\": {\n        \"b22\": 1.0\n    },\n    \"Indirect Inspiration\": {\n        \"b8\": 0.9,\n        \"b3\": 0.8,\n        \"b5\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b21\": 0.6,\n        \"b38\": 0.5,\n        \"b11\": 0.5,\n        \"b17\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is the performance degradation of open-domain question answering (QA) systems when transitioning from traditional keyword-based retrieval methods (like TF-IDF or BM25) to dense retrieval methods. The authors propose a Dense Passage Retriever (DPR) which uses a dual-encoder architecture based on BERT to generate dense vector representations for questions and passages. The key contributions include: 1) demonstrating that fine-tuning the question and passage encoders on existing question-passage pairs can significantly outperform traditional methods without additional pretraining, and 2) validating that improved retrieval precision leads to higher end-to-end QA accuracy.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1.0\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.8,\n    \"b5\": 0.7,\n    \"b21\": 0.6,\n    \"b3\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b11\": 0.5,\n    \"b36\": 0.5,\n    \"b17\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": {\n    \"challenges\": [\n      \"Performance degradation in retrieval for open-domain QA.\",\n      \"Computationally intensive pretraining in previous models.\",\n      \"Suboptimal context encoder representations.\"\n    ],\n    \"inspirations\": [\n      \"Dense embeddings and dual-encoder architecture.\",\n      \"Improving retrieval precision to enhance QA accuracy.\"\n    ]\n  },\n  \"Direct Inspiration\": {\n    \"references\": [\n      \"b22\",\n      \"b8\",\n      \"b3\",\n      \"b5\"\n    ]\n  },\n  \"Indirect Inspiration\": {\n    \"references\": []\n  },\n  \"Other Inspiration\": {\n    \"references\": [\n      \"b21\",\n      \"b11\",\n      \"b38\",\n      \"b36\"\n    ]\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenge addressed in this paper is improving the retrieval component in open-domain QA by using dense embeddings rather than traditional sparse representations like TF-IDF/BM25. The proposed algorithm, Dense Passage Retriever (DPR), leverages BERT and a dual-encoder architecture to fine-tune question and passage encoders on existing question-passage pairs, optimizing for inner product similarity. This method aims to achieve higher retrieval precision and, consequently, better end-to-end QA accuracy without additional pretraining.\",\n  \"Direct Inspiration\": {\n    \"b22\": 1,\n    \"b8\": 0.9,\n    \"b3\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b5\": 0.8,\n    \"b20\": 0.75,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b17\": 0.65,\n    \"b29\": 0.6,\n    \"b34\": 0.6,\n    \"b13\": 0.55,\n    \"b10\": 0.55\n  }\n}\n```"], "5e8d92a09fced0a24b628d13": ["```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper focus on addressing data sparsity in collaborative filtering (CF) recommender systems by leveraging social networks. The authors propose a novel approach called DiffNet++, which integrates both user-item interest graphs and user-user social graphs using graph convolutional networks (GCNs) and multi-level attention networks to enhance recommendation performance.\",\n  \"Direct Inspiration\": {\n    \"b42\": 1.0,\n    \"b40\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b29\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b21\": 0.6,\n    \"b7\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of data sparsity in Collaborative Filtering (CF) based recommender systems by leveraging social networks to improve recommendation performance. The proposed DiffNet++ algorithm models higher-order social influence and interest diffusion in a unified framework using Graph Convolutional Networks (GCNs) and a multi-level attention network structure.\",\n  \"Direct Inspiration\": {\n    \"b42\": 1.0,\n    \"b40\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b29\": 0.7,\n    \"b14\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b7\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the data sparsity issue in collaborative filtering (CF) based recommender systems by leveraging user-user social networks and user-item interest graphs. It proposes DiffNet++, an improved algorithm of DiffNet, which models both higher-order social influence diffusion and interest diffusion using a unified framework. The main contributions include revisiting the social recommendation problem, proposing DiffNet++ to model both social and interest diffusion, and designing a multi-level attention network to aggregate user embeddings from different nodes and graphs. The effectiveness of DiffNet++ is demonstrated through extensive experimental results on real-world datasets.\",\n  \"Direct Inspiration\": {\n    \"b42\": 1.0,\n    \"b40\": 0.9\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.7,\n    \"b29\": 0.7,\n    \"b14\": 0.8\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.6,\n    \"b7\": 0.6,\n    \"b21\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of data sparsity in Collaborative Filtering (CF) based recommender systems by leveraging social networks to improve recommendation performance. It proposes DiffNet++, an improved algorithm of DiffNet, that models both the higher-order social influence diffusion and interest diffusion in a unified framework using Graph Convolutional Networks (GCNs). Key contributions include the introduction of a multi-level attention network structure and extensive experimental validation on real-world datasets.\",\n  \"Direct Inspiration\": [\"b42\"],\n  \"Indirect Inspiration\": [\"b40\"],\n  \"Other Inspiration\": [\"b4\", \"b7\", \"b21\", \"b47\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the data sparsity issue in Collaborative Filtering (CF) by leveraging social networks to enhance recommendation performance. It proposes DiffNet++, an improved algorithm of DiffNet, which integrates higher-order social influence diffusion and interest diffusion within a unified framework using Graph Convolutional Networks (GCNs) and a multi-level attention network.\",\n  \"Direct Inspiration\": {\n    \"b42\": 0.9,\n    \"b40\": 0.8,\n    \"b14\": 0.7\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.6,\n    \"b29\": 0.6,\n    \"b3\": 0.5,\n    \"b47\": 0.5\n  },\n  \"Other Inspiration\": {\n    \"b4\": 0.4,\n    \"b7\": 0.4,\n    \"b21\": 0.4,\n    \"b6\": 0.4\n  }\n}\n```"], "5f5f378a91e0117a861e8942": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of transfer learning in Graph Neural Networks (GNNs), focusing on their transferability across different graphs and domains. It introduces a novel ego-graph information maximization (EGI) model and provides theoretical guarantees for GNN transferability based on structural information analysis.\",\n  \"Direct Inspiration\": [\"b2\", \"b46\"],\n  \"Indirect Inspiration\": [\"b18\", \"b33\", \"b19\"],\n  \"Other Inspiration\": [\"b27\", \"b45\"]\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenges of training Graph Neural Networks (GNNs) on large-scale graphs and their transferability across different domains. It proposes a novel algorithm called EGI (Ego-Graph Information Maximization) which focuses on capturing essential graph information to enhance GNN transferability. The paper introduces a theoretical framework and validates it through synthetic and real-world experiments.\",\n  \"Direct Inspiration\": {\n    \"b2\": 0.9,\n    \"b46\": 0.85,\n    \"b19\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.75,\n    \"b33\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b3\": 0.6,\n    \"b26\": 0.65\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the high training cost of Graph Neural Networks (GNNs) on large-scale graphs and the challenge of transferring learned structural information across graphs. It proposes a theoretically grounded framework for GNN transfer learning, introducing a novel model called EGI (Ego-Graph Information maximization) that captures and transfers graph information effectively. The authors validate their theoretical conclusions through synthetic experiments and real-world datasets, demonstrating the superior performance of EGI in various transfer learning scenarios.\",\n  \"Direct Inspiration\": {\n    \"b2\": 1,\n    \"b46\": 1\n  },\n  \"Indirect Inspiration\": {\n    \"b18\": 0.8,\n    \"b19\": 0.7,\n    \"b33\": 0.6\n  },\n  \"Other Inspiration\": {\n    \"b27\": 0.5,\n    \"b45\": 0.4\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses challenges in the training and transferability of Graph Neural Networks (GNNs) on large-scale graphs. The proposed algorithm, Ego-Graph Information maximization (EGI), aims to capture essential graph information and establish a theoretical framework for GNN transferability.\",\n    \"Direct Inspiration\": {\n        \"b2\": 0.9,\n        \"b46\": 0.85,\n        \"b18\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b25\": 0.75,\n        \"b19\": 0.7,\n        \"b33\": 0.65\n    },\n    \"Other Inspiration\": {\n        \"b27\": 0.6,\n        \"b45\": 0.55\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The primary challenges outlined in the paper are the high memory and computation costs of training GNNs on large-scale graphs and the need to transfer learned structural information across different graphs and domains. The paper proposes a theoretically grounded framework for transfer learning of GNNs, introducing a novel model called EGI based on ego-graph information maximization. This model aims to capture essential graph information and establish a rigorous bound for its transferability.\",\n  \"Direct Inspiration\": [\"b2\", \"b46\"],\n  \"Indirect Inspiration\": [\"b18\"],\n  \"Other Inspiration\": [\"b19\", \"b27\", \"b33\", \"b45\"]\n}\n```"], "5c04967517c44a2c74709321": ["```json\n{\n    \"Summary\": \"The paper addresses the challenges of optimizing deep neural networks for real-time applications by proposing a Hardware-Aware Automated Quantization (HAQ) framework. The primary challenges include handling the large design space for quantization, optimizing latency and energy consumption on different hardware, and automating the quantization process without relying on domain experts. The HAQ framework uses reinforcement learning to predict the quantization policy, directly incorporating hardware feedback to optimize performance.\",\n    \"Direct Inspiration\": {\n        \"b17\": 0.9,\n        \"b12\": 0.8,\n        \"b24\": 0.8\n    },\n    \"Indirect Inspiration\": {\n        \"b0\": 0.7,\n        \"b35\": 0.7,\n        \"b10\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.6,\n        \"b26\": 0.6,\n        \"b25\": 0.6\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing deep neural networks for real-time applications by proposing an automated, hardware-aware quantization framework. This framework uses reinforcement learning to determine the bitwidth of weights and activations for each layer, tailored to specific hardware architectures, thereby optimizing latency, energy, and model size.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.95,\n    \"b6\": 0.9,\n    \"b21\": 0.9,\n    \"b25\": 0.85,\n    \"b26\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.8,\n    \"b24\": 0.8,\n    \"b8\": 0.75,\n    \"b34\": 0.75\n  },\n  \"Other Inspiration\": {\n    \"b14\": 0.7,\n    \"b2\": 0.7\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of optimizing deep neural networks for real-time applications through a novel Hardware-Aware Automated Quantization (HAQ) framework. The key challenges include managing the vast design space for bitwidth allocation across different hardware architectures and optimizing for latency and energy consumption. The proposed solution leverages reinforcement learning to automate the quantization process and directly incorporates hardware feedback.\",\n    \"Direct Inspiration\": {\n        \"b17\": 1, \n        \"b25\": 1, \n        \"b26\": 1, \n        \"b12\": 0.9, \n        \"b24\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b9\": 0.8, \n        \"b21\": 0.8, \n        \"b6\": 0.7\n    },\n    \"Other Inspiration\": {\n        \"b8\": 0.6, \n        \"b34\": 0.6, \n        \"b2\": 0.5, \n        \"b14\": 0.5\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing deep neural networks for hardware efficiency by proposing a Hardware-Aware Automated Quantization (HAQ) framework. This framework uses reinforcement learning to automatically determine the optimal bitwidth for weights and activations in each layer, tailored to different hardware architectures, to minimize latency, energy consumption, and model size.\",\n  \"Direct Inspiration\": {\n    \"b12\": 0.9,\n    \"b24\": 0.9,\n    \"b25\": 0.8,\n    \"b26\": 0.8\n  },\n  \"Indirect Inspiration\": {\n    \"b8\": 0.7,\n    \"b34\": 0.7,\n    \"b17\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b6\": 0.6,\n    \"b21\": 0.6,\n    \"b13\": 0.6\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of optimizing deep neural networks for real-time applications constrained by latency, energy, and model size. It proposes the Hardware-Aware Automated Quantization (HAQ) framework, which uses reinforcement learning to automatically decide the bitwidth for weights and activations in a layer-wise manner, incorporating direct feedback from hardware to optimize performance.\",\n  \"Direct Inspiration\": {\n    \"b17\": 0.9,\n    \"b10\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b12\": 0.75,\n    \"b24\": 0.75,\n    \"b6\": 0.7,\n    \"b21\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b25\": 0.6,\n    \"b26\": 0.6\n  }\n}\n```"], "5d64ff713a55acf547f20de0": ["```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently deploying deep neural networks (DNNs) across diverse hardware platforms with varying constraints. The authors propose a 'once-for-all' network that can be tailored to different configurations without retraining, using a progressive shrinking algorithm to optimize sub-networks' accuracy and efficiency.\",\n  \"Direct Inspiration\": {\n    \"b37\": 0.9,\n    \"b3\": 0.85,\n    \"b29\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b9\": 0.75,\n    \"b12\": 0.75,\n    \"b35\": 0.7,\n    \"b13\": 0.7,\n    \"b38\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b15\": 0.6,\n    \"b30\": 0.6,\n    \"b40\": 0.6,\n    \"b18\": 0.6,\n    \"b26\": 0.6,\n    \"b41\": 0.6,\n    \"b1\": 0.6,\n    \"b24\": 0.6,\n    \"b42\": 0.6,\n    \"b43\": 0.6,\n    \"b2\": 0.6,\n    \"b20\": 0.5,\n    \"b10\": 0.5\n  }\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of deploying deep neural networks (DNNs) efficiently across diverse hardware platforms with varying constraints. The authors propose a 'once-for-all' network that can be trained once and then specialized for different deployment scenarios through a progressive shrinking algorithm. This method allows for different depths, widths, kernel sizes, and resolutions without the need for retraining, significantly reducing the cost and energy consumption associated with conventional methods.\",\n    \"Direct Inspiration\": {\n        \"b3\": 0.9,\n        \"b13\": 0.8,\n        \"b29\": 0.8,\n        \"b38\": 0.9\n    },\n    \"Indirect Inspiration\": {\n        \"b15\": 0.7,\n        \"b30\": 0.7,\n        \"b40\": 0.6,\n        \"b9\": 0.6,\n        \"b12\": 0.6\n    },\n    \"Other Inspiration\": {\n        \"b31\": 0.5,\n        \"b37\": 0.5,\n        \"b35\": 0.5,\n        \"b20\": 0.4,\n        \"b43\": 0.4\n    }\n}\n```", "```json\n{\n  \"Summary\": \"The paper presents the primary challenge of deploying deep neural networks (DNNs) efficiently across diverse hardware platforms with varying constraints. It proposes a novel 'once-for-all' network that can be trained once and then specialized for different deployment scenarios, thus reducing the total cost. The key innovation is the progressive shrinking algorithm, which trains the network by progressively fine-tuning from large to small sub-networks, ensuring efficient training and maintaining accuracy across all sub-networks.\",\n  \"Direct Inspiration\": {\n    \"b3\": 0.9,\n    \"b37\": 0.85\n  },\n  \"Indirect Inspiration\": {\n    \"b15\": 0.75,\n    \"b30\": 0.75,\n    \"b40\": 0.75,\n    \"b9\": 0.7,\n    \"b12\": 0.7\n  },\n  \"Other Inspiration\": {\n    \"b13\": 0.6,\n    \"b0\": 0.6,\n    \"b38\": 0.6,\n    \"b29\": 0.5,\n    \"b23\": 0.5\n  }\n}\n```", "```json\n{\n  \"Summary\": \"The paper addresses the challenge of efficiently deploying Deep Neural Networks (DNNs) on diverse hardware platforms with varying constraints such as latency and energy. It introduces the Once-for-All (OFA) network, which can be deployed under diverse architectural configurations without retraining. The paper proposes a progressive shrinking algorithm to train the OFA network, ensuring the accuracy of all sub-networks derived from it. This approach reduces the total cost of specialized neural network design from O(N) to O(1), significantly improving efficiency and reducing CO2 emissions.\",\n  \"Direct Inspiration\": [\"b13\", \"b0\", \"b38\"],\n  \"Indirect Inspiration\": [\"b3\", \"b35\", \"b29\", \"b31\"],\n  \"Other Inspiration\": [\"b15\", \"b30\", \"b40\", \"b9\", \"b12\", \"b23\"]\n}\n```", "```json\n{\n    \"Summary\": \"The paper addresses the challenge of efficiently deploying deep neural networks (DNNs) on diverse hardware platforms with varying efficiency constraints. It proposes a once-for-all network that can be directly deployed under diverse architectural configurations without retraining, using a progressive shrinking algorithm for training.\",\n    \"Direct Inspiration\": [\"b3\", \"b37\", \"b38\"],\n    \"Indirect Inspiration\": [\"b15\", \"b30\", \"b9\", \"b12\", \"b13\"],\n    \"Other Inspiration\": [\"b23\", \"b31\", \"b29\", \"b35\"]\n}\n```"]}