{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maggiewang/anaconda3/envs/xeek2/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "2024-06-13 08:35:50,102 loading paper_source_trace_train_ans.json ...\n",
      "2024-06-13 08:35:50,150 paper_source_trace_train_ans.json loaded\n",
      "2024-06-13 08:35:50,151 loading paper_source_trace_valid_wo_ans.json ...\n",
      "2024-06-13 08:35:50,158 paper_source_trace_valid_wo_ans.json loaded\n",
      "2024-06-13 08:35:50,159 loading paper_source_trace_test_wo_ans.json ...\n",
      "2024-06-13 08:35:50,168 paper_source_trace_test_wo_ans.json loaded\n"
     ]
    }
   ],
   "source": [
    "## 尝试做特征\n",
    "import os\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict as dd\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, average_precision_score\n",
    "import logging\n",
    "\n",
    "import utils\n",
    "import settings\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "data_dir = settings.DATA_TRACE_DIR\n",
    "\n",
    "papers_train = utils.load_json('', \"paper_source_trace_train_ans.json\")\n",
    "papers_valid = utils.load_json('', \"paper_source_trace_valid_wo_ans.json\")\n",
    "papers_test = utils.load_json('',  \"paper_source_trace_test_wo_ans.json\")\n",
    "\n",
    "\n",
    "files = []\n",
    "data_dir = './'\n",
    "in_dir = join(data_dir, 'paper-xml')\n",
    "for f in os.listdir(in_dir):\n",
    "    if f.endswith('.xml'):\n",
    "        files.append(f)\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import collections\n",
    "import json\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^a-zA-Z]+', ' ', text).lower().strip()\n",
    "\n",
    "import re\n",
    "def get_pattern(input_string):\n",
    "    pattern = r'\\n\\n(.*?)\\n\\n'\n",
    "\n",
    "# 执行正则表达式搜索\n",
    "    match = re.search(pattern, input_string)\n",
    "\n",
    "    # 检查是否有匹配，如果有，则输出匹配到的内容\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "## 读数据-GPT\n",
    "## 读数据\n",
    "\n",
    "\n",
    "## test\n",
    "id_train = [ x['_id'] for x in papers_train]\n",
    "id_valid = [ x['_id'] for x in papers_valid]\n",
    "id_test = [ x['_id'] for x in papers_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saved processed_data\n",
    "import pickle\n",
    "processed_data_name = 'processed_data_0601.pickle'\n",
    "with open(processed_data_name, 'rb') as f:\n",
    "    processed_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_ref_list,  body_ref_count,  pid_to_title,  title_to_pid,  authors_info_dict,  body_ref_count_1, body_ref_count_2, body_ref_count_3, body_ref_count_4, body_ref_count_5, body_ref_count_6, total_author_data_sup, bib_to_contexts_dict = processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read paper\n",
    "def read_file(file, in_dir):\n",
    "    '''\n",
    "    读取XML格式论文文件\n",
    "    '''\n",
    "    file_path = join(in_dir, file)\n",
    "    f = open(file_path, encoding='utf-8')\n",
    "    xml = f.read()\n",
    "    bs = BeautifulSoup(xml, \"xml\")    \n",
    "    return xml, bs\n",
    "\n",
    "def get_ref_list(bs):\n",
    "    '''\n",
    "    获取输入文件中所有引用对应的标题\n",
    "    输入: bs格式的论文XML文件\n",
    "    输出: dict格式的论文引用标题\n",
    "    '''\n",
    "    this_ref_list = {}\n",
    "    references = bs.find_all(\"biblStruct\")\n",
    "    for ref in references:\n",
    "        key = ref.get('xml:id')\n",
    "        try:\n",
    "            title = ref.find('title', level=\"a\").get_text()\n",
    "        except:\n",
    "            try:\n",
    "                title = ref.find('title', level=\"m\").get_text()\n",
    "            except:\n",
    "                title = ref.getText()\n",
    "                title = get_pattern(title)\n",
    "        this_ref_list[key] = clean_text(title)\n",
    "    return this_ref_list\n",
    "\n",
    "\n",
    "def get_body_ref_count(body):\n",
    "    '''\n",
    "    获取论文引用数目\n",
    "    输入: bs格式的论文XML文件\n",
    "    输出: dict格式的论文引用数目\n",
    "    '''\n",
    "    #找到正文\n",
    "    references = body.find_all('ref', type='bibr')\n",
    "    reference_counter = collections.Counter()\n",
    "    for reference in references:\n",
    "        if reference.has_attr('target'):\n",
    "            reference_counter[reference['target'].strip(\"#\")] += 1\n",
    "    return reference_counter\n",
    "\n",
    "\n",
    "def get_bs_title(bs):\n",
    "    '''\n",
    "    获取标题\n",
    "    '''\n",
    "    ##--------------------- pid_to_title / title_to_pid --------------------------\n",
    "    title = clean_text(bs.find(lambda tag: tag.name == \"title\" and tag.get('type') == 'main').text)\n",
    "    return title\n",
    "\n",
    "\n",
    "    ##--------------------- authors_info_dict --------------------------\n",
    "def get_author_info(bs):\n",
    "    authors_info = []\n",
    "    authors = bs.find_all('author')\n",
    "\n",
    "    # Loop through all authors and extract name and affiliation\n",
    "    for author in authors:\n",
    "        # Extracting the author's name (forename and surname)\n",
    "        if author.affiliation:\n",
    "            try:\n",
    "                forename = author.forename.get_text(strip=True)\n",
    "                surname = author.surname.get_text(strip=True)\n",
    "                try:\n",
    "                    country = author.find('country').get_text(strip=True)\n",
    "                except:\n",
    "                    country = \"\"\n",
    "                org_names = author.find_all('orgName', attrs={\"type\": \"institution\"})\n",
    "                authors_info.append({\n",
    "                    \"forename\": forename,\n",
    "                    \"surname\": surname,\n",
    "                    \"country\": country,\n",
    "                    \"org_names\": [org_name.get_text(strip=True) for org_name in org_names]\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return authors_info\n",
    "\n",
    "def find_bib_context(xml, bs, dist=100):\n",
    "    bib_to_context = dd(list)\n",
    "    bibr_strs_to_bid_id = {}\n",
    "    for item in bs.find_all(type='bibr'):\n",
    "        if \"target\" not in item.attrs:\n",
    "            continue\n",
    "        bib_id = item.attrs[\"target\"][1:]\n",
    "        item_str = \"<ref type=\\\"bibr\\\" target=\\\"{}\\\">{}</ref>\".format(item.attrs[\"target\"], item.get_text())\n",
    "        bibr_strs_to_bid_id[item_str] = bib_id\n",
    "\n",
    "    for item_str in bibr_strs_to_bid_id:\n",
    "        bib_id = bibr_strs_to_bid_id[item_str]\n",
    "        cur_bib_context_pos_start = [ii for ii in range(len(xml)) if xml.startswith(item_str, ii)]\n",
    "        for pos in cur_bib_context_pos_start:\n",
    "            bib_to_context[bib_id].append(xml[pos - dist: pos + dist].replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip())\n",
    "    return bib_to_context\n",
    "\n",
    "def  get_bib_context(bs, xml):\n",
    "    return find_bib_context(xml, bs)\n",
    "\n",
    "def find_section_number(header):\n",
    "    if not header:\n",
    "        return None\n",
    "    \n",
    "    section_number = header.get('n')\n",
    "    \n",
    "    if section_number:\n",
    "        return section_number\n",
    "\n",
    "    \n",
    "    text = header.text\n",
    "    \n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    if 'I.' in text or 'introduction' in text.lower():\n",
    "        return '1.'\n",
    "    if 'II.' in text or 'related' in text.lower():\n",
    "        return '2.'\n",
    "    if 'III.' in text or 'method' in text.lower():\n",
    "        return '3.'\n",
    "    if 'IV.' in text:\n",
    "        return '4.'\n",
    "    if 'V.' in text:\n",
    "        return '5.'\n",
    "    if 'VI.' in text:\n",
    "        return '6.'\n",
    "    if 'VII.' in text:\n",
    "        return '7.'\n",
    "    if 'VIII.' in text:\n",
    "        return '8.'\n",
    "    if 'IX.' in text:\n",
    "        return '9.'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_section_bib_number(bs):\n",
    "    sections = bs.find_all(\"div\")\n",
    "    reference_counter_1 = collections.Counter()\n",
    "    reference_counter_2 = collections.Counter()\n",
    "    reference_counter_3 = collections.Counter()\n",
    "    reference_counter_4 = collections.Counter()\n",
    "    reference_counter_5 = collections.Counter()\n",
    "    reference_counter_6 = collections.Counter()\n",
    "\n",
    "    now_section_index = 0\n",
    "\n",
    "    for section in sections:\n",
    "        if not section.find('head'):\n",
    "            continue\n",
    "        \n",
    "        header = section.find(\"head\")\n",
    "        #section_number = header.get('n')\n",
    "        section_number = find_section_number(header)\n",
    "\n",
    "        #if not section_number:\n",
    "        #    continue\n",
    "\n",
    "        if section_number:\n",
    "\n",
    "            section_index = int(section_number.split('.')[0])\n",
    "            section_title = header.text.strip()\n",
    "            now_section_index = section_index\n",
    "\n",
    "        references = section.find_all('ref', type='bibr')\n",
    "        for reference in references:\n",
    "            if reference.has_attr('target'):\n",
    "                if now_section_index == 1:\n",
    "                    reference_counter_1[reference['target'].strip(\"#\")] += 1\n",
    "                elif now_section_index == 2:\n",
    "                    reference_counter_2[reference['target'].strip(\"#\")] += 1\n",
    "                elif now_section_index == 3:\n",
    "                    reference_counter_3[reference['target'].strip(\"#\")] += 1\n",
    "                elif now_section_index == 4:\n",
    "                    reference_counter_4[reference['target'].strip(\"#\")] += 1\n",
    "                elif now_section_index == 5:\n",
    "                    reference_counter_5[reference['target'].strip(\"#\")] += 1\n",
    "                elif now_section_index == 6:\n",
    "                    reference_counter_6[reference['target'].strip(\"#\")] += 1\n",
    "        \n",
    "        return reference_counter_1, reference_counter_2, reference_counter_3, reference_counter_4, reference_counter_5, reference_counter_6\n",
    "    \n",
    "\n",
    "## 解析原文的feat\n",
    "\n",
    "Chinese_surname_list = ['li', 'wang', 'zhang', 'liu', 'chen', 'yang', 'huang', 'zhao', 'wu', 'zhou', 'xiong', 'xiu', 'sun', 'ma', 'zhu', 'hu', 'guo', 'he', 'gao', 'lin', 'luo', 'zheng', 'qian', 'zhen', 'tong', 'zeng', 'zen', 'zhuang',\n",
    "                        'liang', 'xie', 'song', 'tang', 'dong', 'yuan', 'cai', 'feng', 'xiao', 'jiang', 'shi', 'hang', 'ang', 'ong', 'ie', \n",
    "                        'xu', 'sun', 'ma', 'zhu', 'hu', 'guo', 'he', 'gao', 'lin', 'luo', 'zheng', 'qian', 'zhen', 'tong', 'zeng', 'zen', 'zhuang',\n",
    "                        'liang', 'xie', 'song', 'tang', 'dong', 'yuan', 'cai', 'feng', 'xiao', 'jiang', 'shi', 'hang', 'ang', 'ong', 'ie', 'chua',\n",
    "                        'tian', 'jia', 'pan', 'du', 'dai', 'wei', 'yu', 'bai', 'han', 'gu', 'yao', 'kuang', 'shuang','qi','mei']\n",
    "\n",
    "def get_num_author_source(paper):\n",
    "    surname_list = [x['name'] for x in paper['authors']]\n",
    "    num_author = len(surname_list)\n",
    "    return num_author\n",
    "\n",
    "def get_num_chinese_author_source(paper):\n",
    "    surname_list = [x['name'] for x in paper['authors']]\n",
    "    surname_list = [x.split(' ')[-1] for x in surname_list]\n",
    "    chinese_author_list = [x for x in surname_list if x.lower() in Chinese_surname_list]\n",
    "    return len(chinese_author_list)\n",
    "\n",
    "\n",
    "## 引用的信息\n",
    "\n",
    "def get_reference_info(bs):\n",
    "    this_paper_list = {}\n",
    "    references = bs.find_all(\"biblStruct\") \n",
    "    for ref in references:\n",
    "        key = ref.get('xml:id')\n",
    "        this_ref_list = {}\n",
    "        surname_list = [surname.get_text() for surname in ref.find_all(\"surname\")]\n",
    "        this_ref_list['num_author'] = len(surname_list)\n",
    "        this_ref_list['num_chinese_author'] = len([x for x in surname_list if x.lower() in Chinese_surname_list])\n",
    "        try:\n",
    "            journal = ref.find(\"title\", {\"level\": \"m\"}) or ref.find(\"title\", {\"level\": \"j\"})\n",
    "            journal = journal.get_text()\n",
    "        except:\n",
    "            journal = \"\"\n",
    "        this_ref_list['journal'] = journal\n",
    "        this_paper_list[key] = this_ref_list\n",
    "    return this_paper_list\n",
    "\n",
    "    \n",
    "def extract_text(bs):\n",
    "    sections = bs.find_all('div', limit=10) \n",
    "    total_text = []\n",
    "    for i, section in enumerate(sections):\n",
    "        # 处理引用标记\n",
    "        for ref in section.find_all('ref', type='bibr'):\n",
    "            # 获取编号，假设target属性类似于\"#b20\"\n",
    "            try:\n",
    "                ref_num = re.search(r'#b(\\d+)', ref.get('target'))\n",
    "                if ref_num:\n",
    "                    # 将 ref 替换为如 [b20]\n",
    "                    ref.replace_with('[b{}]'.format(ref_num.group(1)))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # 从div标签中提取文本\n",
    "        text = section.get_text(separator=' ', strip=True)\n",
    "        total_text.append(text)\n",
    "    return total_text\n",
    "\n",
    "\n",
    "import random\n",
    "import re\n",
    "from typing import List, Union\n",
    "\n",
    "class PaperParser:\n",
    "    def __init__(self, bs, xml):\n",
    "        self.paper_id = None\n",
    "        self.title = None \n",
    "        self.body = None\n",
    "        self.bs = bs\n",
    "        self.xml = xml\n",
    "        self.ref_info = {}\n",
    "        self.preprocess()\n",
    "        self.extract_ref_author_info()\n",
    "        self.check_reference()\n",
    "\n",
    "    def check_reference(self):\n",
    "        unnumbered_references_texts = []\n",
    "        numbered_references_texts = []\n",
    "        for ref in self.references:\n",
    "            if not ref.has_attr('target'):\n",
    "                unnumbered_references_texts.append(ref.get_text())\n",
    "            else:\n",
    "                numbered_references_texts.append(ref.get_text())\n",
    "        \n",
    "        self.unnumbered_references_texts = unnumbered_references_texts\n",
    "        self.numbered_references_texts = numbered_references_texts\n",
    "        self.n_ref_valid = len(self.numbered_references_texts)\n",
    "        self.n_ref_invalid = len(self.unnumbered_references_texts)\n",
    "\n",
    "\n",
    "    def extract_references_from_text_digit(self, text) -> Union[List[int], None]:\n",
    "        # 定义正则表达式，用于匹配引用格式\n",
    "        pattern = re.compile(r'\\[\\s*(\\d+(?:\\s*;\\s*\\d+)*)\\s*\\]')\n",
    "        \n",
    "        # 在文本中搜索匹配的模式\n",
    "        matches = pattern.findall(text)\n",
    "        \n",
    "        # 用于存储解析后的引用序号\n",
    "        references = []\n",
    "        \n",
    "        # 如果找到匹配模式，解析引用序号\n",
    "        if matches:\n",
    "            for match in matches:\n",
    "                # 分割每个匹配到的文本，并转换成整数序号\n",
    "                refs = match.split(';')\n",
    "                for ref in refs:\n",
    "                    references.append(int(ref.strip()))  # 移除可能的空格，并将文本转换成整数\n",
    "            return 'b' + str(references[0] - 1)\n",
    "        else:\n",
    "            return None  # 如果没有找到匹配，则返回None\n",
    "\n",
    "    def extract_ref_author_info(self):\n",
    "        '''\n",
    "        获取作者信息, key为b_key\n",
    "        '''\n",
    "\n",
    "        def extract_year(ref):\n",
    "            return ref.find('date', {'type': 'published'}).get('when')\n",
    "        \n",
    "        def extract_year_v2(ref):\n",
    "            # 使用BeautifulSoup解析XML数据\n",
    "            \n",
    "            # 寻找所有的<biblScope>标签\n",
    "            bibl_scope_tags = ref.find_all('biblScope')\n",
    "            \n",
    "            # 遍历找到的标签，寻找可能包含年份信息的标签\n",
    "            for tag in bibl_scope_tags:\n",
    "                if tag.get('unit') == 'page':\n",
    "                    # 尝试将内容转换为整数，以验证其是否可能是年份\n",
    "                    try:\n",
    "                        year = int(tag.text)\n",
    "                        # 根据需要可以添加更多的验证步骤，例如检查年份范围等\n",
    "                        # 如果内容成功转换为整数，并且看起来像合理的年份，返回它\n",
    "                        if 1900 <= year <= 2100:\n",
    "                            return tag.text\n",
    "                    except ValueError:\n",
    "                        # 如果内容不能转换为整数，继续遍历\n",
    "                        continue\n",
    "            \n",
    "            return None\n",
    "\n",
    "\n",
    "        def extract_year_from_title(ref):\n",
    "            # 使用BeautifulSoup解析XML数据\n",
    "            \n",
    "            # 寻找<title>标签\n",
    "            title_tag = ref.find('title')\n",
    "            \n",
    "            if title_tag:\n",
    "                # 使用正则表达式从标题文本中提取年份\n",
    "                match = re.search(r'\\b(\\d{4})\\b', title_tag.text)\n",
    "                if match:\n",
    "                    return match.group(1)\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        for ref in self.references_bib:\n",
    "            #title_tag = ref.find('title')\n",
    "            suffix = None\n",
    "            key = ref.get('xml:id')\n",
    "            try:\n",
    "                first_author = [surname.get_text() for surname in ref.find_all(\"surname\")][0]\n",
    "            except:\n",
    "                first_author = None\n",
    "            try:\n",
    "                ref_years = extract_year(ref)\n",
    "            except:\n",
    "                try:\n",
    "                    ref_years = extract_year_v2(ref)\n",
    "                except:\n",
    "                    try:\n",
    "                        ref_years = extract_year_from_title(ref)\n",
    "                    except:\n",
    "                        ref_years = None\n",
    "\n",
    "            if ref_years:\n",
    "                if '-' in ref_years:\n",
    "                    ref_years = ref_years[:4]\n",
    "        \n",
    "\n",
    "            self.ref_info[key] = {'first_author': first_author, 'ref_years': ref_years}\n",
    "\n",
    "\n",
    "    def infer_ref_number_from_text(self, text):\n",
    "        '''返回如b1...'''\n",
    "        ## 第一种情况, ... et al.的形式\n",
    "        ## 从解析的ref_info去匹配\n",
    "        # 用于匹配年份且不以字母结尾的正则表达式\n",
    "        #print('start')\n",
    "        regex_no_letter = re.compile(r'\\d{4}(?!\\w)')\n",
    "\n",
    "        # 用于匹配年份以字母结尾的正则表达式\n",
    "        regex_with_letter = re.compile(r'\\d{4}\\w')\n",
    "\n",
    "        good_match = []\n",
    "\n",
    "        #先匹配作者名字\n",
    "        author_name_match = []\n",
    "        for key, value in self.ref_info.items():\n",
    "            if key is None:\n",
    "                continue\n",
    "            first_author, ref_years = value['first_author'], value['ref_years']\n",
    "            #print(f'first_author is {first_author}, ref_years is {ref_years}')\n",
    "            if first_author is not None and text is not None:\n",
    "                if first_author.lower() in text.lower():\n",
    "                    author_name_match.append(key)\n",
    "\n",
    "        #print(f'author_name_match is {author_name_match}')\n",
    "        \n",
    "        if len(author_name_match) == 1:\n",
    "                return author_name_match[0]\n",
    "        \n",
    "        ## 否则，需要按年份来匹配\n",
    "        for key, value in self.ref_info.items():\n",
    "            if key is None:\n",
    "                continue\n",
    "            first_author, ref_years = value['first_author'], value['ref_years']\n",
    "            #print(f'first_author is {first_author}, ref_years is {ref_years}, text is {text}')\n",
    "            if first_author is not None and text is not None and ref_years is not None:\n",
    "                if first_author.lower() in text.lower() and ref_years in text:\n",
    "                    good_match.append((key, value))\n",
    "\n",
    "        #print(f'good_match is {good_match}')\n",
    "        \n",
    "        if len(good_match) == 1:\n",
    "            return key\n",
    "        \n",
    "        elif len(good_match) > 1:\n",
    "            # 多个匹配\n",
    "            ## 检查以abc结尾\n",
    "            if regex_with_letter.search(text):\n",
    "                ## 找到最后的那个字母\n",
    "                last_letter = regex_with_letter.search(text).group()[-1]\n",
    "                if last_letter == 'a':\n",
    "                    return good_match[0][0]\n",
    "                elif last_letter == 'b':\n",
    "                    return good_match[1][0]\n",
    "                elif last_letter == 'c' and len(good_match) > 2:\n",
    "                    return good_match[2][0]\n",
    "                elif last_letter == 'd' and len(good_match) > 3:\n",
    "                    return good_match[3][0]\n",
    "                else:\n",
    "                    return good_match[0][0]\n",
    "\n",
    "\n",
    "        elif len(good_match) == 0:\n",
    "            ## 猜测是数字的形式\n",
    "            if self.extract_references_from_text_digit(text):\n",
    "                return self.extract_references_from_text_digit(text)\n",
    "        \n",
    "        if len(author_name_match) > 0:\n",
    "            return author_name_match[0]\n",
    "                    \n",
    "        return None\n",
    "            \n",
    "    def replace_refs_with_inferred_numbers(self, body):\n",
    "\n",
    "        self.body_raw = body\n",
    "\n",
    "        # 找到所有的引用标签\n",
    "        refs = body.find_all('ref', type=\"bibr\")\n",
    "        \n",
    "        for ref in refs:\n",
    "            if not ref.has_attr('target'):  # 如果没有 target 属性，则尝试推断引用编号\n",
    "                #print(f'ref.text is {ref.text}')\n",
    "                inferred_ref_number = self.infer_ref_number_from_text(ref.text)\n",
    "                #print(inferred_ref_number)\n",
    "                if inferred_ref_number is not None:\n",
    "                    ref['target'] = '#' + inferred_ref_number  # 添加推断出的编号\n",
    "                    ref.string = \"{} ({})\".format(ref.text, inferred_ref_number)\n",
    "        \n",
    "        # 返回处理后的XML数据\n",
    "        self.body_processed = body\n",
    "        \n",
    "        \n",
    "\n",
    "    def preprocess(self):\n",
    "        '''\n",
    "        初步的解析\n",
    "        '''\n",
    "        self.body = self.bs.find('body')\n",
    "        self.references = self.body.find_all('ref', type='bibr')\n",
    "        self.references_bib = self.bs.find_all(\"biblStruct\")\n",
    "\n",
    "        ### Check 引用\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'The paper with id {self.paper_id}, title {self.title}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7541 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7541/7541 [01:13<00:00, 102.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# 结果的记录\n",
    "other_featuers = {}\n",
    "\n",
    "total_id = id_train + id_valid + id_test\n",
    "\n",
    "papers_list = {}\n",
    "\n",
    "for file in tqdm(files):\n",
    "\n",
    "    # 读文件\n",
    "    paper_key = file.split('.')[0]\n",
    "\n",
    "    # 无关数据不处理\n",
    "    if paper_key not in total_id:\n",
    "        continue\n",
    "\n",
    "    xml, bs = read_file(file, in_dir)\n",
    "    paper = PaperParser(bs = bs, xml = xml)\n",
    "    body = bs.find('body')\n",
    "    paper.replace_refs_with_inferred_numbers(body)\n",
    "    papers_list[paper_key] = paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 读数据 - gpt4给出的Inspiration数据\n",
    "import json\n",
    "with open('gpt4_res_parse_train_level_parsed.json', 'rb') as f:\n",
    "    gpt4_res_level_train = json.load(f)\n",
    "\n",
    "with open('gpt4_res_parse_valid_level_parsed.json', 'rb') as f:\n",
    "    gpt4_res_level_valid = json.load(f)\n",
    "\n",
    "with open('gpt4_res_parse_test_level_parsed.json', 'rb') as f:\n",
    "    gpt4_res_level_test = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 读数据 - gpt4给出的直接打分数据\n",
    "\n",
    "with open('gpt4_res_parse_train_short_parsed.json', 'rb') as f:\n",
    "    gpt4_res_short_train = json.load(f)\n",
    "\n",
    "with open('gpt4_res_parse_valid_short_parsed.json', 'rb') as f:\n",
    "    gpt4_res_short_valid = json.load(f)\n",
    "\n",
    "with open('gpt4_res_parse_test_short_parsed.json', 'rb') as f:\n",
    "    gpt4_res_short_test = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 读数据 - OPUS给出的直接打分数据\n",
    "\n",
    "with open('opus_res_parse_json_test.json', 'rb') as f:\n",
    "    opus_res_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 读数据 - gpt4给出的仅在test上打分的数据\n",
    "with open('gpt4_turbo_res_parse_json_test.json', 'rb') as f:\n",
    "    gpt4_res_test = json.load(f)\n",
    "\n",
    "with open('gpt4_turbo_res_parse_json_test_v2.json', 'rb') as f:\n",
    "    gpt4_res_test_v2 = json.load(f)\n",
    "\n",
    "with open('GPT4_res_test_V3_parse.json', 'rb') as f:\n",
    "    GPT4_res_test_V3_parse = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 读gemini数据\n",
    "with open('gemini_res_parse_json_test.json', 'rb') as f:\n",
    "    gemini_res_test = json.load(f)\n",
    "\n",
    "with open('gemini_res_parse_json_test_round2.json', 'rb') as f:\n",
    "    gemini_res_test_round2 = json.load(f)\n",
    "    \n",
    "with open('gemini_res_parse_json_test_round3.json', 'rb') as f:\n",
    "    gemini_res_test_round3 = json.load(f)\n",
    "\n",
    "with open('gemini_res_parse_json_test_round4.json', 'rb') as f:\n",
    "    gemini_res_test_round4 = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 读取基于note的数据\n",
    "with open('GPT4_res_test_note_parse.json', 'rb') as f:\n",
    "    gpt4_res_test_note = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_is_CVPR(x):\n",
    "    x = str(x)\n",
    "    if 'cvpr' in x.lower() or 'computer vision and pattern recognition' in x.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def get_is_NIPS(x):\n",
    "    x = str(x)\n",
    "    if 'nips' in x.lower() or 'neurips' in x.lower() or 'neural information processing systems' in x.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def get_is_ICML(x):\n",
    "    x = str(x)\n",
    "    if 'icml' in x.lower() or 'international conference on machine learning' in x.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def get_is_ns(x):\n",
    "    x = str(x)\n",
    "    if 'nature' in x.lower() or 'science' in x.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0  \n",
    "\n",
    "def get_is_ECCV(x):\n",
    "    x = str(x)\n",
    "    if 'eccv' in x.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0  \n",
    "    \n",
    "def get_is_KDD(x):\n",
    "    x = str(x)\n",
    "    if 'kdd' in x.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0  \n",
    "    \n",
    "def get_is_famous(x):\n",
    "    if x == '' or x is np.nan or x is None:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "\n",
    "conference_parse_dict = {\n",
    "    'cvpr':'cvpr',\n",
    "    'computer vision and pattern recognition':'cvpr',\n",
    "    'iccv':'iccv',\n",
    "    'international conference on computer vision':'iccv',\n",
    "    'eccv':'eccv',\n",
    "    'europ. conf. computer vision':'eccv',\n",
    "    'european conference on computer vision':'eccv',\n",
    "    'icml':'icml',\n",
    "    'international conference on machine learning':'icml',\n",
    "    'iclr':'iclr',\n",
    "    'international conference on learning representations':'iclr',\n",
    "    'nips':'neurips',\n",
    "    'neurips':'neurips',\n",
    "    'neural information processing systems':'neurips',\n",
    "    'aaai':'aaai',\n",
    "    'advancement of artificial intelligence':'aaai',\n",
    "    'ijcai':'ijcai',\n",
    "    'joint conference on artificial intelligence':'ijcai',\n",
    "    'kdd':'kdd',\n",
    "    'knowledge discovery and data mining':'kdd',\n",
    "    'sigkdd':'kdd',\n",
    "    'acm sigkdd international conference on knowledge discovery and data mining':'kdd',\n",
    "    'www':'www',\n",
    "    'world wide web':'www',\n",
    "    'sigir':'sigir',\n",
    "    'research and development in information retrieval':'sigir',\n",
    "    'naacl':'naacl',\n",
    "    'acl':'acl',\n",
    "    'association for computational linguistics':'acl',\n",
    "    'emnlp':'emnlp',\n",
    "    'empirical methods in natural language processing':'emnlp',\n",
    "    'north american chapter of the association for computational linguistics':'naacl',\n",
    "    'icde':'icde',\n",
    "    'international conference on data engineering':'icde',\n",
    "    'vldb':'vldb',\n",
    "    'very large data bases':'vldb',\n",
    "    'pvldb':'vldb',\n",
    "    'the vldb journal':'vldb',\n",
    "    'sigmod':'sigmod',\n",
    "    'acm sigmod conference':'sigmod',\n",
    "    'acm sigmod international conference on management of data':'sigmod',\n",
    "    'icdm':'icdm',\n",
    "    'conference on data mining':'icdm',\n",
    "    'sdm':'sdm',\n",
    "    'conference on data mining':'sdm',\n",
    "    'cikm':'cikm',\n",
    "    'conference on information and knowledge management':'cikm',\n",
    "    'cell':'cell',\n",
    "    'nature':'nature',\n",
    "    'science':'science',\n",
    "    'pattern analysis and machine intelligence':'pami',\n",
    "    'pami':'pami',\n",
    "    'jmlr':'jmlr',\n",
    "    'machine learning':'jmlr',\n",
    "    'british conference on machine vision':'bmvc',\n",
    "    'bmvc':'bmvc',\n",
    "    'uai':\"uai\",\n",
    "    \"operating systems\":\"os\",\n",
    "    'eurosys':'os',\n",
    "    'chemical':'chemistry'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def conference_parse(x):\n",
    "    x = str(x).lower()\n",
    "    for k, v in conference_parse_dict.items():\n",
    "        if k in x:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def is_type_vision(x):\n",
    "    x = str(x).lower()\n",
    "    if x in ['cvpr', 'eccv', 'iccv', 'bmvc']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def is_type_NLP(x):\n",
    "    x = str(x).lower()\n",
    "    if x in ['acl','naacl','emnlp']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def is_type_ml(x):\n",
    "    x = str(x).lower()\n",
    "    if x in ['icml','neurips','iclr', 'pami', 'jmlr', 'uai']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def is_type_datamining(x):\n",
    "    x = str(x).lower()\n",
    "    if x in ['www','kdd','sigir','cikm','waim','sigmod','vldb','icde']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def is_type_ai(x):\n",
    "    x = str(x).lower()\n",
    "    if x in ['aaai', 'ijcai']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def is_type_os(x):\n",
    "    x = str(x).lower()\n",
    "    if x in ['os']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def is_type_chem(x):\n",
    "    x = str(x).lower()\n",
    "    if x in ['chemistry']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# 开始做特征\n",
    "\n",
    "total_papers = papers_train + papers_valid + papers_test\n",
    "\n",
    "\n",
    "def make_train_features(types = 'train'):\n",
    "\n",
    "    features_train = {}\n",
    "    labels_train = {}\n",
    "\n",
    "    # 获取生产特征的类型\n",
    "    if types == 'train':\n",
    "        papers = papers_train\n",
    "        gpt_res_label = gpt4_res_level_train\n",
    "        gpt_res_short = gpt4_res_short_train\n",
    "    elif types == 'valid':\n",
    "        papers = papers_valid\n",
    "        gpt_res_label = gpt4_res_level_valid\n",
    "        gpt_res_short = gpt4_res_short_valid\n",
    "    elif types == 'test':\n",
    "        papers = papers_test\n",
    "        gpt_res_label = gpt4_res_level_test\n",
    "        gpt_res_short = gpt4_res_short_test\n",
    "        opus_res = opus_res_test\n",
    "        gpt4_turbo_res = gpt4_res_test\n",
    "        gemini_res = gemini_res_test\n",
    "        gpt4_res_v2 = gpt4_res_test_v2\n",
    "        gemini_res_round2 = gemini_res_test_round2\n",
    "        gemini_res_round3 = gemini_res_test_round3\n",
    "        gemini_res_round4 = gemini_res_test_round4\n",
    "        gpt4_res_note = gpt4_res_test_note\n",
    "        gpt_res_short_r2 = GPT4_res_test_V3_parse\n",
    "\n",
    "\n",
    "\n",
    "    # 迭代所有paper\n",
    "    for key in tqdm([x['_id'] for x in papers]):\n",
    "        features_key = {}\n",
    "        labels_key = {}\n",
    "        context =  bib_to_contexts_dict[key]\n",
    "        this_paper = papers_list[key]\n",
    "        ref_info = this_paper.ref_info\n",
    "        #citation_list = submission_ref_citation[key]\n",
    "\n",
    "\n",
    "        if types == 'train':\n",
    "            ref_source = [x for x in papers if x['_id'] == key][0]['refs_trace']\n",
    "            ref_nums = [x['referenced_serial_number'] for x in ref_source if 'referenced_serial_number' in x.keys()]\n",
    "            ref_keys = ['b' + str(x) for x in ref_nums]\n",
    "            #total_ref_keys = list(set(body_ref_count[key].keys()) + set(ref_keys))\n",
    "            for b_key in body_ref_count[key].keys():\n",
    "                if b_key in ref_keys:\n",
    "                    labels_key[b_key] = 1\n",
    "                else:\n",
    "                    labels_key[b_key] = 0\n",
    "\n",
    "        elif types == 'valid' or types == 'test':\n",
    "            for b_key in body_ref_count[key].keys():\n",
    "                labels_key[b_key] = 0\n",
    "\n",
    "\n",
    "        try:\n",
    "            this_gpt_res_label = gpt_res_label[key]\n",
    "            this_gpt_res_short = gpt_res_short[key]\n",
    "        except:\n",
    "            this_gpt_res_label = {}\n",
    "            this_gpt_res_short = {}\n",
    "        \n",
    "        try:\n",
    "            this_opus_res = opus_res[key]\n",
    "        except:\n",
    "            this_opus_res = {}\n",
    "        \n",
    "        try:\n",
    "            this_turbo_res = gpt4_turbo_res[key]\n",
    "        except:\n",
    "            this_turbo_res = {}\n",
    "\n",
    "        try:\n",
    "            this_turbo_res_v2 = gpt4_res_v2[key]\n",
    "        except:\n",
    "            this_turbo_res_v2 = {}\n",
    "\n",
    "        try:\n",
    "            this_gemini_res = gemini_res[key]\n",
    "        except:\n",
    "            this_gemini_res = {}\n",
    "\n",
    "        try:\n",
    "            this_gemini_res_round2 = gemini_res_round2[key]\n",
    "        except:\n",
    "            this_gemini_res_round2 = {}\n",
    "\n",
    "        try:\n",
    "            this_gemini_res_round3 = gemini_res_round3[key]\n",
    "        except:\n",
    "            this_gemini_res_round3 = {}\n",
    "\n",
    "        try:\n",
    "            this_gemini_res_round4 = gemini_res_round4[key]\n",
    "        except:\n",
    "            this_gemini_res_round4 = {}\n",
    "\n",
    "        try:\n",
    "            this_gpt4_res_note = gpt4_res_note[key]\n",
    "        except:\n",
    "            this_gpt4_res_note = {}\n",
    "\n",
    "        try:\n",
    "            this_gpt_res_short_r2 = gpt_res_short_r2[key]\n",
    "        except:\n",
    "            this_gpt_res_short_r2 = {}\n",
    "\n",
    "\n",
    "        max_ref_year = -1\n",
    "        for ref_key in ref_info.keys():\n",
    "            if ref_info[ref_key]['ref_years'] is not None:\n",
    "                ref_year = int(ref_info[ref_key]['ref_years'])\n",
    "                if ref_year > max_ref_year:\n",
    "                    max_ref_year = ref_year\n",
    "\n",
    "        # 制作每个引用文献的特征\n",
    "        for b_key in body_ref_count[key].keys():\n",
    "            if b_key not in submission_ref_list[key].keys():\n",
    "                continue\n",
    "\n",
    "            if int(b_key[1:]) > 500:\n",
    "                continue\n",
    "            \n",
    "            b_feature = {}\n",
    "\n",
    "            ## 该引用文献在文本的出现次数\n",
    "            b_feature['ref_count'] = body_ref_count[key][b_key]\n",
    "\n",
    "            ## ADD 论文年份和引用年份\n",
    "            ref_year = ref_info[b_key]['ref_years']\n",
    "            if ref_year is None:\n",
    "                ref_year = 2024\n",
    "            ref_year = int(ref_year)\n",
    "\n",
    "            b_feature['max_ref_year'] = max_ref_year\n",
    "            b_feature['ref_year'] = ref_year\n",
    "            b_feature['ref_year_diff'] = max_ref_year - ref_year\n",
    "\n",
    "            ## ADD 引用次数\n",
    "            '''\n",
    "            if b_key in citation_list.keys():\n",
    "                b_feature['n_citation'] = citation_list[b_key]\n",
    "            else:\n",
    "                b_feature['n_citation'] = -1\n",
    "            '''\n",
    "\n",
    "            ###\n",
    "            paper = [x for x in papers if x['_id'] == key][0]\n",
    "            try:\n",
    "                b_feature['venue'] = paper['venue']\n",
    "            except:\n",
    "                b_feature['venue'] = \"\"\n",
    "            try:\n",
    "                num_author_source = get_num_author_source(paper)\n",
    "                num_chinese_author_source = get_num_chinese_author_source(paper)\n",
    "            except:\n",
    "                num_author_source = 0\n",
    "                num_chinese_author_source = 0\n",
    "            \n",
    "            b_feature['num_author_source'] = num_author_source\n",
    "            b_feature['num_chinese_author_source'] = num_chinese_author_source\n",
    "\n",
    "\n",
    "\n",
    "            ## 该引用文献在在历史正样本中的出现次数\n",
    "            \n",
    "            assert b_key in submission_ref_list[key].keys(), f'key is {key}, b_key is {b_key}'\n",
    "            title = clean_text(submission_ref_list[key][b_key])\n",
    "            try:\n",
    "                b_feature['positive_count'] = counter_train_sources[title]\n",
    "            except:\n",
    "                b_feature['positive_count'] = 0\n",
    "            \n",
    "\n",
    "            ## 是否出现在所有文件中\n",
    "            '''\n",
    "            if title in title_to_pid.keys():\n",
    "                b_feature['in_files_list'] = 1\n",
    "            else:\n",
    "                b_feature['in_files_list'] = 0\n",
    "            '''\n",
    "\n",
    "\n",
    "            b_feature['num_author'] = total_author_data_sup[key][b_key]['num_author']\n",
    "            b_feature['num_chinese_author'] = total_author_data_sup[key][b_key]['num_chinese_author']\n",
    "            b_feature['journal'] = total_author_data_sup[key][b_key]['journal']\n",
    "            b_feature['chinese_rate'] = b_feature['num_chinese_author'] / (b_feature['num_author'] + 0.01)\n",
    "            \n",
    "            ## 该引用文献的上下文特征\n",
    "            num_inspired_by = 0\n",
    "            num_motivated_by = 0\n",
    "            num_we_present = 0\n",
    "            num_based_on = 0\n",
    "\n",
    "            try:\n",
    "                b_context = context[b_key]\n",
    "                for context_piece in b_context:\n",
    "                    context_piece = context_piece.lower()\n",
    "                    if 'inspired' in context_piece:\n",
    "                        num_inspired_by += 1\n",
    "                    if 'motivated' in context_piece:\n",
    "                        num_motivated_by += 1\n",
    "                    if 'we present' in context_piece or 'we propose' in context_piece:\n",
    "                        num_we_present += 1\n",
    "                    if 'based on' in context_piece:\n",
    "                        num_based_on += 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            b_feature['num_inspired_by'] = num_inspired_by\n",
    "            b_feature['num_motivated_by'] = num_motivated_by\n",
    "            b_feature['num_we_present'] = num_we_present\n",
    "            b_feature['based_on'] = num_based_on\n",
    "\n",
    "            ## 段落引用特征\n",
    "            b_feature['ref_count_1'] = body_ref_count_1[key][b_key]\n",
    "            b_feature['ref_count_2'] = body_ref_count_2[key][b_key]\n",
    "            b_feature['ref_count_3'] = body_ref_count_3[key][b_key]\n",
    "            b_feature['ref_count_4'] = body_ref_count_4[key][b_key]\n",
    "            b_feature['ref_count_5'] = body_ref_count_5[key][b_key]\n",
    "            b_feature['ref_count_6'] = body_ref_count_6[key][b_key]\n",
    "\n",
    "            ## Venue 特征\n",
    "            b_feature['is_CVPR_journal'] = get_is_CVPR(b_feature['journal'])\n",
    "            b_feature['is_NIPS_journal'] = get_is_NIPS(b_feature['journal'])\n",
    "            b_feature['is_ICML_journal'] = get_is_ICML(b_feature['journal'])\n",
    "            b_feature['is_ns_journal'] = get_is_ns(b_feature['journal'])\n",
    "            b_feature['is_ECCV_journal'] = get_is_ECCV(b_feature['journal'])\n",
    "            b_feature['is_KDD_journal'] = get_is_KDD(b_feature['journal'])\n",
    "\n",
    "            b_feature['is_CVPR_venue'] = get_is_CVPR(b_feature['venue'])\n",
    "            b_feature['is_NIPS_venue'] = get_is_NIPS(b_feature['venue'])\n",
    "            b_feature['is_ICML_venue'] = get_is_ICML(b_feature['venue'])\n",
    "            b_feature['is_ns_venue'] = get_is_ns(b_feature['venue'])\n",
    "            b_feature['is_ECCV_venue'] = get_is_ECCV(b_feature['venue'])\n",
    "            b_feature['is_KDD_venue'] = get_is_KDD(b_feature['venue'])\n",
    "\n",
    "            b_feature['venue_type'] = conference_parse(b_feature['venue'])\n",
    "            b_feature['journal_type'] = conference_parse(b_feature['journal'])\n",
    "\n",
    "            b_feature['journal_type_nlp'] = is_type_NLP(b_feature['journal_type'])\n",
    "            b_feature['venue_type_nlp'] = is_type_NLP(b_feature['venue_type'])\n",
    "\n",
    "            b_feature['joirnal_type_ml'] = is_type_ml(b_feature['journal_type'])\n",
    "            b_feature['venue_type_ml'] = is_type_ml(b_feature['venue_type'])\n",
    "\n",
    "            b_feature['journal_type_dm'] = is_type_datamining(b_feature['journal_type'])\n",
    "            b_feature['venue_type_dm'] = is_type_datamining(b_feature['venue_type'])\n",
    "\n",
    "            b_feature['journal_type_ai'] = is_type_ai(b_feature['journal_type'])\n",
    "            b_feature['venue_type_ai'] = is_type_ai(b_feature['venue_type'])\n",
    "\n",
    "            b_feature['journal_type_os'] = is_type_os(b_feature['journal_type'])\n",
    "            b_feature['venue_type_os'] = is_type_os(b_feature['venue_type'])\n",
    "\n",
    "            b_feature['journal_type_chem'] = is_type_chem(b_feature['journal_type'])\n",
    "            b_feature['venue_type_chem'] = is_type_chem(b_feature['venue_type'])\n",
    "\n",
    "            b_feature['journal_type_vision'] = is_type_vision(b_feature['journal_type'])\n",
    "            b_feature['venue_type_vision'] = is_type_vision(b_feature['venue_type'])\n",
    "\n",
    "            try:\n",
    "                b_feature['context'] = '.'.join([x for x in bib_to_contexts_dict[key][b_key]])\n",
    "            except:\n",
    "                b_feature['context'] = \"\"\n",
    "    \n",
    "            features_key[b_key] = b_feature\n",
    "\n",
    "            ### gpt 特征\n",
    "            for ii in range(5):\n",
    "                b_feature[f'gpt_res_label_{ii}'] = 0\n",
    "                b_feature[f'gpt_res_short_{ii}'] = 0\n",
    "                b_feature[f'gpt_res_short_{ii}_r2'] = 0\n",
    "            \n",
    "            for ii in range(5):\n",
    "                try:\n",
    "                    vv = this_gpt_res_label[ii]\n",
    "                    if b_key in vv['s3']:\n",
    "                        b_feature[f'gpt_res_label_{ii}'] = 1\n",
    "                    if b_key in vv['s2']:\n",
    "                        b_feature[f'gpt_res_label_{ii}'] = 2\n",
    "                    if b_key in vv['s1']:\n",
    "                        b_feature[f'gpt_res_label_{ii}'] = 3\n",
    "                except:\n",
    "                    b_feature[f'gpt_res_label_{ii}'] = 0 \n",
    "\n",
    "                try:\n",
    "                    vv = this_gpt_res_short[ii]\n",
    "                    if b_key in vv.keys():\n",
    "                        b_feature[f'gpt_res_short_{ii}'] = vv[b_key]\n",
    "                \n",
    "                except:\n",
    "                    b_feature[f'gpt_res_short_{ii}'] = 0\n",
    "\n",
    "                try:\n",
    "                    vv = this_gpt_res_short_r2[ii]\n",
    "                    if b_key in vv.keys():\n",
    "                        b_feature[f'gpt_res_short_{ii}_r2'] = float(vv[b_key])\n",
    "                except:\n",
    "                    b_feature[f'gpt_res_short_{ii}_r2'] = 0\n",
    "\n",
    "\n",
    "            b_feature['opus_res'] = 0\n",
    "            for ii in range(10):\n",
    "                b_feature[f'turbo_res_{ii}'] = 0\n",
    "                b_feature[f'turbo_res_r2_{ii}'] = 0\n",
    "                b_feature[f'gpt4_res_note_{ii}'] = 0\n",
    "            b_feature['gemini_res'] = 0\n",
    "            b_feature['gemini_res_round2'] = 0\n",
    "            b_feature['gemini_res_round3'] = 0\n",
    "            b_feature['gemini_res_round4'] = 0\n",
    "            \n",
    "            ### opus特征\n",
    "            if types == 'test':\n",
    "                if b_key in this_opus_res.keys():\n",
    "                    b_feature['opus_res'] = this_opus_res[b_key]\n",
    "                \n",
    "                if b_key in this_gemini_res.keys():\n",
    "                    if isinstance(this_gemini_res[b_key], (int, float)):\n",
    "                        b_feature['gemini_res'] = this_gemini_res[b_key]\n",
    "\n",
    "                if b_key in this_gemini_res_round2.keys():\n",
    "                    if isinstance(this_gemini_res_round2[b_key], (int, float)):\n",
    "                        b_feature['gemini_res_round2'] = this_gemini_res_round2[b_key]\n",
    "\n",
    "                if b_key in this_gemini_res_round3.keys():\n",
    "                    if isinstance(this_gemini_res_round3[b_key], (int, float)):\n",
    "                        b_feature['gemini_res_round3'] = this_gemini_res_round3[b_key]\n",
    "\n",
    "                if b_key in this_gemini_res_round4.keys():\n",
    "                    if isinstance(this_gemini_res_round4[b_key], (int, float)):\n",
    "                        b_feature['gemini_res_round4'] = this_gemini_res_round4[b_key]\n",
    "\n",
    "            ### turbo特征\n",
    "                for ii in range(10):\n",
    "                    try:\n",
    "                        vv = this_turbo_res[ii]\n",
    "                        if b_key in vv.keys():\n",
    "                            b_feature[f'turbo_res_{ii}'] = vv[b_key]\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            ### turbo特征v2\n",
    "                for ii in range(len(this_turbo_res_v2)):\n",
    "                    try:\n",
    "                        vv = this_turbo_res_v2[ii]\n",
    "                        if b_key in vv.keys():\n",
    "                            b_feature[f'turbo_res_r2_{ii}'] = float(vv[b_key])\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "            ### gpt4 note 特征\n",
    "                for ii in range(len(gpt4_res_note)):\n",
    "                    try:\n",
    "                        vv = this_gpt4_res_note[ii]\n",
    "                        if b_key in vv.keys():\n",
    "                            b_feature[f'gpt4_res_note_{ii}'] = float(vv[b_key])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "    \n",
    "        labels_train[key] = labels_key\n",
    "        features_train[key] = features_key\n",
    "\n",
    "    ## 建立模型\n",
    "\n",
    "    wide_features = []\n",
    "    total_labels = []\n",
    "\n",
    "    for key in features_train:\n",
    "        this_features = features_train[key]\n",
    "        this_labels = labels_train[key]\n",
    "        max_ref_count = -1\n",
    "        max_positive_count = -1\n",
    "\n",
    "        for b_key in this_features:\n",
    "            this_feature = this_features[b_key]\n",
    "            if this_feature['ref_count'] > max_ref_count:\n",
    "                max_ref_count = this_feature['ref_count']\n",
    "            if this_feature['positive_count'] > max_positive_count:\n",
    "                max_positive_count = this_feature['positive_count']\n",
    "        \n",
    "        for b_key in this_features:\n",
    "            this_feature = this_features[b_key]\n",
    "            this_feature['max_ref_ratio'] = this_feature['ref_count'] / max_ref_count\n",
    "            this_feature['positive_ref_ratio'] = this_feature['positive_count'] / (max_positive_count + 0.001)\n",
    "            this_feature['paper_key'] = key\n",
    "            this_feature['b_key'] = b_key\n",
    "            total_labels.append(this_labels[b_key])\n",
    "            wide_features.append(this_feature)\n",
    "\n",
    "    import pandas as pd\n",
    "    wide_features = pd.DataFrame(wide_features)\n",
    "    total_labels = pd.DataFrame(total_labels)\n",
    "    wide_features['label'] = total_labels\n",
    "    return wide_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 788/788 [00:01<00:00, 415.94it/s]\n",
      "100%|██████████| 394/394 [00:02<00:00, 179.47it/s]\n"
     ]
    }
   ],
   "source": [
    "train_feat = make_train_features(types='train')\n",
    "test_feat = make_train_features(types='test')\n",
    "\n",
    "train_feat.to_csv('train_feat.csv')\n",
    "test_feat.to_csv('test_feat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## XGB\n",
    "## 用random forest 二分类\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "y_col = 'label'\n",
    "x_col = [x for x in train_feat.columns if x != y_col]\n",
    "x_col = [x for x in x_col if x != 'b_key' and x != 'paper_key' and x != 'pred' and x != 'positive_ref_ratio' and x != 'journal']\n",
    "x_col = [x for x in x_col if x != 'positive_count']\n",
    "x_col = [x for x in x_col if x != 'venue' and x != 'Unnamed: 0']\n",
    "x_col = [x for x in x_col if x != 'venue_type' and x != 'journal_type' and x != 'context']\n",
    "x_col = [x for x in x_col if x != 'gemini_res' and x != 'gemini_res_round2' and x!= 'gemini_res_round3']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_feat[x_col], train_feat['label'], test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save \n",
    "import sklearn.externals\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/56/b87bdh2x1kbcxmgsfhmgh1sh0000gn/T/ipykernel_31134/3561791859.py:134: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_feat_rule =  test_feat.groupby(['paper_key']).apply(rerank, rate_rule = 0.035).reset_index(drop = True)\n"
     ]
    }
   ],
   "source": [
    "    clf = lgb.LGBMClassifier(n_estimators = 50, \n",
    "                             learning_rate = 0.1, \n",
    "                             num_leaves = 50, \n",
    "                             max_depth = 5,\n",
    "                             sample_fraction=0.8,\n",
    "                             random_state=42, \n",
    "                             n_jobs=-1,\n",
    "                             verbose=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict_proba(test_feat[x_col])\n",
    "    pred = pred[:,1]\n",
    "    test_feat['pred_lgb'] = pred\n",
    "    joblib.dump(clf, 'lgb.pkl')\n",
    "\n",
    "\n",
    "    clf = cb.CatBoostClassifier(iterations = 50, \n",
    "                                learning_rate = 0.077, \n",
    "                                depth = 5,\n",
    "                                random_seed=42, \n",
    "                                verbose=0)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict_proba(test_feat[x_col])\n",
    "    pred = pred[:,1]\n",
    "    test_feat['pred_cb'] = pred\n",
    "    joblib.dump(clf, 'catboost.pkl')\n",
    "\n",
    "    test_feat['pred'] = test_feat['pred_lgb'] * 0.4 + test_feat['pred_cb'] * 0.6\n",
    "\n",
    "    def get_rule_score(row):\n",
    "        gpt_labels = []\n",
    "        gpt_shorts = []\n",
    "        turbo_res = []\n",
    "        turbo_res_r2 = []\n",
    "        turbo_res_note = []\n",
    "        gpt_shorts_v2 = []\n",
    "        for ii in range(5):\n",
    "            gpt_shorts.append(row[f'gpt_res_short_{ii}'])\n",
    "            gpt_labels.append(row[f'gpt_res_label_{ii}'])\n",
    "            gpt_shorts_v2.append(row[f'gpt_res_short_{ii}_r2'])\n",
    "        opus_res = row['opus_res']\n",
    "        for ii in range(10):\n",
    "            turbo_res.append(row[f'turbo_res_{ii}'])\n",
    "            turbo_res_r2.append(row[f'turbo_res_r2_{ii}'])\n",
    "            turbo_res_note.append(row[f'gpt4_res_note_{ii}'])\n",
    "\n",
    "        gemini_res = row['gemini_res']\n",
    "        gemini_res_r2 =  row['gemini_res_round2']\n",
    "        gemini_res_r3 =  row['gemini_res_round3']\n",
    "        gemini_res_r4 =  row['gemini_res_round4']\n",
    "\n",
    "        num_inspired_by = row['num_inspired_by']\n",
    "        num_motivated_by = row['num_motivated_by']\n",
    "            \n",
    "\n",
    "        def score2rank(x):\n",
    "            if x >= 0.9:\n",
    "                return 3\n",
    "            elif x >= 0.5:\n",
    "                return 2\n",
    "            elif x > 0.4:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "        opus_rank = score2rank(opus_res)\n",
    "        gemini_rank = score2rank(gemini_res)\n",
    "        gemini_rank_r2 = score2rank(gemini_res_r2)\n",
    "        gemini_rank_r3 = score2rank(gemini_res_r3)\n",
    "        gemini_rank_r4 = score2rank(gemini_res_r4)\n",
    "        gemini_res_total = [gemini_res, gemini_res_r2, gemini_res_r3, gemini_res_r4]\n",
    "\n",
    "        turbo_rank_max = np.max([score2rank(x) for x in turbo_res])\n",
    "        turbo_rank_mean = np.median([score2rank(x) for x in turbo_res])\n",
    "        turbo_rank_min = np.min([score2rank(x) for x in turbo_res])\n",
    "\n",
    "        gpt4o_rank_max = np.max([score2rank(x) for x in gpt_shorts])\n",
    "        gpt4o_rank_mean = np.median([score2rank(x) for x in gpt_shorts])\n",
    "        gpt4o_rank_min = np.min([score2rank(x) for x in gpt_shorts])\n",
    "\n",
    "        turbo_rank_max_r2 = np.max([score2rank(x) for x in turbo_res_r2])\n",
    "        turbo_rank_mean_r2 = np.median([score2rank(x) for x in turbo_res_r2])\n",
    "        turbo_rank_min_r2 = np.min([score2rank(x) for x in turbo_res_r2])\n",
    "\n",
    "        turbo_res_note_max = np.max([score2rank(x)  for x in turbo_res_note])\n",
    "        turbo_res_note_mean = np.median([score2rank(x)  for x in turbo_res_note])\n",
    "        turbo_res_note_min = np.min([score2rank(x)  for x in turbo_res_note])\n",
    "\n",
    "        col1 =  [opus_rank] \n",
    "        col2 =  [gemini_rank] + [gemini_rank_r3] + [gemini_rank_r2]\n",
    "        col3 =  [turbo_rank_max] + [gpt4o_rank_max] + [turbo_rank_max_r2] + [turbo_rank_mean_r2] + [turbo_rank_mean] + [gpt4o_rank_mean] \n",
    "        col4 = [turbo_res_note_max] + [turbo_res_note_mean] + [gemini_rank_r4]\n",
    "        \n",
    "        \n",
    "        rule_counter1 = Counter(col1)\n",
    "        rule_counter2 = Counter(col2)\n",
    "        rule_counter3 = Counter(col3)\n",
    "        rule_counter4 = Counter(col4)\n",
    "\n",
    "        num_31, num_21, num_11 = rule_counter1[3] , rule_counter1[2], rule_counter1[1]\n",
    "        num_32, num_22, num_12 = rule_counter2[3] , rule_counter2[2], rule_counter2[1]\n",
    "        num_33, num_23, num_13 = rule_counter3[3] , rule_counter3[2], rule_counter3[1]\n",
    "        num_34, num_24, num_14 = rule_counter4[3] , rule_counter4[2], rule_counter4[1]\n",
    "\n",
    "        k1 = 2\n",
    "        k2 = 1\n",
    "        k3 = 0.5\n",
    "        k4 = 1\n",
    "\n",
    "        score1 = num_31 * k1 + num_32 * k2 + num_33 * k3 + num_34 * k4\n",
    "        score2 = num_21 * k1 + num_22 * k2 + num_23 * k3 + num_24 * k4\n",
    "        score3 = num_11 * k1 + num_12 * k2 + num_13 * k3 + num_14 * k4\n",
    "        \n",
    "        score = score1 * 4 + score2 * 2 + score3 \n",
    "        \n",
    "        \n",
    "\n",
    "        ### ADD Penalty\n",
    "        total_single_score = gpt_shorts + gpt_labels + gpt_shorts_v2 + turbo_res_note + turbo_res + turbo_res_r2 + gemini_res_total + [opus_res]\n",
    "        total_single_score = sorted(total_single_score)\n",
    "        if total_single_score[20] <= 0.2:\n",
    "             score = score / 4\n",
    "        \n",
    "        return score \n",
    "\n",
    "    \n",
    "    test_feat['rule_score'] = test_feat.apply(get_rule_score, axis = 1)\n",
    "\n",
    "\n",
    "    def rerank(df, rate_rule):\n",
    "        df['pred_rule'] = df['pred'] + df['rule_score'] * rate_rule\n",
    "        return df\n",
    "\n",
    "    test_feat_rule =  test_feat.groupby(['paper_key']).apply(rerank, rate_rule = 0.035).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# 写入submission\n",
    "sample_submission = \"submission_example_test.json\"\n",
    "sample_submission = json.load(open(sample_submission, \"r\"))\n",
    "import copy\n",
    "sample_submission_submit = copy.deepcopy(sample_submission)\n",
    "\n",
    "for sub_key in sample_submission.keys():\n",
    "    my_ans = []\n",
    "    my_pred_res = test_feat_rule.loc[test_feat_rule['paper_key'] == sub_key]\n",
    "    num_pred = len(sample_submission[sub_key])\n",
    "    for i in range(num_pred):\n",
    "        b_key = 'b' + str(i)\n",
    "        try:\n",
    "            #my_ans.append(sigmoid(my_pred_res.loc[my_pred_res['b_key'] == b_key]['pred'].values[0]))\n",
    "            my_ans.append(sigmoid(my_pred_res.loc[my_pred_res['b_key'] == b_key]['pred_rule'].values[0]))\n",
    "        except:\n",
    "            my_ans.append(0)\n",
    "\n",
    "    sample_submission_submit[sub_key] = my_ans\n",
    "\n",
    "\n",
    "import json\n",
    "with open('sample_submission_0607_1.json', 'w') as f:\n",
    "    json.dump(sample_submission_submit, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1576"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xeek2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
