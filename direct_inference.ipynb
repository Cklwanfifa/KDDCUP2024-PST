{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maggiewang/anaconda3/envs/xeek2/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "2024-06-13 08:51:56,779 loading paper_source_trace_train_ans.json ...\n",
      "2024-06-13 08:51:56,839 paper_source_trace_train_ans.json loaded\n",
      "2024-06-13 08:51:56,840 loading paper_source_trace_valid_wo_ans.json ...\n",
      "2024-06-13 08:51:56,851 paper_source_trace_valid_wo_ans.json loaded\n",
      "2024-06-13 08:51:56,852 loading paper_source_trace_test_wo_ans.json ...\n",
      "2024-06-13 08:51:56,865 paper_source_trace_test_wo_ans.json loaded\n"
     ]
    }
   ],
   "source": [
    "## 尝试做特征\n",
    "import os\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict as dd\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, average_precision_score\n",
    "import logging\n",
    "\n",
    "import utils\n",
    "import settings\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "data_dir = settings.DATA_TRACE_DIR\n",
    "\n",
    "papers_train = utils.load_json('', \"paper_source_trace_train_ans.json\")\n",
    "papers_valid = utils.load_json('', \"paper_source_trace_valid_wo_ans.json\")\n",
    "papers_test = utils.load_json('',  \"paper_source_trace_test_wo_ans.json\")\n",
    "\n",
    "\n",
    "files = []\n",
    "data_dir = './'\n",
    "in_dir = join(data_dir, 'paper-xml')\n",
    "for f in os.listdir(in_dir):\n",
    "    if f.endswith('.xml'):\n",
    "        files.append(f)\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import collections\n",
    "import json\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^a-zA-Z]+', ' ', text).lower().strip()\n",
    "\n",
    "import re\n",
    "def get_pattern(input_string):\n",
    "    pattern = r'\\n\\n(.*?)\\n\\n'\n",
    "\n",
    "# 执行正则表达式搜索\n",
    "    match = re.search(pattern, input_string)\n",
    "\n",
    "    # 检查是否有匹配，如果有，则输出匹配到的内容\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "## 读数据-GPT\n",
    "## 读数据\n",
    "\n",
    "\n",
    "## test\n",
    "id_train = [ x['_id'] for x in papers_train]\n",
    "id_valid = [ x['_id'] for x in papers_valid]\n",
    "id_test = [ x['_id'] for x in papers_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_feat = pd.read_csv('train_feat.csv')\n",
    "test_feat = pd.read_csv('test_feat.csv')\n",
    "\n",
    "## XGB\n",
    "## 用random forest 二分类\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "y_col = 'label'\n",
    "x_col = [x for x in train_feat.columns if x != y_col]\n",
    "x_col = [x for x in x_col if x != 'b_key' and x != 'paper_key' and x != 'pred' and x != 'positive_ref_ratio' and x != 'journal']\n",
    "x_col = [x for x in x_col if x != 'positive_count']\n",
    "x_col = [x for x in x_col if x != 'venue' and x != 'Unnamed: 0']\n",
    "x_col = [x for x in x_col if x != 'venue_type' and x != 'journal_type' and x != 'context']\n",
    "x_col = [x for x in x_col if x != 'gemini_res' and x != 'gemini_res_round2' and x!= 'gemini_res_round3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load\n",
    "import sklearn.externals\n",
    "import joblib\n",
    "\n",
    "clf_lgb = joblib.load('lgb.pkl')\n",
    "clf_catboost = joblib.load('catboost.pkl')\n",
    "\n",
    "pred = clf_lgb.predict_proba(test_feat[x_col])\n",
    "pred = pred[:,1]\n",
    "test_feat['pred_lgb'] = pred\n",
    "\n",
    "pred = clf_catboost.predict_proba(test_feat[x_col])\n",
    "pred = pred[:,1]\n",
    "test_feat['pred_cb'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/56/b87bdh2x1kbcxmgsfhmgh1sh0000gn/T/ipykernel_32330/358292129.py:107: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_feat_rule =  test_feat.groupby(['paper_key']).apply(rerank, rate_rule = 0.035).reset_index(drop = True)\n"
     ]
    }
   ],
   "source": [
    "    test_feat['pred'] = test_feat['pred_lgb'] * 0.4 + test_feat['pred_cb'] * 0.6\n",
    "\n",
    "    def get_rule_score(row):\n",
    "        gpt_labels = []\n",
    "        gpt_shorts = []\n",
    "        turbo_res = []\n",
    "        turbo_res_r2 = []\n",
    "        turbo_res_note = []\n",
    "        gpt_shorts_v2 = []\n",
    "        for ii in range(5):\n",
    "            gpt_shorts.append(row[f'gpt_res_short_{ii}'])\n",
    "            gpt_labels.append(row[f'gpt_res_label_{ii}'])\n",
    "            gpt_shorts_v2.append(row[f'gpt_res_short_{ii}_r2'])\n",
    "        opus_res = row['opus_res']\n",
    "        for ii in range(10):\n",
    "            turbo_res.append(row[f'turbo_res_{ii}'])\n",
    "            turbo_res_r2.append(row[f'turbo_res_r2_{ii}'])\n",
    "            turbo_res_note.append(row[f'gpt4_res_note_{ii}'])\n",
    "\n",
    "        gemini_res = row['gemini_res']\n",
    "        gemini_res_r2 =  row['gemini_res_round2']\n",
    "        gemini_res_r3 =  row['gemini_res_round3']\n",
    "        gemini_res_r4 =  row['gemini_res_round4']\n",
    "\n",
    "        num_inspired_by = row['num_inspired_by']\n",
    "        num_motivated_by = row['num_motivated_by']\n",
    "            \n",
    "\n",
    "        def score2rank(x):\n",
    "            if x >= 0.9:\n",
    "                return 3\n",
    "            elif x >= 0.5:\n",
    "                return 2\n",
    "            elif x > 0.4:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "        opus_rank = score2rank(opus_res)\n",
    "        gemini_rank = score2rank(gemini_res)\n",
    "        gemini_rank_r2 = score2rank(gemini_res_r2)\n",
    "        gemini_rank_r3 = score2rank(gemini_res_r3)\n",
    "        gemini_rank_r4 = score2rank(gemini_res_r4)\n",
    "        gemini_res_total = [gemini_res, gemini_res_r2, gemini_res_r3, gemini_res_r4]\n",
    "\n",
    "        turbo_rank_max = np.max([score2rank(x) for x in turbo_res])\n",
    "        turbo_rank_mean = np.median([score2rank(x) for x in turbo_res])\n",
    "        turbo_rank_min = np.min([score2rank(x) for x in turbo_res])\n",
    "\n",
    "        gpt4o_rank_max = np.max([score2rank(x) for x in gpt_shorts])\n",
    "        gpt4o_rank_mean = np.median([score2rank(x) for x in gpt_shorts])\n",
    "        gpt4o_rank_min = np.min([score2rank(x) for x in gpt_shorts])\n",
    "\n",
    "        turbo_rank_max_r2 = np.max([score2rank(x) for x in turbo_res_r2])\n",
    "        turbo_rank_mean_r2 = np.median([score2rank(x) for x in turbo_res_r2])\n",
    "        turbo_rank_min_r2 = np.min([score2rank(x) for x in turbo_res_r2])\n",
    "\n",
    "        turbo_res_note_max = np.max([score2rank(x)  for x in turbo_res_note])\n",
    "        turbo_res_note_mean = np.median([score2rank(x)  for x in turbo_res_note])\n",
    "        turbo_res_note_min = np.min([score2rank(x)  for x in turbo_res_note])\n",
    "\n",
    "        col1 =  [opus_rank] \n",
    "        col2 =  [gemini_rank] + [gemini_rank_r3] + [gemini_rank_r2]\n",
    "        col3 =  [turbo_rank_max] + [gpt4o_rank_max] + [turbo_rank_max_r2] + [turbo_rank_mean_r2] + [turbo_rank_mean] + [gpt4o_rank_mean] \n",
    "        col4 = [turbo_res_note_max] + [turbo_res_note_mean] + [gemini_rank_r4]\n",
    "        \n",
    "        \n",
    "        rule_counter1 = Counter(col1)\n",
    "        rule_counter2 = Counter(col2)\n",
    "        rule_counter3 = Counter(col3)\n",
    "        rule_counter4 = Counter(col4)\n",
    "\n",
    "        num_31, num_21, num_11 = rule_counter1[3] , rule_counter1[2], rule_counter1[1]\n",
    "        num_32, num_22, num_12 = rule_counter2[3] , rule_counter2[2], rule_counter2[1]\n",
    "        num_33, num_23, num_13 = rule_counter3[3] , rule_counter3[2], rule_counter3[1]\n",
    "        num_34, num_24, num_14 = rule_counter4[3] , rule_counter4[2], rule_counter4[1]\n",
    "\n",
    "        k1 = 2\n",
    "        k2 = 1\n",
    "        k3 = 0.5\n",
    "        k4 = 1\n",
    "\n",
    "        score1 = num_31 * k1 + num_32 * k2 + num_33 * k3 + num_34 * k4\n",
    "        score2 = num_21 * k1 + num_22 * k2 + num_23 * k3 + num_24 * k4\n",
    "        score3 = num_11 * k1 + num_12 * k2 + num_13 * k3 + num_14 * k4\n",
    "        \n",
    "        score = score1 * 4 + score2 * 2 + score3 \n",
    "        \n",
    "        \n",
    "\n",
    "        ### ADD Penalty\n",
    "        total_single_score = gpt_shorts + gpt_labels + gpt_shorts_v2 + turbo_res_note + turbo_res + turbo_res_r2 + gemini_res_total + [opus_res]\n",
    "        total_single_score = sorted(total_single_score)\n",
    "        if total_single_score[20] <= 0.2:\n",
    "             score = score / 4\n",
    "        \n",
    "        return score \n",
    "\n",
    "    \n",
    "    test_feat['rule_score'] = test_feat.apply(get_rule_score, axis = 1)\n",
    "\n",
    "\n",
    "    def rerank(df, rate_rule):\n",
    "        df['pred_rule'] = df['pred'] + df['rule_score'] * rate_rule\n",
    "        return df\n",
    "\n",
    "    test_feat_rule =  test_feat.groupby(['paper_key']).apply(rerank, rate_rule = 0.035).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# 写入submission\n",
    "sample_submission = \"submission_example_test.json\"\n",
    "sample_submission = json.load(open(sample_submission, \"r\"))\n",
    "import copy\n",
    "sample_submission_submit = copy.deepcopy(sample_submission)\n",
    "\n",
    "for sub_key in sample_submission.keys():\n",
    "    my_ans = []\n",
    "    my_pred_res = test_feat_rule.loc[test_feat_rule['paper_key'] == sub_key]\n",
    "    num_pred = len(sample_submission[sub_key])\n",
    "    for i in range(num_pred):\n",
    "        b_key = 'b' + str(i)\n",
    "        try:\n",
    "            #my_ans.append(sigmoid(my_pred_res.loc[my_pred_res['b_key'] == b_key]['pred'].values[0]))\n",
    "            my_ans.append(sigmoid(my_pred_res.loc[my_pred_res['b_key'] == b_key]['pred_rule'].values[0]))\n",
    "        except:\n",
    "            my_ans.append(0)\n",
    "\n",
    "    sample_submission_submit[sub_key] = my_ans\n",
    "\n",
    "\n",
    "import json\n",
    "with open('sample_submission_0607_1_only_inference.json', 'w') as f:\n",
    "    json.dump(sample_submission_submit, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xeek2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
